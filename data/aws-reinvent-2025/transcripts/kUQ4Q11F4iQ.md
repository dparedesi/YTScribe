---
video_id: kUQ4Q11F4iQ
video_url: https://www.youtube.com/watch?v=kUQ4Q11F4iQ
is_generated: False
is_translatable: True
summary: "This session, \"Supercharge your Karpenter: Tactics for smarter K8s optimization (COP208),\" features Alexei, CTO of Zesty, sharing strategies to optimize Kubernetes clusters using Karpenter. He emphasizes balancing performance and cost by finding the \"sweet spot\" rather than over- or under-provisioning. Key tactics include using Karpenter's consolidation policies with guardrails (Pod Disruption Budgets, consolidation delays) to safely terminate underutilized nodes. Alexei advises diversifying instance types (avoiding too small or too large nodes) and limiting DaemonSets to reduce overhead. He discusses leveraging Spot Instances with node affinity for fallback to On-Demand, using Graviton (ARM) instances for better price/performance, and restricting non-prod workloads to a single AZ to save data transfer costs. Despite Karpenter's benefits, manual tuning is complex. Alexei introduces Zesty's platform as a continuous automation layer that provides visibility, predictive scaling, vertical/horizontal right-sizing (for CPU, RAM, and persistent volumes), and \"fast scale\" hibernated nodes to recover from Spot interruptions in 30 seconds, enabling 70%+ cost savings."
keywords: Karpenter, Kubernetes, Cost Optimization, Spot Instances, Zesty, Autoscaling, Right-sizing, DaemonSets, Graviton, Cloud Efficiency
---

Hey guys, welcome. Thank you for joining the session, and we're gonna spend the next roughly 15 minutes talking about Carpenter and it's great features that will actually help you achieve cost efficiency and make your setup really adaptive at scale. So before we start, I would actually want to ask you a quick question. How much of you guys are using Carpenter today in production? Perfect, amazing, and keep your hands up those who use it for managing spot instances. Amazing, perfect, thank you for sharing. So my name is Alexei. I'm a co-founder and CTO at Zesty, where we are focusing on making cloud really efficient, and I hope you will actually be able to take some actual, um, actual and practical tips from this presentation that you can implement this in your setup. Let's dive in So when actually it comes to stability and performance, it's really easy to solve the problem by just throwing money at the problem, right? We can always take the biggest, the most powerful instances out there and just get an amazing performance at a terrible cost. On the other hand, if we squeeze the budget and we scale down to the bare, bare minimum, and you know, and suddenly efficient becomes unavailable, so as DevOps engineers and platform engineers, our goal is to find the sweet spot. Our goal is to run complex production workloads that are demanding for resources, give them just the right infrastructure, and It shouldn't be overprovisioned or underprovisioned, and this is a tough balance to strike, right? And especially in Kubernetis, it always means tuning the configuration to stay efficient, and it's easier said than done. So let's dive in. Basically one of the um one of the best features in Carpenter, it's very powerful, called consolidation policies. What it actually does, it takes and monitors your uh your nodes and looking for underutilized nodes and actually checking if it can safely terminate them to save costs and move the containers somewhere somewhere else. Now. While this sounds appealing and sounds great, there are some, um, some guardrails and fine tunings to this behavior you want to have in your setup. Number one is using what's called PDB or pod disruption budgets. It actually controls. And making sure that not all of your replicas are being consolidated at the same time. You want to make sure that you control how much replicas Carpenter can interrupt at any given moment to keep your setup safe, right? Another thing is consolidation, consolidation delays. You can always configure how long Carpenter will wait until it actually goes and terminates nodes. And you're setting this in order to prevent carpenter from doing this during short term dips in usage, so it's not going to, you know, go crazy and interrupt your containers. So by control, by actually implementing those controls, you can consolidate, you can avoid consolidation during your peak peak traffic hours, and you can see an example configuration here to set this up. OK? Another important strategy is to choose the right instances for your pool. So it's not just about what's valuable, it's about giving the carpenter the right amount of flexibility and give him, give him a set of of broader choice. So you definitely want to have diversification because carpenter actually balances between cost and availability and trying to find the instance that delivers the best performance at lowest cost. So in theory, the more instance instance types and sizes you you give Carpenter across different families, generations, and sizes, so in theory it can find a better, better fit, but you always wanna want to avoid extremes, right? You don't want to include instances that are too small. Because small instances are causing a lot of network overhead, they actually adding more complexity to scheduling and and monitoring. So in this, in this example, we are actually cutting out all the tiny flavors, for example. But also you don't want to have two large nodes because we spoke about consolid consolidation earlier and basically large nodes that are not fully utilized is a waste of time is a waste of money, right, and making the consolidation less efficient. So to summarize, basically all we, all we want to do is constantly monitor our, our, the behavior of our uh of our cluster, and it's up for, up, up to us to define what's too small or too big in, in our exact case. Another, another factor that is constantly overlooked is demon sets. Basically those are. Containers that are running for system services like monitoring, logging, and telemetry and you know all of the system stuff and running per each node. So when we have larger nodes, we have small overhead of demon sets. And on the opposite, if we have a lot of small nodes, it means we have a large overhead of of demon sets and actually this is consuming resources. And especially if we're using any software that is licensed per node, this can actually silently inflate inflate your costs, so take this also as a consideration. So let's be honest, talking about cost savings without mentioning spot instances probably is not the right thing. And the good news is that Carpenter makes management of spot instances really easy. You just need to allow spot instances in your note pool, and Carpenter will just take it further. It will automatically identify. Spots with best best price and availability, take this into account and choose the right machines and also take care if they're interrupted so pods can move somewhere else and basically provide cost savings. While this this sounds good, so let's think where this can fit, right, where we can use spot instances. And if you ask me, I think everywhere, every workload that can tolerate for interrupts, whether it's a web server behind load balancer, whether it's a stateless microservice, batch jobs, your dev, your staging environments, all of them can recover, recover quickly from spot interruptions, and you can leverage this for cost savings. So the best practice here is actually to configure uh configure spot instances with a failover to on demand, and you can see an example configuration here. This actually will Carpenter in this case will actually try to provision a spot instance for cost savings, and if there is no capacity, we will transparently move you to on demand. So it's a good idea to create two separate node pools for spots and on-demand so you can actually use you can use node affinity in your deployments to configure how much you put to spot and how much you keep on demand for safety. And finally, you always need to keep, keep an eye on fallbacks. If you have too much fallbacks and you're falling to on demand frequently. And we spoke about choosing the right instances. Maybe in this case you want to give Carpenter a much broader selection because you know you can use instances that may be not the perfect fit but still providing more than 60% discount, so it's the right thing to do. Another strategy would be using graviton instances, as you probably know, graviton provides at least the same performance or even better for a lower price. And most of the modern applications are able to run on ARM architectures with no code change. The only investment you have here is to make sure that during your CI process you're building your dockers both for ARM and ICS 86, and you have both both versions of the container in your registry, and Kubernetes will automatically pick the right one depends on the node that this workload is running. Another thing is actually it's a last tip for today, and please use this with caution, not for production, but it can really save some costs. As you probably know, cross availability zone traffic is very expensive. So for a non-production environment, what you can do is actually you can limit your note pull to a specific AZ. And it basically means that you can enforce data locality at the infrastructure level, so it's actually going to reduce significantly the cost of of networking, but again, it's not something you can do in production. I don't recommend putting this stuff in production. But as you can see here in this example, we can also configure a failover, basically use weights and affinity rules to make sure if there is a a failure of a zone, we can actually go and utilize another one. OK, so when we spoke about all of these best practices and the big question comes up, are we done? And the answer is not really. Basically there is still a lot of challenges to to solve, and let's let's mention them. Ubernetis gives us a lot of metrics, and it's really hard to connect them down to costs in order to identify inefficiencies. So especially when we operate large environments with multiple clusters, so we talked about Carpenter a lot. And even if Carpenter makes instant decisions and it's working pretty amazing, it, it doesn't mean that the underlying infrastructure can keep up because it takes a lot of time. It's about a couple of minutes for the infrastructure to be ready to serve our containers. And obviously there's still operational complexity to make sure we constantly modify and adjust our setup to make it truly truly efficient. So basically the question is, can you afford your teams to be constantly invested into making sure your environment is, is configured properly? At the end of the day, in large environments, we actually need a continuous automation. Something that will provide us the ability to be focused on building rather than optimizing, and this is exactly where tools and platforms actually come to assist. So at Zesty we developed a Kubernetis optimization platform that helps closing those gaps. Our platform makes Actually actually provides you with a deep visibility into all of your clusters and containers, both from usage and utilization perspective and making it very easy to identify inefficiencies. It uses predictive models to make scaling decisions far more efficient. We execute optimizations, actions automatically on your behalf, so again, your teams can focus on building and not optimizing. So our platform automatically resize workloads both vertically and horizontally based on workloads behavior. It covers not just CPU and memory but also persistent volumes for stateful workloads that that are requiring a local storage. ZestT automatically distributes, and by the way we're calling this a resource optimization layer that actually goes and doing active right sizing. And also we are navigating and distributing workloads between spot instances and saving plans to reduce the expensive on-demand costs. With our fast scale technology, basically we maintain a fleet of hibernated nodes that are pre-cached and have all of your containers and stuff ready to kick in in 30 seconds. And basically this allows us an instant, instant recovery from spot instance failures so we can move more workloads to utilize spots without the risk of being interrupted or having no capacity, so we call it the financial optimization layer and basically Zesty is the only platform out there that combines both resource optimization and cloud financial optimization practices. To provide at least 70% cost savings for your clusters. And if you guys want to learn more and hear exactly how we can help companies achieve efficiency, our booth is actually located right here to my right. So feel free to come, uh, say hi and ask questions. Thank you so much and have a good conversation, good convention ahead. Thank you.
