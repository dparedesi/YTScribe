---
video_id: L_Q7LPB5HcA
video_url: https://www.youtube.com/watch?v=L_Q7LPB5HcA
title: AWS re:Invent 2025 - Harnessing analytics for humans and AI (INV201)
author: AWS Events
published_date: 2025-12-03
length_minutes: 61.32
views: 1039
description: "Emerging trends, ranging from Open Table Formats (OTF) to agentic infrastructure, are rapidly changing how humans and applications interact with analytics to drive mission-critical business decisions. Join Mai-Lan Tomsen Bukovec, VP of AWS Technology, to explore emerging trends, the evolution of analytics engines and applications, and how to future-proof your data foundation for the rapidly changing landscape of analytics at scale. Learn how AWS is transforming data and analytics services to lea..."
keywords: AWS, Amazon Web Services, AWS Cloud, Amazon Cloud, AWS re:Invent, AWS Summit, AWS re:Inforce, AWS reInforce, AWS reInvent, AWS Events
is_generated: False
is_translatable: True
---

Please welcome to the stage vice president of Technology, data and analytics at AWS Mai-Lan Tomsen Bukovec. Welcome. Welcome to the analytics leadership talk. My name is Mylan Thompson Bukovvic, and I run data and analytics services for AWS. Today, we're gonna walk through 3 emerging trends in the world of analytics and how our customers are steering into them with their data strategies and their AWS services. Let's start first with how Agentic AI is delivering assistance in every step of your data journey, whether it is writing code, processing data in pipelines, or using data products, just like a human would, except much, much faster. Now,
it's incredible how far agents have come in the last year. So many of our customers have put AI agents to work on real production workloads, and we have been doing the same. We have been working hard to integrate AI into our analytic services for you to use for the jobs to be done. As data lakes have grown, if you saw in the keynote this morning, you saw that data lakes were represented as a data ocean in the Sony presentation. Analytics have also been integrated in every aspect of business, and agentic AI, it's starting to become a natural part of how humans and applications work. Now we know that data and AI are increasingly intertwined, so we're building AI smarts into how you work with data on AWS. Let me show you what I mean. Now,
we just launched this week, a highly optimized Spark 3.5.6 engine as part of the latest versions of EMR, Glue, Athena Spark, and in our, our new notebook for Sagemaker. So now you have one AWS optimized Spark engine that's powering these different AWS services and experiences. It's a super exciting launch, especially for the many customers out there using Iceberg as their open data format. So as you can see, our optimized Spark engine, it speeds up both read and write performance on Iceberg data as well as any of the interactive queries that you're doing in Athena. But Spark major version upgrades often require at least some refactoring of the applications that are sitting on top. So we've incorporated almost a decade of helping customers migrate to new versions of Spark into AI assistance, so you can now use AWS AI to more easily migrate your applications to our new optimized Spark engine and EMR and Glue. So in order to take advantage of this AI assistance, you have different ways of doing it. You can do it in local VS code or through Tools and SageMaker Unified Studio, or you can use our remote MCP servers server from Kiro or any of your favorite coding assistance environments. The Spark upgrade agent is going to go all the way back to helping you with upgrades from versions as early as Glue 2.0, EMR and EC2 5.2, and any of your previous EMR serverless versions. We're super excited about this. Now,
in AWS we have a saying, we say that there's no compression algorithm for experience, and our new spark upgrade runs off of a knowledge base that's based off thousands of spark upgrades, some that have gone well, and some that have not. It plans the upgrade. It works through the error messages. It remediates failures automatically. And more. So now you can finish the work of upgrading your app to the latest Spark in EMR and Glue, and you can do it in weeks instead of months. This combination of a highly optimized Spark 3.5.6 engine and AI assistance for upgrades, it's a real game changer for our Spark Spark customers. It lets you modernize quickly with the help of AI from AWS. FINRA, which safeguards the integrity of the US capital markets, tried using our upgrade AI agent earlier this year, and now they plan to anchor their Spark version management on it entirely in 2026. Now,
agents are a great fit for spark upgrades because it, it automates work repetitive workflows, but AI is also really good at helping you write and fix code. So we've built AI into our new notebook in SageMaker, and we're also using AI under the hood inside our AWS services, like how Redshift query Engine uses MLAI to optimize queries and manage serverless clusters. Now,
we launched the new Sagemaker notebook a few weeks ago. And when we launched this capability, we also integrated AI capabilities directly into the new notebook experience. So you have one place to process and analyze using SQL, Python, or natural language with AI assistance built in. We built this new notebook to make working with data easier, and the goal here is to take away the operational burden of managing the analytic services that the notebook uses behind the scenes. And so if you're explorer exploring data analysis, or you're building an ETL pipeline, or you're training a machine learning note, uh, model, The notebook provides you the streamlined programming environment for your end to end workflows, and it doesn't require you to manage any infrastructure. It connects with your data wherever it resides, such as iceberg tables on a 3 or your Amazon Redshift data warehouse. Now,
as part of this notebook experience, we've built in AI assistance. And it's there to support you every step of the way. It can answer your questions, it can write scripts or SQL queries. It can generate pipelines and create visualizations. And so the experience of working with data with a notebook is more intuitive. It's more efficient, and frankly, it's more fun to do data exploration and insight generation. Now,
unlike traditional notebooks like Jupiter Lab IDE, which Sagemaker also supports, this new notebook is entirely serverless, so you don't need to tune or manage or pre-provision query processing infrastructure. Now, what's really interesting about this Sagemaker notebook is that it's polyglot, and that means you can express your code in Python or SQL cells that then interoperate with each other in the same notebook. So that means you can write PySpark using Spark Connect in Python cells, and in Sequel cells, you can write SQL queries against Redshift or Atheneutrino or a third-party analytics engine like Snowflake, and you can reuse your results in Python. This notebook is an all in one data notebook that you can use with any open analytics architecture now and in the future. The AI assistance that we've built into the data notebook, it helps you with the data querying, the exploratory data analysis, and the model development. And the most interesting thing I think, is you can describe your objectives in natural language right in notebooks cells. So the agent that's under the hood for our AI assistance, it's using context, and it's using context from the data catalogs and metadata, and the rest of the work that you do in your notebook to suggest these execution plans to generate code and just to help you build. So you can use the notebook in SageMaker, you can click into it from the AWS management console, or if you want to use the notebook capabilities, you can use a remote MCP server from your favorite coding environment. Now, here to show you how we've integrated our new SageMaker notebooks and our AI agent together in an integrated experience is Diane, the principal engineer from SageMaker Unified Studio. Diane? Hi,
Dad. I got. Thank you Milan. Good afternoon everyone. My name is Diane Alener. I'm a principal engineer on Sagemaker Unified Studio, and I'm really excited to be here because I wanna show you guys our new notebooks, uh,
in action. So let's take a look. So this is our notebook interface. It's a familiar interface, but it's, it's more modernized. It's got a collection of cells where you can express your code in Python, SQL, Markdown, or natural language. You see the data explorer here, I've got the New York City taxi data and an iceberg table on S3, and I have my chat agent uh open on the right. So let's get into some of the basics. I can create a Python cell, read some sample data into a panda's data frame, and see the output of the data in rich data tables. As I explore my data, I might notice a unit price column here, uh,
and there might be another column of interest, well,
say the category column, and then I can use these pieces of information to write a SQL query on my Python data frame. I see the output in the same rich data table, and if I really wanted to step up my data exploration, I could use the notebooks, the native visualization capabilities, and render the data in interactive charts. Great. So the basics out of the way, let's use that New York City taxi data set to get some real-world jobs to be done. So what I can do is I can ask the agent to read my New York City taxi data set into a Spark data frame and give me the shape of the data. So immediately, the agent generates a plan for how it's gonna find and read the data into Spark, and what kind of exploratory analysis it will do. Um, what we can see is that Spark is just available in the notebook. There's no infrastructure set up, no clusters, no complicated configuration. It's just there, ready to go to work on your data. Now what's powerful here is that the system understands where the data lives, automatically chooses the best way to query the data, and it shows me the exploration steps, and upon completion, it summarizes what the steps have been able to accomplish. So this is giving me a sense of the scheme of the data set. It produces some statistical summaries on, on factors such as trip distance, uh, fare, and tip and things like that. So every analysis comes with generated visualizations, and it really takes exploratory data analysis to the next level. I,
as a user, I can just sit back and watch. The notebook is doing all the work. So as we kind of go through this uh data exploration journey, um, there's a, the,
uh, the data set might be large or small, but sometimes what happens is we might run into some issues. Because not everything goes according to plan. Um, when that happens, it's very difficult to kind of, uh, sift through all the large exceptions or try to stack traces, and for developers such as myself and many of you out here it's very difficult to figure out what the actual problem is when visualizations don't render it's really not nice. Um, so what I can do on the notebook is I can, I have fixed with AI as a capability, and all I need to do is click the button. When I click fix with AI, the agent will now analyze the exception, recommend the code that will fix the issue, and all I have to do is take a look at the code that are generated and hit accept and run. Just like that, my problem is solved. My visualizations are rendered. So what this kind of gives me a sense of is that my data might not be of the best quality. So, so given that, I can go ahead and ask the agent to clean my data set, and I could give it some factors it should consider as it performs the cleaning steps. So what's happening here is that the agent understands both the intent and the context of the data. It's doing in seconds what would usually take multiple scripts, queries, and validation steps. The notebook automatically generates spark code, and every step of the way it shows the transformations that it applies to the cleaning process and a summary of the outcome. It's really it's really powerful to also see the outcome of the data and the preparation steps because I can really get a sense of how the quality of the data is improving based on the visualizations that I see on screen. Great. So now that I have some high quality data, I can ask the agent to, to train a model that predicts trip prices. So the agent does here is that it recommends a regression approach. Um, it writes the code to, to train the model and it returns a summary of the results and include, includes things like, uh,
feature importance, uh,
accuracy metrics and things like that. It even generated code so that I can visualize the prediction that the model would make. Um, so once again, like I can just sit back and relax and watch the notebook as it trains the model, shows the model performance, uh,
gives me a sense of the future importance, and shows the visualizations of the predictions and, um, for my success and, and,
and errors. Now what's really powerful here is we're not switching tools. We've gone from raw data to model insights all in one notebook and AI is doing all the heavy lifting. So I've done a lot of work and there's a lot of rich context that's built into the notebook. So finally I can ask the agent, uh,
some general questions about the data. For example, here I ask it, you know, explain the key drivers of, uh of price, which is sort of like a general question. So here now the agent has all the access to all the rich context. So what it gives me is some key findings not only in a text summary, but it's also given me, uh,
a little bit of code. Um, what you'll see here is when I accept and run the code, uh,
what it will what the notebook will do is translate those insights into meaningful visualizations, and I can use this on a dashboard or in a report. Great, so now I've cleaned data from which I can generate insights. It's often useful to write this data back into my data lake so that I can, I can do, uh, business intelligence or things like that. So I asked the agent to write the data back into my, uh,
data lake. It gives me spark code, um, which,
uh, which will write the clean data set as an iceberg table back into my data lake. Um, it runs the code. I can see the schema and gives me a little bit of uh statistics here as it does the right. And now if I dive back into my data explorer and expand my default database, I see the New York City taxi, uh, uh, database here cleaned and ready to go, uh, for further analytics. So what we've just seen here though in the last few minutes like we've taken uh some data that was on Iceberg um on S3 we've cleaned it we've transformed it we trained the predictive model and we produced business ready insights all within a single serverless agentic notebooks. Now this is really what building data products with AI really looks like. It's less about managing infrastructure and more about accelerating the accelerating the path from raw data to intelligent outcomes. Your data is the product, your notebook is the vehicle, and AI is the driver. I'm really excited to see what you all built with the new notebooks. Thank you for having me. It's awesome, Diane. Thank you. Diane. Thanks, Diane. Great demo. It's a lot of fun to use. You can use it now. You can get started with this right after this presentation and, um, and try, try it yourself. As Diane shared, when we built SageMaker Unified Studio, our goal here was to make it easy to work with your data, but we also have MCP servers. We have MCP servers that are both local and remote. So you can use any of these new capabilities for whatever your favorite AI or traditional coding tool is. No matter what your point of access, our whole goal is to build AI into analytics, so it becomes just a natural way for how you work. Now, the second emerging trend in analytics that I want to talk about is open architectures. In today's modern analytics world, customers, they know that not one size fits all in an analytic solution. You can't just jam every capability behind a single API. You want the flexibility to pick a solution that meets a specific Need. And at today's scale, you also know that you don't want to pay for what you don't need. You want the choice to be there now and in the future, whether it's a line of business making a decision on what analytic solution to use, or it's for you at whatever stage you are in your data journey. Now,
the thing that makes these open analytics architectures possible is open data formats. It's like it's parquet and it's iceberg. Open data formats are the key because it means that you can have an open data format in your data. And then you can just swap out these different analytic solutions that you use with your shared data set. And if you use these open data formats, you can do those changes for analytics and you don't have to get roped into an expensive data migration. Now, in S3, we started to see the shift to these open data formats about 5 years ago. And today, S3 stores exabytes of Apache Parquet data, and we average 25 million requests per second, just to parquet. Now,
these open data formats have really taken off because customers don't want to deploy monolithic analytics architectures. They want to let different organizations pick the right analytics solution for them. Whether that's Redshift, that's bringing the data warehouse smarts to a data lake, ad hoc querying using the new Athena for Spark, using our managed Kafka service MSK or managed Flink MSF or any other analytic solution. And open architecture, and open analytics architecture helps you evolve your data strategy without having to do that data migration when you make a decision to go with a different analytics provider. And nobody, nobody does composable building blocks better than AWS. Now, the composability also means that it's cost effective. It's economical because you're not paying for a bundle of things that you're just not going to need. And because we have SageMaker Unified Studio, which you just saw in action, you don't sacrifice on the simplicity of experience. You now have an easy to use analytics development environment with SageMaker Unified Studio, and I'm sure many of you are going to want to live in that notebook. Now,
with AWS we have this choice. We have the choice of services, or you can use the integrated IDE of SageMaker Unified Studio, but we're also creating something new. We're creating something that we call analytics building blocks. OK. An analytics building block is a capability that can be used across all of our analytics services, and it's only available in AWS analytics. If you think about an analytics building block, think about it as disaggregating a core primitive, a core analytics primitive from a specific service, and then being able to use them across all of our different services. Notebook is a great example with its polygot cells. Quick dashboards is another example of this. And this week, we introduced a new cross-service building block, a fully managed materialized view for Iceberg. It's an incredibly powerful concept for builders of any type of application. You can create these materialized views using the Apache Spark version 3.5.6 engine that we launched this week, and these views are then automatically stored in an S3 table with full iceberg support, and it automatically appears as part of your AWS Glue data catalog. It shows up as just another table. Now,
the materialized view automatically detects changes and updates your views as the new data arrives. It's fully managed. We have a whole infrastructure, a whole microservice behind it. So there's no manual orchestration needed on your behalf. It runs on our fully managed infrastructure, so you don't have to worry about provisioning compute. Now,
these materialized views are a building block in itself. And so when you have them in your sage in your, um, uh, glue catalog as a table, it can then be queried by Athena or SageMaker unified tools like the notebook you just saw, editor for VS Code, Jupiter Lab IDE or Redshift can query the iceberg table. Directly or any other third-party analytics solution as long as it's ICEberg compliant. In addition, our Spark engines will automatically rewrite queries to use these views, giving you performance improvements and in our testing up to 8 times in performance improvement without any code changes on your behalf. Another example of a cross-surface building block is an S3 table. Like our materialized view, S3 tables can be queried by any iceberg compliant analytic solution, and that could be an AWS solution like Athena, EMR,
Redshift, or it can be a third-party solution like Snowflake. Now, we iterate pretty fast across these cross, uh,
on these cross-surface building blocks. So as an example, since the launch of S3 tables at reinvent a year ago, we've added over 15 new features, including this week, an intelligent sharing storage support for tables and cross-region cross-count replication. Customers all over the world in every industry are moving to these open analytics architectures because they want to work with data at scale, and many of our customers who are living on the frontier of data are leading the way. To tell you more about how they've evolved their architectures for humans and AI, I'd like to invite Tristan Baker, distinguished engineer for Intuit, to the stage. Tristan? This is Dent. Thank you, thank you,
thank you. Hello, I'm so excited to be here, uh, to talk a little bit about how Intuit uses data and AI, uh,
to power prosperity for its customers. If you're not familiar, uh, Intuit is built on what we call an AI-driven expert platform. Um, and that platform drives our four core products which are TurboTax, Credit Karma, QuickBooks, and MailChimp. Collectively those products serve our consumers and small and mid-market businesses, and I'm gonna talk a little bit about how we pull that off, uh,
the scale that it operates at, and the data strategy that sits behind all of it. Um, so what exists in the Intuit platform I will get to in a minute, but I want to give you a sense of the scale. So the scale is 86 million consumers, uh, collectively receiving $105 billion in refunds from our tax product every year, uh,
managing $11.4 trillion worth of debt with products like Credit Karma. Uh,
we also have 10 million small admin market businesses that manage $2 trillion worth of invoices, um, and manage the payroll for 18 million US workers. So it's a lot of stuff and a lot of stuff means a lot of data, um. And a lot of data means you need really good data and data services. You need artificial and human intelligence built on top. And what that delivers for Intuit is something that we call a system of intelligence. And you contrast that with maybe what Intuit products were a few years ago, which were maybe systems of record. Our customers relied on us to manage. Data to manage their compliance workflows, to file their taxes, to close their books, they didn't necessarily rely on us for intelligence, but if we have this trove of data and we add intelligence on top of that, then what you end up having is a system of intelligence that can actually power the prosperity that we envision powering for our customers on our platform. So what does all those, uh,
what do all those customers mean in terms of what's happening under the covers and the scale that's happening, uh,
in the data layers? Well,
we have 70,000, uh,
tax and financial attributes per customer. Uh,
we're managing 188 million, uh,
natural language conversations through some of our more recent agentic experiences, and that makes up just a fraction of the 180 petabytes of data that we manage in our data lake. Uh, so the, uh, services, uh,
the AWS services that power all this are highlighted here at the bottom. Um, I'm gonna go into more detail and, and more precision about how exactly those things are arranged in a moment, so stick with me for a few more slides. Um, the, uh,
first I want to motivate, uh, our strategy with the problem that we were facing maybe 2.5 years ago. Um, how do you manage, how do you better manage a data lake of 320,000 tables? We established a key metric that we like to call time to discover and access data, and when we measured that metric 2.5 years ago, it was something like 20 days, which is like eons in the speed and scale of the businesses that we are running. Uh, so how do you bring that metric down? Well,
first, let's understand it. Why,
why, why 20 days is what it is. If you, if you look at this screen here, this is a snapshot of a typical data discovery screen that is part of our internal data discovery experience, and what you see is that a lot of important information is missing. We don't really know what the table contains. We don't know who owns it. We don't know where it came from or where it's going, and it's not that this information is not. Knowable, it's just buried in people's heads or in, uh,
non-standard wiki pages and tribal knowledge that takes tons of time to unearth, and that time is measured in days because it takes about that long for the average team to finally get access to the data that they need and to be productive. So this is what motivates our data strategy. It's a bellwether metric for whether or not things are generally going right or wrong. And so we've tracked this over time, and I'm gonna tell a story about how that has improved. Um, so we have a 4-part data strategy, as I mentioned, I'm gonna go through each part here one at a time. The first part of that 4-part strategy is what we call standardizing data semantics. So yes, we have 4 or 5 different product lines, uh,
but more and more of these product lines need to be able to communicate with each other. And whenever something needs to communicate with something else, you have to have a common language that everything, uh, everyone understands. Um,
and it wasn't until we put a concerted effort into it that we could develop something like a common semantic layer. Um,
and I'll give you a preview of kind of what that looks like at a very high level. So, uh,
the semantic layers divided into what we call domains and subdomains. One very popular and important domain, uh,
is customer. So all of our customers have identity profiles and account profiles that help us identify who they are and where they're from. Um, some of our customers are consumers, so consumers then have credit, income,
and tax profiles, and these profiles are the lingua franca of, uh,
how our TurboTax and Credit Karma products communicate with each other. Um, we also have another domain of information that we call business, um,
and our businesses manage profiles that describe commerce, their accounting, their money, the customer relationship management behaviors, as well as the lending. Uh, Collectively, this produces what we call the semantic layer, and it's what allows, it creates the data glue that allows our products to communicate with each other. And it's not just good enough to have this information available, you actually need it available in the right places. And when I say places, I mean data stores optimized for particular query patterns because the transactional query pattern is very, very different than an analytical query pattern. The query might be about the same semantic information, but the questions being asked are vastly different. So this brings us to the second part of the 4-part strategy, which is to standardize common data infrastructure and pathways, and it starts with an observation that, um. Uh,
we want to make the typical thing easy to build and ask people to build typical things. And if we can pull that off, then what we can do is create one-click tooling and one-click architectures that allow our development teams to very quickly and easily get not just an application layer that includes your typical app, service, and database layer, but to get it in a way that allows it to produce data that is available to the next typical thing in the picture. Uh, so we have the typical architecture of, uh,
an Intuit product. We also have, uh, acquisition pipelines because a lot of our customer data is managed by financial institutions that live outside of the Intuit rails, but we still need that information in our environment in order to do useful things. So we make it extremely easy for any development team to both source data from an external, uh, uh,
from an external company or to, uh, produce their own application or service, and collectively these two things produce what we call either product events or third party data. So where do those things go? They go to the next typical thing, which is the typical architecture of a data processing system. Those generally come in two different flavors, be it either streaming for real-time or near real-time processing, or batch pipelines for longer latency, higher workload pipelines. That kind of processing is ultimately what takes the raw data in to produce the insights that we need out. To begin to deliver on our promise of being a system of intelligence. So, uh, one direction of the arrow out of this box is what we call data and insights about customers, and this arrow feeds another typical architecture of what we call, uh,
our business intelligence systems. So this data flows into another section of our data lake where reports and dashboards are easily built on top, where development and data exploration tools, the screenshot that I showed earlier is an example of such a tool, is also available right on top. And this is what feeds our into business leaders, for instance, with the information they need to make strategic decisions about changing product strategy, changing product behavior, going after new portions of a market. Uh, the other thing that comes out of this typical architecture, uh,
of a data processing system is what we call data and insights for customers. So if it's a data and insight for a customer, then it's got to have a path that gets back, uh,
in front of the customer's eyeballs, and that goes up and back to the left. Uh, and so the data insights for customers go up to what we call our customer data cloud, which is where some of those 70,000 attributes I mentioned earlier are managed. On top of that, we sit, uh,
into its Gen OS, uh,
which is a little bit beyond the scope of the conversation today, but suffice to say it's the core engine that drives a lot of our genic and intelligent experiences. Um, and when, uh,
that data flows up through customer data cloud into Gen OS, it essentially becomes the data and advice, uh,
that we promise to deliver to our customers that again helps them power their, uh, financial prosperity. Uh,
so that's the second part of the strategy. Um,
I'll highlight a couple of the important bubbles here, which are the databases, the customer data cloud, the event bus, and the data lake that essentially make up the 3 or 4 different typical data stores where the data and information needs to be available in different ways for different kinds of query patterns. And Essentially this makes intuit zero ETL architecture, and it's not zero ETL in that data is not moving. Data is definitely moving, but from the perspective of the teams that use our,
uh, the development teams that use our typical architectures, it appears to work like magic. If you're writing an application or service, the data is immediately available to your downstream analytics team. Um, if your analytics team is producing new insights that we need to feed back into the product, then that's just magically and immediately available back into those products. Um, I also said that I would highlight more precisely where Intuit services are being leveraged, and this is more or less how they're laid out. Uh, many of Intuit's, uh,
managed database systems like Aurora and Dynamo, uh, are driving our, uh,
transactional workloads for our products. The streaming and batch processing is, uh,
heavily reliant on EMR, uh,
MSK, as well as, uh, Lake Formation Glue data catalog in S3, uh,
using many of those OpenTable formats that Mylan was mentioning previously. And then our BI tools and development tools were built on the backs of Sagemaker, Athena, and Quicksight. Uh,
moving on to the third leg of our strategy is the metadata around the data itself. Often that metadata is what is needed in order to describe this rich context. And if all you had, all you knew about a data asset were the 3 boxes drawn on the screen, you would be left scratching your head with a lot of questions. What does it mean? How did it get there? Who owns it? Those kinds of things are only answerable if you decorate this kind of information with more metadata. And one of the most important and critical pieces of metadata that we had to collect very early on because we didn't have it, or at least not in the way that we needed, was ownership information. Who are the teams? What are the business purposes under which they are operating, what projects? What are the roles of the different team, team members, be they data stewards or data developers? And it wasn't until we had that information that we could then say,
Guess what, thank you for telling us you're an owner of something. Now I have 15 more things that I need you to tell me. Some of those things become, how are you organizing your data assets into data products? What is the semantic meaning of these data products, and how is it related to other data products that your other team members might be creating. So you start stitching all of this information together and you begin to get the picture of the metadata and the context that Intuit has available. Um, so now that we have, especially the ownership information, we can get to the last part of our strategy. Which is to start coming up with rules and requirements and a way of judging people and the work that they are doing, which,
uh, maybe the people being judged don't like to hear, but I certainly like to do it, uh, because there's nothing that makes me happier than clear rules and time-bound goals, uh, that our organization, uh,
can chase after. So we've developed a clear set of requirements broken up across 5 important dimensions, one being stewardship, documentation, data model, data observability, and operational stability, and we have clear requirements that allow us to measure the degree to which everybody's data products are meeting these criteria. If you're doing a very good job, then you end up somewhere on the right-hand side of the spectrum. And if you're doing a less than good job, then you're somewhere on the left-hand side of the spectrum. And then I come knocking and asking, what are you going to do for me by the end of this fiscal year. Um, so now that we have all of this set up, uh, what does that get us? Well, that gets us, uh, a screenshot that looks a little bit more like this one than it did, uh,
in the previous, uh,
slide. So instead of seeing no context and no descriptions, we see rich context and rich descriptions. Um, even though it's blurred out here, there are names of people that own these things, uh,
and we can follow up with them if we have more questions. Uh,
we can see data quality measures measured over time, um,
and advertise, giving a sense of the reliability of the data. Um,
and you also see self-serve access at the top, so we can automate the permissions and the management that allows us to grant teams the ability to touch this data, given that they have the right, uh,
justification to do so. And that means that, um, 2.5 years ago, our data was, well,
less than 1% clean, we can now claim that it is at least 80% clean with goals of getting even higher by the end of this fiscal year. And that means that we took that 20 hour metric over the course of 2 years ago, uh,
we took that from 20 years to 20 days to 14.2 hours, and then in the most recent year, we dropped that from 14.2 hours to 9.4, um,
and my team has goals to get that even lower this year. Uh, so that's kind of what this all looks like in slides and screenshots, but I wanted to give you a sense of, um,
how this comes to life in a demo. Uh,
so how does Intuit use this to build a better, better together product? Uh, there's recently been an effort to bring together more of the customer bases that use both TurboTax and Credit Karma, so that there's more of a seamless experience between these two offerings. The, uh, I'll pull back up some of the, the diagrams we saw earlier, and what this looks like in 3 reasonably simple steps is that first, as most people do, you analyze a business question. What table and query will show the number of Credit Karma members that are also TurboTax users? Assuming you see a total addressable market and a market opportunity, you then ask your your team to go build something. Uh, what API that that team will probably have a question like what API will return a Credit Karma member's unified profile, including their tax filing history and refund status. Um, and if you can answer those two questions reasonably simply and quickly, then you can reasonably quickly justify the, uh,
effort to build something and then reasonably quickly build it, and that allows you to just pretty much ship, ship something. So, uh, in this demo what you'll see is some of the same, um, application screens that we saw earlier, uh,
but now they're kind of in action. So somebody like a business analyst might ask how many Credit Karma members are also TurboTax users. All that rich metadata allows us to run a semantic search that. Uh, levels that table right to the top of the search result. You'll see the star ratings as I described them earlier for this data set laid out here. You get a sense of where the data is coming from and where it's going because lineage metadata is rich and available. The data quality gives you a sense of how reliable this data has been over time, and all of this rich metadata is powering what we call intuit assist, which is intuit's generative AI analytics assistant. Um,
and the same question can be asked, asked of this assistant, and we'll simply just spit out the answer for you, which you can then take into your notebook experience. Um, and I'm gonna cut away from the screen before you see the answer, because that's proprietary intuit information. But, uh, suffice it to say there was an opportunity there. So now you can tell your development team, all right,
let's go build something. Well, we take them to the development portal. Uh,
the development portal has the same, uh, has a different API surface area to the data, but the same semantic meaning. So if you ask a slightly different but semantically similar question of our development portal, like what is a Credit Karma member's unified profile, including their tax filing history and refund status, you get a different answer, an answer delivered in the form of an API rather than a SQL query. But the developer is able to see of the 70,000 attributes, which are the 5 or 6, or maybe there's a dozen here, which are the dozen here that are the most relevant to my question, and what is the graphQL query that I can then build my application around. And that developer is then able to build that application again using some of those application rails, uh,
typical architectures that we discussed previously, and now you see what an experience might look like. A customer of TurboTax finishes their tax return. They're then, uh,
taken immediately into Credit Karma. Uh,
we've been able to negotiate many of the compliance and security rules that come into play when you're trying to transition information between two different products like this, and you're able to see things like federal refund and tax refund, uh, California state refund status in the, in,
in a slightly different application, but have confidence that it means semantically the same thing that you thought it did when you developed the idea. Um, so I'm really proud and excited of the work that we've done, and I see some of my team here in the audience, so I'm, yeah,
there you go, put your hands up. Uh, so I'm representing, I'm representing the work of hundreds of people over many years, um,
so I'm really proud of what we've done. If you've learned anything today, I hope you can take some of these ideas back to your own teams and develop them for yourselves because, uh. Uh,
if you've learned nothing else today, metadata is super important when building both, uh, uh,
agentic experiences for your developers and just building more productive development teams. So with that, I will finish up and hand it back to Milan. Thank you very much. It's pretty amazing, huh? What Tristan and the team have done. Let's hear it F3 and Twit. Woohoo. It's just exciting to see what Intuit and so many customers have been doing with their data strategy, and this level of semantic understanding is going to be increasingly important as people's data lakes get bigger and bigger, and AWS is with you every step of the way. Now,
as part of our commitment to these open architectures, we have been investing heavily in Iceberg support across all of our analytic services from the data layer in Amazon S3 tables to our analytic services like Athena, EMR and Amazon Redshift. And we focus a lot on performance across our services. I talked a little bit about the performance that we did with the, um, with the spark, the optimized Spark 3.5.6 engine that's now powering Glue and Athena and, uh, and EMR, but we've been investing very heavily in Redshift as well. In fact, in this year alone, Redshift has launched 37 new. Features and capabilities, and a couple of them are related back to Iceberg performance. So we recently improved the redshift read performance for queries on Iceberg by more than 2 times, and we did that by supporting distributed bloom filters, metadata caching, and optimized query planning. And right here at Reinvent, we are announcing. The capability of Iceberg table append write support in Redshift. It's a big shift for Redshift to be able to write as well as read into these Iceberg tables, and I know many of our Redshift customers are very excited about that. Now,
this move to open formats is something that one of our AWS data partners, Superbase, knows a lot about. Let's hear from our joint customer Snapchat. Hi, uh,
my name is Derek. I'm a product manager working on spectacles. Lens developers were constrained to client-site development only, and SnapCloud, powered by Super Base, now allows them to build more sophisticated and complex lenses. Over the past 5 years, lenses have gone through an incredible transformation from their origin, which was lightweight face filters, to what they are on spectacles today, something a lot closer to a. application we were quite attracted to the developer experience of Supabase. It was just amazing to see how fast you could get started. We have come to appreciate what I would call the unique attributes of the Postress community. So when you think about the geo extensions that are possible within Postress, how do you store data that is tied to physical locations and stuff like that? I have to say the more we learned, the, um,
the more excited we got about this direction. The hardest part about building. A platform that provides backend as a service is the the the isolation, being able to support multi-tenants, for example, the scaling and backing up of data and all these things that you need to build a resilient infrastructure that can grow with your product, all that is basically handled by the super-based platform. One of the features that we released I found to be one of the most exciting ones was uh support for WebSockets, so that actually allowed developers to connect. The glasses to other remote destinations in real time, which unlocks a lot of really cool use cases. So Base built a poster extension that makes it possible to easily query data from Iceberg files. Since Iceberg is an open data format, we can easily take those iceberg files and use them with our existing tooling, and all of that runs on AWS. With a medium like the glasses, you think about the type of product it is. It is this is an incredibly. Product that has access to camera and microphones able to sense the world around you. The camera is the most sort of like fundamental building block of an AI experience if you think about it. Having that infrastructure isolated in a in an AWS instance that provides you all the flexibility was very, very critical. We are building the future of connected AR together using AWS scale to bay speed and creativity. It's pretty awesome, huh? Now to tell you more about Superbase and what Supabase is doing with AWS analytics and Iceberg and open formats, let's welcome Paul Copplestone, the CEO of Superbase and the co-founder to the stage. Coopel. Awesome. Thank you. Thanks,
Malan. One of the most critical decisions that any business can make when they're getting started is choosing the right database. For companies that choose the wrong database, one that can't scale, they face a pretty daunting exercise. Migrating to a new database, and this always happens at the worst possible time when their business is scaling fast. So choosing a database isn't about what you need today. It's a very forward-thinking decision. You have to think about what you might need 23, or even 5 years down the line. A lot of companies recognize how high stakes this decision is, and they turn to Superbase to help them get it right. I'm Paul, the CEO of Superbase. We are the world's fastest growing post-growth company. And we've launched over 10 million databases for all different types of companies, including AI platforms like Lovable, v0 by Versal, and Sigma Make. They choose us because we're the most developer friendly way to get a production grade database, and we've built everything on AWS. After working with over 5 million developers, I've come to see that there are several common data challenges. The most frequent is choosing which technology to pair with their Postgress database. Almost every developer today chooses Postgress when they're getting started. It's the world's most popular database because it's both versatile and robust, but the versatility also has a downside. Developers often fall into something that I call the convenience trap. They start storing analytical data inside their posts growth database because it's the one that they already have. And this actually works pretty well for a while. But as we all know, Analytical data grows very fast and Postgress isn't designed for analytical data. I'm sure many of you have faced this situation, so you know how to solve it. We introduced an analytics engine. And this is absolutely the right thing to do, but it also introduces another set of challenges. We now have two sets of everything, 2 schemas, Uh, 2 sets of data, 2 libraries, and often 2 sets of expertise to manage the different databases. Even harder, the data is now fragmented between two different databases, so getting critical business insights can become a bit of a nuisance. So how do we all solve this in the industry? A third database, of course, and this time in the form of a data warehouse. Then as the data starts to pile up, we have our final challenge which is costs. So to solve costs, we use S3, which is Good for long term and uh affordable data storage. The craziest thing about this architecture is that every one of these services has their own proprietary data format. So the data ecosystem has redefined redundancy. At Superbase, we don't like complexity, so we set out to reimagine the best data architecture using estuary tables. Estuary Tables stores data in a format called Apache Iceberg. Which is an industry-standard OpenTable format specifically for analytics and data warehouses. S3 tables will become our unified storage layer for data. To build applications,
we still need a database that can provide low latency queries and no surprises, that's Postgras, which delivers millisecond-level response times and is scalable, powering some of the world's largest applications. Superbass keeps these perfectly in sync. When you create a table in Postgress, it's automatically created in an iceberg. When you insert data into Postgress, it appears an iceberg. And when you no longer need the data, you simply delete it out of Postgress, and it's already stored in S3, which is incredibly low cost and resilient. Estuary is not only low cost, it's extremely accessible, and Apache iceberg increases this accessibility. Every major data warehouse already works with Iceberg and all major analytical engines also work with Iceberg. It's become the universal language for analytics and data warehouses. For any CIOs in the audience, this is important for two key reasons. One, it gives freedom of choice to your developers and data engineers. They can choose their favorite data analytics engine to work with the with their data. And 2, it reduces costs because there's no more waste. Analytical data is stored once inside S3 tables, and instead of moving data to every warehouse, you can simply connect the engine and query directly from S3. This is an emerging pattern that we're seeing across the industry and we call it the open warehouse architecture. We believe it's the most efficient and cost-effective way to manage your data. We're launching this today on the Super Base platform, and we're not stopping there. We think data warehousing needs to go beyond traditional analytics. Today at Reinvent, AWS are announcing S3 vectors, which adds support for embeddings. We can use embeddings to build AI native features for our applications, but they also allow us to extract deeper insights from unstructured data. From today, you can use vector uh vector buckets on the superbase platform, and we plan to make them an integral piece of the open warehouse architecture. The old architecture is expensive. It actually goes beyond that, it's extremely siloed, every engine is isolated. Apache iceberg is moving the data world from. Siloed to stinct. With the open warehouse architecture, everything is connected and there's no more waste. I believe that the future of data is open. Postgress is open source and it's the world's most popular database. Apache Iceberg is an open format and has become the universal language for analytics. S3 has become the fundamental substrate for data and AI. Within 6 years, Superbase has become the most popular platform for postgrads, and today we're starting our next journey with Iceberg, again on AWS bringing the same developer friendliness to this new open format. Thank you. Hey, thank you. Uh, Superbase is pretty amazing. Um, you know,
I've enjoyed the many years that I've been working with them, and they have always been on the frontier of data, and it's exciting to see what they're doing with both iceberg and vectors, and that is in fact where the world is going. Vectors Gives semantic meaning to your data. And it means that you can easily search and use your data no matter how much you have. Now,
what I am excited about for vectors is that vectors are the tool that let you understand your data without having to understand what is in your data. And it is a tool that we can use now because of the power of these AI embedding models. And it is, in fact, why vector is emerging as a building block for both data, data semantic understanding and AI in the form of extended agent memory. Vectors are emerging across both of those domains, and it's equally useful in each. Now,
a year or so ago in S3, we saw that vectors were becoming one of our fastest growing storage types, and so we launched our S3 vector preview this past summer. And in only a few months of preview availability, customers have run over 1 billion queries. Now,
I have worked on S3 for a minute, and I have to say this type of super rapid adoption of a new feature in preview is incredibly fast, even for F3. And so here's what we're doing with vectors. Just like we introduced S3. At a cost in economics that made it possible to add any type of data into a data lake, we're doing the same thing with vectors. We're making vectors cost effective. And when we do that, we are going to find that customers of data and AI, the application builders, they're going to use vectors in every aspect of their application. We're going to do with vectors what we did with data and data lakes. And customers can now treat this new data set of AI and, and data understanding, semantic understanding, like any other data in their S3 data lake. They don't have to worry about security and durability and availability, and you can let your vector data set grow at S3 scale because of the cost and economics of vectors in S3. Now, vector APIs are just part of S3 now, and what that means is that it's simple. To use vectors. And like objects and tables in S3, S3 vectors are usable at even the smallest scale indexes all the way up to billions of vectors, meaning that you can focus on building your applications and not thinking about how to size and configure a vector database. And this morning, Matt Garman announced the GA of S3 vectors, and with it some pretty impressive changes since our July preview. We've increased the max capacity. A vector stored per index to 2 billion. That is 40 times the preview capability. That's a lot. You get 10,000 indexes per bucket, which means that 11 vector bucket can store up to 20 trillion vectors. Now,
as you all know, as S3 users, S3 is very good at massive throughput, and that's what you get with vectors too. 1000 vectors per second when you stream a single vector, and all of this at 100 milliseconds or less latency for warm queries. It is a game changer and it is going to fundamentally change how people think about semantic understanding of their data and really other things like agent memory. We're going to see a ton of variety of how customers take advantage of our new vector storage, and we're already seeing that diversity in use cases. If you think about the use cases that we've seen, it's been a combination of two things. It's been, um, being able to get a semantic understanding of metadata for a semantic understanding of media. But it's also being able to extend agent memory. OK? So if you think about extending agent memory, the way that you extend agent memory is that you add more context and vectors, right? And if you add more context and vectors, boy, the,
the, the smarter those chatbots get and the more human they get, because you're able to vectorize context about A human that is interacting with a chatbot. You're able to vectorize context about the agent inform, uh,
sorry, the account information of a human that the chatbot is interacting with, and all of that is going into vector storage and being used to add and create and personalize the responses to it. You can see where that can really take off when you can have virtually unlimited vector storage. One of the fastest growing new types is also hybrid search, and vector storage in S3 has integration with both Bedrock knowledge bases and OpenSearch. Hybrid search has really taken off. With OpenSearch. Now this year we focused on open search on a lot of AI capabilities, such as agentic search that that automatically converts natural language to precise queries, and we've also added automatic semantic enrichment and as of this year, GPU acceleration and auto optimization for uh vector indexing. With GPU acceleration, you can now build these billion scale vector databases, but you can do it much faster. You can do it under 1 hour, and index vectors up to 10 times faster at 25% of the cost. It's going to be super helpful for customers that are building multi-billion scale vector indices. And if you think about it, GPU acceleration is just going to be a game changer for dynamic AI applications that have heavy rights. And because open search serverless gives you a GP GPU acceleration that's completely serverless, you don't have to manage any GPU instances and you only pay for what you use. And so when the right volume drops, the GPUs will automatically scale down and they'll return to the pool. That means you're only paying for acceleration time, which is monitored through cloud watch. It's fully managed, it's fully automated, and it's paid as you use. Now, we also worked on how we could help simplify how you find the optimal configuration for your specific use case so you can balance between cost and performance. The difference between an optimal configuration for a vector database and one that just works can be 10% in search quality, it can be hundreds of milliseconds in latency, or it can actually be 3 times the cost. So getting the right configuration, it just matters a lot. And so we build auto optimization. You simply specify what you think your acceptable search latency and quality requirements are, and there's no expertise in tuning algorithms or quantization that's required. It's also integrated with vector ingestion pipelines, so you can build optimized indexes directly from Amazon S3 data sources. And again, you're paying a flat rate per job, and it's simple, it's fast, and it's fully automated. As you can see, this world of vector is evolving rapidly as a building block, and with AWS vectors and open search combined, we're here to help you steer into this rapidly emerging trend for semantic understanding. I believe that in the upcoming 12 to 18 months, anybody who is going to work with data is going to want to build a semantic layer on top. Of their data set so they can understand what's in their data. And I think this combination of vectors and AI embedding models, and the vector databases like OpenSearch to make it very, very fast for your high, uh,
your high scale, high fast, uh,
workloads are going to be critical for you to steer into this rapidly emerging trend of semantic understanding. It's a super interesting time, I feel to be here working at this intersection of data and AI and here at AWS we are deeply committed to helping you build not just what you need today, but a data foundation that you can evolve easily into all of these emerging trends. Speaking on behalf of all of our teams in data and analytics, I want to say a big thank you to everyone in this room who may who uses our services to make data the backbone of your business. We are inspired by what you do and your commitment to your customers, and we're here to build what you need now and in the future. Thank you, everybody, and have a great reinvent.