---
video_id: TsKKTVepP7k
video_url: https://www.youtube.com/watch?v=TsKKTVepP7k
is_generated: False
is_translatable: True
---

Hello everyone. You can hear me, right? Perfect. I'm super happy with, with, uh, I'm super happy to be here with you. My name is Elizabeth Fuentes. I am developer advocate focused on generative BI and here together with Rosana Suarez, a woman that I really admire because she's amazing, really she's an amazing woman who have a patience to deliver and create free content about the box so everyone who want to grow professionally. It can do it can do it because Rosana, she create the content because she want to break the barriers that we have in the Spanish language. So thank you Rosana for inviting me to deliver this talk with you. I'm super happy to be here. So in this talk we will talk about about the new. This new type of observability that we can do, we can create using gener TBI. Because in today it's not only about to create speed to create to create code with the speed, we need to create systems, systems capable to predict and prevent failures before they impact our users. Now let's start. Our agenda for today is first we will talk about the limits of traditional observability. Second, we will talk about a powered observability in say is intent because come on, who doesn't love to automating away the pain because we are going to focus in and you have actions and first, uh, Rosana, we will show you an agentic application to. And finally we will talk about the impact, the real impact, the lesson learners, and the future. So like a start, you will start with this phrase here. Everything fails all the time for our CTO Verner Boges, and this phrase is true, is super true because everything fails. And it's not if something will fail, it's when and how fast we detect and response because the key here is anticipation. We need the anticipation because we don't want the problem. Now the limits of traditional observability, yeah, we have some limits there, but it's OK. Some solution that used for uh resolve these limits. The limits are we have a proactive and no proactive. Uh, system because we need the attention first. What happened when you have this observability dashboard and it uh alerts you two minutes after our users are getting complaint in social media. Nothing good, right? We have some noise overlap. Sometimes A lot, a lot of alerts in 5 minutes and come on, 90, 90% of that alert probably are noise. We have a lot of noise and we need to figure out what alert is the first that we need to resolve. We have silent signals. We have a lot, a lot of dashwork with different tools, but zero correlation between the dashboard. A slow decision making when we have a problem we need to to keep the solution. We do debating we have the war, the war rooms, and sometimes we spend a lot of time in that and meanwhile our customers are there with problems. Yeah. Traditional observability is pro proactive firefighting, noise proactive. Now let's talk about, about the impact, the real impact of this problem, and it's not just about money because we have another problems. Yeah, the first one is about money because sometimes when we have a an enterprise, the money is the first and when we talk about money, there is so many studies that say that. The downtown cost is between about $50,000 or $500 per hour so you have a revenue impact of that cost per hour. It's not only money too because when you have this downtown you lost the trust of your customer too and then we have. Uh, alert fatigue, about 70% of all the mas engineer, they have alert fatigue, and what happened when they have this problem? We have some engineers with burning out and of course we spent a lot of time doing troubleshooting. We do a lot of troubleshooting and this time it's about 40% so. Sorry, er, when we spend all this time doing troubleshooting, we don't spend this time building innovation, building new stuff for all applications, so we lost our time doing troubleshooting. And this is the gap that this application help us to resolve. We have a lot of data, yeah, we have locks, metric, traces everywhere, data for everywhere. We are down now drawing now in telemetry data. Then we have noise and silos. I already told you we have a lot of dashboard everywhere, of tools, and no correlation between them. We have a slow human decision. It's slower room 40 minute debated what we do. We do roll back what we have to do, meanwhile our customers are there. And finally we have the business impact. Then customer check it down and trust us. And the question is, so how do we move from reactive cows to productive intelligence now Rosanna is going to tell us a sad history. But we are happy ending. rocks rocks is powered we can fight it. The history, we have been there, right? Rocks in Friday nights 11 p.m. Someone get the magic words. It's quite a small change. 10 minutes later, chaos dash around fire are everywhere and someone yet who touched production sound familiar. Uh reading. It's funny because it's true, right? We need intelligence, uh, before they deploy, not after traditional weights deployed break alert fear too late. AI we prevents all kinds. This is rag the intelligent. This is AI powered observability. So now next time into the solution. Let's talk about AI powered auelability in CICD FSC liquid using giga action. There are 4 key reasons why this matters. First, early detections. AI cash read before productions. Second, safety gates. Risky deployments. I love block automatically. Third's, faster recoveries, less on time, more cheapened, and finally better. Developer experience FI what happens right in the pilot, prevent, protect, and accelerate. So what does this mean in real numbers? Is it from 200 alerts per deploy down to just 5 alerts? Doesn't really matter. NTTTR from 2 hours. Why? AI 150 minutes, 8 times faster. Um, that saves a lot of money every single time. AI prevents those incidents, less noise, more insight, zero surprise. These are weird. Resort from productions is in AI driven ccelability. So how does this work inside GICA action? Simple. The moment you push code or create pull request or any deployed step, AI start analyzing immediate declit. It's collect metrics, logs, a pot, even from Cuban edit. It check everything using some Amazonvecro beautiful. Cloth or opening eyed it decides amazing block alert work and send telegrams alert instantly that's it. Three critical moments. The first moment is the pull request. Here AI give advice and show risk before we merge. It's only advice, not blocking. Just a small automation improve the pure requests better. This is the 2nd moment and it's very important pre-deployed here. The AI can uproot or block deployment if the system look, but the AI stop the deployed this protect production. The 3rd moment is post deployed. After we deployed the AI check everything again, it sent a report, an alert is something is wrong. These 3 moments make deployment safer, faster, and easier for the teams. When the mining start, the AI check everything the code and the environment in the poor request. A I write the comment and get cut if everything's OK. It say mm AI pass deployment approved. If something is wrong, the AI stop the deploy and send a notification to Telegram or any tool but about select so developer can see the results in Githa and teams get the alert in real time. And the deployee is approved. The AI run the deployment bill and test security deploy and optional. Post-deployment validation to check if the app is healthy. This makes the process may matter and safer its auxerability counts to you. Let me show you how this works in real life. Developer push good find an start. Beautiful. I said the system take maybe 10 or 20 seconds. Is everything is healthy green light, beautiful date. Problems red like block things and telegram message immediately. That's it. Simple, automatic and safe. No manual approve. Right now, Elizabeth showed our solution. This application work initially with 3 model providers. We have the Amazon bedroom model provider. We have Cloud, and we have OpenEye, but because this is an open source solution, you can change. Uh, you can do a little change in the code and you can put your favorite water provider there. It's a, it's a simple solution that you can change because this solution is using a trans agent and trans agents is an open source framework capable to, uh, manage the capable to resolve the question and all that is an agentic loop. And using the gentle trans agents uh delivered the answers. So the trans agent is the brain for the solution and here we have, sorry, here we have the open eye and Amazon Medro and cloud other providers. And because again this is an open source solution you can switch transagent with um another framework that you want to use and. Then we have the QR netted. Inside the QR netted we have the Prometeos and grafana and this, um, application. They, they are, they capture all the metrics they can see they, they capture the CPU, the CPU, the. Memory they start, they see everything so they can share this information with our trans agent inside there. And then we have two important things in the solution. We have two gates. The first gate is, I can't see anything there with the, the first gate is the pre-deployment health check because every time we put a code there, the agents see everything, see everything before this code go to production. And if the agent doesn't approve the health check, the agent is going to block the deployment, and the second gate is the post-deployment validation. In this gate, uh, the agent validates everything. Everything went well. And finally, Uh, the agent, that this application send a telegram, uh, notification with all the progress the AN, the agent do all, all the information, all the, all the thing the agent capture for this deployment. And everything is triggered. By jihad, so every time you do a push this, uh, the this, uh, flow is going to gate and let's talk about the flow that this uh application is using. We have a, uh, a decision flow just here first we have all the data when we pull metrics from the coordinator. They put the status CPU, everything is there. We send this data to the AM agents and this agent generate a hell score from 0 to 100 and this is no, this is, this is no magic, you know. We have a prone that that have a specialization like a demos to understand everything what has happened there and then this agent. Makes a recommendation if it is approved, come on deploy if it's. Mahominos in Espanol is so so we deploy with warrants, but if it's not OK if the agent stop everything, so if the deployment is not good, if the documentation is split, don't deploy and then it send the notification by telegram. And this is the problem. This prompt is, uh, is they use some part pattern recognition here we have, uh, the agent is not only about CPU. The agent use the CPU. The restart the error locks. The agent try to use everything to give you this recommendation. And you can see there that you used to the crash loop back off patterns it can used to be it used to the image pull back off and this is our like cla cla classic deployment failure that every box engineer knows. But here you don't have a human here because the agent can catching them before they impact the users and in the pro you can see the score you have 90 if if the if the agent put the score if it is 90 to 100 is excellent come on deploy if 7 uh 75 between 75 and 89. Good approved with monitoring, but if the score is 170, between 7 and 74, is um, it's not so good, but approved with warnings and increased monitoring. But if the score is less than the 60 is critical. Um, I recommend you to not deploy. So this is the score that engine put it there, and now Rosanna is going to show you this application in action. The most part on the beach. Now let's see this in action. We will show you to the modes first, a local observability. CD finance uh with CICD uh with GA Action and Amazon Brop. Let's start. We have two demos. The first one a local observability using clothes. Second one is the CICD using GA action and Amazon Backdrop. With demos using the same idea real time, fast figment and safe deployment. Requirements, what you need before running the demos. First, Local environment your laptop with Docker install terminal or access ability to run local script. Second, configuration fights, uh, an environment with AI model providers, selections, cloth, Amazon Bro or OpenEI. Third. The best part for me cued environment with local or remote in this case AWS EKS cluster Prometheus run it yes and application deployment example demo apps or EGX. And 4 ghat uh robot access the the the content and that's it everything is document scan the QR to access the repository and start building. Let me show you the repositorium. Inside theur folder you have important directory. The first analyzed folder contained Q and edit and Prometheus logit. Second, the models folders manages different AI providers Broth, cloth, or Open AI. The notifications folders sent to Telegrams and tools contain the auxerability script. Everything you need is here. Read it to use simple modular Python scan QR. Our system support three AI providers Blood, Bedrock, cloth, or Open AI. In our demo, uh, user tramps connect every team. Each day, uh, stocks. The AI provider is managing the agent logic. This makes our system simple but strong. Let me show you how simple this is. The code shows how we load each provider. If the user select open EI, we load open EI. If they select cloth, we load cloth. And if they select Amazon Bedroom, well, we load Amazon Bedroom. The idea is one code, many AI model. To run the local AI agent, we use this command Python made.PI. The agent connect to the cluster. The real the metrics is calculated on her score from 0 to 100. Then it decides if the system is healthy, risk, or critical. This is a simple way to test the AI locality. This part is important. AI is not magic. It needs good data to make good decisions. Before we continue with the demos, I need to explain something important. Where do the metrics come from? The answer is Prometheus and Grafana. Prometheus our metrics collection system, and Grafana is our vis relation tool. It's query Prometheus and show the data in beautiful dash room. This is the standard stat. Many companies use the con configuration. This powerful. We also use Telegram for notification. Telegram is amazing box system. The AI send the message. The is approved one of block. This is fast and easy for the teams. You can receive alerts in your phone in real time. You can also use other tools Teams, Slack, WhatsApp. But in this demo we use Telegram because it's very easy to automate. The first demo show how local auxidity were using cloud model. Here the AI was the system in real time. It said the Cuban edit and system in helping. Oh no, is ethics is OS is healthy is good. We say system healthing is something looked wrong. The AI explained the problems and what action we should take. In the repositorium we have the script for local execution. With the script we can test different scenario. Healthy system load testing critical failure or recovery. The AI read the logic and the command in real time and show the system the stereo locally so we get local authority before we deployed anything. This file is a .m configuration file. It shows how to set up local environment which shows the AI provider in this case cloud model ID and cloud API key. Bend and the Q and edit settings like the name space up names and cluster name with the small fight we can run the demo locally and get AI auerability before deploying. Here the perfect date, the AI agent ran locally using cloud model is read the metric from Prometheus and check the health of the system right now the system is healthy, we see 100% score, no anomaly because everything looked good. The AI approved the deployment automatically. Now this is where the magic start. We triggered the demo using local script. The AI agent connect to our AWS EKS cluster, the club model, and the target name space. So Prometheus is got the real time data CPU. Memory use at the connection and put her with that information. The agent analyze and AI head score verifying the system is stable. No restart, no anomaly and everything running cleanly. Say notification is beautiful. The the local script. And was analysis complete, the AI automatically approved the deployment and send detailed telegram, including the model used system stereo analysis time and confidence score. Let's see what happened thing go wrong intentionally. The AI detect the problem, the health score is low, so the AI block the deployment. Now we are going to simulate a real failure scenario. We run a local screen and boom. A little chaos start in our EKS cluster. You can see how the main application start to fade, pot and crashing. Start a metric begin to draws fast and at the moment the AI's agent connect to the cloud model, pull light data from Prometheus and start analyzing everything CPU me input events. Including the detect errors and. Suggestion Remediation step. It's beautiful how running in the script analys Prometheus and set notification to telegram. Sowetografana every time light up and red perfect view and control it kills and with the AI we tracey exactly what happening why and how to fix it finally I say detail. Telegram notification. Perfect. Beautiful day. Demo 2. The second demo at AI to Giga action. Now the AI is part of the pipeline. It check everything, pull request and every deployment step, but the important part, the AI auxidity running behind the scene. This is our AI driven obserability inside gigha action. It starts with the AI deployment gate using Amazon Be or any model we choose. Here on the right can see the configuration, the cluster name, the name space, the model provider promit use and. Our telegram telegrambo for notification we just this parameter the AI know where the app is running and how to verify system health. Now let's see. In action, how the pilot actually before and after the project. This is the amazing local file is built the AI analyze the container. We start with Python 3.1 alpine. Our pine is very small and fast and good practice. Then we cope the requirement file. We install the Python packages. We code the application up. We set the working directorate. We create another root user for security and we set the entry point script. This container runs in giga auction. The entry point script is important. I run the Python analyzer. Then insert the exit code. If exit code is 0, the deployment is a probe. The exit code is 1. The deployment is block. If the exit code is 2, there is an error. The spring clean message that you happen, then it exit with the correct code. To publish the action fight. Define define the inputs. For example, you can choose the AI mobile provider. You can set the Q and edit name space. You can provide the the app's name. You can give the cluster name. You can add the telegram token for notification and this import are optional. They have default values if, if you don't provide them. Now, how does the action run? We use Docker. Docker make it simple. The image is uh on Geeka we send setting but the container what settings your AI shows the cluster information you telegram detail Gika at information automatically the finance ID code version, the environment, everything connect automatically. It's easy and safe. Good news. Now the auction is public. Developers can edit the today pland with just a few lights. They make auction simple for any teams. This is how you set the action in your pinna. If you work from fight, you add use rock rock slash a dragons. The box at made. This example show the full step is very simple. You only add one light. The action name the AI will run during the parent and give you a decision because it's easy. Any developer can use it fast. This is our AI driven pilot in Gita action. In a start with the AI gate check help build, deploy and validation against automatically on every push. In this demo we are inside the repositorium that define our ja action workflow. It start with the AI gau job where we pass parameter like cluster name and model providers so the AI know exactly what. Environment is analyzing then we push a new commit and the pilot runs automatically. If we go to giga, we can see each job in action. The AI gate download the action connect to the cluster and start analyzing he all the blackjack apps. It's perfect the runs, the action. Exchange and telling a notification with all details. Environment commit moderate use and help analyzing the pain and the continuum through to the bill security and a post deploys and again send detail is optional synegate with information. That's how AI automa automation work together to make CICD safer, faster, and easy. This the AI oxerity check running and the pure request is analyzed, performed, memory, and session everything passed with perfect head score 100. In the demo we create and put requests from Fish branch on the deployment branch and soon the as the PR is create automatically trigger the AI auerability final and full review of the system current state before merge. We can see the agent runs inside the workflow connect to the cluster and start is analyzing this it where in the I Beijing is magic, magic and beautiful. And it shaped the cluster hair review metrics, even locks everything from CPU memory to application session. The reason. 100%. No critical events. No warning. Everything is written and 7. Once the validation is complete, the AI automatically approved the deployment mark of PR as ready for me. This time the AI oxidity check critical issue when the AI detect problems is block the deployment. The put request shout a red message with detail they prevent failure from production. In the demo we simulate our fail inside the cluster. We then we when then when create poor request to start the validation again when the work front start, the AI agent connect to the cluster of analyzer, but this time things are not OK. They're running the pilot block of AI find error in the application and problem in the pops. So the AI block the deployment immediate delete is approach and create commit message why the reason for the block and the hairs gone and the main issue is form is also sent tele a notification with the same report and simple recommendation handed out by AI xervity in Giant. These are the real-time notifications sent directly to Telegram. On the left we see successful deployment. Amazon background report and perfect health score of 100 and AI confirm system healthy deploy approved on the right. Critical scenario. The AI, the test pot failure. Mark and her 68 out of 100 and immediately block the deployment. This kick time instant visibility. AI decision analyze and next step delivery right here. They are already collaborate. In this case, there is a notification but uh use uh other tools with teams and lat WhatsApp this card. AI in the box is already changing how we delivery software and the future faster decision, more on time and things that shit with confidence. 3 3 take a weight. AI prevent failure not after but before you can choose the AI model, more flexibility, more confidence, clear explanation, build trust. Teams shift faster when they understand why these three ideas make CICD smarter and safer. The future of CICD is AI powered. We don't observe our system anymore. AI protect them. It predict problems, prevent failure, and kick every deploy safe automatically. This is the next generation of D Box. Imagine two companies. Company ones, traditional auceability deployed, wait, something breaks, fixed 3 a.m. calls stressy teams. Imagine company too. AI powering xerability analyze predict blood back deployment approved good ones, no surprise happy teams. So here is the question, which company do you want to be? Do you want kids fight the fighting at 3 a.m. or do you want AI protecting you deployment? The shoes is yours. The technology exists to date. The code is open source. The demos are ready to rant. You saw everything working light now it's your turn. Take the repository. Try the demos. Transform your teams from ratty to ratty. The future is AI powered. Start building it today. Let me close with, uh, let me, uh, let me close with this. AI won't replace internet, but engineers use it maybe AI is a tools. It's made you strong. It's made you faster. It's made you better. They can prevent problems. Don't be afraid of AI. Use it and you will be successful. Here on the user for resource you can learn more about the tool we use. You can explore AIs, D Box, and CICD. This link helps you continue the journey learning never stop. Want to learn more about building AI agents join the course scan the QR code to register this workshop complement everything you learned me today right now, Elizabeth. Comment about this course. Thank you because this is an open source talk. This is a completely free course that we deliver. We, I deliver this course with my colleague Laura Salina, and this course has 4 courses inside. We have the first course is about starting with trans agents, and because using a trans agents is easy, you can deploy a complete agent to production with a few lines of code. The second one is create a advanced agents using MCP. The third one is multi-agent system, and the final course is about put agents into production using agent core. And it's completely free, it's there and. Thank you so much.