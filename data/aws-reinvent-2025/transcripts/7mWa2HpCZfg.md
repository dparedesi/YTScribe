---
video_id: 7mWa2HpCZfg
video_url: https://www.youtube.com/watch?v=7mWa2HpCZfg
is_generated: False
is_translatable: True
---

Um, good afternoon, and this is probably one of the final sessions before, um, before Reinvent comes in the close, and thanks for joining us. I know we've got some lots of competition today with, uh, Werner Vogel's keynote, but, uh, I appreciate your attendance today. Thank you very much. So my name's Stephen Leedig, I'm a principal solutions architect, uh, with the surplus team out of Australia and New Zealand. Um, and I'm joined today by Archana Shikantha, who, um, is a principal engineer with the AWS Lambda team. And today we want to introduce you to a new feature that we launched on earlier this week, um, called Lambda Managed Instances. So AWS has been, has not just pioneered Servius, um, but as you can see here, we've um continually um and rigorously, um, Innovated in this space. From the early beginnings of introducing runtime like you know Python, um, types, uh, uh, Node JS and Java, we've now and and and for now and and bringing this to today where we're virtually um supporting any type of run time. To introducing VPC integration, optimizing network integration through um hyperplane interfaces, and also introducing things like layers and concurrency, and continually optimizing cold starts to, you know, help you build the applications you're building today. And we've also along the way introduced some new services, step functions and Eventbridge to help you orchestrate millions of workflows and build event-driven architectures. And over the last 10 years, customers have been, you know, really enjoying the benefits of the, this really wide portfolio of serverless services, to help them run and build modern applications in the cloud today. And doing that with, you know, with without having to write or um or write a lot of less code and focusing on business logic, and not getting bogged down by scaling and security updates and other maintenance issues. And only paying for the services that, um, you know, are driving value for your organization, um while minimizing waste. And also being able to take advantage of the best practices um that we have for distributed architectures and security. And as you can see here, we've got a huge um diversity in um in um use cases for errs today. Um, we've got financial institutions migrating core banking applications, um, to AWS leveraging LAMDA and other AWS services to cut operational costs and to accelerate future development. We're looking at healthcare, um, providers who've been using LAMDA to automate, um, appointment bookings, um, insurance claims processing, and secure patient portal access. Retail companies are adopting event-driven architectures as a way of being able to innovate quickly and handle um peak shopping, um, peak shopping spikes. And start-up organizations are using Servius to, you know, keep their costs to net zero, especially in the early parts of their growth period, and then, um being able to iterate on features quickly. And government agencies as well, um, are, are leveraging uh servalus, um, agencies like the Australian Bureau of Statistics, who built their last Australian census entirely on servius. And yet, despite all of these successes and all of these, all of this diversity, um, you know, customers are still telling us we like the programming model, but there are still some use cases for which we need to look at other options. And that's led them to, um, you know, significant architectural shifts, um, and taking on much greater responsibility from an operations perspective. And they're asking for more control around where their um functions are running, and what they're running on. And also being able to apply some of the, you know, um, early commitments that they and usage discounts that they're making on other um on other um compute services. And looking at multi-concurrency as a way of um optimizing price and performance um as well. So land and managed instances is a solution to this. Um, we've got, uh, lender managed instances allows you to keep the the same programming model that you're familiar today and build architectures in the same, uh, surplus way as well, while maintaining, uh, a, a consistent and familiar development experience. And we're giving you more control over specialized compute and extensive choices around um the the compute that um you that that your functions are running on. And also taking advantage of no cold starts. And in addition to that, we're driving efficiencies through the um through and and predictability through ECP EC2 pricing mechanisms, while at the same time giving you an option around multi-concurrency invokes. So what is lambda managed instances? Fundamentally, it's the ability for you to run AWS lambda functions on EC2 instances of your choice in your account. You've got access to over 400 different instance types across um general purpose compute optimized, memory optimized instance family to best suit your particular workload needs. And AWS is still handling all of the operational elements. We're dealing with the life cycle uh the appli the the the life cycle of the instance. We're managing the operating system, um, and the runtime patching that is that's built into those instances. We're dealing with all the routing and auto scaling and doing that according to your configurations. And at the same time, you're benefiting from the ability to um apply easy to pricing um constructs like easy to um savings plans, compute savings plans, reserved instances, and any other special agreements that you have with us. So the question is now, when do you use managed instances? Um So managed instances is not a um in place replacement for lander today. Specifically, you would be looking at using managed instances for things like high traffic, steady state workloads that have um you know, smooth and predictable traffic patterns. You'd be looking at using managed instances for applications that um have very specific computational needs or memory requirements or network uh throughput requirements. And for um and for everything else, you would continue to lose lambda. Lambda today, or lambda default as we're calling it, um, is really ideal for um you know, workloads with unpredictable traffic. And where you have short duration um and um and and infrequent invocations. So what we're gonna do today, um, I'm I'm just gonna walk you through, um, an experience around how to build out your lambda managed instances, um, environment. Um, and we'll talk about some key differences between what lambda default provides, and also what lambda, how lambda managed instances works. And then I'll round it off with some part integrations and information about the developer tooling that we're supporting, as well as some pricing information. Over to your channel. Thank you all right. Uh, hi everyone. So as Steven mentioned, I'm Arch. I'm a principal engineer with LAMDA, and most recently I've been the tech lead on this project which we're all very excited to bring to you. Um, so Steven gave us a really great introduction into why we built LAMDA managed instances or LMI as I'm gonna call it for the rest of this talk, um, and I'm gonna take you on a kind of technical deep dive into the experience of actually creating a function on LMI. And um as we walk through that experience um I want us to just pay attention to the ways in which the LMI experience is actually very similar and very familiar with the default lambda experience that you all kind of know and love today and then in a subsequent section we'll talk about the ways in which LMI is different from from lambda default and how that should inform your choice of when to use which platform. All right, so the eventual setup that we're going for here, like Steven mentioned is in your customer account in your VPC Lambda will launch EC2 instances and then deploy your function on those EC2 instances. Now these EC2 instances are what we call lambda. Managed instances and what that means is it's mostly just a regular EC2 instance except that it's fully managed by lambda. We handle the launching of this instance. We handle the OS patching of these instances, uh, and the entire life cycle of the instance right up to the termination of the instance. What you can do with the instance is you can see it in your console you can describe the instance, but you can't touch it in any way, even if you wanted to, so you can't update the instance you can't edit the instance you can't SSH into the instance, nor can you actually even terminate the instance. So the entire management of these instances are completely delegated to lambda, um, as the service managing it. And in terms of billing like Steven mentioned, you know, regular EC-2 billing applies to these instances along with any kind of pricing instruments that you have with EC-2. Now in terms of these functions that are deployed on your on your instances it's what we call um execution environments and for those of you of you who are not familiar, a lambda function execution environment is basically a live running copy of your application so it's your function code, the language run time underneath it, your layers, your extensions, all of that that's kind of bootstrapped and up and running, ready to handle and invoke so that's what we call a function execution environment. Alright, so how do you make all this magic happen in your account? The experience here involves 3 steps. And we'll look at each of these steps in detail, but the first step here is to create a capacity provider. This is where you give us all your instance level configuration and settings. And then you create a function and then you associate it with that capacity provider that you just created and then finally you publish a function version and this is the step that makes all the magic happen so the the launching of the instances and the deployment of your function on those instances. So let's take a look at each of these in a little bit deeper detail, starting with the capacity provider. The capacity provider is a brand new construct. It's a lambda construct that we've introduced just for LMI and like I said, it's basically all things instances all of your overrides and settings that you wanna provide to us in terms of your instance configuration goes in this capacity provider object. And here are some of the settings here. So the first one is your instance VPC config. The second one is the actual instance types that you want us to use, um, for your functions. And finally we'll look at, um, some guard rails that you can put in terms of how we scale your instances when the load on your functions goes up. So let's start with the VPC config. So here we have our new create capacity provider API. The first thing in there is the capacity provider name. You give it a name. You can put tags on it, and then, and then you have to give us a role. So this is the capacity provider operator role. So this is just a standard IAM role where you're giving lambda permissions to actually launch and manage those EC2 instances in your account. And then we come to the VPC config. This is standard VPC config subnets and security groups, and this is the VPC that will launch your instances into. Now in terms of required parameters this is all you really need to create a capacity provider. Now there's a whole bunch of other settings which we're gonna talk about, but just know that those are all kind of they all have defaults they're advanced settings for the power users who really wanna kind of fine tune the capacity that's that's underneath your functions, but in terms of required parameters, that's all you need. You need a role and you need a VPC and you need to give that to us. In terms of the subnets that you give us in your VPC config, um, you know, if it is a prod application, the standard kind of guidance, AWS guidance applies. Give us subnets in 3 availability zones because when you do that we will actually spread the instances that we launch and thus the execution environments across those availability zones evenly. In terms of networking. These are just regular EC2 instances, so they have a primary network interface in your VPC, which means they get an IP address from your VPC and all of the outbound traffic from your function execution environments actually transits through this network interface of the instance. So if your function wants to talk to any downstream dependencies, make sure that you have a path through your, uh, capacity provider VPC to those endpoints of those dependencies. Also, the, the application logs that we ship uh to CloudWatch, those logs also transit through this network interface of the instance. So another thing you wanna remember and make sure is that you actually have a path to the cloud watch endpoint through your your VPC. You can do this by either allowing Internet access to hit the public endpoint or you can use a private link cloud watch endpoint within your VPC, but just remember that all your logs are also transiting through this instance VPC. And then in terms of ingress traffic this is um the same as is true of lambda default functions today uh there is no ingress traffic that's coming in through that network interface to your instance or your execution environment so you can go ahead and kind of close all those inbound rules on the security groups that that you give us, right? All right, the other thing about VPC configuration is because all of the egress traffic from your functions is going through the instances network, um, we actually do not allow you to specify a VPC config at the function level. So in the create function API, if you're creating an LMI function, um, you cannot provide a VPC config. We'll always just use the VPC config that you provide in your capacity provider. All right, let's talk instance types. So the full set of instance types that is supported on LMI is basically these latest generation C, M, and R instance families. So C is the compute optimized DC2 instance family, uh, M is general purpose, and R is memory optimized. And in terms of sizes we support the dot large instance sizes and, and bigger within these families. And um in terms of architectures we support both Intel and AMD for X86 and then we also support the ARM graviton instance types. Now within this kind of large set of instance types that we support by default lambda will select the instance types for your function based on your functions, memory size and configuration, and we'll talk a little bit more about that when we get to the function section. But you can always override if you want to constrain the set of instance types that we use some more like not use this entire set you can do that via an override and that's where we come to this instance requirements section within the capacity provider here you can specify uh allowed instance types which is only use these instance types or you can specify excluded instance types which is saying you know use everything else but these. And then a few other settings in here, um, architecture by default we assume it's an X86 application, uh, you have to overwrite it for ARM. The other thing is to remember that your architecture of your function matches the architecture of your capacity provider. And finally for the um EBS volumes that are attached to your instances by default they're encrypted with a service managed key uh here you can provide your own KMS key that we we can use to to encrypt the EBS volumes. All right, um, scaling, so we have a, we have a, a pretty deep dive into scaling, um, a few sections later, uh, and we'll talk about some of the more advanced scaling configurations there, but here I just wanted to introduce that there is a capacity provider scaling config section, so this is all instance level scaling here that we're talking about, um, and the first setting there is a max VCPU count. Uh, this is basically a limit on the. The maximum instance capacity that we can scale out to uh as the load increases within your capacity provider, um, it's mostly useful as a cost control knob so you can put kind of a hard limit on the kind of instance billing that can occur from a given capacity provider again optional setting, uh, we have defaults you can override it only if you have a need to. Yeah, and like I said, there's a, there's more settings in here which we'll talk about in the context of the scaling section. All right, so now you have a capacity provider. Your next step is to create a function, and this is where kind of the, the familiar lambda experience kicks in. The, the process to create a function is almost exactly the same thing as you would go through today. With the only little change is that when you're creating a function you have to associate it with this capacity provider that you just created, which is what lets us know that this is an LMI function and it needs to be deployed on your instances as opposed to a default lambda function that goes on our infrastructure. Um, and then we'll just talk about some of the function features that are supported with LMI, um, and finally we'll, uh, talk about the function memory and CPU settings and how that influences the, the instance type that we select underneath those functions. All right. So this is our good old familiar create function API and in here we have a new section for the capacity provider config and that's where you provide the capacity provider R and it's just as simple as that to make it an LMI function. And uh you can associate multiple functions with the same capacity provider, in which case all of those functions will share the same instance capacity um within that capacity provider. All right, moving on to function features that are supported, um. Packaging formats, we support both OCI containers and zip format, language runtime, latest versions of Java, Python, Node, and .NET. So in addition to OS patching that's happening at the instance level, you continue to get the benefit of like the actual language run time is being managed and patched by lambda as well so that's the same familiar experience. Uh, other features that we support in terms of kind of the observability space layers and extensions are supported with LMI in terms of invoke kind of dynamics function URLs you can use with LMI response streaming works with LMI. Um The invoked timeout is 15 minutes, which is the same as, uh, default functions today. Um, but finally we did announce, uh, another big launch from Lambda was durable functions which allows you to run longer running functions, um, that can tolerate interruptions and kind of multi-step applications and that also works with LMI. In terms of some things that are not supported or not applicable I should say to LMI, uh, Snapstart is one that is not applicable to LMI because there are actually no cold starts in LMI and we'll talk a little bit more about that in the scaling section, but because we don't have cold cold starts, uh, snapstart is not meaningful in LMI. And, uh, provision and reserved concurrency, um, these are, are not supported with LMI because basically we have, um, equally valent concepts in LMI by means of min and max execution environments again this is something we'll see when we talk about scaling. All right, so let's talk about function settings now. Um, as most of you are familiar, uh, the create function has a memory size setting where you tell us how much memory your functions execution environments should get. Um, in terms of the ranges, the range has gone up higher for LMI, uh, so we do support up to 32 gigs of memory size for your LMI functions. And then another new setting that we've introduced for LMI is this execution environment memory per VCPU. So this is basically a ratio of memory to VCPU for your functions execution environments. So based on your memory and this ratio we will extrapolate how much CPU needs to be allocated. Um, the default is 2 to 1, so, uh, 2 gigs per per VCPU, uh, and it can go up to 4 to 1 or 8 to 1 are the allowed values there. Um, so applying those ratios and the memory settings, this is kind of the table of all combinations that we allow. One thing to note here is that we don't allow fractional VCPUs, so, uh, depending on the ratio that you're using, your memory will kind of jump in multiples of the ratios if you're using 8 to 1, you can only have memory that's in multiples of 8. If you're using 4 to 1, you can only have memory that's multiples of 4. And then in terms of instance type selection, these ratios that we have, they basically match the family, the instance family ratios that we have up there. So if, if you're doing a 2 to 1 kind of ratio, uh, function, then that's when we'll select the, the compute, uh, optimized instance types. If you're on the other end of the spectrum and you're doing an 8 to 1 ratio, then we'll select the memory optimized instance types underneath your function, and 4 to 1 is the general purpose, um, instance types. All right, so now we have a capacity provider we have a function that's designated as an LMI function, but there's still no instances or execution environments in your in your account yet. All of that magic happens when you actually publish a function version. Which is what actually triggers the deployment in this case. So publishing a function version is actually a functionality again that exists today. Uh, it's not required for lambda default functions so the only change here is with LMI functions you have to publish a version to actually deploy it. Um, but there's, there's two ways you can do that. You can either use the existing published version API or we actually have a little bit of syn syntactic sugar in the create and update function API where you can just set a flag to be true, the publish flag to be true, and then every create and update will automatically publish a version for you behind the scenes. So it's when you publish this version that's when all the action starts happening in your account. And we'll take a look at what that deployment looks like and then we'll also take a look at the function level scaling configuration. All right, so when you call published version, the first thing we'll do is we'll actually map your function to the instance type that's appropriate for it. We'll launch those EC2 instances in your account, and on those EC2 instances we'll actually go ahead and initialize, um, by default 3 execution environments on those instances. We'll launch 3 instances in 3 availability zones if you've given us 3 and and initialize 3 execution environments on them. And until until the initialization is complete, your function is actually in a pending state and only after it goes active can you actually start invoking your function, all right? Now these, these 3 execution environments that come up as part of your function activation, uh, we call them min execution environments it's 3 by default, but of course, um, you can always overwrite them which comes to our function scaling config section. So this is yet another new API for you to override some function level scaling parameters. This is the put function scaling configuration API. Here you have min and max execution environments which is bounding at the function level how many execution environments we can scale in and out. In terms of min, like we said, the default is 3, some, some, some kind of reasons why you might wanna override these some min, if you wanna get, uh, if you wanna pre-provision capacity for like a known peak or a known known incoming demand or a buffer, you can set your min to be higher and we'll always keep those execution environments warm and up and running. Um, on the other hand, you, you can override it lower if you don't care for having 3, execution environments up and running and for that kind of high availability stance if you're using dev or test workloads you can override that to be lower. In terms of max execution environments, uh, by default there is no max, so we're basically allowed to scale as much as we need to, um, you can override it for fair sharing between multiple functions that are mapped to the same capacity provider. So like I said, if you have multiple functions in the same capacity provider, they share the instance capacity within that capacity provider so you can cap how much each can scale to, um, to kind of prevent noisy neighbor disturbance. So this is what I was saying. So these are kind of equivalent to the provision capacity reserve uh provision concurrency reserve concurrency model that we have with on demand. We're just doing it slightly differently here for LMI. And, finally, uh, another small trick here, um, you can set your min and max to 0, and what that will do is that will basically cause your function to completely descale and scale down within your capacity provider. It is in a deactivated state at that point, so invokes won't go through, um, so you'll have to come back and set min and max to something greater than 0 for us to scale it back up and allow those invokes to go through. So it's just a way for you to. Basically deactivate the function. So if you're going home at night, um, you know, for your dev test workloads, you don't have to actually delete the function. You can scale it down and then when you come back in the morning you can scale it back up, right? All right, so you know you created a capacity provider, you created a function, you published the version we did the deployment on your instances and now you're ready to invoke and the the beauty of this feature is that your invoke experience is exactly the same as what it is today. Um, so when your invoke comes to us we will check if your function is an LMI function, if it has that capacity provider associated with it, and if it does, then we will just route all of your invokes to these, these function and, uh, execution environments that we've deployed on your instances. And because the invoke experience is exactly the same, you actually get all of the event source integrations that are supported with Lambda today just work out of the box just the same with your LMI functions, um, including kind of the, uh, the, the different invoke types that we have. We have the direct invoke or synchronous invoke as we call it. We also support the event invocation type and all of this whole slew of event integrations work just as is. So you know we saw how the the the create function and the invoke experience basically we we've retained that as close closely as possible with the the lambda default experience but with just a few extra clicks and a few extra steps with the capacity provider we've effectively completely changed the infrastructure underneath your functions from service owned infrastructure to just regular good old EC2 instances in your account. And because of that kind of big capacity shift underneath your functions there are some differences between lambda default and LMI in terms of the the management of the capacity underneath um that you should be aware of. When you're using LMI, so let's take a look at those. The first one here is concurrency. I'm sure for for those of you who've used lambda before, you're probably intimately familiar with the concurrency of lambda default functions. Uh, concurrency has a slightly different meaning in the context of LMI, and Steven briefly mentioned that we do, um, support multi-concurrent functions in LMI, so we'll take a look at that here. The second one is scaling. Uh, this is another thing that Steven mentioned is that we don't have any cold starts in LMI, so we do scaling in a slightly different way in LMI, and we'll, we'll take a deeper look at that and understand how scaling happens in LMI. And finally, um, the security boundary. With, uh, with lambda default you're running in in our service account it's a fully multi-tenant set up here with, uh, LMI you're gonna be running in your account which is a fully kind of single tenant set up so the boundaries are a little bit different there in terms of security and we'll, we'll talk about that. All right, let's start with concurrency. Before we dive into LMI, let's recap a little bit about what concurrency means for um lambda default. So in lambda default when you have a function. You see an invoke come in um if we don't have any execution environments for your function yet, what we will do is in the path of the invoke we will say ah there's no execution environment we'll initialize a new execution environment and then execute your invoke within that execution environment. Now while that invoke is ongoing, if we get a 2nd invoke, we'll say, oh that execution environment one is busy, and what that will do is it'll initialize a second execution environment within which your 2nd invoke will execute. Now these invokes that. Cause the initialization of new execution environments, um, we call them cold starts and they incur slightly higher latency because we have to actually uh run your initialization logic uh as part of kind of the the synchronous invoke request path, right? Now this invoke number 3, it comes in after invoke 1 has completed. So what happens with invoke 3 is we actually keep execution environment 1 around in the hope that you'll send us more invokes. So with invoke 3 we can just route it to a pre-existing execution environment and we don't have to pay the cost of that initialization and this is what we call warm invokes and everyone loves warm invokes because they're super fast and they, they don't incur that latency cost of the in it. So the thing to notice here is that in lambda default these execution environments are all what we call singly concurrent, which means that um there's only ever one invoke being executed out of a given execution environment at a time. It may get reused for multiple invokes later in time, but at a given time only one invoke is active per execution environment. So when we say concurrency in lambda default, what that really means it's just the number of active in-flight invokes or the number of of active execution environments that are serving in those those in-flight invokes that's what concurrency means in lambda on demand. In LMI things are a little bit different, so here we have 3 execution environments that we talked about that were pre-initialized when you published your version. And then when your invokes start to come in an LMI will actually send we can send multiple concurrent invokes to a single execution environment and this is what we call multi-concurrency single execution environment can be handling multiple invokes simultaneously. And um And this is why we, we don't have cold starts because we've pre-initialized these execution environments as part of your function activation. So when the invokes come in they will always get routed to one of these execution environments that we've pre-initialized. So no cold starts in LMI. So the first question here is how do you know how much concurrency or how much multi concurrency can you send to each execution environment? Now we came up with some defaults for the maximum concurrency that an execution environment can take based on the language just after researching all the applications that you all have built on lambda and other kind of serverless um products in our our suite, uh, we looked at all of the workloads and these are kind of the defaults that we came up with. But you can always override it and this is in our create function API in that new capacity provider config section you can also specify a per execution environment environment max concurrency so this is if you have certain bottlenecks that we're not aware of in terms of your dependencies or anything like that, you can come in here and override what's the max concurrency and execution environment can take. Now we, we also realized that it's not easy to come up with this magic max concurrency number and there is no one size fits all, uh, so we have some protections here in place in case you set that max concurrency number to be too high. So let's take a look at how that works. So here you have your managed DC2 instances with a few execution environments deployed on it. And um also on these instances sitting in front of your execution environments is an agent that we deploy we're calling it the LMI agent here and this agent also acts as a proxy to the to your execution environments when the invokes come in. So when the invokes come into our service, we will try to kind of balance the load evenly across all the execution environments that you have. We'll pick an execution environment and we'll route it to this LMI agent on that instance. Now if that this execution environment has either reached the max con concurrency that you've configured or if it's running even before it reaches the max concurrency, if it starts running into high mem uh resource pressure either memory pressure or CPU pressure, then our LMI agent will say hey back off this thing is in is is running hot go try somewhere else and then we will reroute your invoke to a different execution environment. Now if all if all of these execution environments kind of start to heat up and everybody's is telling us to back off, then we'll actually just come back to you and throttle your invoke uh back and we do this like I said for for what I call good put protection. So this is we don't wanna just blindly send invoke traffic all the way through to your execution environments until you hit this kind of max magic max concurrency number and actually brown out or crash all of your execution environments and you have a full kind of outage of your app. So we're measuring and seeing how your execution environments are doing and if it starts to get to a point where we think it's gonna brown out, we'll start rejecting some traffic so that at least you can make forward progress with the with the capacity that you have, right? In terms of metrics, if this starts happening, um, we have new cloud watch metrics, uh, for the exact reason that you got throttled in the bottlenecked resource, um, so you can get throttled because of your max concurrency, that's concurrency throttles, CPU throttles, memory disk, what was the bottleneck resource that caused those throttles. And another thing about multi-concurrency before we move to the next section is because your execution environments are handling multiple invokes simultaneously. Um, it is important for you to realize that you need to have these threat safety best practices, um, in mind as you code and develop your application. So this is again something that's different from lambda, uh, default where you have singly concurrent, um, execution environments here thread safety best practices, you know, avoid mutating any shared objects or global objects when you have multiple invokes running at the same time. Uh, use thread local storage and thread-safe storage for your, your data structures, um. Any shared clients or connections that you're initializing, uh, make sure that the configuration on those are immutable, uh, in the invoke itself and also writing to disk, um, you need to remember if multiple invokes are writing to disk at the same time they can clobber on top of each other so use request specific file names for any, any rights you're doing to slash 10. All right, so that was concurrency. So let's talk about scaling. So again let's recap what scaling means in lambda default. Um, in lambda default, the scaling is just kind of organic. It's these cold starts this is that is when we initialize new execution environments and that's really just how we scale when you send us your big kind of load of invokes if, if you're getting a lot of concurrent invokes, we just naturally scale out the execution environments uh as cold starts in the invoke path. But in LMI, um, we said we don't have cold starts, so the question becomes if all of these existing execution environments start to hit that point of saturation, um, how and when do we actually scale up new execution environments? And the scaling in LMI is actually asynchronous and resource based scaling. So what this means is that we're constantly monitoring again the resource heat on your execution environments and when we see that it's starting to get close to a certain threshold, that's when we'll, uh, asynchronously scale up new execution environments within your capacity provider. And if that needs new instances we'll do that as well, so we'll we'll take a look at that here. So here you have, um, you know, 3 instances with 3 execution environments. You have a friendly LMI agent there and the LMI agent is constantly kind of gathering CPU usage stats from your execution environments and your, your instances and, uh, kind of sending that data over to us. And we are monitoring the kind of average um CPU utilization uh at the capacity provider level but also at your function level and we're trying to maintain this target kind of CPU utilization threshold. And if you're kind of within this plus or minus 10% of that target threshold, then that's when we call you as you know you're in steady state, you're happy, there's no scaling action happening. If you start to heat up your CPU and you start to get into that range above the the the +10%, that's when we'll start to scale up new execution environments. Um, first we'll fill up existing EC2 instances and then if you need more, we'll launch new EC2 instances and, and deploy more execution environments on those. Now if you have a burst of traffic that's um very spiky and it bursts up faster than we can react and add these instances and execution environments to your capacity provider, that's when you can push your CPU utilization into that kind of good put protection mode uh where we have to throttle you until we can actually bring up more instances and more execution environments uh to meet that that load, right? In terms of scale down, it's the opposite. If you then start to cool down and come below that -10% mark, uh, we'll first take, take, uh, scale down your execution environments and then we'll decompress and then actually scale, scale down instances if we can, uh, kind of pack it back in, right. And uh in terms of scale down you're always protected by your min execution environments that you configure which is 3 by default but we'll never scale you down uh below that number even if if you're seeing no invokes to your function. Alright, so the, the question that all of you are probably having is what is this magic target CPU utilization threshold? Um So this is this is where we come back to that create capacity provider scaling config and the more advanced scaling options that I talked about here. We have a scaling mode that can be automatic or manual. If it's automatic, um, that's basically we are looking at your load patterns, your scaling patterns, and automatically tuning that threshold, um. To be at a good place for your application in manual mode you can take things into your hands into your control, and you can manually basically configure a target CPU utilization percentage. In terms of metrics to monitor all of this fun stuff again these are all new metrics that we've added for LMI at the function level we give you CPU utilization, memory utilization, so this is kind of aggregated across all the execution environments for that function. You can also see the concurrent executions like where your max limit is versus what you're actually using and finally, um, the chart at the end there is the actual number of execution environments that we've scaled up to for your function. And we also have similar metrics out at the capacity provider level, um, so this is if you have multiple functions within the capacity provider at the instance level you can look at memory utilization, CPU utilization, and then the actual count of instances that we've launched within your capacity provider. All right, so the scaling takeaway here that I wanted to leave you with, and this is again something that Steven brought up is with lambda default, um, we, you know, when your functions run on our infrastructure we are running a very, very specialized stack that's built on Firecracker and that is hyper optimized to handle these kind of. Spiky sparse bursty workloads, it's hyper optimized for cold starts. It's hyper optimized to keep those cold start latencies down so that we can actually like bring up a new, uh, kind of lightweight firecracker VM, um, in the path of your invoke. Lambda managed instances on the other hand is actually built and designed for a different profile of workloads like Steven was saying this is built more for kind of their stable workloads that have a good baseline of traffic, uh, and more smoother and predictable workloads, you know, the whole, the purpose of this feature is to move away from specialized stacks that we are using behind the scenes and to move to more general purpose kind of easy to instances in your account. Um, so the machinery underneath is very different and that's why the profile of workloads that are that LMI is suited for is, is different, um, from the profile that's suited for for default. So the two are really complementary, um, features of lambda that you can use together to kind of cover more of your use cases. All right, um, the final section here is the security boundary. Like I said, with, uh, Lambda default, all of your functions are running in our service account. It's a big multi-tenant account, um. And uh in lambda default every every functions execution environment runs in its own VM because it's a multi-tenant setup with LMI, uh, everything's running in your account, nothing, no, nobody else's, um, code is running in your account it's a single tenant set up in that sense. And the security boundary here for LMI is really the capacity provider, um, so if you want VM level isolation between your um functions, the way you would do that is to have separate capacity providers because by definition, um. The instances within your two separate capacity providers are going to be different. So if you map two functions to separate capacity providers, they will be on different VMs in different instances. Uh, within the EC2 instance, um, the function execution environments that we deploy, uh, they're, they're basically containers, so they're separated by a container boundary. Also, if you map multiple functions to the capacity provider like I mentioned, um, then the execution environments from those different functions can also share the same instances and again so it's, it's a container boundary there so the takeaway here really is if you want VM level isolation between your functions, um, map them to separate capacity providers because that is the kind of VM level security boundary in LMI. All right, with that, um, I will hand back to Steven to talk about partner integration and tooling and pricing. OK, so that was a really good introduction into um into LMI um and I'm just gonna talk through some of the new partner integrations that we have um and introduce you to our launch partners DataDog and SEI. So Data Dog provides full observability for lambda managed instances. Um, customers can monitor um key metrics to understand the health and the utilization of the lambda managed instances, and customers can also alert on those metrics. Um, and any errors and anomalous, um, behavior, um, that might need attention. Um, using the automatically correlated metrics, logs, and tracers, um, that span upstream and downstream services, um, customers can also investigate and resolve any, um, any issues. And for any new instances that get launched, um, there, there are, um, trace support and auto instrumentation, um, so customers can get automatic trace propagation simply by adding the installing the Data Dog, um, extension. Now SI also supports lambda managed instances um through its autonomous um optimization platform um that helps improve performance, cost and reliability of uh the lambda, EC2 instances and the rest of the environment. Um, this gives, um, engineering and platform teams as well as, uh, fin ops teams, a single workflow, um, um, for making safe, uh, data optimized, um, decisions. Um, and their platform automatically scores lambda managed, uh, lambda functions, um, to show you which are strong candidates for moving to lambda managed instances so you don't have to do the guesswork. And once you're ready to move, ZI's co-pilot also lets you um migrate uh and configure functions um with a single click and it's a simple low friction way of moving and migrating your functions to lambda managed instances. Now AWS app config um also featured flag um capability um and and the other dynamic configuration that comes with that service is also fully managed, supported by lambda managed instances. So by using AWS app config agent um lambda extension. As a, as your, um, as your lambda functions, you can make calling those feature flags, uh, simpler, and also the extension itself includes best practices that simplify using um AWS app config um by by reducing costs. And that cost reduction uh results from fewer API calls uh to the AWS app config service and shorter and also um results in shorter lambda processing times. Now, Amazon Cloudwatch Lambda Insights um also has uh provides uh 11 click deployment for uh from the lambda console. So this allows you to filter uh capacity providers. It gives you, um, it allows you to drill down into instance types and functions, uh. As well as providing you with 12 key metrics, um, um, with 1 minute granularity, that, um, uh, for things like the maximum and average CPU utilization, as well as memory utilization statistics. And this is providing you with a fully integrated experience to monitor your lambda managed instances. Now if you're using power tools for AWS today, uh, Power tools is um is really a suite of utilities that help you, um, um, standardize, um, application development, um, and support across a number of different, um, uh, a number of different, um, uh, use cases, such as observability, batch processing, um, helps you with item potency implementations, as well as, um, dealing with things like feature flags and data extraction. And the the power tool suite is also fully compatible and thread safety, uh a thread safe um uh and ready to run on your lab and managed instances. And of course we've got um full infrastructure as code support uh through AWS cloud formation, the surus application model, and the um AW the AWS cloud development kit, as well as our partners Terraform. And all of those things uh uh all of the, all of that, all of the APIs that Channa was talking about are fully supported um within those um within those frameworks. Now from a pricing perspective, um, AWS um sorry, Lambda Managed instances uses EC EC2 basing uh pricing, um, with the addition of a 15% management fee on top of the EC2 instance costs. Now the price of the instance itself will largely depend on um the amount, the discounts that are applied to those instances. However, the EC2 um management, so the um the management fee is still based on the default price for the EC2 instance. Now on top of that, um, you would still be charged for the um the same uh cost for per invocation um that lambda default has today, uh, with the exception that you're no longer paying for the uh function duration costs because everything is running on your machine. So to wrap things up, let's have a look at some key takeaways. As I mentioned earlier, lambda management instances isn't designed to be a a replacement for the lambda that you're uh lambda functions that you're running on lambda default today. It's really designed for those specific use cases to help you with high traffic and steady state workloads or where you need specialized compute options to run functions um uh for uh for specific use cases. For everything else, lambda, um, lambda uh default, um, provides you, um, you know, uh, the, the ability to continue working, uh, running new applications that have unpredictable traffic, short duration, or infrequent invocations. So that's a really key takeaway here. And also when you've seen what we've, what we've, when we, when we, when you understand what we've just done for you is basically we've allowed you to define your own execution environments. This is you running lambda as you do today, the same experience, you get to maintain the same programming model, you you get to maintain the same architecture, you get to use the same development tools, but you've got more control over where and how your functions are running. And you're also able to apply all of the cost benefits that you're getting with EC-2 on top of that. So that's all we have for you today. Um, we are really interested to see what you build with land and managed instances um and look forward to um to to seeing what you do with that. Thank you very much.