---
video_id: YdV1Hj5YdB4
video_url: https://www.youtube.com/watch?v=YdV1Hj5YdB4
is_generated: False
is_translatable: True
---

Thank you and welcome to IND 371, accelerating product development with Lifecycle of a product digital twin. My name's Steve Blackwell. Um, I lead our product engineering and services at Centre of Excellence here at AWS, um, looking after our automotive and manufacturing customers. So for the next 20 minutes, um I really want to talk about how we're seeing manufacturers really start to look at their data and how they can actually use that data to address the kind of the product development. And this is really kind of realized through what we define as a, a digital twin or in some instances, or what we call a product digital twin, or in some instances people call it kind of a digital threat. What we're seeing is that a lot of manufacturers have got the same industry challenges that they need to be able to differentiate their products in a competitive market. And as part of that competitive market, they need to increase their kind of speed of innovation and being able to bring new features and capabilities to market as fast as they possibly can. A prime example of this is in the automotive industry where we're seeing kind of the traditional OEMs having now to compete with the. Chinese OEMs and examples like BYD who are able to develop new vehicles and develop new features to in-vehicle within kind of a two-year kind of period, where a traditional OEM had a development life cycle of 5 to 7 years to build a kind of new platform and launch that actual vehicle. And the way that, you know, we're seeing these um manufacturers be able to kind of reduce these MPI cycles, new product introduction life cycles from like 5 to 7 years down to 2 years, is really looking at how kind of software can be actually used and how AI can actually be used to kind of accelerate that. And specifically, you know, as part of that whole kind of development life cycle, is how they can actually reduce the kind of total cost of ownership or total cost of kind of development, you know, of that actual product, because the faster they can develop it, the lower they can cost it actually takes to actually develop that. It means that they can actually generate more margin on that product when it actually goes to market. And we're seeing this across all types of manufacturers, everything from consumer electronic manufacturers that do your robot vacuum cleaners, through to, you know, aerospace manufacturers that build aircraft engines and aircrafts. But, you know, when we talk about kind of, you know, the product and the product data, that product data kind of resides across, you know, different parts of the manufacturing value chain. So we have, you know, the engineers who are actually developing and designing a product. You know, they have their applications, and this is a record that they work with. That then obviously gets handed off to um the manufacturing team who actually will be then working with suppliers or um to source raw materials or parts to actually manufacture that product or in some cases kind of assemble that product. And again, they have their own systems that they actually work with, for example, like a manufacturing execution system, you know, and their supplier systems. And then finally once that product has actually been manufactured and it goes out into the, the wild, and it's actually being used by consumers, there are other systems that are now being used to actually maintain and service those products like MRO systems, after sales, service systems and so forth. You know, traditionally, these three different areas of the value chain as what we describe as kind of as designed from the R&D point of view. As manufactured or as built for the manufacturing operations perspective, and then obviously once it's out in the wild kind of as serviced, as is kind of sat in those data silos, and what we're seeing now with manufacturers is that they can actually see value in actually taking all that data and actually connecting it up as part of building kind of a, a digital thread of that, specifically around focus on the product so they could actually start harnessing. And using that data. A great example I'd like to give around this is a lot of manufacturers are now creating a, you know, a connected product, and they have telemetry information flowing in from that product of how that product's being used out in the field, um, either by, you know, characteristics of how they're owners of those products are actually using it or potentially kind of events, you know, occurring on that product like a a motor failure or, you know, a battery getting low charge and so forth. There's a huge amount of information that's coming through from that connected product field. Now let's take a kind of an example, that information of how that product is actually being used is really valuable to the engineers that are actually designing the next iteration of that product. So they could actually start to look at the usage characteristics, you know, how that product is actually being operated, how it's been actually maintained, and use that information to actually feed into their design process so that rather than just purely kind of simulating kind of scenarios, they could actually use real life data to actually feed into kind of their analysis and their design. Similar example around this is when we talk about kind of a quality issue, so we start to see quality issues that are created by service events or product failures out in the field. Is that quality issue or that service failure resulting from the actual manufacturing issue when it was actually manufactured? Is it because a particular part um is out of specification that was supplied by them, or is it actually an engineering defect that's not meeting a specification and quality standard? You know, for the manufacturer to be able to understand exactly what that quality defect is and where it actually originated, they need to be able to link this status together. So we kind of realized this through what we call the manufacturing data flywheel, whereas we're enabling manufacturers across this product digital thread to be able to connect the multiple different data sets together to be able to start to interrogate that data and actually get insights of data to drive increased value in their products, so they develop it, how they manufacture it, and how they actually maintain and run it. So those are kind of a couple of examples of where we're seeing kind of the the product data being used across the manufacturing value chain. But let's start to look at like how do we actually kind of implement and start doing this. So if we look at kind of like a product life cycle of the data, OK, and I've kind of simplified this down, you know, and the product there, obviously we have the sales teams that come in and do kind of a market analysis, requirements, sales forecasting. Then the engineers come in, they kind of design that product based on the requirements that the sales and marketing teams give them, it's then manufactured and then it kind of maintained. And this is a a continuous circle of development as you see a product living over multiple years. But then we look on the outside, there is multiple systems of records that are actually used and interacted with as part of that whole life cycle. You know, as an example, you know, the sales team use like probably a CRM system to track sales forecasting and kind of their customers, you know, as they kind of take that, the marketing analysis, they're going to create requirements they want to actually feed to the engineers to actually need to design against that, so there's going to be a requirement system. Most products today are actually not just hardware. Their hardware, software, they may have electronics, um they may have back-end cloud services that actually support it, so it's gonna be multiple kind of streams of product development that comes into case. So in that design phase, you may have your CAD models, you may have your PLM systems, you have your stimulation data, if you're doing software development, you've got your application life cycle management, you've got electronics, you've got your EDA application. There's a huge amount of data out there. In the manufacturing we've got our MES system that's recording, you know, how that product is being manufactured and kind of assembled, and then obviously when it's out in the field, we've obviously got our connected system and our MRO product that tracks the kind of the maintenance and how it's actually being serviced. So there's a huge amount of data there that we need to bring together to be able to start building this product digital um twin. So how do we do this? We don't do it by basically taking all those systems of record and bringing them in and creating another system of record. We achieve this through what we known as the the knowledge graph, OK? And what the knowledge graph allows us to do is be able to take those different data sets and be able to create relationships between the different data sets and the the basically the entities or records um across the whole life cycle. So as an example here, we have a requirement that's been created by marketing that defines a particular feature um of that actual product that is related to the product itself. When that product actually then goes and be manufactured, that's gonna be part of the bill of materials, but specifically when it's actually manufactured, it's gonna be part of the MOM, the manufacturing bill of materials. When we're actually gonna manufacture it, obviously there's gonna be a process plan that says when we actually take that bill of materials or the parts and materials, this is the way you actually do it, you perform this operation, this operation, this operation to get to there. OK, and then obviously that's got to flow through the actual factory floor, so kind of a routing. As part of that there's obviously gonna be a production order that says how many actual operations, how many parts, how many products we're actually gonna create. So you can see the complexity of all these different types of data, but that different types of data is actually linked together, you know, a, a product is gonna have a part number. That part number is gonna have multiple subpart numbers. You know, when you do operation one, you're gonna take part number 1 and assemble it to part number 2. So you can see there's actually relationships between even different types of data, so this is really where the knowledge graph comes in. It allows us to be able to create the relationship between the different types of data sets on common parts, so a defect at the end may not be associated with a requirement, but we can actually trace stability through the knowledge graph from what a defect is, it's a quality issue, it's done in a production order, it's associated with this part number of this product which is associated with this requirement. So it's very powerful. Additional is with that knowledge graph, you can actually enter and do queries against any part of the actual relationships that you've actually built out. So I could start and ask a question, you know, for this product, what are the part numbers, and it'll list out the part numbers. I could do it from another end and go for this quality defect, what are the associated part numbers and interrogate from there. So that relationship is really key for. Manufacturers to be able to use their data because it doesn't matter if it's the service department, they're probably looking at it from a defect or a service quality issue or an engineer that's looking for requirements or a part number. They're using the same data, they're just interrogating from different ends of the knowledge graph. OK. So I'm talking about kind of, you know, how a knowledge graph is valuable, but when a manufacturer's actually, you know, built that knowledge graph and they've actually got that product digital twin, they can actually look at that product across its whole life cycle. There's really a lot of value from that on top of actually just having the ability to access the data and understand the data in the context of the actual product. You know, first of all, it allows manufacturers to start looking at new ways of actually doing systems development. Some people may have heard of model-based systems engineering. So with MBSC they have the ability to actually reduce that MPI cycle. One of the benefits I spoke about at the front at the beginning. They can look at traceability because now they have the ability to track, you know, defects in the field right back to where that actual defect potentially occurred, if it's an actual bad requirement based on how it's actually going to be operated, or is it again for a manufacturing um operations problem. Allows them to actually collaborate as well, because a lot of manufacturers are working with multiple suppliers for raw materials and parts, so they can actually have traceability, but they also can actually collaborate, so if a subsystem is done by a supplier, that can be integrated into the knowledge graph to actually have that information sharing and traceability and being able to connect across. And then finally you've got that digital continuity. You can actually truly connect all those different systems of record and actually get value from the data um from those systems of record rather than just individually within those systems. So now I want to kind of talk about, you know, how do we actually enable this, OK, so we've developed what we call the digital engineering framework. So this is a set of architectural best practices to enable manufacturers to basically implement this knowledge graph. OK, and this product digital twin is what we bring together, this knowledge graph is what we're calling the product data fabric, OK? So underneath there we've got all our systems of record I spoke about before, and through the digital engineering framework, we can then help manufacturers build that knowledge graph, that product data fabric that they can then interlock all their engineering and development tools across the whole of that MPI life cycle. So it doesn't matter if they're developing the mechanics, the electronics, the software. Their engineering development tools can access that product digital thread through the product data fabric. OK, so what does that product data fabric actually look like from an AWS architecture perspective? OK. So kind of simplifying it very much. First of all we've got our sys a couple of our systems of record out there, OK? And what we wanna do is we wanna take the data from those systems of record and basically bring it into our data store which our knowledge graph can actually reference. So the way that we can do that is using obviously AWS DMS database migration services, to take the data from the database and actually bring it into the S3 bucket. Or you can use AWS Data sync to sync the data from on-premise if you're actually gonna export it and obviously then transfer it up into your S3 bucket. Some instances like for example if you're running SAP you could probably do a direct export from your SAP environment into that S3 bucket, OK? From that S3 bucket, we then obviously then use Amazon Neptune to actually then start building the relationships across all those different data sets so that you've actually got your knowledge graph. So that, that knowledge graph, that product data fabric is basically the core of it is Amazon Neptune. That's the thing that allows us to kind of realize that. OK? Any of you that have actually worked with Amazon, Neptune or Graph databases will know that they have a specific query language that allows you to be able to, you know, access the knowledge graph and be able to interpret it and get information back. That's not generally a common skill set, and this is really where the value of generative AI kind of comes in, because we can use generative AI on specific large language models to be able to use natural language to be able to interrogate our knowledge graph, take the results of our knowledge graph and put it back into um natural language. So this is where we kind of use Amazon Bedrock and obviously Lamber to um do the actual particular queries against Bedrock against our kind of knowledge graph. So rather than having to write, you know, a que a query in kind of Spark or Gremlin, we can actually just do what part numbers are associated with this defect. Obviously Bedrock would then turn that natural language into the query. Interrogate our Amazon Neptune knowledge graph and then bring the results back and then turn it back to it's part numbers 123456, as an example. Obviously, as part of that, we need to provide the ability for people to be able to access and submit those natural language queries. So we need a web-based application and, you know, an APII depending if you want to do it via the web interface or you want to do it via a third-party application. That's associated kind of on the, the right-hand side on block number 6. And obviously a key thing is this is manufacturing and engineering data, you know, this needs to be secure, protected. In most cases, this is the key IP for manufacturers. So we need to make sure that we've got the right access controls in place, that the certain people only have access to the data they need to with the RBAC models. Obviously we need to security, we need to secure by design. So obviously areas of 7 and 8 address that and allows us to be able to audit and kind of monitor that. And obviously then on the complete right hand side we then have our engineering tools and development to be able to then interact with that. So it can be done via kind of a web interface or we can build natural um or native integration with whatever engineering system the customers are using to actually interact with that knowledge graph. OK. With that, I'd like to say thank you, and that's kind of like a very lightning talk kind of brief overview of kind of where we see, you know, a product digital um twing coming in and how that manufacturing knowledge graph, the product data fabric really can drive the value down. I appreciate it. If anyone got any questions, I'll be around for a couple of minutes and I'm happy to answer them.