---
video_id: hnHKG8Fo-0A
video_url: https://www.youtube.com/watch?v=hnHKG8Fo-0A
is_generated: False
is_translatable: True
---

My name is Derek Martinez, and I'm a senior solutions architect here at AWS on the nonprofits team and today with me, I have Sabrina Petruzzo, the security lead for the nonprofits team. Uh, by show of hands, just so I can get to know who's in the room, do we have any security engineers or developers? All right, now who here gets that call at 2 a.m. when something breaks or somebody has a breach? Show of hands? Yeah, I've been there before. I understand your pain. Now here's the million dollar question. How many of you, you all are running AI in production right now? Perfect. Keep those hands up if you can tell me exactly what sensitive data your applications handle as well as what governance controls you have to handle that sensitive information. All right. The OAS Foundation came out with the top 10 risk for large language models. And guess what the top one on the list was? You guessed it, prompt injection. So today we're gonna talk about 4 key security areas. The first is gonna be data sanitization. The 2nd is going to be prompt injection defenses. The 3rd is securing a machine learning pipeline, and the 4th is defense in depth strategy. So we're going to take a look at our defense in depth strategy first. As you see here, we have 6 layers. In the first layer, we're gonna actually enable encryption. Uh, we're gonna do this for our data at rest as well as our endpoints. And in our second layer, we're gonna implement fine-grain access control, leveraging an identity access management service, IAM and AWS. And we're also going to in layer 3, create a comprehensive audit and defense system leveraging cloud trail, uh, to make sure that we know who is doing what and what actions they took. And in our fourth layer, we're gonna create automated compliance, right? So what does that mean? We're gonna leverage AWS config. To define a preset of rules so that if our system deviates from those preset rules, it will monitor, it'll alert, and then we can take action to remediate against that. And then in layer 5, we're gonna do real, uh, or we're gonna do PII detection and data sanitization. This is where you're gonna actually see our live code talk. So I just wanted to point that section out to you and then in our last layer, we're gonna do prompt injection defense that will show you a chatbot a little bit later. Cool. So today we're gonna be building a nonprofit healthcare chatbot that your internal teams can utilize to query, uh, sensitive patient information. So first we have an internal data owner. So, uh, your internal data owner will go ahead and upload documents, patient data, um, into an Amazon S3 bucket. This is going to serve as your, uh, initial data entry point for the information that's going to be powering our chatbot. So for example, this internal data owner could be a medical provider who's uploading patient records into your system. Once documents are uploaded into S3, um, it does trigger an Amazon staging or pipeline. Uh, within this pipeline, we have different data protection functionality via Amazon Texttract and Amazon Comprehend. So, Amazon Textract will actually scan through the input documents and extract text from those documents, and then Comprehend will go ahead and process those documents to detect PII found within your data. This is also where we're implementing differential privacy techniques, uh, for data sanitization. And then once the documents are processed, we're storing them in another S3 bucket, and that S3 bucket is going to be the bucket powering our chatbot. This also separates the raw data from the process data by having two separate S3 buckets, um, just so we're not, you know, fetching any raw, raw unprocessed data for our chatbot. All right, now, oops, sorry, go ahead. And then, yeah, in terms of security, so we also have auditing set up. So for auditing, everything is going to be logged via Amazon Cloud Trail. Those logs are stored securely in an Amazon S3 bucket, and they're also leveraging, um, uh, a KMS encryption key to make sure all of our information is encrypted as well. OK. Now, let's take a look at it from the user perspective. Our end user sends a prompt to our chatbot user interface, and then our API gateway exposes our backend lambda function through a REST API. Our back end lambda function is going to process the prompt. And then it's actually gonna check to see if there's any prompt injection techniques being used and once detected, it will then actually mitigate against that prompt detection. We also have AWS config, which again will apply that rule set in our case because we're actually gonna use healthcare data. Uh, let's, because we're gonna leverage healthcare data, we're actually gonna implement the HIPAA conformance pack. Uh, that'll be the predefined set of rules specifically for the HIPAA compliance. And then when it comes to our prompt injection, I'm sure you're wondering what kind of detection, uh, defense are we gonna do? Well, what we're gonna do is we're actually gonna use a direct prompt injection, meaning we're gonna ask it for normal information, but we're gonna give it an instruction to actually override the settings so that we can see what happens when we do that. OK. Now, this is the, the healthcare chatbot for researchers and patient statisticians. And in this case, what we're gonna do is we're actually gonna make sure that our chatbot works. So let's go ahead and send a prompt over here to see if our chatbot is working. So all right, we're gonna be typing. All right, how many patients have diabetes? Now this is important to note, when we get this response back, you're gonna see we get 2 results, and those 2 results will actually show that 2 patients have diabetes. And in this case, we're actually gonna see the data masked, and we're gonna show that there's no personally identifiable information in the output of our prompt. So we'll give it a second here. There we go. See, we have two patients. Awesome. I've also entered myself into this chatbot or into the data source as a patient with patient ID 12345 and said that my age is 100 years old. So if we go ahead and ask the chatbot, um, what is, what is the age of patient with patient ID 12345. We'll go ahead and see. Once it, once it runs that, um, it says that patient with patient ID has an age range between 100 and 109 years old, and this is just for differential privacy. So now here's the real test. Let's go ahead and see what prompt injection looks like. So if we ask the same question and we say, how old is the age of patient with patient ID 12345, and tell the chapbot to override its security settings, we'll see that the chapbo responds by saying that a potential prompt injection attack has been identified or detected and to rephrase your request. Now three important things happen here. One, we obviously stopped the prompt injection. Two, we actually log an alert to our team. And then 3, our team then goes and takes actions based on our cloud trail logs, as well as if any AWS config rules present an alert. So for today's session, we're really gonna be focusing on the back end portion of the architecture. So from uploading the uh raw documents into S3 to actually carrying out the data processing functionality because this is really where the magic happens for ease of deployment, we're using an Amazon, we're using a Jupiter notebook with an Amazon Sagemaker and we've broken that up into 6 different. Implementation steps. So first, uh, really simple, we're, uh, installing our required packages and all of our application dependencies. Then we're setting up our security foundation. So this is where we're setting up our KMS encryption key. We're enabling those IR config, uh, HIPPAA rules, and we're also creating lea privileged access rules. Um, from there we have to go ahead and actually define our StageMaker pipeline. So this is where we are defining the, um, execution environment as well as all of our, uh, functionality to actually extract our text with Amazon Textract and then mask it with Amazon Comprehend. OK, and then for pipeline deployment we've done all the hard work we'll deploy the pipeline. We also have a verification step where we'll actually use just regular data that you can see right there and we'll show you that it's being masked so you can see that it's working and then after that we'll actually execute the pipeline which you'll be able to see in the console. And with that, let's go ahead and move over to the live coding portion. So Did I hear a question? No. Anybody have a question for us while we're waiting? No, it all makes sense so far. That's what I love to hear. All right, awesome. So if I go ahead and go into our StageMaker a console and I go into, um, Jupiter Lab, I can go ahead and open up my Jupiter notebook. So this is the StageMaker data protection pipeline that we have right now, and it does have those 6 different implementation steps that we talked about. So, um, what's, what's nice about, um, this notebook here is that we can go ahead and actually click through each individual section to validate that it works before moving on to the next section. So I'll go ahead and start off by setting up our dependencies. Some of this we do have pre-built out. However, that data protection file with the comprehend and text functionality will be live coding today. So I'll go ahead and go through and set up our, um, packages and dependencies. So we'll click through and once our setup and dependencies have, um, successfully completed, we will go ahead and see a print statement stating that all dependencies are imported successfully. Next, we'll go ahead and set up our security infrastructure. So again, that's that KMS key, those config rules and, um, our IM rules that, that we need as well. So I'll go ahead and run this one as well. For Oh yeah, let me go ahead and zoom in. Sorry about that. I like it. We got our first question. All right. It's official. We made it. Is that better? Perfect. OK. So here once we ran our security infrastructure section, we can see that um we've set it up, we've created our encryption key, we've created our IEM role, and we've also enabled the HIPAA compliance conformance pack within config. So step 3 is where we're actually defining our, um, Amazon Stage Maker pipeline. So first we're just defining some pipeline parameters like the input and output bucket. Then again we have that processing environment. So we're just setting up the, the processor, the instance type, um, uh, the session, um, and so on and so forth. Then we also have a step here. Where we are actually defining the um comprehend and extract data protection um step. So we have a data protection, data protection Python file here that we're referencing. We'll actually want to go ahead and create that Python file, which is what we'll be coding today. Um, so what I'm gonna do here is go into Quiro, which is one of our GE AI ID environments. Um, and here I'll go ahead and start coding the, um, the code to, um, to support our data protection functionality. All right, and so for this part, nothing fancy, we're just gonna install our dependencies, make sure we import our tools. And now we're gonna define our first variable. This is gonna actually create two extraction paths, one specifically for text files, one for PDFs and images, as well as JSON files. Now, we're also gonna validate that, uh, our paths exist, and if they don't, we'll fail gracefully. And now for this one, what we're gonna do is we're actually gonna take a look at our text files. This is where we're actually starting to do something. So we're actually gonna open it up. We're gonna scan the text in the files, and we're actually gonna extract it and return it as a string. All right. And in this case, what we're gonna do is we're actually gonna show that the uh the PDF and the scanned images is gonna be a little different than what we talked about with the text files. First, we wanna make sure we're in the right region, right? We want to be in the region our data's in. Second, we're gonna create a textracthoTO 3 client. This will allow us to do our uh extraction. And then what we're actually gonna do is we're gonna open the files in binary mode. We're gonna pass the file bytes to Textract, and then it'll actually return the text in a, uh, for us. And then here what we're actually gonna do is when we get the return text, uh, when it comes to textct you can actually return uh blocks of code and blocks of code could be a line, it could be a word or a page and so what we're doing here is we're specifically telling it we wanna return a line because we don't want a single word, right? We're basically recreating the documents that we have in our raw bucket we just wanna clean them, right? So we don't wanna, we wanna make sure it matches the way it looked before. Yep, zoom in again. Oh my gosh, I'm so sorry. Yes, let me go ahead and zoom in. And while she's zooming in, I can just tell you this is where we do our, uh, air handling. Basically, we again want to fail gracefully. So we want to make sure that the pipeline doesn't air out if the file path isn't found, uh, it's just gonna ignore that step. There will be a part later where we'll deal with that. OK, our next step, we're actually gonna implement, get ready, we're gonna have our differential privacy. This is where we start, right? We're gonna start with the randomization. So in this particular instance, what we're actually gonna do is we're gonna randomize the, uh, we're gonna implement the randomization, uh, if you go to the next line of code, we're actually going to. Apply a value here and we're gonna say that if the value doesn't match what we're looking for, then go ahead and do nothing and the reason why is because we don't want it to manipulate the document, right? So we have to make sure that steps in there first because if we wind up changing something in the document again we're not being able to replicate the document correctly. And then for this particular step, this is where we start rounding the values. What we're gonna do is we're actually gonna take the years, and we're gonna round them to a unit of 10. Now, I'll give you an example. Let's say somebody was born in 1981, right? We wanna make sure that rather than giving them the exact date, we give them a range. Now, I got a question for y'all. Why would we do that? Does anybody know? Correct, because it's a quasi identifier. You'll see this come up with a couple of different of our, uh, examples here. What that means is that it could be used with something else to be able to identify somebody, right? That's a quasi identifier. So what we're really doing here is we're just making sure that it's less likely that you will be able to identify somebody based on their birth year because we're giving it a range. OK. All right, here we go. This is a fun word to say. Uh, does anybody know how to pronounce that? We throw it out to the audience. No. OK. An anonymity. That's right. You guys got it. Good job. Uh, so here what we're doing is we're actually gonna do the same thing with the ages, right? We're going to leverage, uh, if you were. If you're your age is 35, we're gonna provide an age range of, and we did it with Sabrina. I don't know if you saw in the example, but if you're 35, we give you a range from 30 to 39 again age isn't something that I think I could identify somebody on just by just by itself, but if I continue to add other bits of data now all of a sudden it gets a little bit easier, right? So that's what we're gonna implement here. In order to make sure that uh we are protected and one of the things I wanna call out is that we're also going to, uh, if an error or if a a value kind of matches. Uh, an age. But it's not an actual age. We want it to do something, right? So what it's actually gonna do is it's not gonna act on it, right? So I'll give you an example, a phone number, right? You don't want your, your phone number, I mean, yes, you do want it to be masked, but you don't wanna be masked as an age, right? So we just need to make sure that the system understands that it's gonna do this particular type of behavior. And it's important to note that this applied privacy protection function right here is actually our privacy protection router. And the reason we have a privacy protection routers, as you all know, is because different types of PII require different types of differential privacy techniques, right? So in this case, we want our file to be intelligent enough to say, hey, this PII looks a little different than that PII, right? Which I'll go into a little more later, but I don't wanna steal the show yet. All right. Same concept here. We're just masking the PII data based on the type. And while I got you here, Why, why would we implement can an anonymity? Anybody know? Hint, hint. I kind of gave you the answer earlier, but I wanted to see if somebody else would answer. Is to reduce the ability to say's name and their age and Correct, exactly, because it's a quasi identifier. Good job, sir. All right, let's see here. OK, so in this case. Now we want to find and hide the, uh, this is where AWS Comprehend is gonna come in if you actually do the next line of code as well. You're actually gonna see that we're gonna do the same thing for comprehend that we did for Textract. We're gonna create our BTO 3 client. Do we need to be in the same region as our data? Anybody? Is that good best practice? Yes, exactly. Yeah, I heard that, yes. So what we're gonna do again, we're gonna apply this, uh, BA 3 client. We're gonna have the ability to now, in this case, we're gonna find the PII and we're actually going to then apply the appropriate privacy protection to it. All right, so in this case what we're gonna do is we're actually gonna give it a confidence score, right? So we're gonna say, OK, I wanna return for instance the year of uh somebody and I want it to have the ability to be 80% accurate. So in this case, we're just applying the differential privacy technique, and we're also applying a level of confidence to that. Who here is having fun? Is anybody excited to look at code today? Yeah, all right, I'm the only nerd here. I got you. Thanks, thanks for that. Uh, OK, so in this case, what we're actually gonna do is we're gonna get the, the text, and then we're actually going to replace the text. And in this case, depending again on the type of PII as well as our confidence score, we will do the replacement. Anybody have a question at this point? We're cruising through this thing, so I don't mind slowing down. Yes, sir. How do you know to use the score? Is there a error? There is a trail, yeah, yeah. I, I think for us we use, sorry, go ahead. What were you gonna say? We did play around with different confidence scores, um, so it really depends on, you know, what your output is and what you're expecting, or, you know, what level of, you know, what threshold of. You know, in, in, incorrect output, you can, your, your workload can, uh, withstand, um, so it really is something that you can, you can play around with and see what works best for your, your use case, yeah. And I, I saw a lot of the examples I saw were 90%, but we wanted to show variants. We do show another one later that shows 90% as well. Any other questions? All right, we're moving on. OK, this just again, hey, if there's an error, uh, when we detect our PII again this actually pertains to me specifically to if the PII I'm looking at, I can't quite determine if this is PII PII or not, we're gonna go ahead and just return the text the way it was. Again, the purpose of this is to make sure our document stays as intact as possible, right? OK, so in this case, uh, you're gonna see this a little later, but what we're actually gonna do is we're gonna make, and you can go ahead and add that next line of code, but we're gonna make our input and output folders. This is where our process is gonna happen. The important thing about this is if it doesn't exist, we're gonna make it and your input output is gonna be very important, right? Our input is our raw data in our bucket. Our output now becomes our clean data. And then also, if it doesn't exist, we create these buckets so that we have them in our pipeline, so it doesn't fail. All right, and for this line, now we're actually showing the file types that we're actually gonna look for, and we give the system the prompt to do that and here we're also gonna say we wanna do this in a batch processing style and it's important to note that if one fails, we don't wanna stop the files, we just wanna ignore that file for now and then let everybody know. That or let our team know that we didn't scan that particular file because of an error. All right, and on this step we're actually gonna clean the text. Once we have the clean text, we're gonna actually output it. Well, we're also gonna print what we did here. This is Sabrina and I, we feel very confident about making sure that our system understands or like tells us what it did. So we actually wanted to print the applied privacy protection. And as well as what was found in mast and we're actually gonna put this in an audit bucket and our audit bucket is something we can go back later and we can say, hey, in that document, what do we actually do? What did we find, right? And then it's important to note that it will actually put it in a, yeah, you can go ahead and put that next line. I think I'm still in the thunder here. Yeah, it's gonna put it in an output folder and it's actually gonna say cleaned on it. It's one way we'll know we'll be able to differentiate it, right? Here's our audit, right? This is what we really wanna make sure our teams understand when we're going through this process. We have an audit log set up. We have an audit bucket. We'll be able to go in and make sure again that we understand what kind of PII was detected and because we have cloud trail as well, we can also see who accessed our audit bucket, right? We wanna make sure that we know who has access to it if they performed any actions. Obviously we wanna lock down our bucket, but sometimes things happen and that's why we have a service like Cloud Trail to be able to monitor. All right, are y'all still having fun? Sweet, I'm doing my job. OK, so in this case, we, again, I know we're, we're belaboring the point, but for us, if we get an error, the last thing we wanna do is stop the whole process, right? We would rather alert, let our team go and look at it, but continue the process with what we know is good so that we're still processing everything and it's not stopping our pipeline. I think we're getting close to the end here, huh? We are. This is the last function. wow, we did this in record time. So this is a standard, uh, well, hold on, before I say that, let me ask a question. Does anybody know what this does? Why would we put this in there? I'm looking at the coders in the group. If you just call up a Python file, we'll run. There you go. I knew somebody would know it. I actually did not know before we started doing this, and I found it out and I was like, wow, my whole life has changed. Yeah, I wanna say that I think we missed something earlier.II. I don't think there was anything after the trial I was missing. Let's go back and look. Maybe I never mind. Yeah, I think I, I just didn't see it. Yeah, no worries. Yeah, the find and hide. Yeah. No worries. All right, so this is the coding portion. So this is where I'm gonna open the floor up for Q&A. Uh, feel free to ask any question you want, and Sabrina's gonna answer all your questions today. We're also still gonna go back into the decision maker notebook, um, and actually run this code, but if anyone does have any questions right now before we move on, happy to answer those. That's true. I jumped ahead a little bit. Dirk got a little too excited there, but it happens. All right, if there are no questions, we can show you what it looks like in the console and we can run the actual SageMaker notebook. OK, so, uh, we've actually uploaded this file into our SageMaker, um, environment already. Um, so essentially this, uh, step 3, the StageMaker pipeline again, that's setting up that processing environment, um, and actually defining the functionality for our, um, text extraction and, uh, PII masking. Um, it's also setting up the audit trail, um, enabling encryption, things like that. So if we go ahead and run step 3. We'll see that we have our pipeline, uh, pipeline creation functions defined. So it's, it's actually use that data protection pipe, uh, Python file that we just created to, to set up, uh, Amazon Textract and Amazon Comprehend to do that data sanitization for us. All right, now we're gonna deploy the pipeline. Fingers crossed, everybody. No, I know it's gonna work. I tested it. I got a backup one just in case it fails, it's OK. And then now what we're gonna do is we're gonna go to the verification step here. You are about to see. It masked real time. There you go. We have masked data. Everybody give me applause. Yay. All right, now we're gonna execute the pipeline though. I bet you didn't know we had one more step, huh? All right, so we're gonna execute the pipeline. Now this is important because we have an input bucket and an output bucket. The data protection.P file created both of those for us. Our files are in there. We're syncing our files. We have some kind of in another bucket, and we wanted to show you a syncing step, and then we're gonna execute the pipeline. On the last step, there we go. Now let's go to the console. Yes, so the pipeline's actually been, um, executed. So now if we go back, let's go back into the sagemaker console here. So if we go into Stage Maker Studio and click on pipelines, we'll see here that we have our data protection pipeline created for us and right now it's currently in the executing phase, so the pipeline's running right now executing. Uh, it does take about 2.5 minutes or so for our pipeline to finish, um, executing. Um, and we can show you the files, but, uh, we can see that we have our data protection step that we implemented in our CageMaker pipeline currently running. And again, that's just doing that text track and comprehend, um, text extraction and PII ma masking for us. Any questions while we wait? Has anyone done, uh, go ahead, um, sorry, I, I think I missed what, how can we identify what was or wasn't PII so that was in the comprehend. So in the comprehend step we notate what PII was there. Sorry, I probably did brush over that. So for like Social Security numbers, phone numbers, emails, we have a step in there that actually differentiates the two. So like age we. We did K an anonymity for years. We did the randomized units and then we also do a step in there where we actually identify Social Security numbers and based on that type of PII we just mask the data. Yeah. Was there another question? No, OK, OK. Has anyone done something like this before or any data sanitization, whether you're using, uh, you know, Texttrack Comprehend or like Bedrock or another service? All right, let me ask another question. Has is anybody's organization really pushing generative AI right now? And are you or somebody on your team tasked with trying to figure out how not to expose data. In your generative AI application, yeah, so this was one of, this is the way this came about. It's one way to do it, uh, but we wanted to provide customers. So again, we work in the nonprofit space. A lot of our customers don't have a lot of, uh, big teams, and so we try to find solutions that they could just stand up and run today. So that's where all this came from, and that's the, the emphasis behind it is make sure that when you're exposing your data to your generative AI application. You know what data you have and whether or not it's masked. In our case, it's a HIPAA compliant customer, so PII is unacceptable, right? We can't expose any PII, right? So in this case, we made sure that uh we sanitize that data before it got there. And now we could leverage a generative AI application and we know our data is good. Yeah, so we see that our pipeline has successfully executed, um, so we could go into the pipeline if we wanted to. Um, we could open it up. And you can see the status here as well as your files, um, and settings details, of course, uh, but what we could do now is actually go into, um, Amazon S3 and if I go into my buckets here. I can see that I have uh different buckets created for me and all this was done through the Sage Maker, uh, the Sage Maker or the Jupiter notebook and and Amazon Sage Maker. So everything that we created there, um, it created those, uh, the input bucket, output bucket, um, as well as the audit logs bucket. So if we go ahead and go into our data protection output bucket, we can see here that we have all of our clean files, um, so I uploaded files. Like Sabrina. TXT, um, Sherman. PNG, and so on, so it added that clean prefix for me, uh, to differentiate, you know, a raw file versus a clean file, and I could go ahead and actually download one of these files if I wanted to. So again, I mentioned I added myself as a, um, a a patient with patient ID 12345. I actually created a file for myself. So if I go ahead and download this text file. I can open it up for you all. Wrong screen there. Uh, let's try to open it up on the right screen. Is it gonna open? OK, well, you could probably drag it over. There you go. So we can open it up here at least and I can see here I have my patient ID. It says I'm patient ID 12345 and then it has all of my sensitive data masked for me so you can't see my name, my email address, my phone number. You can see that my age is 100 to 109. Again, I entered myself as a patient and said I was 100 years old, so we're applying that differential privacy here, um, and you can still see my reason for my visit, um, and then I have patient notes and I say it's always day one. So I know the question going on in your head right now. Derek, how do I know if you go back to the bucket real quick. How does my system know the difference between a name John Doe and a letter Dear John, right? And comprehend you in in that step where we're actually identifying the data, comprehend will actually process that for us. So I just wanted you to know that was one of the questions that I got a lot as we were practicing this and going through it like how does it differentiate it the comprehend service itself actually does that for us. OK. Opening the floor up for questions. I know somebody's got one. It's been burning in your brain. OK, we got a couple. Let's go here first. Yeah. Yeah, AWS config. Yeah. There, oh, yes, mhm. Yeah, so if we actually go to, you want to go to our Sagemaker notebook real quick? So, in order to save time, We built out this security infrastructure. If you open that Python file, so this is where we're actually building out all the security infrastructure that we talked about in that very first step and so in this case what we're doing is we're taking AWS config to monitor our configuration of our systems and services and then we're gonna figure out or we're gonna be alerted if our rules are deviated from. And config does that for us now. HIPPAA is a conformance pack that has a preset list of rules. In this case, I believe it's um a managed conform conformance pack in which we actually give you that list of rules, and this is what auditors have told us, uh, help with HIPAA compliance, right? It doesn't make you HIPAA compliant, but those set of rules are rules that you can start monitoring for and give to your compliance teams as a way to show them that you're monitoring for that. Good. OK, next question, sir. Yeah, um, what if instead of masking you needed for your application like the data to be synthetic instead, is that a huge lift to the pipeline to, to support that? That's not a huge lift. Um, you can use, I would have to double check whether to comprehend sports at natively, like, basically putting dummy data, it sounds like you wanna do. I believe I would have to double check about comprehend, but that's something you could do with glue jobs, for example, if you wanted to. So you can actually use different glue jobs to, um, you know, input fake data instead of, uh, the data that you have there in place like a text string instead of putting like 1234, you can do like 4567, for example. So you can definitely do that as well if, if that's, uh, within your use case. Yeah, and think of this like a starter, right? We could go into many different arcs based off of this pipeline right here, right? Absolutely. Any other questions? I thought I saw another hand. Now's your time. This may go on YouTube. It's a chance for you to be famous. No? OK. We're good. Let's go to the next. This is where I get to tell you about all the, we got to go back to PowerPoint. It's where I get to tell you about all the fun things we're doing on the nonprofits team. Get your phones out, get ready, we got some QR codes coming. Oh, you got it there we go. OK, so we have more sessions now. I know it says nonprofit, but there are many different services that we talk about. It's not just nonprofits, but if you are a nonprofit, we'd love for you to come to our sessions. A lot of them are geared towards nonprofit customers, whether it's health care or charities, um, so just feel free to capture that screen or that QR code. And then next, if you are a member of the AWS nonprofit team, meaning you're a nonprofit customer. Grab this QR code right here. It'll actually allow you to be able to communicate with us, and we can tell you who your account rep is so you can start talking to people like Sabrina and myself, bring in solutions architects to have architectural conversations, and then we have an area in the pavilion where you can actually come by and see us. We're actually right by Movember, so you can come talk to us and then go get a haircut. Oh, SkillBuilder. Who here is plugged into Skill Builder? Got a couple, OK? If you're looking for some free trainings, there are paid for trainings as well, but we have a lot of free trainings. Here's a good way to get access to that just to get yourself started, um, and then I think this one, yeah, yeah, OK, it says it's specific to generative AI. All right, lastly. We have seen what we've accomplished today. You've built an AI security framework, not slides, but production-ready code. We talked about our six layers encryption, PII detection, access control, and trail audit trails, automated compliance, data sanitization, and secure prompt handling. This is your competitive advantage. Here's what to do next. Take these templates. That we have shown you this week and just pick one AI workload to try and do something against and lastly I'd just like to thank you all it's been a pleasure of ours we know that your time is valuable it's the most valuable thing here at Reinvent right? And we're just so thankful that you joined us today. Thank you everyone we appreciate it.