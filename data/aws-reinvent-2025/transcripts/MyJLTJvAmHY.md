---
video_id: MyJLTJvAmHY
video_url: https://www.youtube.com/watch?v=MyJLTJvAmHY
is_generated: False
is_translatable: True
---

Good afternoon. If you don't mind putting on your headsets, we're gonna watch the premiere of Avatar 3, or actually we'll be having a session today, uh, CMP 302, uh, about accelerating engineering using HPC on AWS across industries. So before we get started, um, I wanna remind you that towards the end of the presentation you will have QR codes on the screen. And I encourage you to go take a picture. They will give you all the sessions that will be covering HPC throughout Reinvent this year, and we will motivate you to go and attend them. The second thing that I wanted to say is we really, really appreciate everybody filling out the surveys at the end. That is how we can get better year on year and give you the best content and the best experience when you come back next year at Reinvent 2026. Um, so before we dive into the presentation, I wanted to introduce, uh, two additional speakers that will be on stage with me a little bit later in this hour. Uh, they're sitting up here up front. The first, uh, co-speaker is Karima Dridi from ARM. Um, she is the productivity engineering, uh, vice president of productivity engineering at ARM. She leads global teams that run ARM's large scale design and verification flows, including adopting AI and cloud-based EDA to help ARM's CPU and Neuverse platforms reach customers faster. She will be the first speaker to come on stage after my 20 minute discussion. The second speaker that will join me, her name is Taylor Gowan. She is the modeling and engineering team lead at DTN. For atmospheric science, she leads a team of scientists who build AI-based and physical-based numerical weather prediction systems on AWS, powering DTN's global forecasts and weather intelligence for customers around the world. So today's topic is accelerating engineering cross-industry cloud transformations. Let's take a look at the agenda. I'm gonna start by giving you some key market trend updates, uh, and if you attended Supercomput in Saint Louis two weeks ago, there will be a repeat of what you heard through the Hyperion Research discussions. Um, I'll dive into cloud adoption in HPC. Um, we'll talk about the AWS HPC building blocks. Um, we'll talk about cross-industry use cases to make all of this a little bit more real, and, um, Karima and, uh, uh, Taylor will talk about the journey of ARM. And the journey of DTN as uh they were leveraging AWS for their HBC needs. I'll give you a free uh a few uh comments at the end for recap and remarks, and we should be able to close within an hour. So let's start with a few market trends. As you can imagine, HPC and AI as an overall market is growing quite a bit. Uh, the numbers that you see is the growth in 2024, according to Hyperion Research, was 23.5%. Um, historically over the last 20 years, growth rates were about 7 to 8% a year, so about three times the growth rate we saw last year in 2024. Um, and the overall market size for HPC is now close to $60 billion. And just a little spoiler alert, cloud is about $10 billion out of the $60 billion and you may think, well, of course last year AI grew gangbusters, so really that's the effect of AI, and you're not completely wrong, but what I do want to point out is that if we take out AI as part of the growth of the HPC and AI market, you still see that the HPC portion, traditional simulation, calculation, scientific innovation, still grew 8.4%. And when we look at that growth compared to historical growth of 6 to 8%, that's still faster than we've seen in the past, so compounded HPC traditional workloads and the AI enhancements that we all see and implement in our day to day, um, this has been driving this growth of over 20% year on year. The AI infrastructure is is fantastic, as I said, that portion of the growth was 166%. Um, so now let's double click into the cloud adoption because we're here to talk about cloud, right? So cloud adoption in 2024, I said is about $10 billion and we expect that adoption to continue growing. And if you look at the prediction for 2029, belief is that it will be as high as $23.7 billion which will account for about 20% of the overall HBC and AI market. So now if you do the math. If it was 15% last year and it's going to be 20% of the overall market, that means that cloud is growing faster than on-premise. Not really a surprise, and we'll talk about the reason for which we see that trend, but something to note. Here are 5 reasons that explain why cloud is growing faster than on premise. Number 1 reason, some workloads cannot run on premise. Um, the jobs are blocked, capacity limits, and you want to run a simulation, you want to design a part for a car, or you want to go after the next scientific discovery, but the queue is cloud and you can't run it on premise, so cloud can really help in that situation. The second reason is access to the latest GPUs. If you watched or attended the keynote with Matt Garman, our CEO, you saw that we are already launching GB 300. We're launching ranium 3. We have ranium 4 being announced already that's going to be launched later. All those technologies are moving fast and in the cloud you have access to them as soon as they're available. You don't have to go build a data center, allocate power, and try to bring all this to bear in a record time. Number 3 is run at larger scale. We all think we designed the right environment, the right infrastructure for what we need today. We put a little bit buffer for tomorrow and the day after tomorrow, but we always end up in a situation where we need more, more infrastructure, more access to technology helps you design or build more products for your customers. And cloud elasticity is what makes it possible. If you decided to use 10 instances and tomorrow you want twice or 10 times the number of instances that you were using yesterday, you can do that in a cloud environment. Number 4, time to results and queue times can be shortened. It's just a lever. You want bigger infrastructure, you can get things done faster. You may have to pay a little bit more, but at least you have that flexibility. If you have a looming deadline to go finish a design and you need to go pile it up high so you finish on time, you can make it happen in a cloud environment. The last one is collaboration globally and sustainability. Um, I encourage you to go on the AWS website. There is a page that talks about our efficiency for our data centers. Uh, PUE amazing, uh, considering the scale that we're at, uh, and then collaboration globally. Anybody in the world can access, uh, AWS instances, and if you have your, uh, data in, uh, in, in an AWS environment, everybody technically under your account can have access to it. So If I had to summarize it in one diagram, this is the diagram. On the left you have a wheel, multiple sets to go from the idea to the data. On the right, which cloud and HPC being run in the cloud, you can go a lot faster. 4 steps you go from an idea and before you know you already have the results. You can analyze, um, and that's the flexibility and the power of cloud. So let's dive a bit into the building blocks on AWS. Um, we have built purpose-built building blocks for HBC. You see them here in 3 sections from the bottom to the top. At the bottom you have the infrastructure components HPC optimized compute instances, HPC optimized accelerators, HPC optimized network fabric, HPC optimized storage. You have access to that. Then you get to the next layer is how do you consume. You can consume on demand. You can create savings plans if you want to commit to a longer term utilization. You can have access to spot instances and then recently we also launched capacity blocks as you plan in the future to use that capacity and we reserve it for you. On top of that we've got a set of services and I'll point out 4 of them um PCS, parallel cluster computing Service, Batch, Parallel luster, and RES Research and Engineering studio. Those are 4 services that are available as soon as you build up your environment, and those services help you build and manage your environment for all of you HPC users. Let's dive shortly into the instances that are optimized for. Um, on the compute side you have HPC 6A7A 6 ID 7G. Uh, a little bit of a cheat sheet. The number is the generation, so 6 is two generations ago, 7 is current, 8 is the one that just came out and that will come out. Um, the letter at the end is, uh, the technology. A stands for AMD, I for Intel, uh, and then G for Graviton. On the right hand side you see all the accelerated computing technologies that we have from Nvidia from the L4 all the way to the GB 200, and again if you heard the keynote with Matt Garman this morning, GB 300 being launched already. Now this is just for HPC today. I do have one slide letting you know that pretty soon, I cannot tell you when, but pretty soon in 2026, uh, we will have an HPC 8A which will be Turin-based, uh, which is the 5th generation AMD Epic processor that will be available to all of you. Um, get excited because it's better performance. Cost performance is going to get you into the next tranche of, uh, workloads and your ability to save and go faster for your simulation and designs. OK, let's take a look at the rest. Those were optimized for HPC, but now we also have a broad portfolio, and the portfolio here, I'm not gonna go through every single instance. You can find them online, uh, but I do want to point out that we also at AWS design our own chips. Uh, Graviton is our processor based on ARM.ranium and Inferia are accelerators, uh, and ranium sounds like a training chip, but it's also great for inferencing. Um, so you have those chips here that are available, um, and you can have access to them. If I do a double click into Trainium, you may or may not know that we have made a decision to build a massive set of data centers, uh, under a project called Rainier. That will host 500,000 ranium 3 chips that will be rannium 2 chips that will be used to train the next generation of anthropic cloud model. So if you in your mind you were thinking, well, rannium seems to be an interesting silicon, does it really scale? We're about to demonstrate to the world that yes, it does scale and it does bring the next generation of large language model to life. Um, Graviton 4, I'd be remiss not to mention that we've continued to innovate out of our 4th generation of, uh, Graviton chip, um, based on the Neoverse A core, um, a lot of cash, 7 chilets, 12 DDR 5 channels, and 96 lanes of PCIE Gen 5. OK, so compute is great. Compute without storage is useless. So if you go into your environment, you now have all the compute flexibility that you want been in storage. You have object file and block. Object is S3, which is the most popular, lowest cost solution for all of our customers. But when you get into the file type of portfolio items, we have EFS, FSX for Windows, FSX for NetApp ONTA, FSX for Luster, which is probably the most commonly used high-speed storage solution for most of HPC customers and AI customers, and then FSX opens EFS. And then if you're blocked, then you have Amazon, but all those are connected through our EFA fabric to those compute instances, and you can mix and match and you can do also some clever movement of data between object and file depending on what you're looking to accomplish in your environment. So if you remember the layer cake that I showed you, you have the 3 layers. You have the infrastructure, you have the consumption model, we talked about storage, um, now you have the orchestration layer on top, uh, and I'll only talk about 3. I know there were 4 on the, on the previous slide, but I'll talk about PCS, batch, and, uh, parallel cluster. Uh, PCS is a fully managed service, uh, based on SLRM. And makes it easy and more intuitive to go build your environment if you don't want to go all CLI and build it yourself, or if you have teams that you're managing that don't have the in-depth knowledge, the environment is already preset and I encourage you to try it. We launched it about a year and a half ago and we're at a point where it's fully featured with all the improvements that we've gotten through the initial customers. Second one is Batch. Batch is a computing service that schedules and runs containerized workloads. Um, it has a full range of compute offering, uh, and could be the right solution for you if you're running containers. Um, the last one is parallel cluster. Uh, that's our legacy, uh, service, and, uh, legacy doesn't mean that it's not still gonna be very much used by a lot of you. It's open source, uh, and it's what most of our customers are using for HPC. I would be remiss not to talk about quantum. And this may seem weird because this is not a quantum talk, but We see more and more HBC. Partners, customers, researchers think about quantum and at AWS we have uh made an effort to continue to bring technology to our customers and partners, um, and we have this service called Bracket uh that uses a lot of our partners quantum computers. And makes them available to our AWS customers. So now in one single environment in your AWS account you can have access to HPC environments but also quantum environments and you can do mix and match. And by the way, the world is still trying to figure out where to fit quantum in an overall work. But today we're looking at at least 3 modalities that we have made available ion trap, supercomputing, and neutral atoms. If you're interested as an HPC customer, you even have access to credits to go run some quantum experiments and feel free to talk to me after the presentation today if you want to know a little bit more about it. OK, um, I'm gonna go a little bit faster through the rest of, uh, the, the slides because I, I want, uh, my, my co-speakers to also have plenty of time to talk about their journey, but. All this technology is great. It's theory, it's out there. You can click it, but a lot of customers and partners have used that in the industry, and I'm, I'm gonna go one after the other. The first one I'm gonna talk about is CAE and EDA for structural analysis, finite element analysis, IC design, timing and single single integrity. You recognize a lot of the logos at the bottom. The second area or industry where we've seen a lot of utilization of AWS for an HBC framework is healthcare life science, genomic sequencing, drug discovery, personalized medicine, uh, epi epidemiological modeling. That's a tongue twister, um, but the names at the bottom, you recognize the mergery of them, and they're using AWES for their HBC workloads in HCLS. Energy, power grid simulation, nuclear reaction design, oil and gas, renewable energy system modeling, all those logos that you recognize. The next one is financial services. A lot of financial services platforms are running on AWS and so for an HBC workload, risk analysis, fraud detection, portfolio optimization, market simulation, and forecasting, all those are being used. Mm. Otto. Computer fluid dynamics, crash and safety, advanced driver assistance, autonomous driving algorithms, all those logos that you recognize. And the last ones university and academia where you see high energy physics, astrophysical modeling, computational chemistry, social science and analytics. Actually, I missed one. I should I missed that one, weather. Global climate modeling, weather forecasting, hurricane and storm surge, air quality modeling, I recognize the DTN logo in there. OK, as a slight primer for our next two speakers, I'm just gonna cover briefly some of the challenges that are being resolved in a cloud environment for EDA. I'm gonna skip this and just go to the uh challenges themselves. Why would EDEA run best on AWS? Gives you the ability to innovate faster, collaborate faster and better, reduce risk and reduce cost. Weather Giving you again just a little primer as to why you would wanna run weather modeling on AWS in the cloud, um, look at these. Benefits deploying production weather and climate forecasting faster, collaborating better, reducing risk and reducing cost so you can see a theme here you can do a whole lot in uh the cloud on AWS, but this again is only theory names um so it is my pleasure uh to bring Karima to the stage so she can walk us through her journey at ARM accelerating engineering on AWS. Hello everyone. Um, my name is Karim Adridi. I'm very excited to be on stage today. I'm leading productivity engineering in ARM, and my team is looking after compute strategy and enablement for engineering. Today, I will take you into ARM cloud journey, a transformation that is shaping our business and our industry. I will speak first about our business, then I will take you through our cloud journey, how we use cloud to build our product, and last but not least, I will speak about the optimization and efficiency happening to enable best usage of cloud. So first of all, AM's global compute footprint is unmatched, powering nearly every connected device and enabling innovation across industries. 100% of the connected global population relies on AMS, and over 325 billion arms-based chips have been shipped up to date by our ecosystem. We also have 22 million software developers on AM, so ARM has definitely the scale, the power, efficiency, and the performance that enables to accelerate AI everywhere. In fact, on the infrastructure side, we've come a long way since the launch of our newWs brand 7 years ago for cloud and data centers. We have shipped over 1 billion NuWaves core into data centers and currently are tracking towards 50% unit shipment into the hyperscale in 2025. And we owe this success to the technology leadership as well as the strength of our ecosystem, including. We also working together, we have enabled 70,000 companies to adopt AM in the cloud. 15 years ago it was mainly AM building and supporting to move software to AM and making investment in the infrastructure space. Today all our ecosystems, all hyperscale, are actively investing to ensure supported. The question is not anymore does it support AM. But mainly is it supporting A first and delivering seamless user and developer experience. AWS strategic partnership started back in 2018. It is about a shared vision that is fueled by the less efficient and scalable compute, as well as a joint value proposition with the universe and graviton. OK. So now I, I will speak about and how we are using cloud actually into our environment to build the product. As we look at our current industry pressure, as we look at our current business landscape, we have 4 interconnected challenges that are feeding directly into our strategy and priorities. The first one is the industry pressure driven by increased complexity, the relentless time to market, and the industry dynamics. This is shaping how we plan, execute and deliver our technology. The second one is the explosion for computes. We saw compute, and this is driven by AI workload and also the need for sustainability. We saw definitely shifts from compute being a cost center to compute being a strategic enabler, one that must be power efficient and high performance. The third element which is the strategic goals across our industry. Our business units have their own priorities, yet the shared common goals across scalable compute, the speed of innovation, and the need for transformation. Last but not least, that transformation to be able to move, we need to have adopt cloud agility into everything we are doing. So our strategy is about bringing these 44 parts of the puzzle into one cohesive, forward-looking plan. Our product development cycle can take at least a year or so, and this is based on the complexity and scope of product. It is split into two main phases, front end and back end, and each of these phases has different needs in terms of compute and storage. Front end is about massive regression, a smaller memory file, a single threaded, while backend is about much longer jobs requiring significant memory, and it is multi-threaded. So we are leveraging. On different types of instances from so workload placement is critical to enable iteration, faster iteration of projects and efficiency, and the right sizing instances to fit this workload needs is as important. So cloud agility in summary is a cornerstone in our arms strategy and product development. But it is not only about workload placement. The cloud native approach mandates us to tune our infrastructure to unlock the full potential of computing there. So we build platforms that enable to run different jobs and leverage on the power of the cloud. These platforms were built with the 4 elements taken into account. The first one is fit in purpose platform, which is about the right platform with the right job with efficient data and storage management. Automation plays a crucial role in complexity management. The goal is to increase reliability, reduce costs, and remove human bottlenecks. Third is observability. It is providing real-time insights for failure analysis, cost control, but also any any any fault tolerance there. The goal is really to keep the resource usage optimal. Last but not least, the continuous integration, continuous delivery, which is about streamline application development and release features fast and reliably. So, in summary, it is not only where the app are running, but how they are optimized and built that unlock the full potential of computing. So, I will zoom now on two platforms that we build with the Cloud Native approach. The first one is Cloud Runner. Cloud Runner is optimized for front-end jobs, mainly. Uh Cloud Runner is operating really as a factory. It is leveraging on AWS batch scheduler. So ARM and AWS worked hand in hand to improve the cloud, uh the, the, that AWS batch into the leading. Service it is today. So cloud Runner in a few numbers enables, so we have 5000 concurrent jobs running in parallel, 100 projects executing, 1000 engineers working on those projects. The environment can be ready within 1 hour and we can support the security and export control technology. So this is on top is leveraging on spot, which is benefiting from 90% savings plants because these jobs can be interrupted, and we also leverage on 70% cost reduction in terms of storage as these jobs actually. are running without any shared, they don't require a shared memory. So this is what a factory means in terms of front end and it is enabling us to increase productivity of engineering. So cloud Foundation platform, which is our CFP, uh, it is optimized for back-end jobs and it is uh redefining how we, we see CFP as another uh by making its functionality completely equivalent to another HPC. It is using LSF as a schedule, scheduler and FA6 netApp on top as a storage system. So this is operating exactly like on-prem. Your user identification is using the LDAP system. We can have accesses using Unix permission. We also user can connect to on-prem to run to access GTT repository, but also we can run in batch or interactive using Wii command or interactive line. The main difference here is the elasticity of the cloud, as we no longer need, we no longer need to reserve from day 1 100 for instance terabytes of storage storage. We can start by 5 terabytes. And then scale up as needed. So this is a hybrid environment that enables us to benefit from the security and performance of HPC combined with the elasticity and the performance of the cloud. So, IWS Cloud provi uh help providing flexibility needed to, to run our job. It provides uh the both compute and the VCPU peak. It's enabled to address VCPU peak as well as well as the storage capacity. It is not only a technology shift, it is a transformation of how we build, deliver, and execute our projects. The number of projects we run now in parallel shifted our industry from Limited to Human Limited. So we, we can see there that we could have we could have 4000 slots on prem for 4 months. Now we can have 5 times that within 1 month. So this is a good example for fountain verification where we we we can see the elasticity of the cloud. We can scale up uh to the to burst to our peak, then scale down to reduce costs. We also see that we can benefit from up to 66% of added throughput once we move to the cloud. We saw that the number of projects that we can run in parallel now we can build actually in the space of four years, moved from 15 products to 25 products as of today. OK. I spoke about the cloud agility and how it is a cornerstone in why we are building our products, but the optimization part is critical here. Cloud using the resources in the most efficient way. Is as important. So I will zoom about 3 areas which are the saving plants, the tooling side, and also the storage side. Of different savings plans to enable to save costs for the products we are building, so sport can be used as a way as a type of instances for interruptable jobs. Saving plans like instant savings plans. Mainly about predictable jobs and compute saving plans is mainly for evolving types of jobs. This is an example of how we can stack up this saving model by starting by saving plants, and competing on demand for securing the slots and then move any job that can be interrupted to spot. That's very important and this is how we do it on AM. We can see that we can use we are using a mixed and balanced type of savings plan. It is very important to monitor how we are using these slots. This is an example of one region where it is Oregon. We saw that we are basically having the Capacity massively increasing, but no coverage with so we decided to move some of the jobs on to save costs, and this is important. It is all about timely decisions that we make in this space to enable efficiency and to run the jobs in the most optimal way. Electronic design automation. My colleagues spoke about it. It is critical for our development, yet they can have different behaviors and different needs, and that can impact directly the performance of the jobs we are running, even for the same. So it is very important to look at the characteristics like run time as well as kernel tuning to enable best usage of this tooling and can be some other tools on your side. So understanding how these tools react is very important to optimize both costs at one time. As we can see, this is for front-time jobs, and the bit of choice that EC2 instances AWS providing is there. So it is critical to adapt the tooling to the right instances here. The workload can be very spiky and rewrite performance can vary dynamically, so it is important here as well to monitor the usage, how this workload reacts, mainly for EDI workload. They can have different characteristics again, front end can be limited by metadata. Backend activity can be limited by data throughput, so it is important to have that elasticity for the storage system, and FS 6 NetApp is enabling that scalability, and it is providing very compelling results when it comes to type of workload and the storage systems that go with it. Graviton impacts of front end graviton can have we can see that it is very performant in terms of dollar and also performance at run time. We can see how it is overperforming versus AMD and Intel, and this is compared. Actually Graviton 3 and 4 versus other other instances, other providers, so it is, it is definitely very critical and mainly for our front-end verification where it is providing this great results. In summary, cloud native approach is really critical and enabling us to deliver products in the most efficient way. Efficiency, looking at efficiency is very important. Profiling and benchmarking of the tooling play a crucial role in how we manage that complexity and also optimize costs and runtime. And gravitation instances is really uh providing very compelling and uh advantage for for for our environment and our van. So our cloud journey started back in before 2020. It is work in progress. We need to reinvent the way we are doing and we are optimizing our resources there to unlock the full potential of computing. Thank you. All right, I'm Taylor Gowen. I am the team lead of the atmospheric and modeling engineering group at DTN. Um, at DTN our mission is to provide decision grade weather intelligence data to our customers that have a material business risk. This includes agriculture, uh, aviation, utilities, energy trading, offshore, and so much more. Today I'm gonna tell you a story about how DTN went from an on-prem computing company to a fully cloud-based, uh, system driven by HPC at AWS. Um, we're also gonna talk about how, um, our forecast system is fully integrated with HPC and how we're pushing the boundaries of numerical weather prediction with the advent of AI NWP and we're doing that in production in AWS. So I kind of wanted to get a pulse check before we get onto the um presentation. How many of you, if you show of hands, are familiar with numerical weather prediction? OK. Now how many of you have a weather app on your phone? OK, so that's everyone. OK, so that's output from a numerical weather prediction model. So whether you knew it or not, you are all familiar with numerical weather prediction. What a lot of you probably don't know is that increases in skill in numerical weather prediction have been directly tied to increases in skill and efficiency in HPC. With every new iteration of HPC, we're able to increase our computational power. This is allowing us to move from global course resolution forecast models to convection permitting scales and sub-kilometer forecasts. We're able to assimilate more high quality observations. These are billions of observations from things like satellites, aircrafts, buoys, um, covering the globe in places where we don't have traditional observations available. And then finally, of course it is giving us a better understanding of our atmosphere, so we're getting better physical parameterizations, we're getting better data assimilation, and we're understanding the processes that drive our weather every day. So as I said, I'm gonna be telling you the story of my company and how we are using HPC and AWS. We're gonna talk about where we've been, where we're going, and where we are right now. So for many decades we were using fully on-prem HPC. Um, this worked for us for a while, but then we needed the scaling and elasticity that HPC and AWS um, provides for us. So in 2020 we completed our migration to AWS HPC for both our atmospheric and marine modeling workloads. Um, in 2022 we were starting to push the boundaries of our price and performance with optimized HPC for specialized modeling, which we'll talk about more a little bit later. Um, in 2023 we moved to driving our entire DTN forecast system with AWS HPC and then just this year in 2025 we started running production AI NWP models in the cloud. And then looking forward to what we've got in the future, we're certainly going to be looking at new AWS offerings such as parallel computing service and the new HPC 8A. So some of the building blocks for our main workloads, uh, for the specialized modeling we use parallel cluster to run instances like HPC 6 and 7A. um, we use FSX for luster for our file IO and then we store our customer output in buckets um and. Three buckets and then move to Glacier as needed for our DTN forecast system. We also have everything containerized. We run FSX for Lester and S3, um, and then again you'll see common themes in production AINWP. We're running AWS batch. We are, um. Running instances such as GPU enabled EC2 instances like the G6E's, and we are storing those in S3 as well. OK, so the first, uh, work flow that I'm gonna talk to you about is our optimized HPC for specialized modeling. Um, so specialized model we run two of those in-house, they're computational fluid dynamics models, um, and they are the weather and research and forecasting model or WORF, and the model for prediction across scale. Or MAS we run these predominantly for utility customers who are trying to mitigate their wildfire risk and have to make decisions that affect their customers like public safety power shutoffs. So this has a big implication not only on their business but on the livelihood of their customers. So here's an idea of a workflow that we have within HPC. Um, this is for our wharf modeling. Everything lives in a single AWS region and a VPC. We have, uh, a net gateway and a public subnet that handles the egress and then within. A private subnet we have parallel cluster um handling our HPC 6 and 7A jobs um we use FSX for luster here as I said and then we store all of our data in um our client specific um beckets and so over the years we have done some work with AWS in order to optimize our price for performance, especially for these customized models and one of the biggest, um, successes that we had was working with, um. The AWS experts for HPC and um using a test case for Category 4 Hurricane Laura um back in 2022. So we tested scaling for uh MPASS, that model I was just talking about, um, for a simulation of Hurricane Laura. We used a 15 kilometer. Global mesh so covering the entire world and then a refined grid um at 3 kilometers over the Gulf of Mexico um we ran all kinds of tests where we scaled up from 32 to 128 instances and we found that the sweet spot for price per performance was 64 uh instances. This really taught us a lesson on how we can scale for other work flows and how we can um apply these learnings for the workflows that we do here at DTN. So next moving on to applying our learnings like I just said to uh our global forecast system. So the DTN forecast system is not just a single weather model, it's an intelligent blend of advanced models from global and regional agencies across the globe. We combine these models along with our proprietary models that we run in-house, such as those uh. Specialized custom models as well as the AINWP models that we'll talk about in a few minutes. Um, our DTN forecast system runs hourly out to 15 days. Um, it's a marine and atmospheric coupled model and we produce over 350 parameters every single day. This decision grade data is then sent to our not only our customers but our downstream applications at DTN that um create solutions for sectors like agriculture, utilities, aviation, offshore, and much more. So if we just want to take a look under the hood at the HPC that's driving the DTN forecast system, every single hour we run this forecast model. We take the latest available atmospheric and marine model input and blend it into our model. We compute statistics such as model bias variance to understand the skill of that model over the past month. We then apply model weighting using unsupervised machine learning to pick the models that are performing best at each grid cell for each parameter and each hour of lead time, so we're truly getting the best available forecast. Then we apply observation ramping for the first couple of hours of the forecast because we want to take in again the best, most recent available data that we have possible. Finally, we would go to the post processing uh and derivation step. So this makes sure that all of our parameters post blending, post statistical processing are consistent and physically reasonable. Um, we then create probabilistic outputs that uses that data here, and that's how we get our decision grade data that is sent to our customers every hour. So this is a tremendous amount of data. It's a high powered workflow that we have to run again, like I said, every single hour. And so this poses a challenge if we didn't have AWS and we had the elastic computing that allows us to scale up and work efficiently. Um, every single hour we run this forecast, um, it takes about 25 minutes to run a 15 day hourly global forecast to completion. Um, every single day we generate over 20 terabytes of data, which is a huge amount of forecast data. That's 20,000 gridded global files that are generated every day, um, and to accomplish this we run over 20,000 lambdas, um, and just for the marine modeling alone, I know I talk about the atmospheric side a little bit more because that's my specialty, um, the marine modeling alone consumes 1000 cores running on HPC 6A and 7A and produces over 300 gigabytes of data just in itself. So I know some of the buzzwords this year have been uh AI and how it fits into every sector of technology um and really numerical weather prediction is no different. We are now looking to uh enhance and improve our models using AI wherever we can, and the AI revolution in numerical weather prediction is truly here. Um, this has really been driven by big tech startups, um, and everyone in the industry coming together to see what we can do to improve the weather forecasting across the globe. These numerical weather prediction models are trained on over 50 years of data, so we use reanalysis data, um, from different centers around the world like the ECMWF or the European Center for Mid-rangeange Weather Forecasting, um, and we train those models to get an accurate representation of the atmosphere in the forecast. These AI models are not only trained on decades of data, but they are run at orders of magnitude lower compute costs. The AI models, as you can see in this plot on the left, actually show a clear step function jump in skill compared to its traditional physics-based counterpart. The solid lines here are the 500 millibar geopotential height skill from the ECMWF, which is kind of the gold standard of weather prediction. And then um you can see around 2022 there's a dashed line with the corresponding colors that represent the skill in the ECMWF's AI model skill for the same parameter. So you can see that not only is it fast, it's efficient, but it's improving in skill as well. So with that being said, we do run now in production an AI NWP model on AWS. We run a model called ForecastNet which was developed by Nvidia. It's got a 25 degree global resolution and it's trained on that, um, European reanalysis, uh, version 5. So we run this in AWS and it's fast and it's efficient and we run it for less than $9 a day. Um, we run this and it's, you know, less than the cost of two coffees or if you were making a huge mistake like I did and got a latte from Starbucks here, um, it's, it's less than one latte uh so this this lower compute cost allows for rapid testing and innovation within our. A AI NWP models um and like I said, it runs fast. A deterministic model run finishes in less than 10 minutes and then we run a 30 member ensemble, um, so that's essentially 30 different forecasts, and that compute time is only 15 minutes. So we've been working on this closely with our colleagues um at AWS and Nvidia. We just recently published a blog post um with AWS about how DTN accelerates operational NWP using Nvidia Earth 2 on AWS, and this is a, um, schematic of our workflow. So just giving you a. Idea of how this works, essentially we break it down into three steps. We have data prep, so AWS lambda functions will trigger that Python utilities container to format the initial conditions, which are the, um, I guess what you call the base state conditions that kick off the model run. And then we run model inference. Um, AWS batch deploys Nvidia Earth to Studio on GPU enabled EC2 instances, so in our case those G6E's, um, to execute the forecast net model inference, and then finally we get to the results, which is everybody's, um, what everybody's looking for. So the Python utilities container post processes the output. Um, to quantify the forecast uncertainty and to provide the latest weather data, and this information then goes into our DTN forecast system blend. So just a success story I wanted to share about how we are running these uh AI NWP models in production on HBC. Um, this was a forecast for Hurricane Melissa earlier in October. Um, this hurricane was very strong, hit the island of Jamaica, um, unfortunately caused very much destruction of life and property, um, but I wanted to highlight that this NWP AINWP model, um. That we are running, uh, showed a huge amount of promise in terms of the skill that it provided for this forecast. So this image on the right. It was 3 days of forecast 3 days in advance and it nearly nailed the the point of landfall and then the area of the strongest sustained winds and one thing that I do want to point out that even though I'm saying you know the skill is really good for these models, we're still finding ways that they can be improved. So for instance with hurricanes. AI and BP models tend to underdo the intensity and so we know that it's a known bias and so we want to make sure that we are using these tools in addition to our physics based models to, you know, get the best information out there and because of the elasticity and efficiencies available in HPC and AWS we're able to do that. So this is the same model runs for the same forecast except here we're running our ensembles so we have a 30 member ensemble and what was really neat here again this ran in less than 15 minutes this 30 member ensemble you can see that there is skill in it. Ensemble lines are converging, which means there's more certainty within the forecast, and it really showed the correct track of the hurricane over the island of Jamaica and even showed that sharp right turn that it made just as it was nearing landfall. So again, um, you want to take these models with a grain of salt. They are really powerful. They are fast, um, and we can really only run them because of our our resources that we have with AWS HPC and then we can combine them like I said with our with our physical models. So just going on to what's next for us, I've shown you where we were, where we are, and now where we're going as I mentioned, we're looking into AWS parallel computing service. This is really gonna be um able to be applied to most of our current work flows that we have right now. Um, it's gonna it's gonna allow us to transition off of that parallel cluster system that we're using to the next gen fully managed AWS platforms. We're going to move into more advanced AINWP right now we're just running an AINWP model, but our next step is to actually train one, and we're only gonna be able to do that using the high powered GPU instances from AWS. And then finally we wanna add value added customer models to our suite. We wanna leverage our DTN customer data with pre-trained models and be able to apply them to our customers who have very niche use cases like utilities, fuels, and wildfire. Alright, I wanna thank you all for coming to the session. I know you had a lot of other simultaneous sessions going on, so I appreciate your time and attention, and I'll hand it back. Thank you. Thank you, Taylor. Thank you. Fascinating. Well, everybody, I think we're in the final stretch. I wanted to do a quick recap. So today during the session we talked about the market trends in HBC. Um, you see a lot of growth continuing to consume a lot of infrastructure, innovate on the services offered. Cloud adoption is growing faster than on-premise, uh, growth, and that's not a surprise for all the reasons that we covered in this talk. Um, AWS is continuing to build optimize HPC building blocks, but also general purpose building blocks. Uh, so the set of options that you're gonna have in 206 and the years following are going to continue to grow. Um, we had a few cross industry use cases that I covered really quickly, uh, but then the amazing presentations from Karima and Taylor here talking about ARM and ETN's journey on AWS, hopefully inspiring for some of you, hopefully some ideas, uh, and look, everything. is a journey you have so many options that you want to go and engage with AWS specialists. We're here to help. I did want to bring up a couple additional items that were just announced but didn't really have a lot of coverage. One is. We continue to invest but we continue to invest thinking about all the customers that we don't always take care of. This was announced a few days ago. We are making a $50 billion investment. I know it's $15 billion but it's over 10 years, so it's a bit less per year. Uh, to, to go and build AI and supercomputing infrastructure for US government agencies, and for for countries or agencies outside the US, if we are able to build this for the US agency, you can only imagine that we'd be able to do the same around the world. Um, that was really important for me to to mention because it didn't get a lot of coverage and the second thing is that in Saint Louis two weeks ago. Uh, AWS received for the 8th year in a row the award for best HPC cloud Platform, so I know it's a bit of a brag, but, uh, I didn't want to put that as the last slide. Uh, it is an independent survey that is run by HPC Wire, the authority in HPC and AI, uh, publications, uh, and this is the 8th year. We're continuing to improve. We're continuing to add value because of all of you customers on AWS. We get better every year. So with that, remember I told you you're going to have QR codes. This QR code will point you to additional sessions. If you like this session, by the way, fill out the survey in the app, provide feedback. We get better every year thanks to all the feedback, and I just want to close by saying thank you to all of you that showed up today for this presentation. Special thanks to Karima and Taylor for joining me on stage. Enjoy the rest of the reinvent this year and hopefully we'll see you again next year.