---
video_id: 2vPqiKVh8yI
video_url: https://www.youtube.com/watch?v=2vPqiKVh8yI
is_generated: False
is_translatable: True
---

All right. Looks like a crowd. Wow. Yeah, nice, love the energy from the 3 people that are probably done with their day and have to deal with 1 more person talking about AI, of course. All right, so I'll try to make it fun for the 3 people that are looking at me. Um, I will not be the first person you've heard this from. I will most definitely not be the last person you've heard this from, but 2025 was the year of agents, um, and I would say probably the next decade will be the year of agents. So at Fireworks AI we've helped. Um, a ton of companies right across all different use cases to build agents in production that includes coding agents with a lot of the companies when you walk around here they're using us document agents, um, that process, you know, banking information, um. Legal information, all of these different things, sales and marketing agents that are doing outbound, that are doing inbound, hiring agents that are doing, um, recruiting automatically, customer service agents that are increasing CSAT, reducing average handling time, you name it, right, across all of the industries that you can imagine retail, insurance, finance, education, life science, security, etc. right? And you're saying, OK, well everybody's saying, you know, they build agents, but the reality is a lot of people don't talk about how hard it is to build agents, right? So it sounds really sexy agents are really cool. Everybody loves what talking about agents. What people don't really talk about is how difficult it can be to build agents for specific business use cases, right? So you have all of these pro possible errors that can happen all. This possible failure modes that can happen across multiple layers of the stack, right? So just starting here, like one is, are you gonna use a closed source model or an open source model? If you're using an open source model, are you using a small model, maybe an 8B, a 5B model, or are you using a large model like Kimmy or Deepeek that are in the trillion of parameters, right? Once you pick your selection and you get your model for let's say it's a search use case, what about the latency, right? You try to. Deploy it you want your search to be below 1 2nd. You need your LLM to be below 300 milliseconds, right? And putting that at scale when you have millions of users, that's really freaking hard, right? Um, and then of course quality is king, right? You need to ensure that the accuracy and the quality is the highest it can be once you have all of that set out, let's say you use the closed source provider. You start to launch and then you see holy cow my costs are absolutely ballooning right? so you kinda got across the 1st 3 but now you can't really support it because it's too expensive, right? And then of course if you wanna do open source models then there's a lot of infra complexity involved, right? Are you running an EKS cluster, ECS cluster? How are you deploying across multiple, um, nodes with tens of GPUs, dozens of GPUs, right? Of course there's data privacy. Compliance security availability do you have the ML expertise I can go on and on, right? And you're probably getting bored because there's just like so many of these errors now what I wanna convince you about today is that at Fireworks we are basically the one stop shop, the AI platform inference and customization engine that kinda lets you forget about all of these and really build what we call, I know it's a little bit corny, but magical AI experiences, so. What is a fireworks platform, right? As I said, we are an open source inference and customization engine for open source models at the very top layer we make it really, really easy for developers to use any of the top open source models with day one access, right? So that includes Deep seek models actually Deep Seek released the model today and it's gonna be available on fireworks today. Um, Kimmi models, Lama, Mistral, Queen, you name it, right? Not only LLMs, also vision language models, also voice and ASR models, right? It's incredibly easy to get started, so we are compatible with the OpenAI SDK Python library. So if you're using OpenAI, it's literally two lines of code that you have to switch. One is the model selection, and then two, you just need an API key for fireworks, right? Once you, once the. Developers basically find the proper model that they wanna run. Let's say they're doing a search use case, right? So they're running a really fast, um, 8B or smaller model. Then what happens very likely is that once they want to deploy that, then they want the quality to not only match closed source providers but beat closed source providers, right? And they want to have that latency of let's say 300 milliseconds to be stable as they scale to millions and millions of users. That's where our customization engine, um, fire optimizer comes in, right? So we do, and I'll, I'll talk, I'll talk about it in detail in the next slide, but we do two types of customization. One is workload optimization. That means if it's a search use case, very different deployment options that if you were to do an agentic AI use case, right, like some of our partners that are around here in the area. Um, and then the second one is you want to fine tune models, right? So I have this, this saying that like generic closed source models are built for everyone but optimized for no one, right? They're great, you know, they're 1 inch deep, miles wide, they can do a ton of different things, but if you really push them on a very specific use case they might have a lot of failure and I have a, I have a saying that at fireworks we. Basically have our clients build models with their data and then we optimize for them, right, as I say like you don't one of our um co-founders used the phrase that I like a lot which means which is uh you don't need a bazooka to kill a mosquito, right? And that's exactly what fireworks provides instead of bazooka you need something very specific to kill that mosquito to actually hit it right. That is around the fine tuning and the RFT reinforcement learning fine tuning to really push that quality above the closed source providers and then finally, of course, how do you scale, right? When you're scaling to millions of users, tens of millions of users, you want the platform to be super, super reliable. You want the SLAs to be top notch and you want, uh, to have, you know, the highest reliability there is. We're able to do this because of our phenomenal partners of course of AWS um as well as Nvidia and AMD. We have a virtual cloud infrastructure with dozens of regions across the world where all of the companies basically run their um open source models on. So that is a high level right of our platform now for the few people that are listening to me and are actually interested in the nitty gritty, I'm gonna jump into it. So as I talked about, right? I talked about customization and how we customize across two different levers, right? One is the workload optimization. So I don't know how many of there's literally 5 or 6 people, but a show of hands who took like ML 101. No? No one? OK, a couple. You, you remember grid search, right? Hyperparameter tuning, all that good stuff, right? Um. We computed essentially the space that you have in order to do a deployment across quality, speed, and cost dimensions, which means like specta coder models, the hardware that you're using, the different, uh, execution modes, the kernel options, etc. and it's around 84,000 parameters, right? Uh, that is a very, very large space that no one wants to run through. So what we've developed is our proprietary technology which is called fire optimizer where essentially. Clients work with us and they give us their what we call workload profile so they'll tell us, hey, I have a search use case. I want the latency to be below 200 milliseconds, um, and I'm gonna scale to a throughput of let's say 200 queries per second, right? Um, from that we basically run it through our stack and we identify what the optimal setup is for them to run and we create a personalized configuration deployment with that open source models now. Now why we've been able to support all of these different use cases from the very, very low latency to the super complex agentic AI workloads with huge parameters is that we've built our stack from the ground up so we've optimized for every single layer in our stack that starts at the lowest layer which is the actual Kuda kernels running in the GPUs, right? So we have a custom Kuda kernels called fire attention and then as you move up on the stack. How you deploy across multiple nodes, right? You have to deploy, let's say a Kimi Akimi, which is a 1 trillion parameter model. You need maybe 16, maybe 24 GPUs. How you deploy that is not, is not, uh, trivial at all, right? So we have disaggregated inference, which means you're separating the prefill from the generation, and we make it incredibly easy for clients to kind of forget of all this, and we'll just manage it all for them, right? And then at the very, very high level, which is the model layer, right? So all of these methods they're used across the industry I'd say what we do is we just use them way better um and that is some methods like speculative decoding, right? So for you that don't know what spec decoding is essentially you have a very large model you pair it with a very small model that's actually generating the tokens and then the large model would accept or reject the tokens right? If the acceptance rate is high, then your latency can come much lower if the acceptance rate. Is low then it's actually counterproductive and the latency is worse, right? So we train our own draft or spec decoder models in order to push the acceptance rates way above 70% which makes it so that you can get latencies below 100 milliseconds for certain use cases. So that is the workload optimization part, right? All about latency, cost, scalability. The other part is really around fine tuning, right? I use the, the maybe very shitty but useful metaphor of you don't need a bazooka to kill a mosquito and at fireworks we really think that. Um, company's moat is in owning their own era, their own AI, right? Like the model is their IP, the model is their product, and the way they do it is by using their subject matter expertise, their data to fine tune models. So we have, uh, a fine tuning platform that makes it really, really easy for developers to use either supervised fine tuning for, let's say intent classification, um, sentiment analysis, uh, product catalog cleansing, which is essentially you have. Images, uh, and then you're tagging them whether it's gender whether it's shoes, whether it's you know the different categories, uh, jeans basically the only thing that's coming to my mind is what I'm wearing jackets shirts, etc. right? um so that supervised fine tuning and then we just recently released as well reinforcement fine tuning essentially there instead of actually having the data set and putting, you know, the prompt and then the response that you want the model for you're writing what is called an evaluator function that scores. A model between 0 and 1, so the model will become better as it learns through its environment, right? We've seen incredible success with reinforcement fine tuning both on coding agents as well as a very recent client success story with our partners from Genpark where they use RFT to move from one of the cloud models to an open source fireworks model and they they had around like a 20% increase in quality while reducing latent. Seeing costs. So what we do essentially is we wanna make it really simple for companies to use their expertise through this data flywheel so that they can fine tune really quickly, deploy really quickly, and whenever there's a new model drop, they can just switch the new model, fine tune it again, and deploy again, right? as well for everybody that took their ML 101, there's covariance drift and data drift and things change, right? So we make it really fast for companies to just iterate, uh, through. The data flywheel we have a couple of examples here, right? VLM fine tuning and how we use, uh, a close an open source model, a coin model fine tune it on the specific data set and you see that we don't only match the closed source provider but we beat it, right? And then the same with RFT with reinforcer link fine tuning. This is for a text to SQL. Use case same right? we're not we're not only match but we beat the closed source provider so again really it's all about owning your AI uh the model is your product, the model is your AIP and a company's real moat is in their own data and in their own expertise and how they put that into their AI models for their specific use cases. Now why am I talking about this on uh AWS reinvent? Well, because we've built our entire stack on top of AWS reinvent, so, or sorry, on top of AWS, not on top of AWS reinvent. So essentially we, we use EC2, EAAS, and EKS and we're just. An inference and optimization layer that runs right on top of it, right? So the fire optimizer, the fine tuning that I described, as well as the incredibly fast inference engine, it's just this software layer that runs on top of AWS. Now we work with companies from. You know, Gen AI startups that move incredibly fast and they use our SAS platform all the way to legacy companies that have to be incredibly compliant, incredibly secure in terms of data privacy, and that's why we've developed these different deployment options on top of AWS, right? So from our SAS platform where quote unquote it's, you know, the least private even though we do have zero data retention, etc. but it's all in our environment, right? And then all the way to fully air gapped. So if you wanna deploy fireworks in a fully air gapped environment, meaning absolutely nothing leaves your VPC, then we can do that through deploying through a Kubernetes cluster or as well as Sagemaker and then everything in between, right? So we can deploy with AWS Private link where the network is secure, um, or as well through non-air gap BYOC where we basically just have what we call a control plane, right? OK, now I'm gonna talk in the last 5 minutes that I have about some client success stories so that you guys, you know, kind of believe me and all the BS that I'm saying now. One example, and of course I'm gonna start with one that I can't use, uh, the logo, but a very large grocery delivery platform. They basically use, uh, us to fine tune a very small LA 10B8B model, um. In a search use case, right, so this is the, these are the types of uses that require incredibly low latency. They're handling around 2 to 3 million daily queries of what they call um. Ambiguous queries, right? So let's say that you're using, you know, Uber Eats or Instacart or DoorDash or whatever it is like sometimes users will put something that is very vague, right? and you want to make it so that an LLM, a very fast LLM, can do things like query expansion. It'll rewrite it, maybe it'll tag it, etc. They use fireworks. To fine tune that model, um, reduce the search the search support tickets by 50% and run everything at 300 milliseconds or less, and that's not only, you know, technical targets, but also they actually ran an AM B test and they saw that they increased uplift once they moved to fireworks. Another example is Notion. They're a great, a great partner of us. If you guys are a Notion user, I'm a huge Notion user. A lot of Notion, yeah, good stuff. OK. If you love Notion AI, then you'll love Fireworks AI. A lot of Notion AI runs, uh, on Fireworks AI. They've also made use of fine tuning, um, and very small, very small models for very fast inference. So they actually moved from a closed source provider to us. They saw around a 4x lower latency to 500 milliseconds and below, and they've scaled, you know. To 100 million users plus, right? So again, incredibly low latency at scale while increasing quality through fine tuning. And then the last one I'll talk about is DoorDash, right? So I talked a little bit about VLMs, vision language models. We don't only have text models, uh, we also have VLMs and we have a lot of, I'd say it's a growing field, a lot of interest from clients in these use cases. One very interesting. One is this product catalog cleansing, right? So these companies have millions and millions of images they wanna basically label them as I said, right, jeans, shorts, pants, etc. um, automatically and to a high degree of fidelity and accuracy. What they've done is, for example, DoorDash, they fine tuned an open source model, um, that is running 3x faster than the closed source model that they were using before while increasing quality with fine tuning. And reducing the speed, the, the cost by 10%. So all in all, right, we provide basically a one stop shop for anything that you need to build we call AI, uh, magical AI applications to really match. Beat closed source provider quality while keeping latency incredibly low and cost very controlled um thank you very much if you have any questions we're somewhere around there don't tell me what the what the number of our of our uh. Area is because I have terrible working memory but you will find us somewhere around there and if not I'll walk around here and I can tell you um and yeah happy to answer any questions and hopefully I'll meet the 10 people that listened to me and hopefully I wasn't super boring so anyways thank you so much.