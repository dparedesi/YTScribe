---
video_id: ggfWAg4RnGg
video_url: https://www.youtube.com/watch?v=ggfWAg4RnGg
is_generated: False
is_translatable: True
---

Thanks for having me, Peter. I'm incredibly 
excited to be here today. So excited. In fact,   I wanted to watch all of Peter's past keynotes to 
prepare. But to be honest, I've been really busy   running 12 Labs and needed help. So, I decided 
to have our models watch the keynotes for me.   I indexed over 12 hours of Peter's keynotes from 
the last eight years and asked our models to   analyze them to pick out the key insights to learn 
more about him. First, I asked what are Peter's   uh favorite topics to talk about. That's a lot. 
And I learned really quickly that these are his   favorites. Then I searched for the clips 
where Peter is talking about these topics   and it was really easy for me to find and watch 
specific clips from each of his keynotes. Now,   what started as a fun way to do my research 
was actually a perfect example of what 12   Labs does every day, turning hours of 
video into structured intelligence.   We're living in a world where about 90% 
of data today is unstructured and most of   that by far comes from video. But even though 
video is the biggest source of data, it's also   incredibly complex. It combines visuals, audio, 
movement, and most importantly, time. Now combine   that across pabytes of footage in industries 
like media, entertainment, law enforcement,   and enterprises of every kind. That's millions and 
millions of hours of video. Did you know that 1   million hours of video is equivalent to 114 years 
if you watch this straight through? The scale is   absolutely staggering. And finding meaning in it, 
finding the moments that matter is even harder.   At 12 Labs, we're building foundation models 
that understand video the way humans do. Not   as a sequence of frames or transcripts, 
but as a unified story across site, sound,   and time. Our models, Moringo and Pegasus, power 
some of the heaviest multipetabytes scale video   AI workloads in the world, running inference 
on millions of hours of video. Morango is our   multimodal embedding model that powers precise 
video search and retrieval. Our latest version,   Morango 3, allows you to find the moments that 
matter across your massive archives using any   to any modality search. Pegasus, our video 
language model, turns video into insights.   It performs deep analysis and excels at 
generating text like summaries, captions,   or metadata to power downstream video workflows. 
Now, we like to say 12 Labs was born on AWS.   From the beginning, the AWS startup programs and 
credits literally changed our trajectory. Those   credits didn't just help us train models, 
they gave us the momentum to productize our   research. Running our stack on AWS shortens the 
distance between innovation and deployment for us. The backbone of our data infrastructure is on 
Amazon S3. When a customer sends requests to   one of our model APIs on the 12 laps platform, 
the the system seamlessly ingests, indexes,   and embeds video at pedabyte scale without 
ever moving their data out of S3. Now,   with as capable as our models are, they 
require equally competent vector storage.   What unlocked innovation for our Morango model was 
the integration of Amazon S3 vectors to deliver an   optimized video intelligence offering at scale. 
Now vectors are at the heart of what we do.   Each scene, audio segment, text Q, and video gets 
encoded by Moringo into a multi-dimensional vector   embedding that captures semantic meaning across 
space and time. Now what does that look like?   Let's assume for a single hour of video that's 
thousands of vectors. Now, but we're talking   about customers processing millions of hours 
of video. That's billions of embeddings. Whoa. These embeddings flow directly into 
S3 vector indices. No data migration,   no rearchitecting the infrastructure. The same 
S3 buckets already storing the source video now   store the embeddings that make it searchable with 
the same durability and scalability guarantee.   For example, when a user types a natural 
language query like people watching a space   shuttle take off from a distance, the query gets 
embedded by Moringo into the same vector space.   S3 vectors then performs an approximate 
nearest neighbor search across billions   of stored embeddings and returns video results 
with metadata like video IDs and timestamps that   pinpoint the the moment that matter, the moment 
that match the query. From there, our customers   can take these video results and use them to 
power downstream video tools and workflows.   Now, one of our customers, ARXP, is a perfect 
example. An extension of the Washington Post,   they provide a media management platform powering 
news organizations around the world. Built on S3,   ARXP enables editorial teams to not only store 
and manage their massive archives, but now they   can personalize the stories they create with it. 
Now, a single video or article can be transformed   into tailored variations of for different 
audiences. They leverage 12 labs models to   quickly analyze and enrich archived video content 
and discover related clips to build new stories. That is what accessibility means to us. Not 
just APIs, but infrastructure on AWS that   makes video intelligence viable at any scale. 
Our partnership with S3 enables us to deliver   an incredibly efficient product offering 
directly to our customers infrastructure.   Now given the scale of the data we 
handle that has a meaningful impact   on our customers unit economics 
which unlocks new possibilities. I started by telling you I used our own models to 
prepare for this keynote. That's not just a demo   trick. For decades video has been a ride only 
medium. But now we capture everything. Every   game every meeting every moment but we couldn't 
find it. learn from it or build on it. Today,   that's changed. We're turning the 
world's video into knowledge that's   actually usable. Every organization has their 
own version of Peter's keynotes. Institutional   knowledge trapped in video. Footage that 
could teach you something if you could   only ask it a question. And now you can. We 
can't wait to see what you built. Thank you.