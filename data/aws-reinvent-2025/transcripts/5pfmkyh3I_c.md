---
video_id: 5pfmkyh3I_c
video_url: https://www.youtube.com/watch?v=5pfmkyh3I_c
is_generated: False
is_translatable: True
---

Thank you all for. Attending a session That talks about unstructured data. And different data modalities, and how can you build a strong foundations for AI. We're very excited about this session. We're excited to share some of your experience in the real world as well. And we couldn't be more thrilled uh to be here with you. With me on the stage, I have Avinashradi Iupaka, uh, who's our, um. Speaker from customer, uh, side, he's, uh, a leading, uh, principal, uh, engineering at Bear. I also have Shiv Narayan. He's our product lead for Stage Maker Unified Studio. Uh, throughout our session, uh, we're gonna have be looking at multiple demos as well as, um, some of the real world examples. So thank you, Avinash and, uh, and Shiv. So I just want to start by asking how's everyone doing? I know it's a speaker, I cannot hear you and all that, but it's all good. Thumbs up is good. So what we have done today is we have structured our conversation and story in these 5 or 4 different sections. We're gonna talk about a very important topic called data readiness for AI, right? We are talking about unstructured data today, uh, among other modalities, but it is important for us to align and land on this foundation that talks about how you build your data platform which is ready for agentic and AI and workflows and things like that. We're also gonna look at different data modalities. So my name is Namneet. I lead the data and analytics for AWS for Life Sciences customers. So most of my examples are coming from the life sciences world. Having said that, um, a lot of my other partners and colleagues in the organization work with different industries. I see a lot of similarities we're gonna talk about that too. Then we're gonna look at the look in the demos for Amazon Stage Maker unified studio, Amazon Stage maker governance that how can you build a platform that can serve not only structured but unstructured data and finally we're gonna hear from Avinash on the uh the real world examples or real world use cases of of buyer. Um, I wanna start by this slide, uh, which I'm sure you all are very familiar with, not maybe the slide, but the concept, right? So as you know that, um, a lot of interest and a lot of work that is being done in building generative AI and AI applications and agents and agenttic AI applications. Uh, this is a 40,000 ft view maybe, of course we're not drilling down, but if you look at this pyramid at the bottom of all this is the data foundations, right? And data foundation is something which is evolving and changing with the need and, uh, the, you know, the, the kind of, uh, modalities that we have and also the type of agents that we are writing. So to write an AI application, to write an agentic application, you need to rely on the data foundations. And today we're gonna talk about how you build that, how, uh, what are some of the best practices, what can you use on AWS as an example. So again we're gonna start with this today. So a few years back there was a paradigm in data, uh, called Data mesh. Have you heard of that? Some of you might have heard of Data mesh. And it was an interesting paradigm actually very, very, uh, timely at that point because that was the first time when someone was building a paradigm shift on data that involves people process and governance, not that the governance wasn't done before, it was done before. There were MDMs, there were data governance leads and things like that, but the distribution of data assets across your organization was something, uh, that data mesh was targeting along with people process and technology. In the last couple of years we have seen the same paradigm shift on the generative AI side, right? So keep that data mesh concept in mind. I'll try to merge them together here because how can you leverage your existing platform in actually serving up your agents. So we started from traditional AIML. This is an era where you're writing neural nets, deep learning models, machine learning models, statistical analysis, regressions, you name it, and most of the uses there was you have a notebook or an art studio or something like that. Uh, which is enabling your data scientists to run analysis and, and this is the typical. Now then came the generative AI. It's a large language model building summarization, building notes, and building other sort of content that can be leveraged for your, for your business. But with the introduction of generative AI, uh, based workload, uh, there's another thing that was very much important along with the data is the context, right? That's where you have technologies that rag and stuff. Lately we are seeing a lot of interest, uh, and a lot of work spaces, uh, workloads that customers running building agents, which is basically in my opinion, a unit of work that gets a specific, uh, business metric or a specific work done for you, and we're looking at multi-agent collaboration now so it's not only one agent, but it's a combination of agent with orchestration with chain of thought reasoning and all those things that is helping you building a workload that serve up for your business. And there are several examples in this space. Now what is important when you think about building data and you think about building these agents that the one common theme between the two is that how are you utilizing your data platform to serve up these agents and how that is any different from what you were doing previously, as you see in this slide at the top we have the foundation model, the models that you would be utilizing to build this, those agents, right? But that alone is not sufficient. You need enterprise knowledge at the bottom, as it says in the bottom of the slide. So think about the shift in the data, uh, paradigm shift with the data mesh, and think about the paradigm shift in the Gen AI which is in this slide we're calling it multi-agent collaboration or a mesh. So data is no longer the only product that you're sharing across the organization. You're also sharing agents and, and, and other things. So AI is only as smart as your data like we all understand that and data can be a different type. This is a life sciences example. In your organization you may be having your existing data, uh, that is your relational databases, your warehouses, your, your S3 data, your, your S3 tables or one of our partner solutions. Uh, this data typically is largely structured and also unstructured, but largely what we have seen mainly is that you have, you have rows, columns, some sort of a parquet document, all the stuff stored raw, but you can, you can, uh, query that the new data that you're creating, and this is where we're seeing a lot of data, uh, which is getting created in an unstructured way. In my example, uh, and then Avinash will talk about that in the, uh, later also that a lot of data coming from these instruments, the lab instruments, a lot of it are coming from the, your genomics information and gene information. And other verticals too, um, like social networking, your clinical notes, or images, PDFs, and things like that. All the data that you're creating is largely unstructured along with not, now you have to work with both. You cannot leave the structure data. You have to make sure that your, your guidance and your implementation has both of those things. And then the third party data. This is what you acquire from other, other partners, other sources, and things like that. So how you combine these different data assets and spread that in a data mesh-like setting with the right uh pillars uh so that you can serve up your agentic platform as well as the as as the human usage and things like that is an example of a multimodal data science. Now, even though you're not, say for example if you're not from the life sciences healthcare industry, it still is very much relevant for you at the at the center of this diagram we have somebody called a patient, right? A patient is someone who has certain data records like admissions, discharges, labs, you know, conditions, procedures. We all have that, right? We've all been to, uh, different healthcare facilities and we have this data now if you note that that you have the EHR data which is largely structured other than the clinical notes and the summaries, but you also have the clinical notes, the dictation in physician room or a vaccination data or a social geographic data. This comprises of a multi-modality in any organization, right? So you wanna make sure that as you look at this example is that identify the modalities in your specific lines of business and identify the different type of modality and then create a common model that can help you govern the data that can help, help you build the agents and things like that. On the right hand side of the patient have the OMICS data. Now this also has a large volume like for example microbiome or. Uh, or metabolism or, or, or, or the proteome and all this data, uh, that you have is, is largely unstructured, uh, and requires a great deal of analysis, uh, that, that you need. Now, you might ask that I've been building my data agents. I've been building my data platform for a number of years now. My data is, is very good. Uh, I don't have to do anything else. What is that I need to do to make my data unstructured and structured data, something like what we've seen in previous slide. Uh, ready for the AI now this is a very small example on the left you see, which is the current state, which is a state of data scientists, data engineer engineers interacting with the data as you can see, a researcher investigating drug interaction knows where to start a database, a molecular database that can then go and understand the significance of specific protein bindings and things like that. So they start with a very narrow focus approach. And they know exactly what they're looking for now. You may be having in the right best possible way, the right quality, the right access, and things like that, and they're good with that. The challenge that it doesn't scale, right? So if, if you scale that to hundreds and thousands of data scientists, if you scale it to hundreds and thousands of different data models, chances are you need something that can help you scale, which brings us back on the other side, which is that, hey, I can write agents that can go against this data and really scale it further. What is the challenge there? It starts with speculative exploration, right, because you're pretty much throwing every single data elements to those agents and then they're doing some, for example, in this case I have an agent is tasked with analyzing clinical trial effectiveness, uh, might initially request all the available trial data and then further filter it down. So what is the right balance, and the reason it's important for us to note now more than ever is that when you think about the right balance between how much you want to provide to the agents, uh, and how much you don't want to provide to the agents, the data modality plays a big role for unstructured data, a lot of data, something that an agent can, can bring the value out of it. For structured, you may be requiring a far more governance control and, and things like that. So we have these building blocks that you think about again at a high level. We, my, my colleagues will go deeper into some of these topics, but if you look at this one, we are saying that how you build data ready for AI using 3 building blocks, right? The first building blocks is contextual information. So how do you create a data ready for AI by bringing the right context to the data. OK, the right context means several things. It could mean your enterprise knowledge, your ontologies, your metadata, your business metadata, and, and things like that. Uh, the second thing is that you need your data platform is the tool provisioning. This is how someone is going to be accessing your data. Now remember Data mesh? They had this concept called, uh, endpoints, data product endpoints. So data product can be, uh, called using an API or a SQL or something like that, right? Now you wanna make sure that your data platform is ready to be accessible by different toolings and resources, something like an MCP server, something like another agent, and, and things like that. And then finally, uh, the maturity. Now this is a bit tricky because data maturity for structured data means different things for unstructured means different things, but it doesn't matter to your data scientists like they want data to be to be fully matured. So as you think about modalities and, and, and, and unstructured data, I would highly recommend that look through these three different pillars. Now just a show of hands and um how many of you are actually uh utilizing your business metadata as the context information for your data assets. So if you and and and I can tell you that the metadata is one of the the largest um wealth of information outside of your data that can provide a lot of information for your agents to work uh for the, you know, for for your outcomes, which is my next slide that if you, if you look at the metadata a few years back it was largely treated as documentation like I remember sitting with my team working on metadata building data dictionary that kinda explains what data is documentation pretty much, right? You go through multiple Word docs. You go through multiple data dictionary, not that it was bad, very useful, right, but it was largely a documentation, whereas now in the contextual world, and by the way, uh, the data context metadata is just one of the contexts. There are other contexts depending on the industry that you work with. Uh, now in the GI era, metadata is is actually an intelligent amplifier, right? So you can use metadata for business context, your glossaries or enterpress knowledge, something the data steward can work with. And the relationship discovery across domains as an example and of course a real time trust. So think about your metadata and it's even more important for your unstructured data because structured data I can still query and make some sense out of it, right? Or I may be having at least basic information of the columns that that it has, but for unstructured it is largely unknown for me until I actually tap inside the file and see what is going on. So therefore it is more relevant for you to think about a contextual information. So you build your data platform, have those three pillars in the context, start with business metadata. So how do you get your unstructured data ready for for AI, right? So on, on this screen you see that you have different types of uh sources, uh, some video transcription is coming, some plain documentation is coming. The two things, uh, that I would like you to take as an outcome of this talk, uh, of course it's a complex topic, you know, as you start working, I'm not overly simplifying any of it, but the two areas that we'll be focusing a lot today is one that how do you. Build a unified governance of your data irrespective of its type and modality, right? Even if it is structured or unstructured, your data needs to be treated the same way and so as the agents, for example, right? And the second thing is specifically for unstructured data, how do you use some of the capabilities that we have in Amazon Bedrock. And Amazon, uh, uh, you know, Bedrock Agent Core and Amazon Sagemaker that you can utilize to build insights from the data. Remember, governance is first, building insight is second. So those are the two areas that we uniquely focus today in regards to unstructured data. But as you can see, they could be structured plus unstructured here as well. And of course in most cases, like in my patient's example, you have both of them. And so it's, it's important for you to, to understand that how you can, you can merge the experience there as an example. So I'm gonna talk a little bit on the data funding layer, uh, which is something that very much linked with what I presented earlier on the three topics. So look at the bottom of this layer we call it data readiness for AI, right? This is where you have the data foundations, the structured, unstructured interoperability and things like that. This is how you store your data. Best performance cost modality. If I have a data in Iceberg, I would store that in S3 tables as an example. If I have data unstructured data, store that in S3 general purpose bucket. If I have warehouse data or structured, I'll store that in RDS or red chip warehouse or something like that. If I have unstructured, I'll just simply store that in my S3 buckets and then utilize. But however we store the data shouldn't guide the architecture upstream, right? It is just the storage at the end of the day. What guides the architecture upstream is that how you interact with the data. That's where the interoperability is very handy. So it's something like iceberg compatible data as as an example. On top of that, to have your data ready for AI, you have context, tools, and maturity. Remember those three pillars. Context is extremely important for your agents to work with your data. Without that, it's going to be speculative exploration for the most part, right? And context is not only business metadata. There are things like clinical ontologies in my world, in your world, could be other ontologies and enterprise knowledge and things like that. You have that platform, then you start building agents, so we have. Uh, agents in my case, healthcare life sciences agents, but you may be having some agentsies specific industry, we have, uh, you know, uh, offering there which you can use the accelerators that that our team has built. You have agentic workflow. You have data accelerators, all that you put in there. Then you build a governance layer, flare principle. Everybody knows what fair is, I'm assuming findable, accessible, interoperable, reusable. L is the lineage, uh, which is, which is also something that we've added as part of the, and it's interesting to know that even lineage can act as a context to your data, how the provenance, the transformation and things like that. Then after that you build an experience layer. This is your end users like you as a developer. Uh, as an IT governance lead, you're building all this platform and then you build experience layer with analytics, uh, with, uh, visualization and with chat. There's, those are the typical three what I've seen lately SQL analysis, notebooks, uh, or the chat agent experience. How do you do that on AWS? You have Amazon Sage make a Lake House. There's how you store the data, um, structured, unstructured, federated into existing sources and things like that. Uh, you have the, uh, context of the ontology meta metadata knowledge graph. Your tooling is MCP servers, APIs, and of course the maturity is the quality. Um, there are others, uh, of course, uh, maturity principle that you can follow. Amazon Bedrock and Agent Core for running your building your agents and running your operationalizing production. Uh, Amazonage maker catalog, uh, a unified governance model through which you can unify your governance across any type of modalities of data, including unstructured data. Uh, and then at the top you have StageMaker Unified Studio for building your experience. Uh, you have Amazon SageMaker notebooks and data agents to accelerate your development using AI and then your Amazon QuickSeet for you to build a chat agent experience, uh, for your customers. Um, really quickly, uh, I have the sample use case, uh, for digital labs. It is very healthcare specific. The one thing I just want to call out, call here is that on the left hand side you see there are different type of instruments from where the data is coming out, right? And like I said, every industry has something very specific, uh, to how they're collecting data. In my case, it's coming through that. All that part is not that super important. What is important is that it's getting stored in the storage layer. Remember the previous slide where I showed you the data can be structured unstructured. Focus on the storage, how you stores the best. Don't store your unstructured data in a relation database. It's not a good idea. It's, it's not a cost effective solution, right? Then we have the lineage metadata catalog and the governance built, right? So just those three pieces start small, think big, start small. Don't you have to refactor everything. Your environment currently is good enough for you to build these components starting now. Uh, then you have the semantic layer, Extremely important, the semantic, which is an extended concept there of context. What are the semantics for my own organizations, right? In my case it might be something like what is a brain tumor, what is a diabetes, and what is something like a carcinoma. In your world it could be something else. The point here is that you build a semantic layer and then you build all these consumption models. So in a cloud environment, store your data effectively. Build the right context and then go go on from there. Uh, so some of the challenges that I want to call out before I, I call upon Shiv, uh, the challenge in processing unstructured data. So far we talked about, you know, the approach of data foundation and what is important for readiness and why unstructured data needs to be treated very similar way, but, uh, it should also, uh, shouldn't drive your architectural decision drastically. What are some of the challenges, uh, governance, of course, the governance of unstructured data is very different. At the bottom level, it shouldn't be different at the top level, so the findability that access provisioning, the model and tool selection is a challenge too, where you have the selecting optimal solutions and and modality use cases. Manual pro there's a lot of manual processing for unstructured data, uh, like data extraction, tuning, and, and parameters and things like that. And finally, the orchestration, managing multiple models for unstructured data is, is not easy. It's a challenge. Uh, output integration is a challenge and things like that. So how do we address these challenges that can span across the governance of data. And the uh the usage of it for that I'll call upon my colleague Shiff to talk about the next portion where we'll see some of the Sage maker capabilities to address these challenges. Shiff. All right, hey folks, hopefully you'll be able to hear me. Great. So my name is Shiv Narainan. I'm a product manager on the Sage Maker team, the NextGen Sage Maker. Um, so there are two services, Sage Maker AI, which is the machine learning and artificial intelligence. The NextGen Sage Maker puts everything together. How many of you use SageMaker, uh, today? Awesome. I wanted to actually put up my picture, but then I figured I might as well just put somebody who's from Adobe, you know, um. Uh, so today, my goal is, uh, to kind of expand upon what Navneet said and show you a bunch of demos on Sagemaker, um, Unified Studio. Um, so this is Doctor Smith, and we're gonna be walking through a journey. Doctor Smith is a neurooncologist treating brain cancer, and we're gonna help him prepare for a meeting tomorrow with one of his patients, and one of the patients he's gonna be meeting is a 25 year old person, a female who has been recently diagnosed with a neurological cancer. And uh The meeting tomorrow is super important for Dr. Smith because that's the first time that she's actually meeting with Dr. Smith. And the most important thing that Doctor Smith wants to do in that meeting is to instill hope. And the way you do that in the most effective way is to tell the patient that I've actually seen patients like you. And I have you have hope simply because we've actually done this over and over and there is really light at the end of the tunnel, but what he's he's actually doing right now is actually to stare at those documents, the unstructured data, right? And in those documents one of the couple of things that he's really scared of, like how you probably might be also scared of is. The amount of incredibly insightful information that's scattered across and this data changes every single document he's he's, he changes. This is actually the reality of healthcare today. And more importantly there is demographics data there is medications, there is uh procedures is treatments, but there's also sensitive data. Now it's super important for us to be able to provide Doctor Smith the required tools. In order to make sure that he gains access to this data, you the, the capabilities to use the latest generation technologies like large language models to help process parse this and eventually help him prepare for this meeting, and that's what I'm gonna do now. We as a team are gonna actually go through that to see how to actually help him prepare for that meeting. So in order to do this. The most important thing is that obviously you have to have the right AWS services to do it and the end of the day, our goal is to actually build him a chat agent. And this chat agent needs to be built with all the right governance controls in place and traditionally it was also a little difficult in AWS to do this simply because while we have all the breadth of the services you have to hop on to different consoles. So what is the innovation that we've done in the past year to simplify these problems? With that, I wanted to talk a little bit about the team that I'm part of and very proud of the Next Generation Sagemaker. So this is actually your center for data analytics and AI. Sagemaker is comprised of three important components. The first one is the lake house architecture where you actually store all these documents that you saw. The next one is a catalog. The ability for you to make sure that you actually take all these unstructured and structured documents, catalog them, annotate them, provide the visibility of data quality metrics, associate glossary terms, and you have a unified studio that allows customers to build applications all the way from simple data engineering pipelines to extremely complicated machine learning models and also chat agents with ease all without you having to switch different consoles. So let's, before we dive into uh a demo of how we do this, let's just walk through a little bit more details about the catalog first. How many of you use the SageMaker catalog today? Cool. Very good. So what's different about the Sagemaker catalog is, first of all, it's actually built on Data zone's business that data catalog. So that's, that's the, that's it, it, it, it really is built on a very scalable model to begin with. The second important thing about the catalog is it's just not like a catalog for structured data assets. It's a, it's a catalog for your unstructured data assets. It's a catalog for your business intelligence assets, so it's really much more broader and we continue to enhance the number of types of objects that we support on that. You can also understand the quality metrics. You can understand the data lineage from that catalog. You can also understand the ability for you to search and you can subscribe to these assets and gain permissions to data in an auditable and compliant way. So that is very unique about the the the permissioning model that has been integrated with the SageMaker catalog. In addition to it, there are other components that I'll be talking through my talk in terms of the guardrails and the responsible AI aspects that you'll actually see in a demo also. So the catalog is is an extremely important component for Doctor Smith and his team because first, as a first step, what we want to do is to be able to take all this unstructured data, catalog them, make them discoverable, provide the right guardrails so that he can actually, him and his team can actually get the right access controls. The next part is, how do you build your applications? So that's where we have the unified studio. So unified studio gives you the ability in a single console to build all required apps that is needed for your data engineering work to the AI work. For example, if you're a visual person, if you're like more into visual ETL development, we have a visual ETL that is available for you. If you're more of a code and code-based data engineering, you have notebooks that is embedded in it. Um, you wanna build machine learning models. We have foundational models. We have the, the, the Sagemaker AI models that are available in a model hub that you can actually take them, train them, experiment them with ML flow, and then deploy them. And if you're a G AI developer, we have the ability for you to build chat applications and prompts, the necessary guardrails, all of them together. So that is the unified studio. Now what we're gonna do is actually watch a demo and I'm gonna walk you through this and as a first step to help Doctor Smith prepare for his meeting, we're going to take all this unstructured data that's already available in S3, catalog them, and make sure that it's actually discoverable. That's the step number 1 and here's how we're gonna do it. Uh, before that, let me just quickly walk you through an important launch that we did a couple of months ago, uh, which is the ability for you to actually catalog these unstructured assets. With this you can connect to S3. Either to a bucket or a folder. Take those assets, provide business context, publish those to the data catalog, and then make it discoverable. So with that, let's just watch this demo. So I'm showing you the unified studio and in here what we're gonna do is to go to the data tab of that project and as you can see here I have my lake house which is structured assets and then I have buckets. I'm gonna go ahead and connect to my S3 location, provide a name for the data lake, provide a description, specify the URL where the medication records are stored, provide a role, and then we are going to hit. The ability to connect. So what this does in the back end this is actually going to create a connection to S3, and then it is actually going to first bring in all the assets that you actually watched in the previous screen which Doctor Smith was actually staring at. So these are all the unstructured data that you're actually seeing, and these are all the medications that you were, you, you, you just watched in that. So as a next step, what I'm gonna do is actually to for for brevity of this demo I'm just gonna go ahead and catalog the entire bucket. You can actually catalog catalog individual folders, but what we're gonna do is to add some business metadata to the structured assets. So this is the screen where you can actually add a whole lot of business metadata to begin with what I'm gonna do is to start adding context in terms of the read me file, and this is what Navni talked about the context that is needed for the generative AI purposes, right? So I've added what exactly are these documents? I've provided detailed descriptions, the type of diagnosis, the medications that's there. I'm also associating glossary terms so in the catalog you have the ability to create a glossary in a hierarchical way and then associate terms to these. This allows you to search and it also provides meaningful associations when your other customers are who are actually working on Sagemaker to understand and discover these assets pretty easily. So I'm going to go through a variety of terms and then select them and associate them for these assets. So now that we are happy with adding all those assets, let's go ahead and click yes and you also have the ability to extend this metadata with additional concepts called metadata forms. I'm not gonna do that to keep this demo short and let's go ahead and publish this asset. So when you publish this asset so long you've been actually just kind of working in your own local copy, so, so to speak when you publish this, this now is discoverable by anybody and everybody, not the necessarily the data, but they can actually view the metadata. So now I just want to go to the catalog, make sure that I've actually published this right, and I, and you can actually browse through the assets and you can see um the type of assets, the glossary terms that I associated, very easy for me now to basically say give me the patient data, give me the cancer diagnosis data. I can go through and look at different asset types you might have seen like the different types of assets that we support there. We also, um, uh, provide a couple of other capabilities and extensions for this. It looks really great from my perspective of how I've published metadata. I also have lineage lineage, but this is basic data lineage at this point in time, um, because we just have started to work and as lineage builds up, it's gonna be built up pretty well in this. So what I've done so far is taken that assets that was just in S3, that was not discoverable by anybody, and I've made them discoverable at this point in time for people to come in and start working. So next, we're gonna have a generative AI developer come in and do some work with this data, but before that, let's just look at a few more concepts. Sage Maker Unified Studio is one of the services that brings all the structured and unstructured data together and allows you to build knowledge bases with ease, so we provide the ability in one single place for you to come in and source structured and unstructured data and then easily build knowledge bases for your GII applications. How many of you have built a knowledge base, uh, in Bedrock? Cool. So one of the things, and I, I think like all of us know Nani talked adequately about rags and what, what, what, what's the usefulness of rags. These are the concepts or these are the objects that make your foundational model more intelligent and your organization aware, right? So by using a knowledge base what you can do is to enhance your existing large language models to provide more context of your organizational data. Um, and, and what we from a SageMaker Unified studio and SageMaker as a whole do is to allow for you to create these knowledge bases with ease without having to navigate through multiple APIs with just simply with a few click of buttons, and I'm gonna show you how you can actually do that. Now it's one thing to have a knowledge base, but the most important aspect is even governing to make sure these models are grounded, to make sure that it's not hallucinating, to make sure that it is not being hateful, to make sure that it is actually providing the context or or or the the right data by looking at the data that you have provided. So for this, Bedrock offers guard rails. With guardrails, what you can do is it comes out of the box with a bunch of filters that you can apply to make sure that your models are grounded, but in addition, you can also extend them. With ease with simple natural language to make sure that your prompts are, uh, the, the, the, the result of the, uh, large language models that you're using is appropriate. So for instance, a couple of things that you could do is, for instance, you could say don't show PII information or don't show like, you know, um, I, I don't want you to basically be always grounded with facts and you, you can, you can do a lot with, with, with uh guard rails. And what is unique about Sagemaker's integration with Guardrails is the fact that it, it allows you to basically take your data from your catalog, build that knowledge base, build that guardrails all in one single unified console, and then be able to deploy these to your business users with ease. And that exactly is what we're going to do next. So we have a generative AI developer. Remember we just cataloged as a data producer all these assets in our catalog as a generative AI developer, John comes in, just searches for a term diagnosis. This is a very common term that occurs for many people. They just come and search and immediately they're able to discover that asset. When they discovered, they're able to see the metadata. They're able to see the glossary terms that they've associated. And with a lot of confidence and discussions with their colleagues, this is the right data set that we have to build our knowledge base. So one thing that John does here is he's gonna ask for access to that data and this is the unstructured data access that he is requesting for. So he has requested for that access and he can actually view that that he's actually requested for access. Now the data producer gets this as a notification that oh a generative AI developer actually has asked me access. Now this is at this point in time he can click, talk to the legal teams, make sure that they understand what the use case is for this particular asset, and then approve or reject this particular access. So in this case after discussions with legal and other uh compliance organizations they are approving to make sure that they can actually go ahead and build a chat application or to feed this into a knowledge base. So now We've gotten John access to that data. This is great. So John goes into his project. Goes to the data tab and now confirms that he now has access to that S3 buckets, which is great, he now has access. So the next natural step is to go ahead and start building the chat application and I remember I talked about two concepts, the knowledge base and guardrails, and that's exactly what we're gonna build. Look at how easy it is to build that with unified studio. You just go in there and create a knowledge base. It provides the name of the knowledge base. And what differentiates things here is the fact that you can now use the data sources that you have gotten access directly from here. So I just got access to my 3 locations. I just provide that. I'm selecting the embeddings I need. Sagemaker Unified Studio takes care of all the other required components like making sure that it creates a vector database and Amazon OpenSearch, which is a serverless offering here. And make sure that it indexes all the data and creates that knowledge base for you without you having to do that heavy lifting. It's just a few simple clicks. So once the knowledge base is created, the next step for us is actually to go ahead and create the guardrails because this is the governance application that we need to make sure that, you know, it's, it's, it's, uh, we're, we're applying the right governance controls and as you can see out of the box there is a whole bunch of filters that allow you to control the um the responses of the knowledge base but you can also add your custom. Uh, messages or, or custom prompts to make sure that you're applying your own governance controls. In this case, I don't want anyone here to prompt for address information, the, the demographic. So I'm gonna go ahead and create a deny address and I'm gonna create a definition for it. So I'm just basically saying like don't respond if there's patient uh address information. You can provide some sample phrases that's optional for you. And we're just gonna go ahead and create this guard rail. So once the guardrail is created, once the knowledge base is created, it's now time for us to create the chat app. And again, it's not too difficult. You simply go to the my apps, create a chat app. Um, and here we provide, uh, a name for the chat app. In this case we'll just say it's patient 360. And uh we select the foundational model we now provide the knowledge base that we want to augment this foundation model which is pretty straightforward because we created the knowledge base of the previous step we basically select that we can apply multiple guard rails so we'll just go ahead and apply that specific guard rail here and. What we're gonna do is to save this chat app and. So We'll just go ahead and save. And next step is that now that I've been working on my local copy in my project, the most important step is to actually go ahead and deploy it so that it's, it's basically visible to others and I'm in a position that I can actually start sharing this with others so we'll just go ahead and uh provide a name for it or the alias name that represents that certain version uh or any changes to that version because it's the first time I'm just deploying it. I'm just gonna provide the right uh description for this app. And we just simply deployed that app. It is a pretty straightforward as you can see in a few steps we basically took the um uh data, got access and basically got built a knowledge base and next step we can actually start sharing this. We're gonna share this with Doctor Smith, however, you can also share it with a group of individuals pretty straightforward in terms of how you can share this and then we just went ahead and published this app. So in this demo what we have done is we've shown you like how easy it is for a Gen AI developer to take assets in a very secure compliant manner in an auditable way and take that and create a knowledge base out of it, apply the right governance and guardrails and then deploy it using SageMaker Unified Studio. So now is the time When Doctor Smith comes in after his rounds, he got an email saying, look, the app is ready to, you know, to share. You can start asking the questions, so he doesn't click on the link. Instead he goes and discovers that app, and the app is there. The first thing that always prompts us is like to test the app, right? Like, oh, are you compliant? So let's go ahead and say like, hey, give me the address of a patient who I know, uh, Owen Anderson. Uh, can I get the address? The guard rails kicks in. It says sorry, I can't give you that information. Enforcing that governance controls. The next thing is I'm happy as Dr. Smith to see that it's actually a chat application that's compliant, but I know of a patient. Let me make sure that I can see if that patient actually has that information that I, I can validate, right? Like, is the data right? So he's basically asked like, OK, tell me something about Owen Anderson. The chat app is able to use the knowledge base we created and rightly summarize that information in a very clean way. The next thing, getting more confident, he asks another question. How many patients are there with brain cancer? He uses just a very simple term, not even like any anything like oligodendroglioma, not none of that, just a simple brain cancer. The chat app with the help of large language model is intelligent enough to understand the context with all the metadata that we have provided and provide like really solid responses interacting with a physician, a neuro-oncologist, which is I think pretty cool. And now he basically says like look, I have a meeting tomorrow with a 24 year old patient. She's a female. Find me patients like her and tell me the number of comparable patients. What is the histology of those patients? Tell me like what kind of treatments they've gone through and just be grounded with facts. Don't hallucinate, right? Like he's kind of prompting the system. And the model really behaves well. It basically gets that data, provides the necessary information. It says like, look, I found a patient similar to her. She's on this protocol and all kinds of information. Again, all of this is synthetic data that we manufactured. I can only imagine, you know, if you start using it on your real data, um, I'm sure you'll get pretty good results of this, but at this point, Dr. Smith is able to get. The required responses that um he wanted and he's ready to prepare for his meeting and he's ready to go for tomorrow. So I know we took a healthcare example. You may all be from different industries, but in every part of your journey there is Sagemaker there to help you help accelerate through that journey, and I'm hoping that after this talk you will go and you will try out Sagemaker Unified Studio, our catalog, and uh we just can't wait to hear how, uh, what you do with that. With that, uh, I wanted to invite, uh, our, uh, customer speaker Avinash from Behr to tell you how they're actually doing these things in real world, uh, at Behr, and, uh, thank you very much. Good afternoon, everyone. Can you guys hear me OK? Wonderful. Mahina Shriaka, I'm a principal engineering lead at Biopharmaceutical. Um, so Byer is a 150 year old life sciences organization that is driven by the mission of health for all, hunger for none. We operate across different life science vertical combining corrupt science, consumer health, and pharmaceutical. So, today I'm here to talk about one of the biggest challenge that we have in modern-day drug discovery, that is data fragmentation and contextualization. For decades The Search for new therapies has been a race against time, with massive investments pouring into R&D, but one of the biggest non-scientific challenge that we have is not the complexity of biology, but our ability to leverage existing data and accelerating AI. Data that's trapped in silos, slows down our scientists and brought down critical decision making. We Our mission, our team's mission is very straightforward. We are here to break down the silos, unlock speed, trust, and scale. When the data is logged in silos, we lose speed. When Compliance, lineage and consent are not clear, we lose trust, and without platform level patents, we cannot scale. So the vision that we have for our biodata science ecosystem is a platform built on the principles of distributed data mesh, along with analytical and AI workbenches, plus a central governance spine and a central catalog. We operate across vital data domains like Omics, clinical, chemistry. And We'd operate in a distributed data mesh architecture. Think of DSE as a supply chain fabric that connects data and AI across R&D. In a data mesh architecture, the ownership of these data domains is federated to the data owners. For example, the biomarket team owns the biomarket data. The Omics team owns the sequencing data, so on and so forth, while the central platform manages the. The interoperability and scalability of the platform. Our team Solves this cross-cutting concerns by providing turnkey solutions around data ops, ops, cost hygiene, observability, monitoring, and also validated reusable templates by removing all the infrastructural concerns that bog down our scientists where. Our practitioners like machine learning researchers, bioinformaticians, biostatisticians can focus 100% of their time on science and not service. And the glue that brings all of this together is Sagemka Unified Studio. Sagemka Unified Studio acts as a central governance control plane, a front door where. It enables all of the capabilities and enables these data domain owners to engineer and publish these assets into the catalog. These assets are unstructured S3 tables, structured S3 assets, analytical assets like machine learning features, machine learning models, inference end points, agents, so on and so forth. So The seamless integration where these distributed data domains kind of merge into a unified catalog is what enables the holy grail of R&D, which is multimodal analytics. So just to quickly, quick raise of hands, how many of you are dealing with multi-modal data at your organizations? Great, great. So, Search for new therapies is fundamentally a multimodal challenge. In order for us to understand a disease. Drug mechanism of action. And how a patient is responding, we cannot look at this data in isolation. So If you look at a critical domain like oncology, Approximately 80% of the data across key therapeutic areas is multimodal in nature. None of this would be possible if we have our data in silos, OK? Our internal biomarker archive, a terabyte scale of valuable study specific data, is a clear example. We're strapped in silos, hard to access, barely findable, and generally slowing down the boat. So the challenge we had at hand was to handle the scale. By still maintaining the regulatory compliance and dealing with this multimodal capabilities that we are planning to deliver. So the solution is a 3-step process. The first step is where we start by ingesting our on-prem biomarker data into S3 study specific buckets. Then We enable the trust layer because this data is GXP in nature. We enable automated Alcoa data controls, making this data attributable, legible, contemporaneous, original, and accurate, and we achieve this entirely by AWS native controls. Trust by using S3 object locking mechanism in compliance mode, we ensure immutability of this data, critical for regulatory compliance, security with S3 server-side encryption. With KMS we ensure our data is addressed by leveraging S3 access grants. We make sure that there is a tight boundary of authentication and authorization controls that are in place. Auditability by leveraging versioning and cloud trail, we make sure the technical lineage of data, how the data is accessed, is captured, which is critical, which is critical for GP data. The second step is where our automation engine picks up. So our automation engine gets into action by provisioning all the infrastructure peripherals and the components like Sagemika Unified Studio and its underlying components on the for the for the study projects. And then it starts off by creating an authentication and authorization boundary by leveraging the S3 access grants and directly mapping them to these projects. Third step. Is where we register the study specific unstructured S3 locations as assets into the catalog, immediately making it findable and self-describing. Through this process, we also capture multifaceted metadata, administrative metadata in terms of data ownership, data access. Licensing needs of the data and also domain and asset specific metadata with with rich business contextual metadata tags, creating a control layer, making this data ready for AI. We also then the third phase is where the closed loop execution comes into the picture. Now users start acting on this secured governed data. The AI consumers now consume this data from the central catalog in a secured, governed manner, and they will start engineering additional assets by leveraging this data. These assets can be cross-study fact tables by exploring dimensional data, which is critical for future concern management. AI Consumers can also produce additional analytical assets like machine learning models, machine learning features, agents, etc. And in turn all of these assets are published back into the catalog, so this powerful phenomena of closed loop execution where you consume and produce back into the same catalog really creates a lineage-driven closed loop execution which helps us answer questions. Not only who has access to this data, but what is part of this data packet. It helps us to track the journey of the data from a raw file, from a raw unstructured file to a model really solving the provenance concerns of a model. None of this would be possible, you know, if we were operating in a in a in a siloed environment. This truly unleashes the capability of AI and agentic AI in the space of in the space of R&D. To demonstrate this, I want to showcase. A real-time, a high value agentic AI use case that we are pursuing in the space of biomarker data engineering. So One of the challenges in early clinical trials is time to harmonize data. The ability for us to get data from our laboratory information management systems, from our clinical systems. Running it through data engineering pipelines, QCing data is a week-long effort, manual effort, critically impacting our go no go decisions. So we deployed an Agentic AI solution that's deployed on Bedrock which automates this biomarker data ETL process. So first, The agent starts off by gathering information from our laboratory information management system. Limbs Our clinical systems and our study specific data, then Next, it starts to parse this information. The agent leverages LLM, that is fine tuned on vector indexed on internal protocols and SOPs, and it starts to parse this data. And identify the patents. Second, 3rd, the agent not only acts, but you know it orchestrates the whole flow where it picks the right data engineering pipeline, biomarker data engineering pipeline, and engineers the product and QCs the metrics. The final step is a result of a strategic asset where we have a multi-dimensional. Biomarker DataCube that is further published into the catalog. Which is very valuable for further analysis. So as an outcome, Here we are able to remove a weeks long worth of manual effort. We are able to really accelerate the time to harmonize data for pharmacokinetic and pharmacodynamic analysis, in turn, accelerating our proof of mechanism. And Go no go decisions, which is very, which is very critical for clinical trial success. We are also leveraging patient data in real time, understanding the drug response, improving our success of clinic clinical trials. So this closed loop architecture. Really provides us the much needed value unlocking that we do in terms of trust, speed, and scale. Beyond the quantitative benefits, this architectural shift also delivers, you know, quantifiable business value in terms of capturing multifaceted metadata, technical metadata guaranteed by S3 versioning and cloud trail logs and closed loop execution. We are exactly able to track the journey of the data and exactly answer who has access to this data. With rich business metadata tags and Contextual information that we are capturing plus the cross study fact tables, we are able to answer which data is actually used for this analysis. Let's say in case of a consent-based study, a patient pulls consent, we are able to, the system is able to determine exactly which part of the analysis is impacted, which is critical for consent-based reusability. Our roadmap is very clear. We intend to expand our ecosystem by onboarding more data domains, industrialize our AI use cases, and truly explore the potential of multimodal analytics by impacting our pipeline. Optimizing the cost and time that it takes for bringing Critical life saving therapies to our patients. In closing, Our journey of Migrating a 300 terabyte unstructured data. Which is stuck in an on-prem environment and turning into an AI ready strategic asset is not just a technology story, it is a story about empowering science. We have turned an asset that was barely accessible, barely findable, and accessible only through friction. And turned it into a strategic AI asset which could streamline our pipeline activities. It could improve efficiency of our R&D decision making. And, overall, laying a solid foundation for our future precision medicine activities. So with this, I thank you all for tuning in. So feel free to let us know your feedback. So we can improve and deliver more talks of this sort. Thank you very much.