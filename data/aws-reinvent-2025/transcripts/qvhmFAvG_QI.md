---
video_id: qvhmFAvG_QI
video_url: https://www.youtube.com/watch?v=qvhmFAvG_QI
is_generated: False
is_translatable: True
---

Good afternoon and welcome to the station. Today, I'm really excited to share an incredible transformation story that's reshaping how modern enterprise is gonna handle IT incidents. We will be exploring iHeart's agentic journey, and that demonstrates the transformative power of AI driven operations. Hello. Myself, Shuritahosh. I'm a senior Solutions architect at AWS and today with me, I have our esteemed guest from iHeartMedia. And before we dive deep, I would like them to get introduced. Hello, everybody. My name is Harish Naraj. I'm a VP of uh cloud Engineering for iHeartMedia. Excited to be here presenting our story. Saka Aita, uh, principal engineer at iHeartMedia. Same, uh, love to see you, I, I love seeing people here, uh, to, and I'm looking forward to presentation. Thank you. So as we move along, today, we'll learn about the challenge that not only impact a particular media industry, but broadly any large digital organization. And then we'll also understand the solution approaches for those challenges. And then, of course, we'll dive deep into the eHart story and understand the benefits and the results that they've got out of this agentic journey. We'll also share some of the uh practical implementation guide that might help you. And as you can imagine, it's an evolving journey. So, obviously, we'll be sharing some of the future coming things that we'll be doing with iHeartMedia, as well as sharing some critical lessons learned that might save a few weeks of development type in your own agentic journey. Uh, we will hold the questions till the end of the sessions. We'll be here. So if anybody would like to continue or have any specific questions, we'll be happy to do that after our sessions, based on the time, the permits. OK. So, let me dive deep into the problem that we're trying to solve here, right? So, in, in this particular case, just picture this. You are an on-call engineer, it's 3 a.m. in the morning, right? And suddenly, your pager device start, you know, buzzing, your phone starts ringing. Essentially, you know, you, the panic sets in, right? And probably your better half is giving you that look, while in the meantime, you are looking at something in a multi-screen and trying to figure out what is exactly wrong and why this pager suddenly, you know, rang. So this is what in a modern digital system looks like in a distributed, where finding a particular root cause is something like finding a needle in a haystack. Only the difference is the haystack is on fire and somebody keeps on adding more hay to it. So, let's try to understand a little more context and have some little more context on that particular problem. So, essentially, uh, right, while it is very important to unders you know, eliminate the toil for the support engineers and amplify human productivity, We need to understand what we are dealing with iHeartMedia. So iHeartMedia is not just any other media company. It is having 850+ AM and FM stations, and iHeartRadio reach almost a quarter of a billion people. It is one of the largest podcast platforms in the world, and the uptime expectation is 24/7. So, radio cannot be, you know, off at any point in time. So, anything in this kind of infrastructure breaks, we're not really talking about impacting few people. We are really talking about potentially millions of commuters losing their morning show. Or a major advertiser, the campaign gone dark, right? Interruptions in live streams can potentially impact, a lasting impact for an artist. And in terms of revenue, if you think about, potentially losing tens of thousands of dollars every minute. So to have to deal with this kind of scale, the operations have to evolve. It has to evolve to a more intelligent system that is capable of Self-diagnosis and remediation. Obviously, the repetitive manual task that the team is doing, that is eating up the important engineering effort that could have been better spent in terms of innovation. So, the goal is essentially to amplify the productivity and not just add more people to the whole process. So, let us take you through the steps, the current steps before we have this kind of agentic solution, right? So, what we, I call it, it's more like a 7, you know, circles of on-call hell. But it starts with the first, obviously, the alarm storm. As I said, your prejudge device starts ringing, your phone starts buzzing, essentially, you are just, you know, getting panicked, and then start the login marathon. You need to log into the VPN, you need to log into AWS console, a number of monitoring dashboards, and potentially SSH into production systems. Every such step take precious minutes into that particular incident situation, right? Once that's done, it eventually starts the information hunt, right? You are looking through the metrics, going through the logs that is never ending, and then trying to figure out what really went wrong. In the meantime, obviously, the tribal knowledge dependency comes into the picture. The obvious questions, when was the last time this service, who has worked on this? Is there an operating, uh, procedure already listed where we can find a solution for that? Eventually, you get into a manual diagnosis where you are running certain commands and checking service dependency, checking service health and things like that, right? But remember, all these things is taking time and the clock is still ticking. So, that is very critical at that moment. Eventually, when the manual investigation happens, then we'll go to the solution hunt, uh, solution hunt, and we'll run certain fixes, probably rolling back to the previous stable system and eventually solve the problem. But with that also comes the documentation debt in the sense The problem got solved, but the knowledge is remaining inside in somebody's head, and most likely on other, next time it happens, we'll start all over again. All this has business impact, and I'm not really talking about the obvious downturn, which is extremely painful enough, but the real hidden cost. It caused, you know, burnout engineers, the silos and the inconsistent response. And it seems like when everything is urgent, nothing is really urgent. It's what I call it like uh uh aliphatic syndrome. Now, traditional monitoring system, the challenge is it generates a lot of noise and processing through this noise is expensive. It doesn't generate the signal, and that's why we need an automated system that can help us to pinpoint those root causes in time and also give us recommendations. So, let's imagine a scenario in a similar incident, right? Instead of panicking and going through all the steps I just described in the next couple of minutes. The The on-call engineer, probably just ask an agent or type in a chatbot that what is wrong with my infrastructure or a particular, uh, you know, cluster. And while they do that, instantly, the agent system go through the logs. Correlate events and systems and understand what is the root cause and identify that and showcase what is the recommended options, right? All within 30 to 60 seconds. I'm not really talking about science fiction here. That is how the modern AI ops should look like for a large retail organization, and that's what we built in iHeartMedia. So, let's talk a little bit on the technology side, how we have built it, right? This is a reference architecture, as you can see. So, it's a multi-agent system that was built and broadly speaking, in the left-hand side, I call it like a trigger layer. Essentially, that is where Slack or pager duty or those kind of uh Services are integrated, that understands what the human or any system for that matter, trying to get out of that system. And the central portion is basically the orchestral platform, which is the brain of this particular platform. The purpose of this orchestration is essentially 41 is the intent recognition, that is understanding what the human or the system wants, and then contest assembly, getting relevant contextual data, task delegation, in the sense, understand what is the specialized subagent that is fit for that particular job, and from all this analysis, generate a response. So that is the important activities of this orchestration. In order to successfully work this agentic system, we also have another layer that is called data layer, and that has all the necessary data with respect to, like, the server logs or even the knowledge base that has the previous operations, uh, you know, knowledge, so that the system can actually recommend based on those knowledge. So, that's an overall architecture, but let me dive a little deeper on how that particular orchestration system works or the multi-agent system works. So, we have not really built uh one single monolithic AI agent that is trying to solve everything. Instead, what we did is uh specialized sub-agents who are specifically, you know, trained for a particular type of activity, and they're working together based on the incident and providing contextual solutions. So the first one is the orchestrated agent or SRE agent we might be referring in our discussion today. So that purpose is basically get that particular incident question and then delegate that task to a specific or identify the specific subagents, and then finally delegate those tasks to the subagents. And eventually, when the responses from the subagent comes, it synthesize the responses and send back to the response to the user. From a specialized subagent standpoint, we have multiple subagents. In this case, for example, there are subagents with respect to monitoring solutions that can go through certain monitoring informations. For example, if there is a performance anomalies, latency issues, things like that, and eventually it returns, uh, structured performance metrics to the orchestrator agent. At the same time, there are parallel log agents and kubertis agents, sub engines running. Now log agents are more optimized to run through millions of lines of code and give a fast response. The Kuberti's agent is giving you a pod-level details, like how the pod health is going on, if there is an issue with respect to service mesh and things like that. And of course, we also have a knowledge-based agent in the sense that has the previous operational history, so that based on the incident, it can provide potentially the recommended steps to bring the resolutions. So, that is these multiple sub-agents, and we'll dive deep very shortly and we'll show you how it works, right? So, that's the overall orchestration flow that was billed as a part of this agentic solution. Now, obviously, as you can imagine, and so on in the diagrams, so one of the critical component of this is Bedrock Agent core. So, we have used Bedrock Agent Core to leverage and deploy all these agents or the multi-agent systems that we have talked about. And specifically, I want to call out the bedlock runtime, which is providing developers a secure way to run and deploy your agents at a large scale. Now, obviously, we have also used bedrock memory, which is a critical component and and gateway to interact with other, uh, you know, APIs. From a runtime standpoint, it is important because it allows, uh, you know, to run multiple types of agents, and also it's the most long-running instances that we allow, like up to 8 hours, which is very much important for cases where you are actually doing a multi-agent orchestration. At the same time, uh, runtime also offers a complete session isolation, which is extremely important from a security standpoint. So, these are the key features that is being leveraged as a part of that. I'm sure it's a part of this reinvent you have, uh, you know, have gone through probably some of the agent court sessions, but I just want to call out because that's played a very key role in implementing this particular solution. And then the question is how it actually helped us in terms of agent core. Obviously, we are exploring that, uh, the solution, and at that point in time, we really wanted something that will help us to, you know, build that session, uh, build that particular solution quickly. And for that, we don't have to, you know, kind of get into a situation to manage the infrastructure or operational over it. So, Bedrock Agent Core provides us that runtime that is managed, and we don't have to manage the infrastructure. At the same time, it provides the flexibility in terms of the type of agents we want to run or the type of model that you want to access, which is very critical to have the right agent development. And of course, the third, the most important is the security. So, it is built on the same security principles like any other AWS services, and that's, I think it provides a seamless integration with AWS. So, those are the key differentiators that helped us to kind of leverage Agent Core and build a quick prototype and eventually, you know, iterate on top of it. Another question comes to how you even build an agent, right? And for that, this particular case, we have used strands agent as a framework to build this agent. There are, it's not that Bedrock agent course supports only strands. There are other frameworks also like QA or other standard frameworks. It's also supported. In our case, we have taken. Uh, strands, uh, AI because it is, first of all, it's open source and it gives developer tools to quickly and easily develop and also integrate with uh AWS services and have MCP supports. So these are some of the capabilities that we leveraged to make a scalable solution. So that's the reason of getting strands. But of course, in your case, probably you can, you know, leverage some other frameworks. So I think now we have understood that the problem we are trying to solve and the solution approach and the architecture, how we are trying to solve it, right? So probably it's a good segue for you guys to understand a little more about iHeart Media, the scale, the business, uh, you know, operations that they're running, so that you get the overall context when we start running through the uh demonstration. So, at this point in time, I'm gonna invite Harish to help us with that, uh, iHeart story. Thank you Sudeepha for setting the stage here, um. From the audience, I want to know how many of you are doing return to office on a regular basis. All right, we got some so picture this, right? It's a Monday morning you're driving to office, there's a lot of traffic and you you're stuck basically your mind is racing there's so many things going on you wanna do, um, but traffic isn't moving anywhere so what do you do at that time? You just tap on play when you play that you are now listening to your favorite podcaster now you're immersed in their world and you're no longer in a. Journey at this point, what exactly happened here? This is the power of audio. Audio is a very powerful medium where you don't need to completely dedicate yourself. You can run things in the back end and it will keep you engaged. That's the power of audio here. Why it is important is iHeart Media excels in delivering audio to the masses over the last two decades or so. So going a little bit deeper into what iHeart Media does, iHt Media is an iconic, um, mass media entertainment company based out of San Antonio, Texas. Uh, we have subsidiaries, uh, based in, um, Asia Pacific, uh, Europe, and many other, um, areas across the globe. And some of the things that iHeart Media really excels in and is a leader in is broadcast radio. About 900 stations, radio, uh, FM AM stations across the United States. If you turn on radio, it's likely, more than likely that you're listening to an iHeart or an iHeart partner radio station. That's the level of reach that iHeart has. And another big platform is the digital audio streaming platform where iHeart reaches masses and. In, in the digital audio platform, the major component is podcasting. Podcasting has taken over, over the last 5 to 6 years, and there is humongous demand for on-demand podcasting. Another thing that IH is really popular and in in culture is there are a lot of live events that iHeart is very famous for like Jingle Ball, iHeart Media, a music festival, um, and in addition to that, it's also a social platform and audio advertising. By, by this time you can start looking at what's the scale that we're dealing with at iHeart. Now one of the key factors that are shaping the business as it stands today in 2025, as I mentioned earlier, the digital platform is growing, the demand um for the on-demand, um, audio is ever growing, so that's, that's the number one growing sector, um, in the audio world. And you're no longer gonna listen to audio in one particular device. You can listen to audio from many, many different, um, uh, you know, streams and devices right now. So that is our scale and multi-platform reach that we have. And when you're dealing with such a scale, what's the first thing that you got to take care of? It's streamlining both the operations, both technology operations and business operations equally, right? That is one of our key factors that is shaping our business. And if you look at the overall technology trends, there is a lot of cross-platform integration going on and also there is user generated content. And above all, the, the most important topic here is the AI driven development and operations that we can optimize to better serve the uh the needs. Now let's look at what is iHeart Media technology today. Even though iHeart is an audio company, we are a, a very technologically focused company, and we have to serve, uh, uh, an ever growing demand. Um, most of it is on AWS. We are a completely AWS, uh, platform native solution, and we are always looking at how we can improvise on the cutting edge technology and how we can improve the efficiency. What is it that sets us apart for the next 10 years. And another important factor is we also integrate with many ecosystems such as TikTok, uh, that also brings in a lot of traffic. So this is iHeart MediaTech uh at a high level. And from cloud engineering and site reliability engineering we support three major verticals in iHeart. One is broadcast, which basically is the radio that you listen to, the station that you listen to on the radio. Now again, it might, you might be receiving the signals, uh, RF signals from a tower. But what is powering behind it is all in AWS. That particular broadcast vertical is completely built on serviceist architecture, multi, uh, region, and, uh, automatic failover. It's a very, very complicated beast in itself. So that is the, uh, broadcast vertical that we, that we support. And the second one is ad and sales tech, which is very important. That is actually the revenue generator for iHeart. And, um, you know, if you look at the overall radio um broadcast audio market, uh, iHeart garners about 40% of the ad spend that happens in the audio world. And last but not the least, is the digital. When I say digital, it's the iHeartRadio platform. We'll dig much deeper into it. Our, uh, specific use case here is centered around digital. And digital, if you look at it, we get more than 250 million users a month that are accessing the platform, and 90% of the US population is reached with the digital. Now, as I mentioned earlier, iHeart Digital is where you can listen to, uh, radio, podcasters, your favorite playlists, everything on iHeart.com, iOS, Android, and many, uh, many other options available. And if you look at the traffic, it's, we get about 5 billion to 7 billion requests per month, and we also host a lot of radio stations, uh, local sites across, uh, across the country, about 3,000+ radio stations. And as I mentioned earlier, iHeart is a leader in the, in the podcasting platform, um, so we get about 150 million podcast downloads a month. So we are among the top 10, um, podcast platforms. And if you look at at the peak time we get about 60,000 to 70,000 hits per second. That's the, uh, the scale of traffic that that we get on digital. In addition to that, as I mentioned earlier, you're not receiving um audio from one particular platform. You're looking at it coming from Alexa, Roku, Sonos. There are so many integrations that are already inbuilt with iHeartRadio. Now, let's look at the architecture. Uh, this is a 50,000 ft view of the architecture. If you look at it, we use about um 70+ AWS services across the board, and there is a primary, a secondary DNS failover um setup that is in place, and we have multi-level caching. We use CDN, external CDN, internal caching. It's a very complicated uh uh architecture. We also have multiple uh. Communities clusters, uh, one for backend, one for front-end, and many other, uh, communities clusters across the board. Along with, we use about many, many services in AWS including, uh, you know, some of the foundational, uh, services like S3, RDS, BPC, um, uh, and EC2. In addition to that, we also use SageMaker, uh, EMR, uh, AWS Transform, and many other, uh, cutting edge, um, uh, features as well. Now we also have a huge catalog of items, so there's a whole pipeline that takes care of ingesting the uh catalogs and making it audio ready to be served to the millions, right? So that's another aspect. So if you look at the architecture, uh, it's, it's pretty complicated. It's about 100+ microservices every which way you look at it, and it's completely running in, in AWS uh platform. Now, when you think of this scale of architecture, this is what you're going to look at. You don't know what happens when one thing breaks. It's hard to figure out uh what is the root cause and what exactly is a symptom. That's, that's often the case when there, when there are outages. It's very easy if it's a small subsystem where a user can log in and find the root cause and and figure out and come up with how to remediate it, but in our case we have nested distributed systems that talk to each other, so it's really hard to pinpoint where the issue is and how do we resolve it, especially when, when you are under pressure, when there's an outage going on, right? That is where we um AI comes into agentic AI comes into the picture here. Um, so, so let's look at how the journey started, right? So, um, uh, you know, last few years AI has been the, uh, the buzzword, and we were looking at how can we use agentic AI, uh, to support one of, uh, another important, uh, cause that we have is to optimize the AWS cost across our enterprise. We have close to 170 AWS accounts and it's really hard to even generate a report of, you know, what are the cost of. This particular account last month versus how do you compare where the growth is, where the costs are optimized or not so our initial foray into AI was how can we use agentic AI, make it easy for anybody to obtain that report. So let's say you go to a Slack, you use a Slack bot, you put a command. What was the cost of this particular account in November and it shows up right there, right? We tried that and it really worked well. And that that that gave that was more like a stepping stone for us to uh to move forward. Then we started thinking what's our biggest problem? Cost is definitely a concern uh we're gonna report and we're gonna find optimized ways, but the biggest headache that we have from a site level engineering perspective is, uh, you know, outages and handling the on call handling the, um, you know, SRE activities that happen on a daily basis. So that's where we started looking at, OK, let's pivot now from cost to, um, you know, looking at SRE implementations, how we can improve ourselves, make our own lives better when you are on the on call and you, you have to log into so many, um, uh, you know, portals and go through a lot of things to even find out where the issue is. We want to simplify that first. So that is what we started looking at in the, in the process we, uh, ended up, uh, reducing uh a lot of time. Now, going with um AI agentic AI journey, if you don't have a goal, is very hard. So what we went through was, first, our goal is to have an insights when there is an outage. The initial thing we want is a triage capability. Like we want to be able to understand which particular subsystem caused the issue, which is the root cause, and how did it percolate across other systems. So that is what we first focused on is getting the insights and get the intelligent triaging in place. Second, once you know the issue, then the next step is to see how you can remediate it. What is the recommendation, right? So that is the next step we, uh, we looked at how we can take this forward. And third, of course this is still in evolution is how do you go ahead and remediate it in production. There are a lot of gotchas with it. There are, um, pros and cons with it, but that is, uh, the way we are trying to transform our journey using, um, AI ops, specifically agentic AI. Now some of the benefits that we saw immediately as soon as we we got this going um is we reduced the response time. The response time in particular is more of a triage response time. Nobody is looking at, uh, going into communities clusters, sifting through the logs and figuring out what the issue is. It just throws you in a slack bot, uh, exactly what the issue is. It's very easy to, uh, get the triaging. And then it improves your uh operational efficiency and we reduce the tile at the end of the day. We don't want our on call engineers to get completely burnt out. That is the goal, right? Um, and then with with this we also have uh a level of consistency and reliability. A machine is always, uh, acts much better repeatedly than a human, so we are now transferring the responsibility. Into the machine and it gives you more reliability and the last but not the least is how do you preserve the knowledge? Let's say there was an outage in October and now there's a similar outage happening in November. Now you know exactly what was done in October to fix it. Now you could use the same knowledge and come up with a plan for November, right? So that that that that's how, uh, some of the benefits that we reaped out of it, um. Um, at this point, I want to, um, call upon Sirkan Hayak who will go through on how we implemented this. We've been talking about what iHeartMedia does and what are the challenges, how we solved it, but he's gonna dive deep into it. Thank you thank you. Thank you Arish. Uh, as Arish alluded, uh, we make sure millions of people, um. Uh, can listen to their favorite music, podcast and radio station, uh, all over the world, uh, whether on web, mobile, or smart devices, uh, people dependent on, uh, on us to, uh, provide this service. So when something breaks in the scale, it breaks, uh, at, at, at, at scale, and as you all know, production incidents have impeccable time. Assuming some of you go uh regular on call rotation, what I'm about to tell you might uh trigger some on call PTSD, so. Now imagine it's 3 a.m. in the morning, your phone goes berserk, you got out of the bed, um. Uh, with one eye open, you're in front of, uh, you get in front of your computer. Try to VPNN, uh, it takes 3 traps because, you know, 3 a.m. in the morning, your muscle memory doesn't work. Um, finally connect you need to access to your QTL. Uh, uh, you need to, uh, refresh your A AWS credentials. Classic, uh, then you need to access the logs. You need to open up a new relic tab, uh, Prometheus, maybe your metrics are in Prometheus, another tab. Uh, CDN stats and other tabs for FAS they collect them like Pokemons, right? By the time you got all your tabs open, you have 10 tabs and uh uh uh 10 terminals, and you already forgotten about what the what what alarm woke you up in the first place, right? and you you haven't still troubleshooted yet. So today I'm going to show you how we use AI and multi-agent frameworks to specifically identify, troubleshoot, and investigate and mitigate issues. Basically we are teaching machines to do all that frantic tab opening for us. So next time this alert comes up, I can issue a command. And by the time I get in front of my computer, I know what's broken and how to fix it. Now I'm going to show you exactly how that looks like in a minute. Uh, spoiler alert, it involves way fewer browser tabs and a lot less profanity. But Before I dive into it, let me give you some context about our environment. We have 15 EKS clusters between multiple AWS accounts, multiple VPCs, um. And some clusters are shared between teams. Some are dedicated dedicated to a single team. We are also across all these clusters we've got hundreds of microservices running different tax stacks and different observability stacks. Some users are shipping their logs to new rail, but using shipping metrics to the Prometheus Grafana. Some boats, some send boat logs and metrics to the new rail, right. And, and here's the kicker. Some of these services are interdependent. Uh, Service A in cluster one can call Service B in cluster two. So something when something breaks, good luck. You're playing detective across multiple clusters and VPCs and monitoring systems. All right. Enough talk. Uh, let's see some action. Remember that 3 a.m. disaster I pictured earlier, the 10 tabs and the uh 5 terminals and the VPN dance. Let me show you an alternative universe where that doesn't happen. Picture the same nightmare scenario 3 a.m. in the morning, your phone goes off, uh, slack lights up, some service is down, right? In the old world, the world we just lived in 5 minutes ago, you'll be trying to remember your password and try to log into VPN, etc. but in this world, watch what happens. An alert is triggered. I'm literally going to ask the agent, what's the issue with this service. Agent goes out. Hits all the end points that we were we've been talking about. There's no, no 10 tabs, no 5 terminals, no VPN dances, and we got the answer back. See that answer one question. Zero additional context needed. The agent figured it out and everything else. This is what I mean by teaching agent machines to do all that frantic tab opening for us while the agent is looking, uh, uh, gathering information, checking cube events and the new relic logs. You could literally be making coffee. Now, let's see another example. Here we have a pager duty alert going off. Uh, one of our crunch jobs failed, uh, to, uh, failed to complete successfully. Normally we have run books for this, you know, some, uh, SRE uncle or some developer who, uh, picked the short straw, goes into the cluster, you know, reruns the job, and that will be that. It's a simple task, but it's annoying. Uh You still need to authenticate to the VPN, know which command to run, switch contexts, but here's how we handle it here right now. Uh, the alert is triggered. Agent automatically on the background starts, uh, uh, looking for the, uh, root cause. We click on it. It gives us the root cause and asks us to remediate, basically asking us to, hey, do you want me to rerun this job? We give a thumbs up. Jobs is rerun. And that will be that. I wish I could type better, I guess, for the demo, but And the job is wrong. Now here's the most important part. In both cases, I gave it a zero context, no service name, no cluster name, no cluster ID, no name spaces. The agent figured that in both cases which service to investigate in which, which cluster, which name space, and hit all the right data sources automatically. So how does it do that? That's what we're going to walk through next. We got two main components making this magic happen. We have a Slack bot which is built on top of slack bolt framework. This is basically where humans ask for help. And our multi-agent orchestration layer, which is AWS strands. Uh, now let's talk about how you actually use these things. There, there's one way, uh, basically direct mention you tag the uh bot, uh, on a Slack thread, uh, underneath like a pager duty Slack thread. SRE bot. What is the, uh, issue with the service? The second one is automatic invocation for specific alert types that we know that bot can handle. It just shows up, no tagging needed. The alert fires. The bot investigates. We just approved the remedy just like the previous crunch up failure example that I just gave you guys. Now when an alert fires or it's summoned directly by tagging the bot, we pass some light metadata. The Slack channel ID, the pager duty alert content, the message content, nothing fancy, just the essentials. Uh The flows that information flows into what we call agent. That's the orchestrator layer that you see on the middle of the screen. SRE agent takes one look at that metadata and consults its system prompt. This is what it's basically it's instruction manual for what to do when things break and extracts key details to start the investigation like which cluster, which environment, which name space based on the Slack metadata, as well as what's in the system prompt. But here's where it gets smart. The system prompt. Has a special has special instructions based on the app. Uh, uh Based on the app and the error type. When it determines what kind of problem we are dealing with, it doesn't just figure it out. It invokes the right specialized agents with specific context that agent needs. You need cube events. It calls the new Cubernitis agents for cube events and deployment data. Application logs calls the neuralic agent with the service ID and the time window. CDN issue fastly agents get involved with the service ID that is specific to the application that is having the issue. Each agent launches with exactly the context it needs no wasted tokens asking which cluster, no back and forth, just targeted investigations. Now we also integrated two really powerful capabilities, Bedrock memory and bedrock knowledge bases. Let me give you a concrete example of how memory works in practice. In our demo earlier you guys saw a Chrome job failed to complete successfully, right? Agent picked up uh. The bot investigated, figured out the remedy, asked for our permission to run, rerun it, and we gave it a thumbs up and it ran it. Pretty cool, right? But here's where the memory takes it to the next level. Imagine telling the bot, hey, next time you see this error. Uh, you get this exact same error. So do, do not ask me. Just rerun it automatically. The bot stores that information in the memory, and next time alert fires. It looks into it checks the memory and sees it no longer needs acknowledgement and reruns the job. It's like training your own cold body to not to bother you at 3 a.m. in the morning for things that you know they can handle themselves. Then there's the knowledge base. This is where we store carefully curated institutional knowledge for troubleshooting our apps that you don't want cluttering up your context window. Runbox for specific apps, read me files, RCAs, postmortems. Agents can query this data on demand. Uh, and pull only the relevant information when it's needed. Artist service acting up. Pull in the artist service runbook. Uh, seeing an error you've seen before, grabbed that RCA from three months ago where you actually fix that issue. We are not preloading all the stuff into the context. We are retrieving it just in time, only when it's relevant. Now you might be thinking, OK, this is, this sounds great, uh, but how does it actually handle complex investigation? What happens when it needs to, uh, uh, uh, pull events from Cube and, uh, logs from New Relic and CDN stats from Fastly all for the specific same incident. Here's the thing most people don't realize when building AI agents context windows are finite and they fill up fast. First, let me show you or demonstrate to you what the problem is, then I'll show you how we solve it. So just like the previous example, right, we asked the agent what's the issue with the service. The prompt, uh, system prompt as well as the tool, uh, accounts for 70K, right? That 70K you see over there. I carried forward to the next iteration because our agents decide, OK, I need to get cube events as well. When, when uh uh when the agent calls that uh. Tool that 50K token as well as the response is also added to the agent's context window. That's also coming with us. Now finally our agents decides, OK, I need to get the logs as well, but here's what happens we're already. Over our context window, that's game over. The agent stops. No more tool calls, no more analysis like a pod getting out of memory killed mid request. We only made it through 3 calls, didn't get to check from ETS, query databases, or correlate any other services. So how do we fix it? So instead of one giant agent dragging around an ever growing context blob, we use agent as tools pattern, a pattern where coordinator agent delegates specific tasks. To specialized agents, agents, and here's the key, each specialized agent gets its own context window. Think of it like microservices, right? Microservices for AI agents specifically, you will build a monolith that handles authentication, subscription, billing, all the streaming in one giant code base, right? Same principle applies here isolated context, isolation, isolated execution context with clean interfaces. Coordinator agent sits on top of with 200K token. When it determines it needs to It needs Prometheus metrics. It doesn't call a tool that dumps 150 tokens of raw data on its contacts. Instead, it delegates the task to the Prometheus agent. Spawning it as a completely separate agent with its own context, Prometheus' agent does, uh, deep dive, queries Prometheus, uh, crunches metrics, analyzes trends, then returns the returns to the coordinator a summary of 500 tokens. In summary, basically stating. Uh, service throughput, uh, dropped 80% at 247 UTC correlates with the increased error rates at session service. The coordinator service receives 500, uh, token summary. The Prometheus agents, uh, that it spent 160, it's gone thrown. We don't need to carry that tokens forward. The that part of the investigation is done. Same with Cuban agents, right? Finds parts crashing, uh, identifies OEM killed containers, correlates with the recent deployments, and returns a summary of 1000 tokens. 12 pods killed out of memory in the last hour, memory limit was set to 512, but actual usage spiking at 890 started after deployment 1245 UTC. Again, that full 61K context cube agent to use thrown away, we don't need that anymore. The coordinator only gets 1000 token summary. Same idea with new rail agents queries the logs, identifies stacked traces, maybe identifies connection pool exhaustion, and returns 22,500 tokens. Now look at the coordinator agents context window. After all this investigation, we made 3 deep investigations across 3 different platforms, Prometheus, Cubernitis, New Relic, and we barely made a dent on the coordinator's context window. Coordinator can now synthesize these findings or even delegate more agents if needed, further expand or basically come to the conclusion of the investigation. This is how we can investigate across Cube, Ne Relic, Prometheus, Fastly, or whatever tools you guys are using. Coordinator context window stays manageable because we are not dumping every tool tool's raw output into it. We are getting summaries from isolated agents. So what's next? Currently, our agents handle straightforward incidents, service restarts, pot failures, basic recovery tasks. The next phase involves expanding its capability to execute complex multi-step remediation procedures. Incorporate more tools and services. A key advantage of the current architecture we have is as we incorporate more tools and services, including the new MCPs from our vendors, we significantly expand agents' capabilities, troubleshooting or remediation wise. This allows us to enrich the agent's contacts with additional data sources such as APM metrics, distributed tracing, security posture. Information. The result is more comprehensive view of our infrastructure and enabling more informed decision making and proactive incident management. Here's a big one. And the biggest one that I'm excited about, our next goal is to is an eval system where AI agents create known issues in test clusters, synthetic incidents with predetermined causes and fixes. Then we unleash our agent. On this manufactured problems, see if it arrives to the same diagnosis and remediation as we expect. It's like a dojo for your SRE agent basically. Lesson learned lesson learned, um, alright, so if there's one thing you should remember from this talk except from our stunning presentation. Uh, it's the quality context is everything, you know how we always say garbage in, garbage out. Well, it turns out slapping AI on top of your existing chaos doesn't magically fix it. Your agent is only as good as what you feed it. Feed it garbage context, you will get garbage responses. It's like asking your AI agent. To debug production issues, but the only context you're giving is, well, it's broken and a screenshot of 500 error. Good luck. It's not going to work. Start by crawling. Let your agents to read only stuff first, uh, incident response, data gathering, get comfortable with how it thinks, watch it work, fine tune it, system prompts and tools, build trust. Then you're once you're confident it's not going to suggest that delete the database as fixed, graduate to safe or right operations like let us handle the high toil soil soul crushing task like rerunning the Chrome jobs that I showed you guys earlier. Finally run after you've proven reliability and I mean actually proven it not not just hope really really hard. You can enable high stakes actions, production rollbacks, the stuff that makes your heart rate skip a bit. Build evil infrastructure from day one. Because your agent needs a CICD too prompts, drifts, model update patterns change monthly without proper testing, you are basically shipping production, uh, shipping to production based on oh well it's worked on my computer, right? You won't deploy your code without testing and don't deploy AI without emails. Build a proper evaluation environment and test truly before going to Prague or prepare to explain to your CTO why your agent suggested that to delete database as a fix. So 3 takeaways. Context is everything. AI amplifies whatever you feed it. Crawl, walk, run, build trust before you hand over your keys to the, uh, kingdom. Test like your production depends on it because it does. We become SREs to solve interesting problems, not to be professional tab openers at 3 a.m. So let's teach machines to open tabs. Let's teach them to investigate and maybe eventually let's teach them to fix things while we stay asleep. That's the dream and we are closer than you think. Thank you. Thank you, Secretary. OK. So I think we, we learned a lot and talked about our many services. So, what I want to also call out here is, you know, we have SkillBuilder, probably most of you know, but just wanted to also ensure that all the new things that we talked about, like Agent Core or any other services or solutions, so there are a lot of learning that is available. So, please take advantage with that, and there are a lot of free courses available. So, do explore those and that will help, right? Now, before we wrap up today, I just want to give you with one thought that when we see this 6 to 8 months back, the iHeart engineers were really struggling with respect to keep up with the scale and the issues. But today, with this solution, they have the precious bandwidth to look into the future enhancements and the next generation of streaming, which is very important. So the AI ops revolution is not really coming. It's already here. And with Amazon, Bedrock, you have everything that's needed to join that, right? With that said, thank you for coming to the session. I really hope you enjoy the rest of the reinvent and see you all at Replay. Thank you so much.