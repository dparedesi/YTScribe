---
video_id: 4zFJ3zDWgeM
video_url: https://www.youtube.com/watch?v=4zFJ3zDWgeM
is_generated: False
is_translatable: True
---

Um, I'm Julian. I'm a developer advocate at AWS working with the Serless team, and I love helping developers turbocharge how they build applications using all the cool things Lambda can do. Rajesh, yeah, hello everyone. Uh, I'm Rajesh Pande, principal engineer with AWS Lambda. I've been with Lambda for 6 years now. Uh, and in that time I have watched Lambda grow from thousands of customers to like millions of customers, and it's been an incredible journey for me. Uh, and I'm looking forward to sharing some of those fun stories with you all today. Cool, excellent. Can't wait. So today we're gonna talk, um, continue actually a series of different lambda under the hoods over previous reinvents to help you understand lambda a little bit more. Uh, and I'm gonna go over some lambda fundamentals, how various invokes work, and spend some more time on our polling capabilities. And then Rajesh is gonna come back in and head, head proper down the rabbit hole and do some interesting lessons on how we built lambda. we made the choices we did and even more information on the polars. Now, to be honest with you, a part of the magic of lambda is a lot of what we're going to cover today, you don't actually need to know. We just handle it all for you. It's a super amazing service and we love explaining how it works, um, so it doesn't seem so mysterious, just maybe magical in how it works at such great scale. So let's start uh with the trivia. Who wants to know probably the biggest secret of servers? Anyone? Well, actually there are quite a lot of servers underneath, sorry to tell you, hundreds of thousands in fact, and you as a consumer never see them because we AWS manage all of this underlying infrastructure for you, allowing you to focus on your business scenarios. And with lambda, we like to think of that you just bring your egg, your code, the life of your business, and we've built an incredibly resilient nest at scale to look after your egg. Because it means you don't have to hold, you don't have to build a whole bunch of things yourself. Things like order scaling logic, AZ failover, you know, retry logic, connection pooling, back pressure, you know, lease management, all these different kinds of things, we take that on so you don't have to. And this obviously runs at massive scale, more than 15 trillion invokes each month with a whopping 1.7 trillion invokes just on prime day alone. 99.99% availability and resilience built into the service, so you don't have to pay extra for it. And so let's do a recap of some lambda fundamentals. One of the major technology foundations of lambda is the open-source firecracker, but now we're actually expanding the foundation to include good old EZ 2. Lambda managed instances announced on Sunday gives you lambda's operational simplicity with uh with easy to range of instances. You can choose to use graviton or CPU or memory or network optimized instances for your lambda functions, and you don't have to manage any of these instances for you. You also get up to 12% savings, uh, 12 times savings for at scale workloads and multi-concurrency and EC2 pricing incentives all part of the package. There's also a session CNS 382, which you can find out more about it. With normal lambda, so-called on-demand, everything runs in the lambda service VPC and invokes come in from clients and ultimately run on ECT worker hosts owned by lambda in the lambda service VPC. With lambda managed instances, the control plane stays in the lambda account, but we provision and manage EC2 instances in your account, in your VPC, and then root function invokes the containers running on them. So lambda on demand with firecracker and lambda managed instances, two different compute models, but the rest of the architecture basically stays the same. Lambda's got 3 invoke types, synchronous when the caller calls either directly or via API gateway, and this sends the request to the lambda service, does its processing, waits for a response before returning it to the client. When you do an async request either via client call or maybe something like an S3 change notification or an event bridge rule matching, sending an event, you don't wait for a response from the function code. You hand the event off to lambda and lambda just handles the rest. Places the event in an internal queue and the returns a successful response to the caller saying, I got your event, and then a separate process goes and processes it. An event source mapping is a lambda resource that reads items from a queue like Kinesis, dynamo or from a, sorry, from a stream like Kinesis and dynamo or from a queue like SQS and a producer application would have put messages onto this and then we lambda manages the polars which we're gonna cover today, so read the messages and then send them on to your uh invoke for processing. The lambda API is the front end where all requests to the lambda service land, and it is multi AZ a load balancer resolving to lambda.ha region name. Edmund and AWS.com. And then this routes the invoke request to the data plane and then ultimately lands up on a worker host, yours or ours. That's it. That's the journey of events in lambda. Rajesh, I think we're done. OK. There's obviously a little bit more which we can talk to you about today, so we should continue. The front-end load balancer distinguishes a control plane requests from invoke requests, and these can be function configuration or resource management from the console, CLI tools, or an SDK. The data plane requests, uh, the route, then routes to the separate data plane, one for sync invokes and one for async invokes. And the sync invoke service is actually technically called the front-end invoke service and async the air even invoke front-end service, but you know, renaming, naming things gets a little bit confusing, so we're just gonna stick with sync and async to keep things a little simpler. And this is actually a deliberate choice to separate the data planes, which was built into the system a few years ago when basically we had a huge async spike which overwhelmed the sync service, uh, and so now we can protect the sync service from async floods. The data planes are also fleet of status hosts. Lambda again is an AZ service, a multi AZ service, so you don't have to worry about load balancing your functions across multiple AZs. Lambda handles it for you. For performance and stability, the data play needs to get information about functions from the function versions dynamo table, which we cache both on the hosts and also in an L2AZ cache, because we want to make invokes as fast as possible, so we want to avoid having to look up things on every single invoke, and it also helps with availability this way of doing it too. So let's now look at the synchronous path in more detail. So the ALB distributes the invoke request to a fleet of hosts as part of the sync invoke service. I'm simplifying the diagram here as it's gonna get a little crazy, but lambda is built so everything runs across multiple AZs. So the sync invoke service first performs some authentication and some authorization of the request, and this is going to ensure that lambda is secure as only authorized callers are going to be able to even get through lambda's front door. The service then is going to load the metadata with the request. The front end also calls what's called accounting service. This is going to check whether any quota limits may need to be enforced based on the account or reserve concurrency. The accounting service is also optimized for high throughput, needs to be under 1.5 millisecond latency, as it's called on each invoke. As it's critical to the invoke path, it's also highly available across multiple AZs and superfast. The front end then talks to what is called the assignment service and also there's a control plane service, another component which manages then the coordination and manages background tasks. It handles those control plane things like creating functions and then also manages the life cycle of assignment service nodes. Back to the assignment service, so this is gonna then coordinate the placement service. And so for the first invoker of a function, a new execution environment needs to be started on a worker host. The placement service is then gonna create an execution environment on a worker with a time-based lease. It's also gonna monitor worker health and make the call when to mark a worker as unhealthy. So once the execution environment is then up and running, you can see the init path, the assignment service is then gonna give you supplied IM role and also environment variables with the privilege, uh, the IM roles with the privileges you define, and then also the environment variables and give it to the worker. The execution environment is then going to start the language runtime, you know, whatever it is, no Java Python, and then download the function code or run the container image, and then the function in it process runs. Then the sync invoke service runs invoke and your function runs and sends the results back to the caller. To minimize latency, it's calls directly to an invoke proxy on the worker hosts. And then when subsequent requests come in, well, the front end is gonna check, talk to the assignment service, which says, yes, we have already got an execution environment all up and running, and he's gonna route the invoke payload directly to that execution environment and again run the handler. When the lease of the execution environment is, is up or there's some kind of error, the assignment service is able to gradually drain that the connections, spin down the execution environment, stopping future invokes. So let's take a little bit of a deeper look at some of the lambda workers. So a worker host is just a standard bare metal EC2 instance in which we place execution environments. Now, to be efficient and get invokes happening as fast as possible, we actually maintain empty micro VM pools on the hosts in various different memory sizes. When an invoke comes in, the host management can then allocate the smallest memory size to satisfy the request and then download the code or image. And this then speeds up things a lot as it can be a bit of time to get an execution environment up and running, and so we do them in advance. For firecracker workers, Firecracker is the process that sits above the OS and then manages the single secure execution environment for invokes. Within the execution environment, we maintain some of what we call sandboxes, and these are basically privileged container name spaces, one for your function code, the runtime and extensions, or your container image. And then we also maintain a separate managed sandbox which your code can't access, but allows us to manage the things inside that execution environment. And an interesting thing is we've actually completely rearchitected how this works this year to improve lambda for how it works today and also to build even future functionality. And it's been rolling out across our fleet and you actually wouldn't know it unless you knew where to look. Trillions of invocations happening and we're basically swapping the tires of lambda while the car is moving at crazy speeds. Outside of the sandboxes, we also run a management agent for control plane functionality and also then a telemetry agent which allows us to get metrics and logs out of the execution environment transparently. The worker host then has a bunch of other components. Host management coordinates with other services, builds and destroys those execution environments. The invoke proxy is the direct link from front end to your code. We also need to store micro VM files like runtime, binaries on the host for faster loading, and then we maintain a cache for container images and as many things as we can for performance. We also have agents that they need to talk externally to the container image chunk plane. This is for downloading container images, and S3 for Zip archives, uh, to get your function code into the execution environment. And of course, this needs to connect elsewhere and so there's networking to connect to the outside world. For lando managed instances, it's actually pretty much the same. Although we're not using Firecracker but container-D for the logical separation and then containers instead of actually micro VM execution environments. So let's have a look onto a sink. So it works in a similar way to the sync service but only handling async event requests. We've also recently increased the payload to 1 megabyte so you can now use it for even more use cases. The front end then sends the invoke request to an internal SQSQ and responds with an acknowledgement to the caller to say that lambda is going to invoke your function asynchronously. We got your request. Lambda is gonna manage the SQSQs and you don't actually even have a visibility of them. We run a number of queues and then dynamically scale them up and down depending on the load and the function concurrency. Some of the queues are shared, but we also send some events to dedicated queues to ensure that lambda can handle a huge amount of amount of um async invokes with as little latency as possible. We then have a fleet of polar instances which we manage. We're gonna cover that in more detail, and these polars read the messages from the internal SQSQ and then determine the function, uh, accounts and the payload, and then ultimately it's gonna send the invoke request synchronously again to the sync invoke service. As you can see, all lambda invokes ultimately land up as a sync invoke, and the function uses the same sync pathway I talked about before, uh, returns the function response to the polar, uh, which then deletes the message from the queue. If the request is not successful, the polar then doesn't actually delete the message from the queue and uses the same visibility timeout as you would with your own SQSQs and then the same or another polar can pick up the message and try again. You can also configure event destinations for async invokes to provide callbacks after the processing, whether the invokes are successful or they failed or after all the retries are exhausted. Some other control pane services are involved. There's a queue manager which needs to look after the queues. It's gonna monitor them if they need backups and does the creation and deletion of the new queues, and then it works nicely with the leasing service which then manages which polars are processing which queues. And it can also detect when a polar fails and so that their work can also be passed to another polar. So let's look at now the event source mapping or the polar invokes. A producer application is gonna then put messages onto the stream or queue asynchronously. We then run a number of slightly different polars as we have different clients depending on what the event source is. And this is a very similar architecture to async invokes from then on. The polars read the message from the streamer queue, can also filter them, batch them up, and then send them up to a single payload. And it sends to your function synchronously with the same sync front-end service. Remember, all lambda invokes eventually land up being synchronous. Then for queues, the polar can then delete the message from the queue when your function successfully processes them. Depending on the event source, uh, you can then send information again by the, uh, invoke to SQS SNS EventBridge S3 or even Kafka for Kafka sources, which we announced recently. There are also a number of control plane services, again, state manager, stream tracker, depending on the event source, manage the polars, manage the event sources, discovers work, and then handles scaling the polar fleets. The leasing service is gonna assign polars to work on a specific event or streaming source. If there's a problem again, it's gonna then move its work to another polar. And there are also architectural differences between two types of event sources, queues and streams. The queues is for individual task processing when each message is independent and messages are deleted after the processing. And streams is then for when you're doing, um, when you have multiple consumers, maybe need the same kind of data and order particularly matters, and then messages are going to be retried for replay, two different architectural styles. And the lambda ESM provides a bunch of functionality across all of these event sources. Uh, we've got filtering, batch controls, including for streams, being able to split a batch to find a faulty record. We got about choosing where to start in a stream, retry and failure handling, analytics for Kinesis, and some platform performance configuration options. And the cool thing is the lambda ESM handles this all for you. You don't need to think about the different characteristics beyond just configuring your ESM. We take on the task of making everything actually happen. So let's look a little deeper at the ESM and see how it works with both streaming and queuing. The ESM pulls the event source, the cure stream for available records. Now, so we need to think about even getting actually to the event source. SQS and Kinesis are on public endpoints, uh, but we also have events, uh, sources that are connected to private subnets or even outside AWS, often with Kafka. So we need to handle that as well, as well as the actual functions which could be connected to a VPC. Authentication depends on the event source. AWS ones we can use IM, but for Kafka we want to support all the possible options, uh, authorization methods so the ESM can actually authenticate. If there's ordering, things like SQS FIFO or streaming sources, we always need to ensure that the ordering is maintained with the message processing. For streams, you can configure the starting position as part of the ESM config from the beginning, from a particular time stamp, or just getting from the latest records. And the ESM also needs to remember where it was in the stream using the offset or sequence number. We need to handle retries, or resharding. Uh, streams can grow or shrink, and so we need to continually look at the source and see if there is maybe a new Kafka broker online or new shards or partitions to process from one of the event sources. The parent shard is split, creating a new child shard, and we need to maintain ordering during this process, uh, so we can send the right records to new lambda functions to then scale up the processing. For Kinesis, there's a helpful additional setting called tumbling Windows which you can use for data aggregation. You can pass the state across multiple separate lambda invokes to aggregate data in real-time, sort of quick and easy alternative way for some analytic solutions. There are also two ways to consume from kinesis. You've got shared shared fan out where you have to have up to 5 consumers can share and read throughput of a stream, and then each uh shard supports a read rate and a data rate, and this has got a latency of about 200 milliseconds. But for faster responses to stream data, you can actually set up Kinesis enhanced Fout or EFO. This actually changes the flow from the ESM pulling from Kinesis to Kinesis sending to the ESM using an HTTP push mechanism to get to the data to the ESM faster, as low as 70 milliseconds. So the ESM figures this out, manages the change in logic flow, and this is just handled transparently, something you don't have to look after yourself. There's also a dedicated polar offering for SQS and recent uh for Kafka and recently actually SQS where we can also provision even polars in advance to be able to handle spikes. You can configure various settings, mainly the minimum and maximum polars, to be able to control the throughput, and this can dramatically increase your polling scaling and throughput. 16 times for SQS up to 20,000 concurrency compared to the standard polling of 1250 for SQS. Kafka also simplifies the networking and the cost models when you enable this, and there is an additional charge to be able to do this, and it's based on an event processing unit, uh, which you can manage. And so provision mode is great for high traffic sources where you want low latency and also this can save you some polling, uh, money at big scale. And provision mode allows us to build and bring new functionality to the polars like supporting schema registries and allowing efficient binary formats like Avro and Proto Buff. OK, so we have done the polling, then the filtering allows you to dump records you don't want to process. Why? Well, filtering is a CPU intensive process, and you don't really want to do that and waste lambda invokes in your code. So you can have positive filtering to include messages that you do want to include or negative to exclude all messages that don't match a particular specific criteria. This is very flexible using the same syntax as eventbridge filtering. And here in this example, we are processing messages for the tire pressure less than 32 to send that only those to your function. Next up is batching, uh, batching actually the records for, uh, grouping the records for efficient processing, and this means you can control your lamb invokes for efficiency. So how are batches defined? Well, you get to configure this and control this in a number of 3 ways. You can configure the size, and this is 1 up to a default of 10,000. We normally start at 10, the number of items in the batch, uh, but yeah, up to 10,000, that is really big. And then the batch window is to help improve efficiency when you don't actually have much traffic. So you can at least process messages before a whole batch is formed. And you need to also stay within the lambda 6 meg payload limit. So once there is a batch, we then need to invoke the function via the sync control plane. So we need to then consider different scaling for the different architectures. Streaming is actually an upper bound problem. You want to process data with a stream generally as fast as possible, and so there's a maximum throughput you want to factor in. So with Kinesis and Kafka streams, you're trying to consume and process, uh, process messages at the highest rate possible to keep up with the incoming data, uh, and to minimize the lag. And the challenge is in scaling up to exceed, uh, or meet at least, uh, or possibly exceed the ingestion rate. Queuing is different. This is actually a lower bound problem. You do want to process data efficiently, but you also want to ensure you're not actually overwhelming downstream services like APIs or databases. So with SQS, the queue is actually acting as a buffer or a shock absorber, and you are then controlling the rate where you want to drain the queue to then protect those downstream resources. And the challenge is in maintaining a minimum safe processing rate that is going to respect those downstream resources. So two different architectural styles. And so for SQS to do this, we want to manage the flow control, the rate at which lambda consumes the messages. And so you can configure the max concurrency on the ESM to control how many concurrent executions the lambda, uh, the ESM is then gonna attempt to send to lambda. To prevent overwhelmingly downstream services. And this is actually the all-important buffering control. Reserve concurrency is in a separate setting on the lambda function which you set to reserve capacity from the function, and this ensures it can scale up if needed within the available account concurrency. So you actually rather want to use the max concurrency to maintain, to manage the buffer and the flow, but you can also use both together, uh, and if you do, you wanna make sure that the reserve concurrency is higher than the maximum concurrency to prevent any throttling. So for also streams, we need to scale up how the stream scales. This is automatically, the ESM just needs to figure this out. Uh, Kinesa scales by adding shards and Kafka by adding partitions, and Lambri is automatically gonna figure this all out and it's gonna scale up the consumer polars to match the incoming throughput. Still with message ordering in each partition or shard, more shards or partitions means more processing. But customers wanted even more processing for kinesis, so we came up with what's called paralyzation factor. By default, it's one function per individual shard, but you can scale up to 10 for massive lambda throughput, still maintaining the order, and this is something built specifically for lambda for high throughput stream processing. For error handling, streams and queues also have different semantics. Streams need to produce or preserve the ordering, but you actually need to figure out and decide what to do with the poison pill message. Should that actually stop processing? If it's something like log data or bank transactions, you know, absolutely, you want to stop the steam processing and you want to fix the problem before continuing processing. But maybe for, you know, some frequent sensor data or I don't know, you know, GPS, uh, GPS data from a hire car, you can handle some of that data loss because you know in a few seconds' time the next result is gonna come in and you can still continue shortly. So in this scenario, you don't actually want to stop the stream because you want to keep the message processing going. And then for cues, generally you don't want to stop for a failed message and you wanna continue to process the rest. And we then have, uh, we then have, uh, two batch error handling things to give you some more control. Partial botch, partial batch response is when you know the failed record and you can turn a successful response and then tell, tell the ESM which message has failed and then that's gonna go, uh, to be retried and processing continue. And AWS Power tools for AWS Lambda is a great utility for using for lots of languages, and that's got a batch processing utility to help with this. And for streams, bisecting batches can be super useful when you don't handle the error, and it's not something you normally need to consider for cues. So in this example, message 3 is a bad one, although we wouldn't actually know it yet, and so we process the batch and get a failed response. Instead of endlessly retrying the same batch, the ESM can then split the batch, try the first half to keep ordering, and try to get more messages processed. The split batch fails again as it has the faulty message and so the ESM splits again and we now try and process the new batches. This time, the first batch processes successfully and the single batch with the poison message fails again, and we can then continue with the original second half of the first batch to keep up that ordering of the successful messages and then can fully process as many messages as possible without having to reject a whole batch and then deal with a faulty message if we want to, really efficient way that you can control. And then we need to handle failure, uh, handle function errors, so we need to configure lambda on failure destinations for the, uh, for the function invoke issues. And then you might think, well, why are there actually two different, um, error handling paths for SQS? Well, the SQS DLQ is gonna cap. To messages that fail repeatedly during the polling, uh, polling or the processing. While the on failure destinations is gonna capture invoke errors with your lambda function and this could be things like network issues or throttling or maybe you've deleted the function or there's a function permission issue. And so both serve different processes and can be used together for comprehensive error handling. And so that takes us through the lambda fundamentals. We've covered sync, async, workers, polars, and all the features of the VN source mapping. Again, again, it just looks like a configuration option, but lambda underneath the hood is doing a huge amount of work, doing some polling, doing some pushing, doing some, uh, pulling, uh, you know, being as efficient as possible to be able to get the messages from the different architectures of your streams and your queues. So let me now hand back over to Rajesh, who's gonna dive deeper into the polars and talk about some of the lessons, uh. Uh, we learned about when we were building lambda. OK. Hello. Thanks, Julian. Yeah, the cold start. Uh Yeah, lambda with that culture, you know, uh, yeah, uh, hi, uh, hello everyone again, uh, this is Rajesh, uh, today, uh. I'll be talking about, uh, let me give you the overview of, like, I'll, I have divided this segment of the talk into two parts. One is like I'll walk you through the mental model of like some of the mental models that we have used while designing lambda and the operational complexity that comes with it and how that complexity, uh, actually fueled a lot of innovation, uh, that we have baked into lambda design. So let's start with the mental model first. Last time we stood here, uh, and convinced you that like lambda in some ways is a storage service and applies a lot of learnings from building services like EBS. Foundation service like EBS and this year I want to convince you that Lambda is also a queuing service and like how we have applied a lot of learnings from building queuing services and the queue queuing theory concepts. I know compute as a queuing service seems a bit counterintuitive, but let me explain what I actually mean by that, and then like, I'll walk you through like the thought process of designing this, uh, lambda service. So what's a queue service? Uh, let's start with that. I started with Amazon back in 2013, so it's been more than a decade. Uh, and the first thing that I did was build a queue processor, and basically a queue processor is there's a message buffer where a lot of events and messages are stored. And uh there is a worker which actually pulls so you write a way to deal with the queue and then you have a business logic that you process, uh, uh, your, your messages and do something with it. And for that, uh, I had to build a lot of simulations, and I, I, during, uh, I had to build a lot of simulations to like go through model my upstreams, model my downstreams, what are the failure points, and like what would it take to build a resilient service. Back then, lambda didn't exist, uh, so I had to do a lot of like heavy lifting and I had to build the, uh, key processor from scratch on my own. And when I joined Lambda, uh, that is 2018, uh, 2019 time, I realized that the same pattern shows up everywhere. Many of the features that we built and the many of the things Julian just walked you through are inspired by that same analysis. I've been doing this since 2013, I, as I told you, uh, over a decade. So let me quickly walk you through the queen theory so that will set the foundation for the next of the, uh, rest of the session. Queueing theory is nothing fancy but just basically study of the waiting lines, how things arrive and wait and get processed and, and leave. This diagram is basic. There's a basic shape of every queuing system you have probably, uh, you, you see, probably you have seen some version of it. Uh, events arrive at some rate and that rate is never defined. Like some customers go crazy, uh, they spike, some are like slow and has bounded. Uh, uh, arrival pattern. On the right side what you see is a worker. Their speed is the service rate just like arrivals. This isn't constant. Like sometimes your data, your dependency can take more than, um, uh, if it is taking like 10 milliseconds, sometimes it can take like 100 milliseconds. So like this is also not like relevant. So you have to model different things, uh, around it. And next, what you see here is like the. Buffer, burst and variability. The variance, uh, is when you, uh, you, you're building a queuing service, there, there comes a lot of like variance in terms of if there's a spike or if there's a thread issue or if your services are not, not able to like hand up, uh, like, uh, deal with the process, uh, deal with the workload, then it may run into some issues and that vari is a simple one, variance into the system can lead to. Uh, buildup of the backlog. So I'll talk more about it when we go into like a specific sections of this. Uh, so let's, let's look at the lessons that lambda has learned from the King theory and like how it, it, it applied it. The first thing I want to talk about is the lesson that we learned from King theory is like buffers smooth variants. Arrivals are never uniform. They are busty, they spike, they're unpredictable. And so we don't try to handle them right away. Uh, that's impossible and expensive. You can always provision for like. Like the peak, but then you, you would end up like paying a lot more for it. The second lesson is like we specialize workers for workload types and I'll talk about like how this led to design of like how uh lambda in terms of like how we separated polars from like the lambda execution environment and why the nature of different polar, uh, the polar worker is different. Uh, than the execution environment. And the 3rd is the control variants to prevent instability. Even if an average arrival rate is less than average service rate, variance can cause temporary, uh, very instability, and that variance kills the system. And the fourth is like coordinate shared resources through centralized control. In queuing theory models, the work conservation means if there's a work, there, and if there's a server, so work, server should never sit idle, basically. But theory assumes there is always exist a global state in distributed system. You have to build it, and I'll talk about like how lambda has to, uh, build it. So let's, uh, first start with like this, uh, buffers smooth variants as I was talking about like the traffic pattern is always variable. There are customers with a steady state traffic. There are customers with random traffic. Some customers spike extremely spike like they are. Uh, they, they can spike from like 10x to like, uh, 50x, and you have to have a system which actually, uh, takes care of this thing. There are some customers, those who are like slowly ramping up and suddenly they become the, the real customers, and, uh, some custom, uh, some of the customers can bust like can have micro busty patterns, uh. That, that you have to have a system which can actually handle this. And like, so what, what you're basically building is an injection tier where there are data sources, there's an injection tier, and there's a storage lavish. Where the system, the, the traits of that system is that it can accept highly variable multi-tenant traffic. You need to, after you've taken the traffic, you need to normalize and validate the request, and you have to write the uh message or events to the right queue. You have to persist it and acknowledge it. Before acknowledging, you need to make sure that like it's persisted durably. And then you also have to apply admission control and back pressure to protect yourself. So if you have to build something of this sort. You, you would need to, uh, like you need to have a durable architecture which actually helps you build this like, uh, in decent tier. So what Lambda did, we, this is like the, uh, the layer of the customers. They, they're variable customers. What we do, we have a load balancer where like you, whenever the first time that you invoke, so like Julian was talking about like front end and invoke and, uh, event invoke front end. So this is the front end that, uh, that is the event, event invoke front end. And it has a bunch of things like the distributed load like health checks and all. And once you have that like, it hits the upper front end like this, this is called like a reverse proxy, you can call it that, which is like multi multi AG service, uh, and has like a bunch of, uh, like hosts in it. And at the same time it's also has a like auto scaling, um, rules so that like if there is a spike in, uh, heat in CPU or memory or something, a bunch of these things, then it can auto scale on its own. Apart from that, uh, we also have to build, like, once you have gotten the injection tier done, like you also have to process the message, uh, as I was talking about. So you have to have a buffer, and this buffer needs to be durable, uh, in the sense that like you, you can't lose message once you have, uh, told it to, uh, your customers that like we have accepted the message. But also, what at the end of the day, the, the thing that you want to do is like it should go into the queue where you can process it. We don't want to just index the message, but we also want to process it. So how do we build that like routing layer in between? Uh, let me walk you through that. So first thing is like, there's an invoke request that comes in with the event message. Then it goes through a chain of validators. Um, you do authentication, then you do validate the payload, then you, uh, look at the function, whether there's an active function or not. Um, and then once you pass this, uh, this validation layer. You need to find the queue where that message can be ingested. And again, like I was talking about, we have millions of customers. We can't have like millions of queues, so there has to be like some sort of like a, a multi-tenancy within the queues. So for that, the most popular pattern, I, I think you guys might have seen is like building um a set of queues and then hashing them onto like a hash ring and then they're doing that multi-tenancy for the customers. So what we do when uh. Uh, message passes through all the validation shares. We find the queue finder. We hash it onto like the partition of the hash string. Uh, and once we have found the part. We, we, we take that message, we have gotten the queue, uh, and then we write it to the, that queue, whatever that queue that uh we found on the hash string. But this is, as you know, like, even a single partition can have, can get like really, really hot because now millions of customers, uh, a lot of them can overlap onto like the same partition. So what we do is. We, we, we went back and then we, we, uh, we use a technique called shuffle charting where shuffle charting is nothing basically it's like instead of liking hashing a customer onto like a single queue, now you, you can hash that customers onto like multiple queues. So what we do is we have a single customer based on the account ID and there are some heuristics that we have. Uh, we, we hash that customers into like now two hash ranges. We pick the queues based on those hash ranges and then find the queue which has the lowest queue depth at that time. Uh, and then inserts a message in there. So this is what we do. This is what we call like shuffle charting, and it, it, it does wonders for us, and it helps us remove like this like hard partitioning, and it, it goes away. In addition to this, we also have something called express lane. So once we know, like, OK, we know this customer is going to be noisy and it will be having a lot more traffic, we already create like a sideline queue for them. We let them like absorb and we absorb the traffic, and then once they're done, we can, uh, uh, we can reap that queue and back them, uh, into like those, uh, dynamic shared queues. And uh this was what I have talked about so far is like the cues that are managed by lambda. Those are like event invoke cues. So whenever you ingest lambda, invoke lambda in like asyncromus, that is where it goes. There's another part of the Buffer, which is like helping us like smoothen the variance is like a customer one. So this is like the event source mapping side of things where like the queues and the streams and everything stays uh on the customer boundaries. All we do is like we have the processing and from this point onwards the polling becomes the same for both of both like the either it is customer managed or it is um uh service managed. So let's The, the second lesson that we applied, uh, that specialized workers for workload types. Let's look at how lambda lambda applies this lesson number 2. To understand it, let's understand what a worker is. It's not a typical worker, but a worker is nothing but a processor, which looks at the queue, and whenever it sees a message or an event, it processes it. What queueing theory showed us that heterogeneous servers, servers with different service rates need to be dealt differently for best performance. We applied this. And we separated the polling workloads from execution workloads. Both polling and execution workloads are different in nature. Polling is continuous, stateful, connection-heavy. On the other hand, execution is busty, stateless, and short-lived. So what you see here is there are actually two types of worker. One is a polling worker that is managed by lambda, and there is a lambda function that like customers right. Julian talked about this side of the worker, uh, and, uh, like the lambda worker execution warrant. I want to dive deep, uh, today onto like the polar side, polar worker. So let's jump right in. So what are the key components that make a polar? I divided and I looked into like our, like what we do today. So this, this changes all the time. We learn new things and we evolve, so this is what we have today. What we have like a work acquire, this component looks at like if there's an active work. So it looks into like an assignment manager store. It says that, OK, there's an SKS cube or there's a stream. There's a work that is there. So what it does, it picks up the work and it starts the event source configuration. So like for in the case of a stream, uh, like for KCL, uh, Kinesis, it's KCL for Kafka, it's Kafka Consumer. For SQS it's SDK, uh, SQS client. What it does, like, once it's configured. It kickstarts this work. It's configures the connector and pulls the record and then writes into like an in memory queue buffer. So a lot of features that Julian was talked about are being done in this like in memory queue where we create this view so we don't kind of pressure the event source and we build this view and this is where like you do batching, your filtering, your schema validation if it supports that. Then we have this like an invoke orchestrator layer where once you have your batches ready, then batching can be for different diff uh for different, different ways of batching. Julian talked about like 3 of the ways we do the batching. Once you have the batch ready with your conditions defined, you do the invoke orchestrator. What it does, it like invokes your lambda in synchronous manner, uh, with the batch, and it deals with the record. If it is failing, then it, it, it hands it off to like the error handling, um, component which writes to DLQ or retries, um, whatever is appropriate based on your configuration. In addition to this, like, if your message is successful, we have a checkpointer manager which actually deals with like an advancing the checkpoint so that like this says that, OK, before deleting a message, we make sure that like your, uh, system is, uh, uh, has, uh, processed it and we are not losing any message. In addition to this, uh, this is, uh, an auto scaler we have. So it looks at the current state of the system and says that, OK, whether I need to auto scale. If I'm running hot, I should look into the, uh, the signals of my, uh, worker right now and say that, OK, I need to, uh, scale, scale further. And the scaling can be in, in two parts. One is like vertical scaling. If I have a capacity in my host, so I look at the footprint of my memory and CPU and say that, OK, I can vertically scale to absorb some more, uh, traffic. Or if I cannot, then I will signal it to like some of my global coordinator to say that, OK, I need more help. So can you, like, spin up new workers for me and then, uh, like, take on that work. Apart from that, if I'm running hot, I also want to load shed some of the workers. So like, we have a load shedding component which actually removes the work if I'm running hot. Uh, I don't want to impact my customers. In addition to that, there's a like an overall like a health checker which keeps looking into like different parts of the system and making that data available to different uh components. So auto scaler looks into that data and says that, OK, oh, I think I have, I'm running hot. I need to auto scale and load shedder said, OK, I'm running hot. I need to load shed something in memory key buffers also like similarly like it builds that view and then makes it available, uh, to like different components. So what, what we did so far is like workers, uh, and the view of that worker and like what, what it takes to build that worker, but not every worker is sane. Some workers are more secure, some need to like go through the BPC boundaries. So what we did. It's like we went back to the drawing board and looked hard at it, that what is the anatomy of the polar and what can be done there. So if you remember, this is, this is what we just saw, and we squinted, like literally we squinted and saw that like, OK, if we slice the worker horizontally, We basically get the bottom up part where all the system touchpoints are. We squinted even harder, and I'm talking literally, I squinted a little harder. Uh, our bottom half can be further divided into three parts. With 3 different touch points. And let me share what I meant by that. If you look closely, The 11 part of that worker is actually dealing with the event sources, and one part of the worker is actually dealing with the Internal service communication, doing the internal service communication, and another part is dealing with like the lambda private invocations. And all three have different security boundaries that you are operating with. So what we did is we said, OK, let's pick this part of the worker and wrap it into like one specific set of security boundary. Then pick this side of the worker and put it into one specific, uh, security boundary. And the uh, the last part was like lambda execution environment which like you can have a very private BBC function that we can invoke. And what else, uh, like we applied the same principle that we applied to lambda worker and lambda polars. We applied the same principle within the lambda worker, uh, polar workers and said that, OK, even there, there are 3 parts. So now we can automatically scale a part of that worker which is independent and has nothing to do with the rest of the rest of the worker. So think about like if you have worked with Kafka, then you know the problem around consumers rebalancing. There's like death loop and then like a bunch of things that have been done like a static stability, uh, static, uh, membership, and then cooperative rebalance. What if We can separate these two things, uh, in, in such a way that like, you can deal with the consumers separately than actually increasing throughput by invoking lambda functions. So we did that. We wrapped them into a security boundary where like one is like dealing with the customer source, one is dealing with the internal services, and one is dealing with the function execution. So we can have a VPC pairing between execution environment and the polar workers. We can have a cross-count connectivity with the customer event source and then internal service where like this is within the same boundaries where it is operating. Now that we understand polar workers, let's look at the next lesson that we got from the King theory, which tells us about the production behavior of these workers. Let's look at that. The lesson number 3 is the control variants to prevent instability. And as I, as I said, like variance skills systems. So let's take a deeper look at it. What you see here is a workload, which has a high latency. Once it has gone through and like now, you started to absorb a lot more resources of the system. And that leaves like the workload B and workload C with no resources to work with. And as little law dictates that if processing latency increases, concurrency will spike even with a constant arrival rate. Now at this point. We want to build some sort of like a fairness in our system. So what do we do? To deal with, we built multiple layers of defense control, uh, with, uh, different, uh, different layers of like, uh, mechanisms to maintain the stability into the system. The first one is like variance reduction. The first layer is if you're getting messages into the queue, try to batch them, for example. The second one is like the capacity controls, set hard limits, like capping the concurrency, so a single function can't overrun the fleet. Third is control what comes in. Throttle requests when you, you, you're full, just like a store locking the door at the capacity, for example. And back pressure. React when overloaded. You don't have to suffer. Slow down, polling or shed low priority. These are like 4 layers that that we have built. Again, these are layers that we have built, and some of these layer can be configured by the customers, and some of these are like owned by and managed by services service team. Now that we have arrived, looked at the arrival pattern workers and what keep the service running, look, uh, one more thing that I talked about is. Coordinate shared resources through centralized component. What, what queuing theory, uh, says that, OK, there is always a deterministic or stable pattern whenever one worker dies, the other worker picks it up. We don't have that luxury in a distributed system, so like we have to build this global state on our own. What, what it takes to build that global er uh global state. So global state has 4 traits in it. One is one, work conserving, scheduling is like if there is a work and there's an idle worker, we should try to pair them up. Conflict-free assignment, if one worker is working on it, don't give it to someone else so that they, they're not like, uh, thrashing each other. The capability of your dispatch, like, you know, you are giving work to someone and it can handle that specific work. Like, I don't want to give like a queuing work or uh an SQS work to, uh, an stream polar, for example. And the fallow detection recovery. If someone has claimed the work and it is actually not working on it, then I should have a way to know that someone is not actually actively progressing that. So, what do we do? Uh, we have a control plane. Basically, like whenever you create an ESM or you could create an event invoke config, it goes to that, it goes to a config store. And then we have an ESM life cycle manager, which, whose job is to bring worker and make it available to the system so that it can, it can work on it, act on it. Then we create an assignment. With that assignment, uh, there's another, uh, separate service called assignment Manager where it stores the assignment details into the config, and this is where like we build that global sheet. And as I talked about like the polar worker, it gets the work, then it heartbeats and sends the metadata back to the assignment conferring store, which is the source of, of, of like we have in the global state. And at the same time, like this is like an anti-entropy job which is making sure that everything whoever is claiming of the claiming the work, they have the work and they're acting on it. And if they're not acting on it, then I want to involve the human operators so that like it can take an action on it. And this leads to what you saw, what Julian showed you the lambda Polar worker architecture looks like. And you have seen it like multiple times, and this, this was the under the hood part of building this, this kind of, uh, architecture. Let's quickly recap. Injection tear and buffer, you need to have a buffer smoothing mechanism. There are a worker and there are different types. Some are, some has a different, different, um, traits, so you should like try to separate them out. The stability controls variance kills your system, so you need to have like controls so that you can maintain and build, um, uh, stability to your system and the global state that is needed for you to like make the continuous progress and this is, this is what we needed, uh, need, uh, to keep, uh, fleet healthy. And this basically tells you almost everything that you need to know about how lambda applies lessons from King theory and hope I, by now, I give you some idea on like why I said like lambda is also a key service in, in, in many ways. And Lambda has millions of customers and trillions of events flowing through these systems. So how do we keep everything running? Seeing everything up front is really, really hard, as, and, and as a wise person said. It's like everything fails all the time. So once you have the queue, you have the worker. What if, what do you do if, if, if it fails? So this second part of the talk I talked about is like how do we deal with operational complexity on operating a queue service like this. Obviously, we had to innovate on different dimensions to run it quickly. In queuing theory, it tells us that. The fellows are independent. Systems scale linearly and saturations happen gradually. It's the kind of It's, it It's the kind of world you would like to see as an engineer. But the moment you operate a real multi-tenant distributed system, you realize that everything gets messier. The reality is there are correlated failures. If one service fails, you see like, oh, other service starts to fail. System don't scale linearly. If there's a retry loop happening somewhere, then they, they accelerate faster and the cliff drop. It's like you model something in your simulation. You always miss out that one condition which leads to like some, some, some sort of like a cliff drop. And when I say cliff, I mean this cliff. If you remember. Back in 2013, lambda had an outage where a Latin bug in our front and resurfaced, and that led to an increase in error rate. This is the case I'm talking about. It's really, really hard to model. All the cases while you're testing. Same in the case with the queuing service, you can't model everything, but what you can do is like build a system which can recover from this. While there are many things we do, I captured 3 main resiliency patterns. That we can apply, uh, that we apply to operate lambda. The first one is like the dependency outages. What do we do when the core dependencies that we rely on to build the scheme service goes away? The scale inversion. What if like a bigger fleet starts to hammer the smaller fleet? What do we do? And the third is like availability zone outage. It's very special and very unique to like the problem, how we solve it and like how it is different for like operating a king service. So let's start with the dependency outage. Let's look at the recent outage. October 2025. I think it's pretty fresh in everyone's memory. This was the recent DDB outage with a bunch of services got impacted, and lambda was one of them. We use GDB for various things to operate our data plane. The root cause of this issue was a latent trace condition in Dynamo DB DNS management. I think there is a talk you guys should attend if you're interested in what what happened here. Now, if you think from the point of view of a queue service operator, this becomes even more interesting. What you see here is that when an outage happens, we start to accumulate messages. Now when the dependency is recovered, we have to deal with backlog that we have accumulated with a new traffic which is coming and to catch up, we have to now start, like we have to work twice as hard, right? We have accumulated some messages and the new set of messages are still coming in. So what do we do? The second kind of thing that we see is like a fairness conundrum. Uh, what I call it is like a, a noisy customer is a noisy customer, but a good customer becomes a noisy customer because now there's no way for us to like that noisy customer has 100 thousands of messages or millions of messages. Uh, uh, a good customer, a good citizen also has like millions of messages now because we were out, so we started to accumulate. Like how do we differentiate between that. And So what do we do? Like, whenever there's an outage, we want to clear the backlog as soon as possible, but we also don't want to hammer our service down so that like we delay the recovery. It's, it's, it's a balancing act. It always turns into a balancing act for us whenever there's an outage. If we are underflowing, then we are delaying your recovery. So your message is staying in QQ longer. But if you are overflowing, then we may tip over the service, which is actually, because there is a capacity that we are provision, and there's like a finite amount of work that you could do. You can like have buffer and you can unlock a lot of capacity, but there's still, there is the, uh, the amount of work log that you have accumulated can take more time. There are a few things that we do, and I'll talk about like some of those strategies, and this is not, not exhaustive by any means, but there are a few things which are interesting. I want to want to share that. One thing that we build is we build a gradual ramp using a token bucket. That token bucket rate is controlled through the view of the system. So like how our CPU and memory pressure is looking, how is our sync traffic looking, how is our different modes of invocations are looking, and then we build that and tune the rate of our. TPS that we are generating through this like asynchronous fleet to to to to gradually recover so that we are not tipping over and then we have this handshake mechanism every time there's an an outage uh we build this uh automated handshake mechanism where we keep looking into like dialing up the knob and then looking at the rate at which like how our service is recovering. We also look at, try to convert a FIFOQ into a LIFOQ wherever, like if you have a backlog and you are OK to go ahead and then uh start looking into the first messages, so we start to try to create and share the customers. So the old message the backlog can drain slowly, but your new messages, so that's why you see recovery very fast. We also sometimes, if you have a function, you have a version. So we try to shard your ques into like a function and a function version, and then an alias, so that you start to see recovery much faster. And then as I was saying, A good customer can become noisy. A noisy customer is a noisy customer. So like, how do we isolate these things? So for that, like, you know, think about like a customer has like millions of messages and set their function concurrency to one. So how do we deal with that? Like now that customer is in the queue, we try to isolate that customers from the own queue so it can have its own processing because it's not a noisy customer. It's the outage that made it noisy. So we have another. Kind of like a token bucket mechanism where we assign uh a set buffers or like give tokens based on whatever the function concurrence or downstream configurations are and we assign that one single specific token bucket to that customer, uh, and then like try to isolate them so that like they have even if they are in the CIQ they are now using that token bucket to say that this is their processing capacity. So this, this was about, like, what if, what happens when dependency is out. There's another kind of outage. Uh, that, that we have to deal with is like scale inversion, whereas a big fleet is depending on a small fleet and a small fleet goes out or a big fleet behaves, uh, weirdly, which leads to, uh, a small, uh, fleet going out. So, uh, this is another, uh, outage, uh, where like a large surge of connection led to, um, a, uh, and, and, and a rest storm happened which led to A small print going out. And then also at the same time, like the control plane was like recovering which, uh, uh, which, which, which delayed the recovery. And for us it becomes if you think about like that work the global state view like the leasing services out if this is out for like one minute we have accumulated messages and we have keep continuing to getting like new messages. So how do we recover from that and it becomes that problem for us. And so for that, like, we build static facility. What I mean by static facility is the ability of a workload to continue to correct operation, uh, continue operation despite dependency impairment without requiring reactive changes. The most critical step to achieving this static stability is separating the hot path from the cold path. Data plane doesn't need to call control plane for during the processing. Control plane only needs to like do the life cycle management and data plane is sufficient to operate in the steady state. One of the strategies is like we push down the configuration for lambda's case, we push the lambda function configuration down to the polars instead of like polar asking for that work. The Second is like replicating the state like L1 and L2 cash. I think Julian talked, uh, briefly about it. We also use disk as a backup so that like we can consume the work and like restart the work, uh, resume the work where it, it, it, it, it, it is, uh, if there's an outage and then build a stickiness with a. flexible time out so we try to reduce the false positive from like the false negatives. If there's an outage and we see like it's a temporary outage, don't leave your work. Don't release your work. Lease driven independence means partitions has their own dedicated worker or charters of dedicated workers, and the circuit breaker is like if you see like, uh, the, the error is spread across the whole fleet, that means maybe there is nothing in common with like that one specific coast, but it is spread across the whole, whole, whole of the system. And the third kind of outage is like availability outage. Uh, what I mean with that, like, houses are very weird. Uh, there can be like a thunderstorm somewhere and that led down to, uh, outage of, uh, Uh, like an availability zone. So what happens for us, uh, if there is an AZ, there are 3 AZs in a sample workload, if there's 1 AZ out, we have to migrate because we have millions of queues that we are processing and there are millions of workers and the relationship. So like we have to migrate that work active workload from that AZ to another AZ. And that that migration requires some effort in on on our uh on our side. So what do we do? Like we have easy buffer. This is very standard. Like if you have a capital processing capacity and you need to, uh, move the work, you need to have the buffer so that you can take on the work. The second, and again this is like another balancing act for us where like if you are one is impacted, you need to. Move the work to like the other AZ, but you don't want to impact the other AZ also with it, uh, with that outage. So we have like hardware-based mechanism, uh, where like we are reactively like we let the leases expire and then that becomes available and then gets shared, but it takes longer time. And what if like lambda has recovered other workers we have moved the workers from like unhealthy AZ to like healthy easy. Now we need time to, uh, and if you're taking time to migrate, then there's no point in like. Having that recovery, so we need to do, uh, get more proactive in that. We have divided that into like three parts like detection. We need to first make sure that like there's an easy outage. We are very confident about it, then we evacuate and then we observe continuously observe that like there is an outage, uh, there is a like the work has migrated to that, uh, new A and then like if when the uh recovery happened we need to move it back. So the key takeaways are architectural keyways designed robustity, unpredictable, reduced perability, isolate, and operational key takeaways, uh, around. Multi-tenancy is the default. Failures are constant, and silverity comes before skill. And with that, I'll hand it over to Julian. Excellent. Thank you very much. Um, we've only got a little bit of time left, but I just wanna quickly go through some of the new things with lambda. We talked about lambda managed instances, all the flexibility of lambda with EC2 control plane as well. Lambda durable functions, the whole way, messes up all the way you're thinking about lambda, um, uh, built for asynchronous processing, but all in a single lambda function, Build long-running workflows in your favorite programming languages. Another session is happening on that this afternoon. Tenant isolation, providing execution, isolation between tenants. You just give a tenant ID. This is great for SAS customers who have maybe hundreds of thousands of functions, makes it much better for them. Events source mapping enhancements, we've got schema registry, which has been released, the provision mode for SQS I mentioned, and also a whole bunch of ESM metrics were available to help you building your applications. And the future, well, of course, there's plenty more to do. We'll always be helping with observability, uh, bringing new updates, new runtime as quickly as we can, working hard on price to make lambda applicable for all workloads, more deeper integrations with AWS and third-party services, and always, we continue to listen to your feedback. Let us know what you would like us to make, uh, make to build. Our lambda roadmap is now public, available on GitHub, so you can help us define the future of lambda. Lots of other kinds of sessions, manage instances, durable functions, uh, the future of Servus tomorrow, and I did a talk on best practices yesterday. Hopefully, that'll be on YouTube and useful for you as well. Uh, another slide here just on a bunch of, a lot of service kind of things to continue your Servless learning. Um, but most importantly, thank you so much for today for coming, uh, early in the morning. I appreciate you taking the time to learn a little bit more about lambda. Hopefully, you can understand there's a huge, a huge amount that happens under the hood, so you can just consume lambda as a great service without having to worry about all the flow control, all the other kind of things, uh, that Rajesh and his team is building. So thank you very much for joining us today and enjoy the rest of your read. Thank you.