---
video_id: ulFeSnJlRvk
video_url: https://www.youtube.com/watch?v=ulFeSnJlRvk
is_generated: False
is_translatable: True
---

Hi everybody. Uh, welcome to Reinvent 2025. It's good to see you all here. Thank you for attending this session. Picture this, it's Friday night, and you're scrolling through Netflix, trying to find a new title to watch. If it were me, I would be scrolling through K-drama listings, sci-fi listings, rom-com listings, and Netflix just magically surfaces the one title that catches your eye, along with thumbnail and artwork that catches your eye, makes you click, you watch one episode, 2 episodes, and by the time you're done, you've binge watched the entire season of Squid Games. Um This is not magic, to be honest, it's a series of, it's a product of disciplined experimentation. Netflix is constantly running experiments in the back end using AB testing to see what titles you click, how long you view these titles, whether you come back to watch those titles tomorrow, and. You know what we should also realize is a good experiment um is incomplete if it only speaks about member engagement because for every new row in Netflix home page for every artwork that gets stored, for every smart recommendation, there is an infrastructure footprint on AWS. So there is a cost to running these experiments and at Netflix scale with over 95 billion hours of content viewed just in the first half of 2025, the cost of those experiments can be pretty huge, which is why. Netflix has worked to connect its product experimentation with the cloud spend, treating the AWS bill almost as an, as an experiment metric. The real win here is not just for me being able to binge watch the entire season of Bridgerton, but it's about the way in which the overall experience works, where it keeps the member viewership experience great, it makes decisioning data driven, and it keeps the cloud spent intentional. My name is Manju. I'm a solutions architect with AWS. Um, today Chris and Tushar from Netflix will talk to you about how their teams have connected product experiments to the AWS bill. By the end of the session, you will leave with concrete ideas on how to run experiments, not just to delight your users, but to show up cleanly and predictably in your AWS cloud bill. So your product teams can continue to innovate and deploy new products and your finance teams will actually thank you for that process. Of Chris. Thank you very much. Just, uh, very curious to see with the launch of Stranger Things, the new season, who's, uh, who's dabbled, who's, who started to watch the new season. OK, we got a, a, a few hands. Well, we appreciate it. Uh, well, good morning everybody and thank you for attending our, our session today. Um, I do wanna start with a little bit of a loaded question. Um, so I'm curious to see from the audience how many of you have opened up your, let's say your AWS bill, uh, and promptly closed it out of fear or out of just a visceral reaction. Um, I do see saw a few people put their hand up, uh, a few people looking away, um, that's OK. Um, we've all been there, I think, in, in some way, um, so if that kind of resonated with you, I, I think you're in the right place, uh, as we're going to kind of dive into aspects of that today. Uh, so I'm Chris. I'm a data scientist at Netflix. I'm joined today by Tushar, my colleague, who we'll hear from a little bit more later. Um, so over my 12 years at Netflix, uh, I've been able to have the pleasure of seeing our product evolve, uh, from shipping DVDs and little red envelopes to streaming video and now into new frontiers of live streaming and video gaming, and over those 12 years our products evolved, but so has our infrastructure and our infrastructure has grown increasingly complex, uh, to handle not only the scale but the complexity of the product. To further kind of challenge that is we move at a very fast velocity and as mentioned we use AB testing as that engine of product innovation and this empowers our team to be data driven to validate ideas and to ship features fast. And we come to a little bit of an interesting challenge is that we tend to have a mismatch in our granularity when we look at how we report infrastructure metrics and costs which is gonna be say at the service or application level but when we do innovation we're working on like a feature or experimental level and so we have this fundamental kind of data and cost attribution problem. Uh, we can't just look at our Amazon usage and, and cost data and get an intuitive understanding of how much are our experiments and costing us, but also how much will a future cost us if we want to think about the future. And so we have spent the last couple years building a system to try and resolve this tension and to solve this problem and so we're excited to be here today to share this with you and tell you a little bit about our journey. Now to set the context, I wanna kind of make an illustration of uh like a hypothetical feature and AB test. Um, this should give us a little bit more context about the problem we're trying to solve. So let's imagine we have an idea for a brand new feature. This new feature is called the smarter prefetch feature, and we believe by intelligently preloading and caching data, content, box art, and images on the devices. We can improve the overall experience of our members so we can reduce the app loading time, we can improve the overall responsiveness of the app. Remember this is hypothetical. Now let's say we run this experiment and by all traditional measures it is a big success. Our members are engaging more, uh, the overall experience feels snappier and so we decide to roll this out. We decide to launch this to all of our members so you can have moments of joy. A month or two later, a different story though starts to emerge. One of our finance teams takes a look at our monthly aggregated costs, and they have a visceral reaction. They notice what appears to be a little bit of an anomalous increase in our total compute costs. And they have really no immediate reason why this is out of trend for it and they don't have any context regarding what has changed. Now in addition to that, there's a engineering team that's responsible for a mid-tier service somewhere within our complex environment that has also started to see an increase in demand for their service and application and similarly they don't have an idea or an understanding of why. And both these groups or neither of these groups really suspects that it's due to this feature, this smarter prefetch feature, one, because they lacked the data to understand that, but 2, and more likely is they weren't involved in the experiment or the feature development at all. And so we have a fundamental problem or disconnect here is that on the surface and by traditional measures we have a very clear product win, however, there's this hidden impact to our infrastructure that is meaningful to our business. And so what we would like to do and our problem that we're trying to solve is to prevent these hidden costs, to prevent these surprises. We don't want our finance team in a monthly review asking the question what caused this. We want to give them the data, the tools, as well as our experiment owners and service teams, what they need to know to make better product decisions up front and what that impact on the business is going to be. And so this hypothetical example or scenario led us to kind of build a framework, uh, and we're gonna share that with you today. So over the next 40 minutes we're gonna take you on a little bit of a journey uh regarding our approach, but first we're gonna kind of set the ground and talk about what we call the core challenge, and this is that at Netflix scale attribution becomes an increasingly challenging problem. I'm gonna then hand it over to Tushar who's gonna deep dive into our framework and talk a little bit more about how we do this in practice. We'll discuss some of the outcomes and how we take this data and turn it into decisions and how we use it to make better product decisions, balancing the trade-off of innovation and efficiency. We'll talk a little bit about the real lessons we learned along the way, the hurdles we had to overcome, and give you a glimpse of the roadmap for us, what is in the future. And finally, we'll leave some time for Q&A. If you're interested in some questions at the end, feel free to raise your hand and someone will come by with a microphone, and we'll try to avoid your questions as best as possible. I'm just kidding. Um, now, to understand generally our approach, uh, we need to understand a little bit more about how we approach. Experimentation at Netflix as well as kind of product testing. And so we rely heavily on AB testing as our engine for innovation at Netflix and so our teams are testing everything from simple UI tweaks to new recommendation algorithms to other changes across the product every day. And we don't run these experiments one at a time. There are hundreds if not thousands of experiments running simultaneously at any given time. And all of these experiments tend to operate on shared infrastructure, that is our shared cloud infrastructure. And in doing so we we have this interesting challenge which is that when these experiments are running on shared infrastructure, the requests and information from each of our members whether they are in a test or not, the requests are fanning out across this very large and diverse ecosystem we have so they're touching systems for authentication to get recommendations to get their synopsis, etc. And because of this, it can be very challenging for us to then try to attribute or associate kind of the impact or usage of any given test or members in a test to the actual infrastructure and that's because this infrastructure is not only supporting one test, it's supporting hundreds or thousands along with our regular just baseline production traffic. And so we have this granular attribution problem is we wanna be able to link for any given experiment, we wanna be able to attribute its usage and eventually cost back to its impact on our infrastructure. And so once we're able to solve this fundamental attribution problem, we're gonna have a better idea of the impact to our business. Now, to kind of work in reverse here, what we have built is that we have built a framework to solve this problem that is developed around an attribution and projection framework that we call for cost-wa decision making. Um, at Netflix we run at the scale of, you know, like I said hundreds if not thousands of tests, and so we've had to build this automated framework that in real time, day to day is monitoring the impact of all of these experiments and converting their usage into a projected impact on our business and we have showed this year in a mock dashboard of uh one of our experiment results. Where a test owner or service owner that's running a new product experiment. Gets to see the impact on our core business metrics. For example, in the hypothetical experiment we had the smarter prefetch, we see that there is in the middle column, there's an improvement in our overall streaming. People are engaging more. However, we see right beside it now we see that there's a cost, a cost to the business of $100,000. And what this is, is this is the cost if we are to productize this, roll this out from 1% to 100% of our traffic. Just to be clear, this isn't the cost of running the experiment, this is the cost if we decide to roll this out to everybody. Now this type of information allows us to move from like reactive decision making as we saw before we're reacting month to month when we have these surprises in our bill to being a lot more proactive and we can make a lot better decisions when we have this information up front as well as when it's presented aside our core business metrics. You can think that our test owners now have the price of a feature uh right in front of them and so that they can make better decisions trading off innovation and cost efficiency. And so at the end of the day this has allowed us as a company to start to think about like what does a successful experiment look like? How do we balance uh with so many experiments, you know, velocity, cost, and etc. Now One of the interesting challenges we have is that you know with a company our size our infrastructure is very diverse and heterogeneous. I imagine for a lot of people in the audience you have very similar infrastructure and our infrastructure spans, you know, machine learning pipelines, internal tools, everything to the core critical infrastructure that powers our consumer product. And so from day one when we thought about solving this problem and approaching this framework, we knew we couldn't boil the ocean. We had to start somewhere and that place we decided to pick was our consumer facing systems. The reason we picked these systems to start with is because these tend to auto scale dynamically in response to changes in our customer behavior. So as people get engaged more as they, uh, launch the app more, our tend systems tend to scale dynamically with that, and that has an impact on our cost. So with that context, I'm now gonna pass it over to Touchard, who's gonna walk through us the actual details of how we got to designing and launching this framework. All right. Thank you, Chris. Hello everyone. My name is Tasha, and I'm a senior data engineer at Netflix. I would like to take you behind the scenes at Netflix, and now I will admit it would have been nicer if it was for one of the famous series like Stranger Things that released last week, but for now, we will take a dive into the cost of air experimentation. This is the framework that allows us to connect the dots between product experiment and the AWS bill. Before we get started, here's a quick overview of what's ahead. We'll start by discussing the two main components of the framework, then we will take a closer look at attribution, which we also call as traces to signals. We, we'll also go into the foundations of distributed tracing, which is a key pillar of our approach. Then we will explore the estimation side of the framework, and finally, we're going to see how we put this framework end to end into practice at Netflix today. So let's start with attribution. When we run an experiment, we have a treatment group, which is the set of users who are experiencing a modified or a new feature experience, and a control group, which is which is a set of users who are experiencing a standardized or a default experience. As users from both the groups interact with the Netflix application, we collect a wide variety of metrics across different layers of the system. Sometimes these interactions produce a noticeable or a statistically significant change in the key business metrics. Attribution is this process of determining the root cause of these changes, essentially identifying which action or which feature led to that specific outcome. Within the context of our framework, attribution helps us pinpoint what's driving a shift in the infrastructure metrics, which ultimately translates into the AWS cost. Now once we have measured the significant change, let's move on to the estimation. This is where we use machine learning and statistical methods to to estimate the estimate the true impact by translating these metrics into, into key business relevant insights. In short, estimation is about translating these technical changes which we saw in the attribution. To the quantified impact which helps us determine the true business effect of an experiment. Now let's with that get into the details of the attribution. As we mentioned in the earlier slide, the goal of attribution is to identify a core significant change. We also saw that to identify that core significant change, we need to have members interacting with the Netflix application. Let's call this interaction by members with the infrastructure and the data collected as the usage data. The term usage here in the context of an online request response applications refers to the amount of work that is done by an application as it serves the traffic. So, however, the, the issue that we're dealing with here is we're, we're not only interested in the usage data, but we are looking to compare that usage data between a treatment and a control of an experiment. So let's rephrase that usage data to a usage delta. The graph here represents a usage metric. It's a generic usage metric for now between a treatment and a control. The important thing to note here is we're more interested in the delta between the pink and the green trend lines rather than the absolute values of those usage trends. More concretely, these usage metrics in the context of a request response application can be request latency, request payload size, request volume, or even error rates. So we may want to use one or more, or a combination of these usage metrics to achieve what we really want to to to get. Now with that, we have seen what usage metrics delta mean, but we also need to have an understanding of how we can use these to derive what we want to, and in this case, the change in the cost prediction. In the real world, most applications scale based on the defined policies by the engineering team. Here at Netflix, majority of our online request response applications scale based off the CPU. So that means an increase in a usage metric like a request latency, we need to add more server or we need to scale the application up to be able to meet the rising demand and bring down the request latency to an acceptable value. More importantly, the, this, by measuring this increase or decrease in the, in the application scaling, we can be, we are able to project that change into the, into the AWS cost change. An increase in the number of applications needed will lead to an increase in the AWS bill, or a decrease in the number of application servers needed will probably help the business save some money. So we've seen how usage metrics can be used to predict the cost changes, but let's see which metric did we decide to use. So after a lot of analysis and tests, we decided that most of our applications scale based off the request volume or the number of requests inbound to an application. This is mainly because our CPU is directly proportional to the request volume. So in the context of an experiment, if we measure the request volume for a treatment, and we also measure the request volume for a control for the same time period, we can use that delta to be able to project a cost difference. So now that leads us to a big question of how do we get the request volume, or more specifically, how do we get the request volume separately for treatment and separately for control. And this is where we enter into distributed tracing. Imagine a complex microservice architecture where requests are flowing from one application to the other, and this this architecture is for a single user request coming in from one of our members. So in this case, application 1 calls application 2 and 3, and they in turn call application 45, and 6 respectively. This architecture in the form of a distributed trace can be represented by something like this, which represents a single trace. It's a collection of these rectangles, which technically are called spans. A span is essentially a single request flowing between one application to the other. So for example, in this case, application 1 to application 2 returns into one span. A span itself will have two key properties, a start time, which is the start of the rectangle here, as well as the latency, which is denoted by the length of the rectangle. So in in this diagram here, the purple one has the maximum latency, and it's probably because it's waiting for all the other requests under it to be finished before returning a response back to the user. More importantly, a span also can have certain tags, which are nothing but the key value pairs, and they can be used to store information about the request itself, like the nature of the request, the payload size, uh, the status of the request, etc. or even about the application itself, whether it's the client side or the server side, like the node information or the location of the cluster. This single microservice architecture for every single request turns into something like this for at the scale of Netflix architecture, or more, more precisely for the scale of the experiment turns into something like this, uh, for the, for the scale of Netflix. So an important thing to note here is we are dealing with billions of requests which are flowing through thousands of our applications. Each of these requests, represents a user action, and the and the application may want to treat it differently, depending on whether it's an, it's from a treatment or from a control. The sheer volume and complexity is exactly why the entire process of attribution is so challenging, especially at the scale of Netflix, even though the diagram here represents blue for treatment and the orange for control, in reality, what we are trying to get to is we are trying to measure a single change that ripples through a massive interconnected ecosystem. Of course, tracing isn't without its own challenging. Given the magnitude of traffic that Netflix receives, the first and foremost is we cannot log every single request, and so we have to rely on sampling. And what that means is we only log a fraction of the traffic requests that come into the network's ecosystem. What makes it even more challenging is different business domains may want to have a varied sampling rate, for example, logs mostly at 0.05%, 1% for the web traffic, and maybe a higher percentage for sampling for emerging domains like ads. Even with these sampling rates, we have to deal with the inconsistencies in the data, with the spikes in volume. These may be because of a launch of a famous series like Stranger Thing last week, or a live event that is coming in. Additionally, as with the data collected from, from these applications, this data is usually unstructured, it's dirty, and mostly based off of the best effort from the application itself. And lastly, as with the other big data problems, we also have to deal with the size of data for processing itself, which is usually in billions of rows, even at a very low sampling rate. All these make statistical rigor and careful analysis even more important. So now that we have seen the attribution components, let's try to put them all end to end. We will start with getting the sampled data, which is collected via distributed tracing at the top left. We will all combine this with the anonymized member data, which includes the devices also. We will also use the application's metadata itself. All these data sets go through an ETL process where the data is cleaned, enriched, extracted, and, and made in a way that it can be analyzed. The final output here is the AB test usage data which is now ready for statistical analysis. So, to better understand and see how we use attribution in real in real practice, let's revisit the experiment that Chris talked about in the beginning of the slide. So as a quick reminder, the hypothesis was, if we are able to prefetch the content the user is likely to watch next, we can make the entire Netflix experience faster and more responsive. So for that, the treatment is where we are fetching the content the user is likely to watch next, in the form of text, images, videos, or anything else. Whereas the control is where the, the data is being loaded on demand as the user scrolls through the UI. So applying the attribution on this particular smarterrefetch experiment, we were able to identify the true impact of the, of the experiment. We found a clear and significant signal that was just not a random fluctuation. Members in the treatment group of the smarter Prefetch experiment made 40% more requests to one application in particular, and this was the metadata service. More importantly, even though we could have guessed that to see specific number something like this, based off the hypothesis of the experiment, it's important to note that this was not a hunch anymore. It was a data-driven result that provided us a clear direction. We had confidently isolated the primary impact of the business in a massive interconnected network and away from all the noise in the system. So moving on now that we have a significant signal from attribution, let's, let's go to the second component of the framework, which is the estimation. As the name suggests here, in this step, we will be using the significant signal obtained from attribution and convert it into the real prediction, and in this case, that's the actual dollar or the bills that are being paid to AWS. Only and only if the treatment is rolled out to the entire global audience of Netflix. So in the in the context of the modeler prefetch experiment, this means the 40% increase we saw in the request volume needs to be converted into the actual dollar figure if we if we decide to launch the the treatment to the entire global audience. Estimation is a 3-step process. In the first step, we learn from production. We here we train machine learning models based on historical production data. These models learn the complex relationship between the usage pattern, which in our case happened to be a request volume, and the infrastructure cost for each application. This is the core engine behind our cost estimation. The graph here represents the AWS hourly cost and how it increases with the increase in request volume for the metadata service. The second step is the simulation. Once we have the machine learning models for each application, we use these trained models to generate two parallel universes. The first scenario is where we get the cost based off the original historical usage. This is our baseline cost and is represented by the green T line in the graph for the metadata service. The second scenario is what is is what we imagine what the cost would be like if we were to apply the increased usage across the board. This is represented by the red dotted line in the graph here. The difference between the two universes gives us the incremental cost of the experiment which is denoted by this up and down white arrow. Now once we have obtained this cost difference for every single application, or rather every single application which had a statistically significant change, we, we need to move to the 3rd step, which is aggregating and analyzing these data. This gives us a comprehensive view of the total cost delta we can expect if the experiment is launched to all the users. Now applying this entire three-step process of estimation onto the smarter prefetch experiment, we were able to see a significant increase in the cost by $750,000 of a projection if we were to launch the experiment to all users. And as we had expected, based on the results of the attribution, where we saw 40% increase in the request volume, the biggest increase in this cost came from that single metadata service where we had seen the increase earlier. But it's important to know that now we know the true impact, or how much is the impact. For the first time ever, we can see the true cost of a product win. Now ultimately, this framework really, really enables Netflix to make Netflix smarter and data-driven decisions. There are two main ways in which users or Netflix consumes this entire framework. First, it acts as a safeguard, helping us catch and prevent any unexpected spikes in our infrastructure spending before they even become a problem. Something like Chris showed in the beginning, we will have a dashboard or even have alerts where people will be notified of a significant increase in projected cost. Secondly, it gives us the ability to to have meaningful trade-off discussion. We can directly compare the user benefits of a user of a new feature or even the key core business metrics with the projected infrastructure cost. This means we are able to make much, much more informed decisions even before rolling out an experiment to our entire audience. As we had highlighted earlier, the goal of the framework is not only to help reduce the AWS cost, but to be able to allow us to make much more informed, data-driven decisions. It's, it's mostly about having that operational efficiency with the innovation pace. And with that, I'll hand it over back to you, Chris, to take us through the lessons learned and the roadmap ahead. Thank you very much, Tushar. Now building the framework that Touchard talked about is, is one thing, but trusting the data is a completely different beast. So I want to share, uh, three specific learnings and hurdles that we had to overcome to make this not only production ready and, and viable, but also that our users could trust. And the first And foremost and probably most important is trace completeness. As Tushar mentioned, the idea of tracing and attribution is the core to this entire framework. And if there is something wrong here, then everything else is questionable. And so our attribution framework is based on the assumption that we're able to trace the requests of a user throughout the entirety of our infrastructure. Now what happens if one of those pieces of our infrastructure service doesn't propagate, let's say trace headers well that ends up in a blind spot in our infrastructure. We're now missing parts of our ecosystem that is, we are blind to big parts of our infrastructure spent. Within the context of our framework that what that means is that we may be underreporting costs or you may be underreporting the impact and so attribution and trace completeness here is a very crucial step for us in building this this framework. And to do that, at least at Netflix, there is no silver bullet and so we had to work very closely with our engineering teams, specifically our observability engineering team, to ensure that we had accurate, uh, request tracing and on our side that included setting up automated data audits, alerts to tell us when we had kind of these gaps or these potential visibility issues. And so if you're interested in attempting this within your own organizations, this is where you wanna start you wanna start off with getting high quality infrastructure tracing and or attribution in place. The second is that our infrastructure is always evolving. It's always constantly changing and drifting. And we knew this ahead of time that we were going to be chasing a moving target. Now our teams at Netflix move fast as we had mentioned prior, and these teams are also upgrading infrastructure constantly. Teams may be upgrading EC2 instance types. We may be migrating service architectures, and even the pricing can change. And so we had to take this into account. We couldn't build just one model and apply it across all of our services. One, we have a variety of different services, but also two, the behavior of those services change over time. So if we had a single model we built once, it would be out of date very quickly. And the second challenge is that with an evolving infrastructure, we need to be cognizant of kind of edge cases and we'll talk a little bit more about that in a second. But to address the first case, we ended up implementing continuous retraining for the machine learning models. In this case, we're retraining our models daily or even sometimes even hourly uh to ensure that we capture the the most recent state of our infrastructure. Now with regard to those edge cases we also had to implement robust fallback mechanisms within our modeling and to do this we are able to kind of fall back when we detect drift in our infrastructure to kind of simpler heuristic based cost models. Now they don't necessarily have the precision of the ML models, but they are directionally in scale kind of accurate. And so this was important to us because at the number of tests we're trying to inform as well as the rate at which our infrastructure is changing, we want to always be able to produce an estimate for our end users. And finally, one of the other hurdles we had to overcome was that not all of our services are created or scale equally. And so earlier we had mentioned that we decided to focus on our kind of consumer facing product and the reason we did that is because they mostly auto scale, but the reality is a little more nuanced than that and even within our online consumer facing applications, some of the applications are statically provisioned that is with an increase in demand there is no change in their cost. There can be a few reasons for this. Maybe they are scaled up to a high water mark to be able to absorb, you know, uh, bursts of traffic, or maybe they're just over scaled and they're being, uh, they're not as efficiently scaled. Now, in contrast, you know, to our dynamically auto scaling workloads that where there is this like clear relationship we can learn from, in the static case, there really isn't a relationship we can learn. The cost is almost always constant until we hit a tipping point. Now to make sure our framework was able to handle these cases, we built some logic to be able to classify our services based on their different scaling behaviors so we were able to say that this service is an auto scaling service, this service is statically provisioned, and etc. and the framework is able to use this then to perform its cost estimation and modeling. And this really ensures that we only attribute costs where there is an actual like increase in usage that is going to drive our bill. Now overcoming these kind of technical hurdles was kind of a significant effort, um, but over that time when we were solving this what we realized that this data that we're collecting and these models we're building can also unlock a whole other use case for operational excellence. And the first is that the data can be used to proactively perform capacity planning. The same usage data that Touchard talked about where we're collecting information at an application level and estimating the difference can be useful to helping plan for the rollout of the experiment from the service perspective. So for example, imagine an experiment where we are going to be implementing a very new computationally. Uh, or expensive, uh, process using the framework we're able to within the experiment understand what its potential usage pattern is gonna be and turn that into a potential capacity need. This is we can turn it into a proactive discussion, whereas before we would launch the feature and potentially have a situation where we might be under scaled. So the power of the framework is that as we collect this usage information to project cost, it's also very valuable for helping inform capacity problems as well. The second is that it also enables what we call shift left validation. And so imagine a case where you have a product team implementing a new feature and due to many layers of abstraction or obfuscation they take a dependency on a legacy system that they shouldn't have. Now before our framework came along we would roll this feature out and they would now have a dependency on a service that we're trying to deprecate or worse that wasn't scaled correctly to handle the demand. And given that this framework and the attribution model we designed allows us to get insight into the usage across our entire environment, the test owner and the service owners involved in the product experimentation can now have insight regarding kind of their architectural implementations and so we're able to bring this idea of testing not only just our actual product feature but the infrastructure implementation into the actual experimentation phase as well. And finally, with this granular information as Tushar mentioned with usage, we're able to get better performance insights. This goes beyond just the performance of the test on the business. Are we improving streaming? Are we improving retention? This gives us insight into the health of the actual experiment and the feature in of itself. Recall back to what Touchard had said about measuring latency. As part of this, we can now measure what is the impact from a latency perspective of any given feature. This allows us to tie the impact on the infrastructure to now the member experience, giving not only our test owners but our service owners a more holistic picture of the impact and health of the future. Now this project has been a journey for us and while we're still in the middle of it, the core framework we presented today offers a blueprint that you can even get started to today. And so as we talked about we focused primarily on our online consumer facing systems because they mostly auto scale with demand. however, that's only a fraction or a small picture of our overall cost and if our goal is to really understand the total cost of innovation, then we need to increase our scope and include other drivers of cost. And our next step in that journey is to now consider kind of storage costs. So while compute is transitive, storage is mostly cumulative. A new feature may be increasing storage costs, or more likely a new feature will require new amounts of data. Think for example like generative AI. So in these cases we want to be able to expand our framework to not only include our online Staples services but also take into consideration data storage technologies like Cassandra, S3, EBS, and etc. and this requires different attribution models and mechanisms because we can't necessarily rely on tracing, as Touchhar had mentioned, to be able to tie this all together. And the second area that we want to focus is on our batch services and arguably this is a little more challenging. Our batch services power everything from, you know, offline machine learning algorithms to new recommendation and personalization systems, and this is a challenging area to approach given that a lot of these systems are disconnected from our consumer product system. However, with the rising costs of GPUs and other technologies, this is a crucial area for us to expand our framework into, as these are contributing greatly to our overall spend. Now as we work to apply these methods and attribution processes to our kind of new domains, we want to leave you with a few key takeaways if you're interested in getting started and how you can kind of make this work in practice. And so first Is that attribution is extremely important as we've discussed throughout the presentation, this is the foundation, the bedrock to everything we've discussed today. If you want to start to untangle and understand the impact of an experiment on your infrastructure and then ultimately project that, you need to start with attribution. Cloud costs are typically at the granularity of applications and and services, but features are at the, you know, innovation happens at the feature level and so there's a granularity mismatch there. The second and kind of more interesting is once you have the attribution in place you want to turn that into as Touchard mentioned the cost of success. You want to be able to use your attribution data to then project into the future what is this feature or this product going to look like when you roll it out, and this really does help transform the decision making from, as we mentioned, a reactive process to more proactive. It helps teams then balance the trade-off of innovation and efficiency. And third is that you need to cultivate. An environment of shared ownership with this data, so you don't wanna lock this data within dashboards and in finance reports you want democratize this information to uh all the users who are gonna be using it or should use it to help make decisions. And so you want to be able to place this information right beside key business metrics as we showed in our mock dashboard to allow test owners and service owners the ability to make those smarter decisions and you don't have to police them. And so in closing, uh, kind of what we presented today, uh, is a framework that has allowed us and enabled us as Netflix to redefine what we think a successful experiment looks like, helping us balance those trade-offs of innovation and efficiency and bringing cost into the kind of the decision framework, and this empowers our teams to, as we mentioned, catch issues early and to prevent surprises to our bill. And I think most importantly, it turns cost into a design constraint allowing us to build better products. And with that, thank you very much.