---
video_id: y39NHZhARfo
video_url: https://www.youtube.com/watch?v=y39NHZhARfo
is_generated: False
is_translatable: True
---

Hello Thank you for choosing to attend my session. Um, I'm gonna start with a little story. So about 10 years ago, uh, my grandmother was living in independent living, and she had a fall in the middle of the night, and no one got to her until 678 hours afterwards. She broke her shoulder and was never the same again. So. She had a button on the wall to ask for help, but obviously if you've fallen, you can't press the button and ask for help. So um that's part of the inspiration for my start-up which I'm gonna talk about today. So we're gonna talk about the problem we're solving, which I've sort of, Given you an overview of that already, um, we're gonna go through requirements, persona gathering of, of who cares about this sort of product, uh, we're gonna talk about minimum viable products and minimum lovable products, are they the same thing? Well, let's find out. Uh, enriching events. Uh, it's all about data, it's all about events, you need to know where these events come from and which systems they've passed through. Uh, data partitioning will cover a bit of real-time escalation and some key takeaways. So the typical aged care room. Is basically has some tech, but it's pretty antiquated, so you've got pressure floor mats for identifying when people are walking in certain areas of the room or falling out of bed. You've got uh bed alarms, if someone sits up in bed or falls out of bed, you need someone there pretty quickly to look after them. Um, and there's infrared switches as well, so they can be put on doors to know if someone's gone into a bathroom or whatever. The problem with all this tech is that it's old and it's, uh, you know, it can breakdown, and these are very expensive things to put, put into, into an aged care facility. So how can tech help? So I've drawn a diagram of an aged care room. They're all the same, like similar layout. So you've got a bed and you've got a bathroom. Most falls happen at night, and it's the highest rate of injuries for elderly people over 60. So the tech that we're gonna put in is a millimeter wave radar device. It looks like a little smoke alarm. It goes in the roof, and it emits a radar, and it bounces off bodies, objects, uh, in the room. And so we can identify the XY coordinates of where that person is, and how many people, and the Z coordinates. So center of mass, so you know if they're up or you know if they've fallen. So there's machine learning on board which can send an event to know whether someone has actually fallen and he can get help. So understanding your, you have two personas in an aged care facility. You've got the facility manager who cares about reporting, uh, prioritization of care at the right place at the right time. Uh, they also need to record falls for compliance purposes in most countries. A nurse. Needs to understand when someone is fallen and when they need help, and again prioritization of care, so both requirements, you know, the aged care facility manager, most of those needs can be done by a batch, all the other ones need to be near real time because, As soon as someone falls, so someone needs help, you need to get to them. The current tech doesn't allow for that, but it does. So IOT core is a key piece of this infrastructure, so the device talks to IOT core to send events. So what is IOT core? It's basically an MQTTS, uh MQTT web socket compliant endpoint, managed, you don't have to worry about managing your own MQTT endpoint. It provides all that for you. An IOT device publishes an event to this endpoint. The, there's rules that you can configure on how to deal with events as they come in, and then you have actions which gives you ways to process these events. So an event payload looks like that without the bullet points. Bit of a technical error there. Um, you have a device ID, you have a location in the room, so I said X Y coordinates, and Z, that's them, in relation to the device. Ah, you get an event name, what was the event, the time it happened, and the type of event. So a minimum viable product is when you spend the minimum amount of time coding something. So I used a Graffaa dashboard, so I needed to get on board with a vendor who provides this device to make sure I could be a partner. To do that, I need to show I understand their products and I can query the data. So this Graffaa dashboard is literally just wired up to Athena and Cloudwatch at the back end. Uh, cloud watch for obviously the, the little graphs there, um, and there's some reporting from Athena. Very easy to wire up Grafaa to AWS services, and it's a very easy way to show a dashboard. I also did a little demo video, um, my son features in that, and, um, falls. Well, he will fall, there we go. He gets up out of bed. And he falls and then there's an alert that comes through to say, so all of that was done via Graffana dashboard and the lambda function at the back end. So I like architecture diagrams, so this is the original architecture diagram of what I built. So you can see that there's a um the customer device talks to IOT endpoint. And it goes through a uh transform lambda function to send an alert, so that was the alert. There's also cloud watch logs, the alert, they're also going to cloud watch logs there's adjacent payload and something not as well-known, you can have cloud watch log metrics based on these logs which don't cost very much and have your own custom metric filters, very useful. So that way I can get those graphs of people being in bed, you know, custom things, people in the room, out of the room. And there's a, uh, persisting the event obviously for querying from Athena, uh that goes via firehose to uh estuary. And then obviously the Grafaa dashboard is consuming all these things, there's a little database to say who should get alerts and, That's basically my MVP. So an MVP allowed us to understand the data, be able to query it intelligently even. Um, answering business questions, so, you know, I can show it to customers and go, what sorts of things do you wanna see? So it helps, help that dialogue discussion with with potential customers. Um, not so good. In this case, the data is only set up for a single tenant. I can't scale this, I can't scale Grafaa dashboards. Um, so I need to solve those problems. So one of those problems was adding tenant contacts to that payload. Because that device doesn't understand the concept of tenants, it understands itself, a device, but I need to put it somewhere. So I had to um grab a lambda function to enrich that payload, so that data goes in, it gets enriched on a lambda function, and I'll show you an example of the enrichment at the in the next slide. Oop, um, we have. But then that republishes to a different um MQTT topic path which has the tenants at the beginning of that path, so I can easily query, I can see all the events coming through for the right tenant. And then a rule can grab that and I've got a separate fire hose for each tenant keeping every, all tenant's data separate. And then I've got an SNS as part of that to be able to do real-time processing for those events that I care about. So this is an example of ah querying the event stream when it comes in. Uh, topic 4 in this case is basically you count from the slashes, so before the slash is 1, the first plus is the tenant ID. Devices is number 3 and then after the device is a device ID so that's topic 4, so that gets put in as a device ID in my payload, and then you can see I've got a fire hose delivery stream there for topic 2, which is the tenant ID so I've got 11, fire hose for every tenant, keeping the data separate. So an example of an enriched event, you can add a meta key or something and you can add to that as the data goes through your pipeline. So it gives it time, where it got enriched, some trace IDs, etc. So when you're partitioning this data in S3, there's two ways to do that. There's the hive style where you label the partition, so year equals 2025. That's the traditional way of partitioning. Um, it's the standard way. In our case, uh, There's also non-his, so where you're doing partition projection. Examples of that is cloud trail, uh, ALB logs, a lot of AWS services use that second method. So basically the the the difference between the two, if you're doing hive style, you have to manually add that partition. When you go into a new day, ah, new month, you need to fix that, make sure it's scanning that new partition. In partition projection, you can actually configure Athena. The what what your partitions are, whether they're dates or whatever, and it can work out, you don't have to scan it, you don't have to add partitions, it's all added for you. So the second method is, is, is the best way to do it at scale, so I don't have to worry about missing partitions and missing data. So you can figure that Firehose can do dyna dynamic partitions. So I just pull the year, month, day, and device ID out of my payload, so any Jason payload that goes through Firehose, you can pull out that information and use that as part of your partitioning. So that's what I've done there. So I tell it, this isn't the path where I want to store my data. After doing that, I need to be able to um that yeah and that's what it maps to in my case, so you've got all the presence alerts for someone walking around, you've got a date, I've got a device, and then I've got data. So I can query by device, by date, very easily. And then to to be able to query that non-Hive partition format, you just put a configuration into uh Athena in the create table and you tell it what the format of the partition is, it's days, you know, valid dates, all that sort of thing, and then it, it knows the rest. You don't have to do anymore configuration. So the final part of the solution is falls escalation. Um, that's a key point because you need to be able to escalate to different levels of people to make sure that someone gets to the person who's fallen. So the falls alert will go to a an uh SNS event topic, and you can actually have a queue that listens to a particular filter policy. So in this case my payload type is a fall. I only care about fall alerts to go to my queue. So my SNS topic will go to a queue, um, it goes to lambda function. And then it goes to a Dynamo DB table. And then I have a listener on the Dynamo DB table, which is an event bridge pipe. So one of the good uses of pipes is that you can, you used to have to use lambda functions for this. In this case, you can configure the pipe to listen to the stream, um, you can do some enrichment, I'm not doing any enrichment in this case, but I'm kicking off a step function flow, so it's a good way to wire these things up. And then your, the step function flow looks like this. Um, for the escalation, so about 6 months ago AWS updated step functions console to provide. Why? Seeing the data, or seeing your different steps of your step function flow, and that's a nice sort of visual way of seeing my path. So the data goes through the first level, there's a delay, and then I'll go to the 2nd level, and then there's a delay and it'll go to the 3rd level. Um, if someone acknowledges that alert, the step functions will finish, it checks to see if someone's acknowledged it first. So I, Originally it was a Grafaa dashboard. The last few months I've been building out the um the website, and this is the settings screen which basically shows the front end user interface for the escalation flow. There's reporting via an activity timeline so you can see if someone's in bed, in a chair, you can get lots of dates and times of people moving around, you can. This is very useful data, especially in an aged care facility to understand patterns of movement in a room and also potentially to, you know, if someone's doing something abnormally, you can actually send an alert to someone to see if they should be checked. And then there's a dashboard as well. So the key takeaways when you're doing a product, so that I call that a minimum lovable product. So minimum viable product is your quick and dirty Graffaa dashboard. Minimum lovable product is your don't call it a pop, don't call it anything like that, it's the product that you want your users to use. Listen to their feedback, um, and build it out based on what they want, not what you want. So going low code is fast, um, getting buy-in, building some quick dashboards, everyone loves dashboards, everyone loves data, um, utilizing cloud metric filters to have custom filters for data that you care about, very underrated and very easy to do. Understand your batch in real-time needs. You could see having real time actually adds a bit of complexity, so you only need that for the things that you really need real time for. Most of the time batch is enough. Um, don't over build for real time when it's not needed. There are other ways to do that, but this is the way I've chosen to do it. Keep it simple and only build complexity in when you need to. Um, visualizing the event's super important, um, and enriching your event payloads. So make sure that wherever your event goes in your system, you enrich it with a date and time and, and what, you know, added to that event, because one day you'll wonder how this event got somewhere and you'll need to see the, the trace. And very useful for debugging later. So it's saying I like an MVP proves it works, um, an MLP makes it indispensable, or at least that's what I hope. So, I also have a podcast and I also have a co-founder who couldn't be here today. So this is her part of the presentation. Um, so come and listen, we talk about cloud and AI topics, um, and, uh, yeah, without, without Georgia helping me, this product would not exist. So thank you very much for listening. um I hope you can take something away from this. And if you've got any questions, I don't have time to take them here, but come and see me afterwards. Appreciate your time, thank you very much.