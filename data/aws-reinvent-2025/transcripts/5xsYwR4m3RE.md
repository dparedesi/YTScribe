---
video_id: 5xsYwR4m3RE
video_url: https://www.youtube.com/watch?v=5xsYwR4m3RE
is_generated: False
is_translatable: True
---

Hi everyone. Um. Have we got any F1 fans in the audience today? Lando fans? Max fans? Oh, look at that, Hamilton fans. I won't go through the whole list. Um, hi everyone, my name's Nick Morgan. I'm a solutions architect here at AWS um on stage today with me, um, I'm gonna be joined by Jamie Mullen, a specialist essay for Edge Services. And I'm delighted also to welcome Dave King, um, head of digital er technology at Formula One. But before we sort of get into the meat of the presentation, um, just want to introduce Formula One really and mention that they're celebrating 75 years of this fast-paced sport. And, and it's been truly a sort of a global sport, the season runs for 24 races, um, in 21 countries and has become what we think is one of the greatest spectacles on Earth. Um, with a cumulative audience of 1.6 billion people and over 825 million fans worldwide, the sheer logistics and infrastructure is, is, is truly mind blowing really. If you've ever been to an F1 race and seen what goes on, it's, it's amazing. Um, but with today we're gonna tell you about a couple of examples of how AWS and Formula One deliver not only world world class operations, but also, um, deliver. The racing action direct directly to fans through the innovative world of multi-screen TV. AWS started working with Formula One back in 2018, uh, became a global sponsor for the 2019 season. Um, the partnership was renewed in November 2022, and we currently operate at the top tier of partnerships within Formula One. AWS works with many other sports leagues, so things like the NFL, PGA, Bundesliga, but the, the, the unique thing we saw about F1 really is around, and at the heart of it is the data, and it's been the, the heart, data's been the heart of it since the 1950s. So with Formula One, we sort of focus on deep collaboration in three main pillars. Uh, Transform transformation of data into racing intelligence, fan experience enhancement, and also technical transformation through the joint technology funds that we run with, with F1. Just to pull out a couple of examples along the journey together, so back in uh late 2019, F1 and AWS completed the CFD project to design the 2021 car, reintroducing the ground effect era to Formula One. In 2021, F1 Insight powered by AWS was born. The Insight graphics are displayed during the race and provide key data points allowing fans access to not only entertaining events during the race, but also key metrics and predictive analytics. We expanded the partnership in 2022 and started work on the FA 360 program. Um, in terms of Gen AI, I'm sure you've all, you, you know, the fans out there will know that we produced the trophies for the Canadian Grand Prix in 2024. And then more recently, um, early 2025 saw the launch of F1 TV Premium, a multi-screen over the top platform. Also in this year, we collaborated on the launch of root cause analysis. Um, later we'll tell you a little bit more about these, these projects. So as you can imagine, F1 is a really data heavy sport, and Formula One kind of represents what we think is the, you know, one of the most data intensive sports in the world. Each F1 car, um, generates over a million data points per second. And during a typical race weekend, we process more than 5 million data points across all cars. This is roughly equivalent to streaming 1500 high definition movies simultaneously. The mass of the data um delivers uh insights, innovation, and we fundamentally feel we've transformed the way how teams compete and how the how fans experience the sport. So how is the data captured? To illustrate this, we're gonna use the er circuit in Italy, the Imola circuit circuit in Italy. And if you look at the topology of the track, um, about every 200 m or so there are induction loops around the circuit, and these make up the segments that ultimately make up the sectors that the timing's taken from. In addition to this, there are radio towers, these towers receive the data from the car. That data includes live telemetry data and also the onboard video. If we move over towards uh what's known as the Event Technical Center, this is connected to the circuit via high speed optical links and think of the ETC as like a mini data center, but it also houses a video production gallery, and, and this building's portable and follows the racing season around the world. If we then take the next step on, uh, we go towards the Media and Technology Centre, which is based in Biggin Hill in Kent. Again, uh, linked through high-speed fiber optic link. But in this building, there's a full broadcast suite, including a video production gallery, there's 3 studios, and there's also space to accommodate, uh, commentators and also events. Um, the MNTC, as it's known, also houses other departments that take care of things like, um, IT, so on-prem and cloud, racing systems, track systems, and engineering. So, how does that connect to AWS? So the MNTC connects to AWS via Direct Connect, and these links are used to send a subset of the telemetry data and the finished video feeds. These feeds include the world international feed, all on board cameras, and some pre-canned graphics feeds. This is where we ingest the video content for F1 TV. The telemetry feeds also include sanctioned data points that we use for processing predictions. So now let's take a look at what goes on in AWS. So, we use a mixture of machine learning and mathematical models for F1 insights. Amagen Sagemaker is really the inference engine that sits behind this. And each model has been designed and trained to meet the needs of the F1 broadcast. The MR models are also retrained between race weekends to improve accuracy and keep pace with the drivers and the way the seasons developing. So all models are stored in S3, ready for when the data arrives, but that's not the only use of S3 for Formula One. As I mentioned before, the customer 360 data lake sits within that. And this is a single source of truth for fan profiles and interaction data that's generated across the digital estates, so things like F1.com, F1 TV and other channels. SM S3, Amazon S3 is also used for the video archive, and there's over 7 petabytes of historical data that sits within that archive. So if we take a look at the inference pipeline, This is where the data meets the model and produces new insights. In reality, the data is sent via Amazon API gateway. Under the covers, the code is executed with AWS lambda jobs and the results are stored in Dynamo DB. The code that interacts with the ML models ultimately returns the results via API gateway to the Media and Technology Center. Amazon Dynamo DB is an OSQL key value store as you I'm sure you're aware, but it holds the results of the analysis and it and all of the model training. So this allows um F1 to further enhance the model's accuracy and play back the data to support all the simulation activities. Amazon CloudWatch is used to store log files for operational needs, and interestingly, the data that Amazon CloudWatch is driving at the core of these insights helps um F1 keep track of how these discrete systems are performing across the organization. Having processed the results, the results are now available to the video production teams. This insight data is sent out to the various departments within the, within the MNTC. So the graphics department, for example, prepare the graphics and queue them up ready for the, for the the production of the world international feed, and then the director can select which insights help tell the story on the day if you like. So bear in mind the end to end processing happens in under 1 2nd, so from the car to the track through into the ETC MNTC into AWS process and then back to the MNTC where the graphics are produced ready for for the broadcast. So it's really important for this data to be accurate and really sort of help tell the story of the race on the day. So, this leaves us with a business challenge, as it always does. This is a high stakes environment where everything must work in unison to deliver the racing action to the fans, and given the interdependency and complexities of these systems, being able to fault find a pace is critical for F1's operations. So, let's introduce root cause analysis. RCA allows F1 to quickly fault find and gain critical insights from cloudwatch logs from the applications that underpin each race. The project was initiated through the leadership of Ryan Kirk, head of cloud and DevOps at Formula One. The goal here to harness the power of generative AI to deliver a step change in intelligent root cause analysis. So, let's take a look at how RCA works. Utilising a multi-agent orchestration with supervisor and specialist agents, F1 were able to onboard over 500 applications into this chatbot-based system. Keeping a knowledge base of each application and centralizing all logs into Amazon CloudWatch, we implemented a solution that can perform automated analysis of specific applications to discover faults and report those faults back. With results evaluated and summarized, support tickets created. And uh the the train of thought with the LLM stalled for further and further and later analysis, this system has vastly improved the discovery and resolution of issues. Built around Amazon Bedrock and other AWS services that collate and process the data. Press the button. Um, the vast majority of applications run in Cubanettes and these applications can auto enroll, um, with, um, the, the RCA tool, identifying their core components. Bedrock takes over the Agentic orchestration, calling supervisor specialists to process tens of thousands of rows of log data to find issues and anomalies. So let's take a local, take a closer look at a query. From the chatbot, a human support agent can ask the, er can ask it questions. In this case, we are asking about a database that supports the FO API or the Formula One management API. The first thing that happens um is the application configuration is pulled from a lookup table, metadata about the application's infrastructure and cloud watch log locations are pulled. This information forms part of the initial LLM query and a suitable prompt is sent to a specialist support agent. Each specialist agent is configured to support certain types of queries, such as interact with CloudWatch, connect, um, sort of test database connections, and trawl through things like network and firewall logs for analysis. Results from each specialist agent are sent back to the supervisor. For analysis and processing really to work out what the big picture is essentially with the problem. In this example, Amazon Cloudwatch database connection and network logs have been queried and the following issues have been identified. There's a configuration error in in Kinesis, there's a broken database connection and also an overloaded network device. The support agent then makes a determination, again based on the tuned prompts and creates a summary of its findings. At this point, given that there are errors, a support ticket is generated detailing what's been discovered. The specialist supervisor then returns its findings to the orchestration layer, again summarizing the issue and providing extra information about the ticket and root cause. Finally, the UI receives the findings and relays this back to the human support agent. The chat agent keeps the history of the chat for future reference and analysis of the chain of thought. So in summary, using a multi-agent approach, F1 have been able to realize a step change in RCA productivity with a 95% reduction in full ticket resolution time and the initial triage down from 45 minutes to just 4. AI can programmatically trawl through log files at a machine-level pace, finding issues quickly and efficiently. Automated ticketing puts issues into the system quickly. Alerting support teams across the organization. So harnessing the power of this orchestration, domain level specialist agents, and direct access to cloud cloudwatch centralized logs, and being able to describe your applications in terms of metadata, objects along with carefully crafted prompts, F1 have realized a step change in analysis for mission critical applications. So if you'd like to take a look at our prototyping blog, we've also recently had a blog published on Wired as well. Um, there's some QR code at the end of the presentation. And I'd just like to thank you for your time and I'm gonna pass the baton over to Dave King, head of digital technology at Formula One. This season, you're in the race, fully immersed in every Grand Prix live. Watch what matters most. Create your custom race view from multiple feeds. Look what he's done with the opportunity. Miss Nothing with Christine 4K. See what every team sees in every moment. This is an inexplicable crime. And analyze what really happened, why and when. Because these are cars that have 1000 horsepower. Stream all this and more on up to 6 devices. A new way to see a new era. He's done it. He's won it. He's done it. Hello everyone and thank you Nick for the intro. I asked for a little hype video. I find it's easier to start that way. Um, so I'm David King, I'm head of digital technology at Formula One. my team and I have the privilege of um designing, building and delivering change across our consumer facing digital properties. Some of them you can see on the, on the screen in front of you. You've seen a little trailer for F1 TV Premium. That's what we're gonna talk about in more depth, but we're gonna get there in a minute. So Formula One, we have responsibility for our F1 web, an app. Completely relaunched in 2025, um, bringing vertical video, completely restructured site, um, live blogs during race day, short form video, vertical video, written and editorial content. It's the home of, of everything F1 for us. F1 live timing, which is what you see here on the mobile device, is the service we use. All Dit and Nick talked about collecting from the cars, the telemetry, the positioning, the sector times, the mini sector times, the deltas between the drivers. All accessible as a second screen experience, whether you're consuming the broadcast on F1 TV through Sky Sports UK, through ESPN, through ViaPlay, whoever it might be. That's your second screen experience. You can see what's happening even if the broadcast isn't actively covering it. F1 fantasy, 2 games here. One traditional fantasy game, select your drivers, select your team. Select your boost for a given race, who's going to finish in the top three. So compete against your team, your colleagues, your friends in leagues. And then the F1 Predict game, and a little more ad hoc, 10 questions for each event, launches typically on a Wednesday before an event. Who's going to be on the podium, who's going to gain the most positions in the race, all of that kind of good stuff. Quite good fun. I'm not very good at the first one, not bad at the second one. and it's easier to dip in and dip out. Then F1 TV, the thing we're going to talk about. In more detail. F1 TV is a product that we launched back in 2018. It's our own internal OTT platform. It's available in 132 countries. Some, it's there on its own. In others it sits along alongside our traditional broadcasters. We work across all mobile devices, tablets, big screen devices, and on web. And that platform's been there, as I say, since 2018. We've always had access to all 20 cars on the grid. So all the onboard cameras, two international feeds. And 2 data channels that kind of support the viewing experience. Within that platform there are two subscription tiers, um, a replay tier or a, a full event replay and, and Vod content. And then F1 TV Pro which has always been our live experience. The problem with that live experience was you've got all of those feeds that you can view but we only let you switch between them so you could watch one of them at a time. So you could switch to watch Max um and take that P2 in in last weekend's race, um, off the grid, um, but you couldn't see what else was going on. So what we set out to do Was to build F1 TV Premium. And I should have clicked that twice earlier. So what is F1 TV Premium? In fact, Does anyone use F1 TV first off? And are you a premium subscriber? OK, a few. Enjoy it. Useful? You're worried about Disney? Don't worry about Disney. So what is F1 TV Premium? F1 TV Premium is something we've been working on for about 18 months, 2 years prior to its launch at the start of this year. We wanted to bring a really compelling multi-angle experience to our sport. We're in a pretty unique position to have access to all of the live feeds all of the time for every event. And it's different to multi-view for a single sport like football or American football where you can watch different games and it doesn't matter that things are slightly out of sync. So one of the key challenges we were trying to address here was creating that multi-view experience. By ensuring that everything was in sync, and sync is a thing that you will hear me talk about again and Jamie when he comes on to talk about the media services element of this. But there were a few more things that we added just to make this a little more complicated. So historically, F1 TV has carried HD feeds in SDR at 50 frames a second. The last kind of jump we made off of the kind we made here was moved from 25 to 50 frames a second. 3 or 4 years ago, but we set ourselves quite an aggressive challenge to go from HDHDR to full UHD HDR. And that whilst that had been around in F1 and through traditional broadcast before, anyone who works with media services and distribution and OTT knows that that step from HD to UHD is quite a large one. So that's step one. We also did the same for the on-board cameras, moving them up to HDHDR supporting multi-view, but we wanted to make sure we got that spread of devices as wide as we could with that same compelling experience. And then the less interesting bit but allowing that playback on multiple devices, so creating a multi-view on 5 or 6 devices at the same time. So I'm gonna play this video and I'm gonna talk you through what's happening, but this is on a tablet device. Showing multi-view in action, for those of you who haven't seen it, it puts in context a little more of what we're going to talk through. So this is um coming up to race start. Drivers have just finished the formation lap. And what you can see here is the international feed in the main box. And down the right hand side, a full 22, 23 tiles of video that are available to the fan to select. We've brought one of them on here, we've brought Lando's onboard camera on. I think he's in P2, P3. Um, and international feed is still continuing. Some drivers disappearing down the pit lane. Someone must have changed their engine. Um, or power unit And what we were trying to get to here, and, and this was the nervous bit. Is when those lights go out. Everything moves in sync. Because if it doesn't, It's a bit rubbish. And it gives away the story, you, you kind of eyes caught on one thing, you're misled by something else. So we've pulled up here. 3 different feeds, the international feed, the onboard camera, and the driver tracker, all of which should move at roughly the same time. So 5 lights. And out we go, and the international feed, the onboard camera, and the driver tracker all move at the same time. This isn't just a gimmick, this isn't just a thing that we built for this presentation. You can go back to any full event replay on F1 TV in this past season and go to that lights out scene and pull up your view and see, see that we did what we, we said we were going to do. So once we've got all of this video, We also have all of the commentary, all of the on-board camera audio, all of the subtitles, so it doesn't matter whether you're watching an onboard camera that you want to listen to. You might be interested in listening to Max's comms with his engineer. You can switch that on, you don't have to be watching Max's onboard camera. What we did constrain was the layout. It's probably the one kind of compromise we made. We could have been fully flexible, but. We honed this down to to give what we think is the most flexible and still compelling experience with the options that are available rather than a complete free for all of moving video all over the place and resizing. This is what we think works well. The swap of video is pretty slick as well. I didn't actually talk about that when it happened um. But it works really well. So We're gonna start talking about that video workflow because there are 3 significant pieces that needed to come together in tandem to make this work really well. The first bit Nick talked about in terms of capture at the circuit. So Nick mentioned that all the video for the onboard cameras, so cars traveling at 200 miles an hour down the straight, is radio frequency transferred off the cars around a fiber network at the circuit and back to our event technical center. So that's one element of what we carry on F1 TV. We then have 25 to 30 track cameras around the circuit also connected to that fiber network, also back to the ETC at the circuit. Great. We then ship all that back to our MNTC, our Media and Technology Centre back in Biggin Hill. And that's where the final cut, the production element of F on TV but also the broadcast happens. And then we just push it into the cloud. And magic happens. But It's not quite as simple as that because there are 3 or 4 different classes of video, they all follow slightly different paths. The on-board cameras are relatively raw and untouched when they arrive with us to be processed and ready to be distributed to the cloud, so they followed one path. You then have all of those track cameras, all of the radio frequency cameras from um camera operators up and down the pit lane. Getting all those great shots, be it off the pit wall, be it off the driver doing a tire change. Hopefully not the driver doing the tire change, that would be problematic. um. But they all arrive with us at slightly different points in time and if we go back to one of the things that we were trying to achieve here, was making it really, really great experience. Everything has got to be in sync. So our first job before we even get to AWS is that alignment of feeds within our media and technology center. And insertion of a time code, simply a time reference that says this feed, this feed, and this feed all share the same time code. So before we pass things into AWS, We get those time codes in order. No. Before I hand over to Jamie, there are some architectural principles that apply more to the media services element of this solution that we wanted to try and adhere to. This wasn't our first rodeo. Our target whilst bringing in UHD HDR was that we would have a single media live ABR encoding ladder. We didn't want multiple outputs here. We didn't want multiple instances of Media life trying to do the same thing. We also wanted to continue to support our existing F1 TV Pro experience, that single view experience. Because that's a large part of our fan base already and, and premium and multi-view isn't for everyone. Slightly different price point. Also device compatibility, so there were reasons that we wanted to protect that experience but also protect ourselves operationally to make sure that we could provide the best experience for our fans. In terms of multi-view, We wanted it to be as flexible as possible on the client's side, so the power was in the fan's hand. I said that we constrained it, but I don't think we constrained it that much. We didn't want to be tied to the video stack on operating systems. There was always going to be a problem, you know, iOS 26. You know, Android 14, you, you were constantly fighting a battle, and for the level of control that we need to create that in-player experience, it had to be a player that was built from the ground up. The same applies in terms of giving the the broad device support. And then we talked about synchronization, we'll talk about it again. But we got everything in sync at the start of this. As we pass it into Media Connect and into the media services pipeline. We need to maintain that right through the media services pipeline and through the distribution to a client device. So with that I'm gonna hand over to Jamie Mullen, senior Specialist solution architect in the media space, um who can talk you through in more detail. Thank you. So hey everyone, my name's Jay Mullen, obviously, I'm a specialist leasing architect here at ADBS. I've got the privilege of kind of deep diving the AWS element of media services that help bring the stream part of F1 TV premium to life. So we're gonna focus on these three challenges. We're thinking about, you know, how do we make sure these feeds are aligned like Dave described. We're then gonna think about how are these streams really encoded and how are they packaged. And then finally we're gonna dive into, you know, how do I, how did my incode change to deliver a multi-view experience? We'll actually look at some of the industry term, uh, industry, uh, what we see in the industry, sorry, to, to enable multi-view. But let's focus with the big one first, let's focus on channel and content synchronization. So if you're not familiar with AWS Elemental Media services, this is what we kind of see from a typical workflow. So we have AWS Elemental Media Connect, which is our secure and reliable transport service for live video. We then have that as an input into AWS Element of Media Live, which is our cloud-based broadcast grade video processing service. We then have AWS Emited media package, which is our just in time packaging and origination service, where you can implement DVR like experiences and also things like DRM. And finally, we've got Amazon Cloudfront, which is our content delivery network, which you can distribute via. And then based on your viewer or your client, um, you can obviously pull what manifests and, and stream that you want. But customers like Formula One operate with more resiliency than just one region, especially for those big events. But these multi-region architectures actually have the same synchronization challenges, uh, for failure. Um, so, I'm gonna show you what this looks like for a failure, for example, before kind of relating what this actually looks like for F1. So we've got the same architecture as before, but we're actually split across two regions. So we have region one and Region 2. The only addition here is that we've got AWS Elemental Live, which is our um on-prem encoding appliance, which in our case is doing the contribution encoding. So What happens if something goes wrong for that, for, for Region One? The viewers and the clients are currently streaming from Region One, but what happens in this case? Well, clients might actually start seeing black frames, um, and hopefully, you know, in, in the media and distribution world, you've got an eyes on Glass operations team. And they're looking for issues, and they eventually spot this issue, um, and want to do that trigger to fail over to region two. However, remember, if this is a real team, there might be some delay in spotting this issue, and also there might be some time delay in actually carrying out that failover. So Once that failover is complete, um, the clients can then go, uh, get a new URL for the second region via a URL vending service of some sort. And then they can start streaming again. But when clients rejoin that stream on region 2, there might be some challenges around, you know, is the two feeds aligned between region 1 and region 2? Are they gonna have to go back into the stream and scrub backwards to find where they were before? So the key challenge here and what we're gonna kind of think about in this, in this sense, is how do you keep things in sync and how do you do this automatically. Um, something called time code, which Dave mentioned is part of that answer. So if you don't know, time code is a time reference embedded in each video frame. Time code can be super, super important in the broadcast and distribution world. How we're gonna look at it today is we want to use it for synchronization, so we want our downstream components to use this time code to help us solve this problem. So time code's part of the answer. How does it apply to a failover scenario? Well, we have a couple of features that we're gonna run through quickly just to show you what that might look like. One of which is called seamless cross region failover, and what we mean by seamless is that we want frame accurate alignment across all your origins, so in our case region one and region 2. In the manual file, for example, before this potentially wasn't seamless, nor was it automatic. So we've got the same architecture, but this time we're gonna start with the time code and that's gonna be on the on-prem Elemental Live device or appliance. So that common time code is applied to that contribution source, um, and so that both regions um receive that aligned contribution feed. We then have Media Live which is configured with epoch locking and basically it uses the embedded time code from that contribution feed. And then it, the timing source is set as the input clock. So Media Live outputs in via CMAP ingest into media package, and when you combine CMAF ingest and epoch locking, what you actually get is a regular segmentation cadence based on that epoch time. And media package can also use this as a stateless synchronization to predictably package the output content. So what happens if something does fundamentally go wrong with the video, whether it's slate inputs, incomplete manifests, maybe even even a missing DRM key. Well a media package has the ability to 404 its endpoint. And finally, the missing piece of this puzzle is Cloudfront, and Cloudfront you can use something called origin groups. Where you specify a primary and a secondary, so in our case, region 1 is the primary and region 2 is that secondary. The other important part of this is that it's configured or you can set something called a failover criteria, and that is based on those HTTP status codes. So when media package does 404 that endpoint. That that request will automatically get sent to the second region via the secondary. But what if it isn't such a fundamental issue with the video? Like what if it's just one issue where you've got an intermittent black frames or frozen frames going to that one region? Like, how do you handle that? Well, building on the previous architecture, we have something called media quality aware resiliency uh to help solve this. So on AWS Elemental Media Live, we generate something called an MQCS score for each segment, and an MQ MQCS stands for Media Quality Confidence score. And it's based on multiple input parameters such as input source loss, black frame detection, or even even frozen frame detection. And that score is between 0 and 100, and obviously 100 indicates the best quality. Media package then provides an in-re quality based failover with this, so it can basically select the segment that has that higher score. But not only that, it can actually signal on the egressD, um, and that is signaled via CMSD. And finally, the, the missing piece of this as well is previously we were using origin groups, but we were using the default settings. You can actually use quality-based origin selection instead of that default in the last architecture. Um, and basically the way it works is there's a get in ahead request sent to each region. And then it basically selects that segment that is gonna provide that better user experience to that viewer. So, we've kind of looked at a flow, for example, but how does that actually relate back to, to F1 and F1 TV Premium, especially around the epoch locking and that predictable packaging element. They want to achieve the same stream alignment. But across multiple channels, not just one channel in multiple regions. So if you've not seen uh the AWS Element in Media Live console settings before, this is how you can configure this for epoch locking. So as Dave said, they provide 24 video fees for feeds for a live session, uh, for viewers, and that common time code is applied across all those feeds. Media Live is then configured with the output timing sourced with the input clock. The output locking mode with epoch locking, and then finally the time code source configured as embedded. Another really important aspect to this is encode configuration. So F1 also applies a really consistent, uh, encode parameters for things like frames per second, gop size, and disables things like, uh, scene change detection to keep all those seg uh all those channels encoded and packaged consistently. The end result is this, not only can you get time code alignment in a single channel, you can actually achieve this across multiple channels. So you can see F1 live in the bottom left-hand corner. You've got an on-board camera of Lando Norris on the in P2, I think. Um, and then you've got Max who is P1. Um, but you can see the time code is all configured similarly. So in summary, with timecode, Media Live, and media package, you can actually achieve this server side channel synchronization. So that's the first tick in the box like they were saying. But traditionally, When you deliver content to viewers, that might only be one show or program at a time. When you want to consume content like Formula One, you might want to consume more of those feeds at the same time. So I'm gonna quickly cover some of the industry themes on how you could implement Multiview and how it actually changes your video in code workflow. The first one is server side multiview. This is where you have a pre or a feed precomposed upstream and the client device can just simply consume it like any other OTT feed. However, this presents a challenge. There's no real flexibility, right? If you want more flexibility on the clients, you're going to have to produce more feeds to do this. So taking F1, for example, they do 24 video feeds that you can consume in F1 TV Premium. If you have that 2x2 grid layout like you see on the screen now, that's over 10,000 new streams required to cover every combination. However, there's no downstream synchronization to handle because it's all done further upstream in your video workflow. The only thing you kind of need to think about is you've got those new streams, you might have to change them a little bit for basically your view, viewer experience and just look at how that looks on a device. The second one is multiplayer multi-view. This is where you have multiple streams available to be consumed, and a user can consume one or can decide to consume multiple uh streams at the same time. With more than one feed. The device has to have multiple players to play back these channels, and this can be potentially resource heavy. As all the players are kind of sharing that device's underlying hardware. The main challenges on this approach really is how, how are all the players aware of each other? How do you keep the feeds in sync, and especially around synchronized actions. Like if you wanna pause, play, scrub forwards and backwards, how do you achieve it all on the same, uh, at the same time across all of them? And finally How do you provide wide device coverage, i.e., do you have to implement this on many different operating systems to basically get the same experience across multiple devices? However, What changes in your Eco profile? Well, everything we discussed around channel synchronization could be a way of trying to solve this, um, or help with the timing problems. And technically you can use your existing encode profiles, right, there's no major changes there, you just might have to move tweetpod blocking with Media Live and Media Package. And then finally, the last one is single player Multi-View with HEVC tiled encoding. Advanced Spoiler, this is actually how Formula One decided to implement Multi-View. So with tiled encoding settings configured on Media Live, Media Live can generate a series of independently encoded tiles, which the player and the decoder can use to decode any combination of tile across any bit rate and resolution. Displaying, you know what makes sense for that device or what a user has got configured. The player itself has the technology and the implementation to use those tiles. From the HEVEVC segments and merge them together before going via a single decoder. So in this case, we've got um we're pulling basically the middle resolution or the middle rendition. And that's being pushed through that single DK now. So how would this change your encode profile? Well, first off you'd have to start delivering HEVC. Next you'd have to also start the configure your HEVC tiled encoding settings. Um, in Media Live, but equally you want to also enable epoch locking and all those timing considerations that we spoke about our Media Life media package. Tile configuration is based on multiple parameters. Um, it could be based on a tree block size, um, and the tile width and height might be based on the resolutions or renditions in your bit rate ladder. But for this example, we're gonna keep it really simple, and we're gonna say our tile size is roughly the size, um, of rendition one. So We've got a 1x1 grid on rendition 1. We've then got that same tile size, so we've got 2 x 2 on rendition two. And then on rendition 3 we'll continue to use that same, uh, tile size, but we've got 9 tiles there, so we've got 3 by 3. Now this is really important, you need to apply that common tile size across all your renditions so that it can actually be used by the player. So what does it look like in Media Live config? Where you've got that height, width, um, you've got tile padding, you've got some motion vector config to be disabled, and finally you've got that tree block size setting. So in summary, This is how the streaming part of F1 TV premium is delivered. We have an aligned time code from source. We have epoch blocking in Media Live with that consistent encode configuration for GP size and frames per second, and that's used across all the streams. Media Live then uses HEVC tiled encoding. Media live outputs to media package via CMAF ingest to allow media package to do that predictable packaging. Of its helper. And then it's available for consumption and distribution via Amazon Cloudfront. Then on the player side, the player can then pull those HEVCEVC feeds to decode segments and tiles to how a user has configured on their multi-view setup for that F1 session. So for this one we're pulling the top bit rate of the car, um, we're then looking at the middle for the cockpit view and then finally the lowest rendition for that track view. And then you can see the viewer configure experience in that bottom right corner. So Dave, um. F1 TV Premium went live at the start of this season, right? Um. It was Australia, it was overnight. Um, how did that launch go? How did you, I'm really keen to kind of understand. You were there. And Uh, yeah, I mean we had, we've been working this for a year or more and before Australia this year. um, start of season is always quite twitchy anyway, doing a fairly significant product launch. Yeah, it was, it was touch and go, um, but. We, we had set ourselves up in, in Biggin Hills, where our media and technology center is, met our AWS colleagues, our development partners colleagues, and, and the player um organization were there as well to, to really team together. Nothing quite like working Australia hours in the UK um when you're launching a new set of features. Um, we were pretty confident, but when you've sold something in the weeks and months leading up to that, that you've proven in testing. Um, but not on thousands and thousands of devices around the world, it's always a little nervy, but. Come the end of practice one, I think we were all kind of pinching ourselves and going, Cool. Um, this is, this is going pretty well. It's probably the smoothest product launch I've been involved in, um, in all my, my career. Cool. So, obviously the folk uh the session today was really focused on that video synchronization. Um, we indexed heavily on, you know, how you did it with Element of Media Live and media package, but it wasn't just that part, right, there was. You know, there was a wider piece of work to do both ends to give that end to end video synchronization. Yeah, it's, it's those three parts, the, the media service a bit sits right in the middle and, and everything you've articulated is exactly how that is managed. But there's a whole piece of work that we had to undertake before the start of this year. Within our broadcasting domain because that that was critical, that was within our gift to, to get feeds aligned before they went into AWS um that involved a fairly significant amount of effort from non-digital and into broadcast engineering. You know, that first event we, we have about a 250 millisecond um round trip latency from our M&TC and Biggin Hill over our kind of diverse fiber paths to the circuit. Um, we were there in our high speed track test, so a non-televised session before the weekend begins. We put safety and medical cars around the circuit and do a full system test. And we were there looking at the live video output, making sure that things were visually aligned, like trying to find markers on the circuit from different camera angles to, to get that certainty that what we configured in each of the delays across all of the 24 feeds was, was good. Um, the other variable I didn't actually touch on our international feeds. Um, we delay within the building because we take international commentary contribution from broadcasters. So we have to actually let the commentators see the feed before that we can take it back and embed it and align it in our feed. As well as our own F1 TV production feed which is produced in-house both commentary at the circuit, presenter hits in the pit lane, wraparound content, all of this stuff at different times, um, it's pretty touch and go, but we, we went into that first session pretty confident and came out of it really confident and, and talking about what could we do next, what was the. The next experiment or change that we want to undertake, I guess we're already at the end of the season, right? It's flown by and the way that the the You know, the championships kind of rolled out. You've got that 3-way fight, right, between Max, Oscar, and uh Lando. Like the video sync part is gonna be really interesting, right? So watch that unfold. Yeah, the, the video sync was a thing that we were concerned about with drift or change throughout the season. We haven't touched it since probably race 2, race 3, and that was like real fine tuning off literally 1 or 2 frames. In fact, I think the recording I showed was from race 1. And if you look very closely, Lando's cameras out by about 2 frames versus the international feet. I'll, I'll take that. 2/50 of a second. And But yeah, this is prime. This is, this is why we built it to get that big screen experience from multiple angles at the same time. As I say, it's, it's really easy to, to do this for, I say easy and, and it's not, but it's easier to do this for a multi-event sport where time doesn't matter and it doesn't matter that Messi scored 4 seconds before someone else scored in a different match. It's like this is all happening at the same time and when the lights go out on Sunday in Abu Dhabi. I'm, I'm gonna look at the analytics and metrics and see how many people have got the international feed, Max, Lando, and Oscar as their multi-view to see what happens going into turn one. Are you gonna, are you gonna have that set up or are you gonna have a little bit further back just so that you can actually see what's happening? Uh, no, no, I'll, I'll fortunately be in the position in, in the weekend, probably being in the M&TC on Sunday for lights, so I'll, I'll have a production gallery in front of me as well. Um, so not only did you do multi-view, obviously you've touched upon, you know, doing UHD HDR, and also adding that HEVC tiled encoding. That's a big change. Huge. I mean, I, I said right at the very start that our biggest kind of change in the video pipeline prior to this was probably that jump from 25 to 50 frames per second, which actually I think anyone who was an F1 TV subscriber back when we were 25 frames a second was really thankful that we did that. That small change made such a big difference to the viewing experience. Um, but it was a huge jump, right? We're talking about having trying to have the same encoding profile for UHD HDR with HEVC tile encoding and with epoch locking in place. It's quite a demand to place on something like Media Life. And finally, It was a massive launch, um, lots of things going live. How did you prepare for that launch? Um, a, a lot of what we did was in the weeks and months leading up to it. You don't just turn up and, and expect it to be, to be great. Um, we leveraged what is now known as AWS Unified Operations for Media. Um, and that was pre-event. So typically a service that you deploy when you're having a big launch, a big change, big event involving media. Um, and the guys there in the engineering and service teams helped us review. What Jamie and my architects had specified and deployed as being the best practice and worked through all the different configuration we had, made sure the right alerting was in place, like going through those configurations with a fine tooth comb, but then being there not physically with us but on Slack, um through that first race weekend and pretty much every race weekend since then to a lesser degree, just continuing to provide that level of service and assurance that. Yeah, they still care and and we're still operating as we expect to. Cool. I've got to ask this one, sorry, but you know, for all the fans in the audience for F1, what is actually next for Formula One and F1 TV? The F1 TV one's easy, right? Two more onboard cameras next year. True. Uh, beyond that, some other features coming. It's a, it's a watch this space, um, but there are some things coming next year for Formula One's going to be awesome, right? We've got a big championship decider, but it's the end of an era for the cars that we're seeing. So actually there's going to be a lot to do both in our second screen experience in in terms of our life timing setup, in terms of the broadcast, what those new regulations mean, how the teams adopt to them, who's winning that initial charge. Some driver changes as well. There's going to be a huge amount going on there and a lot of storytelling to do. It's our job as digital to make that as available and and do that storytelling to the fans. Very exciting 2026 season then. Will be. So just quickly wrapping up, if you wanna kind of learn more about, um, you know, media quality or air resiliency or wanna do some further reading on the F1 TV premium or the root cause analysis that Nick uh presented on, here are the links to their retrospective blogs. Finally, if you are interested in leveling up your skills in AWS Cloud and AI, I thoroughly recommend you check out AWS Skill Builder, where there are 1000 over 1000 free courses or free resources available for your TEs. And with that from Nick, Dave and myself, thank you very much for joining our session. Um, really hope you enjoyed the session. Um, but if you will, um, just a quick reminder, can you please fill out the, the survey on the Reinvent app, please. Um, but yeah, thank you again once for your time and I hope you enjoy the rest of Reinvent. Thank you.