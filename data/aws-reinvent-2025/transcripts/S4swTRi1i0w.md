---
video_id: S4swTRi1i0w
video_url: https://www.youtube.com/watch?v=S4swTRi1i0w
is_generated: False
is_translatable: True
---

Everybody, welcome. I hope everyone's having a uh a nice reinvent so far. We're just about at the halfway point. Um, I'm Seth Markle. I'm here today with James Bornholt, and we're doing the uh deep dive on Amazon S3. Uh, we've been using this talk series, uh, over a couple of years now to get into some lower level details of how S3 works. So two years ago, I talked about, uh, the fundamentals of S3. Last year James and I talked about how we use the size, the scale of S3, uh, to our advantage and yours as a customer, and this year. We're going to be talking about how we design for availability. So we're gonna come at this from two angles. I'm gonna start out talking about the system level view, about how we think about failure at the architecture layer. And I'm going to take us through how we designed for read after write consistency as an illustration of this. Then James is going to talk us through the server level view about how we think about failure at the implementation layer of our system. So before we begin, let's define some terms. So availability is all about dealing with failure. And to understand this, you have to define both failure and you have to define what you mean by dealing with it. And so to understand what I mean by failure, let's get into some different ways to look at S3. So the conceptual view of S3 is that it's a storage service that holds over 500 trillion objects, stores hundreds of exabytes of data, serves hundreds of millions of transactions per second. Right, but these are all abstract terms. Like you can't touch an object. And so what is S3 in concrete terms? So in concrete terms, S3 is desks and servers which sit in racks, which sit in buildings. And so we managed 10s of millions of hard drives across millions of servers. In 120 availability zones across 38 regions. And so when we talk about failure, it's these components that make up our fault domains. So within a drive, surface defects can cause individual reads to fail. Or individual drives can stop working altogether. Or servers can suffer from bad fans, or racks can lose power, or buildings can catch fire. Like all these things can fail. And failures can be permanent loss of a component or transient unavailability, which could be due to power issues or networking issues or just overload of a component in a network or CPU. And so these are the sorts of failures that we're talking about today. OK, so back to our definition. Now that we understand what can fail, we need to define what it means to deal with it. And so this is all about design goals. You set your design goals for your system, and then you design your system around those goals, and then you implement the system according to those designs. So here are some of S3's design goals. And if folks are familiar with our product page, you'll you'll see the phrasing, you know, designed for 99.99% availability or designed for 11 9 of durability. And we provide read after write or strong read after write consistency. Except some of you will remember that S3 wasn't read after write consistent until 2020. And this is important for this talk because what this meant for us back then was that an acceptable way to deal with failure was to violate consistency guarantees since we had none. OK, So let's take a look at S3 before the consistency launch to see how we dealt with failure beforehand and what we had to change afterwards as we implemented it. And what I mean by consistency here, such so we're all on the same page, is the property that an object get reflects the most recent put to that object. And so to understand how this works in S3, we have to start with our indexing subsystem. So our indexing subsystem holds all of your object's metadata. These are things like its name, tags, its creation time, you know, sea time. And the index is accessed on every single get put list head delete. Like every single data plane request goes to this index. More requests go to the index than our storage subsystem because head requests, for example, and list requests don't need to go into storage. So at the core of the indexing system is a storage system dedicated to the index that's responsible for durably holding these index entries. So here's a simple illustration of how we um how we store metadata in the indexing system. The data is stored across a set of replicas using a quorum-based algorithm, and as we'll see in a moment, a quorum-based algorithm is very forgiving to failures. Now you're going to see a QR code in the bottom corner here. This is to a particular academic paper. Throughout this talk, James and I will be linking to various papers that are relevant to the systems that we're presenting. If you miss taking a photo here, these will be up on on YouTube when the talk is posted, and so you can capture it there too. OK, so here's, here's how our implementation of Quorum works in the index. So we start with servers first that are running in separate AZs, availability zones, and the reason we do this is because that allows us to avoid any correlation on a single fault domain. Since the failure of any single disk, server, rack, or zone itself will only affect a small subset of data, and it never affects all of the data for a single object or even a majority of the data for a single object. So we spread everything. And the rules when interacting with this system are pretty simple. So reads and writes are only required to hit a majority of servers. So let's walk through it. So let's say you want to write the value of A, and these values here, remember these are the metadata values for your objects. So in this case, the writer succeeds on all nodes. And so all the nodes have the value A. Now let's say you want to write B. OK, in this case, when you're writing B, there's a failure. That note on the right, uh, for whatever reason, is failing to receive B. However, B is succeeding on a majority of the servers, and so there's no availability impact in the situation. And this is the resulting system state. Now we have a reader and they initiate a read. In this case, one of the servers fails to return. It's a different server than failed last time. However, the other two do return, and so the reader sees the value of B. And you might notice that in this case the reader sees both A and B, but it can reason that B wins because of an associated time stamp with it, so there's conflict resolution in the system. And so we saw both reads and writes succeed here despite servers failing in the middle of requests. There was no availability impact. And so this system is available because there are several nodes that we can talk to. And there's an allowance for failure. In that system and so that's quorum-based systems in a nutshell. And this is the central concept in our system level availability design. We have several nodes to route to, and we have a headroom for failure. And we configure and size this system, either the number of replicas, the amount of headroom, um, to achieve our 99.99% goals. Now some of you might have spotted that readers see the values written by the writers in the example that I showed. And that's because reads and writes always overlap since they both require hitting a majority of servers, right? You can't have two requests both hitting a majority and have them not overlap. And so you might be wondering, wait, if this is what S3 did before it launched consistency, why wasn't S3 read after write consistent? Because we should be able to read the rights because they're overlapping. And the answer to that is our caching layer. So S3's front end heavily caches object or like caches heavily accessed objects in the system. And so let's walk through how that cache works. So we start out with a set of empty cash nodes. And remember, the values we have in storage from earlier, they're sitting here BBA just like we had before. They're sitting underneath the cash. So a reader reads through one of the cache nodes at random. And the value B gets returned, just like it did before, to the reader through the cash node, and so the value B is resident in the cash. Now a write for C comes in, so this is an overwrite of your object. But in this case, it goes through a different cache node, and this is because our front-end servers are sitting behind the IP addresses that you get back from DNS and so you're effectively routing to our front end at random. And so the value C gets persisted durably down to the replicas. In this case it succeeded on all the replicas and uh so you see C C C. But now we have values B and C cached on separate nodes. This is our design before 2020. So now when a read comes in, again it could route at random, and so it goes to the former cash node in this case, causing an inconsistent read. So the value B is returned instead of C. C was the last value written. B was returned, and this is because reads and writes are not overlapping in this design. So with Quorum at storage, we saw reads and writes overlap, but in the cache they don't. And so in our cache, like with Quorum, we have this key availability property. Several hosts can receive requests and there's an allowance for failure. But unlike with Quorum, there's no overlap in reads and writes, and this was OK at the time. This was an acceptable way to deal with failure because our design goals didn't have consistency as part of them. However, consistency was probably the most requested feature that we had at the time. And so we set out to solve this overlap problem by building a cache coherency protocol. This protocol had to be fast, efficient and available, which meant we needed to retain that property that multiple servers can receive requests while some are allowed to fail. And the way we decided to do this was with a replicated journal. And so let's take a look at how this works. So the replicated journal is a distributed data structure where nodes are chained together such that rights into the system flow through the nodes sequentially. So the writer sends a through the journal, same A as before, it's your object's metadata. A flows through the nodes in the journal sequentially, so every node forwards to the next node. And then once through the journal, it ends up sent to the quorum of storage nodes like it did before. And when a subsequent right comes in, it also flows through the journal into the storage nodes. What you can see here is that although the storage nodes just have the latest value, the journal is keeping track of the recent history, and every node agrees on that ordering because they're sending requests to each other in sequence. And so in this example, A comes before B. And this reasoning about ordering wasn't possible with just our quorum-based system that we had earlier, because if A and B were written concurrently, some nodes may see A, then B and B before A, and even the timestamp that we use for conflict resolution wasn't sufficient to resolve that because messages could be arbitrarily delayed. And so no single node in the quorum-based system that we have was able to reason about ordering correctly or in a similar way to the other nodes. But by sending all rights through this journal, we create a well defined ordering for mutations that come into the system. And this ordering is the key building block for consistency. It allows us to establish a watermark to write to S3. And so all of these rights get assigned a sequence number, which increases over time. And so in our example, A has sequence number 1, B has sequence number 2, maybe C has 3, and so on. And when the storage nodes are written to you, they learn the sequence number of the value along with the value itself. And then on subsequent reads, like through the cache, the sequence number can be retrieved and stored. And so you see here in the cache along with the value, we're also storing that sequence number. And these sequence numbers plus the well defined ordering allows the cache node to ask this question before serving requests, which is, did any rights for this object arrive after this sequence number? And because of the journal, we're able to answer this question without talking to storage nodes. So in order to answer this question, we built a system that we call a witness. And the sole purpose of the witness is to track the high water mark for for rights to the index. And so there's a couple of simplifying assumptions for this witness system. First, the witness doesn't need to hold the actual data. It just needs to hold the sequence number because it just has to answer that question, did any rights come in? It doesn't have to tell you what the right was. And it's always safe for the sequence or for the witness to tell the caller that they're stale, because in the case that they tell the caller they're stale, they're just going to read from storage anyway and get a correct result. And so the witness can keep an approximation of the right answer as long as it overestimates the last sequence number. And so this leads us to a relatively simple design where the witness is just an in-memory data structure, it's literally just an array of integers, and that sits next to the journal and allows the cache nodes to ask that key question from before. Did any rights come in after the value I have cached? And so with the witness, we can modify our read and write algorithm to look like this. So rights go to the journal like before, and then the witness. And then Reeds talked to the witness, and if the witness says you're you're fine, then the Reeds can return from the cash, and if the witness says no, you're stale, they can go to storage. So whereas with the early cache design, we were available but inconsistent due to non-over writing reads and rights, now we have over overlapping reads and rights. They overlap on that witness. But with what I showed you, we actually lost our failure allowance. And remember that key property of system level availability. We need to have a failure allowance in the system, so we're not available yet. So here's the original architecture of the journal that I've shown you. Only what happens if one of the nodes fails. So in this case the tail of the journal has failed, and so rights can't progress through the system. And this is a very different situation from a node in quorum failing. And the difference is the journal sends rights through it sequentially, whereasas the quorum services rights concurrently. Right, and so if a node in quorum fails, the other ones probably respond to the caller, but in this case, if a node in the middle of the journal fails, there's no one to forward the request on. And so if a node dies, the whole, the whole system can halt. And so we've lost our failure allowance. And so to fix this, we introduced dynamic reconfiguration into the system. So the nodes in the journal pay attention to each other's availability. They're pinging each other all the time. There's messages going through the system all the time. Like I mentioned, we're serving hundreds of millions of transactions per second. And so every node has an up to-date view of its neighbor's availability. And so when they encounter an issue talking to each other, they ask a quorum-based configuration system to reconfigure the journal. And this all happens within milliseconds of a node failing, and the configuration system itself is quorum. And so it's similar to those storage nodes that we had originally. And so high availability in these systems always comes back to quorum, even for systems like the Journal where there's no apparent quorum in there. I mean it's it's a chain replication algorithm. And I didn't depict it here, but the witness system is quorum-based as well. And so now we have a cash with a failure allowance, and we have overriding, overlapping reads and rights. Which is what we ended up launching. And this allowed us to change our correctness goals from this to this, which now means that S3 is both consistent and highly available. So we retained high availability through the consistency launch, but we had to design for it. And so to recap how we think about availability from the system-wide perspective, you need many servers to choose from. While only being required to succeed on some. With the ability to reconfigure the system quickly in the face of failure. And quorum-based algorithms are always lurking somewhere, even when it's not obvious, like with our journal. There's always quorum somewhere in these systems. And with that, I'd like to welcome James onto the stage to talk about how we deal with failure at the implementation layer of our system. Thank you. Well, good morning. Thanks for coming out. Um, so like Seth said, my name is James. I'm an engineer on the S3 team. I wanna talk a little bit about how we deal with failure at the implementation level of our system. How do we design for failures on the nodes themselves? And so the first question we might have to ask when we talk about failure is, well, what can fail? Right, and Seth told you a little bit about this, and he told you about how we have racks and AZ's and that kind of thing, and we want to sort of make that idea a little bit more concrete. Failure isn't just like one node failing at a time. Often the most important failures we have to deal with are correlated failures, failures that happen together. So let's start by thinking about physical failures in a system. Seth kind of showed you a version of this picture already, but here's a physical view of S3 at a super high level, right? We have multiple availability zones. Each of those availability zones has multiple racks of servers. Each of those servers has multiple hard drives in it. There's lots of different things in the system that can fail, right? Kind of the simplest version is individual failures, right? Hard drives might fail on their own. Maybe the motor breaks or the, the platter gets scratched or something like that. Those are kind of the simplest version of a failure. Instead of a hard drive failing, maybe an individual server could fail. Now, even though this is only one component, like maybe the CPU is broken or something, it appears as multiple failures, right, because all of the hard drives that are attached to that server appear as if they've failed. They've become unavailable to the system. Um, there's a really cool thing, by the way, if you saw Andy Warfield's talk yesterday about some things we're doing to design around this with metal volumes to attach drives to different servers to work around this particular kind of failure, but this is a correlated failure, right? This is all of the hard drives attached to this server have failed at the same time. And you can kind of build these pictures up from here, right? So if an entire rack fails, that's as if all the drives in that rack had failed. Maybe the switch in the, in the, in the rack is bad. The worst case for us, of course, is that if the entire availability zone fails, maybe the power goes out, for example. So these are all correlated failures, and they're kind of essential to thinking about availability. If you think about Seth's design for Quorum, it's OK for one node to fail, but if all the nodes, for example, were in the same availability zone or if they're all on the same rack, then your avail availability property is gone. You've lost your failure allowance because they all failed together. So correlated failures are a really important mechanism for thinking about what is it for our for our system to be available. But it's not just physical failures, right, these are physical failure domains. There are other kinds of failure domains we think about as well. So another one that might be sort of less obvious is logical failures. For example, when we deploy new software to our fleet, we don't do it all at once, right, we deploy it to a few nodes at a time, we sort of ramp up from there. The set of servers that have the new software are effectively a failure domain, right, if there's a bug in the new version of the software, all those servers are probably going to fail together. And so we have to be reasonably intelligent about which servers do we deploy to first, what is the domain of servers that get deployed to first, and then how do we ramp that up, so that if there's something wrong with that ser with that software, we can tolerate that level of failure among all of those servers as well. How do you design around this? Our job when designing around correlated failures is to think about how do we expose workloads to different levels of failure. So for example, when you upload an object to S3, you probably already know that we replicate that object, right, we don't just store one copy of it, we store it multiple times. That replication is really important for durability, right? We want to achieve, we design for 11 9s of durability, but that replication is also super important for availability because it means that if any of these correlated failure domains fail, for example, if AZ1 in this picture fails, there's still a copy somewhere else, so the data is still available even though an availability zone has failed or a rack has failed or a server has failed, and so on and so forth. So designing for correlated failure starts with making sure that your workloads are exposed to multiple failure domains. They're not all exposed to the same failure domain, they can't all fail together. So thinking about failure domains like this kind of helps us understand what can fail, but what does failure actually mean? What does it mean for a server or a rack or a heart or a switch to fail? The simplest version that you probably have in your head, which is the sort of simplest version to think about, is what we would call a fail-stop failure. And the way to think about this is basically like the power got yanked, right? The power cord came out of the server, that kind of level of failure. And I say that it's simple because it kind of behaves in the way you might expect, right? The server just stops doing anything. You can't reach it, you can't do anything anymore. That means it's pretty easy to detect, right? The server is just not serving traffic anymore. It's also reasonably easy to react to, kind of in the design that Seth was showing you, right? If you've designed a system that's tolerant to some amount of failure, if that server just goes away, that's OK. And so generally you can you can do a design that can tolerate a certain level of these fail, stop failures, these failures where the server just goes away. It gets a little bit more tricky when we start thinking about components that are not just servers though. So let's think about a different kind of fail-stop failure. Maybe the, the switch in between these two availability zones has failed, and it's failed in a fail-stop way, like it's just not accepting packets anymore, it's not routing packets anymore. So it is a fail-stop failure in the switch itself. In the system it looks a little bit different, right, because in a system some requests in the system are going to continue succeeding, right? So if I need to talk to a server in the same availability zone to serve a request, that request is gonna work fine, right? It doesn't have to traverse the switch that has been failed, but other requests are going to start failing. So even though a request came from the same source, it started on the same server, if I had to traverse that link and that link is gone, that's a failure. And so now I have this kind of like fuzzy failure mode right where some requests are succeeding, some requests are failing, and it kind of depends on a property of the request. So this can be quite hard to detect. Designing around this one looks a little bit different, right? Designing around this one is all about redundancy, right? So in reality we don't have one switch between availability zones, we have multiple. And then a really interesting part of the design that we have here is that actually we have 3 availability zones in all of our regions, and they're kind of linked together as a ring, right? So there's a cool property where if that failure, if that link has failed and we can't reach AZ2 from AZ1. We can always just go the long way around, right? We can go from AZ1 to AZ2 via the rest of the links. Now this is worse, to be clear, right? Like this is a performance problem now, but we preserved the availability, right? So we had a failure on that link, we preserved availability by converting our availability problem into a latency problem. This is actually a really powerful mental model for thinking about availability, right? One thing we often talk about when we're talking about system design is how do I convert one kind of problem into a different kind of problem. So here I've converted an availability problem, I couldn't reach some servers into a latency problem where I can reach them, but maybe it's a little bit slower because I have to go further. Now, that's not always a good thing, right? We care a lot about performance and I don't want to just sacrifice performance, but this, this idea of sort of converting one kind of failure into a different kind of failure is a really useful way to think about system design and kind of the trade-offs that we can make when we're designing a system like this. One more thing about fail-stop that's worth knowing before we talk about other failure modes, fail-stop is particularly tricky to reason about in a stateful system like S3, because it can get the system into states that are otherwise unreachable in the absence of failure. So for example, when we talk about storage systems, a problem we talk about a lot is called crash consistency. The idea of crash consistency is that a system should always return to a consistent state after a fail-stop failure. So here's a really simple program, right, and you can probably just run it in your head and know what it's gonna do. I'm gonna open a file. I'm gonna write two lines of text to that file, and I'm gonna end up with a file that looks like this, right? It's a reasonably simple program, and that's kind of if you look at it, this is kind of the only state that can result from this program, right? If you just run this program every time modulo like running out of file system space or something, you'll end up with a file and it will have these two lines in it. So that's a normal execution of the system. This doesn't work anymore in the presence of fail stop failure. So for example, what happens if the the system fails, loses power, crashes, that kind of thing in the middle of executing this program, right? Maybe I have only written the first line of text to the file and then the system crashed. When the system comes back up, that file is going to be there, but the file is only gonna have half the contents in it, right? I never got a chance to write the second line to the file before I lost power. And so now I've entered this kind of interesting state. This is a state we all kind of agree is unreachable in the system, except for the failure, right? And so states that weren't reachable before are now reachable in the system. And this is a really hard thing to design around. It is a really hard thing to wrap your head around as an engineer, that when I ran this program, actually there are states that are possible to reach only in the presence of failure that are otherwise impossible. It is a pretty tricky problem. We spent a lot of time thinking about it. Um, we wrote a paper a few years ago about Shodstor, which is our storage node software. Runs on all of our hard drives and the paper is kind of about this problem, right? It's about how do you reason about the set of states that the system can reach in the presence of failure, in the presence of concurrency, and things like that. So there's some really cool ideas in that paper about how we help engineers reason about those states while they're building the software themselves. So that's a little bit about fail-stop failures, right? They're kind of like, I kind of describe them as the simplest mode of failure. There are other modes of failure that we think about a lot in S3, and in particular the one we think we spend a lot of time thinking about is what I might call a gray failure, right, a failure that's not just a power failure or a fail-stop failure mode. So how do we detect those, how do we respond to them? Let me give you an example of what I think a gray failure might look like. This is a really simple overview of how a put works in S3, right? Your put comes in from the internet. You reach an S3 front end server, right, a web server that serves your request, and that web server is eventually going to fan that data out to a set of storage nodes, right? Seth showed you a more detailed version of this picture. This simple version is gonna suffice for today. So what happens if this front-end web server is successfully receiving traffic for the internet, so it's accepting requests, but maybe it can't reach some of those downstream hosts that it needs to talk to, right, maybe there is a networking issue or something, it can reach some of them, but not all of them, right? It's accepting traffic. It's gonna return errors back to you when those requests fail, right, because it's gonna try to replicate that data around those, those disks, and some of them are gonna fail. And so it's gonna return you back an error, which is OK. But it's not, it's now hard to reason about the system, right? Like the system is doing work, right? The server is accepting requests, it is responding to requests, hasn't had a power failure or anything like that. It's just that it's not doing useful work, right? It's failing all the requests that it has because of this downstream issue. This is what I might think of as a kind of a gray failure. It's a little bit harder to detect because the system is not really, the server has not failed, it hasn't gone away, it's just doing something weird. So this is a great failure. What do we do about gray failures? Well, there's a few different things we can do. One really powerful technique to be resilient to gray failures is to use retries. So if a request fails, often if you can retry that request, it might go somewhere else, so it might go to a different front in a web server on the retry, and that web server might have a good connection to all those storage nodes, and so your request can succeed. Now this is sort of hoping a little bit, you have to hope that you go to a different place, but actually a lot of the AWS SDKs have pretty sophisticated retry strategies and they're aware of properties like this. So a lot of the SDKs will actually intentionally retry their requests on a different web server, on a different IP address, specifically for this kind of failure, right, specifically to detect the case where a single web server is bad and try to go somewhere else and see if that web server works instead. So retries are a really powerful mechanism for dealing with gray failures because it lets you try a different path through the system. Retries are not a panacea though. retries are a pretty dangerous thing in a distributed system. So think about a distributed system like this. This is now a slightly more sophisticated version of S3, where we have some downstream microservices, right? It's not just one service, but multiple services. If all these services do retries in the face of failure, it's really easy to end up in a situation where you have a massive amplification of work. So say for example that the system at the end of this stack is failing, is failing requests, right? The intermediate server is going to retry maybe 3 times before it fails. Once it fails, the server upstream. it is going to retry 3 times, it's gonna go through this whole stack again, and so on and so forth, right? So you can very quickly end up if you have a failure at the bottom of this stack with many, many retries in the system, maybe 27 times more retries than there were original requests. So you can overload the system just with retries. What this means for us is that we have to be fairly intentional when we design retry strategies. So for example, we might actually intentionally choose to do fewer retries further down the stack, or maybe do no retries at all further down the stack, knowing that things up the stack in the system are going to do those retries, and so the work will still get retried, you'll still get the same effect of availability, protecting you from great failures, but we won't overload the system with those many, many retries. So we have to be a little bit intentional about how we think about retry policies down the stack of services inside of S3, also on the clients themselves. A particularly difficult case for retries is when the failures have been caused not by sort of networking links or hard failures, that kind of thing, but by load. So perhaps the web server that you're talking to is not actually disconnected from anything, it's maybe just overloaded with work, it's very busy. That manifests as being slow, right? It might manifest as a failure, but usually it will come first just being a slow request, it's not serving requests as quickly as you would like. So again, a powerful mechanism for this is to do timeouts, right? Your client should just time out if a request is slow and go and try somewhere else. And again it's the same property, right? If you retry somewhere else, hopefully that web server is not overloaded and it will succeed your request on the retry, right, so timeouts are also a powerful mechanism for a different kind of gray failure where your server is overloaded. But, and this is kind of becoming a theme in this talk, timeouts are also not perfect, right, and timeouts are very hard to design around, especially in the presence of retries. So let me give you an example of that. Suppose that our web server is overloaded, right? It's having a hard time processing requests. In particular, it's building up a queue of requests that it's trying to serve, right? So I'm making a request to this web server. My request enters the queue. It sits in the queue for a while while the server works on other things, and eventually it's taken so long that my client times out, right? My client gives up on this request and says, This is taking too long. I'm gonna go somewhere else. That's good for the client, right? That's the right thing for the client to do because it will get availability by doing that retry somewhere else. But now we kind of have a problem on this overloaded web server because that timed out request is still in the queue, right? The client was the one that did the retry, and so the server has no idea that the client has given up on that request. So that request is gonna sit in the queue, it's gonna keep moving through the queue, and eventually the server is gonna work on that request, even though the client gave up a long time ago, right? And so you end up in a situation where the server is essentially just doing useless work at this point, right? It's working on a request that the client has given up on a long time ago and gone and tried somewhere else and probably succeeded. And so if this happens over and over again, if you're building up a queue of requests, eventually you get to the point where every request in the queue is just being timed out, right? Every request in the queue is a backlog of a request has been retried somewhere else, and you end up in a state that we call congestive collapse, where the server is basically spending all of its time processing requests that have been given up on by their clients. Right? Clients have gone somewhere else to go and get their data, but the server is still having to process these requests. It's kind of a chain reaction effect and it can be self-feeding. Right, because because the queue, the server is overloaded and it's getting slower, that's causing clients to go and retry, that's causing more work to get put into the queue, which is causing more retries, and so on and so forth. So you can get them to the state where all you're doing is serving failed timeouts from retries over and over again. So like I said, we call this congestive collapse. There's a few different ways that we can work around this, and one of the most important ways is to be a little bit smarter about how we use this queue. And so a mechanism we use often when we're dealing with servers that have a queue of work, when they get into a state where they're overloaded, where the queue is full, we often kind of invert how the queue works and actually process the queue from the back. This is kind of unfair, right? It means that the, the requests that arrived most recently get processed first. It becomes a last in, first out kind of queue, and it is unfair, right? It, it penalizes some clients in exchange for making some clients very fast. The nice thing about this though is that once you start processing the queue from the back, some of those requests start succeeding. Right, and so you end, you get out of this, this sort of loop where every request is timing out, at least some of the requests are succeeding now, they're not creating more work, so you get to start sort of digging yourself out of the hole of slow timed out requests because at least some of your clients are now starting to see success, and the hope is you can sort of burn through the rest of the queue as you go. Another thing we do that looks kind of like this is on the client side when we do retries, we don't retry it immediately, right? All of our SDKs as well as our internal services, they back off and they retry, or they leave some space between a failure and another retry. What this does is it gives the server a little bit of time, a hole in that queue, some space for it to try to dig itself out of the backlog. And so the combination of processing the queue differently, backing off and retrying requests more slowly, helps us reduce the backlog and eventually dig the server out of this hole. This kind of failure mode, by the way, is super interesting. It's called a meta stable failure mode. There's a really cool paper by some folks from AWS about how to sort of analyze these failure modes in your system. We call it meta stable because you end up in a state where even though the original problem happened a long time ago, you're now just spending all of your time working through the queue over and over and over again. So the failure happened many times, many minutes ago, many hours ago, but you're still just processing these timed out requests over and over again. So you've entered a different state of the system. This paper is super interesting and so it gives you some ideas on how to think about how to detect these failure modes in your system design, so it is definitely worth checking out. So that's a little bit of how the clients themselves can dig themselves out of this problem. What can clients do about failures. But what should the system itself do at failure, right? How should we design the system itself to recover from failures? Ultimately what we want is we want S3 as a system to heal itself, right? It's not scalable or tenable at the S3 scale for operators to manually intervene when one of these failures occurs, and obviously we can't just turn S3 off and turn it back on again to get rid of failures like this, right? So let's think about a web server example from before. Right, we had a web server that was not successfully responding to requests. Clients were able to protect themselves from this by retrying somewhere else, right? But that server is bad. I want to get rid of that server, it's doing the wrong thing. How do I get that server out of service so that clients don't have to deal with failures in the first case? How do we detect the failure and how do we mitigate it? So a really common answer for this question on a web service like S3 is health checks, right? So think of a health check as another server, and the server's job is to check the functionality of each of these web servers. So this, this, this health check server is just gonna send requests directly to each web server just to ask them, are they functional, right? Are they serving requests successfully? And if any of them ever say no, or they say the request is failing or they're overloaded and timed out, that kind of thing, that health checking service can now take action, right? It can, for example, take that service, that the server out of service by removing it from DNS. Right, so now clients won't even reach that server and won't experience those failures in the first case. So the health check can detect the failure by seeing the server is not responding to requests, and it can mitigate the failure, whether it's by taking it out of DNS or maybe rebooting the server or things like that. So health checks are awesome, right, and they are a really essential tool for automatically healing the server so that someone doesn't have to manually come and look at the server, notice that it's failing, and take it out of service. A cool thing, by the way, about S3 these days is that we actually build S3 on AWS, so we use the same infrastructure that you can for things like this. A lot of S3's web server capacity these days is just network load balancers. Our DNS is done with Route 53, so we're doing the same things that you can do on top of NLBs and Route 53 for our health checks in S3. But again, there's a little bit of a danger with this health check system. Because in reality, a health check is just a server as well, right, just another kind of server. And so what if the health check server itself is the one that is failing, right? The web servers are all fine, the health check is unhealthy for some reason. What if it's that server that's causing the problem? Well, that health check server is gonna go and talk to a whole bunch of web servers, and it's gonna say that they're all bad, right, because the health check server itself is broken. Maybe it has a network link that isn't working or something like that. So it's gonna detect that all these web servers are bad, and then it's gonna go and tell our DNS server, hey, all the servers are bad, you should take them all out, right? Turn them all off. This is obviously really bad, right? We don't want this to happen. So we have to design not just for availability of the servers themselves, right, the web servers themselves, but also the availability of the health check system. So what can we do about this? One of the really important things we do in S3 is that we don't trust a single health check, right? We actually try to get a very holistic view of the health of our web servers from multiple perspectives. So you have health checks that are coming from the same region. We have health checks coming from another region. We have health checks coming from the internet publicly, and we can combine these signals to form a more detailed view of whether a web server is broken or whether something else is broken in the system. This design is especially nice because it helps us to form a view about correlated failure like we were talking about before. So for example, maybe only the requests from US West to a different region, are failing on all the web servers. That actually suggests there's not something wrong with the web servers themselves, but some kind of networking issue, right? There's a networking issue in the system, looks like a correlated failure because it looks like all the web servers have failed, but it's actually something else. So it helps us to diagnose these kinds of issues in addition to just being able to protect ourselves from individual failures on the web servers. It's a really important tenet here and it's so important that I want to put it in really big words. The most important thing we think about when we think about failure is that we never let local systems make local decisions about the health of the service. That's what the original health check was going to do, right? It was going to make a local decision based on what it saw about the system and then react to that. We never allow this to happen when we designed a distributed system. We cannot trust local decisions to correctly understand the bigger picture, the bigger health of the service, and so we intentionally engineer this kind of like correlation, this kind of global view of the system. To make sure that we don't make mistakes with local decisions. Let me give you one more example of this. At S3 scale we have millions of hard drives, right, and sometimes hard drives fail. So we have software that detects whether our hard drive is failing. And when that software detects that a hard drive is failing, it detects triggers some kind of remediation. Maybe we just turn the the hard drive off and off again, like a power cycle of the drive. It might be as complex as actually asking a data center technician to go and replace the hard drive if it's truly failing. But again, this health checking service is making local decisions, right, it is one service looking at a hard drive and deciding that hard drive is bad. And so if that service itself is broken, it can take a lot of hard drives out of service very, very quickly. So again, we want to make sure that we don't make local decisions about the system like this. And so in this case we have a sort of global rate limiter, right, a system that the health check has to go and talk to before it makes a change to the system to ask, like, I think this hard drive is unhealthy. I want to take it out of service. Am I doing the right thing, right? And if this health check service goes, uh, goes haywire, right, if it starts trying to take everything out of service, the limiter is going to stop it. It's gonna say you've taken too many hard drives out of service, something is wrong with you, we have to stop. Right, so it's all about adding about removing the ability for local decisions to change the system and make sure there is a global perspective of how the system is functioning. So that's actually what we have for today. So we've talked a little bit about availability. These are some of the things that we think about when we're designing S3 for higher availability. It starts with with defining, right, what does it mean to be available? What are your correctness goals in the system? Then you need to design the overall behavior of the system, the architecture of the system itself, to meet that goal. That's what Seth was showing me with our quorum-based architecture and moving to journals, our strong consistency design. And then you need to implement the actual pieces of your system, right, when implementing the pieces of the system, it's important to understand how each piece can fail and how each piece can misbehave and how you can remediate those failures without making local decisions about the health of the system. Thank you so much for coming out today. Uh, please take a moment to fill out the session survey if you do. Um, Seth and I really enjoy the chance to do this talk every year to talk about the sort of hard stuff the S3 team does, and it's really valuable to fill out the survey so we know that we can do this kind of talk in the future. Um, enjoy the rest of Reinvent. Thanks.