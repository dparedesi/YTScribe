---
video_id: -7B31Edtn5M
video_url: https://www.youtube.com/watch?v=-7B31Edtn5M
is_generated: False
is_translatable: True
---

Time. Good evening, everyone. Cool. Uh, thank you for, uh, joining the session. Uh, my name is Pawan. I'm one of the principal Survella specialists. Um, and here we're going to talk about, um, you know, it's a core talk about the observ history, um, using stuff. Functions and I'm joined here by my colleague Diego. If you want to introduce yourself? Yeah, uh, my name is Diego. I'm a principal engineer on the AWS step functions team. Been working with step functions for a few years and I'm very happy to be here with Pab want to talk about step functions and the tricky thing that to understand what's going on with my execution and where should I look at, uh, mostly and also talk about the new metrics that we just launched. Amazing. Um, how many of you are using step functions in production today? Just raise show of hands. OK. How many of you have not used step functions at all? OK, cool, nice. Um, so yeah, in terms of the, um, agenda, what it looks like, um, so we're going to start with like building something on step functions so we'll see like, you know, how do you, how do you start off and then we're going to slightly increase the uh level, uh, the complexity, like, you know, we'll show about some of the observability, how do you actually debug it talks about the important concepts that you, uh, would probably want to keep in mind while building the step functions in, in production. Um, so with that, with that, we'll get started. Um, so we have 4 demos. So the first one, so this use case is around building or analyzing the wind speed data. Um, so for this, for this particular, uh, demo, we are actually going to use, uh, one of the open source data set, um, for which, which actually shows what are the wind speed, uh, metrics is across the globe for each of the, each of the stations. So let me quickly. Show the data set. So this is the data set, uh, that, that, uh, that we are that we're going to use. And as you see, um, it includes, uh, you know, all of these, uh, parameters in here, like, you know, the mean temperature, um, dew point, sea level, and various other things. Um, what we are interested is in, in, in, in looking at the wind speed data. Uh, the nice thing is it actually gives you this single line of command where you can actually copy this data set into an S3 bucket, um, in terms of what it looks like in the S3 bucket, um. That's. that up. So the data set itself is partitioned uh by uh year and then it's also partitioned, uh, further into like, you know, the, the station ID. Uh, so you can see that, you know, it includes data all the way from 1929, um, and, and goes up to like 2024, 2025. Um, and if I open in one of these, uh, you know, buckets or the objects, like you can see these, these are the different, uh, station IDs and Um, the data inside that looks something like this. Uh, so let me zoom this up a bit. Um, so you can see that, um, it has the station ID and then the date, parameter, the latitude, longitude, where exactly the, um, the, the station is, uh, and we are interested in looking at these, these parameters. In terms of the end product, um, what, what it actually looks like is, uh, you know, we have this particular dashboard, which actually shows the mean wind speed across all of these different, um, you know, the stations across the globe. And, um, if I click on one of these points, it's actually going to show, um, where exactly it is also located. So let me Let's try that again. So if I click on one of these points, uh, it's going to show that, you know, this is located over here. And then also if I mouse over, it shows what does that mean average wind speed in that location as well. Cool. So with that in mind, uh, let's go ahead and, um, build this, you know, let's see how do you, how do you actually build this workflow. So, step functions, go to the step functions console, let's create a state machine. Um, let's give it a name called wind speed analysis. Um, for this type, I'm going to choose the standard, um, workflow. Now, for those who have actually not used the step functions before, um, so for you to get started, the step function console actually gives you a very nice way to get started. Right, um, under the hood, it actually uses something called as the Amazon State Language. So if you're new to it, like, probably it might be, uh, a bit of a learning curve. So the console actually gives you a great place to, um, get started with. So the first thing that you would do is track and drop the map state here. Um, let's give it a name for this. Uh, let's call it as DMap. And since we are working with the, the distributed map here, um, so what is distributed map? So, distributed map is, um, ideal for scenarios where you want to process large scale data. Like think of, think, think in the order of like millions of objects that perhaps are there in S3 and then you want to like, you know, process them. Um, so the distributed map can really scale massively and then help in processing this data. The data set that you saw, um, the, the wind speed data, just to give you an idea, uh, we have about close to 600,000 objects in the S3 bucket, um, close to about 70, 70 or 70 GB of, uh, worth of data. So let's go ahead and click um the processing mode. So there are two modes here. So either it can be like inline or distributed, so I'm going to choose distributed. And then you can pass in the, um, you know, the source. So either it can be passed in as an input, that's basically the state input that you see here, or in our case, we're going to use S3. And then I have this drop-down where I can choose, like, you know, which, which particular item source that I'm going to use. Um, so this is really, um, you know, this, this list has actually grown recently, so we, we introduced, um, the parque format, uh, the support for Athena data manifest as well. Um, so in our case, I'm just going to choose the S3 object list and um I'm going to point it to the bucket where the data resides. Um, so let's choose the bucket. And um then the next configuration that I want to do is like, you know, because um the way how this um processing works is it'll pick up one object at a time, and that's probably may not be very efficient. You, you know, uh, it's recommended that you would enable the batching and then, uh, you know, you can say how, how, what, what's the batch size that you want to take, um, these, these objects together and process them. Um, so for this use case, let's give it as 500, 500 objects at a time. Um, you could either batch it by the number or you can patch it by the maximum bytes per batch. The next is the, the concurrency limit. This is a powerful feature of the distributed map. So this basically tells, you know, this is basically where it actually, the, the scale comes in, the scale of processing comes into play. Um, so you can set the concurrency limit has 1000 as a default, but it can actually go all the way to 10,000. So think of, you know, um, the, your child workflow scaling concurrently 10,000 times, like in a prom, you know, in, in terms of the concurrency size. So I'll leave it as 1000. Um, and then I can choose whether I want to use the standard execution or the express, um, so leave it as default. Um, in terms of the additional parameters, uh, I'm just going to set the, the tolerated failure threshold. So this is in case if my execution fails for whatever reason, and that threshold crosses a 5%, then I want to deem it has the entire workflow to be, uh, failed. And finally, the last step is, uh, for the, in terms of configuring the distributed map, I'm just going to stay where should this particular processing output the data into. So, uh, I'm going to provide the uh S3 bucket for that as well. So let me choose. Um, The bucket. And then I'm going to prefix it with the As a demo. So, that's in terms of the distributed map. Um, now, let me configure, like, this is where, basically, I need to say, OK, what should happen inside the distributed map. So this is where my business logic comes into play. Um, so in, in, in case of this wind speed, like, you know, it's going to process that wind speed, um, um, data set, right? So my lambda function, which is already pre-created, what it actually does is goes into this individual date object and then looks up all the data and then calculates the mean average speed. It's a very simple lambda function. Um, so I'm going to try to, um, choose the Analysis lambda function here. Um, and then I'm going to also drag and drop another lambda function. So think of this workflow as like a map producer, like, you know, the, the first lambda function does, um, the map processing. So I'm going to call it as analysis function. And the second one is going to be more of like a reducer. So once you have collected all the data, it's going to consolidate and, and, and generate the output. Let's choose the. Reduce a function here. Um, so that's pretty much it, like in terms of what my, um, distributed map looks like. Now, one of the other things that, uh, this data set actually has is if you notice, all of these wind speed data is provided, uh, you know, the unit of measure is in knots. Um, what if, you know, I have a use case where I don't want it to be in the knots, but convert it into like maybe MPH. So I'm going to use another lambda function which can help me out uh to do that. So convert to MPH. It's uh configure that lambda function, and pretty much that's it. Now, you might ask or wonder like, do I need another lambda function to just to do a convert here? Not necessary. Um, you know, the step functions also, um, supports the JSON nata, so you could pretty much use the math expression to do that. So you're already started thinking about like, you know, um, optimizing this, this particular workflow. So you it doesn't necessarily have to be, uh, a lambda function doing this. Um, that's pretty much it. And then let's go ahead and create it. Um, and if you see the screen, uh, it actually prompts you with, uh, what, what the step, step functions of the state machine has identified as, you know, um, based on the workflow, it's defined, it needs the permissions and the policies set. So it's actually giving you that wizard that it's going to create all of these permissions for you. So let's go ahead and say confirm. And It's created the state machine. Um, it's also, you know, you can also go and look at the IAM permissions that it was created. Now, let's go and execute this particular workflow. So, in my case, uh, I don't need to pass an input, but, uh, you know, it's in case if your step, step, step functions, so the distributor map needs an input, like, you know, you can pass it, pass it in here to test it out. So I'm just going to click on start and um wait for the execution. So, um, now you can see the nice part about the, uh in the console where you can see that it has actually started, um, you know, uh, the map run state. So, um, inside the map run, you know, these are some of the observability, uh, things that, that is, that is made visible to you to see that how this, uh, state machine is actually preparing your data set. So what you see here is in the pending state is, um, the distributed map is actually, uh, reading or iterating over that, uh, list of objects in the S3 and preparing it for the execution. Um, I also want you to keep an eye on this particular duration metrics to see, you know, how fast it actually takes to complete this entire execution of like 600,000 objects. Yeah, one thing that you can uh see is that. After we read all of your contents from your bucket. We start dispatching the executions. So the difference from the inline to the distributed map is that each iteration of your map becomes a new workflow execution and this way you can parallellyze higher than just being having a contentious on one execution and then you may face limits with the execution history and so on. So the distributed map gives you that option of each iteration of my map is its own execution with its own limits. And then here you can see that after it started dispatching the executions, it's moving over to the running, and then the executions are there on the list. And we, we keep track of how many are failing because if you set tolerant threshold on failures, in this case, we'll set to 5%, we abort the security and then we fail the state. Also, you can also set a tolerance threshold on count. If you don't want to use percentage, you can say, if 100 failed, then I just wanna borrow. Uh, so one part that it starts getting tricky is that everything that's happening here is asynchronous and then customers ask us, it's very tricky to understand why some of my iterations are getting delayed to happen in the future. Cool. Yep. Yeah, if you, if you see that, you know, it's taken about um 2, roughly 2 minutes, 15 seconds to complete all the executions. And each of these executions, you can see that it has taken 500 items at a batch and process it. Now, if I go into the um the S3 bucket, um, where um we have the results, so you can look at the demo, and then in this execution, you have the output, the CSV file that it has created. Now, I can use the, you know, the, the visualization tool like, uh, you know, the quick site and, and, and show this on the screen. So this is how, you know, you can, you can build a step functions, uh, distributed map to process, uh, you know, large scale data. Yeah, and if you were to have any failures, uh, you would see a separate file here. Um, with the one that says succeeded, and then that gives you the option to reprocess just the items that failed instead of reprocessing the entire thing. So you can even connect that specific file to another distributed map, uh, to redive the ones that failed. Before we move to the next topic, are there any questions we can take maybe a couple of questions. Yeah, so actually you had like, uh, set a limit of 1000 concurrency or something. Yeah, in that case of 1000 lambdas, which is the limit actually given on this account. If it's all exhausted, like you had them on the floor, like you had like, you know, other lamps. So will they even get an opportunity to run or like will be started or what goes on there? Yeah, so the question is. The default concurrency was set to 1000, which is usually the default for the lambda functions. So if that number was higher, would the lambda functions even have the chance to run? OK, so, on, on step functions when you configure your state, and then as you're doing the drag and drop and configuration, there's an option to handle failures. So you can catch throttling exceptions and retry with Beckoff, and then the workflow will take care of you of rerunning them. Um, I believe there, there's a few exception types that you can catch from lambda. Uh, but if you start more than what your downstream service can handle, usually they throttle you and then you can catch and manage on your state machine. And uh just another point on the lambda concurrency limit, that limit is a soft limit to begin with, like, you can always open up a support case and it will be, you know, approved in a, in a few, few, few minutes' time. So it's not a hard limit as such. Cool. Um, we'll move on to the next one. So Diego, question to you. So now we have seen an example of the uh map run, right? What, what, what sort of like visibility are there from an observability standpoint a customers can use uh in order to get like, you know, more deeper insight? Yeah, usually when when you're investigating things that are, uh, asynchronous, you don't directly see failures because they're just running somewhere out there and then. After operating the service for uh some time, we noticed that it would be great if we launched new metrics for our customers and we did a few months ago. So now you can track the limit that you have applied on your account for the amount of open execution, open map runs that runs concurrently. We also publish how many you have, uh, approximate. Running at that specific point of time. And another one is backlog. So let's say we're trying to start more map runs, but because of your limit, we cannot start more. So one thing that's great about step functions is that it doesn't fail. So even if you're getting throttled, those things get scheduled to happen in the future. And as soon as something completes, the other one just takes the limit and it starts running again. But one thing that was tricky is how big is my backlog because I don't know how far or how far are they along or how longer do I need to complete all of them. And we recently launched those 3 metrics and uh maybe you can uh show us how it looks like on the cloud watch. Yeah, so let me switch over to the demo. Um, so while Diego was talking, what I did was, um, I have another, um, load test lambda, um, load test step functions here. Um, so if I just quickly show you, so what it actually does is pretty much takes the. Input of like uh from a CSV file, which has like a bunch of random numbers, and then it goes and calls this particular, um, you know, function, uh the uh another state machine which simulates the uh uh the open map run uh count. So I have a dashboard here and uh if I open, show you the metrics that we are monitoring. So these are the metrics that Diego was mentioning, which was recently launched. Um, so you have the open map run limit, which is the maximum. And then you also have uh the current open map run count, which is, which, which shows you the current status, and then you have the map run backlog size as well. Um, so now, um, while the load test is going on, um, you know, let's, if you, if you Wait for a few seconds, we can actually see that, um, you know, the, um, the map run limit is exceeded, and then you'll start seeing the, um, run backlog size, um, you know, started to increase. Um, so this, yeah, yeah, and the, the important, uh, thing about keeping track of your backlog is that you can even alarm yourself because maybe you don't even know that this is happening. Um And uh what could happen as well is someone sharing the same account goes and deploys something that consumes the limits from that's shared across accounts and then you can identify oh this is happening and then we need to identify like what's going on and what should we stop or maybe what should we talk to support and get higher limits and this is where sometimes um. You can either just directly talk to support or go to service quotas and requesting limit increase, and then you can quickly recover from that state. So I think, uh, we've already seen, um, like it's actually exceeded. So if you see the, um, dashboard over there, uh, the widget, it's actually, um, already exceeded the, um, number of open map run count and then you'll start seeing the, uh, map run backlog size, uh, started to build up. Um, so it has a best practice, like, you know, this is something that you should, um, probably set as a threshold and then, um, generate an alarm, uh, for, for your team to like, you know, take an action or see what's going on. Right, and keep in mind that that metric is only published when there is a backlog. So if you don't have a backlog, you'll see no metric, but eventually when they happen, uh, you start seeing the metrics. And if it's growing, you just see it keeps going up and up and up, and then when it starts raining, you'll see it going down. And eventually you, you'll see the no metric data again. OK. Um, probably we'll take a question before we move, yeah. Or uh distributed map mistake that uses a leg up for processing. Um, if you have like 600,000 files to process with that single lab, I think most accounts have like a limitation on asynchronous implications of the lambda. How does this get around that? Like would it just run mass implications of the land that like the process and capture. Do you want to take that? Yeah, so the, the question is, in this case you had 600,000 files from S3 and then you pass it over to lambda. How does lambda handle that volume because the, the limit is 1000. Did I get it right? OK, so what, what happens here is that the first thing we did was to set a batching. So the payload that you get on your lambda function contains that batching and then you can manage how big the batching is. And then on your lambda function you will handle that. I believe the number was 500. Uh And then as you run your lambda function, the invocations will be limited to the limits you have on that specific function you're calling. And then, uh, similar to the previous question, if the lambda throttles you, you, you need to catch those errors and retry, and then you can manage the back off for how long you want to retry, and then you can even set the jitter on the estate. Yeah, and just an extension on that point, um, um, like the, the example that we have on the, um, the lambda function, right, it actually also calls the S3 uh APIs as well. So sometimes if you're exceeding those limits, like, you know, you can get throttle exceptions on the S3 as well. There's a way to error handle it on the step functions itself like, which is part of your state machine definition, and then you could also like have retry mechanisms as well, um, that's all built on the, on the, on the state machine definition. Yeah. Cool. So we've had a look at the open, uh, map run. Um, so now there's another scenario that, uh, a lot of customers, um, encounter or like, you know, will, will have to keep in mind is the state transition. So Diego, what is state transition in this concept and like what, what sort of behaviors would you encounter in step functions? Yeah, so state transitions, you can assume is when the, the execution starts and when the execution completes. And when it goes from one node to the next one. And when you're talking about distributed map, is each iteration that started. So step functions by default, it has a limit of 5000 per second on state transition. So let's say you started um a number that's higher than that. Step functions gives you 5000 per second, and when you try to run more than that is when you start getting throttle. Usually when it's an API call you get the error immediately back 429 throttling too many exceptions, but again this is an asynchronous process and you may not even see maybe someone went to the account and started consuming the state transitions. This is a shared limit. So you, you start getting throttled and start affecting the other state machines running on the same account. So on the example here, let's assume you have a bucket, every single second, this bucket gets uh refilled. The default limit is 5000 in large regions. And then every time there is a state transition happening, it consumes one from that bucket. So let's, in this case for uh just to illustrate what's going on, let's say you start a huge load on step functions eventually this bucket's gonna be empty, you're gonna get throttled, and the good part is that step function doesn't fail your executions, those state transitions get pushed to happen in the future. With a back off. So let's say the first run after 1 2nd and 2 and 4 and 8, and then it keeps increasing and there is a limit. But then for you as an operator, you go and check your execution. And it looks like nothing happened for 30 seconds, so you may assume, oh, there's something going on with my execution, maybe my code is wrong, maybe my definition, maybe I'm getting throttled somewhere. So as as an operator, where, where should you check to understand if that's the the case for you if you're being affected by this. So maybe power one, we build something that can show that to us. Maybe you have a nice dashboard. Cool. So, um, I have an example where, um, you can simulate the state transition. So by the way, um, all the, uh, samples that we're using here, um, I will, we'll share it with you in case if you want to try it out, test it out. So, uh, towards the end, um, there's a, there's a link that we'll share where you can see, find all these demos. Um, so yeah, this is a state, uh, machine that I have, uh, which basically does loops within a loop, you know, kind of, um, to simulate the state transition. So it takes in like the past state and then has this parallel state, which calls the other, uh, choice state and goes on and on. Um, again, going back to my, um, low test fun um, state machine, I have just initiated, um, The load tests on this specific workflow that, um, that I, that I just showed. Um, now, if I go back into my dashboard, um, there is this metrics, um, that's available in terms of the state transition quota. Um, so let's just open this to see what are the things that we are monitoring. Um, so as Diego mentioned, like, you know, these metrics as the, the, the, the bucket size, the refill rate, um, the consumed capacity, and the throttle events, you know, these are all available here. Um, the best way to like monitor some of these, like, for example, the bucket size, it's the maximum statistics that you can use. Um, same is the case with the refill rate, uh, the max statistics, and then you have the consume capacity, which can be a sum aggregation, um, across, across a minute. Yeah, one thing to keep in mind is that cloud Watch metrics, they're published by minute, but your limits are per second, so it's an average across all the seconds in a minute. So sometimes you see that you got throttled in a second and then you see the data points, but then you, when you look at your limits, uh, it may not correlate exactly to the second. It's just because the metrics are published every minute. Yeah, so, uh, if you see the dashboard, it's already started, um, Showing up that there are some throttles that are happening, the state transitions. Um, and if you want to find out, um, you know, which specific state machine perhaps was contributing to this, so let's go ahead and, um, try and add, um, this. Execution metrics. So what I did was, uh, selected execution throttle has, uh, my, uh, metrics, and then I'm going to choose all my, um, state machines in my account just to see which one is actually, um, causing, causing the problem. Um, and in terms of, um, the statistics, let's choose, um, some, um, and the period as one minute, and let's create it. So now it's created a dashboard for me, uh, or sorry, the widget for me, and in here, I can Mouse over and then I can see which are the top two, state machines which are actually, um, contributed to the, to the throttle. So one is the state transition as expected, um, which is, which is, which is contributing to the throttling here. Yeah and uh uh again. It's important to understand that your executions will not fail because you're getting throttled. They'll just be running, uh, somewhere in the future. And then if you encounter this situation, you can just go to service quotas and request an increase. And depending on the limit, it gets automatically approved and then things will start moving faster and then you recover. Um, any, any questions? Anybody has any questions? Maybe we can take a couple of questions. Yeah, so what's the universal software? These are like 5000 transactions. Is it like? The, it's a soft limit. It's 5000 per second in large regions. Um, there's no hard limit. It's usually given and provided by use case. So you can just reach out and say, hey, I have this use case. I actually need, I don't know, 30,000 per second, and then we discuss and. And and decide what it's per account, yeah, yeah, so these sorry so if you have more of them. Yeah, you consume more state transitions across your account that's right. And having a dashboard that filters out by state machine is easier to point it out which one is the one that's consuming the most. Uh, yeah, so, um, on this case. The limit is different per region. So large regions 5000, the smaller regions is 800. There are soft limits and we can increase. And uh with this uh metric and dashboard, you can easily identify why your executions are being delayed for the case of a state transitional truly. But there's also another case that. Understanding what's going on behind the scenes can simplify the way you design your state machines. And in this case is when you have uh cross account integrations. So let's say you're building a state machine and then you go and invoke a nested state machine that belongs to another account. If the parent and the nested are in the same account, the step functions create something called manage rules on event bridge, and then it listens for the events on the nested execution completion, and then automatically notifies the parent, completes, and move on. But if you're using cross account. Your state machine cannot uh read events from other accounts by itself, and the way it works is by pulling the status of that execution that the parents started. So let's say you created a parent with a child, it's a cross account, the step functions is navigating through your state machine, it reaches a state that calls the start executions on a state machine in another account, and it's an asynchronous process. You just get um a success response back saying, hey, I started, and then it's stuck there, right? It needs to wait for the child execution to complete. And then you're waiting. Somehow the child completed and your parent is still waiting for the completion. So what could be, uh, the reason why the parent didn't get notified from a cross account uh invocation? Uh, I'll pass it over to Pavan that has an example of this happening, and then we can discuss about this. So, in terms of the uh setup to um show you the, the demo of this particular scenario, um, so we have an account where uh you have a workflow that looks something like this, where, um, an input, um, you know, take, take, for example, the product review Um, input which has like the product ID maybe a description of uh the product and maybe a review of that particular product so it goes through the workflow of like, you know, passing through a Bedrock API which then um analyzes it to see whether that particular review was actually a fake review or a real review. um, and then there is a choice state to say, OK, you know, if it is a fake then pass it on to another. Uh, logic which will process it. If it is a real one, then you would just, just bypass it. So that's the workflow that you have in the central account. Now there comes maybe another team that would say, OK, rather than us building this particular workflow again in our our own account, why don't we make use of like, you know, the cross account invocations, like, you know, from create a parent um workflow here and then call the child workflow. So this is what you can, I mean, you can use a cross account um integrations here. Um, typically, what you would use is the dotsync method or run a job, uh, within the step functions, um, naming that you would call this particular child workflow. Now, there are a few things to uh keep in mind while you have this particular setup, right? Now, first thing is like, um, each of these state machine has its own role like if the child function has its own role to call Bedrock APIs or maybe other um um you know, um, AWS services in there and your parent function has, has its own role. But also you need to establish a trust relationship between, you know, the cross account calls. So what does that look like? Um, so this is an example of what that trust policy will look like. You know, your parent would need the permissions to assume role on your child, um, um. Uh, the, the, the, the, the, the role on your second account, which is a child account role. And then On your, um, the child account, you need to have a, a principal which will, um, make sure that, you know, it's actually able to assume the role with the principal of your parent account, and then you're also passing this conditional attribute into that just to avoid or not run into the problem called as a confused deputy where it is you're saying that, um, only allow the assumption, uh, in case if it's coming from this particular account or from this particular state machine. So once you do that, you're establishing the trust relation. So this is how the setup will, the setup looks like. So, um, that's what I have in my account. So let me quickly jump to the demo and show you, um, what it looks like. So in my parent account, I have This particular State machine Which is, uh, calling my, uh, you know, the, the, the, the child workflow that you see, uh, cross-count. And while it's also passing, you know, I'm just using, also using this target role ARN for the cross-count invocation. Um, in my child, uh, I have this, um, particular nested process review workflow. And while things are loading, um, let me explain how it works, uh, behind the scenes on step functions. So we added support for a cross account a few years ago, and the way that function does it is through role training. So you have an execution role on your state machine. That is used to run its own steps, but because we allow cross account that execution role can assume another role on another account and the way you, you create the trust relationship between them it it's how uh Paulan created. So you go to the target row, which is the one that is used for training. And then you say I trust. That state machine that's gonna make the call and then as the execution is going through it calls us some role on that and with that credentials it makes the API call and this is not only for step functions in nasty you can make cross account on the other services as well on the other integrations just make sure you set the target row because that's what's gonna be used for uh role training and keep in mind that that role training is not made by step functions. When the call happens it's made by the source account so you can have some controls on the relationship between them and uh it's not uh step functions making a direct call because maybe the target account doesn't even know that this is coming through step functions and uh probably they they shouldn't know how you architecture your application they just received a call from a role you trust. Cool. Um, so this is what my child workflow looks like. Um, again, the Bedrock API classifies the review and then passes it on. Um, so let's call, um, the parent workflow here. So let's go and, uh, pass in an input. Um, so just for fun, let's give it as the, today's date timestamp just to validate. Um, and if you see the, the payload, it basically has, uh, an ID, an overall review, a review text here, uh, and a few other payload information in here. So let's go ahead and, uh, start this. And, um, You know, this is called the child workflow, and, uh, you know, you don't see it yet completed. Um, this is the problem that, uh, Diego was describing. And if I actually go into my child workflow, um, you can see that this particular workflow has received that input, and it has completed that execution. And if you notice the, uh, you know, the, the, the payload, let's refresh it once again. Um, if you see the payload, um, This is the ID that we passed in. Um, and this is completed, and this has succeeded. Whereas the parent, um, one execution, it's, you know, if you refresh it, it is, it is still showing in an execution state. Um, so something, something's going on. Another option is to just click on the execution link that you re you see guys if you scroll down. There's an execution link that goes directly to the child execution that was started. Yeah, I think there's simple permission there. Yeah, OK, because it's cross account, so in this case you can't go to someone else. Yeah, yeah, but if, if it's on the same account, you can just click there and go to the execution to see oh, it's actually completed. So what's going on with my execution? Um, it, it goes back to what, what I explained before. When it's in a different account. You call start and that process starts somewhere else. But you don't get events happening from that account. The way it works is that you need to poll for the status. So it's very common. That if you are in the situation and you are not um very careful with how you set up the permissions and the relationship behind the scenes that Fox is trying to get the updates for those executions that happened in somebody else's account but it's getting access denied. So in this case, Paul, how, how would you, yeah, maybe, maybe I'll, maybe I'll ask a question to the audience and maybe I'll take some help from the audience. Like how would you troubleshoot this kind of a problem? Like where, which, which, what, what, what tool or what services would you use? Cloudre. OK. Does anybody agree or deny? OK, let's, uh, let's check cloud rail. Yeah, that's a, that's a very good option. OK. Um, and let's maybe filter it by source. Sensitive state. Um, and let's also filter in the last 30 minutes. And, uh, yeah. So you, you're right. Um, so you do see that, um, there is, there are some access denied. That's already showing up there. So if I click on this, like, you know, execution history, it should be the described execution. Sorry, yeah, the other one. Yeah, the described execution. So that's the described state machine. It's probably easier if you scroll to the right and see the state is denied and then. You can also filter by event name described as a. Let's do that. And then you pick the axis. Yeah, yeah. So if you, if you notice this one, this was related to my execution, and, uh, um, you can clearly see that, um, you know, this particular, um, role doesn't have the permissions to, um, um, describe the execution. Um, so in terms of like fixing this, um, Let's go ahead and try and fix that and in the in the meantime what's going on on step functions is that for the first. Uh, 10 minutes, it pauses every minute. And after it, it adds to back off. So let's say you're in this situation, things won't fail. So as soon as you update your uh IM roles and permissions and relationship, if something is wrong. Uh, eventually they'll just recover and succeed. Does it pull only if it's cross account, or does it also pull if you do an execution in the same account? Uh, it also polls if it's the same account, but because it's the same account, events from event bridge is just faster. Yeah, so you rarely completes through the poll, but, um, as Berners always says, yeah, as Berner always says, things fails all the time. So if the event for some reason is not delivered, for some reason is lost on the network, uh, the poll is your backup. And uh it's yeah exactly if it's it's a way to make it more reliable. Uh, because we, we don't want to fail. If the child succeeded. So the, the way we do it is to poll just for guarantees. Monday. So the, the, the parent waits, so the question was, it it doesn't fail the execution, it keeps falling. Yeah, because it actually I see that the execution happened probably make the status. Yeah, that's right, so the parent is still waiting for the completion of the child execution, but the child completed a few minutes ago, yes, exactly, because it needs to know what happened. So if you don't have permission to check somebody else's account. Uh, you can access that information, and the way it works is by the road chain it has, it needs the described execution permission so it can get what happened and then complete your own state and move on. Get the time off at the same time. Because we played for some kind of fun. Yeah, one option is to put the time out, yeah, yeah, but then it, it's gonna fail eventually and move on, right? Um, so if you see the, um, the role permissions, um, on this particular child, um, role, um, right now, it only has a start execution. Um, so what we need to do is we add the, the described execution as well as a stop execution. Um, so let's go ahead and add, um, Those permissions and? Another one that's important for. Uh, nasty and cross account is they stop execution as well if you want the parent. To stop the child execution if the parent fails. So what happens is you have your parents running, starts a child, the child is running. And let's say you wanna abort the parent. So what SE4ction does is the best effort to stop the execution as well, but if it doesn't have permission to stop the execution of the child, uh, the child execution, then it just moves on. So then you can see the access denied on the cloud trail as well if that's the case. Exactly ask for the specifics of function now because. this. Yeah, you don't need the list. It's only the described because you need, you need to know what the output is. Yeah, that, that's right, yeah, the, the question was you don't need the list permission just to describe. And describe is the one that provides your input and output, and they state. All right, let's add the uh stop and describe policy. Um, so that's in here on that specific execution related to my um demo nested workflow. And uh let's go ahead and give it another try now. That's copy. Let's give the overall has 4 and then start the execution. that. This is only a whole so there is no push from the from the rest. correct. The, the question is for cross account, it's only polling. Yeah, that's right. If it's the same account, then the events get notified through the manager that's created on Eventbridge and Eventbridge sends those events to step functions and step functions identifies the event and then oh this is for that execution that's waiting and then completes that task and move on. It's interesting that the previous ones are succeeding exactly, yeah, exactly, yeah. So that's because on the next run, because we back off on the next run, we try to get new credentials to Paul, and then we call the described execution. This time it succeeded, so we can complete the parents that were waiting and then it just moves on. So yeah, all the previous running states have all completed now. So who, who do we talk to to make him pull a little bit faster for one minute for the first hole is kind of high. Yeah, you can always reach out to us and we can customize. You know 10 seconds then back up to 30 then do a minute or something like that, yeah, usually when uh those things run too fast, let's say you started. Tens of thousands of executions. If they're all trying to power, you may get throttled. So those things have to follow also a limit increase for the policies. And uh yeah, this is what what what we discussed the the resolution on this case is because. The parent workflow is trying to call for the status to understand what's going on with the child execution and this is because it's a cross account you cannot read events from somebody else for security reasons and what the function does is, hey, I need permissions to read information from the other account and by doing the role training I'm going to assume a role using your credentials for that account and with the credentials from the training I'm gonna call the described execution API. And after we fixed and updated the permissions on the next, uh, run on the next check, it will just succeed, it will complete, and then it will just move on. So here on the on these permissions you can see they're required for nesting is uh describe and stop and the stop is if you want child executions to be terminated when the parent execution is aborted or fails, let's say the parent execution time is up. You don't want the child executions to keep running maybe there's a. A use case that you want to keep them running and succeed, but in case if you don't, you can just add the permission to stop and step functions is going to make a best effort to stop them. On the events part is because there's a feature from EventBridge called Manage rules. And step functions create a manage row to read events from your account, and then those events get delivered and step functions know exactly how to write them, uh, route them and complete the parent executions and with a specific task that's waiting. And if you don't have those permissions, the create state machine with a dot sync integration is going to fail, uh, because we, we can't create the manager on your behalf. Does it support like wait for task token because that might be a way that you could not have to yeah, the, the question is does it support, uh, wait for test token, uh, it does so you can control yourself you can get a token and get it delivered and then somewhere else, yes, can call it back. You can do that, yeah, that, that's right. But sometimes you can't modify the child so you can't add this extra step, but if you can, this is a faster way of completing. You just need to make sure you have the right permissions because it will be a cross account. So you need to assume a role on that account and call the center success in this case. Or sent us failure if it failed, but then, uh, it's not a managed integration across account anymore. And, you have to, to do it. Any other questions on this? Cool. Um, so yeah, just to conclude, uh, all the demos that we have done in the session, you can find it, um, on that GitHub link, um, so you can scan this QR code, it will take you straight into that, uh, GitHub page. Um, yeah, and, uh, you can try, try it out in your own account, um, and, and test it out. Um, there's also a link to the, the compute blog post. So this is where we would, um, publish, um, you know, um, any, any, anything about step functions, the new releases, launches, some of the best practices, uh, how do you do things at scale and things like that. So all of those will be published in the under the compute block. Yeah. And, uh, for example, if you want to run even faster, you can just either increase the bed size or give it more concurrency. Just be aware that if you get throttled, things may get delayed and then you need to request a limit increase, uh, but those metrics that uh we launched recently for distributed map, they do, they do help on these cases that you can just see what's going on and see, oh I really have a backlog here. I need to handle that.