---
video_id: gLL2eOPxclU
video_url: https://www.youtube.com/watch?v=gLL2eOPxclU
is_generated: False
is_translatable: True
summary: "This session, \"Building on AWS resilience: Innovations for critical success (ARC207),\" explores the engineering principles and behind-the-scenes innovations that drive AWS cloud resilience. Presented by Mike Aiken and Gavin McCullough, the talk emphasizes AWS's \"anti-fragile\" approachâ€”learning from every incident to become stronger. They detail four key \"availability axioms\" guiding AWS engineering: 1) **Region Isolation**: Demonstrated by the transparent migration of the global STS endpoint to regional endpoints to eliminate cross-region dependencies and improve latency. 2) **AZ Resilience**: Focusing on detecting and mitigating \"gray failures\" (ambiguous, partial failures) using tools like \"Fleet Health Service\" (based on CloudWatch Contributor Insights) and \"Zonal Event Detector,\" which powers the customer-facing \"Zonal Auto-Shift\" feature to automatically move traffic away from impaired Availability Zones. 3) **Rigorous Testing**: Highlighting the use of \"Game Days\" and a dedicated, non-customer \"Game Day Region\" to constantly test failure modes and expand the \"competence envelope\" of services. 4) **Overload Protection**: Discussing research into \"metastable failures\" (where a system gets stuck in a failure loop due to retries/backlogs) and tools like \"Metaphor\" to model and prevent them. The session underscores AWS's obsession with resilience to support mission-critical customer workloads."
keywords: AWS Resilience, Availability Axioms, Region Isolation, Zonal Shift, Gray Failures, Metastable Failures, Game Day, Operational Excellence, Chaos Engineering, STS
---

All right, well, welcome to the session. At AWS we frequently talk about the many benefits of cloud computing, like on-demand capacity, automatic scaling, consistent security controls, and managed services that remove undifferentiated heavy lifting. But one of the less visible benefits of running on AWS is the fact that our teams constantly obsess about resilience. And we relentlessly pursue opportunities, both big and small. To improve the resilience of our services and your applications. At AWS we serve millions of customers across 38 regions, handling billions of requests daily. This scale provides us a unique opportunity to innovate and make investments that most companies couldn't justify in their own data centers. It also requires us to spend significant time diving deep on a myriad of intricate details that others may overlook or that many businesses wouldn't bother with. Resilience is more critical than ever for you, our customers, to deliver essential services to every corner of the globe that power economies, governments, critical infrastructure, and more. And after almost 8 years here, I've come to learn that at AWS we think a little bit differently about resilience. And I think it's summed up pretty well by this quote. We're proud of AWS's operating record and we hear feedback routinely from customers that when they moved to the AWS cloud, their reliability improved significantly. But we still have occasionally have incidents, and to some degree they're inevitable. Our obsession with resilience means that we are continuously trying to learn and improve from every incident, so that we'll have fewer future incidents and that they'll be shorter and smaller. We're aiming as an organization to be anti-fragile, so that after each incident, we're better than we were before. And so there's a lot of innovation that goes on to achieve this. Some of that innovation includes delivering transparent improvements in our services that are mostly invisible to customers. Other innovations seek to reduce the work that customers have to do on their side of the shared responsibility model. Letting AWS shoulder more of that burden for you, so you have less to do. My name is Mike Aiken. I'm a senior principal solutions architect supporting customers across AWS build and operate resilient workloads in the cloud. And I'm Gavin McCullough. I'm a senior principal engineer in the AWS resilience organization. We own services like Route 53 and Application recovery Controller, as well as working across AWS on improving our resilience. I also work as an AWS corps leader, which is our equivalent of an incident commander if you're familiar with that term. So today we want to pull back the curtain and share some of the behind the scenes innovation that helps make AWS the best place to run your mission critical workloads. Before we get too far into the details, let's talk a little bit about how we think about operations at AWS. We consider operational excellence to be a centrally important part of being an engineer or an engineering manager or a leader at Amazon. Our role guidelines talk about it. It's part of all of our performance and promotion discussions. It's a key part of what we do. And our developers don't just write code, they're on call for the code very deliberately. So they think carefully about the ser, how the service is going to operate in production. At its core, this is about ownership. We need our developer teams to truly own their customer experience. Both in the way the service works, the semantics of the APIs, and the performance of the service. Service teams will each run their own regular weekly operations meetings where they review recent incidents, examine metrics dashboards for anomalies, and plan out how to improve the operational state of the services they own over the coming weeks and months. And a common example conversation you'd hear at these meetings is, you see, since Tuesday, the 99th percentile latency on that API call has gone up. Do we understand what went on there? It's gone up by like 10 milliseconds. And we're constantly obsessing about little details like that, so that we're aware of all the changes we're making and what's happening. And we have really high expectations for engineers diving deep. It's not just about resolving issues when they come in, but understanding why they occurred and how we prevent them next time or how we become more resilient to them. As with any software services, AWS services do have incidents occasionally. Most are pretty innocuous, customers not even noticing. But we always have to take even the small issues seriously. In fact, the small issues are often the opportunity to learn. Obviously our top priority is always to mitigate customer impact as quickly as possible. But our next priority is to learn. We wanna analyze these events, learn the right lessons, and take action items that don't just correct a specific issue or bug, but make us more resilient for future unexpected events. Finally, we aim to share what we learn across teams. With thousands of developer teams, we want to share our learnings, mental models and best practices widely so that all the teams can learn from the lessons we we find. And this is really where we achieve the anti-fragility that Mike mentioned. As AWS and Amazon have grown, one of the challenges has been working out how to succinctly communicate the sort of core operating ideals across a very large organization. And so to help with that in recent time, we've been working on what we call availability axioms. And these are kind of a a set of concise best practices that our service teams all follow and can expect of each other. And having these short axioms gives us a common language in which to talk about our systems with each other. So let's look at a few examples. The first is what we call region isolation. We expect AWS regions to operate highly independently. So in the rare case that a problem occurs in region A, we know it will be limited to region A and will not affect any other region. And this very simple principle has major availability benefits for customers. As we now have 38 regions and growing, it's critical that each region operates independently. And the second axiom relates to availability zones. For those of you who might not be familiar, each AWS region is built out of 3 or more availability zones for redundancy. These are physically separated groups of data centers within typically one metropolitan area. Every AWS region is designed to gracefully handle one availability zone being impaired or in the rarest case even completely unavailable. So we should be able to lose one or multiple data centers in a single availability zone and just keep going as normal. So with that in mind, our AZ resilience axiom says that every regional AWS service must be built across multiple availability zones and able to tolerate an impairment to one AZ without degrading their customer experience. We push teams pretty hard to meet this standard, and we test it fairly frequently. An important third axiom is centered around testing. The code should be tested prior to going into production hopefully won't be new to anyone. But the axiom goes into quite specific details on best practices around unit integration and acceptance testing and what we consider best practice to be. And in addition, you'll see teams use chaos testing regularly and what we call game days. We'll talk more on that topic later. And one last example axiom is around overload protection. We've learned a lot over the years about protecting systems against overload and the many ways overload can arise. And a lot of people when you say this term, they will immediately think of bad actors, and that kind of thing can happen. But in practice, we often find that overload is not so much caused by bad actors, but how our systems interact in unexpected ways. And so all our systems have to be built to defend themselves against overload from all of their callers, not just outside callers. As with people, it's critically important that our systems have mechanisms to quickly and cheaply say nope when asked to do more work than they can successfully complete. And as with people, deciding who to say no to is also pretty important. So let's explore a set of recent examples in which AWS has been innovating to deliver on these four availability axioms. We're first gonna look at how we improved the regional isolation of AWS services and customer workloads. By exploring the evolution of the security token service over the past 14 years. STS allows a given IAM role or user to assume a different role temporarily to perform some task, usually with different privileges. It's kind of like the pseudo command on a Unix system except with one important difference. STS is aimed at assuming purpose-built, least privileged IAM roles. The mechanics are that when you call STS with the assume role API, it returns you a temporary set of API credentials for the IAM role you're assuming. These credentials are short term, typically expiring within an hour. STS is used in many different ways, from single sign-on systems or when you authorize an AWS service to take action on your behalf. When we launched SDS in 2011, long-term credentials and users were pretty standard across the industry. STS's use of short-term credentials and restricted roles was a significant innovation at the time. This approach may seem obvious now, but back then, this was a genuine innovation. We were exploring new territory and we were still learning about how the service was going to be used. It quickly became apparent though how powerful and important STS was going to be. As its popularity grew, so did its critical nature within the AWS ecosystem as more and more workflows started making use of it. Because STS was distributing credentials for IAM roles, we decided to locate the STSN point in, uh, in the same place as IAM in US East One. Both of these were considered global services, meaning that there was a single endpoint that users and services in all regions used to call its APIs. And this seemed like a minor detail at the time. But as we learned more about how this new service would be used, we started to realize that this wasn't the optimal choice for STS. While we originally thought of STS and IAM being strongly coupled siblings, there's actually some very real difference in how the services are used operationally. First, IAM is globally consistent. We replicate all of your roles and policies to every enabled region, so everything has the same view of your IAM resources. This requires a central source of truth managed by that global endpoint. In contrast, SDS is local and stateless. Tokens once produced, aren't stored by AWS. The same token can't be retrieved twice, so there's no similar need for a central source of truth or global consistency. Second, The IAM APIs are typically used. During IAM resource creation and updates. This normally happens at build time, when you're deploying cloud formation stacks or releasing resources through a pipeline. STS, on the other hand, is frequently used during runtime. Obtaining temporary credentials from STS was becoming part of the critical flow for applications and temporary access in AWS. A lot of services start depending on STS in ways that they wouldn't for IAM. And so this also means that SDS is called much more frequently than I am. And so as a TV show once said, one of these things isn't like the other. So the question we started to ask ourselves about SDS was, given how important it's becoming for AWS services and real-time important. And for our customers, could we make SDS regional and meet our region isolation isolation axiom to give customers that strict region isolation we want. We started work in 2014 to deliver regional isolation for STS. This was a pretty big decision. It involved a lot of investment. Uh, but it was the right thing to do for AWS and customers. And so by early 2015, SDS has launched regional endpoints in every region globally, and every new region we build has its own dedicated SDS. And we're encouraging customers to use the regional endpoints in preference to the global endpoints all the time. And as time passes, we see most customers adopting the regional endpoints, and things are going reasonably well. But slowly it starts to become clear that not everything's migrating, there's a sizable tail that's not moving. So in September of last year, we did a deep dive into this question. Although regional SDS has been available at this point for nearly a decade, a significant set of you, our customers, were still using that global SDS endpoint and not getting the isolation that we wanted everyone to have. So we realized that we needed to change our approach, that we had been thinking about it wrong. We'd made the regional SDS APIs available, recommended them strongly in docs, blogged about them. And we were and, but we were not getting the transition we'd hoped for. And more importantly, you guys were not getting the regional independence we wanted you to have. The fundamental problem was the migration to regional APIs required customers to take action. And at the scale that AWS's customer base is at this point, that was just unrealistic to expect everybody to move. So you can see the global STS API here running in US East 1 and the 16 other enabled by default regions around the world which people are calling it from. If we zoom in on North America, we can see what's happening a little more clearly. Even for workloads in these other regions, customers are regularly making calls back to US East One to get their short-lived credentials from the STS Global Endpoint. Obtaining those credentials introduces a cross-region dependency, and it doesn't provide the regional isolation that we wanted your workloads to have. This is how we want things to work. Requests originating from each region for SES credentials should be answered locally in that region. We didn't want requests to the global endpoint to actually go to US East 1 anymore. A workload, say in US West 2, when it calls the SDS Global Endpoint, should be answered in US West 2. And the same for every other region. This would ideally create that regional isolation that we want in STS without customers in those regions having to do any work to get it. But this giant kind of change can be complex. We really needed to understand the customer requirements and make that change with near perfect transparency. So here's what we did. So one of the biggest challenges in running cloud services, and any large service I think for for a lot of you too, is figuring out how to make big changes to your service while no customers notice. And our goal here, like I say, is near perfect transparency. So let's look at some of the requirements and what we did to meet them. Our core goal here is for every customer to get the their STS calls answered locally in region, without them needing to change anything. The first thing we did was to build out 16 perfect replicas of the global service, one in each additional region. So we have 17 global endpoints in total. The regional service looks slightly different to the global one. So from the perspective of clients, if we just rerouted them to the regional endpoint, some customers would have seen problems with that. So we decided that for now, the simplest thing to do was to actually run a separate global endpoint in each of those default regions. It's worth noting that opt-in regions, the more recent regions that we've launched, already require use of regional SDS out of the box. So we didn't really need to worry about them so much. Customers were not using the global endpoint from there. When we spoke with our identity experts, they asked us to ensure quite carefully that we change, that the change we make was to answer clients in the local region where they were only. So for example, global SDS requests made in Sydney were handled in Virginia before we started. And what we wanted to do was answer them locally in Sydney, but to ensure we never routed them to any other region. So requests that originated outside an AWS region would continue to go to US East One. So nobody was routed to a new region that they weren't normally talking to. To achieve this, we used an integration with the RV 53 resolver, the DNS resolver that you use in your VPC. So when you query SDS. Amazon AWS.com, it now transparently returns you the IPs of the new in-region endpoints. We made this change very carefully, one AZ at a time across each region and monitored to make sure the workloads were continuing to work as normal. One of the other aspects that we had to think about was cloud trail logs. Every time you make a call to STS an audit log is delivered to cloud trail. Most AWS customers configure global cloud trail logs, so that all of their logs are globally delivered to a single region where they can process them. But for customers who don't, which was a sizable minority, cloud trail logs are generally emitted in the region where the service they're calling is running. And up to now, Global SDS was running in US East One, so those logs would have been emitted in US East One. But we need to be careful that even while we now answer that SDS request in Sydney, we want the log to continue to show up in US East One, so no one's taken by surprise. And to ensure we're being open and transparent about this, Cloudtrail actually added an extra metadata line to tell you in which region we answered the request. Over the longer term, our strategy is to eventually turn off the global endpoint. We don't really want it to be global. This project is about accelerating that change and giving customers the benefits much earlier. The newer AWS SDKs were already all using the regional SDS endpoints out of the box. But we realized that a sizable portion of customers were using older major versions which still defaulted to the global endpoint. So as part of the longer term strategy, we're updating, we have updated all the supported SDKs at this point to ensure they default to the regional SDSN point. It's going to take a few years as customers slowly move and deploy and so on. For this to take effect, uh, but, so, but over time we expect traffic to drain from the global endpoint. The last thing we, we, we said near perfect transparency. But there is at least one notable exception when we figured that it would be OK that this change would be visible to customers. If you answer a request that originated in Sydney, locally in Sydney, it's going to be significantly faster as the request doesn't have to travel halfway around the world and back. But we were pretty confident that customers, if they noticed this, would be OK with it. So in April of this year, we publicly announced that this work had been completed. SDS had deployed a dedicated SDS global endpoint infrastructure in every enabled by default region and rerouted traffic to it within those regions from customer VPCs. So now that it's done, what was the effect? As the work proceeded through this project, the SDS team tracked the number of unique accounts they were seeing making cross-region calls back to the global endpoint in the US East One. In order to show it here, we've normalized this as a percentage of the total callers on the Y axis. And so as you can see as the project starts, we're oscillating below 100%, and as the project proceeds from about mid-March to about mid-April, this number drops down to practically 0. Many thousands of accounts which were dependent on cross-region calls to STS are now being answered locally, in region, with no changes made on their part. And as I mentioned, a nice side effect of this work was in the performance or the latency of these calls. As obsessive operators, the STST monitors the performance of calls to, to the global STSN point from every region. So as calls had previously had to transit from Singapore or Sydney or wherever to North Virginia, the latency of responses tended to be dominated by the network round trip time. And you can see the STS team's measurements of latency change as the project proceeds. It was really satisfying to see calls that previously took up to 230 milliseconds, dropping to 20 milliseconds even at P99. So our first availability axiom was ensuring that our regions operate independently. And focusing on this axiom drove a major change to ensure that all customers of STS have their calls answered in region, even those who continue calling the global STS endpoint. And this has significantly improved both the reliability and performance of STS as you experience it. The next axiom we want to talk about is how we're making our services and customer workloads more resilient during single AZ impairments and gray failures. To see what we've done here, we need to spend a little bit of time talking about a service called AWS Lambda. Like SDS, lamb has also become a critical service to both our customers and our own services. I remember back when I was a customer, uh, an AWS Lambda had launched in 2014. Uh, back then as a newbie on AWS, I didn't really understand how or why to use lambda. I was still humming along on EC2 instances. Uh, but now after, uh, a decade later, AWS Lambda has become a core compute primitive for event-driven architectures. I use it in almost everything that I build now. It's a core component of a number of AWS services, and it runs mission critical workloads for customers. But to get to that point, we've had to innovate a lot to make it resilient. Lambda operates at a massive scale. Their fleets often have tens of thousands of nodes in them. And these are the fleets where we run your lambda code, and like everywhere in AWS we really care about your customer experience. It's our top priority. And this makes managing the health of these fleets critical to the to the experience Lambda delivers. But occasionally, there can be a single instance that becomes impaired. It doesn't always outright fail, but something happens where it starts misbehaving, maybe becoming a little slower or producing more errors than the rest. When you have just one bad node in a fleet of 10,000, it can reduce availability to 99.99%. And Lambda's availability goals are actually more aggressive than this. These bad nodes that are limping along, not completely failed, can also be hard to find. So recovery can be difficult and require manual intervention. We went into lambda to be more resilient when these types of gray failures occur. Automated detection and recovery are essential to maintaining really high availability numbers. But to do this, we needed to change our approach to fleet health observability. So especially with big services like lambda, one of the things that we've learned over time, as one customer put it in her blog, was your 9s are not my 9s. A single node in a fleet of 10,000 experiencing faults, as Mike says, might only drop the avail the service availability by 0.001%. If you consider it as a simple aggregate. But that doesn't always capture the real experience of customers. What may be happening, happening in practice is that most customers, Have a perfect availability experience and a small number have noticeably worse. So if you as a customer happen to be unlucky, and the single bad node is hosting many of your lambda invocations, you could be having a significantly worse time. And this is why we consider it so important to obsess over even that one instance in 10,000. We really need to detect and remediate this fast, as there may be a customer who's really noticing it. And when these kinds of situations occur, it would be natural to say, should we not just have better health checks? Is this not that simple? The thing is that when a, when a host fails unambiguously, like he loses power, a simple shallow health check works great. When a host is down or stops responding, we can detect and remove it easily. And in particular, taking down a host that's already, or taking out a host that's already down is not going to make the situation any worse. It's not, it's not serving anything. But the scenarios we're dealing with here are what we call gray failures. The host is still answering. It's doing so slowly, intermittently, with unusually high error rates maybe. And so the health state of this host is a bit more a bit more ambiguous. In particular, if we were to remove a lot of gray failing hosts at the same time, we could actually make the situation worse. So we have to be much more careful with gray failures. One approach is to employ what we call deep health checks. And these examine the host application much more stringently to identify these gray failures. But in doing this, we have to be careful not to set ourselves up for an overreaction. Suppose a significant portion of the fleet might degrade together, due to a spike in load or a problem with a dependency maybe. The hosts are all still doing some good work. Automatically removing all of the hosts that have just gone above some threshold would be a dangerous thing to do. So this leaves us with this kind of hard decision of how deep can I make my health checks without adding further risks. And when we're dealing with great failures, Simple binary health checks are really hard to get right. What we've learned is that when you're dealing with gray failures. Health checks really need to be budgeted. We have to limit how much functioning capacity we would ever take out automatically. And they have to be non-binary ideally. In practice, no host is ever truly perfect. So we need to stack rank the hosts and take out the worst ones and leave the, the good ones in within our budget. So here's an example of health check budgets. Supposed to deal with gray failures, we say any instance is unhealthy if its error rate exceeds some number. And we need to ensure we won't overreact and take too much of the fleet out. We need to know how much we can afford to remove. So I mentioned that this is a non-binary comparison as well. Health checks need to compare health among all available resources, not just against a static threshold. We can see pretty clearly that this instance is an outlier with a much higher error rate. But we're doing that not based on some known threshold, but by looking for the outlier among the large number. We're looking for the proverbial needle in a haystack. We want to find the single instance or two with the problems, when the others might have tiny error rates, but they're generally fine. So how do we do that? How can we compare metrics like errors among a pool of tens of thousands of nodes quickly? In AWS services, we tend to send structured log files to cloud watch logs, typically using our embedded metric format. And this allows us to combine logging and metric production into a single pane of glass. Because we have metric data in our log files, we can analyze that data using Cloudwaatch contributor insights. Contributor Insights actually started as an internal AWS monitoring tool that we made available to customers as well. And if you were to ask our service teams, you'd probably find that contributor Insights is one of their favorite features of CloudWatch, and that's because at our scale we have so many customers, each with their own unique individual experience, and it helps us understand those individual experiences without having a per customer metric. It's way more cost efficient and it allows us to focus on just the top end contributors. And those are the customers that may be having a different experience than everyone else. And so the service allows us to look at very high cardinality data, meaning there are a lot of unique keys like instance IDs or customer IDs, and then graph the top end contributors. Using some data points from those log files, and so it produces visualizations like this. Now we can visually detect when there's an outlier for error count in a fleet of thousands of nodes. But we want to do more than visualize this data, right? We want to take action on it using automation. And so we built a fleet health service in Lambda. Instances send their log files to cloudwatch logs, and you can see here our log files include data like fault count and instance ID. Then we analyze that log data with contributor insights rules, looking for instance contributors to faults. Next, we create a cloud watch metric math alarm on those rules. Here we're looking for any contributor that's responsible for 66% of the total errors the service is seeing. We found that large percentages like this work pretty well. They're fairly conservative, which means they're not going to produce a lot of false positives, but even a large value like 60 to 70%, will reliably find unhealthy instances. Once the alarm is triggered, a worker node responds. First it finds the top contributor's ID. Then it checks its token bucket. This is a really important step. We don't want automation like this to run away due to a bug or some unforeseen condition. So there's a velocity control on how many times the health service can do this within a defined time span. Then if the instance is part of an auto scaling group, it uses the set instance health API to mark it as unhealthy and lets auto scaling do its thing. Otherwise it terminates the instance and allows the service to control replacing it. And so this service started as something that Lambda built, but it's actually become a general purpose health service that lots of AWS services take advantage of. It's an example of how a lot of innovation happens at AWS. An individual team or even a single engineer has a great idea and they build something that's useful for lots of different teams, and it ends up making them all more resilient. Another challenge we saw in lambda were gray failures affecting single availability zones. Where some event impacts the latency or availability of resources in that zone. And while we run your invocations across multiple AZs, even a small impairment in a single AZ. Could have significant impact to your resources. What we realized is that the same outlier detection approach would work equally well to find impaired availability zones. It's basically the same problem with a much lower cardinality. So lambda built zonal event detector. It looks at different metrics to detect what an AZ may be unhealthy. Allowing them to shift away from that zone and only invoke your functions in the AZs that they observe to be healthy. And so like the Fleet Health Service, zonal event detector works so well. Other services wanted to use it too. And so we started to build more capabilities that could use this telemetry for detecting single AZ impairments. And one of the features that we launched in 2022 is. Zonal shift. Zonal shift is an API that allows us to manually or automatically shift traffic and resources away from a specified availability zone. And so combined with the zonal event detector, we now have common signals and standard tools that all our service teams can use to automatically shift away from an availability zone in the event that an impairment happens. And we also put this capability in the hands of customers, so you too can use the same tools as AWS to shift away from an AZ. We even made Zonal shift free to customers. It supports all NLBs and ALBs as well as all the scaling groups and EKS clusters. When you trigger a zonal shift, we inform both the DNS and load balancing layers to stop, so the traffic stops being sent to the AZ you've shifted away from. Auto-scaling sends all new instance scaling and replacements to the remaining AZs. EKS removes endpoint slices from the endpoint and stops scheduling new pods in that impaired AZ. For safety, we strictly limit zonal shifts to one AZ at a time. We don't want to take 2 AZ's out. We've yet to meet a customer who is sufficiently largely scaled that they could run, that they could lose 2 out of 3 AZs and keep going. We also recognize that for customers detecting a zonal event can be quite difficult. So we saw another opportunity, having given the API for free, we saw another opportunity to shift more of the responsibility to our side. We're in a position running the platform. To look at a broader set of telemetry, including our infrastructure metrics, allowing us to better detect when these things happen and faster, and respond quickly on behalf of customers. So we use the zonal event detector service that was born in lambda as a system-wide source of telemetry about AZ health. And it powers what we call zonal auto shift. With Auto shift, if you enable it, we manage all the observability and perform the shift for you if there's an issue in an AZ. When one of our metrics like connection failures or network reachability or the quantity of missing metrics even that shows that one AZ is an outlier, Z goes into alarm, and we trigger a zonal shift. Again, we strictly limit this to one AZ at a time. We use this capability for both AWS services and customer applications. So that means when you're interacting with a regional AWS service like SQS or Lambda or S3, we're automatically responding to the event, so those AWS services recover really quickly. For resources that you've enrolled in AutoShift, we're performing the same action for you at the same time. So this provides a single automated response driven by zonal event detector and ensures that we're all able to make the best possible use of availability zones. So our second availability axiom. Is around resilience to AZ impairments, and in particular to gray failures. And this axiom inspired the Lambdas Fleet Health Service, the zonal event detector, as well as zonal shift and auto shift. The next axiom we're gonna look at is testing rigorously. At AWS we commonly say that if you haven't tested something in the past week, it's probably broken. Regular testing is part of our culture. In order to use Zonal AutoShift, we also enroll you in practice mode. Following a per resource practice schedule, we perform a weekly auto shift to make sure all of our service teams and customers are ready. And can actually perform a zonal shift without impact to their services or applications. We regularly do this in our production fleets, and it helps build confidence that all of this is going to work when we need it. So just like an extra point kicker, they practice over and over, going through the same mechanics every time. And so when it's game day, Nothing changes They do the exact same thing they've been practicing all along. Extra point kickers tend to make most of their most of their kicks. It's like 95 to 96%. Uh, and it's because practice is so similar to the real thing. It's all muscle memory at that point. They have a high degree of confidence they're going to make that kick. Uh, so as an undergrad I was a philosophy major, uh, and I never thought that I would be quoting philosophy on stage, lest it reinvent, uh, but I think our mental model is captured really well by this quote. We are what we repeatedly do. Excellence then is not an act but a habit. And so while Will Durant and Aristotle were actually talking about ethics, I do think this way of thinking is equally applicable to many different disciplines. By practicing over and over, we build up this idea of excellence. When the real thing happens, right, it's just that muscle memory kicking in. So one of the ways that we do this is with what we call game days. We wanna know that our services are resilient to a variety of different failure modes. Individual teams will test their services in pre-production environments, obviously. But when we build new AWS regions, before those regions go live, we usually set aside a week or two to run a set of game day tests on the production services that are ready to go. So this gives us an opportunity to test in a real production environment with all the services there at scale without risking any impact to any customers. We test a variety of different things like power outages, network partitions, and service failures. And they help us validate how the other services will behave during failures, as well as how they're gonna recover when things go back to normal. These also provide an opportunity for our on-call engineers to train for real events, exercise and refine their run books, and ensure the required observability is in place so that they can see and detect the event when it happens. It helps validate what we call our competence envelope. Professor David Woods at the Ohio State University talks about socio-technical systems and having a competent, competence envelope, excuse me. And this is an analogy with an aircraft's flight envelope, which defines the safe boundaries in which an aircraft can operate in terms of altitude and speed. Pilots know to stay within the competence envelope as it is the safe tested space. If they go outside of that, it may be fine, but things become more uncertain, so they try not to do that. So similarly here, a competence envelope is the space within which a system is known to function competently or robustly. And when pushed beyond its competence envelope, systems may still function, but tend to be more prone to failure. And this competence envelope we find is a useful mental model to think about test coverage. If you think about a system you own and all of the modes you expect it to be able to operate in, such as we have a couple of fallback paths in the code where it will call a different way, or how it will deal with the failure of a host if that happens. When was the last time you actually tested all of those? You're assuming they're in your competence envelope, but when did we last test them? So as we think about a competence envelope applied to distributed systems, there's a set of inputs, parameters, and failure modes that our systems operate in with expected behavior. We're generally not near those competence envelope boundaries, so we're comfortable and safe. But as we start to move to the edge of our competence envelope, and even beyond it, by being exposed to modes that we haven't seen before, we enter the danger zone, right, where our system can start to exhibit unexpected behavior to unknown conditions. In our case, it's not flying outside of a max altitude or top speed, it's dealing with failure modes that we haven't practiced with. This is where surprises happen and where danger can occur. So our competence envelope is defined by the failure modes we regularly test against. And while I'm showing our operating mode expanding here, it can also be true that our competence envelope shrinks. If we're not regularly testing these operating modes, we might not get the expected behavior. And so how do we maintain or even expand that competence envelope? Well, in airplanes, you can't really change the characteristics of the flight envelope, right? You're kind of stuck with max altitude and top speed. But fortunately in distributed systems, our ability to produce expected behavior is dynamic. By regularly testing and validating our expected behavior. We can maintain the size of our envelope and we can grow it by regularly exposing our services to a set of failure modes that we might encounter in the future. We want fewer surprises. And we want predictable behavior. And we do that by turning unknowns into knowns through constant testing. And so this is part of the test rigorously axiom. If we haven't tested it recently, it's broken. It won't work as expected, and we'll find ourselves in danger of not meeting our goals. Doing game days for individual teams in pre-production environments in our new region builds is a great start. But we only have so many new region builds and a short window between when all of our services are ready. And when we want to go GA and start onboarding customers. And so we want to do it at a much greater velocity with all of our services in production environments. The results from these new region launch game days were so useful that we decided to make a more significant investment and build a dedicated full AWS region just for this kind of testing. We wanted to be able to run game day tests in a safe space with no customers anytime, and not just in the run-up to a region launch. So we recently finished building an entire test AWS region with 3 availability zones and a full complement of the AWS services that come at region launch. To be clear, it won't be open to you as customers. It's not going to show up on your console, and it won't be on the health status dashboards because we're breaking it all the time. The the game day region is helping us to further expand our understanding of how our systems function in rare conditions. The idea is that our service teams all treat the game day region as just another production region. They don't think of it as gamma. It just happens to be a region that doesn't have any customers in it. And this allows us to continuously propose and schedule game days without waiting for the next region to launch, and the short window we get then. So what are the kinds of things we test? The game day tests tend to be a mix of some standard benchmark tests like powering down or disconnecting one AZ from the network. It's pretty common. Uh, a common dependency service that a lot of teams use, we might break that, or we might uh perform a cold cold restart of a service to see how they will recover and how the services using them will recover. We also commonly perform more specific tests to validate if we have an incident and we, we, we put a fix into some code to solve it, we will actually perform that, simulate that that issue later to validate that in the, in the test region that really does work and does recover the way we think. From time to time we also look at novel rare events that we've just never seen before, but we wanna know what will happen if that event ever happens. So our 3rd axiom focuses on rigorous testing. And this has inspired not only a set of high standards in terms of continuous integration and deployment, but also the concept of running these region-wide game days and building a dedicated region for that purpose. Our final axiom is going to be about protecting against overload. Overload is one of the most common sources of impact that we see. And while we have a number of protection mechanisms like load shedding, rate limiting, adaptive retry strategies, and more, there is a type of overload situation that's quite rare but can be really impactful. We've seen it occur in our services and it's something that we like to prevent. What I'm talking about are meta-stable failures. At a high level of meta stable failure looks like this. First, the system starts in a stable state and everything's good. But then over some period of time, load is going to increase and we cross the threshold and enter this vulnerable state. The system is still healthy And it's not in an overloaded state yet, and the system could run like this for months or even years. And then at some point, there's a trigger, and it creates a condition where the combination of existing load combined with that spike transitions the system into a meta-stable state. In the meta stable state, a feedback loop sustains the overload effect and keeps the system in a stable down state, not recovering. And the system maintains and remains in that state until a big enough corrective action is applied, which is typically through some kind of manual intervention by an operator. So let's look at a little bit more of a concrete example. One common type of scenario where we see meta stable failures happen are in queues. And it's important to know that queues are all around us, right? They might be embedded in libraries that you use or part of how networking equipment manages moving packets around. So lots of places where a queue could exist in your system, even if you're not using one directly like an SQS queue. So under normal conditions, our server here can process messages in the queue and the queue depth stays manageable. Doesn't grow beyond what the system can successfully process. But when there's a sudden load spike, in this case that's that trigger effect, the queue depth grows. And so the server continues to process messages unaware of the spike. But as it's working its way through the backlog, It can't get through them all. Eventually the messages in the queue get older and older. And now things that we're waiting. On those uh messages to be processed, maybe like a TCP client, they've stopped waiting. So we start to have wasted work, right, as the server processes messages, they don't have any chance at success. And because we're processing first in, first out, we're always working on the oldest messages in the queue. And so then we get more new requests coming in. Adding more work to that backlog. And of course those clients that got tired of waiting, well, they're going to retry the requests, continuing to add to this queue depth. Meanwhile, our poor server is just continuing to process the oldest messages in the queue, and it's continuing to perform wasted work. And so the combination of steady state requests and the work amplification of retries have created that sustaining effect. The queue depth at this point is insurmountable, so we can never get to the messages that could be successfully processed at the top. Even after that load spike has been removed, the system is in a meta stable state. It's doing lots of work, just none of it is useful. So we've seen a pattern of these meta stable failures across the industry. They can lie dormant in systems, especially those with queues, though not only queues. So as meta stable failures tend to have long recovery times, they're a high priority for us to find and prevent. So as AWS we brought together a group of scientists within the company to study them deeply and ask, how do we efficiently find systems that have these vulnerabilities, so we can test and fix them proactively. One of the big challenges with so many services and microservices is how would we figure out which services might have a latent meta stable failure. Direct testing is pretty expensive. You don't want to do it in production, obviously, and for each single service, the parameter space of queue lengths, load, and retry rates is very large. So the scientists built a multi-step strategy to efficiently search this parameter space and map out where we should test it. They also open sourced a lot of it and published it so that you guys could potentially use it if you liked too. So their strategy consists of a statistical model that can quickly explore the full space for a given service. This one's been open sourced. A single process simulation that can also run on a laptop and cheaply simulate one single data point. An emulation with physical servers running dummy code that cheaply imitates the real service and clients. And then finally, once we've narrowed down on something that we think is really a potentially potential problem area. We can run tests with the real application and validate the results. And this strategy of kind of quickly searching our way through the space saves us a huge amount of time and energy. And as I mentioned, the statistical modeling piece, the team built and open sourced a tool which we call which they called metaphor. This performs the calculation and creates these visualizations. It's a really lightweight way to begin understanding where a meta stable failure might occur in a system. So in this visualization you can see the queue length on the X axis and the number of outstanding retries on the Y axis. And each point on the graph represents a point in the state space. The arrows visualize the most probable transition in the next time unit. So where you see the arrows moving left, we're likely draining the queue over time. And where the arrows are moving right, the queue is likely growing. And the team released a paper recently with the details of the research and presented it at Hot OS 2025. If you'd like more details, the research paper and GitHub link are there in these QR codes. So our 4th and final axiom for today was for our services to protect themselves against overload. And this axiom has inspired us to fund work on tipping point and meta-stable failures. So we've taken a look at some of the behind the scenes work where AWS is innovating on your behalf in areas that may not be completely obvious obvious to you as our customers. We've helped migrate almost all of the existing global SDS traffic transparently to be served in each region, helping strengthen the regional isolation of customer workloads. We built a fleet health service and zonal event detector that powers zonal auto shift, allowing AWS services and customers to detect and respond quickly to single AZ impairments. We built the AWS test region so that we can run game day activities all of the time, accelerating our ability to continuously validate expected behaviors in our services, and continue to expand our competence envelope for new operating modes. Finally, we're actively pursuing research on meta stable failures to help protect our services against rare but impactful scenarios that can create overload situations. We're always looking for ways to innovate that transparently improves the resilience of the cloud and reduces the burden for customers to achieve their resilience goals. Our obsession with resilience and our culture of innovation makes AWS the best place to run mission critical workloads. Thanks so much for attending the session on building AWS resilience.