---
video_id: Lwvv8Q33eeE
video_url: https://www.youtube.com/watch?v=Lwvv8Q33eeE
is_generated: False
is_translatable: True
---

So the title of this session is balancing Cost, Performance, and reliability for AI at Enterprise scale. I'm Jared Dean. I'm a principal solutions architect with the Bedrock team. And joining me today we have Anker, who's a principal product manager with Amazon Bedrock, and then also Deepen, who's a senior engineering manager for AI Foundations at Intuit. Uh, we're going to go through today and talk about the various aspects. Of um of building this enterprise scale and how you the choices and things that are available from recent releases in the Amazon Bedrock portfolio offering. So first we've done some introductions. Next we're gonna do an overview of options. Then we'll go and look at a customer experience. I know that's always beneficial to many of you. Uh, then we'll go through some technical details and then we'll have some Q&A at the end where we'll all come on stage and be able to answer your questions. All right, how many of you got to Las Vegas for the conference via airplane? Perfect. Alright then, it's hopefully this analogy will then I'll, I'll resonate with all of you. So when we, um, when we travel, we have a variety of choices. So the analogy I'm gonna use here is airline-based since we've all traveled and how that relates to some of the bedrock offerings that we have. It's not a perfect analogy. So please don't look for poking holes, but I, the, the goal here is to give you. Information to help use this analogy. So first we have private planes. So the value of a private plane, you know, I don't have to show hands if we got a private plane, but I'm jealous if you do out here this way. So private planes, they're there for your disposal. They leave on your schedule when you're ready to, not that you have to conform to their schedule, um, and it's the most comfortable option available. OK, next we have the first class cabin. First class you get priority boarding, you get a dedicated overhead space, there's a premium service, but you pay you pay an increased cost for that service. The next one is we have the economy plus. This is where I end up traveling most of the time, is where we have, you have better seats, you get earlier boarding but not the first ones on the plane. And then you for a reasonable price you can upgrade to sit in those seats and then how my flight's gonna be tomorrow evening on my red eye, I'm gonna be at the back of the plane, OK, so we have the basic economy seat, OK, it's the lowest price, you're the last to board. You might have to put your bag, get checked at the end, you might not have overhead space for it, but you, you get on the plane, you're able to get to your destination, and it's the most economical. Keep doing this, the most economical way to get through, OK? So those are the kind of the four ways of air travel, and I assume all of you arrived in Las Vegas via one of those methods, alright? So that is analogous to the inference tiers that are available in Amazon Bedrock. Alright, so in the last two weeks we have announced a number of options that are available and so just to compare those, the analogy here is that we have the priority tier, which would be similar to that first class cabin, the priority access to to fulfilling an inference request. We have the standard cabin, which is the basic, which is the economy cab or the basic economy plus. Where you have that standard request, that's what you've been using in Bedrock thus far when you've done an on-demand request, you use that standard tier. Moving down to the 2nd row, we have what we called flex, and so that would be equivalent to kind of the basic economy in the back and there's some trade-offs there. You get a discount on your tokens served, but it also takes the longest to fulfill those requests. And then finally reserved capacity would be the equivalent of the private plane. There's dedicated capacity available to you that's reserved on your behalf and that allows you to use it at your leisure. All right, those are the four offerings that we have available and the expansion that's been done in the last couple of weeks if you've seen those announcements from the Bedrock team. Make sure I didn't miss anything here on the slides, OK. All right, so with these requests and these new options that you have available to you, we know that you're now trying to figure out how do I take advantage of and optimize. How do we optimize for what to do? All right, so if we think about with every production workload. You have to balance across three axes. There is the accuracy of the of the model. The speed and latency of the inference that you need and then also the cost, and these are the three main components that factor into every workload that you run in some kind of large language model inferencing workload. The critical of these applications is use case dependent and so some some models you are trying to find the smartest answer because you need. Man, I am really struggling with the clicker today. I apologize. Hold it out here like this So with some of with all of these workloads, every workload that you that you encounter, you're trying to balance these three options, um, and so using the smartest model often means an increased price and reduced or increased latency. It takes longer to complete. And so for some workloads you need that high accuracy. For other workloads you need a good enough answer, and so you're willing to sacrifice the accuracy of your model, the accuracy of the answer, in order to reduce the cost and or increase the speed. These are the trade-offs I think you're all familiar with. And so one of the goals of the tiers that we have released is to make that more apparent. Traditionally in Bedrock, you know, looking back over the last year, you really have the opportunity to give on-demand requests with no differentiation of whether this was a high priority traffic or a less important kind of traffic that could tolerate more latency and so we're trying to give you that optionality. And that's the main goal here. So now we, while we realize that. Across cost latency and accuracy, there's other things that factor into than just the inference decision that you're making. That's what we're gonna focus on today. But there's also which models do you pick and also which features do you take advantage of. All right, so I don't want to, I don't want to minimize that the problem is, is a larger scope in that you have lots of choices that you're working through, but here today we're just gonna focus on the inference aspects and the choices that you have at run time for where you're making those decisions. All right, so to give you some examples of where we think, going back to our airline analogy, going back to thinking about which things would be critical or latency sensitive and where we want to use potentially a priority tier for them, uh, some of the ideas I came up with were around flight booking during checkout. So once you decide to purchase that flight, you want to know right away whether that flight you've been successfully gotten a seat or purchased your ticket. Also, if there was a gate change announcement for any of you as you were traveling here, you'd want to know about that gate change announcement immediately so that you could start navigating to the new gate to get on the plane. If that, if we didn't want the airline to say, oh, that's OK if that gets completed in 40 to 45 minutes. Of completion because then you're going to miss your flight. OK. And then the third one would be mobile app check-in. That is when you check in for your flight, you want to get that boarding pass, you can add it to your Apple or Google Wallet so that you're able to then proceed to the gate. Um, so these are kind of all latency sensitive applications and, and again as we talked about today, you now have those options to be able to say this is a latency sensitive application, I want to prioritize that traffic, or, and in the next slide. These are things that are less latency that are more latency tolerant, and I'm happy to take a discount in the pricing of those as well as I don't need them completed right away, OK? And so some things that are latency tolerant as examples in the airline industry would be like crew scheduling and assignments. Those are things that happen, take place weeks and months in advance, um, loyalty program mileage posting. I'm happy to look at what my status is on different airlines, but if that happens in the next day, it happens 2 hours from now, that's not critical to my travel. And so that's a latency sensitive, that's not latency sensitive, and so an airline could process those at a later time. And similarly I assume you have workloads. There are many of the companies that I work with, they have workloads that aren't urgent, that just need to be completed. That need to be completed. At some point in time during the business day but are not urgent right now and so these new tiers allow you the opportunity to prioritize those and tell us which things are latency sensitive and which ones are not, and then corresponding prices for those. All right. So just to kind of recap here before we hand off and, and learn about um some personal experience from, from Intuit is we have the on-demand tier. The reserved tier and then batch inference. OK, so that was what's existed and then what we've added to that is now within that on-demand tier we have priority, in which case you can you can identify that this inference request on a request by request basis, this inference request. Is priority, in which case it will be prioritized, and there's a premium price paid for that. There's standard, which is what you've been consistently using for the last 11, 2 years with Bedrock, and then there's also the new flex tier which then allows you to receive a discount but also integrates in your event-driven architectures that you have today. So the distinction between flex and batch would be that one is event driven and one would be then set into a batch job, OK. Alright, so this is what the this is what the picture looks like today in the landscape of what's available to you as you choose how to do inferencing for the various workloads that you have um on Bedrock. All right, now we're gonna hand over to Intuit to learn about their experience and how they're using these tiers and and the workloads. Thanks. Thank you. Right. Hey guys, uh, I'm Deepain. I lead AI Foundation, uh, hosting service at Intuit, and today I'm here to talk about how Intuit serves, uh, millions of LLM requests using all of this flexible inference option that Bedrock provides, and how Intuit has balanced the cost versus reliability. But before going into that, let me introduce you to the Intuit. Intuit's mission is to power prosperity around the world. And how we do it. So our strategy is to build an AI-driven expert platform. To, uh, serve hundreds of millions of our consumers, small businesses, and mid-market customers, so that they can earn more money, they are confident, and they can make their financial decision with more confidence. These are some of the experiences that are currently powered at Intuit using AI. It's a combination of both generative AI as well as classical AI, uh, dock extraction for receipts, automated data entry for TurboTax, and the third one over here is around cash flow forecasting. Now, taking a deep dive into how Intuit has solved the cost, reliability, and the performance to serve all of these LLM requests. At Intuit, we have one platform. We call it Intuit Genos. It has multiple different components to it, but one of the main components is the model router. Now, we have one model router which serves different products, but we only have one model gateway. So the challenge over there is how we can. So, different use cases, which has different LLM needs. Some requires high reliability, some requires low latency. Some of the use case are very seasonal in nature, especially for TurboTax. And for some of the use case, we see daily spiky traffic, right? And LLMs are expensive. We cannot, uh, host LLMs at scale because it's going to be not cost-effective at all. The return on investment would not be great. So, over here, I'm going to take a double tap into different use case, right, depending on the nature of the traffic and how we have solved for it. So, especially for TurboTax, right, it's very seasonal in nature. So we know that there would be a need for high throughput for a short time, and we can forecast the steady token profile that would be needed for that. We are leveraging the reserved model for bedrock that helps us pre uh purchase that capacity that can help us help us power those experiences with high reliability and predictable latency, right? So the outcome over here is we are paying a premium. It's just like a private plane that Jared was talking about earlier, that we can use it the way we want to, right? It gives us that flexibility, but it gives the guarantee that business needs. Right? But it, it comes at a cost. For other use cases where we see a lot of daily interactive traffic spike, right, for which the reserved model does not work, we cannot provision reserve capacity for a spike in traffic. It would be a lot of cost wastage because of underutilization. And we cannot, uh, forecast those spikes efficiently. That's where the priority or the standard mode offering really helps us where. We know for the critical traffic, which are, which falls under daily spike, for which we want to serve it with low latency, we can leverage the priority mode, which is available. And for the, for the spikes, which are low in, which is, which is not that critical, but still latency important, we can definitely use the standard, the on-demand capacity model from the bedrock. Now, the advantage over here is priority is more expensive than the standard tier, right? But if we have to look at the amortized rate over the whole year, even if we are paying a premium for the priority, it is not as expensive as purchasing reserve capacity for throughout the year or for like months. We are just only paying premium for what we use. It's not for 24 + 7. And the third where In this day and age for generative AI we are seeing a lot of experimentations happening. A lot of new experiments and use cases are being launched under beta testing, right, for which, which are not that critical because they are not yet production ready. And uh so it it falls under non-critical, but still, the nature of the requests are very real time. It's not completely batch. That's where the flex mode helps us to plan the workload accordingly, where we get a lot of discounted rate on off each and every one of our requests. That's where the the throughput, we can get the uh designated throughput that we need for the model, but the latency is not that important because the, the, the flow is not critical for that use case. So that's how we have planned. The LLM serving framework that we have at Intuit, depending on your use case and what what are your traffic, basically, how the traffic looks like for a use case. This is a mental map of Sorry, this is a mental map of uh how we have planned across different capacity, different mode, and different use case. So we have reserved, which is very seasonal, where we know we need guaranteed capacity for, uh, for priority and standard, which is latency sensitive but spiky in nature, and for flex for non-critical and offline jobs. With that, I'll hand it over to Ankur, who will cover the technical details. There you go. Right. Thanks Deep in Hello everyone, I'm Ankur Desai. I'm a product manager on the Bedrock team, uh, and I've been involved with the launches of the service tiers recently. Um, so let's, uh, deep dive into the technical details so that we understand how to use these service tiers and what do they actually mean when you use them. Um, let's start with the simplest one, the standard tier. Um, How many of you have used Bedrock recently? So it is, it is highly likely that all of you have used the standard tier, uh, without calling it the standard tier, right? Because now the nomenclature is out, but we didn't have Priority or flex, you just had Bedrock on demand. Um, so this is a tier that is designed for your day to day GNAI workloads. Um, and these workloads sometimes can tolerate some amount of retries. What that means is, um, even when you're in your Uh, defined bedrock quota, sometimes you can see a little bit of throttling, and when you see that, you have to retry the request and it most likely goes through the next time. Um, so this standard tier is, even though it's best effort, we do have alarming frameworks, we do have dashboards where if we see a lot of throttling, people wake up in the middle of the night to make sure that, you know, we rebalance the capacity. Uh, the throttling limits are under our designated thresholds, right? Um, So, you have all been using standard tier when you use Bedrock via invoke model or uh the recently supported OpenAI SDKs like chat completion, and I believe responses, right? Um, one thing that comes with the standard tier is explicit prompt caching. Now, prompt caching is a great technique, both for performance and cost. Typically, when you need better performance, you have to pay more, right? That's typically how it goes, better performance, I'm paying a premium. But with prompt caching, you get actually both. Because your prompt is cached, it doesn't have to be processed on the GPU for the next request. So that means you get much faster time to first token. Your input has already been processed and because we don't have to use GPUs, we actually pass on the cost benefit to you, so it actually comes at a 90% discount. On your typical input token processing. Uh, so we wanted to make sure that this great benefit actually goes across all of the tiers. Prompt caching is supported, explicit prompt caching is supported across all of the tiers. Uh, and you can see how many of your tokens have been processed as a regular input tokens, how many were written to the prompt cache, how many were read from the cache, all of that is available for you to observe in the CloudWatch metrics. Um, so this is how you would actually look at your usage and establish a baseline. If you have been using Bedrock on demand. Uh, this helps you understand, especially when you're going for the reserve tier where you have to buy a reserve capacity 24/7. If you look at your current usage of your workload, you can establish P90, P50, you know, P25 for your input tokens and your output tokens. And now you can make an educated decision on how much you want to reserve, or when will you use priority versus standard. Next, we'll talk about the resort here. So this is where Um, I think Deepin mentioned for the TurboTax tax season, they know it's gonna be high volume, they know it's gonna require a lot of throughput, and they know they cannot actually have end users sitting in TurboTax asking questions and having to retry, right? Because the user will just be frustrated and go away. Now, when you have situations like that, it's better to just reserve the capacity based on your usage, right? You can look at your P50, maybe that's what you're comfortable spending. But now, once you have reserved the input and output TPM, Tokens per minute, right, based on your usage. We make sure that that capacity is always available to you. So, unlike on-demand, where on standard tier, if there is high demand from all of the customers at the same time, you may see some throttling, right? Because that's the nature of the standard tier. On reserve tier, even if other customers are sending lots of requests at the same time, your capacity is guaranteed. So other people will get throttled, but not you. So that's the benefit here. Um, we also have flexible provisioning of input and output tokens. What that means is, let's say you have a summarization use case where you're sending, you know, walls of text and you just want a paragraph out of it on summary of, you know, the entire content. Here, you would have lots of input tokens and very few output tokens. But let's say the use case is around content generation, right? Where there is a small set of instructions and you have to generate. Maybe a few pages of text. Now, the number of input tokens is really small, and you need a lot of output tokens. So based on your use case, you may need different token profiles, right? Sometimes you may need higher input TPM, sometimes you may need higher output TPM and we give you that flexibility. Um, so it's not like a hardware reservation behind the scenes where we say, OK, here is a GPU enabled instance and you get out of it, what do you get out of it, right? Uh, we have built the smartness on top of it to allow you to actually provision and reserve what is required for your use case. Now, this is a premium offering, so it comes at a cost. It's a fixed hourly cost. Per 1000 input and output tokens per minute. What that means is if you don't use it, you're still paying for it, right? And that's where your baseline comes into effect where you actually look at your usage for a given use case and say maybe I want to reserve only for P50 so that I make sure that I don't waste the reserve capacity. Most of the time I will use it, right? Um, but that the cost of actually having this reserved capacity and processing guarantee. You are paying for it 24/7. It is optimized for workloads such as, you know, the one Deen mentioned for TurboTax, the tax season. Uh, we have other customers where they have, you know, um, trading platforms, and they know what their usage is, what the pattern is, um, so they think for their customers, which is very latency sensitive in the trading world, they cannot tolerate retries. So now when you know your daily usage of your applications and what that translates into bedrock usage, you can once again make an educated guess on how much you want to reserve. So those are the type of workloads where, you know, you can absolutely not tolerate latency delays as well as downtimes. So no throttling and no retries is is kind of the, you know, nature of the game. One other benefit we have with reserve tier is you can burst into on-demand. So now the question is, let's say I reserve for my P50. At my peak, I have a lot more tokens coming into Bedrock. What happens if I run out of reservation, right? So at that time, you just then burst to standard tier. You don't have to worry about getting throttled beyond your reservation. You will automatically be served with standard tier. In future, we may even introduce uh a feature flagware. You can say that when I spill over to pay as you go, process my request with higher priority, so the priority tier, not standard tier. Uh, one other key, um, feature here is we do support explicit prompt caching for reserved tier. What that means is, um, because you're paying for, for a 24/7, right? So how do we actually give you the benefit, the cost-benefit prompt caching? What we do is we use a different burn down rate. So based on the model, you know, let's say anthropic sonnet 4.5, um, the cash rate token pricing is at 90% discount versus regular input token processing price, right? We do the same burn down rate conversion for reserve tier. Let's say, you know, you are sending 1 million tokens per minute and your cash hit rate is 90%. For those 900 key tokens that are being read from the cash, we will apply a burndow factor of 0.1. So those will count as 90K tokens, not 900K tokens. So when you're reserving capacity, you don't have to reserve for 1 million input tokens. You can reserve for the 100K input tokens and the 90K that after burndown, it will be, you know, effectively 90K. So maybe around 2000 that reservation should suffice for your 1 million tokens per minute. So that's how you get the benefits of bond caching in the reserve tier. And then you can observe the usage via cloudWatch. Now, there are a few things to keep in mind here. The first one is you're reserving for a certain threshold, right? When it spills over automatically, it is then served with standard processing. So in CloudWatch, you may want to see. OK, I, I sent, you know, 1 million tokens in this given minute. How many were served with reserved tier? How many were served with standard tier? And based on that, do I need to balance my reservation? Do I need to reduce it? Do I need to increase it, right? So all of that information is available to you in Cloudo Metrics where you can actually see the tier you requested at the time of uh making the bedrock request, and the actual tier. Um, the request was served with at, at the end. All right. So now let's talk about Pri Y. This is actually meant for, this is once again a pay as you go, so you're not reserving anything, you're not paying for anything 24/7. You only pay for it when you use it, right? But there is a price premium. The idea here is the standard tier, you know, it's for workloads that can tolerate a little bit of retries, but there are workloads where you don't want to waste time retrying and you cannot tolerate throttling. One example is we have this new generation banks where it's all online, right, and in their apps as you deposit checks, they have sonnet in their workflow for verification. Now, this is a, this is a workflow where you don't want to reserve capacity, because people don't, you know, um, deposit checks like every day, every hour of the day. But when somebody's trying to do that in your application, you want to make sure that it gets processed immediately and there are no retries, so that, you know, the customer doesn't go away. Now, use cases like this, they don't justify reservation, you know, you don't want to pay 24/7 if you're not gonna use it. But when the request comes, you want a flag where you can say, this is a high priority request, and I'm willing to pay a premium if you give me latency benefits and uptime guarantee, right? So that's what the priority tier is, um. Where it's a spiky traffic, it's sporadic traffic, but it is still important, and you cannot tolerate multiple retries, you cannot tolerate throttling. Uh, it is priced at a premium or standard tier, so somewhere around 75% to 100%, so you are going to pay more per token, but hopefully your use case justifies that kind of premium, right? So the example I gave, if the customer goes away because your check processing fails, there might be downstream implications of a customer loss for that new generation back, right? So you don't want to actually. Um, we lose that customer, it's much easier to pay the premium, um, than, um, the other implications. Now, Pritier also supports prompt caching, right? Like I said, it's uh a wonderful feature that gives you both performance and cost benefits at the same time. The way it works in private tier is if your token is read from cash, uh, typically, there is a 90% discount, right? We apply that discount to the premium rate as well. So let's say you're paying instead of $1 per token, you're paying $1.75 per token. You get 90% discount on 1.75%, so we still provide you the benefit, the cost benefit of pro caching. Um, and for certain models, we actually also give you better end to end latency by giving you better output tokens per minute. So what happens, what happens behind the scenes is if it's a priority request, we send it to a part of the fleet. Where we have optimized the fleet configuration or hardware configuration for speed. In simple words, for standard tier, we may have a batch size of 8 or 4, right? For private tier we keep the batch size smaller. So on the same instance, cranium 2, just taking an example, instead of processing 8 requests simultaneously, we only process 4 requests. And because of that, for each request on a, on a request level, you see better or faster output token processing. So your latency is better. Uh, so that's one other benefit of priority. You get higher priority, you jump the queue, you will not get throttled, and you will also get faster processing. Once again, you know, we have cloud Watch metrics where you can see the latency benefits as well as the uptime benefits. You can actually search your requests by, you know, uh, um, service tier priority, and look at your end to end latency and look at your 503s or pre-limit throttling, and you can compare it to your standard tier uh usage as well. So this is observable in real world in your applications, when, you know, you're using priority tier versus standard tier, you can look at the graph side by side and see how your throttling is lower and your speed is faster on private tier. All right. So now let's talk about Flex Dear. And this is a new offering for us on Bedrock, you know, with um It's actually by customer demand. I think Deepin talked about some experimental workloads uh on Intuit platform. Um, the other major use case for this is agentic workloads. So let's say you have automated workflows end to end where there is no human being sitting for a response, right? There are 3 steps in the process. Whenever the first steps, first step completes, the next step starts, and so on, right? So now because there is no human being sitting. You may want to actually take longer for the processing if you can get a discount. So we're talking about about 50% discount, and instead of 10 seconds, it might take 10 minutes. But your entire workflow still completes in half an hour, you know, which is good enough, right? Um, One example is, I think Jared was talking about. Scheduling of your workflows, or, or workforce. Um, now that doesn't have to be done in real time, right? If you're optimizing, you know, how your employees should operate at the airport, it, it's not a one second thing. It can take longer and you can optimize for that. And you can also get a discount on the processing of the, the data, right? So, it is priced at a discount on the standard tier. We are thinking around 50%. It can depend model to model, it can change model to model, um, but the mental model is around 50%, right? We want to give you that cost benefit if you are willing to wait a little bit longer, uh, so that other high priority requests finish ahead of you, but you will also get your request finished whenever you, you know, uh, need it. Um, it's just a longer duration. Now, Flexier also supports prompt caching. That goes without saying, right? Because if, if we give you a 50% discount on Flex tier and prompt caching is 90% discount, If we didn't support prompt caching here, Your standard tier would actually be more cost effective, right? So, once again, similar to private tier, Flex tier will also give you um similar discount. So let's say on standard tier, you're spending $1 per token. On flex you're spending 50 cents per token, right? If your token is read from cash, we give you 90% discount on 50 cents. So, we are just passing on the same benefits uh to you on Flex tier also. Um And then One thing we wanted to make sure as we add these service tiers is that they're easy to use, they're easy to swap. Maybe you want to use Priority tier if you get, your request gets throttled once or twice, the 3rd time you want to send it with Priority. So that should be easy enough to do, right? If, if you're having these heuristics where if my latency goes above 5 seconds, send the next request with priority. If my request gets throttled twice in a row, send it with priority, right? Uh, so here you can see it's um an example of the chat completion API, but it's a simple service tier, uh, parameter, and you can easily switch, right? So here it is priority, but it could be reserved or default or flex. Um, so all of the service tiers are easily, you know, swappable in your, um, Inference request on a request level, uh, it doesn't require a lot of coding change. It doesn't require, it's just a parameter and you change the value of it, right? So it's, it's very flexible that way you can actually decide to use the right, uh, service tier as it serves you in real time, right? If something happens, if you're seeing high latencies, you just go with priority. One thing I want to call out though, on the reserved tier, right, you need prior reservation. So, your inference request will say reserved. But before it actually gets processed as a reserve request, you have to create a reservation, which is a control plane action. Somebody as an admin in your company will have to do it for you, and then developers can send the reserve to your request. Yeah, and across all of this, we didn't talk about batch much, but this is another inference option that you have at your disposal, um. Typically batch is meant for things like I want to create daily reports for, you know, 100 different businesses or business units in my company. Um, I want to run evaluations of my agents, right? So here I'm gonna do bulk processing of a bunch of prompts. Maybe I'll take hundreds of prompts together, put it in a file, and I send it to Bedrock. It can take its own time, you know, but a few hours later I get answers for all of the prompts at the same time. So, this is also at 50% discount on a token level. You can actually decide when to use it, um, you know, typical workflows uh or workloads are evaluations, reporting, uh, it's a 24 hour completion window. Uh. And then you get, you know, the discount, uh, appropriately. So I want to wrap up now. I want to make sure that we all, we bring all of this together and, you know, you can leave with a mental model of when to use what. Right. So here, reserve tier, I would say is for mission critical workloads with steady traffic. You know I have a usage pattern, and I can reserve capacity for it, and there will be no wastage of cost. Um, so you would do that for workloads such as the tax season at TurboTax, right? I know it's going to be heavy usage. I, from my previous year, I know what the patterns look like. Uh, what my input and output TPM requirement, and I'm gonna reserve that so that I can rest assured that my users will not get throttled or, you know, they don't have to retry. The pricing model is fixed. That means if you don't use it. There could be cost wastage, so you have to be careful on how much you want to reserve. The priority tier is next in the line of priority. Let's think about, you know, these tiers as levels of quality of service. If there is a reserve tier request that gets served first ahead of everyone else, right? And we have made sure that there is enough capacity for reserved. Priority tier request gets served before standard and flex and standard gets served before flex. So it's basically real-time prioritization on the platform. So private DTR is for mission critical workloads. It's for direct traffic. We talked about the check deposit use case. There are other many use cases like that where Um, it's a spiky traffic. It doesn't happen 24/7. It doesn't happen every day of the week, but when it happens, I want to make sure that it gets prioritized and I'm willing to pay a premium for it. So that's what priority year is for. It's a pay as you go pricing, uh, and then you pay, um, you know, per premium, uh, premium per token. Standard tier is for day to day workloads that can tolerate some rate rise. So some of your requests, right, there is a very high threshold for us when we get alarms and we wake up and start rebalancing, but in very rare cases, some of your requests may get throttled, and you can handle that in your code. You can just, it's a 503 retry again, right? It's, it's simple to do that, but that retry actually adds latency for your end users. Um, so as long as that is fine, and I mean, typical is some code generation use cases, right? Uh, a developer asks for a code generation or code completion, it doesn't complete at that point, but you can retry and it takes 5 more seconds, but the code is there, right? That, that's still OK experience in my view. Uh, so this is the pay as you go standard token pricing. And then Flex is for latency tolerable workloads such as agentic workloads, right? There is no human being sitting in front of a desk. Hoping to see a response, right? It's all automated processing and if it takes a few minutes, that's fine, you know, you get the discount and you actually optimize for cost here versus performance. So once again, this is a pay as you go pricing discounted model. Uh, you're, you're paying per token that you send to the platform and you process on the platform. Uh, and then batch is the last, this is, this is specifically for bulk processing, where you have, you know, hundreds of thousands of prompts that you want answers for. You can wait for up to 24 hours, and it's like a repetitive batch bulk processing use case where you do it maybe once a week or every day, right? So that's what batch processing does for you. Um Yeah, so with that, I think we will wrap up and we will open it up for Q&A.