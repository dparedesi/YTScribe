---
video_id: EG6Gse9MJhE
video_url: https://www.youtube.com/watch?v=EG6Gse9MJhE
is_generated: False
is_translatable: True
---

Good afternoon everybody. Hope everybody is having fun at Reinvent. I'm extremely excited to be here. And share the stage with Axel, who will come here shortly. To talk about how is Whirlpool democratizing the entire virtual product development with AWS. And before Axel comes in, I will take a few minutes just to talk about what is AWS doing. In this entire product development, simulation, and product engineering space, and how are we helping our customers transform their product engineering landscape. If you look at the industry trends from a product engineering perspective, uh, there are 3 major trends which are right now impacting the industry. The first one being, as you can, as you know, most of the industrial manufacturing companies, most of the discrete manufacturing companies, you know, they are no more just manufacturing or designing traditional discrete products which are disconnected. These days, all these products are connected, they are smart, which means, you know, you are not just selling a product, but you are selling a product and a service with that because these are connected products. Second, because these are connected products. The entire engineering landscape is becoming more and more complex, and what I mean by that is you are no more just doing mechanical design or electrical design or electronics design, but you're also doing a lot of embedded software design. You're also doing looking at connectivity as an example and hence the entire engineering landscape is becoming very complex. And that leads to longer lead times to design new products, to engineer new products. And then last but not the least, given the market is so competitive, there is a lot of pressure to reduce your cost of You know, the cost of developing a new product, right? Uh, you, you really want to make sure that for you to be competitive you are continuously focused on optimizing your cost, not just to develop the product, but also optimize your cost of quality, and this is what every single customer I talk to every single day is going through these three trends. So what does this mean? Like how is AWS helping our customers solve some of this and also accelerate their journey? From an AWS perspective, you know, we offer 200+ services, right, across multiple use cases. From a Perth engineering perspective, if you look at these 4 boxes across CAD, across CAE, across PLM, across EDA, typically what you would see in any manufacturing customer, these are very disconnected siloed systems. And what AWS is doing with the customers is to really help build that gap so that you're not, you're no more disconnected, but you are unlocking your data silos by leveraging AWS to not just migrate these different applications, different platforms to AWS, but also modernize because that's where you start unlocking real value in terms of, you know, faster time to market, reducing the total time it takes to design new products. And more importantly, once you start leveraging AWS as a platform to design new products. Now you can start using a lot of AWS Gen AI capabilities. For example, you can start using Bedrock as a platform to build new Gen AI applications, which could be around digital threads, digital twins, and I can go on and on and on. So, why AWS? What makes AWS so unique and different in this space? It really starts with, if you're designing new products, if you're into a simulation space, you know, you need flexibility to really scale up and scale down your infrastructure. Because as you are testing new products, as you're doing performance testing, as you're doing load testing, you know, you need that flexibility. Now you don't need that. Uh You know, high infrastructure across 12 months in a year, you may need it for 2 months, but for 2 months to be able to scale up, do all the testing, and then scale down so that you're not incurring the cost is what makes AWS so unique. Second, once you are leveraging AWS for your product development, now you have access to a lot of AWS unique capabilities across HPC, AIML. You know, so to really start leveraging the power of data which you have now unlocked and start using that data to build more and more additional capabilities and additional use cases. Third, you know, it's really about Because AWS offers you so many different options in terms of how you can configure your infrastructure, you know, it really gives you more flexibility in terms of various ways you can optimize your cost to to run your simulation scenarios, to run your engineering scenarios. And then last but not the least, it really helps you unlock your data silos, because at the end of it, data is what helps you then do more modernization, more transformation, and really makes you unique in the market. With that, I would request Axel to come in and talk about the whirlpool story. Thank you. Yes, sir. I'll write this. I believe I'm audible to everyone. Thumbs up. Good, thank you. Uh, thanks everyone for being here, uh, for the opportunity to be here and share a little bit about our story, our journey in terms of virtual product development, the way we democratize the knowledge within, uh, models, uh, so that you can, uh, expedite product development with excellence. Uh, this is a brief, uh, agenda on what we are going to talk today, you know, there will be an introduction to the company. Then we are going to delve into one of the categories that's refrigeration. OK, we're going to talk how do we create the right architecture for for an appliance here in this case, refrigerator, in terms of purpose, scope, scoping and implementation. We are going to delve into virtual simulation capabilities and system requirements here a little bit. How do we do that in a virtual world in order to really Uh, expedite this whole process. Solution space exploration is the way that we obtain the data using simulation so that we can use that data to produce models, machine learning models, uh, and do optimizations up front. The core of this talk is going to be the cloud architecture diagrams and then a video demonstration to wrap that up everything here and at the end, the benefit, what's in there for us. OK, uh, about Worpole, Worpole is a leading home appliance company in constant pursuit for improving life at home. Uh, those are our iconic brands, as you can see. Worpole itself, KitchenAid, Gennaire, Maytag, Amena, Brasstem console, and Insyncerator with iconic products, as you can see, built-in refrigerators, uh, wall ovens, dishwashers, uh, fully automatic coffee makers, espresso machines, you know. Uh, and in terms of value, uh, last year we reported 17 billion in terms of annual sales or revenue, uh, and we are around, we have around 44,000 employees across the globe, uh, with, uh, around 40 manufacturing plus product development or technology research centers. In terms of the purpose, scope, and implementation, why we are doing this, it's very hard to get the translation of the system optimization in terms of system level requirements and to really harmonize all those requirements and deliver on such an emergent type of appliance, you know, that is best in terms of performance, which is translate into the least cost and the best performance. OK, I'm going to translate performance up front here, but that's the challenge here, OK, because traditional methods are fragmented. And it's we can do this manually without surrogate models, without machine learning, without 6 Sigma, you know, but there will be suboptimal solutions being delivered based on that and therefore the company is going to lose because that's not such a winning architecture. The what here is a multi-objective or multi-criteria type of optimization framework across multiple requirements as you can see here. I have energy, capacity, cooling robustness, noise, vibration, harshness. Uh, and also structural manufacturing stimulations. OK, I'm gonna go in more details in the next slide. Just bear with me for a second. And the how a lot of systems engineering, model-based systems engineering, and designed for 6 Sigma because we want to be to have a robust architecture. How can we be resilient, robust in terms of the architecture definition? That's the foundation, the core that we have, even for strategic sampling plans. We use the FSS approach here. Uh, uh, validated simulation models. I'm gonna show what validation stands for, that's different from verification here, very important concept. And then surrogate modeling is pretty much, uh, an equation, a regression model that's based on simulation work, OK? And the two last bullets about how to expedite this whole process in terms of model fitting or training plus tuning within AWS Amaker, and last but not least, how to democratize this using elastic beanstalk within AWS. All right, so. This is not data governance medallion type of governance, OK? This is the way that we assess the maturity or fidelity of simulation models. Uh, first of all, we have developed developmental models, you know, we are trying to understand if something can be simulated. OK? Can I do a fluid dynamics, an airflow distribution inside a refrigerator? Yes, we can. Can I do a taste and odor from ice and water, simulate that? Maybe not using specific softwares, right? And there are some, a lot of talks about intractable physics as well. They are deemed to not be simulatable. OK, so we have this distinction whenever we pass this gate that, OK, this is simulatable. We have the right assets in terms of software to do that. So then what we're going to do? We're going to create the models that's the layer, the bronze layer that's directional. What's the directional model here from a simulation perspective? Can we do it's it's a simulation which is well crafted in a computer, computer software, OK. And we run a sensitivity analysis that means we are playing with different design parameters and it's following physics. If you change the thickness from A to B. The stiffness is going to change. The deflection is going to change for a fully supported beam, for instance, you know, it's following first principles of physics, OK. The next layer is going to be the silver layer here where it's a quantifiable. Quantifiable is a bridge to get into the validated stage. Why I'm saying that? Because from the quantifiable on is from where we are socializing the simulation models with physical test data. We collect a lot of data from physical testing in order, in order to really cross pollinate that data back to the simulation models, update the simulation models, and then see if there's such a, a good degree of accuracy, but the quantifiable stage here is for a limited inference space. It's not for all the architecture, all the product variants. We, we, we get one or a couple families of products and we do this quantifiable, uh, checking, OK? And you get, you can guess what, when you reach the validation stage is for all the architectures, OK. Or all the underlying complexity of our systems or appliances here is not only for the sake uh sake of delivering this, for the sake of obtaining such a good uh accuracy as per our rules, right? We need to pass this threshold in terms of accuracy, simulation versus physical tests to have a match with good accuracy. What else we must have high agreement with all the affected stakeholders for this process because validation. It is pretty much making the the right things from a systems engineering standpoint. What's the right thing here? Simulation must replace physical test. OK, that's the the ground rule here.s Simulation is good enough to replace physical, and if you do that, you can guess what happens, right? Everything is going to flow naturally because we can create our own data proactively, right? I no longer need physical, and I can uh use some strategic sampling plans to really allow us to expedite the construction of machine learning models up front. All right. Uh, so I'm going to talk about multiple requirements here, how we crafted this whole interference space that's multi-objective, that's pretty much translated to that. It takes a lot of response variables or big Ys that we call at the system level or voice of engineering. The first one is as simple as the internal capacity, OK? You measure the volume inside those enclosed or those liners or the interior compartments of the fridge. The freezer and in a refrigerator, as you can see here this is a side by side model we have on the left hand side the freezer and there's a vertical mum that by the way you can vary the position that's a design parameter right? and on the right hand side we have the refrigerator compartment. The model has to really deliver or to to show exactly that the measure capacity is matching. The or a physical measurement that's a validation for this very trivial but very useful because it makes the difference in the trade space whenever you want to understand how can you boost internal capacity and deliver energy and deliver other response variables here accordingly. All right, the next one is an intertwined connection between a coefficient of performance from a refrigeration circuit standpoint. Think about that. In this ratio about performance, we have the design of an evaporator versus all the heat gain that must be the heat that must be taken out from the system. OK, heat gains formed or composed by a lot of insulation walls, the presence on what we call vacuum insulated panels, the footprint or the size of the product, you know, this is making the is affecting the heat gain. And the energy is pretty much how we orchestrate the system in order to remove that heat and deliver food freshness with the right temperatures and so on and so forth to the system, you know, so it's an intertwined, I would say connection between heat gain and energy because heat gain makes a cut to be part of the energy equation here. And uh this, this point is very important. Heat gain quantifies how much heat enters into the system. Energy quantifies how much work the system performs to remove it. All right? There's a balance here. And moving forward here, uh, now a little bit deeper into the heat gate is not only about the construction of the product itself in terms of wall thicknesses because we have some insulation, uh, layer inside those A, B, and C, and D, walls, you know, that's a polyethane foaming, that's what we got in terms of insulation in the product. Uh, so the product dimensions matter, but on top of that we also have the type of blowing agents, the formulation of this foam that goes inside this enclosed volume, you know, and we can either use cyclopentane or HFO. What's the difference? One is more efficient than the other. HFO is more efficient, 30% ish than cyclopentane, but it costs more. It's all about the entrapment or that of those gasses into this kind of cellular structure from the foaming, you know, that makes that makes uh. Uh, the HFO to have such a lower thermal conductivity and therefore better insulation, uh, performance and also the way that we pack the foaming, right, uh, the underlying process, right, we can have single port that's a foam, a single port in terms of injecting the foaming from the bottom deck, bottom, bottom of the products, and the foam is going to grow to the top or multi-port, several ports where we're gonna inject foam in the product. And last but not least, the presence of VIPs. He's not a very important person here. He's a vacuum insulated panel, right? So we use that to boost the insulation in some walls where we need very thin walls so that we can increase capacity, but you can guess what, it comes with a cost, OK. Uh, and VIPs are likely to be 10 times more efficient in terms of installation than the regular foaming, right? You, you can see the benefit of that, but it costs, they are expensive. Now going to more details about the energy itself from a, uh, a system level perspective talking about heat gain. And by the way, there's a, a, a physical test that you use to validate the heat gain model. It's called reverse heat leak. That's the way you collect physical test data to validate the stimulation model through this reverse heat leak test, you know. But heat gain now is one of the input variables for the energy, right? It turns out to be an input variable to energy, right? But on top of that, the second part on the COP ratio, uh, from a refrigeration standpoint, right, we have the control boards in the software, the control logic, we can have multiple types of compressors, right, the heart of the system here. We can have multiple fan motors to deliver temperature distribution so that we can really deliver food freshness in a very stratified way to both compartments and last but not least, heat exchangers in this case condenser. Think about all the different types of components, all the different parameters that can take into account from a control logic perspective, all of those things together to deliver the system level performance here. And then, uh, one very important requirement we can deliver capacity at the right energy consumption. It can be Energyar with, uh, but we cannot penalize what we call robustness. I don't want external sweating on the walls, you know. I always want to have a positive delta between the surface temperature and the dew point, otherwise there will be some external sweating, as you can see in that picture from a water dispensing portion, you know. Imagine in your house is happening that you don't want that. Very important. There's a robustness. There are some other robustness uh requirements from a refrigeration standpoint like frost formation or internal condensation. This is one of the biggest for us, external condensation. And of course this cannot be taken out, right? We always want to minimize this, right? You can have such big, a lot of VIPs everywhere, but can we pay for that? Can we make this product competitive? That's always a challenge in place here. So we have the cost of insulation and the cost of cooling components, right? We call this compounding costs. That's summing up both of both of them as the architecture cost. OK, so the architecture cost must be minimized with a given weight. In terms of penalizing the objective objective functions so that you can optimize the whole system, OK. So moving forward, these are other things that we are working on that can be also taken into account for the model, OK, we want internal distribution for the airflow, right, air delivery to the product or airflow management is structural stiffness. Imagine your products fully loaded with a lot of food storage on the door beams, and the cabinet should be withstanding all that load without any distortion so that you can, if that happens, you can lose insulation and the gaskets are not going to. Compensate and there will be some trade-off in the heat game that's going to affect energy. Everything is connected here. And packaging we don't want to deliver oh sorry, the NVH noise vibration harshness, the airborne ins instructural borne noises, uh, foaming simulation as you can see there, this is a single port foaming injection through the bottom deck of the product as an example for that picture that's a simulation model. And we want to pack the whole foam in. And you can guess what if you have very thin walls, the foam is going to be very obstructed to go smoothly and therefore there's going to be a trade in terms of filling and therefore it's going to create some kind of internal voids and therefore there's going to be likely to have external condensation. OK, everything's traded, traded here is intertwined and packaging. We want to deliver products to the customers without damages from transportation or handling. OK. A lot of VOEs, voice of engineering, or system level requirements that must be harmonized. And they are uh uh highly dependent on several response or several several input variables. OK. Talk about the trade space input variables. This is real, OK? This is exactly what we, we're doing we've done more recently. There is one product that we use this type of approach more recently, 134 factors with two levels. What is that? I can have thickness. With uh thickness A and thickness B, two levels that factor, or the presence of a vacuum insulated panel, you know, foaming or VIPs, OK, that's uh is like a two level settings for for factors, but I don't have only one or two that makes 4 runs, you know, overall. I have 34, OK, uh, there are 3 factors with 4 levels, OK, control boards or condensing systems or type of, uh, foaming properties. In one factor with five levels, if we try to account for all the possible combinations, we are talking about 55, 55 times more than all the stars in the galaxy, 5.5 trillion of combinations. OK, it's a lot. Can we handle this even with HPCs or high performance computing? It's not something we can handle, right? That's why we come up with this kind of approach which is called Whirlpool Tester recommender, OK, founded by or grounded in 6 Sigma, OK, where we use design of experiments approach or token of arrays. OK, it's an application that is spinning in AWS using elastic beanstalk where we have R and Python. At the back end, right, and you have this type of, uh, uh, test of recommender so that there are two questions that we need to answer and it is gonna come up with the right recipe, uh, that we have to start with, OK, in terms of priority. Why is that important? Orthogo races? Because this is the most efficient way of collecting the data in a pairwise, balanced and independent way in the trade space. Undoubtedly that's the best way to collect the data. You can do space filling design. Random sampling whatever but this is still the best way because it's fully balanced is orthogonal that means that there are no such correlations across the input variables, right? Colinarity issues. And this is, uh, you have a pair wise what's a pair wise here? One level setting for any given factor will at least be crossed with all the level settings for all the other factors pair-wise combinations. It's gonna cover the whole trade space, OK? Uh, there, there are some techniques that we use here to, in order to, to have the initial coded design that we call, we do some wrangling on that, OK, some, uh, automatic techniques that we have embedded to this approach here, and we come up on what we call initial encoded array, and we, we use that to fit the model, the machine learning model, OK. Uh, and as a machine learning, what a learner, what we normally do, we use a lot of regression models. We start with the most simple model because this is orthogonal array. I can use linear regression because multiple linearity is not affecting us a lot, right, even though I have this holdout sample from the test set, I can see we start with linear regression in this case here, right? If the model is not good enough in terms of accuracy, OK, for the cross validation of the training set, we do what we call augmentation, but on the way that I run more, we normally proliferate more runs that are easier to run. Energy runs, they take seconds to run or 1 minute to run an energy run. I can produce thousands of runs in a few hours, OK, for energy. Heat game models, they take hours to run. OK, so I can do augmentation on the energy side. And rely on the groundwork given the orthogonal arrays for the heat gain runs, you know, that's the data augmentation if needed. I can tell you that I still didn't see a sampling plan using this technique that was surpassing 1000 runs. OK, Compare 1000 versus 5.5 trillion. OK. And it's covering the inference space. It's allowing us to do the CICD for uh kind of model, it's checking the accuracy of the model, you know, uh, with accuracies above 95% for all those response variables here. All right, efficient DA sampling reduces the manageable design space into a tractable and effective data set used to train machine learning models. We make this tractable to run. All right, so once we have all those models in place, right? We can now we can do inferencing. We can do predictions using those machine learning models because they passed all those safeguards in terms of accuracy for the held out set, which is the test set, but also they were optimized for the the cross validated training set, you know, and now they are available to run what we call optimization. Optimization, very, very effective, effective and efficient type of. Approach here that is based on multi-objective genetic algorithms. You can see multi objective. The MOG I think here, multi-objective is because we have energy cost, capacity, cooling, robustness. There are multiple response variables to be tackled here, right. And those are the key steps we have in place constrained population generation. That's the first step. What is that? Not all the runs are feasible. OK. Some runs inherently, they are not going to survive from the, the just at the beginning to collect the data. Sometimes I can, I can, I cannot even calculate the energy because I'm not kind of delivering the, the minimum temperature that must be delivered to the product. So those runs are eliminated. OK, those are voids that we have in the data set itself. There are some other linear constraints we have in this data set like uh if there's a marketing requirement, the wall thicknesses from both sides must be the same or door's flushiness, right? I can have a door depth from the freezer as a parameter which is different from the door depth from the refrigerator. If you look at the product, you can see a lack of flushiness, right? Is this something. Kind of desirable from a market standpoint. No. So we had a lot of linear constraints or or or variable linkages here. That's what makes the constrained population generation. So we only generate the population for whatever is feasible. OK. Step number 1. Step number 2. Uh, is performance prediction here. So remember, before we had all those machine learning models that were working well with a good degree of accuracy, and now they can be used to pick out that initial sampling and do infferencing, right, calculate the predicted results is step number 43 here, e evolutionary selection. Think about the way it works in genetics, right? If we have a gradient descent, it's gonna very likely give you. A local optima. Why? Because it's all about the way you start collecting the data, OK? It's gonna convert to a local local optimum, but you don't know whether or not there are some other optimums in your infinite space that could be further explored. This is addressed with this approach here because genetic algorithms, they are going to do exploit locally and they're gonna explore away from, uh, from the infinite space on other or on other portions of the infra space. So exploiting. Is like is an analogy for crossover exploring is an analogy for mutations. That's why they are called genetic algorithms because of this inherent nature to exploit and explore, right, and do this several times until you find something that's uh really uh delivering uh uh what it takes to converge from a Pareto front perspective, you know, with the best balance across all those uh. Desired outputs with a given set of weights or importance. That's a way to penalize the objective function by setting up some weights. Because cost can be more important than capacity depending on the The needs, marketing needs, you know, you can always balance that with a given set of weights and when you get that, you have the final weighted composite score that we use to define the best solutions. OK, so again, feasible runs, do the inferencing for that initial population, do crossover mutations, find the optimum from a well balanced perspective here. And now I'm going to go in details about the diagrams here, right? Very important thing to talk here, right? How we do that in AWS. I'm gonna spend a few more minutes here on this one to go to exactly the details of this structure here so think about that. The first element you see right there is an API gateway, very modular. There's an API call that can be used to. In any applications we have in production, you know, using elastic bean stock, you can consume this diagram in terms of an API call so that we can use this to do optimizations or model fitting or tuning here. So this is fully orchestrated by API gateways and lambda. That means it's a serverless approach, right? You can use as needed, OK, with zero overhead, all right, very modular. One core element, as you can see, is the piece that's called ECR there. Look at that. It's all is all the way to the right hand side. It's an elastic container registry with a. Um An image, a container image. That's taking into account an entry point from the container. Of course within the ECR there's a Docker file, our requirements.txt in a dynamic loader. Why I'm not using here a prepacked kind of uh uh training job from Sage Maker container because they are limited to either psychic learn or deep learning. I want all of them plus optimization, OK. So that's what it takes to build a customized container for this approach because it's going to fulfill all the needs for this workflow here, right? And you can see that's an S3 bucket pointing to the ECR image there. It's a dynamic loader. Those are that S3 bucket contains a file that's called Training man that's going to route that file with all the instantiations for the Sagemer training jobs within the entry point that belongs to this ECR, right? And therefore it's going to subsidiate the lambda to trigger the sage maker training here as needed, all right. So after the lambda is triggered, there's a key indicator that you can see yellow color there, operation type. This guy here. OK. This is an environmental environment variable within the lambda function here that orchestrates the whole thing that can distinguish either it's going to do the model training or fitting or the model tuning as you can see on the top, it flows through the model training. And at the bottom it flows to the model tuning here, right? If the user at the front end is going to click on the model training, what's going to create for us? It's going to orchestrate the lambda by parsing all those parameters, you know, and the ECR with the container image to trigger the SageMaker job so that we are going to fit the models and we are going to create. Pled files for the model fitting itself because we don't want this. We want to front loading model feature. I don't want on the user side whenever you open the application, the application is going to fit the model for you. It's going to take all the time to fit the model. I want this to be packed to be front loaded, and very important, is another file here within this model train that's called Shepley or model explainability, OK, because it's part of the application to have explainable AI. One time, maybe some years ago, we used to say that neural networks are kind of black boxes or models are black boxes, but Shapley values came to rescue us in terms of AI explainability. This is also front loaded for the model training here. When I say Shapley values, I'm talking about Pareto of importance, right, and partial dependence plots that shows the direction of change. If you change from thickness A to B, what's going to be the heat gain. Kind of, uh, impact, you know, you can see all those explainable AI graphs into these two files that you can see on the top right corner in terms of that output S3 bucket for training. Now talking a little bit about the model tuning, right, we can trigger the tuna is the library that I use to train all those models, right, to do genetic algorithms to the same algorithms that we use to train to create the optimum architecture we use to hyperparameter tuning as well. Same thing, same thing. Is this a way to instantiate this differently design factors and hyper patterns for a given model, you know. There's a bucket that points to the tuning process here that contains all the tuning configurations, adjacent file that takes into account all the if it's a deep learning, you name it, you know, number of neurons from the first layer, number of neurons from the second layer, is there a dropout, do we have batch normalization and many others, or the learning rate, you name it, right. All those things are going to be taken to this uh modern tuning standpoint, you know, and the tuning itself is going to create a kind of the best balance in terms of tuning results. It's a pickle fire at the bottom S3 bucket, and once the tuning is processed, done, the user can see on the application exactly the the test set accuracy, you know, if the user is satisfied with that, you go there and click the button to train the model, OK. And then we are gonna front load the model training file plus the explainable AI file that we call Shepley. Creation pickle fire, OK. This is pretty much the architecture for this back end for doing all this orchestrations. I know that's quite complex, but this is so useful, very flexible. You call it, consume it. It does a job on demand. You're going to call Sage Maka to do the job. You can pick whatever instance within Sage Maica as you want. If it's deep learning, guess what? You're going to require such a more powerful computing instance, right? Uh, in many other things, you know, there are 11 or 12 different regressors in this approach here, OK. I have Cat boost. I have deep learning. I have, uh, XG boost, uh, Kriggin or Gaussian process regression is there as well. Support vector machine, everything is there, OK, prepacked there, and you can add more by simply changing the SG bucket for the trading code, OK, it's just the sake of adding more. And whenever you add more because it's an S3 bucket with Virging, you're going to see that this API is going to reflect that accordingly. That's why that's the beauty of this flexibility here. OK, before I show the video here, I want to explain a little bit. OK, now you, you, we, we showed the whole diagram for the, the training and tuning process, you know, in a modular way in with this dynamic loader for the container itself. But how about the front end application which is a serverless which lets the Beanstalk, you know, it's a, it's a Beanstalk app which runs within a VPC, OK, virtual private pri private cloud here which can be only accessed. Through a VPN if you are using kind of remotely. Which takes into account what we call auto-scaling and load balancer, which is deploying using a docker container. So if you remember on the previous slide there is Um The output from the model training, it turns to be the input for this diagram here, as you can see in the bucket at the bottom, inputs the train models Pico and the Shapley creation Pico, they from output at the previous slide they turn to be out inputs for this Beanstalk application because they are going to be consumed into the Beanstalk application front end here. And also because there's a lot of explainable AI EDA exploratory data analysis, and so on into in terms of uh into the the front-end application we have the data and app resources. App resources is very interesting here because I'm not, uh, whenever you do an elastic beanstalk using a container you can, uh, as simple as having 3 files application file, the, uh. The Docker file and the requirements.txt, you know, those are the needed files to really deploy an application, a beanstalk application, very easily, but that app.py file that takes, makes the cut for one of those three files that are zipped and be and used to deploy a beanstalk application, uh, orchestrates all the calls for this data in app resources. It's like a microservice type of architecture that we have in S3 bucket with all those packages, right. So they can they are dynamically dynamically called from this app.P file that resides within the beanstalk and orchestrates this whole front end here, OK, meaning that if we change any files within the S3 bucket. That means that your Beanstalk application at the front end is going to see that if you restart the Beanstalk, as simple as like that. You don't need to redeploy. It just restart the server. That's it, because whenever you restart it's going to retrieve whatever is new in S3. That's it. OK. So a private VPC hosted application, internal load balancer, auto scaling AC2 instance, application code, data and train model artifacts are stored in S3, as you can see here in this top S3 bucket. Is a Docker based because we want this to be prevalent and really reflect to be always containerized for the sake of functionality in a robust way. And that's that's it. That's the front end from this Alasia Bing stock application. Now I'm going to showcase a video, OK, that's going to give you some more practical insights on the way that everything that we talked here was wrapped up and put together to deliver an optimal architecture here. Our strategy is to continually improve our products. We are now showcasing our innovative in-house multi-physics optimization workflow, which seamlessly integrates VPD and artificial intelligence. Our advanced model incorporates more than 50 design factors to identify optimal combinations across multiple key performance metrics, including Energy, capacity, robustness, and cost. It enables smart trade-off decisions among these often competing factors, ensuring balanced and optimal solutions. This powerful yet user-friendly engineering tool allows for real-time adjustments to various parameters, such as product dimensions, height, depth, and width. Insulation wall thicknesses, vacuum insulated panels, mullion position, compressor type, and many more, with real-time predictions. We can also run optimizations by setting appropriate targets and weights for all requirements and establishing constraints. On the input variables. For instance, 10,000 product iterations within the design optimization loop can be completed in approximately 5 minutes. With this cutting edge technology, we can significantly accelerate our decision making process in the early stages of product development. Innovation, always in constant pursuit of improving life at home. All right, I hope it was useful to see exactly the way that we encompass all those things, all those different ways that we can visualize the data from an exploratory data analyst standpoint, you know, because another way to check how machine learning models, they do make sense is not only about looking at the accuracy from a cross validated training set, but also at the accuracy of the test set itself, but also if the models are delivering what what they must be delivering to you, OK. And how do we do that if you look at the chorelogram at the, if there's a correlation between one response variable and one x, and you see that's a more pronounced effect in the explainable AI partial dependence plot, that means that this is making sense. The model is giving you the partial dependence plots, given the Shepley file that was created at the front loaded. From that architectural diagram, you know, but the raw data is also giving the same signal to you, OK, so we normally look at that, OK, correlations across the axis that are not a big deal for deep learning, but they, they may turn into a big deal for linear regression, you know, uh, but we always look at the EDA part with chorelograms with parallel coordinate plots, you know, uh, histograms with the actual model results, two layers EDA plus an explainable AI. And also some other layers for prediction to do infercing, you know, from a prediction standpoint, and the last but not least is design optimization. Design optimization here takes into account what the genetic algorithm. And you're going to set up a lot of different weights across all the response variables. You can select if you don't want. I don't want to optimize heat gain. I want to optimize energy because heat gain is an input for energy, right? We can have this election about not caring about heat gain, do not optimize, but caring a lot about energy, and we can either optimize to minimize or maximize or to reach a given target. For internal capacity, we want to be very straight to the target. I want that given capacity with such a very high weight, OK. So we are gonna force the genetic algorithm to really explore because it takes more emphasis into account, more weight right into account, uh, and then we can, um, uh, also for robustness for external condensation. I don't want to have external condensation, but I want to have at least a delta from the surface temperature to the dew point that's 1 degree, 1 degree. OK, that's it. OK, so at least 1 degree, greater than 1 degree. So it can be greater than, it can be straight to the target, can be do not optimize, can be maximize or minimize, OK. And you can select exactly which factors at their respective levels that you want to optimize. There was a study that was done that we didn't want to optimize the product with vacuum insulated panels. We just unflagged them because they are categorical variables with discrete level settings, right? Just unflag them and force the exploration space to only to look on what you want. OK. This is fully democratized and deploying be stock and available to use to the enterprise here. And the last here is one last slide. What's in there for Whirlpool, you know? Is AWSH maker we, we, we, we, we, we estimated up to 70% in terms of efficient efficiency here with much better effectiveness here in terms of model training and tuning, such a modular architecture that can be consumed in any applications, right. And therefore because you can pick whatever instance EC makes sense to you, you can expedite a lot, even very complex machine learning models like deep learning ones, OK, so up to 70% benefit for model training and tuning time and up to 95% at least for uh architecture definition. Imagine doing all this massive work manually, OK? Try to come up with. A sequential approach to multiple design of experiments with only a few levels set is to try to Incrementally do all this exploration and try to come up with a an optimal. If you don't do the right algorithm, you're going to convert to a local optimum that's going to be suboptimal, you know, with this approach you can explore and exploit and converge with the right emphasis or weight for whatever makes sense in terms of desirability for your design, OK? And this is giving us a lot of leverage in terms of effectiveness and efficiency for product development. That's what it takes to really define a winning architecture. Because from a systems level perspective, architecture drives behavior. If you have the right architecture, everything is going to flow naturally. It's going to be more resilient. Your life cycle, or digital thread is going to be more sustainable upfront. Alright, with that being said, that ends the presentation here. I thank you so much for being here to listen to us here today, and I appreciate we can have some offline conversations here, and we sort of and I, we are be, be, be, be more than glad to really answer any questions that you may happen to have, all right.