---
video_id: XxAaidwyNNM
video_url: https://www.youtube.com/watch?v=XxAaidwyNNM
is_generated: False
is_translatable: True
---

Hi everyone. My name is Giovanni Zappella, and with Laurancalo today we will, uh, try to give you a very, very short summary of, uh, the evolution of, uh, you know, coding agents starting from, you know, code completion all the way to the different architectures that we adopted in the last years. We are both principal scientists working on. Quiro and uh in particular on the Quiro Autonomous agent. When I started coding. Ideas looked pretty much like this. You know, you get a little bit of syntax highlighting. There was autocompletion, but it was much more about completion than about being automatic. Support was, uh, you know, a very vague term, and, uh, you still need to remember where to put your semicolonn if you actually want to compile your code. Fast forward 10 years, we get very powerful ideas which are able to basically complete the variable names, the method names, and so on and so forth, everything you're thinking while you write the code. They typically show some kinds of suggestions directly in your editor, and, uh, you know, they are able to complete fairly large chunks of code. At the same time, this is still something that is able to speed up uh your uh typing activity, not necessarily to code on your behalf. Fast forward another couple of years. We start seeing agents like the one that we have in the hero CLI that given a problem in natural language, they are able to autonomously interact with your file system, identify files that need to be changed, and make these changes in order to achieve a certain goal. At the same time, this is not exactly how things started. This is already, you know, something that saw some iterations and progress in the last couple of years. So let's get started from the beginning. First of all, when we speak about agents, You know, we use this term for pretty much every kind of autonomous entity, but I would say there are two big families of agents, and you know it's not really a hard split, you know, it's more of a spectrum and uh some agents position somehow in the middle of these two families, but for simplicity, let's focus on the two extremes and uh I will let you then define, you know, to which family each agent belongs. On one side we have the synchronous experience, the experience like, uh, the Quiro CLI where the agent is interactive, is your companion, and is basically helping you to accelerate in in accomplishing the task you have. On the other hand, we have a an asynchronous experience, or an experience where you can delegate certain tasks to agents and let the agent work autonomously in order to achieve that goal. These are, as I said, two extremes, but They also entails a very different kind of experience for the developer. For instance, if we look at the software developer operating in a sequential way, so starting to solve the task, complete the task, and then move on to the next task, it's a little bit of an oversimplification, I know, but you know, it, it follows a lot of the patterns we see in the day by day, uh, activity of developers. We can see the companion agent, the assistant, speeding up this sequential activity. So the developer will still be able to, you know, complete a much bigger number of goals and tasks, but it's uh still a sequential, somehow similar activity to the one that is doing on its own. When we look at the second family of agents, the autonomous one, we look at something which is a bit different, where the, you know, human developer can start delegating and parallellyze its work, and it will have a very different kind of activities. Some of them will be long running, some of them will be shorter, but generally speaking, it's not a strictly sequential operation anymore. It's also not completely true that these tasks are monolithic blocks. In fact, when we look at the task structure, especially when we want to delegate a task, there are still some touch points between the agent and the human developer. For instance, the task needs to be defined, so the human developer needs to write a specific definition of the task and what needs to be achieved. After this synchronous kind of activity, there is an asynchronous phase where the agent operates autonomously and creates a pull request, for instance. The request still needs to be reviewed by a UN developer typically before being merged, and this is another. A synchronous activity where there is interaction between the two and eventually The agent will need to autonomously iterate on the changes and so on and so forth until the code is not ready to be shipped. This is, uh, uh, again, a little bit of simplification, but should give a little bit the idea of what we call synchronous and what we call a synchronous. We started working on this kind of agents, uh, several, a couple of years ago, and in particular, we focused on the second family of agents, the ones operating asynchronously and operating autonomously. For the rest of this presentation, I will share with you results on a benchmark that is called Swii Bench. You probably saw it around before. I will use that benchmark, not because it's the most important or the only one we have, but because it's something that, uh, has been, uh, around for quite a long time, even though our activities start before we managed to, you know, uh, retroactively compute some of the results for the purpose of this presentation and. Because it will give you also reference points about, you know, how much these agents evolved and changed over time. The purpose of this talk is not to explain to you how to implement an agent in 5 minutes or something like that. The purpose of this talk is to share with you what we built over the last 2 years in order to, for instance, get to the top of the leaderboard of Swee Bench a few times and give you the idea of how the architecture of this agent changed over time. What I would like to achieve is to give you some inspirational ideas that you can use the next time you want to create a new agent. So, let's get started. Uh, for those which are not familiar with the benchmark, the benchmark is pretty simple. It's a scraped from, uh, GitHub. So there is a GitHub issue that was solved by a human developer. The human developer, you know, solves the issue by writing the code which is needed, for instance, to fix the bug, and typically some unit test which verify that the code is correct. What the creator of the benchmark did is to take these two components, remove the code that was written by the human developer, let the agent write the code for that, and verify that the code is correct by writing. By running the unit tests which were provided by the human developer. If the unit tests are passing, we assume that the code is actually correct and solves the problem. If the tests are failing, we assume that the code is incorrect. There are a few assumptions along the way. For instance, you know, Unitest may not cover all the possible cases, or, uh, you know, the reviewer that, uh, first, uh, approved the PR didn't do a great job, but it's fairly correlated with the quality of the code. The start for us was somehow in an epoch which looks very far right now is when people were starting to evaluate LLMs basically on benchmarks like Human eval. These benchmarks were pretty simple, provided the signature of a Python function, for instance, and provided some description. The benchmark required to complete the body of the function to return the correct values. When we switched from this kind of benchmarks using still fairly complicated solutions to something that was realistic, we immediately noticed that these benchmarks were far from capturing the quality of the code required for real, real world application, let's say. And In particular, we see problems in generating actual patches, uh, failures in passing the test, and so on and so forth. This was already a system which was non-trivial and it was based basically on a rag. So we had a retriever retrieving some code, typically some files, passing them to the LLM and let it make the changes. What was even more concerning was the actual inability of the retriever to identify and provide to the LLM the correct pieces of code, the correct code snippets and files. For people which are not familiar with these metrics, the recall is a fraction where that that measures the percentage of um files which are uh files retrieved, uh sorry, given the uh the files which are relevant for the task at hand, the recall measured the fraction of the files which are relevant and are retrieved by the agent. This means that if we have a recall of 50%, only half of the files, which should be modified to complete the task, were actually retrieved. This is a fairly complex problem because we switched from, uh, you know, simple benchmarks that had only one function provided or maybe two functions being provided to a benchmark that relies on a real world code base with thousands of files, possibly, and just identify what needs to be changed, was a challenge on its own. We clearly understood that the RAA was not a solution for that, but at the same time, we wanted to keep something somehow controlled. Effective. And relatively fast in delivering a result. So the first solution we created was a fixed workflow. The workflow was made of a few steps. It was starting by identifying files which were potentially relevant in a with very high recall. So we were retrieving a lot of code in order to make sure that we somehow retrieve at least a large amount of the files which were actually needed. At the following step, we were further expanding the content of the files, for instance, by providing class names, uh, method names, and so on and so forth. And then we ran a 2nd step of retrieval where we were discarding from the initial set the files which were not more relevant, given the new information. After that, we were selecting the code chunks that needed to be modified and rewriting them before generating the actual diff. This is a very, very simple workflow. In fact, it was only using 4 LLM calls. At the same time, this gave us an improvement of roughly 10x compared to rack-based solutions. And it was the most effective agents out there at the time, and it was on top of the leaderboard of Swiss Bench. Was it satisfactory for us? And the answer generally is no, and for a very simple reason. The workflow was extremely efficient, but uh it was not flexible. Flexible is a very vague term to be defined here, but uh you can imagine as a software developer that when you have a task at hand. You don't always follow the same workflow. If you want to fix a bug first, maybe you want to reproduce the bug and then observe what is in the traces and then move on, identify with files to be changed and so on and so forth. While if you want to start a new project from scratch, you start implementing something first and then you run things along the way so. The fixed workflow was not flexible enough to cover all these use cases which are real. And uh we also noticed something which was significantly more uh challenging, and it was the quality of the tools that the humans had in their hands was much, much higher than the quality of the tools that we were providing to the agent. So we moved on, creating the tax code agent. The tax code agent was Basically relying on two big improvements, so the first one was, uh, to create a set of tools and an environment where the agent could actually interact with the code base and the file system in a somehow quote unquote agentic way. It's um it's a very simple concept if you want, but you can imagine that uh human tools are very visual, while the agent relies on text and in particular the text should be fairly short and to the point without, you know, useless information that may confuse the agent in the next term. At the same time we wanted to keep a specific amount of information which is relevant to solve the problem always available to the agent and give it some kind of workspace where to store somehow memorize if you want uh some useful information. So we created uh an agent which was based on a so-called agentic loop where the agent continuously iterate and uses the LLM to select which tool to use at the next step. The agent was equipped with a. A number of tools for opening file, modifying files, selecting code chunks to put in the memory of the workspace, and so on and so forth. This was a significantly more complex agent. But it also gave us a big boost in terms of performance. Uh, on Sweep bench, the full data set, the performance went to roughly from roughly 13% to 19%, but you need to account for the fact that some of these tasks are probably not solvable or they have overly specific unit tests. On the verified that was the verified subset of Swi bench, which is a human, uh, annotated and verified set of test cases, we were seeing a much bigger boost going from 25% to 38%, and this was also the first agent reaching the top of the Libya board unverified. We also noticed that the agent was significantly more flexible than the workflow, which is exactly what we wanted to achieve. But it also created opportunities to expand the use cases supported by the agent. For instance, in this case, we tried to create an agent which was redacting, redacting documentation instead of writing code. It was based exactly on the same tool, on the same workspace, on the same model. We just changed the instruction to make sure it focused on documentation files and in particular on rhythmi files. We also compared it to, you know, specialized workflows which were actually studied and tuned to produce the best rhythm files. We immediately saw that the new structure was flexible enough to actually match the performance and in some kind of perform specialized workflows. This was certainly something that gave us hope to further expand the behavior, uh sorry, the supported use cases and improve the behavior of the agent. At the same time, we were also somehow hitting an obstacle. Once we created the agent, we identified a number of tools we kept adding tools and uh you know, providing better feedback to the agent along the trajectory. But It was pretty clear that the agent was still struggling because The models were not able to understand the so-called semantics of the code. If you think, and uh I don't want to to do any somehow parallel evaluation, but uh when humans think about code they don't typically think about the code itself about the words that they see in the code they kind of, if you want, execute the code in their mind and they think about what that implies and what that kind of actions uh will return as a result. This was not happening here and uh we immediately noticed that uh you know LLMs were not able to effectively self uh correct and uh even in the presence of feedback it was really a big struggle for them to, you know, improve the code when it was incorrect. At the same time, we didn't really have a viable alternative because um if the LLM cannot understand the code, what can we do? Tests were not really generated in a very effective way. As the time goes by, models got much better at generating tests. And we could finally leverage code execution. Code execution was a big, big shift, not because it's particularly complex in, uh, to imagine, but because you need to have on one side a model that is able to generate correct and relevant test and on the other side you need to have all the infrastructure to safely execute both code and test. In a, uh, in a protected environment. You don't want an agent which is a stochastic, uh, based on a stochastic model connected to the Internet that can generate a RAM code and do whatever it wants. So this is a fairly significant uh challenge in engineering terms and. It's also the kind of, uh, the kind of environment where you need to scale and being able to run thousands of these uh of these agents at the same time if you want to effectively evaluate them. So it was a significant effort, but this led us to create what we call the logos agent. And the logos agent allowed us to create a significantly more complex architecture. In fact, we didn't have only a sandbox to run the code, but we could start using the test to leverage, you know, more compute at test time and try to let the agent autonomously identify problematic solutions. So I will give you a quick overview of this. And if you notice on the left side, we have the code writer agent. Which is fairly similar to text code in spirit at least, and it has a number of tools can interact with the file system, but the file system now is in the sandbox environment, it's not the local file system anymore. Conceptually it doesn't change much, but it's safe to run it this way. And uh we could run multiple of these agents, in this case, 3 different versions or also the same agent 3 times. And let another component verify potential regression by running unit tests. So we could run the agent 3 times, generate 3 different patches solving the same problem, then run unit test, not the unit test that we use in the harness to evaluate the performance, but the unit test already present in the code base. Identify regressions which were not related to the task we have at hand or you know we don't want to observe in our solution this card potentially. Some of the patches which cause regression and move on selecting one specific patch. Using a specific uh an LLM based algorithm we can see that this not only give us a significant improvement in terms of, uh, uh, you know, one shot performance where we generate a single patch going from the 38.8% that we saw before to 51%. But also the selection of the patches, having multiple patches in this case free and being able to select a single one was giving us another 4% point improvements which may not sound much right now, but uh if you consider we started just uh probably 15 minutes ago from uh 13, 14%, 4% points are a significant improvement. And By the means of this, we could also scale further and instead of generating 3 patches, we can generate 10 patches, 25 patches, and so on and so forth. And using more computer test time could give us much better performance because we can. Evaluate multiple solutions for the same problem. At the same time, We were still not leveraging everything we could do, in particular with LLMs getting better. We saw the so-called thinking and quote unquote, reasoning, uh, skills of the models getting much better. And they proved to be effective and useful to plan and to specifically think about situations, for instance, when you're trying to solve a bug or something like that. So we created specialized tools offering the agents the opportunity to extensively use the reasoning. And uh to make good use of them, not because we quote unquote overthink along the trajectory, but because the model is able to select when uh actually leverage these skills. They also provided us with a significant boost in terms of performance, roughly 4 or 5% points, which uh is another meaningful improvement. But we were still not getting to the point of solving very large tasks. In particular, uh, we had a problem of, uh, contact size, but we also had a problem of, uh, the flexibility of task which, uh, you know, was certainly improving compared to the fixed workflow but not always, uh, to the point of, uh, of being a. Uh, of being sufficient for practical purposes, for instance, uh, if you try to reproduce a bug. You can try 10 times to reproduce a bug and only be successful the last time, which means that the previous 9 steps are completely useless for the purpose of solving the task. And this consumes part of your context and it's something that remains in your memory, in the memory of your agent. On the contrary, when you have tasks which are significantly different, maybe you don't want to only write code but also change your configuration for deployment, you may want to isolate that and avoid conditioning on those changes when making the next steps. So we created a more complex architecture which we call huang. And uh this is made of a supervisor agent and a sub agent. The supervisor agent is a fully equipped agent able to interact with the file system, reading files, typically creates a plan, and then it's able to leverage sub agents which you can imagine are super editors if you want. Receiving instructions and accomplishing subgoals that are necessary to achieve the final one. Let me give you an idea of how this works. Let's try to implement a new feature in a website. Uh, the website is a simple website, uh, containing detailed pages for restaurants, and, uh, this request. Uh, basically sets a specific goal of creating a new route in this flask website, allowing customers, uh, of this website to vote for a specific restaurant. The page must be password protected because we want uh only to to have a selected amount of people voting. And uh what happens is that the supervisor agent starts exploring the code base. In this case it uses some common line tools like uh find and LS and grab, and it tries to explore the files to identify the relevant templates, the relevant folders, and. Uses some patterns in the file names to identify which files it wants to actually change. After that, it creates a plan. The plan is typically fairly long, so I cannot put it all in one slide, but uh I think you can get uh already a feeling from, uh, from the titles and uh the step one is to add authentication and uh. Uh, create a slash votes route in app.pi, which is a file which is typically used in the flask applications, and then the second step is to actually create a template directory with votes. HTML and then we will create the HTML for the login page. It's fairly logical, and nothing too crazy, but the split makes sense also to humans typically when they try to accomplish this task. Then it starts on the first subagent and the first subagent receives a set of um a set of instructions which is uh uh importing some libraries which are necessary to actually modify this website and to introduce the functionalities that that we want to implement then. We need to create the login route and then we want to have some checks and so on and so forth. These instructions are not part of our prompt, are not part of the request that we got from the user. They are generated by the supervisor agent for the sub-agent. And this gives us significantly more flexibility because given a task, the agent is basically able to expand and provide a detailed set of instructions to the model. What we get at the end is a login page. Which requires a password to access a page where uh you can vote for a specific restaurant. This is a uh a little recap now of how we evolve our agents over the years we went from rag, which was basically a retriever plus one LLM call to a fixed workflow, which was made of only 4 LLM calls, but that already gave a 10x boost of performance. We then switch from the fixed workflow to a quote unquote real agent with a loop where the model select the next action and is not predetermined by the workflow structure. And We also created an environment where uh the agent can interact better with the file system and using better tools and overall more effectively uh move uh towards his goal. Finally, we reach a stage where we want an increased amount of flexibility and we are willing to somehow create more complex agent with the purpose of achieving bigger goals and somehow having long running agents and can complete big tasks. Along the way, we also learned a few lessons and uh I will let Lauren give you a summary of those. Thank you, Giovanni. Um, so, Giovanni covered about 2 years of the evolution of the development of, of coding agents and it's, it has moved as fast as the field of AI has moved during this time. So what I'm going to do now is try to distill all of this experience into 3 lessons which I hope will be useful for you when you think about building your own agents or using agents. The first one is that you need to optimize for your specific use cases. There are a lot of designs that are possible right now. Models are very powerful. You have a wide choice of how to build agents. If you're going to do repetitive tasks or well-defined tasks, use a simple workflow with with LLMs. You get the power of LLMs, but you get the predictability of, of a deterministic workflow and the speed that goes with that. If you need to work on something that is interactive, um, I think a design that favors low latency is, is critical and so you wanna control, you know, what tools your model, your agent is able to use and how often and how fast these tools are, for example. Or if you want your agent to autonomously solve very hard tasks, which is the kind of things that we are working on uh in Kira. Then you want to have designs that maximize the, the power of these agents and so have agents that are able to create their own sub-agents that have access to very rich toolbox to validate their work, to get feedback signals, etc. The other thing that you need is to create metric that metrics that really reflect the customer experience that you want to create. Uh, Giovanni discussed um SW Bench a lot. This has been the, the standout data set in, in the coding agent community for a while, and we, we have used it extensively. We're very grateful for the, the team that released it. It's, it's been super helpful. But as, as we grew experienced, as we gave our agents to our, our customers and observed how they use it, we noticed a few things. First of all, the kind of problems that were in the benchmark were much simpler than the kind of problems that our customers were actually putting to our agents. You can see that, for example, by looking at the number of files that are modified in, in each task. Um, those in the benchmark, the problems in the benchmark typically modify one file, sometimes two, rarely more. Um, customer requests, on the other hand, Require you to, to modify uh 35, 10 files to, to properly solve. And so, relying exclusively on a benchmark that is, that has task that is too simple gives you the wrong idea of how good your agent is and doesn't reflect the customer experience very well. Similarly, We've noticed that um the the the length of the problem statement, the amount of details that was given in this problem statement in in the benchmark is typically quite long, um, over 1000 characters, very, very clear expectations of the task to solve. This is not how people actually use agents often, um, I. When I use them I often start with a vague idea and, and then try to try to refine the problem and that seems to be how our customers use it, uh, perhaps how you do, you do it as well and so we, we have this problem statement that that have, you know, fewer than 100 characters that are very short, um, implement this, do that, uh, very underspecified. So again we need to make sure that we, we have benchmarks that have the right kind of problem statements, um, we. To get a bit closer to to um To the kind of tasks that we are interested in solving, we created a new benchmark called called Polybench which we've made publicly available. Um, it has 4 languages as opposed to, to a single language for, for SW bench. It has harder tasks, um, that are a subset of them is verified, so we know they are actually solvable by agents. There are more varieties of tasks, not just bug fixes, but new features or code refactoring, things like that, um. And in addition, we have created a much richer set of metrics to measure, for example, not just whether the agent is able to solve the task, but whether it is looking in the right place to solve this task, whether it is retrieving the right files, looking at the right functions, not retrieving too much. It is still not perfect, of course. Evaluation is a hard problem in itself, but it helped us better understand how our agents were performing for our customers. The second lesson, um, is that you need a reliable system. That, that might sound obvious in, in many ways, so let me be a little bit more specific about why you, you need a reliable system. Um, agents are based on LLMs, LLMs are stochastic. You can, uh, submit the same task to your agent twice, you will get different, different answers. Sometimes different in subtle ways, sometimes completely different. There is also the, the power behind what Giovanni was talking about, um. Generating multiple solutions and picking the most promising one because you, you have this diversity, but that also means that it's very hard to evaluate, um. Whether a new idea is a good one or not, you need to run a very large number of tests and, and wait for the law of large numbers to kick in, essentially to be able to say whether, whether a new idea is effective or not. These tests are typically long. We're interested in solving complex, complex tasks. They can run for you know 30 minutes, an hour. So multiply this by, by a few 1000 tasks and, and try to do that a few times a day. You, you need a system that is, that is really solid to do that. Um. The Second thing is that agent trajectories are complex. So the trajectory is the sequence of steps that your agent is going to take the LLM calls, tool use, retrieval, etc. Failure attribution is very hard. You, you might, you might have dozens, hundreds of tools accessible to your agent. If one of them fails, models are now intelligent enough to try to find a workaround, try to find an alternative way to, to get what they wanted out of that tool call. This is great. This makes your system more robust, but it also makes it very hard to actually understand uh what works and what doesn't under the hood and so to, to improve your system. Um, and understanding these trajectories is not something you can do through manual inspections anymore. Trajectories, as Giovanni said, we, we started with fairly simple workflows with 4 LLM calls. You, you can put all that on the screen and kind of look at it and understand what your agent does. Now, when you have hundreds and Hundreds of steps with very large context used every time, it's unmanageable. You need, um, great observability tools. So we've, we've been building our, our agents now on top of um the Stre agent SDK and on Amazon Bedrock Agent Core. Um, SRN is an Apache 2.0 Python SDK. Actually, um, today it's also available in TypeScript since today. Um, it's. You know, it's, we've deployed a lot of agents in production and not just us based on this, this SDA. It is proven it provides a lot of different tools and resources to buy, to build powerful agents. An agent core, um, well, I won't elaborate too much on it. Swami talked very eloquently about it this morning, but it offers a broad range of, of tools to, to build, to deploy, and to observe agents and in particular, It provides all the things you need to, to get metrics about how your agent behaves, look at trajectories, analyze them, understand and improve. And the third lesson is that you need to be ready to evolve. This is, this is not a static, uh, space. Um, customers are going to use your agents in ways that you don't expect. You need to be able to observe, understand how they're using, um, your agent, understand how it is performing for them. And be ready to, to change the way you build so that, so that you improve the, the, the power of your agent for their use case. Um There will come new tools that will change fundamentally the way your agents work and the way they should be designed. For example, when we, when we brought in sandbox environments and the possibility to execute code, it implied very different architecture from, from static read and write workflows. Um, your scaffold also needs to adapt to new models. So, um, AI is moving extremely fast. New foundation models are, are released every, every few weeks, every, every few days even at this point. And they come with new capabilities every time. Um, building agents is, is clearly on, on top of mind for, for, um, the people training and releasing models. They try to improve the, the way their models power agents. Means in turn that things that were critical to an agent scaffold with the previous iteration of a model might be completely obsolete. Um, for example, building all kind of um dedicated tools to do code representation or code navigation. It was, it was great a year ago or a year and a half ago. That, that made the whole difference and that allowed you to create state of the art agencies. Completely irrelevant now because uh models are extremely good at using bash, using standard tools that developers use directly to, to explore the code. So, it's a change. Um, and, and finally, you know, we're expecting to, to see more and more small models, faster model. Become become competitive and usable to power agent and what that means is that the, the field of applications that that you can envision changes a lot, right? Um, the, the cheaper and the faster it is to, to run these agents, the more you can envision running them on uh low latency task or on, on, on device or on, on more complex on simple tasks. Um, that completes our presentation. Thank you very much for your attention, uh.