---
video_id: UPL4PqenALo
video_url: https://www.youtube.com/watch?v=UPL4PqenALo
summary: "In the session \"Agentic AI Meets Cybersecurity: eSentire’s Atlas AI Powered by Snowflake & AWS,\" Matt Marzillo from Snowflake and Dustin Hillard, CTO of eSentire, present a compelling case study on operationalizing Agentic AI for high-stakes security operations. Matt opens by clarifying Snowflake’s evolution from a cloud data warehouse to a \"Data Cloud,\" emphasizing its deep integration with AWS services like SageMaker and Bedrock. He introduces Snowflake Cortex, a suite of managed AI services that allows customers to build \"data-centric AI systems.\" Key components include Cortex Search (for hybrid search over unstructured documents), Cortex Analyst (for high-accuracy SQL generation on structured data), and Cortex Agents—orchestrators that can execute complex workflows. Matt specially highlights a cutting-edge architectural pattern: exposing Snowflake Cortex Agents as Model Context Protocol (MCP) servers to AWS Agent Core, enabling a unified \"super-orchestrator\" to manage AI workflows across both Snowflake and AWS environments seamlessly. Dustin then details eSentire's transformation as a Managed Detection and Response (MDR) leader. Facing an explosion of telemetry—ingesting 20 terabytes of log data and 2 petabytes of network traffic daily—eSentire realized that human-only Security Operations Centers (SOCs) could not scale against modern, automated threats. Their solution, Atlas AI, leverages Snowflake as a unified data lake to power an agentic investigation engine. In the past, a human analyst might spend 15 minutes investigating an alert, limited to perhaps 2 or 3 manual queries (\"tool calls\") to check a firewall log or threat feed. In contrast, Atlas AI agents can execute up to 30 complex tool calls in minutes, autonomously correlating data from CrowdStrike, network sensors, and historical tickets to build a comprehensive threat hypothesis. The agentic workflow follows a rigorous loop of \"Hypothesis -> Evidence Collection -> Refinement.\" The agent creates an initial threat hypothesis based on an alert, then autonomously decides which evidence to collect next (e.g., \"I need the process tree from endpoint X\"), refines its hypothesis, and loops until it reaches high confidence. The impact of this shift is profound. Dustin shares that rigorous internal testing shows Atlas AI achieving 95% decision alignment with eSentire’s most senior security principals, validating that the system delivers expert-level judgment at scale. This \"Agentic Capability\" has unlocked entirely new business models. Previously, eSentire’s service was tethered to its centralized North American SOCs. Now, by packaging the platform with its agentic intelligence, they can license the solution to partners in data-sovereign jurisdictions like India and Saudi Arabia, satisfying strict local data residency laws while delivering world-class security outcomes. The talk concludes with eSentire’s expansion of these capabilities beyond security: using Snowflake Intelligence to democratize internal business analytics, allowing non-technical teams to query Salesforce and Gong data through natural language, freeing up R&D capacity from routine reporting tasks."
keywords: Agentic AI, Cybersecurity, Snowflake Cortex, Managed Detection and Response (MDR), AWS Agent Core, Model Context Protocol (MCP), Security Operations Center (SOC), Data Sovereignty, Automated Investigation
is_generated: False
is_translatable: True
---

Hello, everyone. Uh, my name is Matt Marzillo. Uh, I am a partner engineer here at Snowflake. And, uh, I'll let Dustin do a more detailed, uh, introduction when it's his turn to present. Uh, but I'm joined here by Dustin, uh, who's a customer of Snowflake and AWS with EE Entire. I'm going to do a fairly brief introduction into what Snowflake offers for AI and how we're starting to see customers integrate Snowflake's AI services, um, with, with AWS. So really quick, quick introduction on what Snowflake is. Um, we've been around now for over a decade. Uh, it's safe to say that when we first came to market, we grew a lot of market share with being a really great EDW that was purposely built for the cloud, right? So customers who are used to using SQL Server or Oracle on-prem. And they wanted all the niceties of the cloud, the scalability, the low admin, um, but they want that same experience we had that at that time. Since then, we have legitimately grown beyond just the enterprise data warehouse, uh, architecture. We support Data Lake House, a lot of the work, uh, we're, we're doing with Iceberg. We also support data mesh architecture. We have great capabilities around developing applications, data sharing, collaboration, traditional ML capabilities, um, and the thing that's important here is our AI story. Um, worth noting, right, is that we've been deployed on AWS since the start of Snowflake. Uh, we integrate with dozens, we have native integrations with dozens of native AI services. So there is kind of a twofold story in the way that Snowflake integrates with AWS. Number one, you're gonna deploy Snowflake on top of AWS. We're a SAS service. It's gonna be all, you know, S3 and EC2 running under the hood, and we also integrate natively with all these different services. So, S3, um, Kinesis, Data fire hose, Sage Maker, Bedrock, Glue, Glue Catalog. So the, the partnership between AWS and Snowflake is strong. Now, diving into Snowflake's AI functionality is, uh, is looking at Cortex. So Snowflake is a true SAS service. We don't have individual services that you spin up, but rather different functionality, and all the functionality around AI is branded as Cortex. Um, the one thing that I think it's important for customers to remember is that Snowflake is not two things, right? We're not a hyperscaler, so we're not, you know, building data centers, and we're not a lab. We're not actually building models. We have some custom. Models that we have purpose-built. But what we are is focusing on what I think right now is the best experience in the market for building data-centric AI systems, um, with your, with your, your, with your data. And what that's done is that's done with these kind of 6 different boxes, these white boxes that we have here. And it generally falls, workloads generally fall into two different buckets. You're either gonna do a batch processing of unstructured data with LLMs using some of our AI SQL functionality. some of our document processing, so extracting information from scanned documents or, you know, uh, labeling images or uh transcribing audio files or translating uh transcripts. A lot of times customers first go to production with AI with those batch processing use cases. Um, the second use case that we see a lot of customers doing is building these agentic systems centered around their enterprise data. So that starts with using our hybrid search service, Cortex search on unstructured data, and then also using our This is a differentiator for us, our cortex analyst, which allows you to build context on top of your structure data so you can get highly accurate results when you make a request of cortex analyst. You then wrap that together inside of a cortex agent, so the cortex agents embedded inside a snowflake. You make a request to that agent, and it will orchestrate throughout many different tools, analysts search services, and generate a response that can then be materialized anywhere. And we have a, uh, an, an. A native UI experience called Snowflake intelligence, which allows you to interact with all those cortex agents inside a snowflake. So, And this is sort of kind of what that agent experience looks like, right? And this is what we're starting to see and, you know, what Dustin's going to talk about is kind of their journey and how they're using Snowflake and AWS together for AI. This is sort of like the visionary. This is where we're seeing a lot of customers move to, and it's as simple as taking structured and unstructured data. Using our easy to use services with cortex analyst and cortex search, and then wrapping an agent around it, and then using that directly from an application, whether that's, you know, our Snowflake intelligence, Slack, Streamlet, or something else. Now that said, um, we're starting to see customers say, hey, listen, we built an agent inside of Cortex. That's great. That's our data agent. Um, but we also want to use that agent as part of a broader agenttic system built inside of Agent Core. And this is sort of like the, the, the kind of the last 4 weeks, you started to see a lot of customers ask about this. And we have guides and quickstarts that show you how to set this up. And what you're doing is you're gonna build all these different AI systems inside of Snowflake. And then you're gonna connect to them from Agent Corp with our managed MCP server, right? So now the idea is that with one orchestrator inside of Agent Corps, you can consider all the different AI systems that you have inside of Cortex and Snowflake, along with anything else you have inside of AWS. So now you have that unified experience, a single orchestrator orchestrating across um agents and services, whether they're in uh Snowflake or, or AWS. So again, I think the last thing I'll say is that what we do see is a lot of customers start with sort of that batch processing, using AI to supplement your, your unstructured data, and then moving towards agent systems and integrating those agent systems with these kind of broader agenttic systems that are built inside the agent core. So with that, I will turn it over to Dustin. All right, yeah, so I'm Dustin Hillard, uh, and I'm the CTO. I lead our technology and product teams at East Entire. So just, uh, briefly, you know, the problem that we're solving for customers is that every, everybody needs, uh, security, but actually having a full sock, uh, and something that's 24/7 operating like Fortune 500 companies have is something that's very difficult to attain for most, you know, uh, companies outside that Fortune 500. And so, uh, for people here, I don't, I don't wanna restate all of these pieces, but it's, you know, basically it's hard, hard to get the talent, hard to put the technology in place. And so, you know, uh, generative AI and agentic AI is obviously adding to this problem, and so there's, you know, a lot of new threat services, surfaces that are emerging, and the existing tools aren't handling those very well, and the attacks are moving faster as well. So Anthropic just had a nice report that wasn't so nice about, you know, how Chinese state actors are able to use these tools to automate a large portion of how people are able to run attacks today. Um, and then, you know, uh, shadow IT has always been a thing with Gen AI. It's even larger now and so it's kind of growing the problems at an even more rapid pace from the perspective of security and IT teams having to protect environments, you know, on the positive side, it's also enabling us, and so that's, uh, what I'm gonna talk about next is how we're able to use these tools to, you know, strengthen the security posture of our customers. So just briefly about East Entire, uh, you know, we've been around for over 20 years, uh, and have around 2000 customers, uh, you know, a leader in delivering this managed detection response, which is basically the capability of our stock analysts plus our platform, you know, looking at security incidents, understanding if they're real, and then stopping them before they cause business damage. So, you know, this is about how we're using Snowflake to achieve that. And so, you know, one of the challenges we had before we adopted the Snowflake platform was we had data from all kinds of different security telemetry sources, our own threat intelligence data, customer history data, and each of those were siloed and difficult to manage, right? So the first step of our journey with Snowflake was getting all that data into one place that would allow us to easily process it and do our analytics, um. In, in that method, uh, what that also led us towards was building up a bunch of tools that would allow us to process, uh, that data and basically lead to better outcomes for our customers, uh, via our human stock analysts was the first stage of this process and so we were able to actually reduce costs by moving on to Snowflake compared to some of the legacy systems that we are using and then open up a bunch of new capabilities by having all the data in one place. So this is a little bit about kind of what our architecture looks like. So we have the edge, which is basically what our customers have in their own environments, and so the kind of core pieces that we think about there are the network visibility, uh, the agent on, on endpoints, and then log data. And so those are all the sources of data we have coming in that help us understand what's going on in our customer environments and we support, you know, a lot of the market leaders there in terms of how we get that data in. Internal to our platform, uh, basically we want to take that data, process it, and understand and enrich, and then come to a decision about whether a potential security event is a real security incident. So there's a couple of main components to that. We have a low code orchestration platform and then the agentic workflow that I'll describe more in a second here. The kind of scale that we're operating at is, uh, large, right? So on the network side we see over 2 petabytes worth of raw network data per day. Um, on the agent side, about 2 terabytes of kind of metadata coming off of those endpoint systems, and on the log side, uh, 20 terabytes per day of data. And so this is all obviously a large amount of, uh, telemetry that we use to understand and process these events. And so we're ingesting that into Snowflake on the order of around 20 terabytes per day and about 10 petabytes total data that we're seeing. So the key piece here now is that we have all that data in one normalized place, our ability to build an agentic system, uh, to execute it, uh, on top of that. So here's a very simplistic diagram of what that looks like, you know, a key piece of a successful agentic system is really the expertise required and how you can encode that and automate it to replicate. Um, and so, you know, our human analysis, how we've been successful at this for two decades, but what we are finding now is agentic systems can match the quality of our human. Analysts and so we've been able to build this system, you know, with our human security experts designing the prompts and workflows that drive the agentic system and then this gentic code orchestration platform. So for a typical security investigation, you know, previously our analysts would spend 10 to 15 minutes and probably do 2 or 3 tool calls in the course of that investigation. Now we're able to with the kind of automated workflows we have in place to call up to 30 different tool calls, uh, and give a much more comprehensive investigation and report, uh, to what was going on in that particular scenario. So it gives us a very detailed, uh, engagement with our customers and now we're also adding the ability for the customer to follow up with their own, uh, you know, query so that they can not only have the initial, uh, agentic investigation but also do the follow-on experience that you would expect, uh, from how you're using it in a consumer perspective. So all of that is sitting on top of and enabled uh by the normalized data being in Snowflake, right? So that whether no matter what vendor you have, it's all coming in in a normalized way. Our threat intelligence data and all the back end uh kind of ticket information and customer context uh allows these agents to perform very powerful actions in a way that, you know, our human analysts can't do it. Same scale within the time constraints that we have, you know, in order to protect our customers, typically we're trying to make a decision in that 1st 10 or 15 minutes so that we can contain the threat, keep it at the first dose, and, you know, as it starts to get beyond that, it's more difficult to manage and the, and the business damage goes up. And so the capability to do these deep investigations in a very short time frame is a key part of, uh, what's making this successful. So this is kind of one more click down on the what the agent looks like and so if you think about the workflow that it's going through, it's kind of, you know, creating an initial hypothesis given a security potential security incident coming in from a. Uh, usually a vendor technology or one of our own detections, and so it's taking that and then it's going through this loop of evidence collection and refining the hypothesis, right? So it can go, uh, make a call to CrowdStrike to look at a process tree or, or a query to log data to get more telemetry there or threat intelligence, uh, context of what's happened in previous events in the customer environment. And so all of these different, uh, tools are ways to then refine that hypothesis, and we basically continue iterating through that until we can get to a level of confidence to. a determination of whether this is a, you know, true security incident that needs remediation or something that's a false positive benign. So even after, you know, years of tuning and, you know, traditional machine learning and filtering approaches, there's still on the order of a 5 to 1 or 10 to 1 kind of false positive to to true positive ratio that we have in the incidents that we're looking at in our so. So this helps us, you know, uh, do a lot of work, uh, down that path and eventually, uh, deliver. Better outcomes with less effort. Um, one more thing I forgot to mention on that slide is, you know, all that work is great, uh, but only if it's of a reasonable quality, right? So you can do a bunch of work, but if the quality isn't something that matches what our experts would have done on their own, then it's pretty much worthless. And so when we, uh, we've done large scale internal studies and we're running this on every single investigation in our soft today. And so when we look at the comparison, have our most senior analysts, uh, judge the output of that agent to investigation, we're at 95% alignment with what they would have chosen, uh, the outcome to be in a security investigation. So this is really, uh, giving us confidence that it's of, of value and that what we're doing, you know, all the additional work and automation that's driving further output and stronger conclusions is something that's aligning with what our, what our experts, uh, would have done if they had the time to do it themselves. So, uh, These are a bunch of words that you don't need me to read to you. The main impact, right, is that Uh, by using these agentic systems we're able to deliver to customers much more than ever before. Uh, I think a lot of talk about gentic systems is, uh, efficiency and how, how can we, you know, cut down on what humans are doing or maybe need less people to do it. We have a very different perspective, uh, in that it's not just about reducing, uh, how many people you need to do something, but being able to deliver much more. Uh, so I think that's, that's what we're really excited about is the better outcomes that we're bringing to customers, uh, by doing that. Um, the last piece is it's also opened up new licensing models for us and so now we have a, a platform that can deliver this agentic quality investigation, um, and it doesn't necessarily need our sock to do that and so we're going to new markets and so we have a partner in India where, you know, we've licensed the platform to them. They're the service provider, so it's all hosted in India and so it's compliant from a region perspective, uh, so the, the, the data is there and then they're providing the service. And so the people are there and so it meets the needs of that market and then the next place we're headed with that type of solution is Saudi Arabia that has similar types of constraints and so this is, you know, the, the data residency plus the agentic capabilities is allowing us to bring the, you know, new offerings to service providers that wouldn't have been able to do this by themselves. And I'm seeing that this slide didn't get updated with the, the content that we put in it. So you can imagine what I'm about to tell you, which is that snowflake intelligence is awesome. Um, the, our teams, you know, we've been doing the platform utilization, uh, side of things for our product use cases that are for our end customers, but as Snowflake has grown and we've, uh, become comfortable with and like the outcomes that we can get with it, we've also started now to bring a bunch of our internal data into, uh, Snowflake itself. And so things like Salesforce data, ServiceNow. Uh, finance and customer call data from Gong. These are all pieces that we now have in Snowflake, and the Snowflake intelligence layer that Matt was talking to you about, um, uh, allows those teams to execute on top of that data. So, you know, they're, they're analytics use cases and important customer, uh, analysis that my R&D teams didn't have the time to help them execute on. But with Snowflake Intelligence now they're able to kind of, uh, once we put the data in it, they're making a lot of progress on those customary analytic use cases and so it's been a, been a good journey with uh Snowflake intelligence for us because it's really made that, uh, those workflows accessible to non-technical teams with business critical data so it's, you know, expanded the set of problems that we're able to solve for. And with that, we're at thank you. So I appreciate your call coming.