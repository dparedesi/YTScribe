---
video_id: T8T8QPiJgnw
video_url: https://www.youtube.com/watch?v=T8T8QPiJgnw
is_generated: False
is_translatable: True
summary: "This session, titled \"Build AI your way with Amazon Nova customization\" (AIM382), presents a deep dive into the robust customization capabilities of the Amazon Nova model family, arguing that for enterprises to gain durable differentiation, they must transcend generic \"one-size-fits-all\" models and adopt domain-specific solutions tailored to their unique intellectual property, brand voice, and proprietary data. Veda Raman, an AWS Solutions Architect, begins by introducing the recently launched Nova 2 family—comprising Nova 2 Lite for cost-effective reasoning, Nova 2 Pro for complex multimodal tasks, the audio-visual capable Omni model, and the speech-to-speech Nova 2 Sonic—before systematically outlining four primary customization techniques: Retrieval Augmented Generation (RAG) for grounding responses in trusted knowledge bases, Supervised Fine-Tuning (SFT) for optimizing specific task performance, Alignment for calibrating brand tone through reinforcement learning, and Continued Pre-training for ingesting vast amounts of niche domain unstructured data. She further elucidates the diverse tooling ecosystem available to developers, ranging from the managed simplicity of Amazon Bedrock APIs to the granular infrastructure control of SageMaker AI and the advanced, deep customization options of the newly launched Nova Forge, which supports sophisticated workflows like knowledge distillation and direct checkpoint access. The narrative then shifts to Dan Sinreich, Product Manager for Responsible AI, who addresses a prevalent friction point in enterprise adoption: the conflict between rigid, out-of-the-box safety guardrails and legitimate high-risk use cases in sensitive sectors like cybersecurity, law enforcement, and media. He explains how Nova’s innovative architecture permits \"Responsible AI\" customization via LoRA adapters, allowing organizations to selectively \"unlearn\" specific restrictions—such as the inability to generate malware code for defensive analysis or process violent content for media monitoring—without abandoning broader safety protocols. This method ensures models remain effective in specialized environments where standard filters would essentially render them useless. The session concludes with a compelling practical application from Gal Malachi, co-founder of Tera Security, who details their use of a customized Nova model to drive \"agentic penetration testing,\" an automated offensive security approach. Malachi introduces the \"Goggle Paradox,\" highlighting the challenge of building an agent that must be capable of generating destructive payloads (like SQL injections) to identify vulnerabilities, yet simultaneously constrained enough to never actually damage the production systems it tests. He describes their solution: a \"guardrail checker\" agent powered by a fine-tuned, distilled Nova Lite model trained on human-verified datasets, which learns to distinguish between safe test exploits and catastrophic actions. This customization strategy successfully raised their true positive block rate from 80% to 92%, powerfully illustrating how injecting specific business context into foundational models is the key to unlocking their full potential in complex, high-stakes real-world scenarios."
keywords: Nova Customization, Responsible AI, Agentic Penetration Testing, LoRA Adapters, Guardrails

Welcome everybody. Genetic models fail in specific ways. Let's fix that. Thank you for joining us today on this session where we talk about Nova customization, specifically for security and content moderation. My name is Veda Raman, and I'm a solutions architect for Gen AI at AWS, and I'm joined by my wonderful co-speakers Dan and Gel. Would you like to introduce yourselves? Yes, hi, I'm Dan Sinreich. I'm a product manager with Amazon AGI, uh, working with the responsible AI controls. Thanks. Yeah, and hi everyone. Uh, my name is Gal Malachi. I'm the co-founder and CTO of Tera Security, and we are doing an, agentic, uh, penetration testing platform. Awesome. Thank you. So before we dive deeper into Nova and customization, quick show of hands, how many of you have used Amazon Nova models? OK, quite a few. Uh, another question for you. How many of you have customized Nova models or any model for that matter? OK, not many. OK, so I hope by the end of this session, we can convince you to customize and use Novo models. So, um, over the next one hour we're gonna talk about, uh, the Nova models, introduce the new Nova models, and talk about, uh, how to customize Nova models and why customization is really necessary. And then Dan will talk about um how we customize Nova models to enable sensitive content moderation use cases and then finally Gal is gonna talk about how at Terrasecurity they customize Nova models to enable uh agentic pen testing. Before I dive deeper into customization, I wanted to take a quick moment to introduce you to the new Nova 2 family of models that we launched yesterday. Starting with Nova 2 Light, Nova 2 Light is our most price performant hybrid reasoning model, um, with excellent performance on agentic and tool calling, uh, use cases. Um, and then Nova 2 Lite is generally available. Um, Nova 2 Pro, uh, is our highly capable multimodal model with increased performance on complex tasks such as coding and agentic use cases, uh, which is also a hybrid reasoning model. And then we also launched Nova to Omni, which is our multi-modal reasoning model which can accept in addition to text, image, video, it can also accept audio, um, and speech as inputs, and it's capable of producing text and image as output. And finally we also launched Nova 2 Sonic, which is our speech to speech uh model. So here's a deeper look at the capabilities of each of these models, um, as you can see, uh, Nova 2 Omni, in addition to text, image and video, it can also accept audio input and produce speech and text, uh, uh, sorry, uh, and produce image and, um, text as outputs. And all of these models have 1 million context window length. All right, so with that introduction to the Nova family of models. Let's dive deeper into why we need to customize them. So Gardner predicts that by 2027, more than half of the Gen AI models used by enterprises will be domain specific. The one size fits all approach of these general purpose models will not be sufficient for your specialized needs. As your enterprise integrates more and more AI into your business, there's a growing demand for the models to understand your business context and your data. And accuracy alone isn't competitive. Every organization has access to good models now. What sets you apart is how you use it. And customization becomes your bridge between the generic AI and your specific business reality. So how can customization really help? So customization is how you can capture your unique IP and how tasks are done. You have your unique workflows, your unique business processes, and customization helps you embed these into the model. Customization also allows you to align the responses to your brand voice. A generic model doesn't sound like you. Customization is what brings um that consistency across every interaction. And customization helps you ground the model in your proprietary knowledge. There are no more generic answers when you customize the model. It helps improve accuracy and safety in domain specific scenarios, and Dan is gonna dive deeper into how we use customization to enable sensitive content moderation use cases in this case. And lastly, it's also how you gain durable differentiation more than what the generic models can offer. Generic models can always catch up, but your customization won't. Alright, so now that you understand why customization is necessary, let's look at how you can actually do customization. And Nova offers these 4 types of customizations, each serving a different purpose. Starting on the left, RAG, which is short for retrieval augmented generation, I think everybody has done RAG by now. Um, RAG is how you use the in context learning capabilities of the model and, uh, customize the model and, and ground the responses, uh, in your own knowledge, and it's the simplest and the easiest way to get started because you're not really changing the model parameters or the model weights when using RAG. The next option is uh supervised fine tuning, which is a little more involved and uh a little more complex because you're actually changing the weights of the model. And supervised fine tuning trains the model with that with your specified specialized knowledge for specific tasks. So if you have a task that where you want the model to be really good at summarization or really good at Q&A, you can, uh, use examples from your workflows and train the model with that input and output, uh, data sets. And 3rd type of customization is alignment. And alignment is when you want the model to sound like your brand. You want the model to have a specific tone, and you would have heard about, uh, things like reinforcement learning, uh, these are all alignment techniques. And alignment, when you want to align the model, you use feedback either from human preferences or from reward models and tune the model to be uh your own brand voice. And finally, the last technique is continued pre-training, and this is really useful when you have niche domain data that the model might not have seen before and you want to, you want the model to gain gain that deep domain expertise. You continue the pre-training process and use your unstructured data, uh, to train the model. The model not only now absorbs the general knowledge, but also your niche domain knowledge. So here's a look at a deeper look at what are all the customization techniques available uh for Nova models and all the options available for you. You can customize on Bedrock or StageMaker AI or use the newly launched Nova Forge to customize Nova models as well. And on the left here you see all the different customization techniques, um, supervised fine tuning you have theft or parameter efficient fine tuning and full fine tuning techniques in terms of alignment you can either do direct preference optimization or PPO proximal policy or the newly launched reinforcement fine tuning as well. Or if you wanna do knowledge distillation that's supported on Bedrock or Sagemaker AI or Nova Forge as well, in knowledge distillation, you're taking a larger model and uh as a teacher model and training a smaller model which is a student model. The student model, which is a smaller model will be more cost efficient but as intelligent as the teacher model now. So let's take a deeper look at each of the options, um, and how you can use them to customize, starting with uh Bedrock and Bedrock provides you a managed way to customize um your Nova models. Bedrock provides you Bedrock console access or an API method to customize the models, so you can get started in 3 easy steps. You select the source model that you want to customize. You specify the hyperparametters and the input data that you want to customize on. And then using the API or the console you set up the customization job and Bedrock takes care of customizing the model for you. But if you wanna do a full customization, you can use Sage Maker AI and Sage Maker AI has these pre-built recipes for both fine tuning as well as continued pre-training. And these recipes make it easier for you. It takes away a lot of heavy lifting for customization. For example, you need to figure out what's the right type of instance, what what is the number of instances that you wanna use for customization. All of that is taken care of for you. It comes with those recommendations so that you don't have to experiment and figure out and, and run multiple different runs iterations of customization. So with Sagemaker, you can also easily switch between multiple different accelerators for your training. Again with Sagemaker AI you get started with 3 easy steps. Uh, you specify the training and the validation data directories. You select the recipe, uh, from, uh, either from SageMaker Hyperpod or on StageMaker AI, and yeah, you run the recipe and you get the customized model as a result. And once you customize, you want to do inference with those models and you can bring those customized models to Bedrock to do inference and Bedrock offers you two different ways again so you can either do on-demand inference or you can provision um the capacity for doing inference with on-demand you, you get instant access to the uh to the model via the API. And you pay based on per token pricing. But when you want to uh do provision inference, this is mostly for production use cases, um, for dedicated performance you get you get fixed pricing based on the number of model units that you provision. And finally, uh, I'm gonna talk a little bit about Nova Forge. This was recently launched, and Nova Forge is a program that you can use to, uh, for to do deeper customization of Nova models. With Nova Forge, you get access to multiple different checkpoints of the Nova models. Be it pre-training or mid-training or post-training checkpoints. You can bring your own reward function and use the newly launched reinforcement-based fine tuning to do alignment and plug your real world, uh, proprietary environments into the Nova Forge. You can also do knowledge distillation. You can distill from a larger teacher model to a student model. And you also get access to responsible AI toolkit, which Dan is going to talk about in a bit. And with Nova Forge, you get early access to the newer models which are in preview. The Nova 2 Pro model as well as Nova 2 Omni. And with that I'll turn it over to Dan to talk about content moderation and how customization can help you over there. Great, thank you very much, Veda. Appreciate it. OK, so I'm gonna do what Veda did and maybe ask for a show of hands. Um, how many of you have run into safety guard rails either using Nova or using another large language model, or maybe you anticipate that you might run into safety guard rails because you have sensitive content that you're processing. OK, I see a few hands. OK, thank you. So, uh, thank you for the feedback. So whether you've encountered these guard rails or you're accepting, uh, or you're anticipating, uh, that you run into them, don't worry, you'll see that there's very good reasons for running into them, and, uh, you're in a safe space in this room. So, uh, so we'll show you how we can customize those for you. Um, so the good news is that we've built solutions to allow you to customize these guardrails. Uh, and before we dive, uh, deep, um, into the solutions, I think it's worth just highlighting a little bit about, uh, Nova's, um, responsible AI, um, architecture. Um, first, I wanna highlight that we are responsible by design. We ground ourselves in the eight core dimensions of responsible AI. There's science papers about this. I think the most important thing here is to remember are these dimensions around safety, privacy and security, fairness, explainability, and so on. Um, I'll show you how we can customize these later on, but keep these in mind. Uh, this is really how we design our models. We also participate in various industry leading collaborations. For example, we work um with organizations like the Frontier Model Forum, the Partnership on AI and various other government forums. Uh, and for example, earlier this year, we published our Frontier Model Safety Framework, uh, which supported the Korea Frontier AI safety commitments. Uh, we also party with third-party evaluators. We find those to be very good, uh, especially for red teaming. They have special skills and capabilities that we want to take advantage of, um, uh, as we build and design our models. And we also work very closely with academia. So for example, earlier this year we hosted our first Amazon Nova AI challenge. We had 10 elite university teams that competed in a head to head tournament. Half of them, so 5, tried to build jailbreak bots. The other half, um, built the safety guard rails, and they tried to harden the models, and we had actually a lot of innovations and, and ideas that come back, uh, came out of that. We've also published on that. If you're interested, let me know. I'm happy to point you to it. We've announced our second one, focused on AI, um, uh, on trusted software agents next year. So if you're affiliated with the university and wanna participate, look that up or come, come let me know after the, after the, the call, after the talk. Um, we also work very closely with customers to understand their needs. Everything that Veda spoke about, everything that, that I will is grounded in, in customer feedback. And one of the, uh, pieces of customer feedback, something that we got. Is that some of the content moderation guardrails don't work out of the box for all customers. So let me explain and give you some examples of that. So for example, let's say you are an internet or cybersecurity firm or let's say you're building uh maybe um security tools and you're looking to use a large language model. Um, those use cases can include generating test malware code, simulating cyber attacks, and developing various security testing scenarios. Um, that malicious code, that those malicious code pen testing may get deflected by content moderation guardrails, so we wanna customize those. Same thing in law enforcement in law enforcement and media and entertainment. Honestly, sometimes you'll see very similar. Content around crimes, around drugs, violence, illegal substances, violent content, mature themes, they sort of go together in these two, in these two industries oftentimes, and they're, they're valid use cases why you may want to use a large language model to understand that content. And similar for online platforms to think about um uh uh digitally native platform, they need to moderate their content and there's all sorts of things out there that typically a large language model would deflect even if you're using it for, for, for valid purposes. So you can see that there's a real diversity uh of use cases here. They're all valid business needs that require uh customized content moderation settings. Um, so how do we enable these use cases? We did so using a lot of the concepts that Veda talked about, uh, and I'll touch on with alignment and fine tuning. Before we do that, let's look at some of the, some of the, our core components of our Nova models and our, our content moderation tools around the Nova models. So there's really 3 components. There's alignment, and notice it's the same word that Veda used when she talked about the capabilities of customization. This, um, this refers to the fact that the, that, um, that the models, um, are, are trained to respond in a certain way. So we use supervised fine tuning or SFT, which is one of the, uh, methods Vera talked about, and RLHF for reinforcement learning with human feedback to align the models and, and to make sure that they're designed in such a way that they respond. in ways that are consistent with those 88 dimensions that I mentioned, which means that for example if you ask them to generate maybe foul language, even if you're asking the model to hey summarize a web page and tell me, hey, is there foul language, it may not do so because it's designed not to generate that same thing with dangerous weapons and other adult content, things like that. So that's the alignment piece of the model. Guardrails, um, these are guardrails. I'll show you a diagram in just a second, but guardrails, think of the first and last line of defense, uh, around the model. So we have input moderation guardrails and output uh moderation guardrails that help us quickly and robustly respond to any gaps that the model might have. So maybe the model does on occasion, it's a stochastic process in the end and on occasion it may generate content that it shouldn't. We have a guard rail around that. Um, and then we have extensive safety evalves. So, so we have lots of internal benchmarks that we use to test our models before they're released. We have over 300 distinct red teaming techniques, and this is also where we use, um, uh, and collaborate with external firms, especially in the area, um, of chemical and biological risks where there's some, uh, firms out there that have really good expertise in that. So, let me show you how we put these into practice, um, and how they work when you use Nova and then when you have a, and when you want it to, to give you a response. So, here is our RAI framework, um, as it's applied at what we call runtime. So, runtime is when you actually say, hey, here's a, here's a, um, a question, here's a request, and the model provides a response. You can see that the user provides an input. That input is first moderated by by input moderation guard rails. Um, we won't touch too much onto these we don't customize these too much, um, but if that input, if that request is not deflected, that prompt passes on to the model itself and again that, that model will, uh, is designed around RAI dimensions during training, so it generates aligned content. Um, and it'll deflect unaligned content on topics that I was mentioning earlier, weapons, um, mature language and themes and things like that, malicious code. Uh, let's say, let's say the prompt goes through, uh, the model processes it, and then we have output moderation guardrails, and these filter out sensitive content that the model again could occasionally, uh, generate. OK. So now, and then at the end, you get a system, you get a system output. So this is every time you do a request, this is what happens at runtime very quickly goes through these, um, and it, it generates or it'll deflect if it, if it's, uh, if the content is not aligned. OK. So, now that you see how um how the framework works, let me show you some examples of where we wanted to provide additional flexibility to solve the use cases I mentioned earlier. So what we've done is that is that using again some of the same concepts that Veda spoke about we've customized for specific types of content uh for those use cases. So for example, safety, you can now generate content in the area of safety which includes dangerous weapons, controlled substances, and, and, and the like, um, sensitive content is profanity, bullying, nudity, other mature, uh, themes, things like that. Fairness have to do with bias and culture considerations. Think about stereotypes against various groups, uh, and then security covers content such as malware, phishing emails, and malicious and other malicious content, malicious code that you might find. What's interesting is that what we usually see is that security tends to stand alone. Uh, there's definitely a Venn diagram where this overlaps, but, but security tends to stand alone. It tends to be used in, in cybersecurity use cases where safety sensitive content and fairness you. Tend to see them where you see one you usually will see the other. So again in a think of a TV drama or a movie script, if you're gonna see, you know, weapons and controlled substances, you're likely going to see profanity and nudity and things like that, not always, but general. So, um, so those are the four dimensions, um, that we customize and then let me show you the technical details and get down to exactly how we do that using, uh, custom models. So, um, these are the three components that work together, uh, to enable customization. So we use Laura adapters, uh, for the core model. Uh, we, we use content classification for the output model, and then all this is available using Amazon Bedrock. Just take one of them at a time. So for the core model, we do, we do train using SFT supervised fine tuning, exactly what Veda spoke about earlier. We use a, a LA adapter. The way the LA adapter works is that um it, it, it unlearns specific RAI dimensions uh while we, while maintaining core safety. There's a couple of references at the bottom left. These are some really nice science uh research that we published around how to do that. But basically with a lower adapter, the original model weights stay the same, but you're adding these small additive modifications in select layers of the model specific to one of the content areas that I spoke earlier. So if you wanna just allow list, for example, security, we can make changes just in the layers that have to do with security or safety or sensitive content, the other things, uh, the other things I was, I was discussing. So those are lower adapters, and so that helps us to unlearn the alignment of the model. Then uh for the guardrail, the output moderation. Um, uh, it, uh, our, our output moderation actually will classify the different types of content, and we know that for example, if, um, uh, if a certain type of content is allowed listed for the core model, we will also allow listed for the, uh, in, in the guard rail in the content classification. So if a customer says I need to use security, then we say great, let security go through. So that's the, that's the content classification on the output. And what's nice is the 3rd component this is all available on Amazon Bedrock as in custom on-demand model, so you do custom on-demand inference. What's nice is that it's the same style and same method as if you're using an out of the box model. Your code, your. PI stays the same. You're just using a custom model iron instead of the, the base model iron. So that's very nice. So that's on and, and the pricing is the same for, for bedrock inference for a custom model and for a uh out of the base model. So very nice and elegant way. So I'll just finish. Uh this is how it all fits together. I'll finish with just with a couple of examples here. Well, the first one is, uh, in media and entertainment. By the way, I'm using Bedrock playgrounds here. You can actually test all this in very quickly in Bedrock playgrounds. You can see here this first example is a hypothetical TV drama script. I've asked it that I would like to create, uh, you give me an idea for a TV drama that is targeted at adult audiences, um, and it should have, you know, mentions of dangerous weapons, of violence, things like that, um, and you can see that, uh, on the right hand side it's blocked by the core model because it includes dangerous weapons, includes profanity, things like that. But on the left-hand side with the adapter and the allow listing I was showing, you actually get to see and you, you can generate the ideas for the script. So that's the first example and the second example before I turn it over to Gaal is in security, very similar. This is a security example. It's used to analyze the terminal session where a malicious actor got rude access to the machine and started executing ping commands um to various malicious IPs. Um, this is actually a very common use case. You can see that when you ask the base model to explain, hey, why is this malicious? What you know what happened in this code, it'll actually, it'll actually deflect because it doesn't wanna produce additional malicious code again it's been, it's been trained not to do that. Um, but on the left hand side when we add the adapter, it actually will analyze them, it'll analyze that script, um, it'll explain what the issues are and correctly identify the security risks. So this is, this is how you can use that customization to address, uh, uh, these use cases. Alright, so that's my, my part. I'll turn it over to Gal who'll go over how Terra Security is using custom models for their award winning Agentic AI powered, uh, pen testing product. So thank you. Hi everyone, and thank you, uh, Dan for and Vera for this beautiful uh technology and uh uh introduction what we've just seen here. Um, so again, um, my name is Gaal. I'm the co-founder and CEO of Tir Security and Uh, just before, um, I'll show you how we leverage everything that we've just seen here, um, just allow me to, uh, quickly introduce Terra and what we do. So, um, just last month, um, as you probably heard, uh, Entropic published, uh, reported that it blocked the first, um, um, known AI orchestrated cyber attack. This isn't science fiction anymore. The attackers actually managed to infiltrate multiple organizations by weaponizing Clod code. And This highlights something that every security team, every organization is now dealing with. Attackers can scale faster than defenders. And this is exactly why we founded Terror Security. If attackers are using AI for offensive operations, Then defenders need AI-driven offensive capabilities of their own. So before we dive in, let's just take a quick step back and uh talk about what penetration testing or pen testing uh is all about. So pen testing is, is a practice where ethical hackers, the good guys. Um, simulate real world attacks and try to discover real vulnerabilities in life systems and in pen testing is, is not just about finding pen testing is also about exposing the blast radius. In 2025, almost 2026, in a world where everything else is automated, pen testing is still 90% manual. And web applications specifically are dynamic creatures. They change all the time and they don't have unified structure or or any standard, and this makes it impossible to automate or to to hardcore complex attacks. Every attempt so far failed, and it also makes the process slow and expensive, and it, it just doesn't scale. So at Terra, our take is simple. The future of pen testing is agentic. For the first time, there's technology that allows us to reason in real time, just like a human would. And at Terra we haven't removed the human from the loop completely. Instead of replacing, we are augmenting. So, how do we do this? We teach AI agents to hack responsibly. Our agents are trained to do every part of the penetration testing process from discovering the assets, generating test cases, and ultimately executing real payloads, uh, to discover real vulnerabilities and this last step, execution is where value and risk live. So we test life systems and therefore we have two non-negotiables. First, reliability. We must find everything, don't miss any threat, and safety. Don't harm the system or the users. And we do this by using goggles. And you'll see there is a built-in contradiction here. We need to attack, make sure that we don't miss anything, but also As you can, uh, as you can as you can understand, do it in a safe and balanced way, and this is very hard to do. So at Terra we have a name for that. We call this trade-off the Gle paradox. How do we still protect our, uh, users and customers from destructive operations, but also how we, uh, don't block the system completely from doing its job. So let me explain, um, does anyone feel a little bit overwhelmed by this SQL injection payload? Anyone? OK, a little bit. OK, me too. Um, so this is uh SQL injection, uh payload. It's, it's a very simple one, but it's also very destructive. It's a great example of something that we should never run, right? Never. And why never? Because even in dev environment if we'll execute this query, we will delete all the uh users from the uh database, including the user that is used for testing and then the testing won't be able to proceed, right? So we cannot run it um and any guesses if you'll ask an AI agent to generate um a SQL injection payload, what would be the first choice. Well, You guessed right, it's a drop table users. And what about this one? So this one, it's not an obvious destructive, uh, payload here we, uh, we changed the role of, of a user to admin, but in some environment, in some cases it might, uh, for instance, you, you might don't want to run this in production, right? So I think you get the idea, right? We need to choose carefully what to run and well, and the difficult part about it. Is that everything is determined in runtime based on the context that exists at the moment of the execution. So context matters. And to add more complexity to a fairly complex process, context is dynamic and changing. We operate on different environments, different customers with different tolerance of risk, and we have this saying, uh, uh, at Terra, what's safe at noon in staging might be unsafe at 2:00 p.m. in production. So our goggles have to adapt in real time. So we came up with this with this system of of gales. Um, first we chose Amazon Ova Pro as our base model thanks to its balance between, uh, cost and performance and out out of the box the model comes with Gatos by the model provider, so things that the model provider won't allow you to do for our use case. It was a little bit problematic because out of the box the model won't allow, won't allow us to generate offensive payload as uh uh Dan mentioned earlier, so we worked with the Nova team who collaborated and we got uh a custom model with uh um uh content moderation settings that are aligned with our needs. On top of that we have terragules, things that we won't allow the models to do like never drop any database table regardless of the environment that you are uh testing against. And lastly, we give our customers the ability to provide their own goggles and they can say whatever they want like when you hit a mongo the beast, stop, don't do anything, OK? And. Each one of these layers is a must. So let's see what we've got so far. So we have this is a very simplified, uh, uh, version of our agent. Uh, we have, uh, this is the payload generator. It, it's based on the, uh, model that we've got from, uh, uh, the Nova team, uh, the Nova Pro Security adapter, and we have our layers of gutters and the customs guidos on top of that. Uh, the, this agent has attached to it a tool that will will allow it to execute the payloads against the target system. So, when we see that, we think, OK, we, we should be safe, right? Well, actually not quite. We still see destructive payloads, uh, like drop table users, although it doesn't happen too much, we can never allow any of this, right? as, as we've uh explained before, so why are we still seeing them? Let's use an example. We all know how large language models love to use the M dash, right? Um, have you ever tried to ask them to stop using the M dash, someone? OK, so, if you did, um, you know that they will listen to you, but just some of the time. And this is exactly the problem that we are having here, right? The problem is large language models, they've they've seen so many M dashes in the training data, so when they need to print a dash, there is high probability or some probability that it will be an M dash. Same goes to our agent. When, when they uh when it needs to uh print uh SQL injection payload, there's some probability it will be drop table users. So If godles can't guard everything, We need a 2nd line of defense. If payload generation is offense, then we need a dedicated defense. And like every problem we have a terra we solve it with a new agent, uh, this time they got little checker and Unlike the other agent, the payload generator who has two jobs generating payloads and making sure that they are safe. This agent has one and only job block malicious payloads. And it does the job very well. The problem a little bit too well. So here's another example. This is an example of payload that we do not want to block. Here we are just changing the last login of a user to now, and there's nothing destructive about it. The problem, since we are uh changing a sensitive entity in the system, a user, our agents sometimes might think that this is something that needs to get blocked, and this is not good because it keeps us from running all the tests that we need to run. So, here's the thing. Not all blocked payloads are malicious. And when we are actually thinking about it, we have converted one problem with another. Now we have a battle between two agents, the offense, who tries to generate payloads for offensive payload for testing, and the defense who tries to block them. And when you give an AI agent a job to do and there's a missing context of a doubt, they will usually default to do the job that they were trained to do, right? So in our case, when there is a missing context or a doubt, the defender will mostly default to block or in other words, it will default to do what it already knows. So, we are stuck, right? How do we proceed from here? So we need to teach the LLMs new behaviors. So when our agents uh need to decide whether to block or to allow a payload, it is based on our set of rules. And we've seen that most of the problems that we are having here are due to the default behaviors of the LLMs. To do this, we utilize our own custom data, but first, obviously we need to collect this data. So at Terra we didn't remove the human from the loop completely as I mentioned earlier. Our researchers work with our agents, um, they provide assistance, guidance. Sometimes they will even do the last step on the last step on a complex attack. As our researchers work with the system, they rank the performance of our AI agents. This creates our initial dataset. Each result, together with the human feedback, is then saved to a bucket. So Let's assume that we have our data and the data is ready. What are our options? We've seen that we already tried uh a prompt engineering, which really doesn't cut it. Rug or hygenic rug are still uh they might help us get a more relevant context for the problem that we are trying to solve, but these are still uh methods of in context uh uh learning and they won't change the default behavior of the LLM so it's either, uh, uh, fine tuning or uh continuous pre-training. For our use case, since we already have high quality label data by our researchers, fine tuning was a very easy choice for us and. You might say that or you might think that uh creating a custom model is a very hard job, right? Well, actually not anymore, and we'll see that in a second, but now with Amazon Sage Maker and Amazon Bedrock, it is super easy to create your own custom model and, and make it available, um, so we'll see that in a second. So before that let's just talk about uh data curation. So we have our uh human in the loop. They are collecting the examples. We set this data into a bucket and then in SageMaker we designed a very simple pipeline. First, uh, we, um, uh, anonymized the data using LLM, uh, because we want to remove biases. We want to, we don't want to get affected by a specific customer data or specific application. Then um we uh normalize the data and split it into training data set and test the data set. Training data set will be used for the fine tuning job and the test data set for the evaluation of our newly created model. And Since we are already creating our own custom model, we have the opportunity to make it smaller, faster and cheaper. And the idea is simple. We don't need all the extra knowledge that the larger model has, right? But we do want to extract or to transfer the knowledge that the knowledge that is relevant to us from the bigger model to our smaller model. Um, to do that, we use a method called uh model distillation. Uh, let's see the complete, the complete flow. So, Again, we are starting with the collected data, good and bad examples, good examples where our researchers reached an agreement with the agent. Then since we uh in the data cleansing process we removed a lot of uh uh data anonym anonymization and normalization then we want to bring back some of the uh the context, right? So for instance like the activated goggle uh goggles, the risk profile and the environment that this example, uh, belong to and so on. And we, once we have this data, we use the teacher model, the bigger model, um, and, and, and actually add additional insights on the on this data. We are enriching the data with additional insights by the teacher model. This creates our final, uh, data set for the fine tuning job. OK, so, finally, uh, we are ready to, uh, fine tune our model. And in Amazon Sagemaker, uh, we start by selecting the best model, uh, as I said earlier, Amazon Nova Light, uh, we chose that for the fine tuning, uh, uh, job. We pull our anonymized and normalized training data and we push it into a fine tuning job. Uh, then we use the test data that we put aside earlier to evaluate our newly created model if it passes our threshold. We are done. We can uh just uh uh push it into Amazon Bedrock and make it accessible on demand. Super simple, super powerful. OK, so, to recap. We started with uh a very simplified agent that is based on the Nova Procurity adapter, uh, which is our perro generator and it had the uh and it has our gutters uh on top of it. Then it did a great job but not good enough because we still we've seen uh uh payloads that we should block. Then we've introduced the guardro checker agent that its whole purpose is to uh decide whether we we need to block or allow payloads, and he did a very good job. Problem too good, so. We use our researchers, uh, human in the loop to collect examples, uh, uh, bad and good examples where we reach an agreement or disagreement with the model, uh, choice. We put this data aside, we enrich this data, and eventually we use that to fine tune a new LLM that eventually replaces the brain of our gual checker. So, when we think about it, we actually injected Tra's business context into the flow. And the nice thing about it. We don't plan to stop here. We already have the data, the pipelines, everything is ready, so we keep iterating, collecting examples, building data sets, and improving our models continuously. OK, so what does this actually mean? We go from 80% truth positive, which are correct blocks, to 92 after the first iteration. And this is a huge improvement after just one iteration. And it also shows how by injecting your business context into the the the process you can, you can really unlock this shift from average to great. And as I said before, we plan on keep keep improving until there's nothing else to improve. So to summarize, um, we've seen that attackers, uh, already use AI, but with terror defenders finally have an AI powered advantage that is safe, predictable, and. Always on their side. Thank you for listening and thanks to AWS for the plat for the platform to make this happen. Um, Dan Veda and I will be here off stage to take to take questions. Thank you so much for listening. Who Thank you.