---
video_id: M75JFvwhjtQ
video_url: https://www.youtube.com/watch?v=M75JFvwhjtQ
is_generated: False
is_translatable: True
---

This is DAT 444. We like the number 4. Let D is the fourth letter of the alphabet. We're data geeks, so we geek out over this, but this is deep dives into DocumentDB and its innovation. So what we're gonna do here today is talk about all the big feature releases that we've announced for DocumentDB in 2025. DocumentDB is a purpose-built database for managing your document database workloads at enterprise level scale. This past year we've released a lot of really cool features errless, uh, our new compression ratio, we've had improvements to things like global failover, uh, global, um, switchover, and we're gonna tell you about these features, get kind of deep in them again. It's a 400 level course and uh. Show, show you how you can improve your workloads using a lot of these new features on DocumentDB. I guess we haven't really introduced ourselves. No, I guess we haven't. I'm, I'm Cody Allen. Uh, I'm a, uh, solution architect with the service with me is Vin. Hey everybody, my name is Vin. I'm one of the product managers with the DocumentDB team, and I'm super excited to share, uh, you know, what we've, what we've, uh, launched this year. Awesome. So we're gonna start off. We're gonna talk about the architecture. Um, before I start, who, who's never been to reinvent before? Who this is their first year here. Dude, this is awesome. We had a session earlier and like 80% of the attendees were brand new, so that's awesome. I'm glad that you're here. Anybody been here for like, like 5 times, 6 times, 7 times, man, that's awesome. We, uh, I don't know how many years reinvent's been going on, but 56 years in a row is awesome. Um, just the reason I ask that is because, uh, we have names and numbers for sessions. This one's DAT 444. DAT means it's a database session for. 44 pick any one of them. That just means it's an expert level. So an expert level session means that we kind of expect you to know a little bit about DocumentDB. You've probably been hands on with it. You're using it in a production environment. What that means is we're gonna skip over the, the basic stuff. We're gonna go straight into the kind of expert level materials, but we are gonna start off with the architecture. I wanna set a foundation for us because DocumentDB was purpose built and one of the key features of it is its architecture. So we're gonna spend some time on that. And we're gonna get into the fun stuff. We're gonna go over those things like enhanced compression, serverless, Graviton 4. We were so excited about Graviton 4 we skipped Graviton 3. we came out with Document DB 8.0. Again, we just skipped 6 and 7 and went straight to 8, and we're gonna talk about new query planner, uh, one of the new features that you get with a document DB 8.0. But like I said, um, 400 level session, uh, we're, we're, we're gonna skip over the marketing slide. We're not gonna spend time on the fluffy stuff that you can read on our web page. We don't have a need for it. You know what you're talking. Oh, son of a, uh, sorry, that was not supposed to be there. Sorry, um, so we're gonna skip over the marketing slide. Sometimes you'll see like the NASCAR slides, you know, they'll say, oh look at Aurora and look at all of its customers. We have to justify ourselves, and they'll segment out their customers by saying, oh look, we have customers here in the marketing department, we need to justify. Oh, son of a, uh, so ignoring that we're not gonna talk about that kind of stuff, help me baby Jesus. There we go, cool architecture. Let's jump right into it. Now. DocumentDB is a purpose-built database, and the main feature here is a separation of compute and storage. The reason we did this is so that you could scale your workloads based on the resources that you that you need, either at that compute layer, that storage layer, and that storage layer is actually broken up into two different pieces. We have the distributed storage volume that holds your data, and we have your backups that go to S3. We'll, we'll dive deep into that, uh, here in a little bit. Your data is automatically replicated 6 times across 3 availability zones. That means you're in multiple physical data centers, and that gives you that enterprise grade durability. What's really cool about this durability is you get 6 copies of it even if you have a single compute instance. It's not dependent on how many compute instances you have. Uh, you can even run headless. You can run with no documentDB instances whatsoever and still have 6 copies of your data. Now you can't interact with it because you do have to have a compute instance to actually query your data, but, but you could do that, um, because. This data durability is handled at that storage layer again, you don't have to worry about that compute layer at all. We scale up to 128 terabytes of data for you. You don't have to worry about it, and that means that you're not going to hit any kind of compute layer size restrictions and some database systems because the compute and storage is co-located together. If you hit a certain amount of storage that you're using in order to get more storage, you have to scale up. You have to add more VCPU to it. You have to add more RAM. DocumentDB, you scale independently. You don't have to do this. This also eliminates the need to do things like charting where you have to split your data up into certain segments on different clusters now. All of this is gonna reduce that operational overhead. It's gonna reduce the risk of having data hotspots. And because your data is replicated, has 6 copies across 3 availability zones, no matter how many instances you have, no matter what size you have, it means that it's gonna grow with you. It's gonna grow with your workload. Document databases, no SQL, uh, most legacy systems were built on relational databases, Microsoft SQL Server, Oracle, any, any relational DBAs, database folks in here. My people, I love you. You're gonna love what we show a little bit later. Um, document databases are still kind of green field, they're still net new. You do see some migrations to it, but it's really a developer's database, so you're building from scratch, and when you do that. You don't really care about durability or availability, right? You just wanna develop something that's see, see if it's gonna work, and that quickly grows, right, as you have success for it, it's gonna build up. We all know about the, the server under the developer's desk that's running a production system because they developed it. It looked great. They sold it to leadership, and now it's sitting under their desk still. That's, that's it just kind of hides there in the shadow. With DocumentDB you get this no matter what size instance you have as you scale up, so you're gonna have that enterprise level durability and availability and security from start to finish. Now, the data in DocumentDB is log structured. Most traditional databases use a block storage device. This is designed around database interaction, so that means that it doesn't have to concern itself with that underlying storage layer whatsoever. Data is going to continuously stream to S3. Most database systems you have to do a, you know, I'm gonna take us back to our SQL server days. You have to do your full backup and then you have to do your, uh, your your incremental backups that all runs in the compute layer, right? We have to use compute layer resources in order to do those backups, but because of the log structure of DocumentDB and because we stream this from the distributed storage volume. That happens outside of the critical path. That means that your applications are not affected by any of these backups whatsoever. It all happens seamlessly for you behind the scenes. Um, these blocks of data that we store in, these are called, these are about 10 gigs in size, and this is called a, uh, protection group. These are the things that get replicated across 10 or across 6 different AZs. This is why your storage grows in 10 gigabyte chunks, and this is why the minimum you're charged for storage is 10 gigabytes, because that's the smallest block of data that we have in DocumentDB. Now this storage devi the storage layer is designed around uh handling failures. It can lose an entire availability zone and a block of data in another AZ and still be and still be accessible. We have a quorum model that means that in order to have a. Successful, right, we have waited for an acknowledgement of 4 out of those 6 blocks. And when we, so basically your application sends right, we write to 4 blocks as soon as 4 blocks have acknowledged it, we tell the application you're good to go. We have the data and at the same time we write those last 2 blocks to give you those 6 copies of data. Now for reads, we're just looking for 3 out of the 6 blocks, which is what you see here and honestly for recovery purposes for reading, we're really just looking for 1 healthy node to come back and give us that data back. Now, in this situation, we have that worst case scenario, that failure situation, and guess what, the database is still available because of this quorum model. Uh, the storage volume has a self-healing architecture that means that if a block is down, it's failed, or it's gone under maintenance, whatever, it's not available, uh, the storage nodes actually use a peer to peer gossip protocol where they're able to talk to each other and rebalance that data among themselves while it's getting those other data blocks back. Other cool thing about this is traditional database systems. If you have 10 terabytes of data and that node goes down. You're recovering 10 terabytes of data that can take a minute or two, right? That that might take a second to get that back. Our storage blocks are 10 gigs, so the mean time to recovery is much faster. It takes a lot less time to restore that 10 gigs of data. All of this without having to do anything. This is all handled for you in the back end. Last thing I wanna talk about is the checkpointing processes. Databases use a regular checkpointing process, and nearly all of them. It's a single threaded process. The writer writes block after block after block with DocumentDB with this distributed storage model, we can actually make these, uh, in parallel, and we can also recover in parallel, and that makes that recovery process a whole lot faster than most traditional database systems. Now you would think that again, like traditional database systems that readers would have to go all the way back to that distributed storage volume to get new data as data is updated. And that would make reading slower, but again with DocumentDB. What we've designed is we send updates to distributed storage volume and then we send them directly to the reader instances via write ahead logs so the readers will look at those changes and if they have them in their storage they apply them if not they ignore them. In other words, if I have a document that says Cody's favorite color is purple. And I do an update. Well, that update goes to the writer instance. The writer gets it and says, OK, Cody's favorite color is now red. I'm gonna write that storage volume. The writer is then gonna tell the the secondaries. He's gonna say, Hey, I have an update to Cody's document. If you have that memory, update it. The reader instance, is gonna look at it and says. Yes, I have that. I'm gonna update my information that I have in my cache. If it doesn't, just throws it away. It ignores it, right? That's gonna lower your replication lag as well, but it also shows you that we don't dirty the cache of reader instances from the primaries. We keep these caches completely separate from each other. So now that we have a background done in our architecture, let's talk about these features, and the first one I wanna talk about is this enhanced compression. Um, 2 years ago, I think it was 2 years ago, we released uh LZ4 compression with DocumentDB, and this made a pretty significant impact for your storage, uh, volume because we didn't have any compression before that. This was the first, uh, kind of compression that we, uh, that we released out. This year we've released Z standard dictionary compression, and this is a very significant step forward with the storage savings you can see with DocumentDB. Now with LZ4, the biggest gains were with large documents. The larger the object is, the better it compresses, but with Zan, we're seeing significant gains even in smaller documents. LZ4 has to analyze each document independently. It doesn't look at a broad range of documents, it looks at each one that it's coming across. LZ4. Or with Z standard we're pre-analyzing common fill patterns and this is creating a dictionary of these so we're seeing highly efficient compression when you have repeated field names, repeated schemas, and this all translates to significant stores, uh, cost decreases, a couple other performance increases that we'll talk about in a second. All right, that's the marketing slide. What does this mean? What does this look like? So here we have a sample schema, right? Dictionary compression is going to replace frequently occurring values with uh references to a dictionary. So in this example, we have these very verbose field names like customer_ identification_n and product description text, right? These are really long. I'm gonna pause here for a second, because I wanna take a segue before we go down the compression route. Does anybody see an issue with, well, maybe issue is not the right word. Does anybody see an anti-pattern here or maybe something that we can improve even without compression? What can we do with this? JSON is great. JSON data model is fantastic because it's very easy to parse, it's human readable. We can all look at this and determine what it's doing, what the, the data is, right? It's very helpful. But you have to remember that we're taking up a lot of extra space with these verbose names, right? Every single character that we have in here is more bytes that we're adding to our document. Simplifying field names can save us a ton of space. So for example, if we shorten these, we made customer identification number and custom ID, we're saving 23 bytes per document just on shortening that field name. Product description text, we shorten that to 4 bytes, right? We just saved 20 bytes per document, right? Imagine you had a a relatively small collection of about 10 million of these documents. We do the same thing for every single one of those fields. Guess what? We just saved 60% of our storage on that document by using shorter field names. We haven't even brought compression into this, right? We're just saying shorten those field names. Something to keep in mind, right? If you go, if you go back and look at your documents, think about this. Look at your field names. They don't have to be verbose. It's good that they're human readable, but. You and I are not reading at the speed and frequency that most of our workloads are running through. All right, so that being said, This is uh this is showing you what that looks like. So the dictionary compression is gonna go through and it's gonna create a mapping of those common field names and it's going to replace them. Now the data that's stored in DocumentDB when you use this is storing the references. It's not storing the full field, it's storing just the references to that. Essentially it's building this very sophisticated dictionary by analyzing this large sample set. You have to have at least 1000 documents before it kicks in. So as soon as you, as soon as you turn it on until you get to that 1000 documents, you're not gonna have that dictionary, and then then it'll create it. It's going to, uh, you know, with when you have millions of documents, it's gonna see that customer identification number occurs in predictable positions and predictable context, and it's gonna create a more efficient coding standard because of that. We did some tests. Uh, we'll start off with red because red is negative, red means bad, right? What we see in red is the CPU usage or CPU utilization when we turn on the standard compression. Um, it went up by about 17%. It went up from 59% to 69%. Well, clearly compression isn't free, right? That CPU represents that computational cost of compressing and decompressing those documents. Next, the obvious impact here is the blue line, right? Even versus uh LZ4, the amount of storage that we're, we're keeping on disk with this compression is significantly lower. We dropped from about 340 gigs down to 125 gigs. And then finally what we see in red, purple, pink, I don't know, it's kind of a weird color, uh, the same color as your headphones, I think. Um, we saw that right throughput went down from 142 megabits to 85, but what's cool here is it went down. And our operations increased, so we were able to increase from 92,000 inserts per second to 143,000 inserts per second while lowering this throughput. So you're, you're getting improvements across the board. We also wanted to test how this worked with different schemas, right? I told you that, uh, LZ4 is great with big documents, Z standard is great in small documents. So we started off with the GitHub user database. Uh, this is a publicly available, uh, data set and documents here are about 1 kilobyte in size. And when we use the standard over LZ4, we saw a greater than 3x improvement in compression ratio. We also created our own retail schema, about 2 kilobyte documents, and we saw about a 2x increase in that compression ratio. At the very end, we have this large metadata, 13 megabyte documents, and we saw the compression ratio go from about 18 to 1 with LZ4 to 147 to 1. Imagine that your 13 megabyte document just went down to 90 kilobytes. That's a very substantial decrease. What's cool about that very last graph, that last example. That's not a test. That is a production customer that did that. When we released that, they applied this to their workload, and they were able to see an 8x increase not from uncompressed versus LZ4. Now, at enterprise scale, uh, we'll we'll talk about the CPU again, the negative part, right? I want to focus on that, but at enterprise scale, storage costs often exceed the CPU costs of slightly, uh, slower compression. So for example, a 40% compression ratio can translate into pretty substantial, uh, cost reductions for what you're paying for, uh, storage. Beyond direct storage cost, compression reduces your IO operation. It lowers your backup footprint, and it even decreases your network bandwidth usage. Compressed documents require fewer IO operations. This is because more logical data fits within each physical IO operation. Compressed document gives you better cash utilization, right, because you can fit more of these documents in cash. We keep documents compressed on storage over the wire and in your cash on those compute instances. So what that means is that might decrease the need for your compute instance size. So that gives you additional cost savings through cost optimizations. Here we're just seeing what the cost difference is because that's the physical thing. Now everything else that we're talking about your cash, your compute size, that depends on your workload. Here I wanna, I wanna kinda stretch your minds a little bit. I wanna think about this as a multi-dimensional, uh, improvement. A couple of years ago we released uh IO optimized storage for our customers that have very busy, very heavy IO heavy workloads, and this was really marketed as a cost improvement, right? It, it allowed you to stop paying for IO and basically get it for free, but the trade-off is you're paying more for storage. But behind the scenes there's actually a lot of performance increases, right throughput increases. Uh, and latency decreases for that. We won't go into that too much here, but you get cost benefits and performance benefits. Now the trade-off, what is the trade-off here? You don't pay for IO anymore, but you do pay 3 times more for storage. So you're paying as of right now, US East one, you're paying 10 cents per gigabyte per month with standard. With IO optimized, you're paying 30 cents, so, so 3 times more. In this example, with LZ4 we were at 1.7 terabytes compressed, but with the standard we went down to 500 gigabytes. What that means is that our inflection point for where I optimize can make sense goes down significantly. So our break even point for standard was about 1.7 billion operations per month, about 660 operations per second. Once we use the standard, that inflection that that inflection point lowered to about 500 million operations per month or about 193 operations per second, so you don't have to have this massive enterprise scale to start recognizing these cost savings when you start combining these. Z standard allows you to combine with other document DV features and really multiply your savings and possibly even multiply your performance. Where do you use it? Where does this work best? Well, dictionary compression is really good when you have repetitive field names and, and repetitive structured elements, collections that have deeply nested fields with consistent schemas, standardized schemas. They're gonna, they're gonna get the best out of this, right, because they're, they have field names that are in a predictable spot and they can make the the dictionary can help that. However, the other side of this, if you have a lot of documents that have variable scheme names, very sparse schemas or highly variable structures, well, you, you're only gonna see modest improvements. You're still gonna see improvement, but not nearly as much as if you had standardized schema. Compressssion is usually better with larger documents. That's it. 400 level session expert. That's what you learn. Comression works with big things. That's it. Thank you. Good night. No, I'm kidding. Compressssion works great on big objects, but, and that's where LZ4 Focus Zan works with those big documents we saw it with the 13 megabyte documents, but also in the smaller documents. Read heavy workloads generally are gonna benefit more from Z standard than write heavy just because there's less IO bandwidth and we can process those a lot more efficiently in cash. Things like high frequency point queries are gonna have an improved latency, but on the other side. If you're doing large range queries that are returning multiple documents, you might see a performance decrease, and that's because of that overhead, right? That compression and decompression overhead we saw earlier for right operations, simple inserts benefit from this again because of those lower IO operations. However, if you are doing any kind of complex operations where you're doing multiple rights at once, there's a lot more compression that has to happen, higher CPU usage. Oh, and we have pretty pictures that go with it. Alright, next, graviton for instances. Is anybody using graviton for instance undocumentedB yet? You started to raise your hand. You want to. You're, you're like right there, OK, not with DocDB. Got you. Um, so Graviton, stepping away from DocumentDB, Graviton is unique to AWS, right? It was really designed to give you the best price performance for cloud workloads. Um, Graviton instances are really good at CPU intensive workloads. They're really good for memory intensive workloads. Like databases, so it really makes sense. When we released, uh, graviton 2 instances in DocumentDB, we saw a 30% price performance increase. Each graviton instance that we released from 2 to 3 and now to 4 brings pretty significant performance gains, and this 4th generation is seeing 30% performance gains over graviton 3, not graviton 2, but Graviton 3. So similar to when we released Graviton 2, we're seeing pretty significant performance impact to workloads and cost, uh, benefits from using these. Well, what does that look like? So we looked at it from an in-memory versus out of memory workload to see what kind of performance impact we could have. Clearly, in memory workloads are gonna benefit significantly from this. Uh, for we're seeing over 100% price performance increase in read workloads and update workloads 136%. Even for those workloads that are out of memory where things don't fit in cash, you're still seeing a significant increase in your performance there. Now, there is a price difference. Um, we started off with R5 instances. We actually started off with R4, we'll ignore, ignore those. We started off with R5, we released Graviton 2 R6Gs, and we lowered the cost by 5%. So what was cool is. Even if I did nothing except change my R5 instances to R6G, I saved money and my performance increased. It was, it was dead simple. It made sense. With R8G, you're seeing a 5% increase. So essentially if you're going from R5 to R8, there's no difference. It it's staying the same. It went down, it went up. Despite the cost increase though, we can see a performance increase that far exceeds that price premium that you're paying for these performance gains are consistent across all instance sizes, whether it's extra large up to 16X large, but what you're seeing here is this performance benefit by switching over to these. Next, serverless, so no R8G users. Anybody using DocumentDB serverless currently or tried it out. Oh man, y'all have so much to do when you get back, you need to try out serverless. But to talk about this, I'm gonna hand things over to Vin and he's gonna walk us through that. All right, thanks, Cody. Really awesome actually some of those things that uh. Cody shared just earlier about the performance benefits of RAG. We actually, when we launched it, we actually already had a few customers come to me. I actually met some of them earlier today about how all they had to do was switch over from an R6G to RAG. There was no functional changes, and they just got better price perf. In fact, some of them were able to actually scale down their instance, so there's a cost, uh, you know, indirect cost savings there as well. Alright, so, um, you know, for those who have missed the first few minutes, I just saw a few folks coming in later. My name is Vin. I'm one of the product managers with DocumentDB. Again, and we're gonna dive right into Servili where I'm gonna provide a short overview and we're gonna dive into some of the inner workings of how we made Servili work for DocumentDB. So the core concept of service list is that the system scales with your workload requirements, right? So as compared to a traditional system where you could either A underprovision, which causes some challenges if you have a peak, so they might cause some application um challenges there if it goes over your system requirements, it cause some challenges, or you could overprovision. Again, this is gonna cause some other types of challenges in terms of being inefficient use of your resources. Now, in between this, you could manually alter and change the way, uh, change your resources as you go, however, that is a uh extra extra step for you. The core idea is that Serverless really scales with your workload needs, and this is exactly what Serverless uh in DocumentDB does. So in August we launched DocumentDB Serverless. So if you haven't tried it, I don't blame you. It was just right before the holidays and it was at the end of summer, maybe you're coming back from your vacation. Um, so the idea is that if you have larger workloads, the instance will scale up to meet the needs of your instance, uh, meet the needs of your workload, and it could scale anywhere from as less than 1 CPU with 2 gigs of memory all the way to 64 CPUs with 512 gigs of memory. So anywhere in between in fairly granular form. You're not making those jumps like you do with a, say, R6G large which has 2 CPUs to an XL which has 4 CPUs, which is a more discreet jump. You can easily switch between er list and provision instances so you can see here I have a erless instance in the middle there, it's sitting beside two other instances, two other replicas which are different provision instances. What this really enables you to do is to adopter list without having to do any data movement because of that benefit of separation of compute and storage. So whether you have 100 terabytes of data or 10 gigabytes of data, the amount of time to switch from between provisioned and serverless is pretty much constant. And last but not least, we introduced a new unit to measure uh database serverless resources and it's gonna be called DocumentDB capacity units. So let's talk a little bit about that as well because there's some cost implications here. Uh, database capacity units, DCU for short, has one DCU has approximately 2 gigabytes of memory, some CPU, and network resources, and this comes in a package that is, we don't scale your CPU, your memory, independently. This comes as a package. And on the right side you can see that there's a max and minimum DCU. When you're provisioning your cluster, what you could do is specify the number of max DCUs or minimum DCUs to help you control your cost or provide performance guarantees, and our system does scale in increments as small as half a DCU. So you're not scaling between, I was saying, uh, in discrete steps of, say, you know, 2 VCPUs all the way to 4 CPUs. This does, this does, uh, more incremental scaling. And we scale on many dimensions. There's an internal algorithm where we scale on CPU utilization, memory pressure, uh, network throughput, all these aspects will contribute to scaling and adding more DCUs to your system. And the last point, this is actually a very important point, um, and I'll tell you why later, so just keep a note of this last point here later in the slide. I'll be referencing this and also an interesting aspect is that we don't scale linearly. That is, if you have a larger database, it will scale faster than that of a small one. So let's say you have a 20 DCU instance, it's gonna be able to add 10 DCUs faster than that of an instance that has only 1 DCU as a starting point. All right, let's get into dive a little bit deeper on actually some of the um implementation details. Actually one of the key aspects how we made serverless work for DocumentDB and that is actually one thing specifically I wanna talk about is buffer cache, uh, resizing actually for a while now you could actually already today. Dynamically change your resources applied to a virtual system. So if you're working with C groups and Linux or containers, which is built on the concept of C groups, you could actually change the, the setting of how many CPUs and how much memory is being applied to your container or in Kubernetes. However, the challenge that with the database is if you just start, uh, if you just decide to, hey, look, I'm gonna just reduce the amount of memory. This could cause a lot of thrashing. That is, you may be evicting the wrong pages, and that's going to cause a lot of performance issues and it's going to drive up IOS as well. So let's talk a little bit about, you know, this diagram. So here in the, the bars you see from the pink to the red, these represent pages inside our buffer buffer pool. The, the darker the red, the more frequently it is accessed. And I think one of the things that we follow that we have internal algorithm of how we sort the pages is a combination of both least recently used and least frequently used. So let's say I do, I do a lot of frequent reads from my buffer pool, my application, those are in the deep red, and let's say I need to go to disk to get some data. So here I'm gonna go fetch some data from disk and notice how the way we sort it is not in the very front that is it may just because we we we accessed it we accessed it recently doesn't mean we access it very frequently. So later on when we do do any resizing activities, we may just truncate those pages uh before we do more frequent ones. More frequent ones. Alright, let's take a look at how we shrink, and it's pretty simple. So for pages that haven't been used for some time and it is so based on a combination of LRU and and LFU, we're just gonna truncate and drop those pages from our, our buffer pool. And this is gonna enable us to shrink our bufferer pool and resize uh our buffero pool. So this is what you see when a service scales back down when you're not using that many resources. This is how it happens. And again, I really wanna stress that this is a little bit of magic here because again if we evict the wrong pages very frequently, it's gonna drive up IOS and this stra uh this strategy really ensures that we're not doing some of those things that cause performance and cost issues. Let's go to an example here. I want to show you two workloads. Uh, one is the workloads against a provision instance, and the other workload is against a serverless instance. The view you are seeing here is the view we have a tool called Performance insights. It measures how much load is on your instance, and that dotted line you see in the middle is how many resources in measured VCPUs on your instance, your system. And the reason why this VCPU is why we choose VCPU is because one, if you have this represents approximately how many processes do you go run at any given time. So you can see in this diagram that most of the time we have about 3 or 4 average active sessions. That is, there are, there are 3 or 4 queries that want to be run at any given time on average. However, we only have 2 CPUs, 2 VCPUs, even if we add more load to our database, it's static. It's not adding uh adding any more it's not changing the amount of resources and what this means is that I may be experiencing additional query latency even if my indexes are in place, even if my my my queries are optimized, my database is underprovisioned. Let's take a look at it from another view. So we could slice this by weights, and what this tells me is, what is my process spending a lot of its time doing. And here, notice how there's a lot of red, uh, depending on the color, maybe brown from the back, um, this is IOs, and what this indicates to me is that not only am I trying to do 3 or 4 things at any given time, and I only have 2 VCPUs, I'm actually spending a lot of time going to disk, spending a lot of time going to IO. To me this indicates that hey maybe I don't have enough memory. I don't have enough memory for the working set of my query, so I'm actually very being very inefficient and it's gonna increase the query latency of my workload. Now let's take a look at the same workload on a serverless instance. So on the bottom there on the the the bar graphs you see that it's about 3, that's how many again 3 average active sessions, but you notice that the bar is already at 16. So what this tells me is, hey, look, the system needed more resources, uh, perhaps probably for memory, right? So the working, so even though I'm only running 33 processes at any given time, I needed uh 16 VCP or DCUs worth of resources to run this query. Now later on to the right, what I did was I just added more artificial load just for demonstration purposes to show you that that that that VCPU line which represents the number of resources on my instance is not static. It does scale as I'm adding more load to our database. Now I wanna show you from another view. This is the view based on weights what are my CPUs doing? Notice here that it's mostly green and it's actually not spending a lot of time doing IOs and this signals to me that, hey, this is actually quite efficient. I'm not spending a lot of time going to disk, getting data, bringing it back and processing it. I'm actually spending a lot of time in CPU, which means I'm just processing this data, so quite efficient there. So if you haven't tried El Servius, highly recommend it. It's really easy to switch back and forth from provisioned, um, as well. All right, switching gears to document DB 8.0 and the new query planner, which is core of DocumentDB8.0. Uh, who here heard that we launched DocumentDB8.0? Very few folks, well, I wouldn't blame you because we actually just launched just 3 weeks ago, pre-reinvent, probably during Thanksgiving, uh, for this thing, uh, before this event, uh, so if you missed it, this is a very high level recap of what's included. A big focus this year, uh, in terms of our latest launch is really around performance, and I think Cody already covered some of those aspects with compression. But the new query planner is also a very key part of the DocumentDBA.0, which I'm gonna go into shortly in a bit. Uh, the last performance piece is around vector indexes. So, you know, to future proof you a little bit about, you know, some of those apps that you are gonna build on DocumentDB, we introduced the parallel index build for vector indexes which provides up to 30 times faster index builds for vector indexes. Uh, the, the second piece of it is Moga DB API compatibility. We heard from a lot of our customers, they want to bring their Manga DB 76, and 8.0 applications. They have the drivers that work with those versions. You can now bring those workloads to DocumentDB as well. We've also introduced a handful of new Moga DB APIs and functions such as views and collations that's included as well. So if you are waiting on some of these features to adopt DocumentDB, we have them now. Let's go into the new query planner, which we're gonna spend the last bit of this, uh, session on. So starting 8.0, we launched a new query planner, and this is gonna be a large collection of improvements. However, for the remainder of the time, I'm only gonna show two examples that highlight some of the things that the new query planner can do. First is choosing um improvements around choosing an index. So the nuclear planner has expanded index support uh delivering performance improvements for certain uh certain operations, and this includes, for example, if you have negation operations, we will now choose an index over a collection. So let's walk through a very simple example. I have a sample document here, a very simple one. I'm gonna create an index on it and then I wanna do a find, but notice how that find has a nin operation or not in operation. Now with query planner one, now you could actually rewrite your query to do this, to take advantage of the index, but the default behavior was to use, uh, the winning plan was to use a collection scan. You can see here it says cold scan. And however, with a new career planner we default to the to the winning stage of using an index scan. As noted there, the second thing I'd like to call out and point out to you is we also, when you do explain plan, we also included the planner version as part of the output. So, uh, if you are doing a migration and you want to use the first, um, you know, the old planner version as you're switching over, you, we allow you to switch between planner versions, but it's highly recommended that you use the latest one. So that was a very simple example where we change a single stage and we swapped it from collection and scan to index scan, but let's take a look at a more complex example with uh with aggregation pipelines. Not only do we change certain stages, we actually have the ability to take a look at your query as a whole and find optimizations. So here I'm, there's 22 stages of my aggregation pipeline. One of them is a lookup if you're familiar with SQL, this is equivalent to a join. And unwind is a pretty much an operation that flattens out uh any any document that has an array in it. So by doing first what this query is gonna go do is first do a join effectively and then flatten out all the all the arrays. But with the new query planner we actually simplify this and do a single step and I'll show you that through an example in a bit. Now for those who are unfamiliar with unwind. Um, I wanna show just a very simple example of what it kinda looks like with emojis. Uh, if your document looks something like this, um, and you, uh, you do an unwind on the array field, you get 3 documents. So, uh, uh, you do an unwind on the chef, uh, that chef chef emoji there, you get 3 instead of 1. Alright, so let's go through an example. So let's say I have two collections. One of them is my orders, uh, collection here. I have the customer name and the product IDs of which they ordered. And then I have a 2nd collection which has all the product details, so laptop, the price, phone, the price, and so on. Now let's say I want a I want a set of documents that has all the orders for each product detail, right, so you can see here effectively what I'm expanding on is the orders, the orders uh collection. I wanna blow that up and I wanna get a detail of, of every single product, uh, product that is being bought in the orders. The way I would do this, the exact same query I've shown before, first I'm gonna join these two collections, and then I'm gonna do an unwind on it, which flattens out each one of those arrays. Um, so with planner version one, and I know there's a lot going on here, what I'm showing you on the left, and this is for reference purposes, this is the output of the explained plan, but I have the tree view equivalent on the very right. When I am, one thing to note is that this nested loop lookup stage, what I'm actually doing is I'm actually joining these two collections, and then I'm gonna store it in memory so they can unwind it later. Now this ends up to look something like this, where I have order, order number 1, and I have 2 products on the product details, order number 2, and I have multiple product details. You can imagine that if Amaya Ray had hundreds of hundreds of entries and I have lots of different products to look at this object could be can be very, very big, and the fact that we're just storing it in memory means that this intermediate step could be very, very expensive. Whereas Plant version 31 thing that one thing that we do is that we do not create this temporary object and store it in into memory. Instead, if we take a look at both of these collections, one thing that you'll note is that. The final stages produces the documents and how the the query planner does that is it notices that hey look, I'm trying to do an unwind after a lookup stage. So instead of building that temporary structure and then just to unwind it later, as I'm doing the lookup for each individual product ID why don't I just look, do the lookup against the product's collection and stream those results as the final set and that's exactly what we do. So if you compare the tree structure of both these query plans, uh, one thing I'd like to point out is this nested loop, uh, loop lookup returned 8 documents. I know, obviously I'm working with a very small collection here, but if you have a lot of, uh, a lot of documents, you could think of this as almost like, um, ON squared notation of runtime. So this, this stage can use up a lot of memory. So these again these are just two examples of the new query planner. There's actually like tens of 10s and hundreds of optimizations that we're planning to go do, um, that you could check on our developer docs. This is just to to showcase what we're doing with our new query planner. All right, one more thing, I know this wasn't on the agenda slide, but I really want to show this, uh, slide, and I just really wanna highlight that not everything we do in DocumentDB is an explicit feature. There are a lot of things that we do behind the scenes just to make things better, and I wanna show you something that's actually what our engineers see. Uh, it's very chaotic, chaotic graph, but this actually represents all the time somewhat, uh, our customers do when they do a switchover. So if you're not familiar with switchover, this is when you're running a cluster across two different regions and you want to promote one over the other. Um, between August 4th and August 11th on this graph, uh, before we actually introduced a, an optimization, the P90 time to do a complete switchover was 300 seconds. Uh, we actually rolled out a change starting August 11th. And we were finished our deployment where we simplify the um the workflow, the promotion workflow behind the scenes, and we actually found that our P99 can go as low as 30 seconds. Our P90 as low as 30 seconds. So this is a 90% increase over switchover. So this is just there are a lot of things that we do as a part of our service, um, part of our service that just makes things better. So this is something I really want to show you all there. OK, summary recap, there's a lot of things that we launched this year, um, you know, this, this recording, this, I think this, this, uh, recording will be available on YouTube later, hence is being recorded in the back. So if you miss anything you can always come back. You can always check out our what's new or our release notes pages which has every single thing that we've launched this year as well. One thing I like to call out, you may have seen some folks wearing this hoodie. I think the expo is just about to open tonight or tomorrow, um, if you go to the database booth, remember that thing I talked about that was really important about serviceless databases. So one thing that you can go to them is if you go to this booth, go to them and tell them one thing, one interesting thing that you learned, and that is, as a reminder if you forgot about it is that with serverless instances, large instances scale faster than that of smaller ones. So remember that, uh, you could I think, I think that, uh, the booth is just opening up right now and not we are one of the ones that, uh, one of the sessions that just follow right before the booth opening. So hopefully you all are actually getting the. The secret there before others do, given that the session is on a Monday. And with that said, thank you so much for attending the session. I know it's Monday 5:30 again, but the last ask I have all of you, and this enables Cody and I to come back, uh, year after year is really complete the session survey in the mobile app. Um, so thank you so much and have a great reinvent.