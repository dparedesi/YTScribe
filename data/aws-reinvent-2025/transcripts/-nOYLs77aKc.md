---
video_id: -nOYLs77aKc
video_url: https://www.youtube.com/watch?v=-nOYLs77aKc
is_generated: False
is_translatable: True
---

So this session is about machine learning, inference endpoint on a different type of hardware. We are going to show you how to deploy a machine learning workload on Apple Silicon, on Amazon EC2 Mac. You know that sometimes GPUs are not easy to access. They are super expensive. And you might have these McMini that are laying around your desk on which you have unused cycle during your pipeline. So why not use these McMini also for something else and something they are very capable about? It's about inference, machine learning in France, or even training. So my name is Sebastian Sebastian Stormack. I'm a developer advocate at AWS and my partner in crime today is taking pictures in the back of the room. It's everyone. Eran is going to do the hard work. It's a code talk, so we are going to spend time showing you code with Jupiter notebook, a lot of Python, and going down into the mathematical details and the coding details of running large language models on Apple Silicon. As you know, as I said already, getting access to a GPU, it's extremely complicated. It might be quite expensive. Most of our customers are telling us that they need a large number of GPUs to run their machine learning workloads, and getting access to the GPU is not the answer of the problem. It's only one part of the problem, because once you have the GPU, you also need a lot of storage. You need extremely fast, high bandwidth network to move the data around the different nodes inside your cluster. And building this infrastructure, even if it is in the clouds, it costs you time and money. You are in a super competitive market. Every minute that you spend trying to create infrastructure or to manage infrastructure or update infrastructure, it's a minute that you cannot spend on what matters for your customers, the application or the end result. GPU are super powerful, but they come with one bottleneck as well. They have that siloed memory. The CPU has its own memory and the GPU has its own memory. They are totally separated. So when you have a training or inference task that needs to move data between the CPU and the GPU, all the data are going through a bus which might add extra latency. And to solve that problem, Apple came with a radically different approach. So inside a Mac Mini today or even inside your phone, inside the M chip, the Apple silicon chip, we have a system on a chip that combines on the same chip the CPU, the GPU, the neural engine. And one large bank of unified memory, so it means the CPU and the GPU can share the same memory. There is no more memory transfer in between between the two, of course. The GPU cannot use the entire memory. You need to leave a bit of room for the operating system to work. So typically you can go up to 70% of the memory on these systems. So they are not as fast as the raw, discrete performance you can get from an Nvidia chip today, but they are very power efficient. This is what you have in your pocket if you have a recent iPhone, and this is what you have on your desk if you have a Mac. And the good news is that you can get Macs. In the cloud as well. So around 2020 we launched Amazon EC 2 Mac. EC 2 Mac, these are Mac mini that are in a special enclosure inside our data center. They are connected through the Thunderball port to an AWS Nitrocar. Nitrocart is the system that allows us to connect to the rest of the AWS network to provide security as well. So it's everything that you know and love about Amazon EC 2 for the last 20 years applied to a Mac. So it's a real Mac. It's for you. It's a dedicated host. There is no virtual machines. You have access to the raw hardware, but at the same time it can access your VPC. It has security groups, IM policies. It boots from an external volume. I told you everything you know and love from EC2 is available, but for Mac OS. So we have different types of Amazon EC 2 Mac. I'm not going into the whole list, but we start with the Intel one which are out of the scope for this talk. Obviously we need Apple Silicon for this talk, but we have M1, M2. M2 Pro, M1 Ultra, M4 M4 Pro that we launched recently and yesterday during Matt Garman keynote we announced M3 Ultra and M4 something, I forgot it was just yesterday. So these are coming. They are either in preview or pre-announced for later, not later this year, for next year. Look at the number of cores you have there. On M4 Pro, you have 14 cores. You have 16 neural engines. On M1 Ultra, you have 32 neural engines. So that's a lot of processing powers that are available there. To run your large language model on Apple Silicon. Oops, I'm going a bit too fast. We need a framework and we are going to use MLix. So MLix, it's an open source array framework that is purpose built for Apple Silicon. It's a very flexible tool that can be used for basic numerical computation. All the way to running the largest model directly on your Apple device. So if you want to generate text, generate image, generate video or audio with a large language model on your Mac, MLix is the framework that allows you to do that. You can also use it to train, to fine tune, or otherwise customize your large language model. So MLX equals Apple Silicon. I mean MLX is designed to run on Apple Silicon, but it has a very similar API. So if you're familiar with Pytorch or NumPy, you will find that the API from MLX, it's very similar. So it's very easy to port code between Pytorch, NumPy, and Jack to Melix, and Melix integrates with other tools. Maybe you are using LLM Studio on your Mac, so you can. Have ML installed alongside LM Studio and use large language model through LM Studio on your Mac. Of course it has a Python binding Python API, but not only it has a SWIFT API. SWIFT is the open source programming language created by Apple a bit more than 10 years ago, and it even has a C++ and a C API. So Eran will show you a bit more, but look at the similarity of the code. On the left side you have a very short example of code written for Mix, and on the right side you have the same code for Pytorch. We are calling the linear function on the object there. And you see, the main difference is at the end we call the reduction linear unit the function on the nn object directly on the ML version, and we call it on the layer on the Pytorch version. So you see very, very similar code between ML and Pytorch. And that was my introduction. I promise you that we are going to have a lot of code today. It's a code session, so we are going to dive into the code and everyone will show you how to actually uses. Thank you very much. Hey everyone, uh, nice meeting you all. My name is Erigan Eon. I'm a solutions architect from Tel Aviv, Israel. Uh, I'm with AWS for the past 5 years, very excited for everything data related, uh, which also, uh, led me to the world of, of neural nets and, uh, the, the resources and needs that it brings to us. So, uh, in, in our example today, uh, what do I wanna do with, uh, with you, I wanna talk about, uh, MLX. I wanna talk about a bit of, uh, things that Apple did within MLX in order to allow it, uh, to, to be optimized for the different hardware devices that, that they have. Uh, for example, if, if you're running something on an M1 or an M4 or something else like an A whatever chip. Uh, then it will try to use the best scores that it can for that specific task because it has the resources from within that device. So, uh, I, I wanted to talk about a few things in, in, in MLX. Uh, 11 of the most important part of MLX is that it's lazy computation and that, that it has, uh, uh, function transformers from within it. So if we'll start, uh, going through, uh, our first Jupiter notebook. Uh, as you can see, I'm running a local host. This is an SSH pipe to an M1 Ultra Mac on AWS. Uh, so that, that M1 Ultra is running a Jupiter lab, and we're connected to it through a pipe, and we're running it here. So as you can see, uh, just a basic example of a comparison between MLX and NumPy. Afterwards we'll start discussing also other stuff that Apple add, added like the neural network uh class and the MLX LM extension that they built. Uh, so let's, let's get started. So, uh, we'll just import MLX core and NumPy for a second. We'll also import time just to be able to, uh, to check things out. And if we look at how we're creating arrays in both, in both array frameworks. So as, as you, you can see we have an AMP example on top and then we have a, an MLX example on the bottom. If we look They are pretty much the same. Uh, most of the differences come from things like, uh, we can see that the random function looks a bit different. Here we're using uniform from the random class and we're providing it a shape. We can also provide it a data type. So, uh, this can be, this can also include a D type and say, all right, I want MX, uh, I don't know, uh, some float. At this moment we'll use it as is and you'll be able to see. That uh we're also getting that data type out and we can, as, as I said earlier, say all right, we wanna use a different data type, let's use float uh Uh, float 16. Then, sorry. So, Amex.flows in and you can see that we've chosen the data type for everything and we can, we can continue forward. Now, one important thing about MLX is that we can decide on, on which device we want each calculation to actually happen. Now, as I said before, MLL as lazy computation, like we know on other frameworks. Python specifically is an eager evaluator, so there's quite a difference there. Uh, but if, if we'll talk at the basic level of things, uh, when we're, once we're doing something in, in MLX, uh, like a basic operation on it. Uh, nothing should happen up until the point that it, it requires either an eval, a type conversion, which again requires an evaluation or something of that sort. Uh, so, Here, let's just take the same calculation. We have two arrays and we wanna add one to another, and we can actually decide on which stream we would like to run this on. So there's like a GPU calc and a CPU cal that I did here. We'll see the the results will be the same because we're expecting it. Uh, but you can see that we can just decide on which to run. Now, uh, we'll get to the lazy evaluation in a bit. I wanna show you, uh, a bit of basic operations differences between them. So as you can see, uh, pretty much again looks the same, the same, very similar. Some differences come by things like APIs. So for example, here you have NP, so we don't have that one in MLX we have MathML, and again we can use the Python derivative in order to do that. Uh, so we don't necessarily need to, and we, we can see again that we are receiving the same values and so on. Now let's get back to the lazy evaluation. Now when MLX is uh is actually running things, uh, again, if we're not forcing an eval or type conversion, it will not evaluate the, the actual value. What happens in the background is that we're Getting uh a a a compute graph built for the amount of, uh, sorry, for the set of uh of, of things that we need to run on the device in order to build that calculation. Now, one thing to remember here is that sometimes we would also want to compile. So, uh, one thing that we can do, we can change multiple uh multiple nodes on the, on the compute graph into a single compute. Compute graph object a node, because for example if we have a function that we're using all the time and it will run exactly the same when it gets an input and provides an output, then we can compile it and then we'll have less things in the compute graph which will also reduce the swaps in. In the actions, so it can actually benefit even more. So if you look at this example, we have again two arrays and we can combine them, and once we do that, nothing will happen. We'll have that compute graph behind the scenes, and once we force an eval, then we will actually do an evaluation. Again, same will happen if we print it out. It's a type type conversion to a string, so we will need to know the value of it. Same goes with the list because we're trying to convert it to a list. So if we run this, we can see exactly the same things, uh, you just evaluated them once uh once we wanted it. Now, uh, we discussed a bit about uh the performance for Mac. So why is it that important to use the right tools for our hardware? Uh, at the end of the day, if we're looking at it, that CPU bus and, and the world of working with different memory areas is very complex in most use cases. Sometimes we have, well, I can see customers doing things like computing something on a GPU, but, but then batching that on a CPU. So you have an umpire in the middle, you have a ton of memory bandwidth in and out from the GPU in order to just run that specific batch. And in that case. This could have probably run faster on a single device if they would have moved everything to the GPU. That's a great thing, but we're even seeing it today in things like VLLM taking a mixture of experts and trying to provide the best way to do so on multiple GPUs in a large model. In in that case, uh, you'll have RDMA in, in the middle, and sometimes that RDMA library uses CPU. So we're getting into the right, the, the same bottlenecks all, all the time and it really matters what we use and when. So if uh we're taking a bit of a performance comparison uh on a larger scale. So in, in this case we have again, a random NumPy array and a random uh MLX array. And we'll, we will do a matrix multiplication on both, and we wanna see how much time it takes. Now you'll be able to see that sometimes NumPy is faster on smaller stuff, and that's because, well, again, for comparison, we're using CPU on both. Sometimes I can decide, all right, I wanna do it on a GPU again, not as fast, but Looks, looks similar. Sometimes the uh the MLX output will be faster, but once we get into larger sized array, now we will be able to actually feel the benefit of it. Sorry. So, as you can see, uh, once we're doing it on, on MLX, then it's much faster. But here we did it on a GPU, so let's do it on a CPU. And we'll see that again MLs will be faster, and that's probably because of the the compute graph, some optimization that can do alongside it, and the fact that these functions are are built as as best as possible to run on Macs. So that's a small comparison on, on a 6000 by 6000 dimension matrix multiplication. So, another cool thing about MLX is function transformers. So the idea of function transformations is actually taking a function as an input and returning a new function as an output. So MLX has two types of function transformations. We have automatic differentiation and we have graph optimization. Like I've mentioned before that we can compile multiple actions into a single compute node graph. So, automatic differentiation will be functions like Amex.grad, like getting the gradient descent of a function, which I will showcase in a, in a second. And we can pretty much compute, uh automatically compute the gradient of any function with it. And we can do uh like a second derivative of that as well, as I'll show in a second. And the second type of of uh function transformation will be operation that optimize the computer graph, like we've discussed the compile function. So if we look at automatic differentiation, which is more interesting in that, in that case, we have a very simple function here and we have a simple input and we want to just have, have our gradient of that function. So we can Say, alright, let's have a a compute function here and that we are sending a value to it, and then we can take that function and do MX.grad on it and it'll just give us, return as a function which is the graded function of our function. So we can see that we can just run it and uh we'll have it uh output as, as of that. Uh, and we can see the expected gradient of it. And again, we can do the same thing with like a second grade. So in that case, we'll take a sin and we'll do MX.sin, a very simple function, and we'll take again, uh, we'll throw that in a gradient and we'll take the gradient of the gradient. So we have a second derivative of sin, and in that case we can just run our array through it. In that case, we brought a single object array and we can have the 2nd derivative of sin and just have our output ready. Now, again, as, as I've mentioned, uh, we can decide uh which type of object, uh which type of device we want to run each calculation. So each calculation that we do, we can decide which, which we want to run it. So for example, that sin. We can take that scene and again take that input of it and we can use the stream in order to decide on which we would like it to, to, to run on. So in that case you can see that we ran it on a CPU. So I have a CPU and a GPU, but let's run it on. The GP will be able to see the same outputs, but it just ran on the. So, that's that for a small intro to MLX. As for some, some basic things on top of that, so we have an array framework. Now we wanna have some real tools in order to build linear layers and to build actual neural nets. So Apple has built MLX.NN. So within MLX we have the neural net class, and that neural net class will contain a lot of things that we're expecting from, like we've seen before, uh, in the comparison to Pytorch, which I'll also showcase in a bit. Uh, and it will pretty much uh contain the, the tools that we need to build, uh, uh, neural networks on top of Macs. So again, let's just import everything and, uh, in this example, we can see basic neural nets. So, for example, we want a linear layer of a size, then we're taking neural net.linear, as you can see, we brought MLX.NN this way, that's why we're, we're missing MX.something. Um Uh, for example, in that case, we can see we have the shape and bias of it. If we want a convolutional layer, uh, we can do that as well. We can decide how much input channels and output channels that layer will have, and we can set the kernel size of it and so on. And again, we can easily output it, and I'll show you in a second. And on top of that, we have built in uh normalization layers. So for example, uh, batch norm and so on will be uh a part of it, so we can just get, get a layer uh using that. And uh moving forward, something that's uh very related to the, the world of, of uh of large language models is activation functions that we see throughout the layers. Uh, and in, in that case, we again, we have activations pre-built, and those activations again are optimized to run on, on Mac, so they use the underlying metal, uh, derivatives in order to build it as, as optimized as possible. So instead of building something like a sigmoid or a gel, you can just use it from, uh, from the library. Uh, again, if we, we'll continue, uh, I'll just run this for a second. So you'll be able to see we have our layers, we're able to watch, uh, the shapes of, of set layers and find the biases for it. And again, uh, we can see the activation functions and so on. And this is specifically a value effect that we've edited at the end. So again, if we're comparing it to how we would have built it with Pytorch, for example, so in Pytorch, if we're looking at at at differences between the two, we generally have the uh the call function and the way that we're actually calling. The activation function. In in MLX we're using uh the class and we're sending the object to it, and on Pytorch and and others will usually just use the object and, and it'll have an activation function on top of that. So, if we're looking a bit further on, now, we wanna do uh multiple layers and we wanna actually build real networks. So we have two examples of complex models here. Today it's not that complex, but more than a linear layer. So, uh, the first is a multi-layer perception. Uh, in that case, we can see that we have our layers, so When we in it, that's not mine. Uh, so, when, when we in it, uh, then we can see that uh we're just creating our objects, and on top of that, we'll uh add our layers. So in, in that case, we can create a, a, a neuro layer, an activation function and, and another dropout, and uh we can uh just Say, all right, uh, the way we'll call the layers is we'll go for each layer, we'll pass the the data through it, and then later on, once we'll try to use it, we'll do MLP and we'll send it to it. It'll just go through the call. So, if we'll look at a simple convolutional network, again we'll have the convolutional layers. So we're using nn.com2D in order to receive. Uh, the actual layers from it to build the layer in it. And we can see that those layers are expecting to be one after the other because we can see that the output channel of one will be the input channel of the other, and we'll also build a classifier. In, in that classifier, we can again take sequential as an object from our neural network library. And we can say, all right, we have a sequential uh uh object that's running through all of these layers, and we'll have our first linear layer, um, an activation function, a dropout, and another linear layer. And then when we look at, at our call function, we can again run through, uh, use the Amex and, and it's uh internals, and we can run through our various, uh, various layers and return our object. So if we're looking at at those, so you can see in the MLP we'll we'll have those layers, and in the simple CNN we'll have our convolutional layers and our classifier, and we can do a forward pass on those and see what happens. So, alongside it, MLX also has optimizers. So, sometimes we would want to do something like an RMS prop or other types of optimilizers. Again, instead of building them on your own, what you can do, you can just use uh what's built within the optimizers, within MLX. If, if you look up here, sorry. It's not refreshing. All right. Never mind. The, the import was uh import MLX. optimizers as uh as optimm. Sorry, uh, I wanted to show that to you. Uh, so again, as you can see, we can go through optimizers, we can take those optimizers for us and uh we can actually uh use them instead of building them on our own. As you can see, We'll have those different optimizers from MLX. optimizers and uh we'll have uh some examples of running through them. So if we look at like a complete training example with MLX, then a compu a complete training will look something like this. So we'll have like a, a classification, uh uh um. Sorry. Let's generate some synthetic data. Uh, that's, that's data classified for various types of use cases, and then we'll create a data set from it. We'll take a train training data set and an evaluation data set, and we'll convert them to uh MLX arrays. So we'll have our train and uh and uh validation arrays, and then we'll be able to take our, our model and Also use optimizers and build our loss function and accuracy function and actually train it. So if we're looking at the training loop, what we'll do, we'll have multiple epochs and we'll generally just pass through the loss and grad function and train our our model. All right. So, uh, that's pretty much neural networks on, on MLX. Uh, you can do it in, in, in one of many ways. There are plenty of uh implementations within it. Uh, it's just a matter of what you're looking to build and, and how, how to build it. So, fastly moving on to MLX LM. So Apple understood that uh in, in order to actually build large language models or large models, uh, there are things that needs to be wrapped around the neural networks and things that need, needs to add on. So, in, in this case, Apple released another, uh, another GitHub repo called MLXLM. And MLXLM allows you to have that wraparound for the large models part. So, if we look at it, we have a few interesting stuff. We still need MLX. core, but we also have from MLX LM we have load and generate in order to load large models and generate them. There will be also implementations there on how to actually inference various types of models, because again, Different types of models have different types of infrastructure and, and, and layers and ways of, of being built and ways of being actually inferenced, and the inference engine will need to know some of those in order to actually be able to run through all of the layers that we are expecting in order to get our output. So, that's that. Later on, we'll, we'll talk about uh uh prompt cash a bit. It's sort of a KV cash for prompts, but yet again, it's not a real KV cash like we're expecting on other, uh, other places that we're saying KV cash. Uh, it's just a way to, uh, to, uh, uh, cash the K and Vs of the prompts that we're heavily using. So that's that. And it's also built from within MLXLM. And we also have a convert function. We'll talk about the convert function in a bit, and it's very important because quantization is a, is a very uh important tool for a lot of inference techniques. Uh, we'll talk about it in a bit. So yet again, we're bringing on some prerequisites, uh, and we'll continue down. So, if we look at the model candidates. Uh, we're taking a few different models here. Uh, at, at the, at the base, it's a single model, but various types of sizes of it. And we can see that uh we have a 4 bit quantized model, 8 bit quantized model. We have BF 16 quantized as well, and we have the full size of that model. Yet again, it's quite a small model. It's 3 billion parameters, which is yet again. Uh, let's call it big in terms of, uh, uh, neural networks. So in this case, you can see that we're creating a model antagonizer that we'll use in a bit. Uh, we'll just, uh, choose a single model from uh, from our examples. In that case, I wanna take the BF 16 1. Uh, so we'll be able to change as well, sorry, down the road, um. And what we'll do, we'll use the load function with the model name, and we will get a model object and a tokenizer. Now this model name comes from an integration between MLX and hugging face. So if that model is on a hugging face, then you can just use that with that name. That's why we also have the MLX community, which is a part of the model name. And yet again, we'll, we'll see down uh in a bit when we quantize the model, we can select again the name of it and upload to uh uh upload to a hugging face yet again. So in in this use case, we'll just load a model and we'll see how much time it takes. So I already have that model uh weights on, on this specific Mac that we're using, so that's why we didn't download anything. So it was quite fast, but we can see that we have a low time here and we received our model and tokenizer, and we've ran an encode and decode through that tokenizer and we've actually received. You know, well, we, we received the actual post tokenization output that we have, uh, and as you can see, the tokenizer edit the beginning of text because that's a part of that prompt. Using the model object from within MLXLM, we can also look at the model parameters. So in case of maybe we're fine tuning a lower layer or doing various things and we want to understand if a parameter changed or something, we can use that in order to to to scroll through that. And we can also see the model layers. And that's, that's very, very interesting because sometimes later on when we quantize you'll see that maybe we don't want to quantize the entire model. Maybe we want to try and quantize a specific layer. Sometimes we're even fine tuning just a specific layer. And it really depends on what we're trying to do and how we're trying to modify a specific model into our use case, uh, but being able to understand what is where and and which type of uh of layers we have really helps us understand what uh what we're working with. So that's that. Uh, I, I wanted to move on to like an interactive chat with, uh, with Lama. So in this case, we have, uh, LAMA 3B, uh, and yet again, we're taking the BF 16 model. As you can see, we have just A list of prompts and, and uh this code will generally just show us how to uh how to actually run and, and use the the object. What we'll do, we'll just take the first prompt, inference it, take the next prompt, inference it, take the next prompt, inference set. And we'll measure the uh the memory usage that we used, so we can use Mex.get active memory to see how, how much actual data we're using. How much memory, and we will also use our, our time in order to uh to calculate how much time it took to generate. So If we look at that for a second, let's let it start talking. So, it introduces itself and it's not that important as for that part, but we can see we're getting pretty nice tokens per second and it was quite fast. It used only 6 gigs and continuing down the road, we're we're seeing pretty much the same. Now, we're, we, what we can do, we can choose a larger model and we can use that one. So let's try to use our largest model here. I've brought uh uh LA 370 billion parameters, but it's again 4 bit quantized. So let's take that. It'll take a second to load. We'll see the tokenizer providing us the output from here. Again, sorry. Size or or bandwidth. So, this specific one is an M1 Ultra. It contains 128 gigabytes of RAM. Uh, the thing is that it's important to know that you can use only 75% of that RAM, uh, for a specific device. So, for example, the GPU will be able to, to get 75% of that 128. So you can actually load quite, quite big models on top of it. Uh, let's see why it does not want to work. Everything breaks, you know. Um, All right, maybe I'll just Uh, let's just look back to our Full-size model here. I think maybe the colonel stuck. Just try it out again, sorry. See if it works. Alright. So, uh, yet again we're taking the larger one, it's not the 70B, and I'm not sure why it didn't work, uh, but it's the 3B, uh, but it's uh, it's the full size of it. So as for that, yet again, let's try and run our interactive chat through it and we, we should be able to see a bit of a difference in the, in the output, so um. Not, not the output values, but the outputs uh as uh, as of the tokens per second, quite similar and so on. All right. So let's talk about quantization. So, uh, for those who don't know quantization, quantization is the idea of using smaller sizes of, uh, of data types in order to reduce the amount of calculation and the amount of memory that we actually need. So, uh, there are many ways to do quantization. Uh, the best way to do a quantization will be to take the training input. And actually quantize that, remove the, the extra dots in the data types, and then uh take that and, and do a real training on it. Do a full-on training uh in order to actually uh uh calculate the weights between and, and understand what, what the uh the actual activations should be. Now, other things that we can do, we can just take the uh the the uh model parameters as we have them, and we can just reduce those and uh we can do a quantization by that and just use that with the lower size. Although there will be probably a bit of difference in, in the uh In the accuracy of what we would have expected, but it, it's still uh a good enough example for most use cases and a good enough uh implementation of quantization. So if we look here, uh, generally what what we're, what this code will do, it'll just uh look at, at the specific model that it received. It checks that uh that the sizes of it are what we're expecting, and this is a very nice uh table that uh I had a hard time making sure that it, it, it's printed out properly. Uh, but the, the interesting thing is to remember that uh we have different quantization formats and we have various types of uh sizes of data types. They will change things. They will change the quality, they'll change the speed, and the use case to use them will be probably a bit different. Because sometimes we would wanna have a slower speed, but we wanna have a higher accuracy. And sometimes we can uh maybe decide on, on changing some of it. Maybe we, we can uh use a specific layer and, and have that specific layer less, uh, uh, let's call it less accurate, but faster and it will still provide us the results that we would like, and it's really important to, to, to look for those kind of things. So in this case again we can see that we have like our our large model, it weighs about 6 gigs and we can see that if it had various types of uh of uh of sizes, it probably would have been better for if it's research, development, and so on or running on on an edge device. And I, I would want to talk about running on Edge device in a bit because there is a really cool example of what, what can be done. So if we continue down, We talked about the MLX LM's convert function. So MLXLM provides us a convert function that actually wraps a convert function within MLX. So we already have quantization logics within uh within MLX itself, within MLX core. But for large language models, there are a bit small, small taints that we need, uh, traits and taints that we need to know. And in that case, we have, uh, our convert. And as you can see, we have a hugging face path of, of the model that we want and we can say, all right, quantize it, uh, decide a new path for our new MLX converted model, and we can have uh the amount of bits that we would want. So we, we, we're deciding, we're deciding the data size that we want to have on our quantized model. So if I run that, this will actually run uh at this moment, and we can see that uh inside this library that I've created my quantized models, uh we'll, we'll, we'll be able to see our model. So this is our actually our quantized model here. And if we open up the uh the config Jason, I, sorry, I'll open up this text. So if we look at it, we can see we have a quantization and a quantization config, and we actually quantized our, our, uh, model to 8 bits. We can do things that are more complex as I mentioned. So, we, instead of doing that, we can say, all right, uh, I, I, I don't necessarily want every layer to be uh quantized to 8 bits. Let's use a quant predicate and let's use a function that will get the layer and we'll decide by the layer if it's the name of it or the layer itself in, in this case. By the layer, we will decide how we would like to quantize it. In this case, we can see that we built a quantized per layer, a quant predicate function. In that case, we took LM head and we said, all right, let's quantize the LM head to 6 bits and all the others, let's quantize them to 4 bits. So if we will do that, Again, it takes a few seconds because we don't have like a post training here or something. It's just a matter of uh of reducing the sizes. So if we'll get back into that, we can see we have another directory here, our variable quantized uh uh model. And if we will open its configson, sorry again. We can see that now in the quantization we have an object representing each layer and each layer's actual group size and and object size. So we can see the bits for each layer and the group size of it. So in that case we can see again our LM had remained 6 bits and the rest is 4 bits quantized. All of the layers So that's for that. Now, Just switching over to a browser here. Sorry, just a second. So again, talking about Pytorch for a second. So, uh, all of what we've just seen is optimizations that are are able to be provided to us because we're using MLX. MLX will always get the latest from Apple, and Apple is the one maintaining it. So when there's something new, we'll probably have an MLX update that will allow us to use that new, if it's a core or if it's a uh a trick of some, of some sort. If we're using Pytorch and we want to use PyTorch optimized on Mac, then the way to do it is to use an MPS backend. So Apple released a backend for Pytorch, allowing that. It's quite similar to what we just released on Traium on our side of things. So on Traium 3, right after the launch, we also launched that. Uh, released native support for, uh, uh, for Pytorch on, on ranium and infer chips. So it's pretty much the same. It's building out the lower level implementation behind that specific framework to allow it to actually use the various types of devices. So if we're talking about the lowest level, the metal, uh, the metal uh libraries from Apple, someone needs to build that that implementation. So the best thing you can, you can get on Macs if you are still wanting to use PyTorch and not moving to the MLX very similar implementation of PyTorch, you can use, uh, you can use that backend again. If anything changes, uh, We will be reliant on Apple releasing a new backend for it or someone else fixing a backend for it, and, and that's a downside. And we have pretty much the same thing for for Tensorflow as well. So in Tensorflow, uh, it's a pluggable device and you're, you're just defining that device and and adding to it. Again, Apple provides all of the examples and how that looks, and that's pretty much it. Now going forward, a few cool things that Apple also included in MLX. So in MLX Apple has included the fast library. Again, thinking about all the things that we're capable of doing. If we have a, a pre a pre-set things that we know that happen a lot, we can optimize them. So Apple did it for us for a few things. An an example will be RMS norm or any other thing that you're seeing here, but an RMS norm is something that happens a lot in uh, in, in large language models and in models uh as a whole, and instead of us Building our own RMS norms and causing an array framework to actually have multiple calls onto the SOC to provide the instruction sets of do this now and this afterwards and this afterwards. Apple just pre-compiled the best implementation that it could have created for us to use so we can use the fast library from MLX core and we have things there. Again, we can also see those for our transformations as we've discussed as well, and so on. The last thing I wanted to talk about is the opportunity here. So if we're talking about opportunities, Sorry. If we're talking about, about the opportunities here, because we're seeing those SOCs uh coming to the market, uh, people have iPads and, and so on. There are more and more things that will probably We will want to use the same core for. So Apple also released lately uh fast VLM. So that's a single example uh from Apple on how to build a model that's very optimized for using it. And they are actually using Pytorch as part of it as well. I have to be, to be frank here. Uh, but the idea here is that at the end of the day you're capable of, of getting more from that specific device. If we look at various types of workloads, uh, sometimes we have workloads that require to move through multiple different layers or multiple different neural nets that will need to have them hot in, in memory in order to not load them to the memory, and, and we don't necessarily need too many cores in order to to inference through them. A lot of use cases like that can use can use these types of hardwares. And down the road I really hope seeing more and more we're seeing AMD now also building rising AI chips that It's just a matter of, of them building the same support like MLX in order to uh provide us more and more tools to use it and uh continuing forward, I really believe in, in the direction of SOCs that provide more than a single device. Uh, I personally, and I really hope seeing more and more, and I, I'm, I'm really looking forward to seeing what, uh, what our customers will be able to do with, uh, these types of, uh, of interesting course. That's pretty much it. So if you have to summarize what did we learn first, you have really a lot of computing power on your desk when you have a MacMini with an Apple silicon chip. These chips are containing the CPU, the neural engine, and other types of engine with a unified memory that makes them extremely power efficient and fast to process your machine learning workload. Second. You have the Mix framework that allows you to create your model, to train your model, to customize your model, or MLM that allows you to run existing models from hugging face directly so you can download the model and run them directly. On your Mac and third, if you are still in the Pytorch world, which makes a lot of sense, you can also use Pytorch directly on the neural engine with the back end that Eliran just showed you. So take your existing code optimized for neuralin thanks to the Pytorch. Wow, that was dense. We still have a couple of minutes, so we might open the floor for questions. So thank you so much for um your, your interest and all the questions. Um thank you Eliran for doing the hard work there. Thank you very much guys. uh we're really looking forward to seeing what you'll build. Have a good one.