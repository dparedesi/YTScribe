---
video_id: G9KOtz_GkoI
video_url: https://www.youtube.com/watch?v=G9KOtz_GkoI
is_generated: False
is_translatable: True
summary: "Brian Emerson, Chief Product Officer at New Relic, leads this session on the future of \"Intelligent Observability,\" joined by panelists Carrie Hattie (VP of Engineering at Ancestry) and Steve Evans (formerly of Chegg). The discussion centers on the industry's shift from passive dashboarding—simply displaying metrics like CPU and memory—to active, AI-driven intelligence where observability platforms autonomously detect anomalies, diagnose root causes, and even take remediation actions on behalf of engineers. Emerson unveils New Relic's vision of becoming a \"system of intelligence,\" demonstrating a future state where an AI agent not only resolves a 2 AM outage by auto-scaling clusters and rolling back updates but also proactively optimizes code and updates runbooks to prevent recurrence.

The panel delves into the practical realities of this transition, emphasizing that in a microservices and AI-driven world, infrastructure health is secondary to **Business Observability**. Evans argues that traditional metrics (e.g., \"server CPU is 87%\") are becoming irrelevant compared to user experience indicators (\"can the user check out?\"). This is especially true with non-deterministic AI components, where a system might be \"green\" technically but failing to deliver value due to poor prompt responses. The group highlights a critical gap in the current landscape: while SREs communicate in error rates, business leaders speak in revenue and conversion, and the future of observability relies on merging these lexicons through tools like the newly announced integration with **Amazon Q Business Index**.

A significant portion of the conversation focuses on the \"untapped audience\" for observability: the developer. The panelists agree that observability is often treated as an afterthought or a tool solely for incident response. Instead, it should be shifted left into the development cycle, helping engineers catch inefficient SQL queries or performance bottlenecks before code ever reaches production. They share \"hard truths\" about current practices, noting that most organizations suffer from alert fatigue—having too many noisy alerts that add no value—and fail to conduct rigorous retrospectives to prune them. Hattie advises to \"get your data right now\" to prepare for the AI Ops revolution, warning that AI cannot fix fundamental data incongruencies or missing telemetry.

The session concludes with New Relic announcing deeper two-way integrations with AWS, including **Agentic solutions** where AWS DevOps agents can query New Relic for context, and improved security vulnerability analysis. The overarching advice for navigating the future of observability is to prioritize human relationships and alignment with business goals over tooling perfection, ensuring that the metrics being tracked actually correlate to the success of the enterprise."
keywords:
  - Intelligent Observability
  - New Relic
  - Business Observability
  - AI Ops
  - Agentic AI
  - Amazon Q Business Index
  - Incident Response
  - Developer Experience
  - Alert Fatigue
  - User Experience Monitoring
---

All right, thank you 1st, 1st off for, uh, for joining us today. How many people got 10,000 steps walking over to this? Just to this meeting, all right, yeah, it's pretty incredible. So, uh, having a, a 16 year old boy and an 18-year-old boy, it looks like I'm watching or waiting for a bunch of people to start playing Fortnite. This is kind of interesting. This is my first time presenting to a bunch of, uh, DJs in the audience, um, but my name is Brian Emerson, chief product officer at New Relic, uh, and, and I'm excited for this session. So there's a couple of things we're gonna do. Number one, I'm gonna spend a little bit of time just talking about New Relic, some of the things that we're focused on, how we're thinking about the future of observability from our perspective, and what does that mean from an investment, uh, investment standpoint? Like where are we investing our R&D dollars? How are we thinking about the future? And I'm gonna bring up a couple of panelists, uh, customers of New Relic to talk about their own perspective, and part of this is just to kind of riff on the what is the future of observability look like, what are some of the things that we need to think about, what are gonna be some of the roadblocks in allowing us to achieve more intelligence in our observability, uh, environments. But before I get started, I just wanna get a little bit of a landscape of the room. So if you're a developer, raise your hand. OK, what about like platform engineering SRE? Half and half. Any like product managers, business owners, a few product managers in IT. I'll throw IT out there. One person from IT. OK, well, welcome. Um, alright, so we're gonna jump into it first off, just a little bit about New Relic. I know some of you know New Relic well. Lots of big numbers up here. I think the thing that we're most proud of as a company is we have been rated a leader in the observability space for quite a long time, right? We got our roots in APM. We've obviously expanded into. Full observability stack and a lot of where we're focused our energy now is how do we derive based on the data sets we have in our platform, how do we derive better insights that help you more effectively run your applications, run your platforms, and support the business. 85,000 customers. We added 4000 new customers over this last year. And the the thing I think that's just so interesting to me is the is the is the chief product officer is the folks that I get to engage with and also the brands that are relying on our platform and the resilience our platform provides to the services that they're providing to their end customers so you can see a few of them here, um, I just wanted to expand it out a little bit, right? So we're really proud of the fact that we have some of these brands that are using our platform on a daily basis to to support their their mission critical applications and services. We just recently ran a state of observability study with almost 2000 participants, some new Relic customers, some not new Relic customers, and I just wanted to put, put it up here. I'm not gonna go through all of these key points, but the number one thing that still kind of hampers us in this entire space is the cost of outages, right? I think, as all of you know, you're probably running and supporting infrastructure that's, that's revenue generating, that's supporting revenue generating applications every time those applications go down. You're, you're having to manage this. A good observability strategy can cut that loss in half, right? So if you're spending $2 million every time per hour, every time you have an outage, we can cut that in half by having a good observability strategy, but it also has to be the right one. The second thing is obviously the AI driver. You've probably been around this conference. Has, has anyone been in a session where somebody didn't talk about AI? I really want to know what that session is after the, I don't know how to communicate with you, but I need to know what that session is. Um, AI is the number one driver. Look, the way I, I kind of look at AI is. At the core it's another element in the stack, right? So you know I, I was around the days of like VMs all of a sudden became a thing and you're like, hey, there's a new management layer and something we have to manage. Containers came out, same thing. At the core there's some element of that from an observability standpoint, right? New elements in our applications that we need to make sure are reliable and available. The one really interesting thing that we're gonna get, get into as we have our panel discussion is there's also this nondeterministic element that AI provides inside of the application and like what is our role and observability of helping there and it's kind of like this merging of what business requirements and what the business cares about with what. Uh, with platform engineering and and developers need to care about, uh, in in in supporting the things that the business needs. All right, I'm also, uh, just gonna do a quick plug. These are the capabilities that we announced this week in terms of integrations with Amazon. Uh, the first one I wanna highlight is really the agentic solutions. And so think about this as two-way integrations. Uh, a couple of capabilities from Amazon being able to call out to New Relic and ask questions. Hey, there's an issue going on in the environment. Tell me what's going on, right? Like go and talk to New Relic and come back with an answer. Right, so that's, that's a one way integration that we have with AWS DevOps agent and also Amazon QuickSuite. And then on the flip side we're also using from a new Relic AI perspective we're tying into Amazon Q Business Index, and that allows us to pull a lot of business context into new relics. So when we're looking at things like how do you correlate observability metrics to business outcomes, we can start pulling in some of the business insights that exist there. Directly into the new Relic platform and be able to provide visibility and insights um straight from RUI. The second area is around auto discovery, making it very easy to be able to onboard your cloud environments into new Relic. And then the last area that we, uh we highlighted is around security, right? Helping you understand from a developer perspective when you have vulnerabilities and what's the, the best automated path to be able to resolve those. And so I'm just gonna leave it at that. I would say if you have more interest in these integrations in particular, please stop by our booth. Um, I know that the venue is very large, but if you have any questions on where our booth is, uh, after the session, feel free to come up to me or us, uh, one of us can help answer that for you. All right, the vision. So the vision statement for us is system of intelligence for the AI age, right? We're an observability platform, but a lot of where observability started was the simplest way I think about it is like dashboards and then there's kind of intelligence, right? And, and the start of observability was really around dashboards and. We're looking at a bunch of metrics and we're kind of surfacing those up. I think the next generation is all about what are the insights that we can actually derive from those metrics so you're not having to go and fish for answers. The system is literally providing answers for you and getting to a world where it's acting on your behalf, right? And I wanna show you, I think the best way to paint kind of a picture of what this looks like is to actually walk through and show you guys a. A quick vision demo. Of the way that we see the world shaping up. All right, here we go. Good morning, Alex. What could have been a long night wasn't because I responded to an emerging pattern in your system that would have eaten into your business. Between 2:03 and 2:28 a.m. checkout latency surged, and while your other services are fine, I checked, customers began abandoning their cards. By increasing trace sampling, I identified the root cause, a delay in the fraud check microservice triggered by a recent database update. To stabilize performance, I auto-scaled your performance cluster and brought it back down wood load reduced, so your cloud cost spend is still in line with your budget and creating Jira tickets for a rollback and patch development. With system health and e-commerce backed up, I ran code analysis and made configuration updates that cut the microservice's query time in half, saving you over 100 hours of engineering. To keep your system in an ideal state, I updated runbooks, dialed in alert thresholds, and built game day simulations before updating my AI models and knowledge graphs. To prevent future issues, I ran simulations across your various usage scenarios to identify likely points of failure or degradation. The findings are in the performance inbox with code change suggestions to resolve them. And to push back on risk, I reviewed active vulnerabilities and found 3 at-risk hosts in production that I have flagged for remediation. The owning teams have Jira tickets with remediation steps waiting. I also analyzed network traffic over the last 24 hours and found no anomalous activity that would indicate an attack. From finding a root cause to engineering excellence to controlling costs, I'm here to keep your business moving in the right direction. So, where now? All right. And so, in the next like 20 minutes or so, we're gonna kind of riff on this idea of the future of observability. A lot of things that we just, that you just saw a video of how do we make these things come to life and what are some of the things we need to think about on our. Our journey. So give me one more second. I'm gonna do a quick. Swap back to slides. All right, and I'm welcoming up Carrie and Steve. Come on up and join me. Everyone a little bit of a hand, but. By the way, if you're ever, if you're ever in the position of presenting and listening to like watching a video and you can't actually hear what everyone else is hearing, it's a very strange feeling like sitting there waiting, like it's anticipation. Thanks guys. Hey, hey, good to see you, good to see you again. I just saw you like 5 minutes ago. Yes, you saw me 5 minutes ago and probably like an hour before that. Welcome. Thanks guys for, for joining us. Um, let's just get started, I think. Let's go with some introductions. So just tell us a little bit about yourself, the role you play, the company you work for, maybe a little bit about what the company does if it's not, if it's not obvious, and we'll go from there, OK. Uh, yeah, Carrie Hattie, uh, VPE at, uh, Ancestry.com is. You probably imagine it's a largely family history business and then also uh the largest consumer uh genomics company in the world and so that's our two prong focus um been there for 21 years, uh, so forever, um, and um. That's me. Cool. I'm Steve Evans. I was formerly with Chug. It's an ed tech that helps college students. Uh, I wrapped my time up with him about a month ago and uh I'm thinking about what I'm gonna do next. Cool. Well, welcome. Thanks for joining the conversation. All right, so one of, one of the things that I kind of hinted, hinted on as we got started here is this notion of intelligence, right? And I wanted to just dive into that as a starting point and whether you're thinking about it in terms of, you know, like AI or just intelligence in general, when you think about your observability mission, the things that you're focused on from an observability standpoint, how do you think about the word intelligence and what it means to getting the job done? Maybe I'll, I'll start with, I'll start with Steve. Um, so when, when I think about observability and I think about, you know, intelligence and where we're, we're heading towards. You know, if we rewind the clock 10 years, we spent a lot of time thinking about infrastructure. What was the CPU on this server? Is this server healthy? And then microservices came along, ephemeral compute came along, a lot of the stuff that, you know, AWS and other cloud compute platforms enabled for us and honestly I don't really care about infrastructure anymore. Infrastructure is just there to serve a purpose, right? Um, especially in a microservices world if infrastructure is unhealthy, honestly it's AWS's job to deal with that. Uh, if the application is healthy, I don't care about the infrastructure. I, I, I, I see the future of, of, you know, development in general, tech in general, but especially around observability continuing to move up that stack. Um, I, I often talk about, do I care what the error rate of a microservice is if the user experiences is happy. Like if users can log in, if users can check out, if users can have the experience they're supposed to have, is the underlying application health truly important now obviously it is because that's what drives that user experience, but with the complexity growing, um, continue, continued complexity growing. How we think about observability in that intelligence light really needs to move up that stack and then we let platforms or observability platforms really deal with how should we be thinking about the, the health of the applications, how's that driving the, the usability, the user experience piece of that, you know, I, I think, you know, continuing to think about like, oh, the error rate of this service being above 3.2% is a problem. As humans we're just not going to be able to keep up with that for much longer. We really have to continue to move up that that stack on the intelligence piece. I think that the interesting thing about that too is also the alignment that you then need to have in your role of observability with like what the business cares about, right? When you think about digital experience like are the goals of. The product manager, the business around that service, are they aligned with the way that you're thinking about like the things you're measuring and observability, and I think often times there's just the gap, right? Like the observability is like, hey, the application is healthy, OK, especially when AI is introduced is there's this whole notion of but. You don't know if the experience is good, right? Like bad returns from prompts could drive a horrible user experience even though the system is saying green. Absolutely. And if you rewind the clock 10 years, you used to be able to have the infrastructure team say, Oh, CPU is at 87%. That's a problem. Let's go fix it, and ignore the development team. We're really not in a world where infrastructure teams and development teams can't talk together anymore. We're moving towards a world where tech teams and business teams have to continue to talk more and more together. Like all these worlds, AI is driving a world where roles are merging together more and more. Uh, the Venn diagrams are overlapping. We're gonna have to see this more and more in how we think about observing not just the tech stack, but also the user experience, the business world. All of this is to me is, is coming together. Cool. I love it, Carrie, and, uh, yeah, when I think of observability, you know, I think it's a, it's a tool that we use. The primary purpose is the revenue, right, of, of your company and, and as tech people here. Uh, this is one part of how we get there and as most companies are, they, they only care about observability, uh, when it goes wrong. The customers are impacted, the site's down, then suddenly they care and so. You know, like, like you, you know, uh, we, we, uh, lifted and shifted, uh, you know, many years ago to the cloud, and it's, it's, it's a big journey, observability. It's not something you just, uh, it's ever done, uh, and so if, if you think, oh, we're not quite there yet, you know, I've been, you know, doing, I think we've, uh, moved 8 years ago and we're still doing it. It's, it's always a work in progress, um. You know, at one point we used to have a big problem with, uh, you know, kind of sitewide site-wide problems where it impacted, you know, thousands and hundreds of thousands of customers, and now we've kind of gotten past that and now it's, now it's down to the, you know, individual errors. How can we get rid of those from a very small subset of people and. And so I, you know, I, I, and I imagine that's how you should look at it. It's really this journey that you're on and don't, uh, don't ever beat yourself too much up because, you know, we're all, we're all figuring it out under, uh, a shifting, a shifting, uh, platform underneath us and, uh. So that's how I, I look at it. It's, it's a tool uh for revenue and it has to be there. So yeah, I mean, and, and part of the way we're thinking about it at New Relic is how do we surface up some of the business, call it the business KPIs or metrics that product managers in the business care about inside of New Relic, right, and that helps, I think, create tighter alignment between the goals that you have in terms of managing your observability stack with the, the way the business actually thinks about success and whether that's revenue success like user experience like, like you mentioned. Um, or other things that it's actually married together to, to, to me I think one of the most important trends, especially with the introduction of AI and kind of the blurring of lines that you talked about is the communication between. Everybody sitting in the audience right now and your business counterpartparts is going to be more and more important. Are you actually aligned and calibrated against the same goals? And then how do you actually measure that from a tooling perspective? And I think oftentimes like there's bifurcation in the tool sets that both sides use. I think we're gonna see over time a merging of those together and we're, we're trying to look at opportunities for us to represent that in a unique way inside of the new relic platform. Yeah, and I see, yeah, I see the same thing at, uh, Ancestry, you know, we just, uh, we just built an internal, uh, we call it user, uh, user business events, and it's, uh, run out of Redshift because, uh, you know, we have to, we have to store it separately then. Uh, than traditional observability, uh, metrics around, uh, CPU and memory, uh, you know, all those types of observability, uh, metrics that we track, I think that the future is, is, is that you would have, uh, business observability and then you'd have, uh, more infrastructure observability and, and I'm hoping that, uh, yeah, somehow there could, uh, we could expand the term, uh, observability out there because I have to do it myself now. Yeah, yeah, it's kind of like a stitched together project, right? Like you want it to be, you want it to be out of the box and curated. That's right. And if you were to ask, uh, um, the, the board, they think observability as a business observability, like, uh, you know how performance to them is. Business performance, it's not how fast the site loads and so, uh, you know, typically us in this room we use all these, uh, terms that the, the business owners use differently and we're not, we're not doing enough to observe that part of, uh, of, of the business. OK, cool. So let's, so let's let's switch, uh, topics a little bit and just talk about the data that exists inside of the observability stack like who in the organization needs to have access to it or use it. And is it expanding to different personas in your organization? We kind of mentioned the business, mentioned product managers, you know, like, obviously like platform engineering is like the key user of it. But do, do you see as part of the future that the observability stack starts moving out and having broader scope in terms of who cares about the data that new, like the new relic platform is providing to you? Who here is a developer? Lots of them. Half of them. Do you care about observability data? No. Wait, like half the room raised their hand and then no one raised their hand when I said do you care about observability data. I mean you're sitting here so I assume you do so you said platform engineering mostly cares about it. Developers about developers better care about it, right about that? Yeah, what about beyond that? Beyond the tech team, the tech teams, that's a good point. OK, let's let's call it that. Thank you. I just wanted to. Go ahead. What would you say? No, no, you go ahead. You answer the question. Well, again, I think, I think it goes back, I, I loved how you framed it of, of the board or then in the C-suite or in the business think when they hear observability they think about orders revenue. Um, I mean, let's, let's be really clear, if, if they could drive as many, as much revenue without having to spend as much as they do on tech, they would do it in a heartbeat. That's just the way the world works, right? The the reason we invest in tech is because in our modern world that's how we drive revenue for most companies, especially companies that participate and reinvent. Now, The big disconnect we've had is that too often on the tech side we don't think of our job as driving the business, we think of it as driving tech. And I think especially with what's happened in the last few years with AI and moving forward you're gonna see this come together more and more as technologists who don't understand the need to drive business outcomes they're gonna struggle more and more. Now, so I think that's gonna drive the need to bring together observability in in service of business outcomes and it's kind of getting back to what we've already talked about. Now, to the, to the notion of like, who does, who does observability data serve? Who does observability data serve? I, I think the most underutilized audience is the developer. I, I, I think too often the developer gets really interested in observability when there's an issue, when there's a production issue in particular, and I think the massive underserved opportunity is when there's, is in the development process. When there's a bug that you're, you're trying to track down, when you're in the, uh, you're, you're developing a feature and there's, there's a performance issue, when you're trying to uh uh focus on scalability issues, when you're trying to get ahead of issues. I think that's the massive untapped potential of observability. Um, it, you know, I, I've often, I, I've, I've been harping on you guys for years on this. Brian's been with the company for 6 weeks, so I love picking on him for everything I've been harping on New Relic for for years, um, that, you know, most, most companies, they spend $10 on. Engineers And they spend $2 on their cloud platform, they spend $1 on observability, roughly round numbers, right? Um, and then they spend a lot of time thinking about how do they reduce the cost of the cloud and how they reduce the cost of observability. As opposed to how do they optimize the cost of the actual humans that are working on all these problems, right? Um And I think the massive untapped potential is use observability data to increase. Uh, speed of development To how fast you track down bugs, to how, how much you can optimize, uh, site performance if, if that's, you know, the area that the company is focusing on, to, to whatever the case may be, yeah, and I think when you start getting in that direction, that's where like AI becomes interesting and getting back into insights, right? Like, hey, what's the work that the system can go and do on behalf of the developer, so it's not the developer having to track these things down themselves, right? It's just like, hey, here's the things that we think are kind of like going but that are wonky that are going on in your system that you need to take a look at. And then potentially like hey we've taken actions because we know some of these wonky things actually are gonna cause issues had we not taken action. So getting, getting back to the, the vision demo that we, we showed, right, it's, I think that's the opportunity, right? So you look at that persona, you're like, hey, underserved maybe in terms of how do they get more out of observability it's the opportunity to say let insights be the way that you kind of serve the development community what they need to become better developers like more efficient and. Yeah, the only thing I add to that is that the video really focuses on here's what happened in production overnight. I would just extend that to, oh, and by the way, we noticed you have the sequel query that you're writing that you haven't released yet and that your sequel query is crap. Here's a better version of that exact same that sequel query that will have the same outcome, but. When you have 1000 concurrent users you're not gonna fall over, by the way, yeah, then adding to that the, uh, you know, we all, we all have, uh, you know, years' worth of, uh, incident uh data of problems that have happened on the site, you know how can AI look at at my individual, um, you know, not, not your data but my. Data because we have different applications and use that to forward uh warn me hey uh this is on the path to to disaster here but um so start raising the alarms before it affects very many users because we we already have seen that path before so use that to to warn me not just fix problems quickly when they happen. So that's the uh that would be the holy grail for me yeah and then you get into uh the fact that in a lot of cases applications are dependent on other applications, right? So like if you start it starts creating a a list of dependencies that you have to manage, um, I think that kind of leads us into hard truths. Like what are some of the hard truths and like getting this right that we just have to like you just have to manage on a day to day basis when you think about, OK, I've got a strategy, I've got a goal I'm trying to achieve in terms of moving observability more to insights like what, what gets in the way or what do you guys see getting in the way there? I think a big challenge is, uh, the, the alerting, uh, the amount of alerting is it too much? Is it not enough, uh, you know, same with traces, you know, getting that right that like. And uh, you know, what we're logging, all those three, I think is the foundation of saying how, how quickly can we recover from an incident um and get uh start making money again. And you know, too often, you know, we, we're alerting on everything and then everyone just starts ignoring all of them or there's too many gaps in our alerting and then we, we don't notice anything until a customer calls. So that's, uh, that's been our, our challenge over the years is what's, what, what's the right balance and, uh, you know, we're getting better at that, but we for sure don't have all the answers yet either. Yeah, and you're bringing up a good point, right, because it's like this, it's a world of you're dealing with complexity in the sense of. There's applications that are different life cycles, right? Like you have legacy applications you're having to manage that might be driving a big part of the business that don't move as fast, right, that have like a different way you need to think about like telemetry around that app then maybe your more modern apps and it's just like the mix of all of that together is like what's the right recipe to make sure you're not. You know, like hammering folks with noise, but then also like not providing enough insights or enough noise for them to, to like make better decisions and our applications are changing. All of a sudden someone showed up one day, hey, I have to have, you have to have MFA on everything, Carrie, and so all of a sudden that get in and. That opened up a whole new suite of problems and so everything's shifting underneath us at the same time, but the, the general concept is you have to get uh alerting and tracing, uh, right or you'll, you'll get too, too much noise or too little noise and um so that's my opinion. Steve, Steve, we, we had lunch to kind of talk through what we're gonna talk about. He just stole my answer. I just want to point that out for the record. I did, yeah, I didn't think you're the way you phrased it. Um, So I, I. My, my, what was the question, hard truths, I think is the way you phrased it. I think the hard truth is. The fundamentals are super important. And, and I, so I completely agree with you because I hate to admit that. Uh, Carrie and I have known each other for a long time. Um, So If if I was to show up somewhere. Unless it was a super well oiled machine, the 9 times out of 10, the place I would start is I would want to look at what's, what's your retrospective process. Um, now, a lot of places call it postmortems. Just quick aside, I came from a place that was in the health tech industry. In health, postmortems involve a dead body in the room, so we call them retrospectives, and that's just stuck with me for 10 years now. So retrospectives. Um I, I'm very forgiving of mistakes. It's the mistakes you repeat that I'm very unforgiving of. And retrospectives are the opportunity for the organization to learn from our mistakes. And so like you talked about the missing alert or what often is the bigger problem, the 8 alerts that add no value. Um And and so what I would be looking for in an organization is what is your process to when you, when you have something go wrong, how do you learn from it? What, what did go well, but more importantly, what didn't go well? Uh, you know, how did the bug make it into production, but more importantly, why didn't we detect it as quickly as we wanted to? Why, why wasn't it mitigated? Why did that bug cascade into a larger customer impact than it needed to be? Um, what were, what were the alerts that detected it? What were the, what were the, what are the alerts we had this week that went off that added no value? When I started at CHG 8 years ago now, our SRE team was getting paged about 150 times a week. Um, and we were not having 150 incidents a week thankfully, and we're only having about 100, um, and, and we just started doing some really simple reporting of like, hey, we got paid 45 times for this one thing last week, why? Like what did you do to fix it and we just started working through those one by one, this really simple basic stuff, right? But it starts clawing you back to stop making the stupid mistakes and start making the hard mistakes, right? Um, and getting yourself in a healthy place so then you can start focusing on how do you up level into the intelligence stuff, how, how do you start really focusing on true business value, um. And and you know start really elevating the craft to where we all wanna be operating that that's my hard truth. I have another hard truth. OK, so, uh, you know, I started off at Ancestry as a Java developer and one of the last things I wanted to do was add observability data a lot through the software development life cycle and so that's that so I see that a lot with, uh, the developers and I'm sure. Uh, you have the same, uh, challenge which is how do you get other people in the corporation to care as much as you do about this information and, and so I'd say, you know, you just gotta be vigilant at it and, uh, point it out when it's not happening and you can't just keep carrying this tech debt from project to project and only care about reservability once you have a problem and so, you know, make sure you're talking to your, uh, uh, senior directors, VPs, and making just making it a topic of conversation to. To, to not carry this tech debt forward, cool. And then, you know, because both of you brought up alerts, right, and alerts can be noisy. Do you have the right ones? One of the things that we're developing and releasing early next year is this notion of smart alerts, and it's both helping with the configuration and set up of alerts to begin with to make it simple, but then also. Alerts that can tune themselves over time, right? So getting back to this problem of if the system can help kind of optimize the alerts for the right band of how much noise are they creating and how do you get to the right level of noise when it's the maybe it's not noise at that point, it's like insights because it's the most meaningful things that actually make sense and. Understand when there's things that we can remove for low frequency and high frequency. When's that gonna release? So that's early. Uh, there's gonna be a preview in February. February will be our February. That's a commitment February of this next year. February 1st, February 29th, yeah, is it a leap year for them? Is it a leap year 28th? Twenty-eighth, twenty-ninth, that's funny. Yeah, let's move on let's move on, but I, I, I, I, I would say like just getting back to if that resonates with you of like yep the alerting is something that is like we're still trying to get a handle on. I definitely just encourage everyone to, to take a look at that and, and happy post this session if you have any questions on that to, to answer them for you. Alright, advice. I don't know. Maybe the advice and the, the hard truths kind of go together, but, uh, any like if you just think about like parting advice on the observability journey, the working with new relic, uh, maybe wherever you wanna take it, like what would, what would be your parting words here? Hm, I mean, that's a lot of overlap over what we just talked about. The piece I'd add is uh. Don't, don't let perfection be the enemy of progress. I think I hear a lot like, oh we need this additional data. Oh, we need, you know, oh, you know, we don't have traces enabled here. We don't have like. There's so much you can do with where you're at. And don't let Don't let the lack of something keep you from Making progress with what you do have. There's always like. There's an infinite amount of data you can bring into your observability stack. Um, it's a very long tail of value, and so what you have is the most valuable data you have because it's what you have. Go. Add data once you know you find. Once you hit a gap, then you can worry about adding data. You know, mine would be, uh, yeah, get your data right now because what you're gearing up for is the AI, AI ops, right, the promise of AI ops. And so now is the time that you've got to get, uh, your data in the right place to be ready for that so it can, it can act and, uh, serve you, um, same type of another one, you know, don't get too depressed of all the, uh, all the problems you're having. Everyone else is having them too. They just, you just don't work for that other company. Um, and so, and don't let it, yeah, don't let, uh, a great, uh, uh, incident problem stop you from, uh, putting a lot of, uh, new, new infrastructure in place to prevent that type of things in the future, yeah, over and over, uh, you know, use that and, uh, grow and, you know, be happy with the, the progress you're making because we're all doing the same thing, but again, my. The main thing is you've got to get your observability data right. You've got to, if you're, if they're coming from multiple sources, you've gotta figure out a way to, uh, combine them. So then when AI ops is mature enough that it's useful that you'd be, you'll be ready to hit the ground running, but you can't just assume that's gonna solve your data incongruencies. Don't, don't use US East one. And my advice is like, and I almost go back to like the advice I give my kids is that relationships matter so much and I, and the reason I say that is I, I, I feel like in the world of agentic and it maybe gets back to the rounding out this conversation, the how often are you going and talking to your business counterparts or product managers and aligning on the metrics because like those are gonna matter more than anything and I think in a in a world where we tend to technology tends to be one of these things that disconnects us a lot. Like as organizations it's like those connections and relationships and alignment is part of those relationships that help make sure that like the investments you're making around observability and like whatever features you're focused on actually aligned to things that are creating the right outcomes for like at the end of the day of the business. Yeah, I, I, I completely agree with that as AI makes technology easier. The relationships, the people, the non-technical parts is gonna become more and more important, even in the tech field, yep. 100%. So go out and have a conversation. Maybe that's the best advice. Take these headphones off. Go talk to someone, OK. So, uh, with that, we left a lot of time for, for Q&A. Um, it turns out that this is not a very good way to be able to do Q&A. And so I think what we're gonna do is I'm gonna, we're gonna stop here. I'm gonna say thank you very much, both, first of all, to, to both of you, um. Hopefully this was an, you know, informative, uh, session, you know, part of what we wanted to do is just kind of get just some thinking going, right, like maybe, you know, like we obviously you go through a lot of sessions looking at. Features and lists of features, but you kinda step back for a second like what are we trying to do and like what's the right way to approach it? Hopefully you came away with with some nuggets. Um, really appreciate the time. Thank you everyone for the 10,000 steps to get over here and if you, uh, if you have any questions for us, number one, you can come up number 2, come and meet us at our booth. Uh, other than that, have a great rest of the, the conference. Thank you.