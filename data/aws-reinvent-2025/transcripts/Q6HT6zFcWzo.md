---
video_id: Q6HT6zFcWzo
video_url: https://www.youtube.com/watch?v=Q6HT6zFcWzo
title: AWS re:Invent 2025 - The future of Kubernetes on AWS (CNS205)
author: AWS Events
published_date: 2025-12-03
length_minutes: 60.53
views: 1684
description: "Kubernetes has become a standard way for organizations looking to modernize their application portfolios. AWS developed Amazon EKS to make Kubernetes more accessible to organizations of all sizes, helping them focus on what matters most for their businesses. In this session, join Amazon EKS product leadership to learn about the latest innovations and strategies for building Kubernetes platforms and applications faster. Discover how various organizations use Amazon EKS to run their most demanding..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

OK, good afternoon. Welcome to Reinvent and welcome to the, the future of Kubernetes, which I admit is a bit of a mysterious sounding title, but I assure you we're gonna cover a real concrete overview of recent enhancements to EKS as well as a sneak preview of what's coming in the next couple of years. It was, it was probably 6 to 9 months ago and Netflix came to us with a, a vision that I imagine some of you in the audience have had where they said we wanna use Kubernetes but we wanna make scale and operations somebody else's problem. And I'm lucky enough today to be joined by Neil Mullen, who's the senior director of cloud infrastructure at Netflix, and he's gonna talk about how Netflix migrated to EKS in a matter of months, not years, and then I'm gonna cover how we're making that philosophy available to all customers no matter their size. And we're also joined by Ishwar Bala, who's the director of uh director of engineering at containers, and he's gonna talk about some of the scaling innovation we've done over the last year to make sure that we support the very large scale that Netflix runs at. OK, Kubernetes, I assume everybody in this room is, is familiar with, with the word. This is the data from the latest CNCF survey in 2024 which says that now 80% of enterprises are using Kubernetes in production. I think that's up from 66% when it was done in 2023, so over 90% of companies out there are at least evaluating, if not using Kubernetes at this point. And you know why, why is that the case? Why has Kubernetes become so popular? Simplicity. In in our view is the biggest reason why, why it's become that way. Uh, I think there was a, there was a recent tweet that said something like Kubernetes is the, the wrapper of, you know, 15 years of bash scripts, run books, cookbooks that SREs have made all wrapped up behind a simple to use declarative re reconciling set of APIs, and that really resonated with me and what I've heard from customers as well. Kubernetes is a very simple way to manage your cloud infrastructure. The other two that come up are consistency and extensibility. With with simplicity first, you're never starting from scratch when you're using Kubernetes. If you need to run a Spark job, you go use the Spark operator. You need to run an ML training job, you go to a project like Cubefo. Very rarely are you starting from scratch, and that's simple. You don't have to write a lot of custom stuff. Consistency is the other one that we hear a lot from customers. You can run Kubernetes on AWS, on Prem, and other clouds, on the Edge, on fighter jets, which we've seen with EKS. You can run it anywhere. And then extensibility, while Kubernetes is simple. It doesn't cover every single use case you you might need and while Kubernetes by default is a container orchestrator really with the CRD model and its extensibility, you can customize it to do anything you want and we see more and more customers, uh, today starting to use EKS and Kubernetes to operate their entire business, not just orchestrating containers. So we announced EKS back in 2017 it it reinvent or the I believe a preview of it. So it's now been eight years uh since we launched EKS and we've, we've advanced from, you know, the early days where it was just to manage Kubernetes control plane. We've launched open source projects that have become the new standard in the Kubernetes ecosystem. We've expanded beyond the control plane to do. Add-ons, data plan, and today we'll cover what our latest expansion is EKS capabilities that we announced just last night. So, so Kubernetes in context, you know, your, your goal is to deliver business value, not necessarily operate infrastructure, and our goal is to deliver the fundamental components that you need to build a production ready Kubernetes environment. And what does that mean at the infrastructure layer compute networking storage? None, none of that's ever gonna change. And one of our jobs on the EKS team is to make sure you get those fundamental building blocks that AWS gives in a Kubernetes native way, things like our CNI that integrates with the VPC or Carpenter that integrates with EC2, or our storage drivers with integrating with storage services. That foundation is super important. We're always investing there. Then the control plane, right, that's the hardest part of doing Kubernetes, not for the faint of heart. Nearly everybody who's running Kubernetes on AWS these days, including Netflix now, is using EKS. They're not doing it themselves. Management tooling. Governance compliance, security, that's the next layer you have on top we've expanded to start to help you there and then developer tooling, uh, I, I said in that earlier slide. That I think 80% of customers are are evaluating Kubernetes now. Some recent data has actually shown that, uh, developer familiarity with with Kubernetes is decreasing. And while you might say, oh, is that, uh, you know, harbinger Kubernetes is losing popularity, we actually see it's the opposite and that Kubernetes is just becoming a layer in the stack like say Linux has become. Where developers don't really have have to think about Kubernetes anymore, they just use their familiar tooling, deployments, IDPs, jobs, and the workflows, and that runs on Kubernetes underneath, but it's not something they have to be terribly familiar with. I wanna talk briefly about container registry and ECR. I think it's the unsung hero of, of the, of the AWS container world, and it's something myself and Ishwar focus on in addition to Kubernetti. So I'll go through quickly some of the, the advancements we've made in the container registry space. Uh, ECR does over 2 billion image pulls a day. I think we like to think of it as the unsung hero of AWS container services. Every application you're running in a container orchestrator, if you're using ECR, it's starting with an image pulled from there. Some recent enhancements we've made, uh, enhanced scanning. We did an integration with Inspector where we provide, uh, advanced scanning for images, and one of the features that we're most excited about, and I think myself and Ishore especially talk a lot about how can we do better together across container services, how can EKS, ECS, ECR work better, and this is one of those examples where we said how can we make, uh, you know, security, uh, folks' lives easier. We run a scan, you get a report of a of a vulnerable image. Well, now where is that running? You might have dozens, hundreds, thousands of clusters depending on the environment you're running about. So this feature we launched a few months ago makes it easier to see a live inventory of vulnerable images that are running. Another one is authenticated pull through cache, and one of the enhancements we made was ECR to ECR pull through cash. With ECR we let you pull images from upstream registries like Docker Hub, but you can and ECR Public, but you can also pull within your own ECR repositories. So that's across regions, across accounts, and that's a feature we made earlier this year. Some other notable 2025 launches and the one I wanna highlight, I think this was, I'll talk about the containers roadmap in a little bit. I believe the highest up voted issue on the entire containers roadmap was this tag immutability one where certain images you wanna upload and they can never change. There's security and compliance reasons, but the latest tag specifically often in development workflows, you are OK. You do wanna change that and within a repo we now give you that flexibility where you can set certain, uh, tags, for example, latest to have image immutability. And then our two new launches for reinvent, one archival, we talked to a lot of uh ECR customers that when you get to a certain scale for compliance reasons they still need to keep images around. However, you know. You can be running up terabytes of storage that you don't necessarily need. You may need someday because an auditor or compliance comes back and says you need it, but you don't need that in your prime primary registry to be pulled down for for actual usage. And so with ECR archival we now give you an easy way, meaning we can, you can look at the time an image was last. Hold to decide if you want an archive and image and put that into a separate storage class that comes at a lower price if you do for whatever reason need that image back eventually to run in workloads, you can bring it back from uh the archive storage class very similar to services like S3 which have different tiers of storage classes. We now have that in ECR. And then the last one in ECR managed in image signing you're always gonna hear about security, uh, in, in an AWS talk. It's always one of the top things we think about. We now provide managed signing in ECR and so you don't have to run separate, uh, infrastructure. It's fully automated and then it's integrated with AWS Signer. Uh, it's integrated with all of your, you know, tools, you know, in AWS like, like cloud cloud trail. Now with a simple API call you can sign, uh, an image and have that, uh, automatically done in ECR. OK, onto, on, on or back to Kubernetes. So EKS. We run lots of clusters, uh, tens of millions of clusters. This is tracking the number of create calls we see to our API over the course of the year, and I think we all agree Kubernetes is, is awesome, but Kubernetes is hard to run, um. We, we make it a point in EKS that we are always gonna run vanilla upstream Kubernetes. Conformance is super important to us. We're never gonna run a version of Kubernetes that differs from what you get in upstream. If your workload runs in Kubernetes somewhere else, it's gonna work in EKS, and that's a super important tenet for us. OK, I'm gonna now cover, uh, quickly a lot of the enhancements we've made, uh, over the course of the year for EKS upgrades. Uh, it's, it's a word that probably makes a lot of you in this audience shudder when you, when you hear about Kubernetes and EKS. I think, you know, death, taxes, Kubernetes upgrades. When you make the choice to adopt Kubernetes, upgrades are, are just something you have to stay up with. We recognize that. We, we know, we know it's painful, but when you're, when you're any, any managed open source service that you're adopting in AWS generally has upgrades. So a few things we've done this year. Uh, cluster insights, we scan your clusters. We look for certain things that may impact your ability to, uh, upgrade to the next version of Kubernetes. Are you using deprecated APIs? Uh, are you running a cubelit version that's more than 2 versions behind? There's a whole host of things we look for, and we surface that to you for upgrade through upgrade insights. You can now refresh that on demand. One of the pieces of feedback we heard was great. You surface to that once a day. If I go fix that, it takes too long for me to figure out if it was addressed or not. Um, and then the other one I'll mention is Kubernetti's version support acceleration. We've put a lot of effort in over the last couple of years to make sure we stay up to date on Kubernetti's versions. They come out every 4 months now. If you look over the last 2 years, I believe every single release of a version in EKS has been within 45 days. If you had talked to us a couple of years ago, we, we were not there. We've put in a lot of work to make sure that's, that's the place, and you can be assured that moving forward within that 45 day window, new versions will be available in EKS. Here's just a screenshot, upgrade insights. I feel like. Is a is a hidden feature. I talked to customers and they're like, oh, we didn't even know you surfaced that. So it's available in the console if you go to the monitor cluster dashboard, these we run every day by default. You can now upgrade them yourselves, but this is every time we release a new version we're going and checking, is there something new we should look for? I think in 133 we started checking, are you using Amazon Linux 2 because that's no longer supported in 1.33. So this is something we're always improving over time, adding new things to look for, um, as new versions come out. One of the, one of the pieces of feedback, uh, that I got from customers is, is great, we need to upgrade. I don't even know which clusters I need to go upgrade. I, I, I have clusters across dozens of accounts, dozens of regions, account managers reaching out to me saying, hey, can you go figure out where all my customers' clusters are because they know they need to upgrade, they just can't find them. And as customers run more clusters across these various boundaries, we knew we needed an easy way to make it, uh. To make it viewable where those clusters are, so we did a lot of research. There's various services in AWS that had regional multi-region dashboards, uh, others that had like EC2 I believe has a global console. Other services had cross-account dashboards back up. Nobody, as far as we knew had yet done a true global. Cross account cross region dashboard and we knew if we had to solve this problem of figuring out what's going on across my clusters that we had to solve both of those things. So earlier this year we launched this EKS global dashboard which gives you a centralized uh inventory of all of your clusters. It's not necessarily meant as a high severity troubleshooting. It's more of an executive level dashboard. You log in Monday morning, your cup of coffee. Let's see what versions clusters are on, who do I have to go chase for upgrades? It's, it's that type of workflow where you just need to understand what is the estate of all my Kubernetes clusters look like. We've continued to work on on observability, making it easier to understand what's going on in your cluster, troubleshoot issues. This, this monitor cluster button that's now there in the EKS console, there's a whole bunch of sections of it now. You can see node health, uh, one of the ones that I think is also kind of hidden that is super helpful. Is we surface through cloudWatch log query insights you can see who are the top talkers to the API server. We see a lot of cases, tickets opened, and it turns out, you know, some new deployment, a policy agent got released in the cluster and it started making thousands of list calls and it took down the API server. You run this query, it makes it immediately obvious that it was some deployment that did that. Networking, I'm sure Ishwar and his and his team will, uh, agree with me, you know, we've seen every possible way that Kubernetes can fail over the 7 years we've been doing EKS. A lot of the time, most of the time it's networking. And feedback we got from customers was look, we'd rather not have to open support tickets we'd we'd much better off if you can give us the visibility we can troubleshoot ourselves. It's often, you know, something going on in the environment we need more visibility into what's going on and so we spent a lot of time understanding those needs and last week we launched uh enhanced container network observability. So this is, it's a single agent you run in your cluster that exposes metrics, ships them to. To cloud watch as well, it's useful for proactive network, um, monitoring, so understanding, you know, are you nearing, uh, DNS packet limits? are you like retransmission timeouts, all of these ways that like Kuberneti's networking, I think got a lot of things right. It's a single flat networking name space. It's a pod, uh, an IP per pod. When it works it's great. When it doesn't, it can be hard to troubleshoot. And so we're really excited about this one. It's a, it's an easy way to turn it on. You get out of the box, uh, visualizations which I believe I have here. So for many years I've wanted this one to show the pods networking flow. So once you turn this on, you get a uh a native, uh, service map so you can understand what are the, you know, which pods are even talking to each other. Oftentimes that's the first point when it comes to figuring out what's a networking issue going on. And then two, the, the flow view you can see, uh. Within the cluster who's talking to who and you can see both cross AZ, uh, you can see pod to service. So today we support S3 and Dynamo in a lot of cases ML training does a lot of chatting to S3. We make that easy to understand and then, uh, also pod to, uh, traffic that's external to the cluster. So check this one out. It really help with understanding what's going on in the, in the networking observability space. Another recent launch and this again comes uh a lot down to to troubleshooting is troubleshooting is is hard but it's easier if you have the right context. We launched a MCP server uh earlier this year in preview and a lot of the feedback was this is good but until you manage it for us we're not actually gonna use it in production and so we launched last week. Uh, a hosted version of the EKSMCP server, it's available in a public preview. Uh, troubleshooting and getting started are the really two first big use cases we have. Troublesshooting, especially we have, uh, again 7 years of EKS we've seen all the way Kubernetes can fail and we have run books and troubleshooting books that are built into the MCP server. So if you ask it something that's going on we can go use that to go look up the same knowledge that a support team might look up when you open a case. You can now do that yourself through this MCP server and it's integrated with all of the. Uh, tools you're used to Que console, um, and security, of course, cloud trail, the hosted version, all, all of those enterprise grade features that you need to actually use it in production you can now do. It's integrated into Q. so if you go into the EKS console now as of last week and you see something that's failed, whether it's a, you know, a pod that's crashed loop back off or I don't know, some networking issue with the new feature I just talked about, you can now click, uh, hey, tell me what's going on. It automatically integrates with the MCP server behind the scenes. You don't have to do that yourself, and, uh, this, this can be helpful to understand more quickly what's going on in your environment. For an even deeper level of observability, CloudWatch Container Insights, uh, uh, this, this product has come a long way in the last two years, uh, and we recently launched, um, EBS metrics, more detailed GPU metrics, application signals support. Look, there's lots of uh observability tools out there that you might be using. Cloud Watch Container Insights is our uh native version that's the easy button. If you don't wanna have to think about which metrics do I scrape, which alarms do I set up, just give me the opinionated version of a of a Kubernetes monitoring stack. This is gonna be your answer. When you, when you move to Kubernetes, you get a lot of efficiency, efficiency benefits, right? You're not running a single app per VM. You're getting that bin packing where you can have multiple applications running on the same instance, and that has a lot of benefits, but it comes at the expense of cost visibility. It's hard to figure out with multiple applications running on the same instance who's actually contributing most to the cost. Uh, so we have several options there. We have a partnership with Cube Cost, which is an open source tool. We have an EKS version you can run that. That's still an agent you have to run in your cluster. For the fully managed version that you can check a box, there's no additional cost, we now integrate with split cost allocation, and we announced this last year, but two of the biggest, uh, feature requests that we heard following that were support for Kubernetti's labels. So not just understanding at a name space level what's the cost, but at an individual label level on deployments as well as support for GPUs. So these were two of the big feature requests that are as of several weeks ago now supported in EKS. OK, a couple more, uh, cross account pod identity. We launched pod identity last year to make it easier to connect, uh, or, or authenticate from your pods and deployments to AWS services like RDS or S3 or Dynamo. And I think this this was honestly a lesson learned for us. Almost all of you are running multiple accounts. When you're running real world production workloads, you might have, you know, your, your, um, applications in a separate account and your compute in another. And so making pot identity work for cross account was, uh, a big request once we launched that feature, we made that work this year. There's enhancements like, um. Session session token support so you can do say things with pod identity like uh pod in this name space can only read from S3 bucket if it has the prefix that's the same as the name space and you don't need to have 10 different policies to do that. You can do that with a single policy using session session tags. Cluster deletion production, there are a lot of mission critical workloads running on EKS and. You know, a simple thing like an infrastructure is code tool bug that you clicked plan and you clicked apply and it did something you didn't want to do, we heard that is a is a real risk from a lot of customers like, hey, we have, we have really, really critical workloads running in EKS. Simple feature we, we added cluster deletion protection that's similar to what other services have. EKS add-ons, uh, we launched this a while back now, but we've continued to expand this. This isn't that theme of, you know, I wanna run Kubernetes, but I just want AWS to take on more of the heavy lifting. We launched community add-ons this year, so some of the most common add-ons we see people running in their clusters, metric server, external DNS, those are now available for you as well. And then another, another feature that we just launched is backup support for EKS similar mission critical workloads, stateful workloads, uh. We wanted to make it easier for customers to both back up and show compliance to their auditors. AWS backup support for recast launched, I believe, a couple of weeks ago at Qupon. It's agentless. It's fully managed. It works across all of your other AWS services that are already integrated with backup. It works cross account, cross region, so this one out of the box we believe will cover most of the use cases you need to back up. And then restoring, uh, you can restore just specific name spaces, you can restore to existing clusters or to new clusters, it's a pretty flexible approach. Uh, EBS, definitely one of the jobs of the product team is to make sure that the rest of AWS are building Kubernetti's native integrations that work well, and EBS is a good example of a team that we've worked really closely with. And so they've launched recent features like, um, you know, enhanced data protection, faster initialization, volume cloning, all of these, as the day they launched an EBS, they were available in the EBS CSI drivers. I think EBS is a good example of uh. Of a case where we work closely with another service team to make sure that their innovations and their features can come to EKS as soon as they're launched. OK, I'm gonna go quickly through these. Uh, EKS runs everywhere you need to be. We're a launch blocking service. We are in every region. We are in every availability zone. That'll continue to be the case. We run anywhere you need to be, whether that's fully in the cloud or on premises, and we have a whole spectrum of offerings available to help you there. Uh, last year at Reinvent we launched hybrid nodes. This is our newest approach to supporting on premises infrastructure so you can have the control plane in the cloud, worker nodes on premises, which is a better way of doing in our case as long as you can have connectivity to the cloud, uh, versus EKS anywhere which we launched several years back, which is a fully, um, uh, on premises air gapped, uh, approach to running clusters, but hybrid nodes is an easier approach to running on premises if you have that connectivity back to the region. Some of the features we launched this year bottle rocket support, expanded support for psyllium, and then configuring networking on premises can be difficult, so we surface a lot of insights as that, uh, as that setup goes. Auto mode was our other big launch of Reinvent last year. Uh, Auto mode is our Is our fully managed data plane and uh we've launched various features over the years managed node groups Carpenter we've EKS Fargate we've learned a lot of things and Auto mode we think is the most Kubernete's native conformant but fully managed data plane that takes away the heavy lifting you need to do. Uh, and the way we did this is we worked with EC2. So previously without auto mode you're running, uh, the EC2 instances you're running all the controllers in your cluster. With auto mode we take on responsibility for running the controllers, so things like the EBS driver, carpenter, the load balancer controller, we run those on our side. EC2 manage instances run, uh, in your account but managed by us, and that was an innovation we worked with EC2 to make happen. Uh, we've continued to, to iterate on auto mode this year. Um, static capacity, advanced networking options was a big ask to support running pods in separate subnets, uh, region expansion, faster image pole, we'll continue to, uh, iterate on auto mode, and there's a link there to the, uh, change log. Almost every week we're launching new features based there. OK, my last one before I hand it off to Ishu. Probably our biggest, uh, update to EKS really since we launched 7 years ago, you get a, you get a cluster. And it's production ready, but that's not enough to actually run your applications. How do you deploy applications to those clusters? Those applications generally need, uh, other AWS services. Maybe you need an S3 bucket. You need an Elastic cache. How do you provision those and make it easy to connect to the workloads? So we announced DKS capabilities last night. And this is our take of expanding beyond the cluster to managing that heavy lifting of the platform that everybody's building around Kubernetes and we started with both um deployments so uh we now have a managed version of Argo CD. Generally our take with with capabilities is when there's uh an existing community standard that the majority of our customers are using we're just gonna take that and manage it for you and so that's what we did with Argo. It's very loosely opinionated. If it works in Argo it's gonna work in this, but also where possible we're gonna make AWS integrations that make sense and make better. So Secrets is a pain point with GetOps. We added a native integration with Secrets. Manage it to make that easier, setting up credentials to get can be challenging, so we have an integration with, uh, code Code commit, and I think one of the most, uh, underrated innovations that we have with our version of managed Argo that you can't do when you manage it yourself is we manage the networking sync traffic behind the scenes. So this works across account it works across region. You don't have to worry about network connectivity across those boundaries that's handled by us. And then the other one we launched is managed capabilities for ACK and Crow. These are two open source projects. Uh, ACK we launched several years back. This is a Kubernete's native way to manage AWS resources, and then Crow is a way to build abstractions around those resources and publish those as APIs. And we see this is really useful for, uh, a common pattern we see now is rather than using traditional infrastructure, cloud formation or terraform. Where a developer needs an S3 bucket, they go to a team, they open a ticket, they get their bucket, they come back, they hook it up. Now they define their infrastructure alongside their Kubernetes applications all in a Kubernetes native way. And yeah, the customer experience, so as an EKS administrator you're creating the capability as a developer they're using the familiar tools they're with just as if they were using them in them in the open source. And so putting it all together, a self-managed platform with EKS looks like this. When you combine it with capabilities in auto modes, we're managing a lot more of the pieces for you. OK, with that I'm gonna hand it over to Ishawar who's gonna talk about uh some of the large scale innovations we've made this year to support running really, really large workloads on EKS. Thanks. Thank you, Mike. As Mike highlighted, Cubunity's simplicity and extensibility made it the go to platform for containerized workloads. Now, what I wanna share is how we are taking EKS to unprecedented scales to meet the demands of AI and ML workloads. The innovations I'm about to show enable capabilities that seemed Out of reach just a few years back. Let's dive in. Now we observed 3 key developments that are shaping the communities and AI landscape. First, Kubinities has become absolutely central to AI and ML operations. This is not by accident. You know, it's decorative and extensive model, the robust orchestration capabilities, and the rich community tooling. is exactly what makes complex air workloads easier. The second development is about the scaling loss that we observe right now. Increasing model sizes correlate with the model capabilities. And we've moved from models with millions of parameters to, to hundreds of billions and even trillions these days. And every step in that. Evolution in the in the model size demands a lot more from the infrastructure. Which brings us to the 3rd point. Scale requirements have exploded. Um, modern AA training is not just about managing GPUs, it's also about making sure that you get the best out of all the other infrastructures like compute and storage. And what we also observe is customers never just run a training inside inside the single cluster that they have. They run a variety of diverse workloads in the same cluster to, to actually share the capacity. They run their inference workloads, um, and we also observe startups running domain-specific trained models, um, to hide throughput inference services. You know, this projection from Gartner is, is absolutely stunning. 95% of new AI workloads, um, is gonna, is gonna run on communities dramatically from 30% increase, dramatic increase from 30% from, from what's today. And what this means is Cuban is evolving beyond the general purpose container orchest orchestrator to become the de facto substrate for the AIML workloads. We launched EK's Ultra clusters in July, and we worked really closely with Anthropic, one of our customers, to address the challenges that they were facing in their critical AIML workloads. While we initially developed this capability with AI in mind, what we are observing is customers like to use this ultracluster setup for other workloads. These clusters can scale beyond, like close to 100,000 nodes, and they can harness up to 800,000 GPUs in a single cluster or 1.6 million trainee accelerators. And we are proud, we are proud that we have achieved the scale while maintaining full community conformance. This means you can use your existing tools, infrastructure, workflows, without any modification. Just at a larger scale And this graph here illustrates what running AI workloads in practice means. Uh, we observe our customers, like I mentioned, running multiple distinct workloads. Um, in our testing, we orchestrated three distinct workload types simultaneously. You see here, we have training staple sets. We also have fine tuning jobs and inference, um, worker sets. As you can see, we, we have pushed the system to handle up to 100,000 concurrent pods, scale up to 100,000 concurrent pods in a really short duration. Um, you see the climb up in, in, in a few minutes actually. And you can see rapid scaling events where thousands of resources are provisioned at rapid time, like across all these 3 different workload types. Um, and the key point here is managing the scale reliably is about maintaining consistent performance at any duration in this life cycle of the workloads, and this is exactly what we've, how we've reimagined our control plane architecture to achieve. Let's look at the EKS control plane here right today. The control plane stores and manages cluster cluster state. Um, in ECD. And, and at the heart of the cluster is it's HCD managing both your cluster configuration, and we actually run 3 HCD nodes in a single cluster. And it serves as the backbone for the entire cluster, storing all configuration data. And the state of your community's objects. It uses the raft consensus protocol to ensure the data consistency across all nodes. And there's also an MVCC layer, which is multi-version concurrency control, that manages concurrent access to the data and provides the key isolation property if you're actually having multiple data rights or reads at the same time. And the system here is anchored by Bold DB, which is an in-memory key value store. It's backed by a storage as well. And we also see a write-ahead log, which guarantees the durability of your cluster state. And we've carefully engineered this setup to right size as the demand um requires, uh, but also making sure that we have really regular rapid backups to restore the cluster state, uh, if in an unlikely event of the cluster going down. And this architecture has served us well. But as you'll see in the next slide, we've made some foundational changes to ECD uh to support the ultra-scale operations. There are 3 innovations that I want to highlight here, right? Like I'll start with in-memory database, which is the BoltB that I was mentioning. Uh, we moved it from a network attached storage to an in-memory TEFS based solution. And, and, and this shift delivers order of magnitude. In performance improvements in both read and write operations. Second, we have partitioned the key spaces that are stored inside the cluster, and that allows hard resource types to be split into separate SCD clusters. And our testing shows this delivers up to 5 times the right throughput, while preserving the the durability and the rich APA semantics. But the most critical advancement we've actually done is how we've offloaded the consensus management. We've moved from traditional raft-based consensus to leveraging ADBS's journal system. Uh, technology we've been perfecting for over, over a decade now, and it actually serves it underpins all of your favorite services that you can, many of your favorite services that you can think of, and this allows us to scale ECD ECD replicas without being bounded by quorum requirements. There's no need for ECD peer to peer communication anymore. And while this also delivers ultra-fast auto data replication with multi AZ durability. And together these three innovations form the foundation that enables EKS to support ultra clusters. And And here you see the, the, the new architecture in EKS. The key transformation, as I mentioned, is how we handle consensus. Um, you see that on the left is the storage layer, the MBCC backed by bold DB and on the right is the replication layer, which is actually the consensus layer. And we've integrated that with AWS's battle-tested Multi AZ transaction Journal. And, and while we did that we've actually maintained the familiar GRPC interface which meant that we kept the the the same interface between the current ETD using RAF-based journal and the new uh sorry RA-based consensus and the new journal-based um consensus. And this, like what this means is you get massive improvements without sacrificing compatibility or durability. So while we've enhanced the cluster control plane, we've also made significant improvements to boost your application's performance and reliability with 4 key advancements. First, we've introduced multi-network interface support for pods, enabling network bandwidth up to 100 gigabytes per second. Um, it's critical for AI workloads moving massive data sets. Second, we've also implemented um concurrent download and unpacking of images using in our new container runtime based on our Sochi image pool. And what that allows us to do is cut the container image poles in half the time that it takes today. And for network efficiency, we've moved from individual IP assignments to your pods to prefix delegation that assigns a cider range for an instance at chart. And this dramatically improves node launch rates up to threefold, while optimizing your VPC address range. And finally, we've also launched auto repair capabilities for all of your compute, including the accelerated compute. We automatically detect and replace your unhealthy nodes to maintain consistent performance. And let's talk about the real-world impact here, right? The adoption of GPU workloads in EKS has been remarkable. Since 2024, we've seen GPU instance usage with EKS more than double. And while AI and ML remain significant drivers, of course, we're seeing GPU adoption across various domains. We see scientific simulations, we see video processing at scale, real-time rendering, and high performance computing. You know, while these are groundbreaking capabilities for massive workloads, we asked ourselves a fundamental question how can we bring this performance improvements to all of our customers? I'm sure not everyone needs a 100,000 nodes cluster, but I think every customer deserves predictable high performance control plane operations like whether you're running your microservices platform or managing enterprise critical applications, and we are bringing the performance benefits of our ultra scale architecture for all clusters. And I'm, I'm really excited to, to introduce the provision control plan. Uh, it's a, it's a first of a kind offering that brings the performance benefits of our ultra scale architecture to all EKS clusters. At its core, the provision control plane gives you the ability to select high performance control plane scaling tiers with pre-allocated capacity. Think of it as shifting from on-demand, um, capacity with auto scaling behind to a provision reserve capacity model for your control plane. And it's particularly valuable for any workload that demands predictable performance, uh, whether you're running AML training or you need consistent pots getting rates or operating multi-tenant platform, you all expect the control plane to, to have a better performance, um, as your workload scales. And there are 3 key benefits in provision control plane. First, you can provision your control plane capacity, eliminating the uncertainty of dynamic scaling, which is what happens today. Um, the cluster control plane scales up and down based on the workloads that you are, uh, putting on the workload load you're putting on the cluster. And this means you'll have consistent predictable performance for your critical workloads. No more worrying about control plane scaling, uh, delays during critical operations. Second, you gain access to increased compute capacity. We're talking about multiples of standard performance levels, um, and you have, you get capabilities of like processing up to 6800 concurrent API requests in our highest tier. And 3, you can set up the control plane tiers to handle unexpected demand spikes, and this is particularly, particularly valuable when you're planning for high traffic events. And we have designed these options with flexibility in mind. As you can see, EKS now offers two distinct control plane modes. On the left we call standard, and on the right is provisioned, and you can switch between them based on your needs. And the standard mode, which remains our default option, like I said, the control plane scales up and down as, as you put the load on it. Obviously there's some delay in between scaling between the, the, the run levels that we have today, and this is perfect for general purpose workloads as we, as you do today. With provision mode, you're pre-allocating the specific tiers for guaranteed capacity, and as your workload requirements evolve, you can adjust your control plane configuration accordingly. And it's super easy to, to either create a new cluster or update an existing cluster to provision mode. You pick the capacity tier and during the create or you can actually do it during the update. And I want to dive into the specific capabilities each tier offers for clusters. And each tier is engineered around 3 critical aspects that directly impact your workload's responsiveness and stability. First, API request concurrency. This determines how many operations your cluster can handle simultaneously. Think of this as your, your ability or your cluster's ability to multitask, whether you're rolling out deployments or you're scaling applications, you're handling health checks. And higher concurrency means faster, more responsive operations. Pot scheduling rate is crucial for your workload agility. Like it's about how quickly your cluster can respond to scaling events or recover, recover from disruptions, and this becomes particularly vital in any AML workflows where you need to rapidly orchestrate training jobs or or scale inference services. The cluster database size is the 3rd 3rd dimension here. It ensures you have sufficient headroom for your application metadata, which is stored, which is stored on the HCD side. Um, it's usually your config map, your secrets, your resource like pods and name spaces and all of that stuff, um. And, and each tier maintains a 16 GB cluster size, uh, and which we have found optimal for most workload patterns. And these tiers represent carefully engineered performance levels that match real-world operational needs. You know, we We, we made it really, really easy to to consume these capabilities, like with just a simple CLI or command or click in the console, you can configure your desired control plane capacity. I encourage you all to try it out, um, on your new cluster or an existing cluster. So, now that I've shared how we're pushing the boundaries of the scale and performance with EKS. I'm excited to introduce uh Neil Mull Mullen here, senior director of cloud infrastructure at Netflix, and Netflix operates some of the world's largest container platforms serving hundreds of millions of customers globally, and they've been at the forefront of cloud native innovation, and their journey to EKS is, is a story of evolution and scale. Uh, Neil, over to you. Thank you. Hi everyone. My name's Neil Mullen. I lead what we call cloud infrastructure engineering at Netflix. Am I in the right place? Yeah. So, this is a story more about how moving existing large stuff to manage services or to have someone else do some of the heavy lifting can be done. I'm gonna talk a bit about our background at Netflix, what makes us unique, different and hard to do this with, and our journey to EKS over the past two years. So, compute at Netflix. We're, we have a lot of compute. We're super dense compared to most AWS customers. Not alone do we have 300 million paying customers that we have to run a website off, which is more like a billion user profiles or users. Um, that takes a lot of compute. We have large scale personalization to make all of those predictions appear for what you want to watch before you even know what you want to watch. And that takes a lot of compute. But even that is dwarfed by the thousands of hours of video we're shooting every week in 8K that we're converting to run from everything from your 10 year old Roku device to the latest AK TVs. And not alone is it just this week's video, we're constantly re-encoding the entire back catalog of Netflix to use the latest codex and to be rendered in all those resolutions. In addition to that, we have a rapidly growing ads business. And a burgeoning gaming business, each of which are large sources of compute. So we have a lot of compute at Netflix. And from talking to peer customers or other large customers at AWS, the proportion of our spend that is compute-based is 50 to 100% higher than many other large customers. So let's talk about the setup at Netflix. We came to the cloud 15 years ago. So actually a majority of compute at Netflix still runs on a very mature, very well run EC2 workflow that enables developers build Java and node services primarily and runs them direct on EC2. A large minority of compute though, runs on a system called Titus that we delivered 8 or so years ago. It's a majority of actual service count at Netflix, so most new code at Netflix gets written by default on this. And it's a large scale multi-tenant container platform, which effectively does container as a service. We originally built it on MSOs, we shifted it to Kubernetes seamlessly under the hood about 5 years ago. And that's the scope of what we're going to talk about today in terms of what we took EKS to work with. However, there's a lot of key differences as to why we're even more awkward than that. We run 4 core streaming regions, a couple of additional twin regions for some of our encoding and gaming use cases. We run a regional availability model. That means when we have problems, we flip an entire region in about 5 minutes. In the middle of October, AWS had a bad day, happens about once a decade or so. We had a bad 15 minutes and we were out of US East 1. Secondly, we have the concept of the trough. There's a diurnal cycle in almost everybody's services, which can be as much as it runs to about 45% delta from the peak to the trough we find in many services. And that's basically hundreds of thousands of CPUs that are sitting idle at different points of the day. You've two choices. You can either judge what's the sweet spot. Spot to how much you purchase on demand or how much you purchase reserved instances. Or you buy the whole lot in reserved instances and you pack every cycle of that with all of that time insensitive work I described earlier. We run about 97% of our reserved instances as utilized doing something. I'm not saying we run them all well, that's a different argument, but we do. The main point is, we are moving a lot of compute around, a lot of the time, more so than perhaps anybody else. Let's talk about some of our scale. We have 4 primary regions, hundreds of thousands of containers in each of those regions. Um, but when we talk about what we have to plan for in adopting a service like EKS, it's not just the steady-state launch rate. And it's not just even the region evacuation launch rate when we're getting out of US East 1. It's when 100 million people are watching Mike Tyson get his lights punched out and we have to get out of US East One, and that's what we have to plan for. So when we first talked to EKS about this 4 years ago. Yeah, 3 years ago, 3 years ago, I think it was 3 years ago here. The answer was hell no when we sketched the kind of launch rates we need. And I have to give kudos to Pinterest, who brought EKS on a journey through 2023 that brought us to the point that 2 years ago it was, well, we don't need to double what you guys can do. And so we started on our journey. Um We don't do cluster as a service. We're different on that front as well. And we run less than 20 production clusters in our four regions. We believe that the high percentile scale and availability story of running very large clusters, multi-tenant on very large boxes works better. But that makes us weird and different for a service like EKS who has to cater to hundreds of thousands of different customers and all of their oddities. So we've up to 10,000 machines, big ones, 24 and 48 XLs per region. We've about 80,000 containers or pods, we use those pretty interchangeably for how this is configured in each of those clusters. ECD is still running north of 5 gigabytes, though it was more than twice that at one point. We've had to do a lot of work to bring that down. That is one of the sets of changes we have to do. And we run launch rates of 70,000 instances in a 5 minute period, or sorry, containers in a 5 minute period when we flip regions. So why EKS if we have all this already? One of our core engineering principles at Netflix is build only when necessary. We built all of this stuff that I've described over the last 15 years because it was necessary, because there was nothing there to do things at the scale we wanted to do it. And we still want to build lots of things today, but we don't need to build what others can provide. So we want to offload the undifferentiated work and make scale somebody else's, in this case AWS's problem. Let's have a quick look at what we did after build. I'm not gonna go into the details of this slide, I'll talk through what we had to do. But the green pieces were the integrations we had to do with EKS. The red pieces were all of the services and the existing code of ours that we got to delete as we moved the control plane to EKS. So here's the one text heavy slide that we have to read through. This is the list of what we spent our time doing. We worked for about a 9 month period with EKS to work through getting their scale to where we needed to and doing these integrations. Um, we had to use EKS itself. We had a lot of changes to make around our regional control planes which are consolidated an account to give EKS its own space and its own control plane and its own VPC so we could isolate it. Um, we have changes to our identity model. We have our own identity model which works in uh both inside and outside AWS. We had to integrate with IA for the EKS control plane integrations. And we had some integrations with Cloudwatch to do as the Kubernetes logs go into Cloudwatch and some Prometheus integrations to support that. But all in all, we were able to work through this with a fairly small engineering team. And we took the aggressive goal. That we were going to, sorry. So we ended up with a system that as you recreate a container, it creates on the new one. We decided to migrate the entire fleet in one quarter. Getting long-term metrics at Netflix is another bugbear of mine, but it's a pain. But this is the tail end of that migration. And within, we ran a little over a quarter, about 11 days over, and we migrated every last container from our existing system to launching on the new AKS control plane. And there's not a lot of pain in this story. Nothing ever goes smoothly. And we did find the limits. EKS doesn't like 120,000 pods inside a single cluster. At least it didn't back in March. I'm excited to see the hyperscale announcements that ISWAR has been talking about today. I'm going to go and test out some of those limits. And what we find at our mutation rates is that the numbers of EKSA they support, they support a little less when we're spinning the pods at the rate we are. Um, it was so successful, we also run a federation layer above those clusters, where we choose which pods go into which cluster. Some of them need to be in the same cluster, some want to be in different clusters for availability regions. We've also moved that federation layer to SCD ahead of schedule because of how smooth this experience was. So what's next? We spent this year evolving the Titus data plane to be less weird. It consisted of a virtual cubulet and like 60,000 lines of custom code. And that was a bit a bridge too far to bring to AWS or to EKS. So now it's running on the stock cubett, and we're going through another one of those crazy migrations, which we're racing towards year end to try and be done, and we'll probably be about 11 days over again. But that opens the door to an EKS data plane migration in the future. Um, EKS hybrid nodes. Open the door for use cases we have, where we're seeing more and more edge use cases for our gaming and increasingly heterogeneous encoding use cases. EKS auto mode allows us potentially getting rid of having to look after the OS for many of these use cases. So they're all the things we're looking at in this coming year. We're also experimenting with thinner containers. If any of you can do the math on some of those earlier slides, you'll figure that our containers are big, spanning many CPUs in many cases, but we're seeing more and more thin container use cases. So this is what we're gonna work on in 2026, and hopefully we'll come back and tell a story about how we migrated the data plane too to EKS in future years. So, thank you for the time and the opportunity to talk here. Sorry, handover. The handover art. Thank you Neil. Uh, looking forward to you continue to, to pushing the boundaries of what we, what we can support at scale. OK, so finally the title, uh, with 7 minutes to go of actually what's, what's coming next? We, we've done a lot of work this year, but we love unsatisfied customers and we know we have more to do. This isn't an exact quote, but it's one that I've put together from an aggregation of talking to hundreds of customers over the last. Year or two whether it's newer Kubernetes adopters, even existing customers who are early adopters of Kubernetes are are realizing that it's not really useful to my business to tweak every last add-on, every last setting, own every last thing. It's, it's the same refrain of I wanna use Kubernetes, it's become the standard. I wanna use the tooling. I don't wanna think about clusters, upgrades, any of the hard parts. So there's often You know there's pure technology companies out there right whose whose goal is to solve these these hard problems at scale and we have customers who also need to solve these hard problems but they have different focuses you might be in health care, gaming, pharmaceutical airlines there's all kinds of different industries out there. You don't necessarily have the time to to focus on running and and managing large open source projects yourself, um. And so you know how do you, how do you use technology without becoming a pure technology company? I think every company these days wants to say they're a tech company. But that's not really what the end core business focus is of a lot of these companies out there and you come to a conference like this you're seeing a lot of projects, innovations, and, and how do you go back and make use of that yourselves open source software, you know, contributing to several small projects running at small scale with open source, doing it yourself, that's doable as soon as you start getting to 20 open source projects, putting that together, running at scale, you need a lot of time and expertise, um. And so really what what we think about at EKS is taking open standards combining that with AWS and accelerating your time to value it's using all of EKS without um. Without having to to manage these open source projects yourselves, so it's lowering the cost of entry uh to to run these projects. Every type of workload runs on runs on EKS these days. We just talked a lot about AIML. You heard Neil talk about streaming and encoding, gaming, web applications, data processing. You name the workload, it's running on EKS and so we have a very broad diverse, uh, customer base types of workloads. We're gonna make sure our service works for all of those, of course. AIML is, is the hot topic these days, but there's lots of other workloads, stateful workloads in particular. Spark, flank, we wanna make those easier. How do you upgrade stateful workloads? Uh, EKS, I think we've showed this slide in, in a few years. We started with just the managed control plane. That's the really hard part of Kubernetes running at CD, scaling, patching API servers. It's, it's not easy to do, and that's where we started back in 2018. Over the time, uh, you know, we've moved beyond, beyond just clusters into hybrid running compute outside of AWS managing more of the platform components or EKS capabilities which I talked about this year. Uh, our launch this year will continue to take on heavy lifting of some of those components that you're running in addition to the cluster and then the developer experience. What if there was a world where you could just give EKS your application manifest? You didn't even have to create a cluster in the first place. You just gave us the application you wanna run. Let us do the heavy lifting like Neil just talked about a, a, a federated layer above all of their clusters that they've built to figure out where pods go. We'd love to do something like that so you can get Kubernetes without even having to think about clusters. So what are our priorities for the next 3 years? Uh, one, critical workload patterns any scale. That's really what we've talked about today. We just talked about all the different workload patterns. The scale requirements just keep getting larger. We're, you know, we're gonna have to look at splitting out across multiple clusters because at a certain scale you just, you just run into this is as large as a single cluster can get. How can we make running, uh, workloads across multiple clusters easier. AWS integrations this, this is a large part of me and Ishwar's job is, is going and working with the other teams within AWS, making sure that they're building the right things. Kubernetes is really the front door to the cloud for a lot of customers we talked to. They're not going necessarily through. 50 AWS services they're going through Kubernetes and the EBS driver provisions EBS. The S3 driver provisions S3, they're using AWS through Kubernetes and it's really important for us to make sure that these other services across AWS work well for Kubernete's native customers. And so that's a, that's a big part of it. You'll see us working with other teams, more integrations coming. Meeting your workloads where they are, that slide I showed earlier, everything from EKS Distro, which is take it, run it yourself anywhere possible on a jet, on a cruise ship, EKS in the cloud, and anywhere in between, we're working on improving our, um, story of managing clusters on outposts to supporting the new SKU and server types of outposts that are coming. We will continue to make sure we meet your workloads where they are. I wrote simplify platform building here. It may be a little controversial to say eliminate platform building. We, we wanna just launch more and more managed capabilities so you don't have to run or, or need huge platform teams in order to use EKS. We, we really don't think you should need massive teams to run Kubernetes again use Kubernetes without having to, to operate it. And then lastly is just, uh, you know, accelerating the flywheel of innovation. We continue to work in the community, open source projects generally I'd say our our philosophy is if there's an existing standard in the open source community, we'll take it and we'll, we'll run with it. We did that with the launch of Managed Argo that you saw this year. Something like Carpenter was one where we went out into the community. There was an existing standard cluster auto scaler. We thought we could do better and it's a, it's a, uh, a big decision to say there's an existing standard we, you know, we're gonna build an entirely new project. There's, I don't know, I think a meme it's 16 standards we're gonna go do the new one now there's 17 standards, but in this case we actually built the new standards. Uh, nearly every cloud provider supports a carpenter and then there's other cases like ACK, AWS controllers for Kuberneti's more of a just open source AWS only project, but there's reasons to, to open source that. So we'll continue to work in the community. Uh, our public roadmap, I get emails every time there's something somebody makes a comment on here, so I check this every day. It's a way to get feedback to us. Uh, there's more sessions coming, so. If you wanna learn more about EKS capabilities, if you want to learn more about the container network observability, and if you wanna go, go, go even deeper than what Ishwar talked about today on the high scale ultra scale performance, uh, I believe that one's a 500 level, which is one of the few 500 levels. So if you wanna get really deep on some of the architecture changes, go check out that one. And yeah, some resources, best practices guide, blueprints and that's it. Thank you very much.