---
video_id: Vu91YwZxskY
video_url: https://www.youtube.com/watch?v=Vu91YwZxskY
is_generated: False
is_translatable: True
summary: "This session, titled \"Mastering model choice: The 3-step Amazon Bedrock advantage\" (AIM391), addresses the \"paradox of choice\" facing developers today with over 2 million public models available, by proposing a systematic framework—Identify, Evaluate, and Optimize—to confidently select and deploy the right AI models for enterprise applications. Speakers Scott Munson, John Liu, and Brian Coe (from CoinMarketCap) begin by outlining the current landscape where model selection is a critical bottleneck that can derail Proofs of Concept (POCs). The \"Identify\" phase involves filtering models by modality (text, image, video) and leveraging resources like Artificial Analysis for performance benchmarks, while also considering differentiated capabilities like reasoning (e.g., DeepSeek, Claude), customization via fine-tuning (often using open weights models), and domain specificity (e.g., finance or healthcare models). The \"Evaluate\" phase is central to the talk, where John Liu introduces the concept of a \"Golden Dataset\"—a curated ground truth set of around 100 use cases, including 5% adversarial examples, derived from subject matter experts (SMEs). He demonstrates how to scale this dataset using a multi-agent setup: a \"User Simulator\" agent that generates inputs, a \"Task Agent\" that executes the work, and a \"Critique Agent\" that acts as a judge against a human-defined rubric to refine the Golden Dataset autonomously. The session then moves to the \"Optimize\" phase, discussing strategies like \"system optimization\" through intelligent routing (using rule-based, ML classifiers, or LLM-based routers to direct queries to the most cost-effective model) and fine-tuning/distillation to achieve latency targets. A substantial portion of the talk is dedicated to a case study by Brian Coe from CoinMarketCap (CMS), a crypto data platform serving 65 million monthly users and consuming 10 billion tokens daily. Coe details their evolution from simple \"AI FAQs\" to complex \"CMC AI\" chatbots, emphasizing the maxim that \"specialists beat generalists.\" He breaks down their 5 specific AI tasks—Sentiment Extraction (requiring high speed/low cost), Planning (requiring high reasoning), Data Retrieval (requiring tool use precision), Summarization (requiring long context windows), and Translation (where simple chat models suffice)—and explains their rigorous evaluation metrics, specifically focusing on \"F1 scores\" for tool calling accuracy and \"groundedness\" checks to prevent hallucinations. The presentation concludes by reinforcing that evaluation must be a continuous product, not a one-off event, and highlights how CoinMarketCap leverages Amazon Bedrock's infrastructure to switch models seamlessly without managing underlying servers, ultimately achieving an 80% cost reduction for some workloads by moving from general-purpose large models to optimized, task-specific smaller models."
keywords: Model Selection Framework, Golden Dataset, Amazon Bedrock, Model Optimization, AI Evaluation, CoinMarketCap, Agentic Evaluation

Welcome everyone. Uh, thank you for joining this session on mastering model choice in Amazon Bedrock. We're glad you're here. Uh, how many of you, uh, are overwhelmed by the number of AI models available when building AI applications? Great, uh, and how many of your organizations have a systematic framework for picking out models for AI applications? All right, fewer hands there, great. Well, we're here to help, uh. We're gonna talk through a simple framework that you and your organization can use to identify models, evaluate those models, and optimize them for production based on the work we're doing with our customers, uh, at AWS. I'm Scott Munson, principal worldwide AI specialist with Amazon Bedrock team. Along with me today are John Liu, principal product manager with Amazon Bedrock, and Brian Coe, senior AI product manager with Coin Market Cap. So quick agenda, uh, we're gonna start off by just an overview, kind of set the context. What are the challenges that our customers are seeing and, uh, how are we helping them and then talk through the framework itself. So how do we identify and evaluate and optimize models and then we'll have a chance to hear from Brian to uh how he's using this framework and his with his team to, uh, deploy AI applications that are serving over 65 million monthly active users. So Get a sense of the landscape. Uh, 2.19 million public models available in hugging Face today. Uh, chatted with the chief product officer of Hugging Face yesterday. He said there's a new model every 10 seconds, uh, incredible pace, right? That adds up to about 4000 models per day. That's just publicly available open weight models. Uh, add to that all the proprietary model makers, Anthropic, Amazon, OpenAI, all the rest. Uh, we have so much choice. This is in every modality. Uh, text, image, video, audio, new modalities, there's so much innovation, um, it's an exciting time to be building these applications with AI models, but there's a little bit of a bottleneck. I mean, we're hearing from customers that picking the model itself is actually a challenging, uh, task. So why is it challenging? What's the consequence of this? What we're hearing is that the POCs matter when you're building out an AI application for the first time with your organization. You wanna make sure that that's a successful trial run. They're not all guaranteed to to be successful, but it matters to our customers that they have a a good win early on in this process. And so, uh, this matters for the time and labor spent, but also just the momentum and organizational kind of reputational impact. So it, it can be a tough loss if they pick the wrong model for the job and build around that. We're also hearing this specific problem of no selection framework to to work through all the models uh with and so we're gonna talk through that today of course and the pace of innovation is challenging. If it takes 4 weeks to test out your models and pick the the one you like and a better model came out yesterday, it's sort of a challenging, uh, environment to work in, so pace is important to our customers. And then of course uh any customer facing application, any large organization application needs to function at scale. So sometimes our POCs aren't, uh, optimized for the big production workloads and we wanna make sure that our customers are successful in that arena as well. So Amazon Bedrock is uh AWS's solution to provide uh developers what they need to focus on their developing of applications and, and we can kind of provide the rest, uh, you know, Bedrock has a great selection of top models uh that are kind of coming out continuously and we're committed to providing great model choices. We optimize uh our inference to make sure that our customers can use this inference in a serverless manner so they can scale up and scale down they can use different modes of inference, um, they can balance the requirements of their application without having to do all the deployment of AI models themselves. A lot of our customers are seeing success by leveraging their data, so that could be in the form of a rag knowledge-based solution or it could be in, uh, model distillation, uh, or fine tuning, uh, according to their, their specific proprietary data. And, um, of course safety and responsibility are critical. So Amazon Bedrock provides guardrails, uh, along with other features that help our customers ship their products with, uh, with kind of confidence in the reliability and, uh, responsibility of their AI applications. And we're doing a lot with gentic work. uh, this week we launched a number of agent cop, uh, capabilities that are very exciting to help you seamlessly deploy and operate agents at scale. To go a bit further on, uh, model, uh, options themselves, we really do work with top model providers and continue to expand that number over time. Uh, we think that's important. We're hearing from our customers they want that choice of models, so we're committed to, to giving them that model evaluation tools are a critical kind of counterpart with that to make sure that customers know what the right model is and. We also allow people to import their own models. So if you fine tune a model or have another model that you're interested in using, um, we can import that and through custom model import and provide that inference, uh, along with the other models available, which is a big kind of convenience factor again letting us handle that, uh, inference task for customers. And again, Bedrock has, uh, this slide shows 13. We added 4 additional model providers this week. We launched models from Google, from Minimax, from Nvidia, and from Moonshot, uh, very exciting. We're over 80 models now available on Amazon Bedrock. Uh, we really do, uh, hear from our customers and are convinced that us providing this inference for them, this Bedrock service gives them the simplicity, the scale. And security that they need so they can just focus on their domain and their application development. So Great to have all those models. You still have to pick, uh, some to use for your application and so this is the model selection framework. Uh, we've kind of pulled this from working with a lot of customers and, and trying to simplify it down so you can work with your stakeholders and kind of, uh, understand easily what stage you're in. Uh, this, the process itself is, is relatively simple if you, uh, think about it in these terms. First up, you identify candidate models so you look at all these. Options and filter them down. We'll go into exactly how to do that in a moment. Then you evaluate those models on your data on your use case. This is really critical. Generalized benchmarks are not the greatest indicator of what model is gonna be most performant to meet your application's requirements. And then we have another step to optimize those models that might be through breaking up the AI workload into multiple models, uh, to meet the performance requirements you need or potentially through fine tuning, uh, as well. So we'll talk through all that today. Um, and to help kind of keep us grounded, we're gonna use this, uh, use case to help understand the process. So we've got a financial crimes investigation agent scenario. This is one that our customers have built that we've talked, uh, know what this looks like. Um, in this case, uh, you know, a financial institution, something gets flagged for potential financial crime, uh, they still are a human review process required, so these analysts are manually reviewing a lot of documents, transaction data. It's time intensive work and they want to accelerate this workflow for them. The requirements we're looking at for this application would be to process structured and unstructured data. We wanna generate really accurate, concise summaries. Um, this is a large scale, uh, and so we're looking at about 5 billion tokens per day. So we wanna make sure we're keeping that in mind as we're picking out the models we wanna use. And then of course security is essential. Um, the goal here would be to hit 20% efficiency for these humans workload and, and be able to work more quickly. So, the identification stage, um, I'll kinda go into more detail on this. Put simply, we're gonna filter out all the models that are relevant by modality and just consider the ones that are relevant for our use case. Next up we'll look at all those benchmarks. They are valuable. There is utility there, especially the more detailed benchmarks, and then considering benchmarks alongside other metrics that you care about. And finally we'll look at some differentiated capabilities. There's so much innovation. There's a lot of categories of models, but there's a few that I wanna highlight that really we're seeing matter for application success. So first up, modality. This one's relatively simple. What's the input type that the model can ingest and what's the output type that it, uh, that it provides? Um, huge amount of text data, of course, in business. A lot of these models were initially started there, so we've got some great options within text, but we're seeing, uh, you know, a lot of great use cases developed with an image input, video, audio, as well as the outputs, uh, of all of those modalities. So step one, just look at the models that are relevant for your use case. And here's a little view on what's available in Bedrock, uh, bi-modality this can be helpful, right? A lot of great models available within the text category, uh, multimodal input, so those are often text plus image, uh, also some video understanding available, um, multi-modal output, and then embeddings, a lot of growth in those areas. We're launching new models, uh, all the time and, uh, meeting more of our customers' needs that way. So we've got it filtered by modality. Now let's think about uh. A way to kind of compare quickly. Artificial analysis is a very popular third party resource. They provide an index and metrics for a huge range of models as well as, uh, inference providers, and it's nice to be able to go here and consider, you know, top intelligence, uh, and price or latency or whatever your, uh, metrics you care most about, um, side by side to kind of quickly evaluate what's the one that I might care about. Uh, they also provide this capability to do both X and Y axis. Find kind of the, the sweet spot quadrant where you've got, um, you know, optimal price, uh, to intelligence ratios. Uh, in this example we see a few of our bedrock models, uh, Claude Haiku, Deep Seek, as well as the GPT OSS models really standing out in, uh, really high intelligence, uh, but, but really considerably lower prices. So, uh, important to take note of that. I think the other thing I wanna make sure that we call out is that within uh benchmarks often we talk about just a single kind of combined intelligence score which is really valuable for rapid comparison but there are really detailed benchmarks and if you know you have something in your use case that's relevant for example long context reasoning uh. That's something that's really relevant in this use case we're talking about. You can look at specific benchmarks for that. In this case we've got Claude, uh, standing out alongside Deep Seek, uh, as some really good contenders for long context reasoning capabilities. This is also a an agentic application. So, um, I, I've highlighted here as well, uh, you know, we've got agentic tool use listed. This is an artificial analysis resource here, but Galileo is another agent leaderboard that's available. It's also Berkeley, uh, within Agenic. Leader boards you've got some, uh, a lot of variety, you know how is it as an orchestration or managing level model and how is it as kind of an executor, uh, worker, uh, model? How is the tool use selection? There's a lot of meaningful benchmarks that are available when you're trying to pick out the models and which role they should play within an agentic application. So this again filters down to kind of consider hey what's a couple of top models we should we should think about uh modalities done we're thinking about metrics where does the price performance or other kind of combined ratios, what benchmarks are really strong. I also wanna talk about a few other. Uh, categories of models, and these are ones that I decided to highlight because I'd see a lot of our customers, uh, drive a lot of success with these. Uh, there are others, but I really think these are the ones to focus on. First up is reasoning. Uh, you know, Deep Sek landed in January. Uh, there was reasoning models before then, but a lot of attention, a lot of major model providers, uh, adopted reasoning models as part of their even hybrid architecture. Uh, so there's a lot available, um, you know, reasoning models can do more complex step by step roles. Often again playing that orchestration role can be valuable in agentic use cases. They can explain the rationale, very, uh, useful, uh, when you need to kind of understand why the, the model made a choice. Uh, traceability, uh, is very, very nice to have through that method, of course, science, math, and coding. Within this category we have a variety available. The cloud models are good examples. The GPT OSS models. We've got models from Deep Seek from Quinn, and a number of others that are all, uh, reasoning models. Again, agentic, uh, you know, autonomous task completion, very exciting. I think the best practices are being developed, new offerings are being developed every day, uh, and. These are important to consider, you know, what's the tool selection capabilities, orchestration of multiple, uh, tools, uh, all the rest there. Customization is worth, uh, kind of pausing and considering as well. This tends to be dominated by open weight models. There are some abilities to customize proprietary models, but this is the category where we see the most popularity for open weight models where the, uh, models can be used to fine tune. And the, I'll say from our experience we're seeing customers fine tune when they have a very specific performance requirement they're having a hard time hitting in any other way. Nothing off the shelf is getting them. Uh, to the goal they need, and often it's latency sensitive. So if you have a user experience that requires a really quick model response time, uh, often we'll, we'll see a customer take a smaller model, distill from a larger, or really have a very specific use case. We're also seeing this grow in gentic applications. So if you've got a worker model that does a very specific task every time, you can fine tune a model and get really strong results hitting the latency and cost needs that you might require. So it's worth considering these open weight models, these customizable models as a candidate in your first identification stage in case you need to, uh, fine tune or want to have the option to fine tune later, uh, and we're seeing this also within a specific domain. Terminology may be relevant. And last, this domain specificity. This is something that's important. Uh, we're seeing a lot of models develop in this, uh, space. This, uh, I'd say finance, healthcare are two that really stand out. Um, this is, uh, driving more success if you have specific domains where the language is very relevant for that domain, you can sometimes get higher performance, higher intelligence in the model itself, uh. You know, Bedrock Marketplace has a lot of offerings in this, uh, area. Uh, a couple examples. Upstage is a really good model for Korean language translation. Uh, there's also from writer there's a Palmyra, uh, financial model as well as a healthcare model. So a few to consider, uh, as additional options to, um, that maybe if your, if your domain has some offering in that space. So back to our use case, let's look at the financial crimes investigation agent. Um, first step there, filter by modality. We just, these are just text problems that we're solving right now. So text to text, document analysis summarization. Um, we'd wanna look at those benchmarks. We, we considered summarization specifically long context reasoning, uh, some of these agentic capabilities as well, uh, down to the differentiated capabilities, and then I think it would be in this case because of the financial crimes there's some terminology we probably wanna include a customizable model as well. So that gives us a result, right? We're kinda going from all the models in the world down to a short list that we think could be really strong for this. This would include Cloud Sonnet, OpenAI, uh, GPTOSS in this case, and Deepeek V3.1. So, uh, that's the identify stage. Uh, I'm gonna now hand off to my colleague John Liu who's gonna talk through evaluation and optimization. Thank you. All right, so now we get to go into the evaluation stage, and when it comes to evaluation, as Scott mentioned earlier, it's important to have a framework in place to do this, right? Some of our customers have put in place frameworks that let them evaluate a model and decide whether they want to include that in their production workload within 24 to 48 hours, and today we're gonna present a framework that'll help you hopefully set something similar in place. When it comes to evaluation, the first step is always create your golden data set, which is that source of truth that you wanna benchmark all your models that you're trying against to see how well it performs. And then when you benchmark these models, you want to make sure you're continuously benchmarking the models because even if you don't change the model themselves, the incoming context might change which could lead to unexpected results. So let's go into our sample golden data set and see what that actually is about. The golden this data set is really just made up of two pieces, right? One is going to be the prompt. In this case, it describes out a particular use case we want to measure in our financial crimes agent. And the second part is the source of truth. This is why we call it the golden data set, the ground truth. This is what you want to measure your the models that you want to try against and see how close they come to this source of truth. When you create a golden data set, first you start with the basics. You want to have a comprehensive data set, about 100 use cases, right, of which, you know, you wanna choose. They're very specific to your particular terminology, the way that you're thinking of designing your workflow, because only by creating this tailored data set do you know whether the model that you selected is going to be useful for you, right? The, the. The generic benchmarks that Scott presented first, they're a very good starting point, but you want to see how much of that actually translates to the use case you have in mind. You also don't wanna just live in what we call the easy mode, right? You don't, you wanna also select cases that are adversarial, right? You wanna trip up the models because you're trying to define where the limits of your model is going to be. And typically these adversarial use cases, these tricky use cases, they make up about 5% of the 100 use cases that I mentioned earlier. And a good way to start, you set up 10 use cases and then you get an internal feedback loop with your subject matter experts and build that up to 100. But then as you're rolling out the production, you're introducing more models you're now thinking about 200-300 use cases and potentially there's a better way to scale these golden data sets than continuously lean on human resources, right? Subject matter experts are expensive. So let's take a look if we can do that. What humans are not so great at or not very efficient at doing, you know, is all these detailed tasks, right, these detailed low level tasks or trying to create multiple iterations of data sets or reading detailed SOPs, standard operating procedures, and translating that and making sure that's captured in your golden data set. We can do it, but it's not efficient. And what humans are quite good at doing, a good use of our intuition, right, is the high level judgment that we have. And we're also quite good at looking at total frameworks and seeing hey what works well and what doesn't work well. So you see here there are 3 things high level judgment, setting rules and standards through rubrics, and then looking for ways to improve solutions. So if there's a way that we can leverage the strength of humans and then delegate we're called the low level tasks to agents, then we've got a pretty good solution to help us scale our golden data set from that 100 to the 200, 300 type of use cases. And now let's look what that actually looks like. We start with the first agent, right? This is the user simulator, and this you can think of that is actually the KYC expert. He's doing work to find out whether there's a potential financial crime associated or fraudulent detection, fraudulent activity associated with this, uh, particular transaction. You have a very descriptive mission for the agent and a persona and then some example questions because the more descriptive you are with your agents, the more accurate they're going to be. You pair this user simulator with your task agent, and this is the agent that actually goes through and does all the work, you know, looks through all the documents, looks through the online offline credit cards, for example, synthesize that information and comes back with a recommendation to the user saying, hey, is this a fraudulent or not fraudulent activity? Here you can see again mission persona described and they have an action they have tool calls because they're actually going through and pulling in this information. Finally, you pair this with the critique agent, and this is the actual um agent that looks through how close the task agent is or how well the task agent performs against a set of rubrics or rules that the subject matter expert, the human, set in place earlier. Here you see the mission and your job is to go and evaluate the task agent, make sure it does a good job, and your persona, you're an expert teacher and you've got some tools as well. Your, your actions are, you have some global rec global like evaluation methods, and you have some relative evaluation methods that you call dynamically when you're actually evaluating the task agent itself. Let's put that together We have our user simulator agent. It passed through a request over to the task agent. Now paired with this of course is our critique agent which is reading rules from this rubric that was defined by the humans. The critique agent continuously iterates what the results are from the task agent until the critique agent says yes. The response that you gave meets the rubrics that were set in place, and at that point, the critique agent of course passes the correct answer back to the, the task agent, and the task agent now sends it to user simulator, but also the critique critique agent passes that to the golden data set. So now you're automatically scaling your golden data set. Now where's the human, right? The human is actually evaluating this entire system at this time, right? They're not in the detailed weeds of like looking at every single one of these, these, um, responses. They look at how the overall framework behaves, and they can make suggestions that says how can I improve how the critique agent is actually guiding the task agent. Maybe we need to have more tables that are coming through so it's easier for our, our cus our customers to read. So they update the rubric and now this entire golden disk set gets stronger and the rubric gets stronger as well. Now to build this, customers can benefit from AWS's Agentic stack. Customers can start with the open source framework strands and very quickly build, deploy locally agents. And as they're ready to scale, they can now lean on agent core and benefit from enterprise grade security and also dedicated runtime environments and memory management and there were lots of announcements this reinvents around our agent core. So we've gone through, we've created our golden data set. Now we get into the metrics that we want to evaluate against. The operational metrics are the foundational piece, cost, latency, and scalability. Just like almost any software, right? Um, some things to keep in mind, of course, is when you're actually measuring these operational metrics, make sure you're sending in inference requests that cover a variety of, um, patterns, right? You wanna look for your P95, your P99 type of response context window lens so you can find out what the latency is, and you also want to send a variety of different workloads so you can see where the model or the model serving solution you're using runs into errors. Once you have your operational metrics covered, you can move into the comprehensive evaluation metrics around semantics knowledge, right? So you've probably seen the quality and accuracy, the style and usability, responsible AI. This is pretty much again, um, ground. Foundational aspects that customers are now used to. However, as customers have been rolling these into production over the last year, they've also introduced their own custom metrics, because what these custom metrics actually measure, just like any software, there are KPIs that you wanna bring in. For example, it could be click through rates of a particular task you wanna get done, or it could be, you know, the type of overrides any time a, a particular user decides to override what the agent does, maybe that's another KPI you wanna bring in. So keep these in mind as you're evaluating your model. Don't just lean on the tried and true quality and accuracy. These are important, but bring in the actual custom metrics from your workload as well. Some other metrics to keep in mind, and, you know, it's a temporal consistency I mentioned a little bit earlier, you have to consistently look and evaluate your model. You don't wanna make sure you want tech model drift early on. And as you're introducing more modalities, you wanna make sure you're measuring the modality not just in isolation but also as they interact with each other. And finally agentic capabilities, right? How well is your entire system or your entire model behaving when it's calling tools or when it's actually orchestrating across different agents. So we've gone through, we've got our metrics. Let's look at what methods are used to evaluate these models and really break down into 3, right. We start with the benchmarks, the programmatic type of evaluations, and these are really good when it has like a true right or wrong answer. You can think of, uh, benchmarks like the MMLU, which measures how good a model behaves for translation, or you could GM 8 SK, which measures how good a model is for mathematic calculations, right? Very good starting point. But if you wanna pick up the nuances of like summarization or how people talk, how people think, right, then you start leaning on humans, right? That's this is where human evaluation comes in. Your subject matters have to come in and look for those nuanced semantic type of evaluation. Now to help us scale we can now lean on models as well because they've got much deeper reasoning capabilities that's been developed over the last year. So customers are now using powerful models whether it's a single powerful model as a judge to evaluate the output of other models, or they put in place maybe 10 to 12 lightweight models and as as a jury evaluate the output of the model that's being evaluated. Amazon Bedrock model evaluation supports all three of these, right? We've got our programmatic evaluation, the human evaluation, and also LM is a judge. Importantly, we also support custom metrics. And if you want to evaluate models that are not directly within Bedrock, you can do that too. You can bring the responses from those models and run them through Amazon Bedrock model evaluation. So let's dive a little bit under EM as a judge because that's what we're gonna use for our financial crimes agent that we've been using as our reference case. Under the hood, what actually happens is we've optimized the prompts for a variety, a variety of our evaluator models which include the anthropic models, some of the meta models, and Nova models as well. We tell it how to behave and we tell it what type of response to give. And what gets passed in is the Jason online file of the golden data set, right? Here's all my prompts, here's my reference answer, and here's also the response from the model that you're gonna evaluate against. And to set this up with Amazon Bedrock model evaluation, you start with selecting the models. There are two things you want to select the model you want to be the evaluator. And the model you want to evaluate in this case we're picking the Sona 37 as the evaluator and then we're gonna evaluate the GPTOS 120B. You select the metrics, right? Um, Amazon Bedrock model evaluation has 12 metrics, and of course you can also import your custom metrics as mentioned earlier. And now you run the model evaluation and you get a result and you can compare different models against each other. In this case I've run a simple evaluation against the GPT OSS as mentioned earlier and also Deep Seek V3.1, and you can see a radar chart of how well they perform. You can dive deep a little bit as well, like into each of the prompts that you're evaluating, it gives you a score. Did it from 0 to 1, or 1 being the best score. And if I want to dive in and understand why I got a score as such, I can do that too. I click in and it have an explanation. In this case it got a one because guess what? The valdo that you're evaluating matched very well with the ground truth that you actually gave in. Now we can move into the optimized state, and we've gone through, we've evaluated, we have a set of models that we want to choose against. And when we think about optimizing, we wanna start by thinking it's not just optimizing a single model, you optimize the entire system, so you can choose single models to replace and you can also introduce multiple models you can optimize the entire end to end workflow. You can further customize individual models through fine tuning and distillation and then you can optimize how you're sending your inference requests for cost and latency as an example, you know, we announced. The inference tiers that was in today's today's the Dave Brown keynote actually he's talking about the inference tiers around that and there that's where you actually pay a little premium, for example, to access the priority tiers to make sure that your inference are at the top of the line or you maybe for the less time sensitive ones you can pay up to a 50% discount and use the flex tier, and this is how you start optimizing cost and versus the kind of latency. And when it comes to optimizing these multiple models, you have to consider the routing strategies you have, and they're really just 4 things I'd like to share with you today. You can start with rule-based routing. You can go through, look at the queries, single, simple queries, use light model, complex queries, use the heavier models. Private queries, if it's highly sensitive ones, and maybe I wanna use a model that I'm hosting myself. But that has a challenge too. This means that someone's always writing these rules, so we can benefit from machine learning based routing using classic machine learning. You can train classifiers around the data set that you have and then route these incoming inference requests to the proper model. Of course the challenge there is you need a training data set. So you can further benefit from using the LLMs themselves, right? The LMs themselves can look at the incoming data and say, well, based on this, we think the profile matches, we should send it to what type of model light, medium, heavy. And then you can further fine tune that as you get more data. The best way, of course, combine all three of them, right, that gives you a very robust way to route these inference requests to the different models. When you look at the individual models themselves, you can customize them through a variety of methods through Amazon Bedrock. You can do straight fine tuning and you can also do distillation where you take a teacher model and then bring the knowledge of that into a lighter model, or you can do some advanced customization on your own through maybe SageMaker and bring those models into Bedrock through Amazon Bedrock's custom model import. And we just announced Nova Forge, um, about two days ago, and that further helps customers customize their models because now you can not just benefit from your own data set, you can benefit from Nova's pre-trained data set as well. You can bring these two data sets together and train the models. So looking at our financial crimes investigation agent after these two steps, what are we looking at? Well, we've got the GBTOSS 120 for fast classification tasks, and then now we've got the sonnet for the complex analysis and we fine tuned the GBTOSS model and then we optimize our inference through the different inference tiers that I mentioned, which leads us to an 80% reduction while scaling the workload from 500 million inference requests to 5 billion per day. And we've not talked about the customer behind this, but this is a real customer use case that we've seen in production. Benefiting from model choice, the optimizations, the framework that we've talked about so far. Now let's review and just put it all together. In our three step framework in action, we start with the identify state. You have a library of models that you wanna look at. You review the general benchmarks that are out there, the modalities, and then you select from there the models you want and you evaluate these models against your specific use case, against the golden data set that you've created. You then optimize the setup that you have and you can have single model type of workloads you can have multiple model type of workloads. And importantly, you wanna get all this information and bring that feedback so you can improve your particular framework, including monitoring for latency, cost, and accuracy. And also getting the real-time user feedback and having that information improve that golden data set. Another benefit of having a strong golden data set doesn't just power up the model evaluation that you have, you can also use it to fine tune models, so you get almost like 2 for the price of 1. And that's how this 3 step framework works. You put something like this in place, you can be able to evaluate a model for your workload within 24 to 48 hours of it dropping. So with that said, I'd like to bring on Brian who can show you how Coin Market Cap has done similar type of framework and enabled Gen AI workloads for millions of users daily. Thanks, John. Hi everyone. My name is Brian and I manage AI products in Coin Market Cap. In the next roughly 20 minutes, I'll be walking you through how CoinMarket Cap selects models. The analogy I like to use is, although you can use an F1 car to deliver groceries, you wouldn't want to do that. You can use the strongest models, the most advanced models to do the simplest task. You wouldn't want to do that for different reasons, cost being one of them. Before I continue, how many of you know about CoinMticcap? OK. Not many, not many people here know about CoinMarchitect. So this slide will be useful. Coin Market Cap is a cryptocurrency data platform and we've been around since 2013. For reference, Bitcoin, one Bitcoin at that time cost roughly $100. When I checked this morning, a Bitcoin was roughly $93,000. But more than a decade later, we're at the home of crypto with over 65 million active monthly active users and more than 1 billion page views. If you're more if you're more familiar with traditional finance, we're often referred to as the Bloomberg of crypto. We have more than 1 million API users with institutions like Google, Yahoo Finance, Coinbase, and central banks around the world using our crypto data. Our gen AI journey started in Q3 2023, and since then, we have more than 10 user facing products in production. Our AI products are used by millions around the world, and since 2023, we have consumed trillions of LLM tokens, and today, we consume more than 10 billion LLM tokens every single day. This scale is exactly why we are rigorous about our model selection and why Amazon Bedrock matters in our stack. So, what do we use these more than 10 billion tokens for every single day? Four main things. First, users come to our site to find alphas, to find signals. With more than 27 million cryptocurrencies tracked by coin market cap and thousands more created every single day, there is a lot of noise. LLM compresses this noise into signals and users use our AI products to find these signals. Second, explain. Users come to our site to try to understand why did, why is Bitcoin price up, why is Bitcoin price down, what is Bitcoin, what is proof of work, and as we all know, AI does a great job explaining all of these. Third, forecasts. Users want price predictions so that they can make money. They want potential scenarios and although nobody has a crystal ball, AI can help lay out those options and give reasoning behind each of them. 4th, act. Insight is only useful if you can act on it. We turn AI outputs into watchlists, alerts and automations, so users can act on these insights. Let me show you one of our use cases, what we call CMCAI. This is essentially a chatbot. So imagine, You wake up one day, like today, Bitcoin is up and you, you wonder why Bitcoin is up. Instead of having to open 10 different tabs, you just go to CMC and ask CMC AI. Using live crypto data, on chain data, and social sentiment data, it gives you an answer. And then you, you're wondering, you read about this hack that just happened and you're wondering why did it happen. Instead of having to deep dive into many news articles, many social posts, you just ask CMCAI. And of course, this experience is also on our app. Millions of users use CMC's portfolio feature to track their portfolio and because we have this data, we can personalize the answers for users. The same question by a different user would yield different results, personalized results. Of course, we didn't start from that chatbot that you just saw. Back in 2023, we, when we started our AI journey, our aim was to minimize risk and maximize learning. We needed 22 critical components before we scale to the AI chatbot. First is stakeholder buy-in, and the second is a deep understanding on how to build on top of the non-deterministic nature of LLMs. The image you see on the screen is AIFEQs, one of our earlier products. And when we, when creating this AIFEQs, the reason we chose this is because it's on a less trafficked part of the page, so the risk is lower and whenever, for all the AIFEQs we generate, a human would thoroughly review them to make sure that it is correct, it is accurate before we go, go live with them. And this manual review process, although, although tedious and although it took a long time, it was very important because we learned the limitations and strengths of LLMs. With this product, we, and with more organization, organizational buying over time, we build AI products are more prominent part of, parts of our product and more deeply integrated AI within our product. So, as we moved from simple use cases like AI FAQs to what we have now, which is the AI chatbot and many other integrations across the site, one thing we realized is that we cannot have the same model do many different tasks. The example I gave earlier is you don't want to use an F1 car to deliver groceries, and I'll give another analogy. In an F1 pit crew, you have different people for different tasks and speed and efficiency comes because of that specialization. I'll highlight 5 different tasks that we do within CMC using AI. First, a sentiment extractor. Every day, we process millions of social posts, millions of news articles. And in order, and we want to extract the sentiment within these social posts and news articles. Is it positive? Is it very positive, or is it the opposite? The LLM we select here needs to be cheap because I just mentioned the scale. We process millions of social posts and news articles, and it needs to be fast. So, the most advanced models typically don't do, typically don't suit this that well. The second is planning. This is about taking the user's query, however complex or ambiguous, and transforming them into a list of to do lists. This LLM needs to be very good at reasoning and understanding a user's query and converting that into a to do list. The third is data retrieval. This is about taking the data, the, the to do list from the 2nd step and converting that into a list of tool calls. What are tools? Tools that you can think of tools as API calls that LLMs can make in order to retrieve context. This is context, this context can be the latest data, the latest news that the LLM doesn't have access to within its training data. This the model that we select here needs to be good at choosing the right tools and the right parameters within those tools. The fourth is summarization. Now that we have retrieved all that data for the large language model, this large in this step, the LLM needs to take hundreds of pages of data and news articles and convert that into one single page of data for the user to read. And the 5th is translating natural language into CMCIDs because we track more than 27 million cryptocurrencies with the same name can represent many different cryptocurrencies. What we found here is that we don't need the strongest model to do this conversion. A simple chat model generally works very good for this. How do, so we, we care about three main things when selecting models for the, the different use cases I mentioned earlier. One is quality. Quality looks different for each use case. So, metrics like factual accuracy, relevance, proper evidence usage, groundedness matter to us depending on the use case. Second is speed. Time to first token, time tokens per second, and time to last token matters a lot to us. And the third is cost efficiency. We don't care about, we don't care very much about the raw token, raw cost per token. What we care more about is the cost to quality ratio. So, with so many models coming out, I think Scott previously shared that there are, there's one model coming out every 10 seconds. How do we choose which models to test? We can't, we can't possibly test every single model every 10 seconds. So, in CMC we have 3 main categories of models. The first, at least the way we think about it, there are 3 main categories of models. The first is full reasoning. These are your top tier models. They are very good at complex tasks. Using the cloud family of models as examples in this slide, this would be your Sonnet 4.5 and your Opus 4.5. Under the light reasoning models, these are mid-tier models deal with reasoning and are generally faster and more cost efficient. This would be your Haiku 4.5. And then the third, fast and low cost, these are lightweight models, typically chat models without any reasoning that are used for simple and high volume tasks. This would be your Haiku 3.5. John talked quite a bit about golden data set and that's what I'm gonna be talking about within this slide. So, in order to prepare the golden data set, we always start by setting the scope. We need to make clear what is being tested and with over the next couple of slides, we will use to calling to as an example. If a model has access to 10 tools, we want to make sure that it is calling the right tools to retrieve the right data to generate the answer for the users. Say a user's query only requires 2 of the tools because it doesn't need that much data. The model should not go and fetch data from 5 of the tools and be excessive about 2 calling. So, that is the scope, 2 calling accuracy. The second is ground truth. Humans will prepare the golden data set. Say we have 1000 user queries that we know users care about. For each of these user queries, we will, we will think about, hey, which tool do you need to call to retrieve the data to answer the question. So, say for question one, we select 4 different tools, that is the ground truth. Third step is we run checks. We call the LLM API and we give it the 10 tools and we see what it actually returns to us. Then we compare what they returned to us with our ground truth. If the LLM selected 4 tools, when our ground truth has 2 tools, then we know something is wrong. There are 3 metrics we care about here, precision, recall, and F1, and I'll talk more about these shortly. And the last is update. This step is super important. Your ground truth will shift over time. Your ground truth needs to reflect what users care about and what your users care about changes over time. So, using the same set of 1000 questions, maybe 100 of 1000 questions will not matter 3 months from now. So, you need to remove those 3 100 questions and change it with something new. Let's dive a bit deeper into tool use. Precision is the first metric we care about, and what it tells us is amongst the tools that the model actually called, how many of them were correct, how many of them were necessary. Recall tells us out of all the tools the models should have called, how many did it indeed call. And F1 score combines both of them. So, F1 score is essentially a formula between the two of them and it's what we use to compare between models. And the reason why F1 score is important is because, say there are 10 tools and the model calls all 10 tools. In this case, recall is full because it called all the tools it needed to call. However, precision is extremely low, and the F1 score will reflect that. So, if model A has an F1 score of 0.7 and model B has the F1 score of 0.4, then model A is better and we will choose that for the two calling use case. Let's dive into another use case, summarization. Summarization, like I mentioned earlier, is about taking hundreds of pages of content and converting that into a single page or maybe two pages of content for the user to read. The model needs to have very good ability to understand large context, and there are a few, there are many different metrics we care about here. I'll mention 4. 1st, is the answer relevant to the user's question. 2, is it readable? 3, is the answer well structured? And 4, groundedness. Is the final answer grounded in the context it was provided? We all know LLMs hallucinate. So, what we found is that sometimes LLMs in the final answer will come up with facts that we did not provide it within the context and groundedness is super important. The chart you see is our is our test of seven different LLMs for groundedness. The orange line represents median, the box represents the inter quartile range, which means the 50% the the middle 50% of the results, and the dots represent outliers. You see that the first result has a tight inter quoile range and a decent median score. The second result has a very big inter quota range and the highest median result, so it has the potential for the highest result and it has also potential for low results. And then the other, all the other results just are generally not so good looking metrics. So, in this particular test, we selected model one because although model 2 had a potential for very good results, it was too unpredictable. So, we cannot go with that. Internally, we have this system called GLAS, which stands for generic LLM as a Judge evaluation Service. What this does is it allows us to use LLM as judge very easily. It allows any team member to call LLM as judge and to get the results in hours instead of in days. And if anybody wants to use LLM as a judge internally, the main thing they have to do is to define the JSON input. They define the goal of the test, they define the dimensions that they care about, and they define the candidates that they are testing. After they have this JSON input, they simply call the API and underneath the hood of the API we actually call many different leading models, and all of these models combined together to produce a PDF output that contains charts like what you saw earlier, and also AI insights. Using these AI insights combined with more human evaluation, we can decide which model is good for the job. Model evaluation is not a one-off event. New models come out very frequently, and there are typically 32 triggers that make us want to do a model evaluation. One is a product insight, second is a new model coming out that we think it's worth testing. The first step of model evaluation is always human evaluation. A lot of models get filtered out simply in this step. So, humans, our team, myself or my colleagues will go in and test the model on a few important use cases. If it passes this stage, we will run the glass system, which is what I mentioned in the previous slide. And then combined with AI insights and more human evaluation, we will know whether this model is suitable for the use case, and if it's suitable, we will push it to production. In production, every single answer. is run through an LLMS judge. So we know the quality of all answers generated in production and we have alerts if something goes wrong. This step is very important because we have gained many insights just from this step. Of course, I need to have a slide talking about how amazing AWS and Amazon Bedrock is and to be fair, they have been great partners. So, I think they fully deserve this. The first is inference. Many models in the latest models are all available on on Amazon Bedrock. Whenever Claude come comes out with a new model, Amazon Bedrock has it the same day. To us, calling LLM APIs is just an LLM APIs. We don't deal with infra, we don't deal with scaling, and we are very grateful for that. So, That's, that's the main thing that AWS Bedrock helps us with. Amazon Bedrock helps, helps us with. The second thing is rack. We use Amazon's open search service and their coherent with ranker. This helps us to fetch the right content for the model to use to answer the user's question. The 3rd is security. Built-in features like zero data retention is super important because we process the portfolio data of millions of users. So we need to have this for our, for our inference provider. And the 4th is hands-on partnership. Since the beginning, Amazon hasn't been just an influence provider. They have always been very helpful, offering deep technical support and in fact, we are working together on, on a crypto AI benchmark and this benchmark will compare general purpose chatbots with CMC AI's ability for crypto use cases. So, to sum, to sum up, 4 points. First is the use case drives model choice. The best model, a Ferrari car, isn't always suitable for every job, isn't always suitable for delivering groceries. Specialists usually beats generalists for for different use cases. The second is that evaluation is not a side project. It is a product. It's never set and forget and through continuous evaluation, can you then can you convert what may be a fun demo into a production-ready product. The third is humans matter. As much as we try to automate everything away, what we found is that humans alone or humans combined with AI brings us the best insights. And the 4th is the right partner multiplies output. A AWS has been super helpful. Amazon Bedrock has been super helpful, and you need to pick the right inference partners. What's next for CMC? There are three things that we, we, we will be focusing on in 2026. The first is reducing the cost of inference and the speed of inference. And there are a couple of ways we are thinking of doing this. First is through fine tuning models for different use cases, and the second is model distillation model distillation, which is available on Amazon Bedrock. The second is B2B opportunities. So, many enterprises have indicated interest in integrating AI within their products, and instead of building it from the ground up, they come to us and ask us for it. So, if any of you are in crypto and you want it, reach out to me. And the third is better AI UX and expanded CMC AI capabilities. Users mainly interact with large language models through a chat interface right now, and in order to get the best from that, users need to be expert prompters and they need to be, they need to understand the limitations of a chat interface. I don't think that is the future of AI and I think that that AI needs to be more proactive, it needs to be easier to use, it needs to be more personalized, and that's what we will be focusing on in 2026. So I hope now 20 minutes later you are better at choosing models and if any of you are in crypto, try out CMC AI and let me know what you think. I'll hand the time back to Scott. Thank you so much, Brian. Uh, the work that Brian and his teams are doing is really fantastic, uh, exciting to see the level of maturity and the scale of production they're working at. I really appreciate you sharing all that with our audience today. Uh, quick recap, uh, we've got, uh, just, you know, wanted to kind of reconsider what we talked about today. Uh, 3 steps identifying, uh, models, look at the general benchmarks, consider modalities, and pick good candidate models that are relevant for your use case. Uh, next step is evaluate, run those models in your environment, uh, on your data, on your use case, and, and, and generate that golden data set so that you can, uh, verify and know that you're picking the right models for your use case. And then optimize uh those models through model combinations or through fine tuning to make sure that you're achieving uh the benchmarks in the performance requirements that you need and uh great to hear uh from Coin Maret Cap as well to understand what that looks like in production uh for them today. I wanna close with a few uh resources that I think could be helpful for you all. The first is an article written by one of our VPs talking about the value of model choice, nice kind of general purpose, uh, explanation of why we're committed to that at Amazon Bedrock. We've also got a model choice page for the Amazon Bedrock website. On that is all those new models we launched, uh, 18 new models, uh, in the openweight category to over 20. Uh, this week and, uh, we've also got those new model providers, so lots more coming, uh, we, uh, keep stay tuned, keep watching more models are on the way. Uh, we've also got a few GitHub resources. There's a model evaluation application. This is downloadable. You can run it locally in tandem with the Bedrock tools for evaluation. This is one that you can use, uh, generates a nice visual, just a streamlet application. That you can compare, uh, you know, intelligence as well as accuracy, uh, latency, performance side by side. It's a nice one to kinda share with stakeholders, help them understand why you're picking the model you're picking and then if you wanna go even further, there's a model evaluation workshop available also on GitHub. So take a shot of those, uh, great resources to, uh, get whatever level you want to kinda understand, uh, what's available within Amazon Bedrock. I wanna thank you all for being here. Uh, it was, uh, great getting to talk through this with you. There is a little uh survey if you, if you don't mind filling that out, that does help us out. We do read those surveys. We're really committed to trying to uh deliver great presentations for you all. Uh, thank you again for being here today and enjoy the rest of reinvent.