---
video_id: Ns88rkEvhTE
video_url: https://www.youtube.com/watch?v=Ns88rkEvhTE
is_generated: False
is_translatable: True
summary: "This session, \"From code to cloud: Accelerate application development with Amazon ECS (CNS341),\" introduces Amazon ECS \"Express Mode,\" a new feature designed to simplify the deployment of containerized applications. Jennifer, an ECS expert at AWS, explains that Express Mode abstracts away complex infrastructure provisioning (ALBs, VPCs, security groups) while maintaining the flexibility for developers to \"break glass\" and configure underlying resources if needed. She highlights that ECS handles over 3 billion tasks weekly and is a foundational service for Amazon itself.\n\nThe session features a guest speaker, Keith Bartholomew, Principal Engineer at GoDaddy, who shares their journey of building \"Katana,\" an internal developer platform on top of ECS. Before features like Express Mode existed, GoDaddy built a decentralized platform to support over 2,000 engineers across diverse tech stacks. Katana provides a \"single pane of glass\" for observability, automated \"push-button\" resilience with multi-region failover, and baked-in security and compliance. Keith emphasizes the importance of design principles like lifecycle management, economies of scale, and creating \"escape hatches\" for power users who need custom configurations (e.g., specific WAF rules). He also details how they use generative AI agents to help developers troubleshoot ECS failures by analyzing events and providing actionable remediation steps."\nkeywords: Amazon ECS, Express Mode, Container Orchestration, Developer Platform, GoDaddy, Katana, Fargate, Application Deployment, Platform Engineering, Generative AI for DevOps\n---

Uh, this is CNS 341. You're in the right place if you are a developer that is looking for faster ways to get your code deployed in AWS. You are also in the right place if you are a platform team or infrastructure engineer, uh, looking for patterns or guidance of how to accelerate your teams. Uh, my name is Jennifer. This is Sahi. Uh, we are your UCS experts today. We're also going to be joined by Keith a little bit later. Uh, he's from GoDaddy and he's built a platform on Amazon ECS that he's going to talk about. Thank you guys. um, with that, let's get started. Uh, first, to set the context, uh, I wanted to lay the foundation of the service that we're going to be talking about today, and that is Amazon ECS. Amazon ECS is our fully managed container orchestration service that provides the easiest way for organizations to build, deploy, manage containerized applications on AWS. Um, even as a compute layer we offer a lot of flexibility. You can run containers on EC2 instances. You can even run them on your own hardware with ECS anywhere. But the majority of our customers run on AWS Fargate. Fargate gives you the ultimate simplicity with serverless. We manage the compute completely. You pay for what you use, which means you're not bound by EC2 instance sizes ratios. ECS manage instances is a new offering that we just came out with in the last month, uh, that is a fully managed compute that eliminates infrastructure management, uh, while giving you access to a really broad, uh, set of EC2 instance size. This is great if you, oh. Um If you're looking for specific EC2 instances like GPUs or network optimized instances or memory optimized instances, uh, while still taking advantage of some of the, the great aspects of Fargate like the, uh, the maintenance and the patching and the, the scaling that we take care of, uh, ECS managed instance is a is a great option when you need that specificity, but you still want, uh, ECS to take care of that operational burden. Continuing on that theme of removing the operational burden, what makes ECS really unique is that it is fully managed and versionless. There is no control plane for you to manage. There are no upgrades of the control plane to coordinate, no patching of the control plane to schedule. We handle all of that operational complexity. Um, this is something that I've been hearing from customers all week that is, is part of why they really love UCS is that, uh, we handle so much of that on, on, on their behalf. When you create a cluster, it is essentially just a logical grouping. It is not really. Something that you have to to treat as something precious it is it is you can create as many clusters as you want and and and use them as you you want to in order to group services together, um, because we're managing that control plane and you don't have to think about it and if you're using Fargate or manage instances you don't need to provision or scale servers, uh, you don't need to manage or patch the operating system we manage all of that on your behalf. And specifically with Fargate, you get tenant isolation and our financial services customers really love this specifically. Uh, because the security boundary becomes not the container but the EC2 instance because in Fargate every task gets a unique EC2 instance. You can also achieve this with ECS managed instances, but it, it kind of, you lose some of the value of managed instances and, and the, the optimizations that we're providing with the, the bin packing and, and scaling of the, the underlying instances. And then we come to some of the, the native things that we've built into ECS. So the first one being, uh, our native service discovery. Uh, this one I feel like is kind of, uh, an underrated thing that we have in ECS. Uh, we build in service discovery and, uh, service mesh, and that is Service Connect is, is the name of the service. Um, you don't have to install or patch or or maintain Service Connect. It is just there and it's available for you to use. You have a unified way of accessing all of your services. Uh, you don't have to maintain DNS, uh, any, any complex management there. Um, it's, it's available for you to, to set up and, and access at any time. Uh, finally, native deployment mechanisms, we've always had, um, a deployment mechanism in ECS. You can, you've always been able to, um, update your services using a rolling deployment mechanism, but we found that a lot of customers are going outside of ECS to use, um, other, other external deployment strategies, uh, like code deploy to, to do blue-green, for example. Um, so just this past summer we launched native Blue-green strategies, um, as well as just about, about a month ago we launched Canary and Linear strategies, uh, all natively within ECS, and this is really important because, um, by making that native to EC. We're removing that burden of having to go outside of UCS in order to set up that deployment strategy, but also all of the wiring of the target groups in order to set up the blue and the green side. um, all of that exists within ECS, and it just makes the deployment of, of each of those things a lot simpler. Now all of those things brought together are a lot of why customers adopt ACS today, and when you do adopt ECS, you're in very good company. Over 3 billion tasks are launched in ECS every week. Over 65% of all new AWS container customers use Amazon ECS, um, and it is very heavily used internally within Amazon. Um, we actually call, uh, Amazon ECS one of our foundational services, and the reason we call it that is because every time we stand up, uh, a new region in, in AWS, uh, ECS is one of the very first services that has to go in, and the reason is because so many other AWS services build their infrastructure using ACS, um, so you're, you're in good company when you're using ACS. Uh, so that's a little bit about the service that we're talking about today. Uh, I wanna hand it off to Tahi to talk a little bit about applications. Thank you, Jen. Um, so, In the next part of the session, I'm going to cover how actually deploying apps to production actually looks like, but before we get there, we need to understand how the deployment mechanism works. I'm also going to shift in between two perspectives, one of those of the development team, those who are building apps, deploying them into production, and the platform builders, those who are building tools and automation in order to support the development teams in order to get their job easier. So let's look at the deployment process from the developer's point of view. From the moment we have our container image packaged built into Amazon ECR, the process of deploying it into production can be quite complex and involve many different steps, which I'm going to cover in this slide. It starts with having a networking, so AWS VPC. An Amazon ECS cluster to provision which basically hold all of our applications into it. Then we need to to have a way to tell ECS what is the configuration of our app, and we do it through an ECS task definition. That's the mechanism in order to do it. And then a common pattern of exposing an app is usually through a load balancer, an application load balancer, in order to do that, we need to have to do it securely. We need to have a certificate. We need to make sure we have that, and then we need to integrate it with the ALB itself alongside with all of its resources, so target group, listener rules, routing configuration, and so on, and Just now we got to a point where we can integrate this all together, create an ECS service, which basically takes the task definition and creates multiple instances of this task, integrate them with the ALB and then expose the app to the outside world, um. But the thing is, our app is not a static entity, of course we need to have an auto scaling policies in place. We need to take care of that. And in order to operate and maintain our app over time, we need to be able to observe it, so we need to ship logs, traces, and metrics into the observability service. In our case, Amazon Cloudwatch, and you could do it easily with a container insight which integrated with DC. Now all of that complex process is just for a single app. If we flip the perspective around and we look from platform builders, we also have some other challenges. There are many different development teams that platform teams builders need to support. Each comes with its own application and different requirements. And even if we standardize on the compute layer, so for example, if you standard with Amazon to provision our resources. Each different app might need a different backing service. So for example, some might need the nest necessary bucket and some might need the dyno DB table, and we need to be able to scale it across the organization. So it's not just about how do I deploy a single app, it's how do I make sure all the requirements of the different teams are satisfied. So like any good engineers, we usually automate things and by automation, You probably understand it is building developers' platform. We need to build a tool, a platform which will help developers get their task easier. Now, like any other. System or app in the organization, it has to has to have design principles, and today I want to cover three important design principles for building developer platforms. Those are life cycle management, economies of scale, economies of scale, and break glass procedures. So let's dive deep into each of them separately. Life cycle management means owning the entire life cycle of the deployment of the app from the creation through update all the way up to decommission. It's not just about how do I deploy the app, it's about maintaining it over time with updates and upgrades. It also includes some sort of an entry point for developers to be to be able to interact with the system, and this can be anything from an API, a CLI, or even a collection of templates which I will cover in the in the next part. Economies of scale is all about being smart with resources. Don't provision things that you don't need to, and make sure you're reusing things over time. And this means reusing resources like in the previous example, if you have an ALB, we don't need to provision another ALB to expose a different app, a second app. We can reuse the same ALB with different routing configuration. We also need to configure shared resources, so the cluster, the VPC configuration, the monitoring dashboards. We need all of that to be shared across all of the different apps. And in reality, it's about balancing between the approach of replicating things for each and every app, but avoiding the complexity of having everything interconnected that you create. You can end up with a mess of resources in your system. And the last thing, which I think is the most important one, is the break glass procedures. This is your escape hatches. No platform is gonna be perfect for every use case from the start, and even if you graduate and, and, and introduce more features, you're not gonna hit. All the marks for all the deployments. So you need to give your users, the developers, an easier way to transition between platform offerings. So if they use one deployment method, you want to enable them to migrate or move away in the future into other deployment methods within the platform. We also want to enable them to extend the platform capabilities so if the platform doesn't support anything. They can extend it with their own customized tooling, scripts, or anything else. And the last thing is about self-managing resources in the future. There might be a case, and we've seen it with a lot of customers that Users need to adopt. They need to self-manage resources because they have reached a point where they have very specific requirements that the platform cannot provide them. They need to be able to self-manage those resources instead of redeploying and migrating out of the platform. You don't want your users to feel they're trapped into the platform limitation. And this brings us to philosophical questions that is an ongoing one between platform teams and developer teams, and it's all about how do you design the platform interface. Should we use abstraction, which is hiding the implementation of the underlying components, or should we use composition, which is all about combining resources with default but exposing the underlying implementation to the users, or maybe you should use a mixture, a mixture of both of them. And whatever you choose, it basically impacts the way users understand what's going on in the system. In reality, it's a balance of understanding what's going on or how things work under the hood and achieving the user's goal, which is in our case, getting from code to cloud. So let's dive deep into each of them separately. With obstruction, we users don't need to know what's happening under the hood. They can be free from any implementation decision and get started very quickly and easily getting the things they need. This brings some advantages like having a really low barrier for entry. We don't need our users to be an AWS expert. They can just spin up, use the platform in order to deploy their own resources. It also creates ensure consistency across the organization. Every everyone using the same patterns, everyone using the same deployment mechanism, and it's easier to understand what's going on. And life cycle management in this case is usually exposed through an API, which is an easier way to interact with platforms. Sorry, however, it does come with some challenges. We do rely on platform teams to provision to get new features. So whenever we need to extend the platform capability, we need to wait for the platform team to build this and integrate it with the platform. It also creates some challenges when things go wrong, when there's an error and we need to debug or investigate something. We ended up doing. And understanding what are the underlying resources actually are and digging up into the actual configuration which kind of meets the purpose of having lower barrier for entry. So this is another thing we need to take into account and it also creates some higher maintenance effort. This is a system like any other app and we need to ensure we maintain this over time, including updates, upgrades, and operation efficiency. So in reality it's a simple way to get started, but evolution is bottlenecked by the platform teams. Composition on the other hand, is different. It's all about automating the setup but keeping everything visible for the users. The good thing about that is that developers can use part of what the platform gives them and get the results they need. They can do it. They can adapt it very quickly and get slice and dice whatever the platform gives them and deploy it to production. Flexibility is also. A positive thing here because they can adjust whatever template or whatever tooling they have in order to get the results they need and what you see is what you get. There's no hiding, there's no hidden implementation under the hood, so meaning when they need to see something, when they need to update something, they can just do it. easily. however, this brings us a couple of other challenges. So it does require a steeper learning curve. You need to, you don't need to be an expert, but you need to have domain knowledge into what you're operating the domain you're operating in. So for example, in our case, developers do need to understand Amazon ECS concept, ECS cluster, AB, and so on. It also creates fragmentation around deployment. Because each team can customize their own deployment method, this can lead to the fact that each one has a different way of deploying things, which in our case from a platform perspective, it can be hard to manage across the organization. And the last thing about life cycle management, it can become tricky, especially when we talk about templates. It can become tricky to scale this across the organization. So the trade-off here is about requiring more domain knowledge from the users, but teams can evolve independently. You got Summarizing these two approaches, it would be nice if we can blend them together, if we can provide a simpler way to get started, but without hiding the implementation detail upfront. When you need to update something, you can still do it. So let's look at the same entry point of what we started at the beginning of having an app deployed, an app package in ECR ready to be deployed. If we could provide. A simple way to get started, like a very light interface that requires a couple of more, a couple of parameters and no more, but it does come with a life cycle management, so every time we update something, this kind of service takes control of the entire life cycle operation, and in turn, it provisions everything you've seen in the previous sites, so the cluster, the the the the VPC configuration, the load by the ALB. And the integration with CloudWatch, but this is all kept visible to the users. And whenever we have another app in the organization, it automatically uses the same deployment mechanism and integrates and reuse the same resources that we already provision. This approach balances between the level of knowledge to the level of knowledge needed to get started and the flexibility to change things in the future. In the next part of this session, Jen is going to cover how Amazon can help you achieve all of that. Mr. Thank you Zahi. I am so honored. To introduce to you today Amazon ECS express mode. We introduced this feature just last week, uh, we made it for developers to experience ECS in a whole new way. Taking advantage of all the things that Zahi just talked about, of all the the years of platform experience that that ECS has has learned from. Um, and pass that knowledge onto our users so that you can stand up your applications faster. We started the way we do all applications at at products at Amazon. We worked backwards from the customer, uh, and we found that most ECS customers were implementing a really repeatable pattern, the same one that Saki was showing earlier, but, uh, a lot of that pattern existed outside of ECS. It included other AWS services like load balancers, auto scaling, domain names, certificates. Networking observability, um, so we asked ourselves, could we do what UCS does best and relieve customers of that burden? Could we take more responsibility, um. Let me show you what we've done. To create an app with Amazon ECS express mode, you only have to give us 3 things your container image and 2 IM rolls. We take defaults for everything else, but there are additional configurations available. As soon as you um. Complete the deployment, you do get an application URL. And that URL allows you to test your application end to end. Those options do allow you to, you know, configure the application, and we'll, we'll go through what all those options are in just a minute, um, but they're greatly reduced compared to the hundreds of parameters that you would have had to go through on all of these, these, uh, resources that we're provisioning that that I'm showing you on the screen now. Um, and express mode provisions all of those resources you need to stand up a highly available scalable containerized service using AWS best practices, and those include things like canary deployments, alarm-based rollbacks, uh, TLS certificates, auto scaling policies, cloud watch logging, AZ rebalancing, minimally permissive inbound security groups, all configured and wired together so you don't have to think about it just to get started. And at the end of this deployment, you have a you a live application. The command line view is similarly very simple. One container, two IM rolls. The first I am roll is the, the task execution roll, which if you're, you're familiar with ECS, you'll know is what we use to, to get your container image from ECR and to set up, um, your logging. The second one is new. It's an infrastructure role, and that's what we use to provision all of the resources on the right. Uh, and you might ask how did we come to this architecture, uh, the defaults that we selected, and we were trying to balance a couple of things. So one at AWS we love data. So we looked at what our customers configuring today and this was a very common pattern. Uh, customers were configuring services and not individual tasks. They were hooking up load balancers and as opposed to other types of networking. Um, 2, we, we took a look at uh the best practices and we talked to our principal engineers and solution architects and. We had a lot of really difficult conversations, a lot of heated conversations, um, and we looked at how do we strike a balance between helping customers get started fast, um, with dev workloads and test workloads, and also making sure that you're set up for the long term. Uh, because really we wanna make sure this is a place that, uh, you, you don't, you're not overburdened by, uh, what you would need in order to run a production workload, but it's also a place that you can get started and know that you have everything in place to run that production workload. So we do things like set up, um, all of the subnets that you would need and all of the AZs and make sure that we turn on AZ rebalancing so that when you're ready. Um, you can just increase your desired counts to 3, and, and now you are highly available in 3 availability zones, um, but we, we start with just 1 so that you're, you're not burdened with, with so many, um, tasks and, and, and how to deal with the cost of all of that. Um, all of this is, is built around trying to make things really simple and easy to get started. And this is something we're really excited about, um, something you don't see often in the AWSCLI. We know that a lot of developers love working right in your IDE or terminal. If you add this monitor resources, uh, flag to your call, uh, you'll get the following interactive experience. This is really similar to the console, so you can see what's happening in your ECS deployment in a really super visual way. Uh, and this is happening on the create, the update, and the delete of an express mode service. We feel like the observability of something that to go back to what Sahi was talking about, a composition is, uh, a really important concept. So in both the console and the CLI experiences you're seeing the iron of the resource, the status, any events or errors that are that we're receiving are being piped through to these, these views so that you have a really robust understanding of what's happening in the environment. that. I, I just alluded to this, but back to what Sahi was saying about life cycle management. Express mode is a complete life cycle. There is a create, an update, a delete of this experience, and if you are content with that experience, you never have to learn about the underlying resources. You don't have to go look at your ALB or your target group or even understand what all the settings are in the task definition. For novice users this can be a really great way to experience launching containers, um. We also do a lot of things here that that make things in ECS a lot simpler than they are today, things like updating the port or the health check. If you've done that today in ECS, you know that's a potentially disruptive and really difficult thing to coordinate. We handle all of that for you in express mode, um, moving from a public service to a private service or vice versa is a matter of handing us different subnets and pushing update. There's things that we do in express mode that are very complex that that become very simple. Let me show you. So for observability, we show you the normal CPU and memory, but because we have load balancer, there's also target response time 4 XX 5 XX. You also get your application logs. We added a new resources tab that you've seen in the timeline view, but it will also have the, the list of deep links. Now for the update, this is gonna show all of the create options. You have your container settings, port, health check, environment variables, secrets, commands, and the task role that you can add to access other IM, uh, other AWS services via IM for compute. You have CPU and memory, auto scaling. We have CPU memory and requests, um, men and max for networking you have some nets and security groups, and you can also name your own, uh, log groups and, um. Uh, log stream prefixes. Uh, when we delete, you can also see that the process of deleting. We're gonna delete any resources that are unique to that service. Sorry. You might be asking yourself about infrastructure as code. I would love to tell you about that. On the left we have the full cloud formation of the express mode architecture and on the right we have the express gateway Service resource with the required uh parameters. I'm really excited about that reduction. I don't know about you. Yeah, thank you, you can clap. It's a silent session, but you're still allowed to clap. Uh, here we have the optional parameters as well, but I've highlighted where you can bring in your own resources like a cluster or a subnet or a security group, um. This is uh uh gives you some flexibility to, to bring your own, um, bring your own definitions in. Now Sahi also talked about economies of scale. And I'm super pleased to share that express mode services that are deployed to the same set of subnets in the same account will share ALBs. Uh, we do this using host header-based listener rules. And not only do we share them, we also scale them. What do I mean by that? Let me show you. So we have a load balancer here that's been provisioned by express mode. And in the listener, you can see that we have 25 express mode services. We can share uh uh ALB with up to 25 express mode services. Um, they also all have unique TLS certificates. Now I'm gonna go back to ECS and to express mode and provision. A twenty-sixth service so we can see what happens. Just pulling everyone's favorite ingenic image here. And I'm not even gonna wait for the deployment to finish because it's already gonna kick off. We're gonna go back to the EC2 console, look at the, uh, the load balancers and see. That we have started provisioning a second load balancer. Uh, and so this is what I mean by scaling the load balancers. When you provision that 26 service, we will provision a second load balancer. When you delete the 26 service, we will delete that second load balancer. Anything that is unique to the service will be deleted. Um, now. There are a lot of services out there that will take a container and give you a URL. There are even some in AWS today already, so I, I don't wanna be shy about saying that, um, and I, but I do want to be clear about what I think is really unique about express mode. And that is that. You have access to all of these resources that are in your account and you have the ability to mutate them. And because of that, going back to what Sahi was talking about with abstraction and composition. And if you disagree with me here, I would love to go and have a philosophical discussion with you afterwards. It might be kind of fun, um, but I, I believe that express mode is a composition within an abstraction. You have the ability to stay within the abstraction if you want to. You never have to know what's underneath you. You get the simplicity of the abstraction and you, you get to have that. You stay within that world if you want to, but underneath that is a composition and you get access to all of the, the goodness of that composition and the flexibility of that if you want to. Now also You don't have to graduate or migrate in order to get access to that and in many services in order to go from one state to another, you have to draw that hard line. You have to say, OK, well I, I have to forfeit that simplicity in order to get access to that feature that I need and not with express mode. With express mode, if you want to go and and access that thing that you need, you can go and turn that on. You can go and configure that parameter, and then you can come back to express mode to continue. Updating your image or your auto scaling or whatever it is that you're doing on a daily basis, um, and I think that is very differentiating. To be that that empowers you as a developer to to keep a simple model but also have access to the full feature set of ECS, ECS that has been around for more than 10 years that has been operating. Very large services for more than 10 years and and all of these other services that we're provisioning as well. ALB is also a very robust service in itself. Um, you're, you're getting access to all of the features of these services, uh, in order to provision whatever you need to do with your application. You're starting simple, but you have access to whatever you need. Let me show you a little bit of what I mean. So we're gonna take an express mode service. Uh, go to the resources tab and make a change to the task definition. So I'm gonna go right into the Jason. In the ECS console you can actually edit the JSON. So I'm gonna add a second container definition. This is what we call a sidecar if you're not already familiar. A lot of customers in ECS, um, add logging sidecars. If a common one is called, uh, Fire lens, so you can send your logs to another destination rather than cloud watch. Um, and in order to update a service or your task definition in ECS you need to create a new task definition revision and then go and update your service. So that's what we're doing here. We've just updated the service. And we're watching that deployment happen back in the express mode, uh, service you. Now what we wanna see now that we've we've we've done that update out of band from, from the express mode service is can we go back to express mode and do another update. And make sure that that change was persistent. So we're gonna update the image in express mode. We just moved to the latest in my ECR repo. Go back to the Jason and look. My image is the new one that's the latest Shaw. And then my second container definition is there. So that's kind of the power is that you have the ability to to make changes in these resources we will persist the changes and you can continue using express mode for those simple updates to everyday changes. Um Like ECS itself, express mode is available at no additional charge. You only pay for the resources that are provisioned and in the context of this that it would be Fargate and uh the ALB itself which we're helping distribute the cost of across all of the the services we showed the console we showed the CLI uh API SDK we showed infrastructure as code. It's also in the CDK as an L1 construct. Uh, we launched Terraform last week. And I'm super pleased to announce that about 2 days ago, 1 day ago, we launched a new GitHub action, so you are able to take a GitHub repo. Use the other GitHub actions to build that into a container. Push it to ECR, and then use our action to push it to an express mode service and get a URL right from your repo. Um, so with that. I express mode was was built for developers to help them get started fast in ECS. We've also been talking to a lot of platform teams who see this as a way to accelerate, you know, their devx, to, to, to give an experience to, to their application teams or to stand up in small POCs for, for things that they're, they're careless about what the architecture looks like. Um, so even if you're a platform team, maybe consider checking it out. Um, But with that I think we also wanna take a look today at how platform teams uh are using Amazon ECS today and what are some best practices uh. GoDaddy has done an excellent job of that. Keith specifically, and, uh, we're really excited to show you what he's done. He did not have access to express mode or ECS managed instances or native Bluere, but, um, I think I've certainly drawn a lot of inspiration from what he's built, so I'm excited for you to see it. Thank you, Jen. Hi everybody, my name is Keith Bartholomew. I am a principal engineer at GoDaddy. Um, as Jen said, uh, a lot of what I'm about to show you today is how we've used ECS to build a platform, uh, for our GoDaddy engineers, and we did all that before features like express mode, before their Blue-green Canary deployments, um, and before managed instances. So everything that I'm about to see, recognize that it's about 10 times easier for you to do this yourself if you wanted to. OK, we did it the hard way and now you get to do it the easy way. Uh, so you may know of GoDaddy as a domains registrar that is the oldest business, that's really what we're known for, um, but today we do a lot more than that. Uh, we call our customers everyday entrepreneurs. They're the kind of person who has a side hustle, a business that they run on the side. Um, you know, it's not their main source of income, but it's something that they're really passionate about, and we provide services that help these everyday entrepreneurs do everything they need to do to run a small business. And so that does include getting a domain name, getting a website, but also running an e-commerce storefront or taking, you know, point of sale payments at a farmer's market or something like that. And so we call all of those touch points the entrepreneur's wheel. Now just like generative AI has changed the way that we as engineers and technical people work, it's also changed how our everyday entrepreneurs do the things they need to do. So GoDaddy Aero is our AI powered um experience that runs the gamut of every product at GoDaddy. So. From getting the idea for a domain name to generating a logo on a website and even doing social media marketing, we're taking these ideas, those back of the napkin business ideas that seem out of reach, and we're putting them into these everyday entrepreneurs' hands so that they can make every business idea more accessible than it ever has been. Now behind all these innovations are thousands of builders working tirelessly to um stand up all of the services that it takes to run something like this. And so that's where I'm excited to tell you today about GoDaddy's uh decentralized developer platform. Um, so it all starts with CICD where our developers do kind of whatever they want to. So some of them are running self-managed Jenkins, uh, clusters. Um, some of them will stand up GitHub Actions workers when they need to, and a few people have hand rolled their own CICD things because they think that's important. Infrastructure we really want to let developers choose, so there's a little bit of ECS, there's some EKS, and some of our teams are using OpenStack on-prem. And security we really don't want to get in their way and so we let them, you know, patch their vulnerabilities whenever they feel like it. Uh, it's none of our business, right? And observability we've got people who like Prometheus. Some people really enjoy that and they wanna use that and that's fine. Some people like elastic and they can do that too. This is a pretty cringe worthy slide, right? Can we all agree that this is not what we want to see in a in a platform? Yes, OK. So this is with some hyperbole, this is roughly what it looked like at GoDaddy before we started building this platform and so we, we took a lot of the good things that were happening in these things and we brought them together to make a centralized unified platform for all of our engineers. So our engineers have access to push button hardened private GitHub action runners that are automatically hooked up to our IM system that we have so they can just, you know, deploy with only literally a push of a button. Um, Katana itself, that's the name of our, um, infrastructure platform, is a managed compute platform that makes them, uh, like have push button access to ECS Fargate services, ALBs, and all the other supporting things they need there. Uh, we've unified our securities. We work really closely with our security and governance teams to make sure that all the lambda runtime get updated when they need to get updated, that all of the ECR images are, you know, free of their vulnerabilities and things like that, and we're able to do that through a really close, uh, coordination with that team. Um, and then centralized observability, we have a grade A world class observability team who I'm so lucky to work with, um, and they've done a lot of work to make sure that everyone using this platform with zero config gets all of their logs, traces, and metrics piped to our centralized observability platform where they can then go create dashboards and alarms and and understand how their applications are behaving. So the way that most of our engineers interact with this platform is with this unified single pane of glass. I know that's kind of an overused term, but our, our developers really enjoy this. Um, and to underscore the value of this, consider that there's over 2000 engineers at GoDaddy, and for each of those teams they're following the best practice of having a dev account, a test account, a stage account, and a prod account, and we have some others in there as well. And then also consider that large organizations are messy. Most engineers don't work on a single project. They kind of, you know, share their time with 34, or 5 projects. Since we have engineers who need access to 30 AWS accounts, they need to deploy that application to 30 different AWS accounts. Going around and clicking through all the ADOS consoles can be pretty tiring, and so we've brought all that together where our engineers have a single place to look to see how is this service doing, you know, how did that rollback happen? What were the logs that happened from ECS when this failed, um, how is this being impacted by an outage with a different app? They're able to get all that together in a single place. They also use GitHub actions which we manage a lot to do the same things from more of an automated CI perspective. This unified dashboard really comes into play and is really valuable when it comes to observability. I, I can't tell you how many times I've been paged and I get on the phone call with the product team and the first thing that I see on the screen share is the screen. They're they're looking at this to see how is the load balancer, you know, in USC. compared to the low load balancer in US West 2, you know, are these error rates spiking over here? Are they idle over here? And are developers really enjoy using this as a way to understand at a really big picture level how is everything across multiple accounts, multiple services, how is everything behaving? We've also built in generative AI support, so even though we've gone through a lot of effort to make our platform as simple as we can and easy for any engineer to understand, it's still complicated, right? And there's some times when there will be an ECS failure that we can't really wrap or abstract, and so we just have to show them the direct failure from ECS and the engineer doesn't really understand what that means. And so we've built an agent right. That dashboard that you just saw that has access to a bedrock knowledge base with all of our documentation as well as information about how we specifically tailor our infrastructure to their needs and it's able to use tools and function calling to reach into their account and live see OK how is your service doing? Let me read the deployment events. Let me read the cloud formation events and understand how that's going. And then it provides our users with a very detailed, not generic and actionable statement that they can then use to self-resolve their problem or in some cases they'll even use this with another tool to come create a ticket in our support channel where they can bring, you know, an entirely pre-triaged incident to us and then we can start working on a support issue when we need to there. So it's been very, very powerful for us. So at this scale, right, we've got hundreds and hundreds of AWS accounts. Accessing all those accounts is essential. Obviously we need to be able to see what's going on in them as a platform to understand how, you know, the health of our many services that we're managing. Our users need to see that information. Um, and we need to do that securely. And so the way that we've done this is by installing an agent, and this is not the AI kind of agents we, we named it that before they took that word from us, um, the old style agent, we run one of those in each account, and that, uh, agent comprises an API gateway and several lambdas, and we chose these because they're very low cost to run. When they're idle, they consume practically $0. I think we paid maybe like 1 penny for the S3 code for the lambdas. Um, and that's great because it means that someone who's using the platform and using us for our management capabilities, they're not taking on an undue burden just to use the platform, um, you know, so the, the control plane of what we're doing is as low cost as possible, um, but security is also critical here and so API gateway allows us to use IM authentication, uh, for that entry point where we can say only the IM role in our dedicated management account is allowed to invoke this API gateway and all that happens without me writing a single line of code. And I can't tell you how much that helps me sleep at night knowing that my code did not contribute to any problems. I trust the AWS IM engineers much more than I trust myself. Um, but we also then limit the things that this agent can do, right? So this is not carte blanche access to everything within the account, um, you know, we need to get select information about ECS, about cloud formation, about ALBs, um, but other things that are not any of our concern. There are not lambda handlers that are capable of doing that, so we're limiting our blast radius in that way and really the only right operations that we do are managing cloud formation stacks or um. What else do we do? We restart ECS services from time to time if you just need to, you know, sometimes rebooting fixes the problem. What our engineers get from all this though is push button resilience. So 2,000+ engineers, they come from a wide range of backgrounds. Some of them are really, really good DevOps engineers. They're intimately familiar with every AWS product. They're certified. Some of them could not tell you the difference between an ALB and an NLB, and we need to make sure that all of those engineers get the same experience. Um, so it all starts with Route 53 records that ensure that anyone who's running on our system is able to get latency-based routing between regions and then failover between those regions whenever there's an outage. And so some of our teams are operating in 4 and 5 different regions. Some are just in 2. It really depends on their needs. They don't have to configure this failover. They get it out of the box. Um, those Route 53 records then point to the networking resources that take you into the application. In most cases that's an ALB. We've also got teams who are using Cloudfront to help sort of provide more of a point of presence at the edge. And so we're integrating all that directly with the Route 53 records. And then you get down finally to the ECS service. And so for each deployment we're running a new ECS service and then we allow some fairly complicated. Routing from the ALB to the service where engineers can either do 50/50 splits if they're doing an AB test or they can use a 5% split for a canary or something like that so they're able to get a lot of these really kind of detailed and and fairly complex deployment strategies again at the push of a button. So key to that, what I was just mentioning with these these deployments, um, every deployment on Katana creates a new ECS service, and we do this so that we have a really reliable sort of warm standby in the case of any bad things happening. So let's say here that version version one of my service is what's running in production, and so all of my users are going to version one, and as little as possible, I really don't want to touch that service. Right, it's like the, um, Indiana Jones and the Temple of Doom. Like if, if it's not broke, don't fix it. And so we don't want to touch Service one, but the development team is working on version 2. They're making some changes and they need to deploy that. And so when they finally make that deployment and they deploy version 2, it stands up a completely isolated ECS service, a completely isolated target group, and service 1 is not touched at all. It stays, you know, exactly pristine the way that it was. Now you do need to know that version 2 is working and so with a specific header we have like X version 2 or a query string that says X version 2 or even a cookie, you could have your QA team test version 2 of the application or you could run automated smoke tests against version 2 of your application to prove that it's working the way that you want it to. And then finally, when you're confident with that, you make a small change, and the only thing that changes is we change a target group to point to version 2 and not version 1. Again, version 1 has not been touched, we're just moving traffic away from it. You can do this all at once if you want to go YOLO, or you can do it slowly with a percentage route to sort of ramp traffic up to to version 2 of the application. And so then in this case we've got version 1 running as a warm standby. So if when it comes under load version 2 starts to have a degradation, version 1 is still there as a fallback. Again, this is just built into the platform. This is a fairly complicated way to manage deployments, and our engineers don't think about this on a day to day basis, but they have used it in some really creative ways to have live preview environments. Sometimes they're not just running two versions at a time, they're running 56, up to a dozen, I think I've seen, um, you know, where they've got really busy teams who are constantly testing things and, and deploying these using these sort of dark release patterns. Now, as Sahi and Jen both mentioned, escape patches are really important for a platform, right? I think we've made a lot of good choices about the correct kinds of abstractions, um, and in most cases that works, but every now and then people need to sort of grow up from that or, or they have something that they really want to do in their own specific way. And so one example of the way that we do this is with WAF. So every load balancer, uh, that we provision has a WAF ACL, and I'm not sure if you've ever configured a WAF ACL on your own, but they're fairly complicated, like the, the JSON language for expressing rules and bypasses and the orders. It's a lot. It, it was a whole day of documentation reading for me. And so we don't require our users to know all of that. We just say, you know what, most of you are going to need a rate limit. You're going to need to allow and block certain subnets. You're going to need, we're going to give you AWS managed rules to prevent common attacks. But if you find that your app really needs a large body size and a request, you can opt out of those things as it meets your needs. And so most of our users are very successful with this, and they never touch the JSON language of a WAP ACL. But every now and then we get someone who has a really complicated IP set and they need to do, you know, something that this doesn't support, and instead of trying to build that into the abstraction and slowing them down while we build these features, we just say, OK, go create your own WAF ACL. Tell us the RN of the ACL that you created, and we'll associate that with a load balancer, and then we move on. And so they're able to take control. They, they, they inherit the responsibility for the WAF ACL, and they don't have to sacrifice any of their benefits they get from the rest of the platform. They continue using all that as they did before. Uh, we do this with a few other things too, so security groups, you know, we'll automatically configure security groups to allow the things that we manage to talk to each other, um, but if they have a complicated security group, they can pipe that in, or IM policies, we'll let them define that in line if it works, and then sometimes those policies get a little bit large to manage and so we just say, OK, just tell us the R of your policy, and we'll attach it to the task rule. Um, everyone at GoDaddy uses CDK. We, we have centralized on CDK as our, our management tool for all of our ADbus infrastructure, um, and it's very common for us to find developers that use Katana for what it's good at, but then they need to extend it, you know, they, they need to associate that ECS task with a Dynamo DB table or they want to change the behavior of something. And so they're able to use constructs to extend what Catana provisions within their own CDK code. And so you could say I'm going to create a new Catana environment. This is actually just looking up and adding to context, you know, values. What do I have here? The listener gets a random string that they can't predict ahead of time. And so we'll look that up live, get that context in there, and then they can use this piece of context to add their own custom listener rule later and and take the infrastructure that we've managed and augment it in a way that meets their needs. So all these guard rails and off-ramps are very important. It's all built around a governance system that I think one of my colleagues in the row over here is using. Um, we use cloud formation hooks to restrict what people are able to do, uh, so that developers really can't, um, step a foot wrong. So you can't create an ECS service that has a public IP. It just doesn't happen. And Katana builds on top of this governance system which gives us a lot of confidence that, um, you're not, we're not going to do something wrong as a platform. Uh, and, and anything that you do within the platform is going to adhere to these rules. Like I said, we're all using a common CDK tool set, and so that gives us sort of a lingua franca to use, uh, between teams. Uh, CDK is the way that we communicate. The Katana managed infrastructure itself is actually a very, very large CDK application. And so in in some cases users are able to just learn from our code and I'm fine with that. They, they, they can steal the source code and say, oh, I see what you did there and apply it to their own things. um, and then there's no magic, so there's nothing that our platform does that users could not do themselves. We don't have magic permissions. We don't have any, you know, elevated things that we can do this but you can't because only, you know, we're trusted to do it. We're doing things that users could do themselves and we're just accelerating their path to do that. Um, which gives them a really powerful off-ramp. So if they ever needed to take one of our cloud formation stacks and manage it themselves separately, they can do that and they'll have no issues, uh, doing that within our, our governance framework. Now at a company of GoDaddy size, managing infrastructure is important, but really where our engineers start to see the value is in how we glue that infrastructure to the rest of the business. So like I said, we have a very good observability team, and out of the box, every one of the the ECS services that Katana manages has a fluent bit sidecar that uses fire lens to send their logs to our observability system. Uh, we're working on a shared hotel collector that collects those traces and does the same thing there, and, uh, this is, you know, huge for our engineers. They don't have to do anything to get this benefit. It's just there for them. Compliance and security, like I mentioned, we work really closely with those teams to make sure that everything is always patched constantly and because of our, um, kind of oversight of the entire platform, we're able to do this at a scale that individual teams can't do on their own. Um, Gore is a big certificate registrar or certificate issuer and DNS, and so we work with our internal certificate API to, you know, let the engineer who's provisioned their certificate get that imported to the Rite AidWS account so that it works with their load balancer and get those renewed automatically. Um, and then experimentation. This is, uh, experimentation is a huge part of the DNA of GoDaddy, and you might think of experimentation as just a front-end concern of just AB testing in a browser, um, but we believe that even infrastructure changes are experiments as well, you know, version 2 of my application might solve a performance bottleneck that version 1 had, and that might improve a click through rate on a checkout funnel or something. And so every time a new application is deployed or a new instance version of an application is deployed on Katana. We send that data to our experimentation platform so that our data scientists can correlate that with their hypotheses and other data sources to see how that feeds into that. And then enterprise networking is tricky. It's messy. We've got a mix of AWS accounts on-prem. I think if you need on-prem access, I know how to do it. You have to attach yourself to a certain subnet group, something like that. Uh, for our users, it's a check box. We do these things to, you know, work with those teams and make sure that networking is as easy as we can make it. Now why ECS Fargate? Uh, there's a lot of compute platforms at, uh, at AWS. Uh, many of them support containers, which is good for us. Um, we chose ECS Fargate because it really meant the happy medium for cost management and flexibility for us. Management is very important, so we have a small team that runs our platform, about 7 or 8 engineers, and we have 2100 engineers that we have to serve. So if we're patching E uh EKS instances, uh, every day. Can't do it. It's not manageable to do that at that scale. So ECS Fargate gives us the ability to not have to manage or patch those instances, um, and with, like I said, this was invented before ECS managed instances, which changes that a little bit, um, but we also don't have to deal with scheduling or anything like that. And as someone who has previously run ISTO exactly once, I can tell you that this is so nice not to have to do that. But we don't sacrifice anything with our flexibility, so they still have full access to all their container settings, to VPC connections, things like Service Connect, and it's a good cost fit for the kind of services that we run because we have a lot of always on customer facing web apps that work really well for the way that ECS is parked. Um, so finally our lessons learned. Our users really value the single pane of glass. I underestimated how much they would enjoy that. Operating via cloud formation is kind of difficult. Cloud formation has a very static way of expressing infrastructure, and ECS services are very dynamic. You know, you, you make a deployment and that creates hundreds of events, and it's checking health checks, so there's quite a bit going on there. And so getting the impedance to match between those things has been something that we've had to put some work into. Um, but it does have its perks. If we ever get ourselves back into a corner, we can, you know, move traffic away from a region, delete all the stacks, recreate them exactly the way they were before, and the app is, is running, uh, as we want to, um, and we have to partner with other teams. We kind of call ourselves a platform of platforms. We're all thinking with a platform mindset, but I can't fix everything myself. And so, uh, you know, working with our observability teams, our security teams, our governance teams, uh, has really been essential to making a platform that meets the needs that our developers have. And so I'll lead you, uh, we had our philosophical tensions, so there's one more philosophical tension to, to share with you, um. I think it'd be a big mistake to think of your developers as one kind of person, to say that on this spectrum of being an AWS expert or an AWS newbie, that there's only one kind of engineer at your company. You're obviously going to find people who exist all across that. And so what we found is that to meet their needs you have to understand that they have a wide spectrum and build a platform that meets all of those needs that allows the AWS experts to thrive and have the control that they want and that allows the less AWS centric people to get the job done without having to, you know, know everything or get certified or anything like that. um, so as you leave, if you, if you're going to consider building platforms or if you're working on a platform of your own. Um, please, you know, consider making your platform meet as wide of a spectrum of your engineers as you possibly can, um, so that you can get success for a wide range of people, um, and I'll hand it back to Jen. Thank you. I'll say it again. I just, I, I feel like I've drawn so much inspiration from what Keith has built from Cratana. I know we're already thinking about things we wanna add to our road map from, from some of the things that he's done. So I thank you again, Keith, for sharing what you've built. Um, I hope you gained a lot from this session. Um, please try out ECS Express mode. Um, I hope you, you've learned a little bit about accelerating application development, about building platforms on ECS. Check out this QR code. It is specific to this session. You can download a PDF of the slides. Uh, we also have a link to the ECS immersion day workshop which now includes ECS express mode, and we also have a link to the GitHub, uh, action that we just launched. Um, thank you again and please fill out the, um, session survey. Thank you.