---
video_id: K49S79wOGl8
video_url: https://www.youtube.com/watch?v=K49S79wOGl8
is_generated: False
is_translatable: True
---

>> Thank you Peter. >> And it's exciting to
announce what we've been working on together. Today I'm
going to show you something that's completely new,
something the world hasn't seen before. It's a new category of
GenAI Foundation Models called real Time Live Visual
Intelligence. Take a look at the screens to my left and
right. We're all here right now at re:Invent. But what if we
want it to be all powered up? Earlier, when Peter walked on
stage, he looked like Werner and that was our model's art.
Decart Foundation Models running on Trainium3 live.
Every pixel you see right now is being generated on this
stage in real time. At Decart, we're an efficiency first AI
research lab, and we're building foundational models
for visual intelligence. Vision is how we see the world.
It's how we communicate. It's how humans understand each
other, and it's how robots will be able to understand our world,
to train themselves in generative simulations. This
this is what video and world models are all about. And it's
happening right now live. Because for the first time, we
could take foundational models for LLMs and video diffusion
models and get them to run at the same time with zero latency.
Trainium3 has been a huge enabler for our workload, and
creating this new GenAI category of real time visual
intelligence. At Decart, we have a lot of proprietary IP
that allows us to train and inference models much faster
and more efficiently, and we worked closely with the AWS
team to port our model stack, to port our infrastructure to
Trainium, to let us work with Trainium for building models
and running them. And we optimize it all using NKI
language, which Peter just talked about, and it made the
transition just much faster than what we expected. I'm
talking that we're on a path to getting to four x, better
performance in frames per second than what we can do in
state of the art GPUs. It's 80% Tensor Core utilization. These
are metrics we just don't hear of in AI. Now, the reason we
get this performance, it's a it's a result of how we combine
Trainium and our models. So the models that we train at Decart,
they have three components. An LLM that does reasoning
understands the world, a video model that understands pixels,
it understands structure, and an encoder that lets the the
two connect and run together. So usually we have to run these
in sequence one after the other. But we're able to build a
Trainium megakernel that we wrote and it got it to run all
three at the same time with zero latency on the same chip,
achieving maximum HBM memory utilization, tensor engine
utilization all at the same time with no latency. We're
already seeing how our models are changing how we behave
online. On Twitch, we have streamers that create new,
compelling experiences with our audiences. In shopping. Imagine
you see this cool lamp, and before you buy it, you see it.
Next time you watch a movie, it gets placed in that movie just
for you. We're trying to make sure that every product before
you buy it online, you can try it on on yourself and your home,
see what you feel about it. See how it looks. One of the
experiences I love most is basketball. So you're watching
the game and to keep your kids engaged, you let them watch the
Decart AI cartoon version, which is being generated live
from the match. We're taking AI and we're putting it into live
sports in the arena, in your living rooms for gaming, we're
creating a completely new foundation where it's games
that generate themselves as they're being played. And
frankly, everything you just saw, all these experiences, we
didn't come up with them. It was builders. It was developers
that took our API. You guys know your industry's way better
than we do, and you can understand how to take these
models and extract value out of them for your customers, for
your industries. If we step into the future step a few
years of the future. I really believe in taking this tech to
robotics and simulations. Robots are already using models
like this to envision infinite possibilities, infinite
outcomes on the road, in homes, factories, manufacturing, and
in the future, we'll be able to train robots fully in
generative simulations before they ever hit the ground. And
when they hit the ground, they will use live visual
intelligence to understand the world around them, to
understand the human environment. I'm really excited
to be here and announce with AWS this new category of GenAI
Live Visual Intelligence. We're taking it to every industry,
every market at any scale on powered by Trainium3 using the
cart models. Everything we build, we put it on our API.
Every few weeks, we launch it at our mobile app and the world
is shifting for the first time. We can take stuff that's in our
imagination and connect it to what we see with our eyes in
reality. Live. I really can't wait to see what all of you
build with it. Thank you all.