---
video_id: P2YX43I8GFI
video_url: https://www.youtube.com/watch?v=P2YX43I8GFI
is_generated: False
is_translatable: True
---

OK, very good morning to everyone. Welcome to Rainman 2025, and it's a pleasure to welcome you all and to our session on accelerating development and deaths of cops with Amazon Bedrock and Quiro. There's a funny thing, you know, 2 weeks back. We had to kind of quickly change. From Amazon Que to Quiro to give you, you know. Feel about how to use Quiro and Quiro CLI. So we've been chasing a lot of people to get the things updated. Uh, so finally, we have some uh good content to show you on the latest uh services as well. So, um, I just wanted to understand, you know, uh, the people here before we get started with the introduction. Um, can you tell me how many of you in the room are developers? Please, uh, raise your hands. OK. I see quite a lot of uh development community. Fantastic. And can you tell us uh how many of you like hold a title as a leadership, like technology leaders or managers? Wow. It looks like more than 50%, right? And how many are solution architects? OK, looks like, uh, like Fabio. You all have multiple caps, fantastic, right? So, in this session, we're gonna show you. How um you can accelerate software development life cycle using agents beyond uh just code generation, right? So that's what we're gonna cover today. Um, so my name is Dana Verdiwele. I lead the EMIA Partner Solutions architecture team, uh, for strategic consulting Partners. I'm, I'm based in London. I'll let my co-speakers introduce themselves. Hello. My name is with his accent, I'm Brazilian. I support partners in the Dahe CE region. I'm located in Munich, uh, so I'm part of the Danas team, and also I'm here probably because I built part of the system we will see later on, part of the demo, so happy to be here. Thank you. Hi everybody, my name is George Knein. I'm the CTO for Oiva Cloud and also an AWS ambassador, and uh I have something great to show you, uh, as well, uh, during this session. So hang in there. Thank you. So let me give you a bit of story, right, uh, why we are here and how we actually landed here, right? Um, so, a year ago, um. We started getting a lot of questions from our uh partner network and as well as customers, right? Um, so, tell us how to use, you know, Bedrock uh to solve certain use cases or certain adoption issues, right? Tell us how to use MCP. Tell us how to integrate with the existing tools, the data silos, right? So, then, me and my team, we took this as a mission. We started creating uh quite a lot of architecture patterns, um. Design best practices and, and we also went and started creating some sample codes, um, you know, sample applications to show the art of possible, right? And then slowly, slowly we start seeing our partners start adapting this and then we saw a trend of, you know, creating a platform-driven approach, right? By, you know, bringing all the different architecture patterns that we created. And then we saw some of our partners fantastically created a platform-driven approach to address all the different personas within the software development life cycle, right? Not just, uh, you know, limited to the developer community. And that's where it becomes even more interesting, right? Then, uh, our partners started creating their own offering and offering that through the marketplace, and some of them are combined with our customers. And that's how we actually landed here. So we wanted to uh share our learnings, best practices, and experience with you, and we would love to hear back from you in terms of how you like to use it for your organization, right? Uh, so when you, um, At the end of the session, you will have a clear takeaway in terms of how to do this transformation within your organization. So stay tuned. We're going to show a lot of QR codes for architectures, sample codes, and Fabio is going to show a demo, and then later Joris is going to come and explain about. The Zybia platform as well, right? So In the next 50 minutes, we're gonna cover this. We'll start with the traditional SDLC challenges. Um, I'll quantify with some financial impact in terms of what we see from the industry, and then we'll move into why agent TKI is unavoidable in, in software development life cycle, right? And why Gartner in the recent Magic Quadrant says, The integration of Agentic AI is very key across the software development life cycle, and then we'll go dive deep into the Platform-driven approach using various architecture patterns that we created. So Fabio is going to introduce some of our virtual stakeholders like Emily, um, Anna, Adam, and Chris from different personas in terms of business analyst, solutions architect, right-hand developers, and Devsar corps engineer, and Fabio will also run through in terms of day in the life scenario. Obviously, uh, we have a limited time today, so we're gonna show you a few demos and use cases, but we have, you know, 2021 plus architecture patterns are actually published available today, um, so we will share the QR code as well. And finally, J is going to talk about how Delta Airlines have achieved this remarkable journey with Zybia transforming their SDLC life cycle, right? So with that, let's get started. So Let's be honest about where we are today, right? Most of the development team spend lots of time in terms of repeated tasks, manual activities, and missing documentations, which is resulting in a productivity drain. And let me have, you know, also share stats from one of the verified industry analyst report, right? 78% of the software engineers spending at least 30% of their time in terms of doing this manual repeated task. So, let me give you an impact in terms of dollar value, right? So let's take a team size of about 250 people, right, in, in a, in a software development team. If I take an average salary of about $107,000 per annum, the financial impact we are looking at is about $8 million in terms of lost productivity. Another report from Atlassian states that 20% of the development team. Lose their time every week due to inefficiencies such as constant contact switching and technical debt accumulation. If I do the same math, it is another 5.4 million in terms of lost productivity annually, right? So if I sum it up, it's about 13.4 million, right? It's definitely. Much bigger number I'm talking about. Um, I'll give you an example, right? So how many of you here, um, your teams are spending at least more than 40 hours in a week whenever there is a tight deadline to release, uh, you know, products or any production issues, rice hands, yeah, that I see, right? So this is the burnout crisis, right? I'm not even counting that fact, how much is the money spent or the lost in terms of productivity. And then the Atlassian research also states that If you provide a better developer experience, and you will see a 30% productivity gain for your development team, right, which means that we are talking about the lost opportunity cost, right? So, these are something which is I'm not factoring in, right? So this is not a sustainable momentum. That's why the AI agents or agentic AI is so transformative. So they don't just make the old process faster. They fundamentally change the process and make it better and cost efficient. Um, In terms of Gartner Magic Quadrant for AI code Assistant, and this is the one which got published in September 2025, right? Um, so, Gartner found that AI tools are transforming everything from requirements gathering to design to testing to development to deployment and operations. And, but here is the number that you should make a note of, right? By 2028, 90% of the enterprise software engineering teams will start using, you know, AI code assistance, right, which is a big jump, right? 14% in early 2024 to where we are looking at in the future, right, which is 6X increase, just 4 years, right? So And this is happening, right? This transformation is happening. Whether the question is not you are, you know, embedding AI across the SDLC, right? So this transformation is gonna happen with or without you, right? So the question is, Are you gonna lead this transformation or we're gonna be following the competitions, right? So, so you, many of you will be thinking, Dana, this is great, right? How about the real world challenges? That is exactly why we are here, so we want to share with you the Experience and the challenges that we learned from our partner network and customers. Let me bucket this into, summarize into three categories, right? The first one is the tool sets and development. The development teams face primary challenges in terms of fragmented planning and fragmented development environments, so which means it creates disconnected tool sets for them to actually collaborate better. So therefore it slows them down in terms of the release cycle and also creates a barrier for team across the team-wide collaboration. So this is one common challenge that we learned from our experience. And then the second one is the integration and the interoperability, right? We already said that there are fragmented environments, fragmented tools, which is also creating data silos, right? So, therefore, there is a need for everyone to actually, how do you orchestrate this complex tooling data, right? So, nobody has the time to do this, manage this whole orchestration requirement, right? And then the last one is enterprise-wide security, observability. And scalability, right? So again, there are teams who are lacking in terms of the limited monitoring and control, so which again poses them a challenge when they start scaling into production, right? So POC is great. Now I need to move forward, scale it from, you know, 10 users to 100s of users and thousands of users, right? And that's where security compliance challenges hit. And of course, you know, it is important for everyone to start, you know. Observing the performance of the agents, the behavior of the agents, right? So all these things are the enterprise level challenges, and this is exactly why AWS developed agency AI services and, and why it is important to kind of have a platform-driven approach to address these challenges. So, how many of you in the rooms are already building agents? Can you show off hands? OK, and how many of you have deployed the agents in production? OK. So I see about like 30, 40% just in the benefit of bringing everyone on a common page. I'll quickly cover a couple of next slides to give you a, uh, um, you know, some foundation, and then we'll dive deep into the architecture, right? Um, so, at AWS we define um. Agents or uh agentic AI um as autonomous software systems that actually leverage to reason, plan, and complete tasks on behalf of humans or you know, systems. I'll give you an example, right? I'm sure most of you are using a generative AI today, right? So, generative AI, you give a prompt, it gives you back, you know. The response, which is one shot interaction. On the other side, AI agents, it can actually read your business requirements. It can read your user stories, epics, right, and it can communicate, interact with your tools like Jira, Confluence, and understand your, let's say, non-functional requirements, understand your standards within the organization when it comes to compliance, let's say GDPR, for instance, right? And, and therefore, It can help you to create codes. It can help you to do the implementation, do the documentation. So all the challenges that we talked about earlier in terms of losing 30% of time due to repeated manual tasks and 20% of time lost every week due to technical debt and the constant context switching, that is where the gentic AI can help you. You know, um, avoid those repetitive tasks and improve the productivity, right? So this iterative problem solving capability is what exactly transforms, um, the every stages of SDLC. Now, let's quickly look at in terms of how ANT AI works, right? So, we have agents in the middle, right? It can be hosted on a uh AWS fully managed um service, which is Amazon Bedrock um uh Agent Core, which Fabio is gonna talk about shortly, um, or you can even run agents on a uh Amazon EKS uh ECS platform. Uh, where you, you need to kind of manage everything, right? So it has got 4 key components. The first one is the memory to retain the context between all the personas, between the sessions in terms of short term and long-term memory. And then the second component is tool integration, right? So that the agents can execute tasks by writing, you know. issuing um all the tasks in Jira, right? Based on the user stories and Epic. Just giving you an example, right? And you can even, you know, integrate with your ERP tools or CRM tools where the data is residing. Right. And then the third one is the planning capability to kind of, you know, go through any of one or more of the options in terms of, you know, reflection, self-critiques, right, chain of thoughts so that the entire task execution can be planned properly. And then the last one is the actions, right? The actions can be performed uh automatically or with human oversight, right? So all these components work together to actually help agent complete the task. And now let's look at, you know, what are the key design best practices that one should actually consider when you are actually taking these agents to production, right? Again, this is purely coming from our learnings that we've been working with partners and customers. There are 4 key buckets we summarized. Um, the first one is the selection of framework, right? Um, so, it is important to consider the complexity needs, uh, um, of your project, of your application that you are building, right? And, and what is the level of modularity that the framework can offer you. And the integration capabilities and the memory management capabilities. And then you also need to factor in things like, uh, you know, scalability, performance things, right? These things will determine, you know, how, uh, you know, best your agent care is going to help you to scale in production. And then the second one is the tools and knowledge base, right? Common tools, capabilities like, you know, web search, um, code interpreter. So you need to actually evaluate those capabilities and, and making sure that provides you a reliable output, what is needed for your application, right? So you need to test that, um, you know, ability and, and check the reliability. And then the third one is selecting the LLM, right? So, again, the reasoning capabilities and the execution speed, all that matters for your application. And that will also determine what is the model size and performance, which will also lead to the cost of the influencing. And then the last one is the execution technique. So there are many options available within the react, plan and execute reflection. So you need to decide which one is better suited for your needs. And then finally, there are frameworks, agent care frameworks. It comes with its own determined way of doing workflow automation, or you may have to do a custom workflow automation. So again, you need to think before you start. Using any of the agentic framework. And finally, make sure you all aligned the complexity needs of your project and use cases to the selection of LLM. So, AWS meets where you are in your agentic AI journey, right? Um, our vision is to provide the best place to build and deploy most trusted and performant agents in the world. So at AWS we understand different organizations having different needs. So, so therefore, um, We are offering three distinct approaches here, right? Just follow along with me. The first one is for those who are seeking specialized solutions, right? So we offer the agent power applications. So that's the first bucket. So within that, there are 3 services we offer. One is Amazon Quick Suite. You might have heard that recently launched a service which is mainly focused on enterprise productivity and workflow automation. So the good part of Amazon Quick Suite is it has an inbuilt connection and integration with all the tools that I talked about, right? So It is a platform for no code or low code for you to quickly create a workflow automation by integrating with different tools and the data available within your environment to get a specific task executed. And then the second one is the uh Quirogentic ID. So Faabio is gonna dive deep and show you the console and the use case. Um, it's, we, we are taking this Quiro into the next level of spec-driven development. So, you just give Quiro, uh, you know, what you wanted to build, just follow along, it creates the implementation for you, right? And then the last one is the Kiro. It is a command new command line driven interface powered by Agentic AI from AWS, and it enables developers and IT professionals in terms of helping them to scan your codes for errors, do security vulnerabilities, helps you to fix errors, and automate the workflows in a few seconds. So this is one bucket, and the second bucket is the Teams wanting a kind of control and convenience. That's why we offer a fully managed. Amazon and Bedrock agents, right? And that provides the built-in foundation models along with the archestration capabilities, right? And then the third one is for organizations who are actually wanting maximum flexibility, so we offer a DIY approach. So right now there are two SDKs, AND framework strands and, and NAAC SDK. So with this, you can actually build your agent, and for all these options, one or more options, uh, you're gonna see here from our partner um ZyBia today, how they are actually using this to build a platform and, and drive at scale, right? Um, so for all these things, you need a platform to kind of scale from literally 0 sessions to thousands of sessions, right? And that's where Amazon and Bedrock Agent Core that comes in, provides you, you know, capabilities such as Identity and access management, the gateway capability, observability, and, and then the tools that we talked about in terms of code interpreter, web browsing capability. All these things helps you to manage the agentic development life cycle. Right. So Now, let's talk about uh the architecture, right? Um, so the, as I said, we started creating the architecture patterns for each of the personas and then slowly we started seeing there is a need to actually bring all those uh architecture personas together and give them a UI and mainly for the non-developers, right, who are not using the IDE for example, right? So that's where we saw there is a need for building a platform-driven approach. So I'll quickly walk you through the solution approach. Again, uh, this is more to inspire our, you know, uh, customers and partners. It's, it's not a kind of fully product, uh, or something like that, right? It's a reference architecture, best practices, uh, is published in AWS samples. It's open source today. Anyone can actually, uh, clone it, modify, customize it, and test it and use it for the way you want, right? So we have a user persona. We created a custom UI based on React JS application and, and we use the cognito to manage the you know the authentication and authorization because there is a role-based access needed, right? You don't need to necessarily give every full rights to a business analyst, right? Rather you wanted to restrict them to a specific task, right? So that's where the role-based access comes very critical. And then the back end is we created, uh, you know, the SDLC agent uh using Bedrock uh agent. And which is uh inbuilt foundation models with archestration capabilities. So we integrated this as an MCP client um integration and the fast API with our uh UI, right? So now, let's move on to the patterns that I was talking about, right? So, we first we started creating an architecture design pattern for the business analysts to help them do the requirements planning, um, you know, more efficiently and effectively, right? Um, so we created an integration between Confluence and Jira using MCP server with Bedrock. And then we connected that to the MCP client. So what we realized when, when we started doing this, uh, before we get the next persona to start, you know, taking from what the business analysts have created, so it is important to kind of hold that, you know, output, right? So that's where we created another pattern called this project knowledge base, right? And this gives you a historical data of every persona who, what they are actually working on, which project, what Epic user stories they have created, what are the acceptance criteria, how you can track the traceability back, right? So all those information that they generate from the first architecture pattern needs to be stored, captured, and that needs to be reused, right? So that's where Uh, we use Amazon Bedrock Knowledge Base and, and Amazon S3 vector. So we created an MCP server and we integrated that back to the SDLC agent. And then we started creating other patterns, which is the architecture design pattern mainly focused on architects. So here they will basically able to create architecture diagrams using DraO. They can create open API specifications, and then the next persona is the developer persona, right? So here, For Quiro to kind of have all the information and create your specification in the first place. So again, we are actually integrating that with the knowledge base so that um it can be able to understand what was the requirements of the project and what is the design that was created by the architect, and, and so on and so forth, right? So then we were able to give the kind of entire flow, integrate each one of the stages of SDLC and then the testing persona. Again, the same integration that we did using MCP server. So in this case we took Jmeter because most of our customers, we saw this is a trend, and you can actually create literally any other MCP servers or the MCP servers that are available openly, right, the open source, and that's how we created each one of the other personas. And the last one is for maintenance. Um, which again, we used the publicly available MCP servers from Splunk, pager duty, um, and we integrated that with Slack so that um your ops engineer gets notified when there is an anomaly in your application automatically, right? And then it gives you an opportunity for ops engineers to further do RCA analysis. So that's where Bedrock comes in, do the RCA analysis by gathering all the logs from Splunk, right? And also, you know, Takes all the knowledge bases that are available in your ITSM tool. So in this case, we used pager duty. If you have Service now again with the MCP you can very well connect with the knowledge base so that all the previous incidents and the resolutions that took place, right? So the agent can be able to read and then it can be able to suggest you remediation recommendations, right? And, and that's how the whole workflow uh starts moving in. So that's a QR code. Where you can actually scan and see all the patterns. As I said, we have more than 20 plus patents and uh multiple use cases, but we have many more coming up, right? So, uh, make sure you store it, save it, um, we will share again once, once more at the end of the session. So let me quickly summarize in terms of the uh architectural benefits. So, uh, by taking this platform-driven approach, it enables you to implement AI across the software development life cycle, right? And it also provides you an ability to add, remove tools, um, you know, break all those fragmented uh things that we talked about in terms of data silos, um, and also by uh making an UI so you can able to customize the way how you want for your uh users, for your personas and organization, right? And then, um, We've showed you the project knowledge base to hold all the information throughout the software development life cycle so that the information gets passed on. The agent only refers to the MCP server for authorized information. So that's the way how we are grounding the context and avoiding the hallucinations, right? So these are some of the problems that we actually solved in real life. And then we're letting agents to manage the complex hawkeration here, right? Because the agent can identify based on your need, which MCP server to choose, right? Rather than you, you know, creating your own orchestration problems, right? And finally, using the guardrails features that are available, we use um to, you know, enable the responsible AI and other aspects. So, we are going to come to the demo time. I know I'm running a bit late. So, for the demo, um, we, we are taking uh uh in any company reads as an example scenario. Um, so currently, uh, this company is going through a traditional SDLC approach, um, which is resulting in the software releases taking 18 months of time, and that is mainly due to, you know, challenges that includes, you know, Multiple endless requirement meetings and the changes that happens to the design documents very often and therefore you know it gets outdated before even the coding begins, right? and the team could not avoid the scope creep and budget overrun, right. And in this demo, we're going to show you how to use the platform-driven approach so that any company leads can accelerate all the SDLC stages and reduce the release cycles to 6 to 8 months. Thank you, David. Fabul. OK, thank you. Thank you. Hi, hi, he invent, who wants to see a demo? So yeah, I'm joking. I think you don't have a choice. I will show anyway. So uh now, well, let's go with me. We'll go through uh some of the SLC phases and we'll see how uh our approach actually is changing the lives of some of our collaborators. So then let's come, let's see how it goes. So first, uh, let's meet Emily. So, Emily is our business analyst. Um, she, she task is really to go and to create Epics, from Epics, create requirements, uh, to write acceptance criteria. Uh, so basically she uses, uh, the usual tools like a knowledge management system like Conference or Jira for, uh, the project management to create like the tasks. So, let's see how uh actually uh ArgenskiI system can actually help her with her tasks and uh sometimes, for example, she's the building like functional and non-functional requirements, she may take like many, many. Hours days sometimes to even to create like no ambiguous requirements so how identically I can actually make this from a couple of hours or days to a couple of minutes. So let's take a look. So, Emily's journey starts in confluence, as the majority of our work that normally starts in a knowledge management system. Uh, so she writes requirements in plain English, as usual, so no requirements and no functional and functional requirements, and then she can move, for example, to our uh platform. We have here represented all the different S LC stages. So you have here on the right-hand side, you have a chatbot where most of the work will be done. She can ask, for example, uh, to the agents to extract the requirements and help her to build the epics from, from extracting from confluence. So, the system is using an MCP server to con contact con uh connect and to extract the information, and here we have it. So we have like the user authentication epic, we have the book catalog epic, uh, so all of them with different requirements inside. So you have like a user hesitation, authentication. The book catalog epic, we have like different features. So, With the integration between confluence and Jira so this information can be extracted by the agents and put back on Jira as tasks. So, uh, then, so you can see e-commerce features, social features. It's quite a complex system. Uh, so now, what we can do, so what Emily can do is to ask the agents to start to write acceptance criteria. So, based on the organization, best practices. So and here we have it. So it's using like a given when then format for the different epics for different requirements, and this can be populated back to Jira as well. And as Dana said, so what is cool about this platform, we have a central knowledge base and Every persona is contributing for this knowledge base. So, everything that we are building now is populating the central project knowledge base that will be shared among the different personas. So, moving on, now it's time for the team to get a little bit more technical. So we will meet Adam. Adam is our solution architect, and uh his task is to combine the, the work done by Emily, the functional requirements, and is to start to add like more information about the non-functional requirements. And also follow the organization policies, best practices, compliance, so he needs actually to digest all this information and start to build architectural diagrams, documents, specifications, contracts between different systems, so he uses some of the tools like diagramming applications, tools. He needs to go through sometimes to hundreds of documents to go and to distill that as architectural documents. So let's see how AI powered solutions can actually help him to accelerate his work. That sometimes can take like a couple of weeks, sometimes months, depending on the complexity, to come up with some of the solutions. So again, we are back now to our solution to our AI genic platform, and we have built some shortcuts to help Aden to go through to some of the tasks. So for example, here per Epic, we have a button to, for example, to generate a solution diagram, and this will create the nice prompt in the chat. And this will trigger the process and combining the Agenic AI system with MCP servers to create the diagrams, we have something like that. So we have like a diagram actually in this case with a serveless solution, so it looks legit. So we have 3 buckets, we have CDNs, we have servers, we have lambdas, we have databases. Adam can even, like, for example, continue the discussion. This is basically based on the uh organization uh best practices, but can, for example, you start to change the architecture for something like containers. So maybe moving on. Part of the uh Adam uh role is also to build like some contracts, so using open API to build open API specifications so he can go back. Select one of the shortcuts and for example here we have to build some of the contacts for one of the epics so we have like 4 endpoints, we have the schemas defined, the security defined, so this is normally a Jason object as you can see. The open API specification, who likes to write Jason specifications by hand, isn't it? So the entic system can help you with that and uh even like highlighting the human in the loop, so Adam can actually go and preview the, the results using a tool like Swagger in this case, and we can see here we have like get post, get my ID. Put the lead by IG, so the different end points for that particular epic, everything created by the agent, and now again part of our central knowledge base, what helps them to build up the knowledge about this single project. So, moving on, so our next stop is to talk with Ana. So Anna is our developer, so she is a lead developer in our any company, fictional company, and She leads her team and some of her tasks are to go collect the requirements basically now built by Emily first, and now all the functional and nonfunctional requirements documentation built by Aden. Put together and start to build the low level design artifacts that helps her team to start to split the development in development tasks, so setting the environment, setting the CICD pipeline, creating the documentation, so. There is a lot of documents and as you can see the things start to compound so we have like the requirements we have the design, so everything that needs to be distilled by. To go to build the software, so let's see how we can help Anna with those pain points. So, Anna and her team, they are developers, and we know the best place to write code is an ID. So that's why we will move to her favorite ID, Quiro. Uh, Quiro is our Aziny ID, um, and basically, Quiro is MECP compatible, so we have built an MECP server that connects Quiro to our central knowledge base. And she, because of that, she can do things like she can ask, what are the active projects now in our knowledge base, ask details about uh uh uh a single project like the any company reads, and with that, uh, the system will start to show in the ID without leaving the ID information about the different ethics that are available, the functional and the functional requirements. Uh, and when she's ready, she can, for example, even ask to start the work in one particular epic, and so in this case, we will start to work on the book catalog epic. So because of that integration between Quiro and our central knowledge base, you will see the information about like the contracts, the build by Aden, the functional functional requirements, design decisions, entities, end points as we saw before, and As soon as, as soon as we, uh, uh, Anna is ready, she can ask for example, to Kiro to start to build the, to use the spec-driven development approach to break out the uh requirements in uh requirements, design and tasks, as you can see. Here we start to help then to uh split now to design the presentation layer, the business logic layer, the data access layer. So, This is about the technological stack, so front end, tables, entities, so everything will be part of the project now, part of the repository, and after that she can, for example, even start to ask to split in technical tasks. And here you have it. So now the project is empty. So Quiro is actually creating tasks to build the project from scratch, so to build the different directories, set up the Quakus framework, it's a Java application, the front end. So that's basically now, so. Can start to click on the different tasks, and this will start. It will provide Kito access to the terminal where Kito will start to take over and start to run commands to build the project infrastructure, the directory, the different microservices, and so on. That takes a while. So that's why we will move a little bit forward in time. Yes, so here we have it. So after They're clicking some tasks and moving forward so we can see key to help to build different microservices, so we have models. We have repositories in Java, as I said before, DTOs. Repositories. Services And even tests. For all the different microservices we have in this epic. We have deployment scripts. We have the front end. And even like the CICD pipeline, so in this case, we are using Gitlab, so Kito helped to go and to build the CICD pipeline. And as soon as we deploy these codes, so we commit these codes to Gitlab. Get a view Run the first time that the pipeline, so we have like tests, unit tests, security tests, we have the setup of the infrastructure, we have the build of the different microservices, and finally, the deployment of production. As you can see, we even have like some uh security tests, some security scans. On the first stage, so that is probably already making our Debse Cops team smile, um, and talking about them, let's meet our next one. And our next persona, Chris. So Chris is our DevOps engineer, and uh he has like many, many responsibilities. So, he needs, he is responsible for the security monitoring. Performance, uh, infrastructure management, compliance and maintenance, so many, many responsibilities, and she needs, he needs to access many different platforms. So cloud platforms, monitoring tools, uh, login tools, ticketing systems, communication tools. So there is a lot of overheads, so cognitive overhead really. So with all these different tools and time is in production. Time is, is matters a lot. So he needs to be 24/7 available. He needs to have like a incident response. Access to different systems, the data that helps him to solve his issues. So let's see how AI can actually streamline his workflow to help him to solve the issues more efficiently. So here we are moving to Kito CLI. Keto CLI is our CLI-based agentic tool, and uh with keto CLI he can, for example, in plain English, ask the tool to check the logs, see what are the issues happening in the last 7 days, and categorize them in um uh priority. So the plain English request is translated in request to the. ASPI using another server. And this will bring back information about what are the issues that are happening. And here we have it. So keto, even like classified and different priorities, the different issues, and with some recommendations. So let's move back and take a look. So we have for the high priority issues, we have some issues with our security token with the refresh of our security token for the medium priority issues we have some bad actors that looks like the eye scanning some endpoints, so maybe something that we need to take a look. So, and uh Kiro even like provides some recommendations, what to do next. So Chris can now, even like in a proactive way, you start to plan what would be the next best actions. And we have like a, a couple of different AWSS partners that are actually providing official MCP servers that you can, for example, even connect his Slack with Page duty, Splunk, uh Dataoc to have all the information in a single place, so he can, for example, see what is going on in production, the issues that are happening right now. Can even check the logs, the metrics, and based on the knowledge base, again, the run books, the playbooks, see what is the best next action and what is the issues or maybe the risks related with any of the mediation actions. So with that, Greece is well prepared to do his job. And I think that's it. So we have done it. So I think in 15 or less minutes, we probably went to from from scratch, from zero to a production production ready application deployed with security to all the capabilities. So, and I think that the best part of this demo, it is available as open source, so we will uh share the QR code at the end. So stay tuned. So Dana will probably show that his source page, the source slides, so you can take a look, see, learn, and who knows, maybe even like build a product out of it. Uh, so But maybe let's go and take a look a little bit at what helped us to, with a very small team to build this uh sample you can see here today. Uh, so. Even hearing from customers and partners, we understand that AWS start to see that it's very important to have an infrastructure that is not only your way to build Agent AI systems. I think even like Andy Jesse in the shareholder letter in 2024 said something how speed matters all the time in all industries. So I think that things didn't slow down since then, even the opposite. So you cannot like have like a a a platform that will slow down taking care of the small details, but empowers you to build complex applications, complex agentic applications, and this platform needs to be flexible. So this adapt to your technical stack, not the other way around. So whatever is your choice, the platform that needs to support you with your choices and it needs to be scalable, secure, provides you with all the telemetry so you can have an enterprise grade application deployed on production. So that's why we leased Amazon Bedrock Agent Core. Agent Core provides different modules, so you can use them together or separately. We have, like, for example, agent called runtime. That's the place where in our sample you are running our agents, we are running our MCP servers, so it's a selfless runtime that provides you uh all the security. So basically every agent or MCP server is running in a session. For security, it is a related session for that particular request. We have the agent core gateway that's where you will expose the different tools to the agents so it provides a seman search even to not overload the context of the agent looking for tools to execute and also provides some built-in tools like Agent Core browser. That will provide like a sandbox environment for the agents to access the internet, to access like a server-based like a browser who can make actions and execute actions to the outside world, or even like a code interpreter that will provide also a sandbox environment where he can write codes. Execute the code and receive the outcome, so to have like a more predictive data-driven outcomes. You have like identical identity to secure the end to end um conversation between like the user from the user to the agent, agents to your tool for your MSCP for example, based tools, and from the MCP to the outside world I think called memory for your session memory, for your long term memory. So to have like a place to store, for example, in our case, information about the preferences, the project, so to have a more long term memory, even to share between different agents, and finally, identical observability. So to have information about the telemetry, what's going on, how many tokens were consumed by the different agents, so to have like an open telemetry even based information so you can guide the optimization of your system. And now I think I will leave the stage to my colleague the audience that will go to our customers' story. See you. Thank you. So, CBI is a global technology firm founded in 2001, and we're active in over 16 countries. And our experts help organizations from Through software engineering, cloud, data and AI, DevOps, and agile transformation. We work with the world's uh some of the world's leading enterprises from top global banks, and major airlines, and retail and healthcare. As an AWS premier tier partner. We've built deep experience over a decade. Our team of 750 specialists. Hold 19 competencies and have done 20 map migrations in the recent years. How do I So let's talk about Delta. Delta Airlines is operating in a high volume, safety critical environment where reliability, speed, and innovation directly impacts. Customer experience. And operational excellence. Sorry. We've we've been tasked to reduce. Error rates And improve productivity. We took an approach that took generative AI and transformed the traditional software development life cycle into a generative AI one. With the goal to bring more efficiency to the personas. So what were the challenges and the business impacts? Pre-developments and and architectural hurdles. Unclear requirements. Inconsistent criteria, slow prioritization, paired with a pattern of inconsistent security and non-functional requirements. Creating foundational issues, leading into longer cycle times. Development and testing bottlenecks, code duplication. Inconsistent reviews. Weak test coverage. And poor data quality. Delivers uh delivery impact and impacts quality, leading into quality variance. Pipeline and operational friction. Problematic CICD workflows, alert fatigue, delayed troubleshooting, and limited cost visibility, leading into higher operational overhead and cost. To solve these challenges, we developed a platform. And this platform is called CBI Ace. We've combined CBS value realization framework with AWS best practices. And with this system in place, we have an ability to identify and track measurable benefits. This enables us to drive development, the the development life cycle transformation through various personas. We've identified 50 more than 50 use cases, and 50% are already in production, and we have a 100% adoption rate. With this platform in place, we we we observed a 40% increase of productivity and a 30% reduction in error rates. The time spent on writing specifications went from days to minutes. Cycle times and manual effort were reduced and additional cost was avoided. So our our AI powered software development life cycle acceleration platform is an end to end solution that leverages the generative AI and AWS services to streamline automatically the critical stages of a software development process. Ace will will have a holistic view from the start till the end of your your your software development profile. It takes the unstructured requirements. Into a production ready artifact, significantly reducing development time. And improve quality consistency across your enterprise software projects. An example of these artifacts is, for example, a C4 architectural diagram, which is being generated by our platform. And that will power your architectural decisions. With all these rich structures in place, we can feed solutions like Quiro, like you just saw in Fabio's demo. We also have a 15-day trial available for this program, and we will show a QR code at the end, so please stay there for for that. So, quickly, let's go over the main components, because I'm also seeing that I'm running a little bit late. So we have various personas just like the other uh product uh that can use the system. Solution Ace is fully running on Agent Core, so. That powers the heart of our solution. We use bedrock and the bedrock models to do the inference. And we use a knowledge base powered by OpenSearch to do vectorized searching. We have a data layer that will actually feed these factor stores, so that the agents are powered and make sure that they can do their job. And we connect various tools like Jira confluence through MCP servers to actually create epics, stories, confluence pages, or Figma designs. Oh. Sorry about that. The whole solution was designed to be surless, secure and scalable. And this, this is just the beginning. I'm uh exciting for the future what the will bring for this platform. Thank you, Jores. So, um, Like Zybia, we have other partners, um, built the platform-driven approach, um, like Integratech, um, X1, IBM, Cognizant and PWC. Um, you can see a lot more partners are actually gonna adapt this and, and they are actually innovating quite fast, right? What you just heard from, um, There are a couple of more case studies we have in the limited time. I'm going to kind of run through very, very fast, but the slides and the entire session will be available on YouTube later, so you could be able to see them as well. Hissan is a Dubai-based building management company. They had challenges, the traditional challenges in and scattered requirements. Integratech, one of our premier consulting partners, they built a platform-driven approach called iCode, and using that. They could be able to address, solve some of those uh use cases as well, right? Specifically, the time consumed in terms of designs, reviews. They were taking quite a lot of time in terms of fixing, uh, you know, issues in the production because there is no um you know, automated way to understand what is happening, right? Um, so this is one, and, um, and the other case study is from 0 on 10 on 1. 01 is another AWS premier consulting partner, so they created an offering to help Zone Voice. It is a customer based out of Middle East, and they also serve customers across Asia, Africa, and Europe. So that challenges some of the business critical applications were actually hosted on. Java 8 application having outdated critical libraries. So again, 0 and 1, they created an AI powered STLC integration and with GitHub automation that actually accelerated the modernization part. So call to action. So these are some of the resources we have actually put together for you in terms of reimagining the operating model from an SDLC. Hundreds of use cases that we actually envisioned that you could actually create and implement based on the value that delivers to the customers. And then the last one is the accelerators, which is the samples and reference architecture that you have seen. So these are the QR codes for each one of them. And if you wanted to get your organization on a learning path with agentic AI, so we have courses which are created for partners and customers as well. Um, so log into AWS Skill Builder and then you would be able to scan this. Uh, once you scan the QR code, it takes you to the whole, uh, learning session like level 100, 200, 300, right? And uh as Juris was telling, they are offering 15 days trial for their SDLC platform. So please feel free to contact them and, and they will arrange an access to the platform. Um, so if you wanted to talk to us, we're gonna be staying there for 15 minutes in the corner. Uh, Feel free to come and reach us out. Um, you can also fill in the form so that some of our solutions architects will get in touch with you in case if you wanted to work and help you with some of the customer acceleration challenges. With that, thank you so much for attending this session and thank you so much for our co-speakers. I hope you enjoyed the session and uh have a great rest of the reignment. Thank you so much.