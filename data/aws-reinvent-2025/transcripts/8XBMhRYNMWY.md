---
video_id: 8XBMhRYNMWY
video_url: https://www.youtube.com/watch?v=8XBMhRYNMWY
is_generated: False
is_translatable: True
---

Alright. Why are we talking today about voice? Voice is actually not yet out of fashion. Voice still exists and is very persistent actually across different technologies from fixed to mobile to voice assistants and actually now in AI powered voice communication. And what we're going to do now today is really talk about how AI powered voice communication has benefits to improve customer experience, to reduce costs, and actually increase security in financial services. I'm here today actually with Chao, and I'll let you present yourself, Chao. Sure, so my name is Chao. I'm a director of applied science with Amazon. I've been with Amazon for about 10 years, initially, uh, working in Alexa on the speech, uh, interactive experiences, and now I lead a team in AGI Foundations focusing on building the speech to speech foundation model. So I heard, I hope you've all heard about the announcement of Nova Sonic launch this morning and I'm really excited to have this opportunity to talk to you about Sonic a little bit and also about the collaboration with Vonage to use speech foundation models to build um AI agents that can interact with users as naturally and intelligently as human agents. Thank you. Uh my name is Christoph. I'm a president of the business unit API at uh at Vonage, um, and we are a company responsible for cloud communication, doing a communication platform as a service and also network API allowing applications to communicate with, with users now. As I said, voice is very persistent and it has been so over time across multiple technologies. It started all with fixed voice. You could dial a human, connecting offices, connecting homes. Then it was on the mobile site. You could actually talk. And communicate on the move, then to voice assistants actually using your voice to do a command or ask for the weather or a certain command and a response, and then now AI powered voice where actually you can have a real interaction with an agent which is multi-turn and which also remembers your previous interactions. And why is voice persistent? It's because it's fast, it's faster than dialing, it's situational wherever you are, uh, and it also supports actually context and nuance and personalization. What are the benefits that we are seeing of AI powered voice when we look at the finance sector? On one hand, there is an enhanced customer experience, so personalization actually of that is possible. It remembers your past interactions that you had. It's 24/7 available. You can always call an AI powered agent. Voice agent is always there, and it's emotionally intelligent now. So basically it can, it can grasp actually frustration and can show more empathy or adapt a bit the pace and and the reaction. Secondly, it, it results in efficiency. Uh, which type of efficiencies, well, of course, you can automate repetitive tasks like your balance inquiry. It can also, it's scalable in a sense that when you have peaks, for example, on the tax returns, it can absorb that by by by your agents and of course cost saving from less agents in your call center. But also important for security reasons. Why for security, well, um, with your voice footprint is an ability to be authenticated. Or of course when there is fraud, you can be called by a voice agent when there is a fraudulent activity on your account, and this is 24/7. So there is also a lot of security and compliance applications that that are applicable for uh for AI powered voice and also towards the future, but Chao will will will get more into detail on that. We will see there is an opportunity for much more proactivity and also integration with other types of channels like text so that you have a unified customer experience, not only purely voice. Now when what what are actually some of the use cases that it enables uh when you look at uh financial services actually there's a multitude of of use cases that are possible um when we look at the banking and financial services there is actually really customer support as I said, uh, very basic routine. Balance inquiries can be done by agents. It actually can personalize, give personalized recommendation. An agent can give you a recommendation for investments, uh, but also fraud protection, as I said, using actually your voice footprint for authentication. It can call you when, when there is fraudulent activity on your, on your account that has been spotted. And it can also, of course, for compliance reasons record certain transactions that you are having and know actually when to do that. For wealth and management and investment. Other use cases are for example proactive recommendation for fund investments um and and and many other engagements on the insurance side we could imagine actually basic claims entering basic information around the claim can be done through AAI agents but also lead qualification. You can by asking simple questions, it could do a prioritization of leads or an assessment, a risk assessment of a potential lead in in insurance. So a lot of use cases that are possible, but then comes the question how do we do this technology wise, and that's why we're going to now give both a presentation on what is Amazon Novasonic enabling actually from a speech to speech modeling perspective and how the combination with the voice infrastructure of Vonage can make it easy for developers and. And enterprises to build agents in an easy way. So Ciao, I'll let you, thank you. So. OK, so, I want to tell you a little bit more about Novasonic. Um, Novasonic is a speech to speech foundation model for real-time human-like conversational AI, and, uh, maybe I can walk you through this diagram first because that will give you an idea of why. A speech to speech foundation model is like the future. It revolutionizes how we build these kind of agents. So, the first thing when you look at this picture is how simple it is because if you've worked with voice agents for a long time, then you'll probably be very familiar with the traditional cascade kind of system architecture where you start with an ASR system that converts speech into text, then You might have like an NOU system that derives meaning out of that text and you have some kind of dialogue control that figures out how to sort of respond to the customer and then you have a TTS system that turns the response text into speech. With foundation model, literally, you just integrate with a bi-directional streaming interface and this interface handles everything. So you have speech coming in. And then inside this model, it's going to do streaming-based processing as soon as the speech is received by the model and the inference engine. It's going to actually generate live transcript out of. Of the speech and it has all it can also generate, um, API tool calling kind of commands if the input of the user requires that kind of communication requires that kind of handling. And then when the tool call information comes back, the model can generate the response incorporating the information coming back from the tool, coming back from, you know, the developer's design of what this agent is supposed to do, and then at that point in time, it's going to generate agent speech. And the beauty of this whole pipeline is that We actually have the transcript, which is the text representation of what the user said as an intermediate output, which helps grounding sort of like what you think, you know, what the like what the agents think the user is saying. So it brings transparency about sort of like the agent's behavior in terms of how to handle this. It also is able to Generate the speech, taking into account all the information that's available to this large model that includes all the dialogue history as well as the input user speech so that the agent's speech can be not only natural and expressive, but also it's paying attention to how the user is expressing themselves in addition to the content of what they say. Um, that way, the agent's behavior will be a lot more empathetic and human-like. So, I just want to walk through a little bit of this. So essentially with this architecture, you can do very fluent, uh, fluid dialogue handling and natural turn-taking. By that, what I mean is as soon as the user finishes speaking, The system is going to make a determination to say, oh, the customer finished speaking and it's going to start responding, but if the customer interrupts the agent, uh, responses, it's going to stop almost immediately and to listen to the customer and then respond to what they said next. We also, um, are very proud of our speech. Recognition performance and you probably see in this morning's, um, in this morning's presentation, keynote presentation that we achieve state of the art accuracy in speech recognition. And I mentioned that we support adaptive speech responses. Adaptive meaning it's adapting to how the user is speaking, not only just what they're speaking. Um, and I mentioned that we have very expressive voices. I hope you'll show a demo that, uh, demonstrates that, and it's available in multiple languages and because of this very simple architecture of speech to speech, it naturally achieves very low latency in responding. All right. And actually what we're doing is combining the Amazon Novasonic technology with the Vonage voice API. So basically combining the Amazon Novasonic AI capabilities and speech to speech models with the voice infrastructure of Vonage. So actually that it is very. Easy to do inbound calls, outbound calling. It can be done on multiple technologies, so it can be done on mobile apps, web apps, or the very traditional calls. So it's really the combination of the two, the voice infrastructure together with Novasonic that makes it actually easy for enterprises, for developers. to build agents and that's what we really try to do by that partnership, by that combination is to make it easy for developers to build agents for the for the future. But of course we want to demo this. This is always better than explaining slides and how it could work. So let's let's have a little sneak peek on how that can work. Hello, this is your financial services assistant. For your security, please say the phrase my voice confirms my identity. My voice confirms my identity. Authentication is complete and secure. Hello, how can I assist you today? Uh, what is my current savings account balance? Let me retrieve your current savings account balance for you. Your savings balance is $23,540.15. Let's switch to your investments. Your portfolio is doing quite well this year, with a total value of $512,040.75 which is up by 6.2% year to date. OK, um, let's schedule a call with, uh, my financial advisor. Can you help me with that? Certainly, I can help you schedule a call with your financial advisor. Let me check the availability for next week. It looks like your advisor is available on Tuesday at 10:00 a.m. Would that work for you? Um, do you have any openings for Wednesday? Let me check the availability for Wednesday. It appears that your advisor is available on Wednesday at 2 p.m. Would that time work for you? That works. Great. I've scheduled a meeting with your financial advisor for Wednesday at 2:00 p.m. You will receive a confirmation email shortly. Alright, so that's how it works. Cool. So, uh, now of course we, we explained a bit what we have now, but maybe you can, without divulging any secrets, tell us a bit what is next, uh, what is our, well, actually it's easy for me because we just announced Sonic, uh, Nova 2 Sonic, which is the next generation of Sonic, and let me just give you a sneak peek of the new features that we launched. So Sonic in version one supports English, French, Italian, German, and Spanish, and with Sonic 2, we added new languages, Hindi and Portuguese, and we have an exciting language roadmap in the pipeline that hopefully we will share in coming quarters. Um, we also added the ability, uh, we also added, um, what we call polyglot voices, which means the voice can speak multiple languages even though the original voice talent were only monolingual. And this way we can handle sessions with language switching, uh, very seamlessly. A user can literally switch between You know, let's say English and Spanish, depending on how they feel comfortable, almost naturally expressing themselves, and the agent can the same agent voice can respond in English or Spanish given that context. We are we also added a feature called asynchronous. Tool calling because, you know, these models, they're very fast if you're just doing chit chatting, but if sometimes you go off to do a tool call, that can take a long time. And if you're just waiting for the tool call to come back to con continue the conversation, then you sometimes get stuck in this very Awkward silence with the user and sometimes the user would be wondering like what's going on because the system almost feels like it's kind of dead or broken. So we enabled this asynchronous tool calling capability where the agents can basically continue the conversation. With the customer when the tool call is being executed in the background and when the tool call in, you know, the results come back, the agent is going to incorporate that information into the dialogue to continue sort of the, the conversation with the customer in a very natural way. Um, another feature we added is actually to add text into the speech input as well. So it's not just speech to speech, it's actually speech and text to speech and text. That way, um, even though we, we say that these are voice first kind of experiences, having text capabilities is helpful for a lot of use cases. For example, if you want the agent to say something before the customers say anything, then you can use the text input from the developer side to prompt the model so that the agent, the model will start speaking without waiting for the, for the user to have speech input. You can also use that capability to handle like touchpad kind of input so that these touchpads. The key presses can be converted into text as input into the model. The benefit of this is all of these are seamlessly incorporated into the model as if it's like just a thing like there's never loss of context or switch of context when you have different modality of input. It's all the same context information available to the model. One last feature that I want to talk to you about is the um, Oh, configurable um turn taking behavior. So we all like these agents to be like snappy. You talk and then the agents come back, but there are actually scenarios where users prefer the agents to be a little bit more relaxed so they don't feel like they have to talk in case they. Stopped and the agent will cut them off. So there is ability to configure these kind of thresholds to be sensitivity low, medium, high so that you can basically trade off sort of some of these latencies for just a more relaxed kind of interaction vibe. So I think that's what I can share now. Hopefully, you know, if you watch for our Nova pages, then you'll see we, we always launch, um, update and launch new new features over the, over the course of the year. Yeah and if you wanna learn, oh, that's right. So I should mention this. So there are quite a few sessions to talk about Sonic if you want to learn how to build with Sonic. There's a workshop which is amazing. It's 2 hours long and it tells you everything about how to build these kind of agents. Um, unfortunately, it's happening right now, so you're not going to be able to catch it, but these sessions are recorded, so you should be able to look it up, searching for keywords. Or the session to find, you know, to, to watch it at a at a later time. There's these chalk chalk talks that are happening in the next couple of days and also there's a breakout session which goes into one hour long presentation about Sonic. I hope you'll be able to catch that one tomorrow. And you can, you're always welcome at the Vonic meeting room, and we have also drinks and presentation later tonight at Paris. I think some leaflets will be handed out right over there if you're interested to come and learn more. So thank you all for your attention. Thank you so much for attending.