---
video_id: wqmeZOT6mmc
video_url: https://www.youtube.com/watch?v=wqmeZOT6mmc
is_generated: False
is_translatable: True
---

Yeah, so I'm, I'm Mark Brooker. I'm a VP and Distinguished Engineer at AWS. And I spend most of my time working on Agentic AI and infrastructure for Agentic AI, uh, including Agent Core. I, I hope, you know, most of you watched Matt's keynote this morning, uh, which, uh, touched on some really exciting launches, some of the work that we've been doing in Agent Core, uh, to build, you know, great infrastructure for AI agents. And I'm going to get into some of the details of some of those launches as the talk goes. Here we go, I do have a clicker, um. But we're gonna start off with looking inside an agent, what goes on inside AI agents, what I mean when I say this word, AI agents. Then we're gonna talk about what agents need. What you need to get agents into production. We're gonna talk about runtime, places to run code, memory, gateway, connecting your agent to the outside world, evaluations, and some of the cool neurosymbolic work the team has been doing. I have a very important problem in my life that I need to solve every day, like many of you do. Every day, I need to multiply the number of R's in strawberry by the hours of the current time and the current outdoor temperature in Seattle. And then I need to calculate the log of the gamma function of the result. It should be obvious to all of you why I need to do this. You know, this is the kind of thing that we all do in our day to day lives. And so to automate this, which I was doing manually with a pencil and paper, I needed to build an AI agent. Let's talk about how I would do that and how an AI model would help me do this. Well, counting the number of R's in strawberry, famous problem. Models mostly can do this pretty well themselves. At the very least, it requires no state from the outside world. It is a fixed function of the input, and so a model can calculate the output pretty reliably for modern models. Then we get to the next two problems. I need to get the the hours of the current time. In isolation, an AI model just can't do this. It doesn't know what the time is. But many, many models have the current time or time and date in their system prompt somewhere. And so I can put the time into the system prompt of my agent and have the model know this answer already, without having to reach out to the outside world. Models can also do math, not particularly well, as we might see later in the talk, but they can do basic multiplication pretty reliably, as long as the numbers don't get too big. But then we run into the problems that a model by itself just cannot do. It cannot know what the current outdoor temperature in Seattle is. Again, I could look that up, and I could put that into the system prompt, but then I have to look up everything and put it into the system prompt. And so I need to give the model a tool to use to look up this kind of information. And finally, models aren't particularly good at anything looking like advanced math, math with big numbers, math with floating points, and so on. And so I also need to give this model a tool to do this mathematical work that I'm asking it to do. And so we have kind of 3 problems here. We have a relatively easy closed problem for the model to solve. We have a kind of medium problem. It should be able to do, maybe with some additional prompting, or maybe we'll need a tool call. And then we have these two problems that modern models can't solve, and even with a bunch of model advancement they can't solve because they're facts about the current world. We also have this larger meta problem of having to read this whole prompt and figure out what it means and decide what to do. And at its core, that is what we're doing with the model. And so, when I talk about AI agents, what I'm talking about is systems that are given a goal. Some kind of goal here, calculating this very important number that I need to calculate every day in my day to day life. And that loops between inference with an AI model and tool calls to achieve that goal. Eventually, ideally, achieving it and giving me a correct and trustworthy answer. This isn't quite the entire picture for modern agents. More and more agents I see are including code in their definitions. Both tools defined in code, code generated by the model, and code that implements parts of a workflow, a few steps of a workflow, where it is used to improve the reliability, lower latency because it requires fewer inference calls. And, um, uh, and, uh, yeah, improve reliability and lower latency and lower costs, uh, because it requires fewer inference calls. So let's dive one layer deeper into the implementation of this very useful agent that I'm busy building. And here I'm going to use the strands agent framework. This is an agent framework that we developed at AWS to build our internal agents, including those foundational agents you saw launched in the keynote today. But Agentre with Agent Core, you can use any AI framework. You can use lang chain, you can use whatever SDK you can build your own SDK or just use no framework at all. I like strands because it's very flexible. It allows me to build anything between agents that are just a single prompt and agents that are complex, hard-coded workflows. There's also some cool new features in strands like constraints that we've launched over the last few weeks. So here's what a snippet, let's say, of the strand's code looks like. Here, I'm using this, this at tool notation to locally here in my agent code. Define a tool that is available to my agent. I can also connect tools into this agent using an MCP client, using an open API client, or over any other protocol that I choose. The next line of code is the definition of this tool. This is a tool description. This is given to the model, as we will see later, to help it understand when this tool should be used. If you're not familiar with this gamma function, by the way, This is a kind of a real number version of the factorial function. So if you ever wondered what like 5.5 factorial is, this is the way that you work it out. Um, and then I define my agent. I say, here's a tool for getting the current time, a tool for getting the weather, and this math tool. And then I pass my prompt into the agent. I pass in that goal. This is as simple as implementing an agent in strands can be. Just a handful of lines of code in Python. I think the minimum one is probably 4 lines of code. Let's go one layer deeper and talk about what's happening at the inference API. And here we're going to look at the Bedrock API. One of the cool things about Agent Core is that you can use it with any inference API. But here I'm going to use Bedrock because it's a great secure place to get at cutting edge AI models. Here's what goes in the wire when I make my first tool call, sorry, my first inference call to Bedrock. I'm telling The model, what my goal is. Multiply the number of R's in strawberry, and so on. I'm giving the model my tool specifications. This is a list of tools and their descriptions that the model can then use to choose when it makes a tool call and uh to, to get a particular piece of data. And then I'm giving it a system prompt that I defined. I didn't put this in the strands code, but this is a system prompt that I build in that describes the kind of overall goals of this agent. Here I'm saying, you are a helpful assistant, and giving it a few extra hints about the tools that it has. This isn't strictly necessary, but in this case, it improves performance of the model. So the first time I call the inference API, the response I get back is a tool use request. Bedrock says to me, OK. I ran the model, and what the model asked me to do is uh asked you to do is to run this tool and call it again. So here, I have to call get weather fact, which is going to get this outdoor temperature, which the model needs. And so it has looked at that prompt. It's planned the way that it's going to do this, right? It's obviously going to get the temperature and then it's going to do some math, and we're going to kind of step through this and go around the loop. If I ran this again, it might do a different step in different order. This is one of the challenges of building reliable AI agents. But here I'm getting the weather fact. I call the tool, locally. Strands handles this for me. I don't need to do it. But I essentially just run that Python function and give its results. And here you can see that the request is growing and growing. This is the 2nd call, the 2nd turn around the loop. And I give the model the whole thing again. My original content, multiply the number of R's. It's response, saying, I'll help you calculate this, step by step. Let me gather the information. Let's do a tool call. And then my response saying, I did that tool call for you, and here is the text that I got back from the tool call. The text which is apparently 3. that's a bit of an alarming outdoor temperature, at least in Fahrenheit for Seattle. Hopefully it's a little bit warmer than that. And so you might be wondering, well, if we're just growing this over and over, aren't we doing N squared work? And yes, in the naive implementation, that is basically what we're doing, but there are some techniques that we can use to bring that back to a more scalable standing. I'm not gonna go down to the next level, but what's happening inside Bedrock here is taking this structured API and turning it into a blob of text or a blob of tokens that are passed to the model. Exactly how that is done is model dependent. There are a bunch of delimiters. Some of them look a bit like Jason, some of them look a little bit like ad hoc XML. Some of them look like magic spells. But essentially, I take all of this stuff, I put it into a blob of text, I pass it to the model, and the model responds to me. Now if you're paying attention to that. You'll notice that that kind of uh delimiting is a little bit ad hoc. And it is one of the kind of core challenges of building secure agents. Because if you're a security person, you will notice that that is in band signaling. Where we are putting control signals to the agents and user-defined content into the same document in a way that is not particularly clearly delimited. And so here I go around and around this inference loop. In this case, I go around it on average about 3.5 times to solve this problem. And as I go around it 3.5 times, it sort of steps towards the solution, and gives me the result. That 3.5 times is also a little bit surprising. It means that there's some nondeterminism in the system. Sometimes the agent is able to figure out how to do it in 3 steps, and sometimes it's able to do it in 4 steps. If I wanted to optimize this to reduce my token usage, given that it is a fixed plan every time, I might lift the sort of planning step of this away from the LLM and turn this into a more fixed function, step by step workflow. I might still want to use the LLM to put the big picture together, but not necessarily to figure out the step by step plan. Now we understand the internals of agents. Let's step a little bit into what components you need to run agents successfully in production. And here we're going to use a slightly less silly example. I like to do some outdoor sports with my family, and that is very weather dependent in the Pacific Northwest. And so I want to build an agent that gets the snow conditions, weather, and river levels near me, and then tells me if I should go skiing or boating. Is there a practical thing to want to do with a personal agent. How might I run this in the cloud? Well, again, I might implement this agent using a framework like Strands. I need to run that agent code somewhere. I need to connect that agent code to the tools that are gonna provide it the information that it needs to run the agent. The weather API, the river-level API, the snow-level API. Maybe if I choose to go skiing, I might ask it in my, in the system prompt to book parking for me. And so I want to give it an API to do that too. I need to connect it to memory, so it remembers my preferences over time. We'll get into that in the talk. And then I might need to want to have guardrails, uh, that make sure that the responses are safe. So the core of Agent Core is Agent Core runtime. This is the place where you can securely run Agent code. This is where I would run that strands code. When I say securely, I mean isolated with a per session, hardware-backed virtual machine. This is one of the most powerful features of Agent Core. Provides really strong security isolation around code. I'm gonna go into that later. Then we have the agent called Gateway. Agent called Gateway allows you to attach agents or, or to connect agents to the tools inside your company or to externally available tools. You're not going to transform your whole company to agents overnight. You've still got microservices. You've got tons of databases. Those databases have some of your company's most important assets in. And so being able to connect them to your agents securely in a place where you can do audit and control and so on is very important. Agent Care Gateway makes it really easy to connect open API tools, MCP tools, and other types of tools into your agents, talking the protocol that your agents use. It provides features to do, uh, um, to chew, to, uh, expose the right set of tools, tool curation to your agents, which I'm also going to talk about, uh, a little bit later in the talk. Then we have memory, which allows our agent to remember things over time between sessions. It allows state to be sticky between sessions. I talked about how runtime gives you a new VM per session. And at the end of that session, all of that state is securely deleted. That's great for security, but it does mean that your agents have no memory whatsoever. And so you're bringing that back with an explicit memory component, where you add back in the ability for agents to remember between sessions. I need to connect the user's identity to tools. Here, that ski parking booking API might need an identity from me as a human. And so here, I might need to go through an OAuth type flow. And provide credentials for that identity to the agent to use when it makes the decision to use them. Agent core identity makes it easy to set up these flows in a secure way. I need to be able to connect my agents to websites. Uh, here, uh, instead of there being a ski parking API, you know, we could all hope that everything we interact with has APIs, but they don't. And so we also provide an agent called browser use, which is a secure, isolated environment that allows you to automate around agents using a browser to figure out how to click through an API flow through a website flow. What I haven't pictured here is Agent Core compu uh code interpreter, which is a secure environment for you to run code. And I didn't picture it here because Agent Core code interpreter is very useful, but it's also a little bit of a minority case. Because of the isolation model of Agent Core runtime, a lot of, in a lot of cases, you can take the code that your agent generates and run it right there in the runtime. Where it will still be strongly isolated from other sessions and other users of the same agent. Code interpreter is super useful when you want to give that uh code, that running code, less than the full permissions that your agent has. So let's dive a little deeper into each of these components. Run time is a place for your agents to live. It's a place for your agent code to run. Hooking your agent up, an agent built in, in strands, or lang chain or whatever, is very easy. It's just a handful of lines of code. You add this app entry point annotation. Um, And then, uh, and then essentially just run your code. Here, I'm passing in the gateway URL and a little bit more configuration. I'm running the agent. And then I'm returning uh the response. What you'll notice here is this allows a multi-tone conversation with an agent inside the session. And so I could easily build a client that allowed me to have a step by step conversation with this agent, as I might have, um, with an agent that I asked to help me plan reinvent, for example. If you came to the innovation talk yesterday, you would have seen my silly reinvent agent. And uh that was implemented in exactly this way. If I call again with the same session ID, I will get the same runtime with this copy of the agent running with everything that was in memory before. If I call with another session ID, I will get a different VM, a new VM that is strongly isolated from this one. Agent runtime, I believe, need to be serverless, scalable, secure, and offered fine-grained automation. Agents offer, or one of the challenges of agents is a novel set of security threats, and one of the great ways to mitigate that is by strongly isolating agents, for example, in a VM where you can make precise statements about the way that state flows around your system and what agents can do. We'll talk a little bit more about safety later. I like Servius in this space because it allows me to use operational agents, like the one that Matt announced in his keynote this morning, to do all of these operations for me without having to worry about the complexities of running infrastructure and so on. Agent Care Runtime is built on Firecracker. Firecracker is a. Micro VM hypervisor that we built about 10 years ago, maybe 7 years ago. We announced that a reinvent 2018. This is a piece of technology we use to power AWS Lambda, DSQL, and many of our analytics tools. Here we've used Firecracker with its proven track record of high security. And strong stability and performance to provide powerful isolation for agents. This is a real virtual machine, started up for your agent in milliseconds. If you wanna know how that works, I'm gonna go super deep into that, down to the kind of memory page level in a talk tomorrow, 500 level talk tomorrow. Using Firecracker allows us to bring the fine-grained isolation level of lambda, make it even finer-grained down to the session level, and still offer great performance, scalability, and cost-effectiveness for your agents. Unlike lambda, uh, agent core runtime is priced based on run, based on busy time. So here you can see I have, I've got a different haircut here than on the Mac. So here you can see that I have uh multiple conversations with my agent. And in the purple, it's running, it's using up both CPU and memory. And then it gets back to me and says, I need some more input to do this task. And during the time that it is waiting for me or waiting for a tool call or, you know, waiting on something to happen, you don't pay for the memory of the agent core run. Sorry, you don't pay for the CPU of the agent core runtime. And so the cost of running an agent goes down substantially by about, uh, probably on average around 20x, uh, between when it is running and when it is idle waiting for a response. This is a huge cost benefit for agents that spend time waiting on humans or waiting on long tool calls, maybe waiting on other agents. If you're familiar with lambda, you might be a little bit excited about this. Now we have runtime isolation. My conversation with this agent, and the other customer's conversation with this agent, as I said, gets different VMs. Why is this interesting? Well, as I said, it's interesting for security reasons. It means that I don't have to worry about my conversation, and that other user's conversation being mixed up. There is no prompt injection, no bug in the code, or even remote code execution bug that would allow me to get the access to the content of the other concurrent session. It is running in a different hardware-backed virtual machine, strongly isolated from this one. But this is also nice because it helps you write simpler agent code. You don't have to think about multi-tenancy when you're writing your agent code. You don't have to think about building your agent code to be multi-threaded. You don't have to think about propagating identity and cleaning variables and clearing memory, and so on in agent code. That is all handled in the infrastructure. You can write your agent code in a very simple single threaded, single user kind of way and have all of the isolation and multi-user handled by the infrastructure. Building agents reliably is challenging enough. And so we've tried to use the infrastructure here to simplify the journey of building agents and make it much, much easier. It also means you can bring any framework, any library, any language. Here I've used Python. I also like to build uh agents in Rust from time to time. I like to bring libraries. Maybe I needed another math library to do that gamma function. All of these things are possible because it is a real virtual machine. You're not limited in what you can bring. You can even mix languages. You can create new processes. You can fork off new threads, whatever you need to do to get the job done. Now let's talk about memory. One of the really annoying things about talking to naive agents is that they forget things. Here, I'm interacting with an agent to help me choose a meal during the week. And the first thing I'm going to say to it is, I don't like pizza. Then I'm gonna come back to it the next day and say, suggest a meal for me. And it says, have you tried that new pizza place? I mean, if this was a joke, it would be a pretty decent one, but it is really annoying that agents don't remember this. Memory is the simple idea that we can take the conversations that I have with agents, and write them down, and reuse them. There are many forms of memory, but probably the most useful single one is user preference memory. Where, behind the scenes, with just a few clicks in agent core, we can set up a pipeline for you that takes the conversations that your users have with your agents, runs them through a model that extracts the preferences that those users have. Like, I don't like pizza, and uh provides those to future invocations of this agent for the same user. And so that user isolation is handled, the setting up of the pipeline is handled, the prompting of the model is handled, retries are handled, and everything is built in to agent core memory. So all you really need to do is create the memory resource, prompt your agent to use it, and wire it in. It's a little bit more boilerplate. I was hoping we would fix this and make the SDK make this a lot cleaner, but we didn't have time, unfortunately, so you get to see the long ugly version for the couple of weeks that it's going to be around. There are a couple of small things here worth pointing out. Here I'm asking, uh, when, when retrieval happens, when I run the agent again, for a maximum of 5 facts about this user, the 5 most relevant facts. And I'm setting a kind of filter of, of relevance. I'm saying, don't give me things that are truly irrelevant to this conversation. And so this is a way of managing context size. One thing you will have noticed that if you're, if you're an agent builder, is that if you tell a model a bunch of irrelevant stuff, its performance tends to go down. And so you, it's, it's useful to tell models relevant things. And so with that integration with memory, I can say I don't like pizza. Here we go. And it's gonna remember that. It's gonna take the trace of that conversation, it's going to extract that preference, and it's going to write it down in agent core memory. That takes up to a second or two to happen in the background. And then the next conversation I have with the agent. It remembers that for me. It's a very simple way to make agents feel a lot more useful to humans. A lot less cold. To have them remember the things that your customers say. Let's talk about Gateway. And how agents do things. Fundamentally, tool calls are what make agents useful. If an agent can't call tools, if it can't interact with the outside world, well, it's not doing a whole lot. You can have agents that just talk to customers, you know, chat with them. But it is tool calls and their ability to have side effects on the outside world that makes agents so powerful and so useful. Gateway, we built Gateway to make those tool calls easy to set up, easy to curate, and possibly most importantly, controllable. Auditable You're able to set policy, which I'll dive into in a minute. Tools allow you to connect your agent to the data you need, and the effects that your agent needs to have on the outside world. This could include services and microservices, data in databases, data in storage, software as a service, applications, or even other agents. Gateway provides a single place to connect all of those into your agent. It also provides a place to do tool curation. One of the anti-patterns I see when folks build agents, is they provide every tool in their organization. Here, hey agent, there are 1000 tools. How would you feel as a human if somebody said, Here's a toolbox with 1000 tools. Good luck picking up the right one for the job based on a little label on each one. Curating sets of tools and giving them to agents is really critical, uh, to having your agents perform well and having them have good cost-performance trade-offs. Setting the right tool description is also super useful. Let's talk a little bit about some data in tool curation. Remember I talked about multiplication, and doing multiplication with the models, and how they're sometimes good at that. I also showed when we were talking about the Bedrock API how a tool call is another trip through the model. Most of the time. They can't do multiple tool calls in a single trip, but that consumes input tokens, it consumes output tokens. And so here I'm um I, I, I'm comparing the performance of that first agent that I built between a version where the model does the math and a version where I give it a multiplication tool and have the multiplication tool do the math. Looking just at tokens, what we see is that doing the math in the model is substantially more token efficient. It's substantially lower latency and higher performance. That's great news. Giving this model too many tools has made the agent's outcome substantially worse, even in this very, very simple example with a cutting edge model. But before we get to the conclusion, There's a little bit of a caveat. It only got the multiplication right about 40% of the time. And so giving it a multiplication tool cost more tokens. It cost more latency, but made the success rate of their agent, this agent, jump up from 40% to 100%. And here I'm defining success as correctly calculated the result. And so tool curation isn't about only giving a minimal set of tools, it's about giving the right set of tools to an agent, and Gateway provides a very convenient place to do that outside the agent code where you can iterate on that, you can test it, you can test it, and many other things. Now, Let's talk about one of the most exciting uh launches from this morning. One of the things that we've been doing at Amazon for over a decade is investing heavily in automated and formal reasoning. This is the kind of reasoning that maybe would have been called AI in the 1990s. Using things like SAT and SMT solvers, and other formal reasoning approaches to reason about mathematics symbolically. It is an extremely powerful set of tools that allows us to say very, very precise things about the world. It also was a little bit of a dead end on the route to AI because it is quite inflexible. It doesn't have the same kind of natural language understanding that LLMs have. It's not great at planning in the ways that some LLMs are. And so we've been investing heavily over the last few years at AWS in neurosymbolic AI that mixes the neural approaches with the symbolic approaches to provide the best of both. And so, what excites me most as a technologist about policy, is this is a big release based on neurosymbolic AI. You would have seen automated reasoning guardrails for bedrock release at Reinvent last year, I think it was at Reinvent. Based on similar technology, and we've been using that to iterate quickly on making this more and more flexible, more and more powerful, and more and more accurate. And now we use it here to power agent call policy. Let's talk about what policy is. Agent Care policy is a layer inside gateway that allows you to very crisply, with mathematical certainty, say what your agent is allowed to do. When I think about agent safety, I think about what agents are allowed to say. And that's where I would use something like bedrock guardrails. And I think about what agents are allowed to do. And that's where I would use authorization, and I would use bedrock policy. Sorry, bedrock agent call policy. So, policy sits on the call path out of your gateway to your tools, to those data sources, to those services, to that software as a service. And allows you to specify, at a very, very fine grain, exactly what your agents are allowed to do with those tools. This is done with the CEDA policy language. CEDA is a policy language that is um designed. To be powerful, designed to be expressive, has an implementation that is formally verified, and has really cool semantics for doing things like composition of policies. If you're a security person, you might have tried to take multiple policies and understand what happens when you stick them together into one policy. CEDA has extremely well-defined semantics for composition. That allows us to run many policies, uh, very efficiently and with well-defined semantics. The other great thing about CEDA's sort of crisp mathematical nature, is that policy will give you a statement, um, based in mathematical logic about why it made a particular decision. You should never look at these. Ideally, you do never look at them. But, if one day your agents do something unexpected, they're there. They're there for you to look at and see exactly what the inputs and outputs were, that made your policy not have the effect that you thought it was going to have. Here's an example of a policy. It's a very simple. Authorization example. All I'm saying is this tool call is only allowed for one particular principle. One particular Oauth principle in this particular case. I can attach this into my gateway without having this feature built into any of the tools, without changing my agent code, and give different users of my agents different tool called permissions. I can then have my security team look at and audit all of these policies, see what they look like when composed together, and understand their effects. Here's a slightly more detailed example. Those two agents that I talked about earlier in the talk, get weather data. I asked them to get weather data from Seattle. But maybe They're gonna hallucinate. Maybe they're going to make bad decisions. Maybe one of my users is going to try and make that agent go off track and be a general weather agent, looking at weather around the world. So here I've written this policy saying, when you call the weather service API. It can only be called for Washington State. Maybe I could scope that down further and say it can only be called for Seattle. No matter what the user does with this with this agent, what kind of prompt injection they do, even if they get remote code execution, whatever decisions the model makes, there is no way for this agent to call the weather API without setting its scope to Washington. This is a really powerful place to control what your agents can do and what they can ask for, without needing that kind of fine-grained authorization baked into every tool, and without needing to trust the agent code. When I think about safety and security of agents, I think about putting agents in a box. This is the architecture in my head. Here I have two agentic applications, one of them with 2 agents, one of them with agents in memory. Each one gets their own gateway and their own policy layer that defines for that application, what tools it can have. So this is the tool curation that I talked about, and what powers it should have when it calls those tools. You might be familiar with, and this is a bit of a, you know, uh AWS naming fund. You might be familiar with gateways in other contexts. Things like AWS API gateway. And so it might have the mental model of, well, I'm gonna take a gateway and I'm gonna put it in front of each tool. My favorite use of agent core gateway isn't like that. I want to associate an agent core gateway with each application, with each set of agents with a defined purpose. And there I can govern them, I can connect them to the right set of tools, and I can set policy based on that application. This is a really powerful way to control what your agents can do in your organization outside the agent code. Now remember that these agents are running in their own virtual machines, and so they have no way to get out of this box, except through the gateway. The only way they can send packets to the world is through the gateway. Writing things down in memory or on storage has no effect, because that memory and storage is erased at the end of the session securely. And so here I have built a box around my agent. I don't need to worry about what's happening inside its head. I don't need to worry about untrusted inputs. I only need to worry about what it can do in the outside world. This isn't a mental model you can use for every agent. Some agents are too complicated. Some policies too subtle to express in a certain way. But it is the beginning of the way that I always think about building safety and security into agents. And with policy available now, you have a really powerful toolkit. Um, you have a really powerful toolkit, uh, to express what you need to, what's allowed out of this box, what's allowed into the box, and so on. Now, let's talk about evaluations. This is the other thing that Matt announced this morning, or one of the other things that Matt announced this morning, about Agent Core. Why do I think that evaluations are so powerful? Um, and what, you know, what are evaluations? Well, evaluations is the simple idea of, is my agent doing the thing I'm expecting it to do. That's a subtle question, more subtle than you might think. Is it doing things efficiently? Is it saying things to users in a, in a good way? Is it calling tools in the right way? I want to measure these things, so I can know how to operate my agent. I can know when it's off track, so I can improve my prompts, so I can improve my tool descriptions, so I can improve my memory policies. Um, I want to measure these things so I can know whether I, whether I can safely change models. Maybe I want to try out a new model. Maybe I want to try out a smaller model to save time and money. Maybe I want to try out a big, exciting new model to see if I can make this agent even more powerful. Evaluations help you answer this basic question of, are those things working? If you're like me from a more sort of traditional software engineering background, You would have built these CICD pipelines, where you have multiple steps of testing and evaluation. You know, you're gonna put something into gamma, you're gonna run some tests against it. You're gonna see whether it works, you're gonna set standards. And you could, evaluations are the tool that allows you to do those kinds of things on AI agents. But they're different from regular integration tests, because they're more open-ended. Because by their nature, agents are more flexible. They have more agency. The whole reason you're building an AI agent instead of a traditional workflow is because you want it to be flexible. You want it to be adaptable. You want it to be maybe even creative in finding solutions. If you just wanted to go through a fixed function workflow, well, you would use step functions. It's a great solution for that. You wouldn't build an agent. You want to build an agent because it has dynamic behavior, but then you have to answer, is this dynamic behavior good? It is a tough question. I'm an electrical engineer by training, so I think about the world very much as a set of feedback loops. Evaluations are feedback for agents. They allow us to feed the agent's performance back into our development processes, so we can improve agents. Dynamic systems fundamentally don't work reliably without feedback. And so evaluations are the core of making this work. Unlike a lot of folks in the industry, We have focused on what we call online evaluations for Agent Core. That means evaluating agents in the infrastructure, in the cloud, rather than at research time. This is a product mainly aimed at developers, system operators, DevOps and SRE teams, rather than aimed at science and research teams. I love my science and research teams, but they have a lot of evaluation tools to choose from. Agent core evaluations is there for when you get into or near production, when you want to test against the real tools, ideally with real user inputs, and know when things are working. You can use evaluation at development time too. There's nothing stopping you from putting your agent traces into agent core observability and having them evaluated while running your agent on your desktop. That's exactly what I do when I build agents. But it is most powerful once you get into production, once you get onto the real infrastructure, once you are interacting with real tools. And, uh, and, and real user inputs. Because ultimately, that's what matters. We all know as software developers that writing tests against stubs and mocks and so on, has, it's useful, but it has fairly limited utility. Those mocks and stubs and so on, never quite have the same behavior as the real world. We always learn something new about our software when it gets real data, real user input. And agents are the same. Perhaps more. You're going to learn so much about the behavior of your agents when they are exposed to the real world. When you are actually asking them to do that dynamic process of going around a loop and solving for a goal. Evaluations is the beginning of a journey, but it is already super powerful. We have canned evaluators built into evaluations with a single click. You can set up a goal success rate, how often your agent actually is able to iterate around until it achieves its goal. You can set up for conciseness. Maybe you want that agent's result to be, you should ski today, or you should boat today, instead of 3000 word essay and its life story. We all know that models sometimes are a little bit more verbose than we would like. You can set up a tool calling success rate, how often your agent is calling a tool, succeeding at providing the right inputs, succeeding at passing those policies, succeeding at providing the right authentication and authorization materials, and so on. And there are many more built into agent core evaluations, allowing you to think about the performance of your agents all built into agent core without you having to build an evaluation pipeline that gets all of that data. So this is just scratching the surface of the components you need to bring agents into production. But I've been focusing here over the last year. Because I spoke to a lot of customers who were saying, hey, we were building agents on my laptop, on my desktop. We were having a great time. The POCs look fantastic, but now we don't know how to get into production. We don't know how to make it reliable. We don't know how to make it secure, or isolated, or operatable. We don't know how to observe it. We don't know how to control it. And so as we've built our agent core gateway, that has been our, sorry, just agent core, that has been our focus is solving those problems of how you run real agents in production, how you get to reliability, security, and ultimately to positive ROI on your businesses. When I talk to folks. They want their teams to move fast. They want to unblock their developers and even non-developers, business users, to build agents with low risk. And when I talk to folks, they say, well, I wanna move fast, but I need to be able to move fast, securely and safely. And so by building a box that you can put agents in, that you can reason about, that you can have your security team think about, you can unlock your development teams and say, go off and build agents. We're gonna give you the sandbox to play in. And then once you've shown that you can do the right thing, we'll start giving you access to production tools where you can get access to real data, access to real customers, and then ultimately start changing the world, start having those side effects. And that is the core of endpoint of the agent journey. And so this is all about accelerating building agents by providing technology and infrastructure that makes it easier, faster, and lower risk to build agents. Thank you very much.