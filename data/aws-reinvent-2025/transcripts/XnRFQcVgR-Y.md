---
video_id: XnRFQcVgR-Y
video_url: https://www.youtube.com/watch?v=XnRFQcVgR-Y
is_generated: False
is_translatable: True
---

Welcome to RC 312. We're gonna talk about how AWS builds regions and hopefully what that means for you. Uh, my name's Don McGary. I'm a principal engineer in EC2, and my name is Michelle Tisson. I'm a principal TPM also with EC2. So before we kick it off, let's do a quick overview on what we're actually gonna be talking about today. So we'll start with a very quick recap on AWS global infrastructure concepts. We'll talk about how we build for resiliency and how you can do the same. We'll take you on a journey back in time on how we used to do builds and then transition to how we do things today because things have changed a lot in the past decade. We'll then talk about availability zones and local zones and then talk very in a lot more detail um on dependencies since those are. The primary problem when it comes to to builds, and then we'll leave you with some key takeaways. So first, let's talk about our AWS global infrastructure. So this map you'll see um all of the AWS regions we have the ones in green are the ones that have already been launched and the ones in red are the ones we're currently working on, and those include the EU sovereign cloud, Chile, and the Kingdom of Saudi Arabia. So overall right now we have 38 regions and we're continuing to add to those as we go, including more availability zones and. Other expansion options like local zones. So this is our local zone map. We have 38, uh, metro areas at the moment and we're expanding heavily in that space as well because, um, there's a lot of customer interest in leveraging local zones, but we'll dive more into that later. So next up, let's dive a little bit deeper on how you can think about our global infrastructure. So the, the highest level of abstraction in AWS's global infrastructure is uh a partition. Most customers uh don't necessarily need to think in terms of partitions because most customers are working in what we would call the commercial partition. If you look at an RN, it says RN, colon, colon, AWS, that's the partition, so the commercial partition is the AWS partition. That's where most customers live, but we've had multiple partitions for quite some time. There's the US GovCloud partition, which is US-gov. Uh, there's the China partition, AWS dash CN, and the reason we wanted to kind of call this out, uh, in this talk is the upcoming EU sovereign cloud is going to launch, and that will be its own partition. So a lot more customers will be having to deal with the partition abstraction than before. The difference, uh, with, you know, between partitions is partitions have an instance of our global services in them. So most notably IAM and Route 53 are the two global services that will, um, you know, come to mind when you need to work across multiple partitions. And what does that mean is, um. An IAM identity which includes an account number in the commercial partition, there's no concept of that identity in another partition because they're two separate stacks. So you'll, you know, if you need to migrate an application across uh or build it in a new. Partition things like IM principles and account numbers will be different because those are two different software stacks, um, so you know, outside of the global services partitions are just grouped for one or more regions that can be contained within one and only one partition. Now we'll dig into a little bit more in terms of regions, availability zones, and local zones. So a region is just a geographic area where we offer our services. Regions are comprised of 3 or more availability zones, and as a customer, you can just kind of think of an availability zone as a data center. In reality, there can be tens to even in some cases, hundreds of data centers that make up an individual availability zone. Those data centers within an availability zone are very close to each other from a latency perspective, both from a network path and from a shared fate perspective. So each availability zone within that zone, you can kind of think of it as, as sharing fate from a, uh, an availability perspective. But when we pick where we want our availability zones to be in a region, we want them to be close enough to each other to maintain single. digit millisecond latency between those availability zones, but they're far enough away that they won't share fate with each other from things like natural disasters, power events. So our, our regions are architected such that if you spread your workload across multiple availability zones, it's unlikely to have shared fate in an outage across those availability zones, and we extend that into our software stack as well, which we'll talk a little bit more about. We kind of extended the idea of an availability zone to this thing called a local zone. In a local zone you can think of it as an extension of an availability zone, so a local zone is parented to a single AZ within a region, but that local zone can be, uh, from a latency perspective, a pretty far distance away, and that what that does is the control plane that will, you know, launch instances in that local zone exists in the parent availability zone. But the capacity that's running your workload is in a metro area closer to your customers or in some cases for like machine learning in an area where we're able to acquire um lots more power to be able to run machine. Learning either inference or training workloads. So we, we've used the availability zone concept to essentially stretch capacity in an AZ to either bring it closer to your customers' workloads or to provide additional capacity for things like machine learning. Michelle will dig a little bit more into like how these concepts apply when, when you're thinking about them in, in your architecture. So let's talk a bit about how we can build for resiliency. So we've talked about regions, availability zones, um, so let's put this into, um, an example here. So we have a region we've selected two availability zones and our VPC spans those two availability zones. And what we wanna, uh, explain here is that the understanding the scope of the services that you're using is really important. So in this architecture diagram we have our load bouncer, an auto skilling group, and our EC2 instances. But what you'll see is that our AC2 instances are in specific availability zones, and that's because when you launch an EC2 instance you specify the AZ and so this is a zonal service. As opposed to these services such as S3, CloudWatch, Dynamo DB, these are regional services, so they exist at the regional level. When you create an S3 bucket and put objects into that bucket, you do it at the region level. You don't specify an AZ. And the reason why understanding this is important is because you can plan your operational responses accordingly. You can look at what metrics are important to your business and then make business decisions based off of that, for example, knowing when to scale up your EC2 instances or when you need to fail over in the event of a disaster recovery scenario. For the majority of our customers, you know, leveraging multiple availability zones within a region is sufficient, as Don mentioned. Um, however, there are specific use cases where a multi-region strategy might make sense, um, but that does come with some additional complexities managing DNS, um, and costs if you're replicating your entire environment in a different region. So hopefully this is a good introduction into some global infrastructure concepts kind of framing it um with resiliency in mind. Let's actually jump into how we build new regions. So region builds have changed a ton over the last decade. Even in the last few years, things have changed significantly. Um, however, there are two primary work streams that have remained fairly consistent. The first is the physical. So I know we talked about the elusive AWS cloud. At the end of the day they're still servers in data centers and so, uh, the physical work stream consists of everything from picking proposed proposed sites including looking at expansion options. When we first build a region, you know, we have 3 availability zones, but in the future we might want to add additional availability zones, um, so we have to keep that in mind when planning and picking our locations because as Don mentioned we have requirements on how far apart the data centers can be so. We need to consider all of that when picking the sites. Sorry. And then we have to construct the data centers, delivering cable all of the racks, and then connect everything to the AWS network. The second work stream is of course the software build and we're gonna spend most of our time in this work stream, um, and this involves everything from building out the most foundational things such as, you know, making sure that services can authenticate all the way out to building all of the AWS services in the new region. So as I mentioned, we've always had these two work streams, but the way we go about builds has actually changed a lot. So let's take it back to 2010 when when we were building the first region in South America. Yeah, so in 2010. AWS launched, uh, uh, announced the launch of the Sao Paulo region. It was our 8th region. Uh, we are the first major cloud provider to launch a region in, uh, on the South American continent. And uh AWS looked a lot different back then and the way that we, you know, you can see from the uh the blog post with the old logo, um, the way that we built regions was pretty different back then and, and to, to talk about how we built Brazil we'll have to meet you to introduce you to, uh, John, our fictional character, uh, John, uh, which John was an actual person that's not John, um. They don't let us put, you know, real people in the slides. Uh, uh, John, his job title was Region bootstrap ninja, and John would be at the office in Seattle and, you know, it'd be like, hey, it's time to, it's time to go, uh, build Sao Paulo. We've built the data centers, the racks arrived. John, here's 3 jump drives, one for each availability zone, and John would get on a plane, go up to the availability zone, kind of walk in there. Whatever server kind of, you know, struck his fancy, he'd plug in that jump drive and literally start hand hacking away, um, installing, uh, all of the bootstrap scripts to be able to launch our, at the time, our internal virtualization infrastructure that predated EC2, um, and this worked good for a while, but the problem with this is, you know. Over time, John needed to learn a lot of stuff, right? So you know it was like provisioning network equipment and then, you know, provisioning, uh, you know, our Zen stack and then like having to do a whole bunch of like red hat things and, and, you know, so John needed to learn more and more and more, so it was kind of hard to train more Johns over time. Uh, the second thing that got pretty challenging was, uh, you know, for all of us technically inclined here, you know, if I sit down at a computer and you ask me to do a step of about 30 or so different things, I'll usually do about 28 of them really good, uh, the first time, and then, you know, I'll fat finger one or two things and then it won't work and I'll have to go back and do. Bug it and then you know by this time I'm on the 2nd and the 3rd AZ I'm getting kind of lazy so I'm making more mistakes. So like it it it's, you know, sending one person there to do lots of stuff is a pretty error prone process, um, and flying people around the world, uh, that are a limited resource isn't a good way to be able to scale to multiple region. At once, so you know, we, we, we, we managed to build 8 regions this way, which is, you know, I think pretty good, um, but there were some other changes happening on, on, you know, kind of behind the scenes that, you know, unfortunately, uh, you know, put, put John's ninja bootstrap role, uh, kind of on ice and the first thing was. We, we've kind of continually innovated over time. So I mentioned that when AWS first launched, we ran the, the, the original services like S3 and SQS and, and some of those early services ran. On the infrastructure that this website Amazon.com, you may have heard of it, uh, ran on, uh, and, and then we launched DC2 and you know we thought that was really cool. I think it's the best service, uh, and, and that over time. We started to build all of our AWS services on top of EC2. Um, in fact, today, when you launch an instance in EC2, the machine that talks to the server that launches your, your instance for you. That is actually an EC2 instance. So EC2 runs on top of EC2. So everything runs on EC2 now, which kind of makes the, you know, John walks into a data center and goes to plug in jump drives and run servers. That doesn't really work because you need EC2 to be able to run EC2 and. You know, your head starts to hurt. The other thing is, uh, we, we came up with, uh, things like IAM opt-in regions where when we launch a new region your IAM, uh, information for your account doesn't exist in that region yet until you as a customer opt into using that region and then your IAM materials will propagate to a regional propagator in that region. Again, that made it really, really challenging for, you know, oh I walk into the data center and I start installing stuff cause like we need this regional identity stack to exist. So like we kept innovating over time. Customers wanted things like digital sovereignty and data protection and you know, more regions to be able to run their workload closer to their customers and so the demand for us to grow our global footprint continued to increase over time and that kind of gets to one of like the, the fundamental value perhaps that we talked about about cloud uh from the first reinvent actually is the ability to go global in minutes, right? I can be sitting. In my house in Northern Virginia and launch an instance in uh Australia just from the management console, right? I don't have to travel there and figure out Colo space and do all this other stuff. And you know, in order to make good on that value proposition to customers, we needed to expand our global footprint pretty aggressively. So, you know, the global reach of what, what our customers wanted to run on our platform really, really pushed us to expand. Uh, further and further. So with all this, how the heck do we build regions today? Well, it's now, uh, 2025. I can't believe I'm saying that. That's wild, um, and what have we done? Well, we have continued to launch more regions and availability zones and local zones. I just included these, uh, blog posts from some of the more recent regions that we've launched, um, but this just wouldn't have been possible using the approach that we used a while ago. So let's dive into how we build regions today. So as Don mentioned, you know, it was a very sequential process. We would do the physical build and then fly people in and do the software build. And in order to scale we had to parallellyze this work. So how do we do that? Well, we just build AWS on AWS, um, so we have all of these existing regions around the world. So what we do now is we use those existing regions to bootstrap the new regions. So instead of waiting around until the physical work stream is complete, we go and get it, we get a head start with the software build and so we can parallellyze the effort. So we have a build region here it's still under construction. Um, we're still super early on, um, in the planning phase. No racks have landed, but we're gonna get a head start on this region build. So we're gonna select an existing AWS region. We'll refer to this as our bootstrap region. Now we're very careful when selecting our bootstrap region because it's, you know, a production region we have customers running there. So we look into things like uh capacity considerations, um, the network latency between the bootstrap region and the build region because at the end of the day, you know, we need traffic to go back and forth and we don't wanna be traveling all the way around the world every single time, um, and then fiber um considerations and so once we selected this bootstrap region we go ahead and start that software build. So even super super super early on in this build we're designing with resiliency in mind. So our bootstrap VPC here is already spanning 3 availability zones and everything that we build in this bootstrap environment, every service, everything thinks it exists in the new build region. And we connect it up and eventually we will, you know, get the racks landed and everything good to go, but let's dive a little bit deeper into what's happening in that bootstrap VPC. Yeah, so we start with a VPC and we have an account just like any other customer. Well, We need to make this VPC look like the region that we're building. So we need to start building some services. Well, that's pretty easy. I can launch an instance and I can give it to a service team and say, hey, here you go. It's all good. Well, the service team's gonna go, hey, this is great, except I need to get, I need to get my AWS software on there. How am I gonna do that? Oh, that's a problem. Well, So, uh, the Amazon deployment system can't tell this AWS account from any of the other millions of AWS accounts. So we need a way to authenticate that host, and then we need a way to talk to our deployment system, and then we need a way. To make it look like it's the build region inside there, well, but OK, so we need to install some software on that that's just like an RPM, right? And what's really an RPM repo? It's just a web server and we're gonna need to do some DNS trickery because we're gonna need to make DNS inside this environment look like the region or the zone that we're building in. But what really is DNS? It's just something that answers on UDP 53 based on, you know, something I look up in a database. I can surely fake that out. And then most of our host identification, well, that's just a certificate. I can issue certificates. So what we, what we did in order to kinda. Fake all this stuff until the real services that do all these things come up because we launched another instance and we called it the Franken instance because it just kinda pieces together all of the stuff that you need to break the circular dependencies to get that host to. Talk to the deployment system to be able to get software on it. So we made a Franken instance and it does DNS and it does PKI and it does a little RPM repo, and that'll turn that generic EC2 instance into an Amazon service team host. And we can do that to build the early services like I am because that's what you need before you need anything else, right? Is who, you know, who, who am I talking to and how do I authorize them. All right, cool, so we built IM, right? So this should be easy. Now we just gotta build EC2. So in order to build EC-2 I've got IAM great. Well, remember when I mentioned that like everything runs on top of EC2? EC2 also uses some other AWS services because, you know, we didn't want to be left out of the fun. So we use most notably S3 and Dynamo DB. And in a production region, the way that we use S3 and Dynamo DB is really strongly kind of controlled and monitored such that if those services impair, like we've got a way to either continue portions of our operations or we'll degrade relative to those services degrading, but early on in the build we've got like a chicken and the egg problem, right? Because S3 needs EC2 to build, but EC2 needs S3 to build. We've got a problem. But I'm running in an AWS account. And there's S3 and Dynamo in the region. And what's really the difference between S3 and Dynamo in the bootstrap region and S3 and Dynamo in the build region? Well, the only difference really is the DNS is different, right? So if I was building US East 2, it would be S3.SE2. Amazon AWS.com, and if my donor was US East 1, the donor would be S3.Seast1. Amazon AWS. That's just DNS. I can fix that because I have the Franken instance. I can just make DNS whatever I want. And then when I. I need to off against that regional opt-in stack because when you're in a regionalized account, you need to talk to the regional opt-in stack. Well, so what if I just threw a little proxy server in there that responded on that DNS end point, shuffled your off over to the off that we just built, but then when it actually goes to store your data, it just turns around and uses the one in the bootstrap region. That's what we do. We have these little intelligent proxies which, like a good mullet, is build region in the front, bootstrap region in the back, and that's how we break the circular dependencies early on in EC2 to be able to allow us to use S3 in order to build DC2. And then once we build EC2, then S3 and Dynamo can build, and Michelle will talk a little bit about that. Yeah, so at this point, you know, EC2's core dependencies are up, and we can build out the EC2 control plane. We also build out some core networking services and other services in this bootstrap environment. And then You know, in our build region we're at the point where the racks have been cabled and we can actually launch EC2 instances into the build region using the control plane that exists in the bootstrap VPC. And once we can launch instances, well, that means that other services can start their builds. And so the first few services that build in the build region are S3 and Dynamo DB among a few other core services. Once S3 and Dynamo are available in the build region, we actually go ahead and migrate things over. So we take all of the data stored in the bootstrap region S3 and Dynamo DB. And we copy that over to S3 and Dynamo DB in the build region. We also then migrate all of the services that were built in the bootstrap VPC into the build region, um, and the way we do this is, um, service teams will scale up their instances in the build region and then scale down those in the bootstrap VPC. And remember this is all possible because. To a service, they are the same region, whether it's the VPC environment or the build region environment. And so, um, once the control plane is up and running in the build region, we have EC2, S3, Dynamo DB, all of the core services in the build region, so we have a very stable environment so that a bunch of other services can start their builds as well. Once that's done, we go ahead and terminate all of the resources in the bootstrap S3 and Dynamo DB. We also shut down and eventually terminate all of the instances that we're running in the bootstrap VPC. We then apply an access control list um to block traffic from the bootstrap region to the build region and vice versa. We then give it some bake time just to make sure that nothing is trying to call back and forth anymore. If everything looks good, we go ahead and disconnect the build region from the bootstrap region and then the build region is on its own. We build out all of the services we run our game days, and then we launch the region. So, um, that is at a very high level how we build a new AWS region. Uh, let's now jump to how we do this for availability zones and local zones. Yeah, so availability zones, it's a very similar process, albeit, uh, a lot simpler, actually. Um, but what's interesting about availability zones is It actually takes us longer to add a new availability zone onto an existing region than it does to build a new region from scratch and you're probably like come on man, why we're working on it though we're we are working on making it better but but the reason is. Our isolation and, and our, you know, our promise to, to customers is that, you know, one region is strongly isolated from another region, so. And, and when we're running in that bootstrap VPC we don't do anything special, we're just like any other customer, so we're not doing anything that will impact the region that we're running in. We, we really are running like a brand new region, so that's pretty isolated from everything else in the global AWS infrastructure stack, um, and there's no customers in the build region yet, so we can go really fast, um, and we do with an availability zone. You're operating in a live region with live customers, so from an availability perspective, there are a whole bunch of rules that we follow about deployment safety and not deploying to more than one zone in a region on a day and not deploying to so many zones and so like all of those rules apply and, and we wanna be really, really cautious that we don't do something in a build availability zone that would impact customers running in, uh, the parent region. Um, so we are expanding, uh, our availability zones, uh, worldwide, and that it's in, in some ways nice because it's a way to get additional resiliency and additional capacity available to customers without you having to rebuild your stack in a, in a new region. So, you know, provide some, uh, greater flexibility there. And the way that we do it is we essentially instead of having a bootstrap region, we have a bootstrap AZ and we just create a VPC there and it's a much smaller, you know, that that is to scale. It's a much smaller VPC. Uh, we only build the zonal services that comprise the EC2 control plane there. Um, as well as some of the zonal networking services, and then we do some fancy under the hood network peering to the, uh, zone that's being built. And once we can do that, we can provision, uh, hardware and launch instances and be able to do that same migration process that we do in the, um, uh, region build process. So, somewhat similar, um, slightly different, and the, you know, the fun fact is it actually takes longer, um, mostly due to deployment safety. So let's talk a bit about local zones next. So local zones exist, so customers can run their applications on AWS infrastructure closer to their end users, um, and to provide additional, uh, capacity. So we have a few different flavors of local zones. We have our standard local zones and we have, um, dedicated local zones which. Are dedicated to a specific customer, um, which means they might come with, um, additional, you know, security services or whatever that customer may require. And similar to uh an availability zone we have a parent AZ and the EC-2 control plane lives in that parent AZ and then we just extend over to the local zone. um, so the, the set of services that we build for a local zone is much, much smaller. Um, most of the complexity in a local zone build actually comes from setting up the, the network connectivity between the parent AZ and the local zone. Um, but similar to the AZ, you know, once we have that established, we can provision that. Um, capacity, the, the data plane essentially in the local zone. And um one of the, the huge benefits here is that um customers can run their workloads in different countries for example, so say you have a parent region in country A, um, but due to data residency requirements you need to store all of your data in country B. Well, using a local zone you could use some of the local zone versions. Of regional services like S3 Express to ensure that all of your data is stored in Country B to meet the data residency requirements you may have, um, so it's a, it's a really nifty way of, you know, extending AWS infrastructure capabilities to locations beyond, um, where we have regions. Um, next up we're going to jump into perhaps the most tricky part in all of this, um, building, so we're gonna talk about dependencies and how we deal with this, um, during builds. So Don touched on this a little bit earlier, um, it's. Tricky because we have a lot of circular dependencies between services and it continues to become more and more complicated as we add more services, more features, um, and more services wanting to leverage the benefits of services that we've developed previously like think of Lambda, Fargate. If you're a service team, why wouldn't you want to use those, right? So we've kind of broken it up into four main categories. Uh, the first is service bring up. So, uh, this is all about how we bring up all of the AWS services during a build. So we have, you know, hundreds of external AWS services, but probably thousands of internal services. So the way we handle bringing those up in batches, um. Involves quite a bit of complexity and we'll go through that using an example later then of dependency management um dependencies change all the time. It's almost impossible to maintain an accurate list of dependencies, especially at a granular level from a program management perspective when we build a new region or availability zone or local zone, we do have a high level dependency graph. This allows us to, you know, plan the schedule and make sure we launch the the builds on time. Um, but it's much higher level than, uh, you know, the individual service level because that's just almost impossible to, to maintain. Then we have static stability, so making sure things recover gracefully in the event that something goes wrong, um, and then continuous testing, so ensuring that all the things that we planned for, um, we actually test and make sure that they work. So to frame dependencies and how problematic they can be, we're gonna run you through an example. Uh, we'll use EC2's most foundational API run instances and talk you through it. Yeah, so up until this point, everything sounds pretty easy, right? I mean, I think it's easy. So, what does it take to run an instance? This is a really simple run instance call, right? Give me an instance from that Ay, make it a T3 micro, and I, I'll give it a key pair. What does that look like? Oh. Wow. So this is actually a screenshot from our internal API orchestration, uh, software that we use in EC2. That is what a run instance call looks like. Every one of those boxes on that chart, be them purple or yellow, are what we call agents, and an agent represents a service team. And a service team in AWS is a 2 pizza team, so it's roughly 8 to 10 engineers, and, and when you work on a two pizza team, you are responsible for your service and you own everything about that service, um, so, uh, in this API orchestration framework thingy that we have, if you hover. Over a box when you hover over it, the green lines highlight what gets called when that agent is getting called. So if you'd like stepped down the left side at various times, all of those little green lines will be pointing at different boxes. So each step in that process is what, you know, number of services that get called underneath the hood. So you can see from this, oh, this is a problem. So, um. This when we talk about a service. We talk about one of those little boxes on the screen, even though most people are like, 0, AC2, it's a service, um, so. When you come into region build and, and you know, obviously we need to run instances to be able to deliver capacity to people during region build, right? So like we gotta build, uh, you know, at a minimum the number of boxes that are on the screen and turns out actually a lot more. So when we talk about like, oh we need to build EC2, we're talking in the order of a couple 100 different services and those services just like, you know, EC2 depends on S3 and Dynamo, those services depend on each other. Um, so it gets really tricky with this dependency thing and like the, the engineer thing you wanna do when you come in is go like, well wait, there, there exists a dependency graph in which I could order every box on that screen in the perfect order to know what order I need to build everything in to, to be able to, to build EC2 and while that is factually accurate. The reality is that if you were able to piece together that dependency graph, 1, it would take you a while to do it, and 2, if I was able to like wave my magic wand and have that dependency graph exist at this very second in time, it would be out of date tomorrow. Because every one of those teams of 8 to 10 software engineers, they're writing code every day. And they're changing the behavior of the system every day and actually the reason why we built this API orchestration framework is we need every box on that to be able to change how API calls work in order to deliver new features for you and not be. Dependent on, oh, I need to because it used to be we had to talk to every person on that on that screen to go, uh, we're gonna have to change your own instance like everybody needs to get on the same page. So chasing dependencies in a distributed system can be a bit of a fool's errand. So how the heck do we make this thing actually work in a region build? Well, we've got this trick up our sleeve, and the trick is this thing called static stability. Static stability, usually when we talk about static stability, we talk about it in the context of, you know, have you tried unplugging it and plugging it back in, right? So, you know, we, we do that for entire data centers or availability zones. If the power goes out and the generators for some reason don't come on, which happens from time to time. You know, all of the servers in the data center go off, and then usually the power comes back on at some point and once the power comes back on, all of the servers in the data center turn back on and wouldn't it be nice if when that happened. EC2 just came back to life and it didn't require hundreds of engineers going in and going, oh my gosh, I gotta get my service working and then you have that dependency problem. Well, that's static stability, right? So one, because we run EC2 on EC2, we want the special EC2 instances that run EC2 to be able to come back and not lose their state. And secondary to that, we want all of the services to come up and just start working without hundreds of engineers having to go in and and poke things. Well, in order to make that work, you actually have to build your services in a certain way, and, and the way that we build our services is kind of twofold. The first is that I don't take a lot of dependencies to get my, to get my software from, you know, existing to running. I may not be able to do much, right? Like maybe I need, you know, in order to run a web server, I probably can't start if I don't have my SSL certificate, right? Because like that's not gonna work very well. I, I, I kinda need that. Um, but do I really need to talk to the monitoring system to be able to admit metrics just to get started? No. Do I need to be able to actually talk to the database? Do I need to talk to Dynamo DB to start my service up? Well, not really, right? I, I can handle API requests. I can't do anything with them, uh, you know, I'll get my, I'll get my little web server running. And it'll happily sit there and return a 500 error to any anything that that calls me. But if, if I can minimize the dependency surface that it takes just to get my service started and, and, you know, for those few dependencies that I need like my SSL cert or, you know, my IAM creds to be able to call something, those things are kind of important and you need those to start up. I'll just sit in an infinite loop. And go, can I start Crash? Can I start? Oh, I can start. I got my IM creds my cert, so I'll start. Oh, I can't talk to any end of my dependencies, so everybody who calls me just gets an error, but that's data that I didn't have before. So static stability starts in how we build software and that our services continually try to activate themselves no matter what, and they'll start with a minimum set of, uh, you know, a minimal dependency footprint. And then respond, you know, based on, you know, hey, when you make an API call to me, if I can't talk to my dependency, I'll return you an error, but when my dependency comes online, the next time you make an API call, oh, you get a 200, great, it works. We can use this in region build, because while I can't build that perfect dependency map of what EC2 looks like today, I can put stuff in buckets, right? I can, I can put stuff in the, you know, foundational database bucket of EC2 and the provisioning bucket and the fancy API layer bucket. And if I group services into, you know, groups of like 20 or 30 or 50, all I need to do is launch all 50. And then kind of let God sort it out for a little bit, right? Like let them all sit and spin for a little bit and send API traffic through the system, and in a perfect world, everything will get to active, all of the dependency things will solve each other and API calls will start to work. In the more realistic scenario where you know maybe 75% of everything works, well, at least I got 75% working and then I can kind of go poke along the API call path, fix the one or two things that didn't come up, and now everything is working. So we, we can use this thing that we do to recover from major uh outages and actually use it to our advantage to not have to really get into granular dependency mapping and project management and instead kind of go with the like well let's just do this in batches and and let this thing figure itself out. Um, so kind of moving on, uh, from that, we're able to talk about like, alright, that sounds cool, how do we actually like make this happen in real life and do things like testing to make sure that this stuff actually works. Yeah, so this, this all sounds great in theory, right? And we might plan for this engineer for it, but how do we know it actually works? Well, we run game days, so regardless of the type of build, regions, AZs, local zones, we have time in the schedule dedicated to game day testing. And that time is used to actually test things like what would happen if we power down an entire availability zone and so we'll just pull the plug on the AZ and we'll see what happens. Do things recover gracefully or do we have some gaps to close? And these game days have proven to be so valuable we actually built an entire test region to do game day testing. So we did an entire region built just for this test region. It has 3 availability zones. All of the AWS services are there and we run game days there. Very frequently and what this allowed us to do is um move the game days away from just being tied to specific builds um because you know there are only so many builds in a year, so many game days you can do in this test environment uh we can run game days whenever we want to um because those learnings are extremely valuable. What better way to learn than when things go disastrously wrong. Um, and sometimes things do go wrong. So whenever there is an incident, whether during a game day or in a production region. Our first priority is always to mitigate impact. Um, however, after we've done that, we wanna make sure that we figure out what went wrong. So, uh, one of the mechanisms we have at Amazon, not just in the build space but across the company, is a correction of errors mechanism or COE. Um, so after we've mitigated the impact of an event, um, the service team will actually go in and do a root cause analysis. So they'll author this document. There's a specific format for it. There are tight deadlines around it, um, and you really dive deep into what caused the issue. Um, you assign action items to your own engineering team and to other engineering teams to close gaps, um, and the, the primary thing here is it's not meant to be punitive, so the root cause of an issue. be oh it was operator error they fat fingered something um the reason why we don't let that be a root cause is because we're not going deep enough you know what enabled them to be able to do something like that it can't just be a person's fault. And so the, the COE process is very effective in getting learnings from when things go wrong, um, but we actually extend that to beyond just that individual team. So at AWS we have ops meetings. So these are at various levels we have them at the org level, EC2, and even AWS wide. And during these ops meetings, which are typically led by our principal engineers, um, everyone will review COEs and dashboards, and there'll be a discussion on, you know, one or two COEs, um, and in fact sometimes the principal engineers will assign action items to other teams because oftentimes when there's an operational issue and a team. Um, identifies some gaps. It's not just that team that could learn from that event. There, there are learnings that could be, uh, implemented in other places as well, and the ops meeting is one way that we as a company really prioritize sharing those learnings, um, and they happen on a weekly basis. Um, attendance is required, so it's, uh, it's a huge part of the culture and, um, a great mechanism to ensure that we're improving. And then finally, um, operational readiness reviews or ORRs. So this is a, a process teams go through on a regular basis to ensure that their services are operationally healthy, um, and so this is for existing services, but also if a service team is developing a new service or a new feature they would undergo this, um, ORR process, um, which also incorporates some of the learnings from, you know, noteworthy uh COEs. So these are some of the mechanisms that we use um across the company to you know really drive this culture of continuous improvement um through those mechanisms and uh testing. Let's move on to some more key takeaways. Um, hopefully you can learn a bit from what we've learned over the last few years. Yeah, so the first is, you know, I, I kind of. Excuse me, I got sick before reinvent, so. Figures, um, one is, you know, we, we, we kind of alluded to it like throughout the talk, but all of our services that that we build, you know, are kind of architected from the ground up with how we do resiliency, right? So our services are aware of, you know, hey, I am a zonal service and I am running in this zone and here are the other zonal services that I talked to you like that's really baked in. To all of our services, especially our stateful services, but even, you know, services that handle API call path for EC2, they are like, you know, built from the beginning, you know, those AZ's and the bootstrap VPC are not just there for show. We need that because our software depends on knowing what, you know, oh, I am a zonal service and I am running in this zone. Therefore, like I talk to other stuff in that zone and. That's handled through things like DNS, so trying to be able to incorporate that as you build services on our infrastructure and decide like what, what failure modes do you want for what components of your system and how do you design your foundational infrastructure in your VPC networking and in your DNS infrastructure around that to, to really have those strong silos um between your between your components. Second is, you know, when to centralize tasks versus when to distribute tasks. And we didn't quite get into this a ton, but Michelle and I worked on, on the same team in EC2 for a number of years, and that team was actually dedicated to doing region build for EC2. So for those hundreds of teams that were on the, you know, kind of cringe API, uh, diagram. We actually were a team that if, if, if a service team met our bar for automating their region build, we would do their region build for them. Uh, fun fact, if you're on a service team, nobody likes doing region builds like everybody, so like everybody is like, oh yeah, you wanna do my region build for me? Sure, I'll make sure my region build is automated so the central team could do it and, and that was actually like a novel thing that we discovered over time. So that that that program is called manage builds inside of inside of EC2. We also have another program called managed fleets. So each two pizza team, when I said they're responsible for everything that included, you know, how many hosts do I have in every zone in every region and oh I have to patch my hosts and I have to like fix broken hosts and do all these things with hosts and um for a service team like EC2. Where you're running in every region around the world and we keep adding more regions that actually is a lot of work and it's very again people don't like doing it nobody likes patching their infrastructure, right? Like let's be honest, so we, we actually started to develop centralized, you know, the, the problem is. Every team spends an hour a week on patching their hosts, if they're doing a good job. So when you have 1,000s of teams, they're spending 1,000s of hours on doing this undifferentiated work. But no one team is centralized to actually make that any better. What happens when you take That 1000 hours of work and put it on one central team, well, all of a sudden they're really incentivized to build really cool systems to make patching hosts happen at Amazon scale really, really fast and efficient and safe, and that's what happened in the region build space. Amazon generally speaking was this like kind of, uh, I guess you could call it anarchy right of all these teams just run around and kind of do whatever strikes their fancy. That's just how we do things over. Time we discovered that while that while that provides a lot of value and you can go really fast sometimes centralizing things is actually better and it it's really important you know each business is gonna be different because it's different how you do things but you really should think about hey when is it time to centralize this work versus when is it time to keep it distributed because the flip side of that is, you know, we struggle as a you know a centralized team with well when Region Builds become somebody else's. Problem my incentive to maintain my automation is not that great, um, and my incentive to like not do screwy things with my hosts is not that great, right? So there's a natural tension there which is why Amazon defaulted to, oh no, you're a service team, you own everything and oh by the way you're going on call for it. So if you do something silly and you get paged at 2 in the morning, you will be incentivized to fix that. So there's a trade off there and I think you know exploring that as part of your business and as part of your development life cycle is really important. Um Next, do everything in code. Please do everything in code. Um, all of our infrastructure is in code. We, you know, we obviously we use a lot of CDK internally. Everything is complicated when you run a low-level service because cloud formation runs on us, so we can't use cloud formation. So we have an internal infrastructure as code framework that we use, but like everything is an infrastructure is code and any type of regionalized config is actually auto generated as part of the build. Process right? and that gets into things like have predictable DNS end points for your service either at the regional or zonal level because then you could just have a little ruby script that runs that generates those end points for every single region that you ever wanna build in and then you never need to think about it again and same goes for configuration in in the software that you run like generate that at build time don't think about it have that live in your source control. We go, you know, we go a step further and, and actually this is becoming a little bit more interesting as we're starting to use, um, some of the generative AI tools for coding more is, um, we, most, most stuff in Amazon, most knowledge is in this gigantic wiki site, um, and you can write infrastructure as code to generate wiki pages, um, because we actually use wiki pages like for our operational dashboards for services so. Even our run books and dashboards for services are captured in our infrastructures code. The reason I mentioned kind of the Gen AI thing is a lot more teams have been doing it. You can get it to write halfway decent documentation. I, I, I can attest to this, um, and it can do things like. Graphs and diagrams pretty well and mark down so more stuff, more documentation is actually going into source control as marked down over time as some of our projects like start to use this stuff more. So I'd encourage you to like really give that a try because like the more stuff we put in source control, it's easier to search. It goes through to. PR like it generally has a structure of like if you show me a repo I know generally where to find stuff depending on what programming language it's written in so like really having everything in your source control repository system versus like some document repository that no one really likes is generally uh a best practice. And we have uh continuous improvement so we'll use builds as an example. Builds are, you know, a very, very long project, um, and so there's a tendency to say, oh, you know, we'll, we'll talk about lessons learned and retrospectives once the build is done. The problem is so much time passes, things have changed, people forget, um, so we really encourage, you know, to, to continuously look at what didn't go well and iterate, um, quickly. And the way we've done this, um, as an example, a few years ago there was this real push, um, at AWS to have service teams automate their builds, um, and we'd noticed trends. So for example we'd see that, you know, 1000 different teams were running into. The same blocker when trying to automate their builds and so instead of you know waiting and letting every single team work through that issue themselves um we took a step back, paused and said OK well this doesn't make sense why would we have every single team work through the same issue? um how about we cut a tracking SIM, make sure it's documented. Um, and we escalate that and ensure that the owning team resolves that centrally and that way, you know, the next build, this isn't an automation blocker for those 100 teams and those 100 teams don't have to worry about implementing workarounds themselves. Similarly we've had um sticky parts of the build where you know there was a lot of churn ownership isn't clear, a lot of back and forth on tickets um and so instead of just having that same issue every single build because it's just a tricky. Part of the build, um, what we've done in the past is we've actually assigned principal engineers to that ambiguous problem and then have them own that space find owners and drive improvements, um, instead of you know just letting it be an issue over and over again um and you know taking that approach of looking at each specific phase of the build and doing a quick retrospective. Lessons learned anything so you can quickly implement some changes before the next build um has really helped us improve things um very rapidly and the time we see the most improvement is actually when we have several builds back to back with like a short gap in between because we can implement lessons learned and then hit the next one, without those issues and so we slowly make progress across um each build. Um, and we've actually in the past staggered builds on purpose, um, to make sure that we're not just running into the exact same issue in four different builds at the same time, which is very painful for everyone involved. Um, and then finally culture. So one thing that's incredibly important in general but specifically in the build space is, you know, we have engineers who are actually doing the work. They are building out their services, um, they're building their automation. Um, and so they need to feel empowered to vocalize when things are just not working, you know, if they are running into those automation blockers or if they've been going back and forth on this issue over and over again, um, we wanna make sure that those concerns are heard and escalated to leadership so we can actually prioritize fixing them and so, um, at Amazon we have this very healthy escalation culture so when our engineers or anyone really if they. Say like hey this is a really big problem uh we make sure that our leadership hears that um leadership does need to be willing to listen and luckily our leadership does um but so we use that escalation culture to very quickly align on a path forward and this is specifically important for builds because you know once we make a commitment to our customers to launch a region at a specific time of year we need to meet that commitment and so builds move very very quickly. And if there's any friction, um, we need to escalate that quickly and um I'll get on the same page so we can move forward so culture is extremely important in general but especially if you have, you know, very large projects that span, um, multiple months or even years. So these are some of our, you know, main takeaways hopefully, um, they've been helpful to you. If you want to learn more, we have, uh, two topics that might be interesting if you enjoyed this session and then we have some, uh, builder library resources, one on static stability, um, and then one on building resilient services. So those are the two QR codes. We thank you so much for your time. Um, please, please, please take the survey we, we need that feedback, um, and we hope you have a great rest of your reinvent. Thank you. Thanks.