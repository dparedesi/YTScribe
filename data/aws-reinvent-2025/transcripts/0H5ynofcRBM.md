---
video_id: 0H5ynofcRBM
video_url: https://www.youtube.com/watch?v=0H5ynofcRBM
is_generated: False
is_translatable: True
---

Alright, welcome everybody. My name is Sohab Khatariwala. Uh, I'm a senior open search specialist essay with Amazon Open Search Service, and joining me today is, hey, I'm Joshua Bright, product manager within the Open searcher team, and today we're gonna be talking about intelligent observability and modernization with Amazon Open Search Service. So the takeaways for today's session are gonna be first we're gonna start a little bit about, you know, just using Amazon open search service for observability then we're gonna go into the new analytic features that have been launched over the past year and then finally we're gonna talk about agentic AI for observability so you wanna make sure you stick around for that one. But before we dive into the technical details, let's first talk about any company, a fictional company. Any company is a fast growing e-commerce platform that helps small and medium businesses create their own marketplaces. And it the goal is they're starting to become more and more popular with businesses looking to compete with major retailers. They have an architecture that contains of modern microservices, so their front end is react-based with a customer portal and a vendor dashboard. Their back end services are order and payment management and inventory management, authentication, etc. as you might have, um, and then their infrastructure is containerized services on AWS using EKS and they are also using databases like RDS and Dynamo DB for their databases, OK. But they have some challenges with this architecture. Number one, we're gonna call microservices complexity. This is where any company's distributed architecture creates some visibility challenges across lots of microservices that they have. When checkout failures spike during flash sales, teams are struggling to find whether it's a payment service that has an issue or the inventory service that has an issue or some sort of connectivity, right? Uh, so that's the. Microservices complexity that the first challenge is. The second one we're gonna call data pipeline chaos. So with multiple data sources and databases and data formats from different services coming in, the administrators and engineers face constant challenges making sure the pipelines are calibrated and they're performing, uh, up to par. The third is developer productivity. So DevOps teams spend hours manually looking through logs and correlating issues across traces and metrics, and they need to write complex queries to understand what's going on, to build some custom visualizations for the different stakeholders that want to know what's going on across our different uh applications. And so it takes a lot of time for these developers to be productive. And then finally, cost management, right? This is where the observability observability data volume continues to grow and explode. As with many customers, uh, for the past year it's grown over 300% for any company, and they're continuing to scale, which makes traditional monitoring solutions cost prohibitive. Are any of you facing any similar challenges in your organization? I see some hands and some head shaking so this is what we hear from a lot of our customers, right? So we've kind of compiled these things from all the customer conversations that we have and we're representing it through this, uh, fictional company called Any Company. Now, what we see is the, the common challenges that customers tell us every day. Most modern applications like the e-commerce platform that any company has are distributed across many, many services and microservices as you can see in the diagram. And with these many uh services and microservices, the visibility into them can be extremely low, especially for inter microservice interactions and interaction with other AWS services. When any company's check out uh failures spike during their Black Friday traffic surge. The engineering team was left wondering what was the issue? Was it a bug, uh, in the checkout service? Was it a payment API failure? Was it a database connectivity issue? What's really going on? And the reality is that these decoupled code and services are really, really hard to diagnose. So Each of any company's components emit some signals through logs, metrics and traces, also called telemetry, and their checkout service logs errors, payment service tracks transaction metrics, etc. etc. and this is creating, you know, a lot of different signals and having to manually correlate and find the, the needle in the haystack becomes very challenging, and this is where teams get stuck during a crisis. Recently, for example, any company's main website went down and engineers had to manually grip their logs and spent hours trying to find what was happening across their distributed, uh, system. And so to remediate these failures effectively, you need analysis of interactions and code across all these distributed components. This is where a unified observability becomes critical and where a comprehensive observability solution transforms how any company's uh teams can monitor, troubleshoot and optimize their modern cloud native applications. So what is an observability platform and why is it useful? Just to recap, an observability platform collects information from your entire system in real time to help find and resolve the unexpected or unknown issues. It helps the builders and administrators efficiently detect, investigate, and remediate issues. So usually this is done by creating insights over telemetry such as metrics, logs, and traces, and an observability platform offers developers the ability to understand the applications better and tools to analyze root causes in the event of failures and as you know it's a very, very important part of a workload just like other things such as scalability and loose coupling observability is very critical to get right. So AWS as a whole provides many choices in monitoring and observability services that you can use to collect, to store, to investigate, and alarm on data from your infrastructure and your applications, and together these services complement each other by providing insights and analytics using predefined instrumentation. And visualizations, for example, Amazon CloudWatch, which you can see there is a service that monitors applications and responds to performance changes uh, it optimizes resource use. It's really useful for simpler uh real-time performance monitoring of your AWS environment. And then you also see Amazon managed service for Prometheus, which is a managed monitoring uh and alerting service which provides data and actionable insights for container environments that are deployed at scale. And then you also see Amazon open search Service which allows for deep log analysis for complex search needs across logs, metrics and traces, especially for a longer term data storage needs. So as you've heard, any company has a very complex tech stack with many components that require deeper analysis capabilities, and they'd require longer term storage, and this is what makes Amazon open source service a great fit for any company. So today of course we're going to focus on achieving a world class level of data driven insights using Amazon Open Search Service and all the exciting new uh enhancements and features that have been launched that make achieving this full stack observability easier than ever. So OpenSearch itself is a community driven open source platform which is extremely versatile and covers lots of use cases that include lexical search, vector search, semantic search, and then they also include observability. And thousands of customers trust OpenSearch with their production workloads. In 2024 and 2025, uh, OpenSearch Software Foundation has become a project of the Linux Foundation now. So this is to foster an open collaboration for search analytics and observability. And as you can see, many companies have already, uh, joined the project that you can see listed there to continue supporting the open source project and the ecosystem. The OpenSearch project continues to grow with 1.3 billion downloads and more and more than 3400 active contributors and now at 28 releases and you can see many, many, uh, members are joining contributors are joining so you know one thing, um, you can take away from here is if you're interested in contributing back this is a great community to get involved with. Now Amazon Open Search Service is an AWS managed service that lets you run and scale open search clusters without having to worry about the managing the monitoring and the maintenance of the infrastructure or having to have the expertise in the operations of managing the cluster. That's what Amazon Open Search Service gives you. Open search and open search service is a robust ecosystem of tools that can make it easy and fast to build a robust observability platform, so to ingest, to filter, to transform, to enrich the data and route it from your applications to open search domain or serverless collection. You can use the Amazon open search ingestion, which is a feature of open search service which is great at ingestion to store the data. You can use either open source, uh, cluster of open search that you've deployed yourself and managed yourself, or what we recommend is to use Amazon Open Search Service, which takes all the, uh, administration out of having to deploy it yourself and then to take it even further, you can use Amazon Open Search Servi list which is completely zero administration, automatic scaling, and you pay for what you use. And then finally You can use uh to to do application debugging to do visualizations, and to analyze application behavior you can use open search dashboards, which is a purpose built user experience to get insights out of your data. So now I'm gonna go into each of these pieces of the uh architecture and talk about how any company can use open search service to accomplish their goal uh of addressing those challenges that we talked about earlier. So first we'll talk about how any company can start collecting signals from their microservices and pieces of their uh infrastructure. Right, so your applications just like any companies can be either AWS native or they could be custom applications with infrastructure such as databases, containers, virtual environments. So any company collects these, uh, by running which is generally called agents, and agents are basically small software processes that run next to, uh, their application or run in parallel. To the containers to collect metrics to collect logs and traces and export them forward to the observability solution. And one of the most popular mechanisms for collecting the data we see that AWS customers using is open telemetry. It's a set of vendor agnostic SDKs and libraries to instrument your applications. It supports logs, metrics, and traces collection, and due to the popularity we've seen many, many vendors start to support it. And so AWS also offers open distribution of open telemetry which is uh a way that supports natively many many AWS services and it offers collection from and storing telemetry into the native AWS solutions like open search service. So now any company sends their data from these collectors into Amazon Open Search ingestion, which is a feature of Open search service which gathers the data and buffers it using its built-in buffering capability and then before, uh, writing it to open search it, you can transform the data so any company can transform it to format the data in the way that they would like and then they forward it to, uh, Amazon Open Search Service. They also enrich their data. They can parse logs and metrics during this ingestion using ingestion, uh, open search ingestion. So now that the data is collected, it's transformed now open search injection pipelines write this data to Amazon Open Search Service uh clusters or domains for short to medium medium term storage and analysis and then open search service also has built in tiering that allows any company to retain data for longer periods of time at lower costs, so. You may decide to store some of the data in open search into the hot tier and then a warmer tier or directly in S3 as a really, really sort of a cold level tier of data and read it into open search service on demand. All of this data has the insights that any company needs to debug. Or understand their applications. OK, so now the data is stored. How do they surface this, this data and allow their engineers and DevOps teams to get insights out of it? So to surface these insights, open search service comes with a built-in guided user experience called Open search dashboards. And um now it's called OpenSearch UI which we will discuss in detail in the next section. So Now that we've seen how we've um collected the data, we transformed it, we've stored it, we can use dashboards now we need to see how to measure and gain insights from this data. So let's go back to any company's engineering team, right? They need to know exactly what to measure across their distributed architecture and how to interpret those measurements and turn them into performance improvements. So the DevOps teams need to measure check out latency. They need to measure the response times from the different services, etc. So how will they be able to do that? Well, They can use open search dashboards, which is a purpose-built user experience to get the most out of your observability data. So this is sort of the landing place where you connect to open search UI open search dashboards. You see, you know, the dashboards that you've created. It comes preconfigured with widgets that that give you insights, uh, that you need and can help you perform root cause analysis. And it also offers the ability to create new visualizations with easy drag and drop capability. Once you're happy with your visualizations you can embed them in your applications, uh, and it's multi-tenant which means you can have multiple teams, uh, with access to different applications and data and have the right people looking into the right data that they're responsible for. Now the analysts and developers can also use something called the Discover uh experience. Discover is used to explore the data using a variety of different supported languages, including dashboard query language, Lucine, uh, standard SQL, and PPL pipe processing language, as well as natural language queries as well, which you can see in the, uh, screenshot below powered by Amazon Que. So when you're debugging or when you're analyzing application behavior, you need an ability to filter to calculate statistics to sort the data, right, that gives you the insights, uh, that you're looking for. Open search service has a powerful pipe processing language which you can use to filter and measure various metrics or KPIs. For example, you can construct your desired outcome. In a step by step manner with each step getting you closer to your desired result, now dashboards and the discovery experience are really good if you are there and you're looking at them and you know what to go after, but what if you're not around, right? What if you're not actively logged in and looking at this user experience and you really want the observability solution to keep an eye on the application telemetry for you. So open source service has a robust monitoring and anomaly detection capability which keeps an eye on your data and sends you an alert in the case of a failure or in the case that it finds any unusual or anomalous patterns and you don't really need any machine learning experience to configure these anomaly detectors and set up alerts which makes it really popular amongst um any company's teams. So speaking of alerts and notifications, you can receive these notifications in a variety of different ways or channels, and you can send it to your favorite Slack channel, for example, any companies, teams routed to their Slack channel and they're routed to their mobile devices using their built-in, uh, alerting and incident management tools such as Pager Duty or Ops Genie, and generally you have a link back to the dashboards in the notification that is sent to these channels. So the developers can quickly click on that link and continue the investigation by logging into the UI. So, The other experience what we see is, you know, microservices are very common amongst most of our customers that we talked to today and when you're working with microservices there are many moving parts and investigating all of these moving parts can be difficult. So to make this simpler, we use what's called traces, right? Traces will capture the communication between different services so that you can know what happens when a service calls another service, was it successful or not, etc. OpenSearch dashboards offers purpose-built visualizations that analyze this data. We have something called a service map with which you can see if there's any error in your application, uh, at a glance or if your applications are facing any issues, uh, with higher latency and error rates, etc. You also have something called trace group visualization which groups related traces together into a single widget and allows you to see if a certain function in your application, for example, check out is facing an issue. Right, so now with these different visualization capabilities you can pinpoint exactly where to look, you can uncover the logs that cause the issues using traces, and then you can start analyzing those logs. So now the volume of operational data that customers need to analyze is continuing to grow, as we hear from many customers, the data volumes are continuing to grow. The same thing with uh any company. Open search service already supports observability workloads up to 25 petabytes, and of course in the future, uh, like everything else, it's continuing to grow. To meet this growing scale, customers often store operational data across multiple open search service deployments. Maybe they have multiple clusters. Maybe they have some clusters and some open search serverless collections, right? And customers are increasingly asking open search to work across multiple data sources. So, so they want the dashboard experience, but they don't wanna tie it to a specific cluster. So to centralize all this data management and give this view in a single place we've launched the next generation uh open search UI. The next generation OpenSearch UI is an independent dashboard application which is designed to help customers aggregate comprehensive insights into a single unified view which is. Uh, which allows you to see and view data across multiple open search domains and collections, and currently applications can be associated with multiple open search clusters, open search serverless collections, and even other sources like direct query to S3. Now that all these dashboard instances are consolidated, it becomes even more important to have a way to organize the data and the dashboards from the different sources, the different alerts, the different saved queries. So that's why we introduced workspaces in OpenSearch UI with the workspaces you can easily create your dashboards, save them. And as well as your alerts and queries in a private space, this private space allows you to manage permissions tailored to how your team, uh, needs to share their data. And Workspaces gives you a curated experience for popular use cases as well, such as observability, such as security analytics, etc. so you can find it straightforward to build content, uh, for your use case. Workspace also supports collaborator management so that you can share your workspace only to your intended collaborators, right, and manage permissions for each collaborator as you'd like. So well that was all launched last year. There have been some exciting updates and features that we've added to OpenSearch UI uh this year so far and now I'll turn it over to Joshua to walk you through, uh, those new updates now. Awesome, hey, thanks. So hey, I really appreciate it. Everybody able to hear me OK? OK, perfect. Um, before I get started in talking about, you know, what's new within OpenSearch Service, I wanted to just take a moment. We're on the heels of Thanksgiving holiday, so I just wanna thank all of, uh, OpenSearch customers as well as the community, open source community. We really appreciate your partnership and really love working backwards with you, uh, and I look forward to announcing more what's new features next year, um, in conjunction with you. OK, so the what's new section is gonna be two parts. So one of them is more of your visual deterministic analytics with OpenSearch UI. That'll be my section, and the second section will be with, uh, Sohai, who will be talking about, um, agentic development. Let's go ahead and get started though. So we looked into the issues that any company was um. You know, calling out as as problems, um, and we realized that, you know, it seemed like everyone was trying to solve this with more features and more complexity, um, we took kind of the opposite approach. What if we made log analytics simpler, not more complex? What if we prioritize the user experience. Instead of adding additional features and bolting things on. And that's what we've done With OpenSearch UI's observability workspace. We've made pipe processing language, the language that So hey we're talking about a little bit earlier, we made that as the forefront and we've segmented it or complemented it with, um, AI in the form of natural language as well as, uh, with a result summarization feature. So now we've made this experience integrated instead of it being kind of this bolted on um uh workflow uh now you can query with PPL and supplement it with the natural language experience pretty great. Um, so in addition to that though, we've also made it easier to ingest data from open telemetry so that teams like those at any company can go from their raw data to actual insights with very little effort. And let me give you a concrete example of what I mean, so any company's data admins. We're complaining, right, like, hey, it takes a lot of time for me to set up these pipelines that I, I want to ingest it into open search, um, or into other log analytics tools. Gotta configure parsers. You probably have all run into this configure all of your mappings, um, and you're kind of debugging and playing whack a mole trying to figure out like why data is not landing the way that you would expect. Um, the setup is, uh, a lot of overhead and super frustrating, uh, when you're just trying to get something out the door. So our approach kind of flips this completely, uh, we provide out of the box blueprints that will get things set up for you so we cover things, you know, popular AWS logs certainly like AOB logs, cloud trail logs, lambda logs, as well as third party logs like Jira integration, um, open telemetry certainly, as well as, uh, HTTP, uh, Apache logs. So, but that wasn't enough. We also wanted to make the workflow fundamentally easier so we have a new get started workflow you can see it in the open search console where you have this new set up that allows you to set up an open telemetry pipeline all the bells and whistles are included. You point it over at your cluster and it will automatically set up an open search UI instance for you. So no more dillydallying trying to get your proof of concepts up or new pipelines super easy, um. So any company teams can now get started analyzing their logs whether that be their React front end, order processing service or recommendation engine in minutes instead of days. But even when you get through the onboarding, right? You hit another wall which is actually being able to utilize the tooling like so customers like any company told us that they have lots of queries and dashboards that already set up in another tool, you know they don't want to relearn. Relearn a new tool, a new language, or new work flows. So we made a fundamental decision. We made pipe processing language feel familiar. If you know piped delimited languages from, you know, Unix or other tools, you'll feel at home in open search service. We aligned our syntax, our commands, our functions to feel natural. And the result, any company's team's existing knowledge becomes an asset. It's not a liability, and their migration from which was gonna be taking, you know, uh, 6 months, you know, to a year now is something that can be accomplished within, you know, a few weeks. But language is just the start, isn't it? You need also need the right words or in this case commands to express the complex ideas that you're interested in um. Um, extracting, so over the past year we've more than doubled pipe processing language capabilities. We've added joins and lookups to be able to join indices together. Um, we've added comprehensive, uh, time analysis commands like time chart and event stats that allow you to understand events over time, and in addition we've included the ability to extract unstructured data, not possible in in open search currently. The ability to extract the data and create new uh fields for your analysis using Rx and SPth so. Um, this isn't about just like adding more features, it's about having the right tools to ask sophisticated questions of your data. And now any company can correlate their checkout failures with their recommendation engine um in a single query. Let me show you what I mean. Anyone can write a query that says show me all the errors, right? That's table stakes. But uh observability isn't just about collecting data, it's about asking sophisticated questions. And the real insights come from when you can quickly identify errors that are occurring, quantify how large the impact is, and understand the next course of action. For any company that means connecting checkout failures. With their authentication service performance and payment processing latency. That's where you find the root cause, not just symptoms. And now you have all the tools that you need to gather those insights. Any company's teams were losing quite a few hours moving in between work flows as well, so we talked a little bit about a familiar syntax we talked a little bit about the new commands and functions that exist. The last piece that we wanted to target was the improved user experience and streamlining that so. At any company they were moving between the querying experience and the visualization building experience and the dashboarding experience and what we've done is we said OK forget all that like let's consolidate everything into the discovery experience and so what we've done instead. Is we've built out that those different work flows into Discover so now when you analyze your data you're also able to not only do that from a results perspective but also you can enhance and complement that with visualizations. Um, and then very easily be able to add that into your dashboard, so no longer you kind of moving in between different areas of your logging tool you can capture all of those critical things that you need to set up and support your APIs all within Discover. So now, you know, folks can stay in the flow from question to answer to action. And this applies to your data in OpenSearch but certainly applies to the data where it rests. So, so have been mentioned, we also have integrations with. Uh, Amazon S3 for historical and audit logs, um, your data in cloud Watch logs instead of piping your data from cloud Watch logs into OpenSearch, you can analyze your data in cloud watch logs from OpenSearch and then certainly from, you know, doing, uh, security investigations for Security Lake. So let's bring this together. We built our solution on 3 pillars easy startup. So you have easy start up with these new blueprints that allow you to get started with very common log types. Be able to create this new get started work flow that allows you to very quickly create the new pipelines and utilize OpenSearch UI. We've made it easier to get started because we built out a familiar syntax. That everyone is, um, everyone coming from pipe languages, uh, easily understands and we've also made it easy to get started. By incorporating the natural language prompts within the querying experience itself so you can ask questions of your data and get that analysis back. And if that wasn't enough. We also have the AI summarization feature which allows you to understand you could as you type in your query it will summarize the results and provide you an understanding of uh what is um in the results set. So That's kind of the easy startup section. The next section that we did is we. Added additional commands and functions in pipe processing language to really unlock insights like never before. And then last we created a cohesive work experience so now you no longer have to move in through different work flows you can accomplish all of your insights right within Open search Discover and be able to very easily create your visualizations and add those over into a dashboard. I'm gonna go ahead and get over to the demo because there's a lot of talking but I like to see action so we'll move over to the demo next. What we're gonna do is we're going to um. We understand that there's a problem with the low generator service and so what we're gonna do is we're going to uh. Query the load generator service um and or understand what kind of errors are coming through so I do here I do a simple ware statement which pulls all of the errors from the logs. Um, and that's cool. So now we can see that the load generator service is showing up for the next section. What we'll do is I wanna show off this unstructured, the ability to pull out unstructured data. So I'm gonna show here that we have the uh Rex command which allows you to extract the data using regular expression. Um, so you can see in the table below that you have error type as well as error message. Really, and the nice thing is is we have all of these different types of visualizations that you can select from right within the Discover experience so that's what I mean about having this comprehensive and cohesive experience it's all within Discover no more kind of having to move around to accomplish those tasks that you were hoping to do before. Next, what I would like to do is understand the error rate. So, um, you know, I'm going to be filing a ticket, uh, because so I'm gonna do this investigation. I need to file a ticket, so I need to understand what's happening, um, from an error perspective so. You can see I use event stats which allows me to track the the error over time and I'm doing the total events as well as the error count above um and calculating the error rate. I don't know about you, but, uh, 16% error rate, that's no bueno, that's no bueno. So we gotta do better and I'm gonna assign this ticket to someone who's going to help us out. In order to do that though, because I don't inherently know where the ticket needs to go, I'm going to join our this air data with our service catalog so now with um. But before we do that. We need to understand where um when the error had occurred um so that we can fill out the ticket properly so we use the time chart command to go on ahead and understand what happened and so with this lovely visualization I'm able to very quickly understand when the error started, when the error ended, so I can fill out the full case detail of this ticket. And then last, that's when we get to the join. So I have a service catalog off the side. It has all of the details of who is responsible for what and now I'm able to join my data across the errors uh or the logs, uh, with the, um, service catalog data, and I see that Charlie, uh, unfortunately Charlie's in trouble. Charlie is going to get a ticket, um, but now you know that's, I think that's a really great thing. So now we have. All sorts of these new commands pipe processing language that we we weren't able to do before with DQL, uh, so now we're able to unlock all sorts of, um, new insights, uh, and we're really excited about that, um. But what if you're not, you know, a piped language expert or guru? Well, I talked a little bit earlier about the AI assistant that helps you build out your queries so I can very easily just come in here and type out an English prompt. The nice thing is not only do I get the results, fantastic of course, but I also get the PPL statement as well so you're able to learn. With um You're able to learn as well as get the results that you need uh using this language assistant. And I can iterate on it, see, so now I can go over into the query bar, make adjustments to the query if I wanted to, um, so super easy. But let's say it's 2 in the morning and I got paged, super frustrating. I know where we've all been there. I have the AI summary feature as well so I can go and execute the query and then use the AI summary feature to extract those insights from the results set to very quickly give me hints as to what to do next and who to contact. I mentioned the visualizations and all of the options that we have with visualizations, but really it is as easy as you execute your query, you analyze your results, you get that perfect visualization that you need for, you know, supplemental materials for your ticket and you can add it to a dashboard from right within the Discover experience. Bada bing bada boom. Now we got a dashboard. Easy, easy. So what's even easier. Right, we, we talked about, you know, having a easy set up, great rich analytics experience, cohesive experience from a discover visualization. Soheb's gonna talk to us about agentic development, so. Looking forward to that. Thank you. Alright, now the exciting stuff, right? So we've seen how any company engineers and analysts can use this improved UI experience to easily get started, to use familiar languages, and analyze data using a rich set of features and query languages to reduce the time to resolution. But, uh, you know, before I jump into the, the next thing, uh, which is a separate team in any company wants to take it really even a step further. They want to know how they can use AI. To uh an agents to help with the same kind of things that we saw to speed up investigations and the time to resolution. Without needing to even have any engineers and analysts using this UI experience and so what what we hear from customers are these two different sort of, uh, requirements, right? One is how can we work without having a team of engineers and analysts. So this is where teams are, you know, staffed, uh, short, and they don't have the time to go and do the investigation and use PPL like we saw, and this is where they could use the help of an AI agent. And there are other teams where they already have engineers and analysts that are used to using these tools and they just wanna make it easier and they prefer to go themselves and build visualizations and build these um integrated queries and visualizations so they want the best of both. They want the ability to easily do the UI and they also want this easy agentic experience and that's why we have uh the second piece of it which is how can we use uh AI agents to make this even easier. So any company wants to provide deep and actionable visibility uh to AI agents that can monitor and analyze and reason and improve observability process within their organization. They want to have an AI agent that has access to tools such as the list of indexes in the open search clusters and other data sources that Joshua showed like S3 and CloudWatch, etc. And they want this AI agent to be able to get the metadata from the open search cluster as well to see what are all the indexes there, what are they called, what are the fields there so that it can properly know which fields to query and where to run the filters, and they want to do this very easily to get started and do a quick POC to see does this work before they decide to move, you know, if they wanna move this into production, so they wanna. See how can we start to do this and what steps can we start using uh to try to remediate future issues as well using AI agents. And so this is where one of the main components of Open search becomes helpful where you know OpenSearch already has a capability called Model context protocol or MCP. I'm sure you've heard a lot about it in many other sessions, but just as a recap, traditionally connecting multiple AI agents to different data sources required individual connections from each of the agents to each source and so MCP simplifies this by introducing a centralized component. This component handles all the boilerplate code for connectivity for making the system more efficient and manageable. The MCP consists of these two parts usually where you have an MCP server and an MCP client. The server is a lightweight program that invokes the rest APIs of services like open search service in this case, and the server or and the client is an adapter for allowing the AI agent to use the server's functionalities. And for open source specifically it has this MCP server in the open source, uh, community. It was developed by the community, um, and it's community driven development and support. So that's one major benefit of using OpenSearch, uh, and the open search MCP server. It has flexible communications and it supports standard IO protocols as well as streaming. Um, and it's adaptable and then it has a comprehensive suite of tools available, uh, read only tools such as, you know, the ability to search data, the ability to check the cluster's health itself, uh, and check the performance metrics, etc. and then finally has robust, uh, security options with different, um, authentication methods. So we've put together a quick uh demo for any company to show the power of using this MCP server to connect to OpenSearch with their AI agents. For this demo I'm using Amazon Que developer CLI, which is now, um, renamed to Quiro, um, you know, CLI. Uh, as you might hear about it, uh, as well, so to get to easily get started, this is a very simple architecture where we have, uh, Q developer CLI. We have the MCP server stood up and connected to the Queveer CLI, and it knows about the open search, uh, cluster through the MCP server. So to set context for the demo that I'm gonna be showing you. Um, it's a POC demo. Uh, it's Black Friday morning for any company, right? Traffic is 10 times normal levels, and suddenly checkout failures start occurring. Uh, the engineer team gets alerts, but they're overwhelmed with, uh, data from many, many different microservices also sending alerts, and this is where they wanna see if the AI observability agent powered by the MCP server can come to the rescue. So here um I already have Q developer CLI launched and I've already connected it to the open search MCP server which you can see at the top, uh, it's loaded already successfully. And so right away we can just start asking questions. So the first thing what we wanna do is, you know, uh, oh by the way, this is the, the indexes that exist in my open search cluster already. So there is a any company, uh, app logs index. There is any company metrics index, and then there's a traces index. We've put them in the same cluster, but they could be across different, um, different, uh, sources. So the first question we're gonna ask is, you know, we're seeing increased error rates across our checkout services can you investigate what's happening and provide us a root cause analysis, so. The agent uses all the tools that it has uh to figure out what's going on so immediately we can see that it starts running queries against the open search cluster against the indexes to fetch the data and we can, we can even see the query that it ran and we can see the index name is any company app logs we can see the cluster name and then we can also see additional things such as filter clauses in the query itself. It's filtering for a service called checkout. You know this is so useful for uh for for analysts which they don't have to write queries and usually when you write a query you have to run it a bunch of times and maybe adjust it and we can see the agent does that also you know the first query didn't get the right results so it actually went back to another tool called index mapping to learn more about what's the metadata, what fields exist, and then rewrite the query using the correct field name. And then it does the same thing again and again to the other indexes that seem relevant so it checks traces as well and it also checks metrics and it keeps going back and forth between running queries, checking meta metadata for what fields exist in my index, and then readjusting the filters to apply. Let's check this field, let's check this field, etc. so it's going through and running multiple queries. So useful to save, you know, hours and hours of time of a human having to do all of this and then finally it takes all the data gathered from all the queries and synthesizes it together into a single, uh, analysis so it tells us, you know, based on the investigation of the data, here's what happens. So first we get a timeline of events super useful. We know at 1:0:00 a.m. things were normal and then at 10:30 there was an incident peak. And it gives us the data points to to prove why it thinks that's the incident peak because it saw error rates spiking and then finally it gives us an actual root cause which is what we're looking for and the primary cause is because, uh, you know, the service was unavailable, uh, the checkout service became unavailable. So now we know that this checkout service is really what caused all the subsequent alerts and, and issues that started the whole, uh, snowball. And it even gives us from the trace analysis one specific service name called process order which had the error starting. So now we want to dive a little deeper, right? We say, OK, the checkout errors seem to be related to the payment, um, processing. Can you trace the request flow and identify where the actual bottleneck is occurring that caused these issues to happen? So again, it'll go through basically the same process, uh, again, um, it's running some queries. You'll see the queries are a little bit different now. The filters are different. It's looking for payment first, um, and then it'll synthesize this data to try to find what's the actual bottleneck that caused, um, that service to fail and give errors. OK, so now it's going to get the trace flow. We can even see trace IDs so it's identify, OK, trace 003 and 004 correlate to this issue. So we'll learn, we'll dig in deeper to those specific trace IDs and find the correlated logs, um, again. Uh, you know, saves hours of time for a human to have to do all this analysis, uh. And then finally it's going to look further into the logs. And now metrics, so, uh, any related metrics if not, um, it can just kind of ignore that so now it's it's synthesize all of that, uh, and now let's see what the bottleneck is. The bottleneck identified in this case seems to be the database time out uh in the payment service and so it gives us another uh bit of data that's helpful but really at the bottom you see the root cause that's where the helpful piece of information is that I was looking for which is tell me what the root cause is and primary issue. Seems to be the connection timed out after 5,005,000 milliseconds on our database service. So due to the spike of queries against our database, the queries are running slower and then eventually they hit a time out which caused the errors which cause other downstream effects and it even shows us other failures that are correlated. So super helpful, um, and then, you know, again it summarizes it all so if you wanted you could kind of copy and paste this and send it to leadership to say OK here's what what's going on, um, and now of course we wanna know is, OK, we identified the issue what's the impact, right? What what's the impact of this, um, time out and how can we fix it? That's the next natural questions. So we'll ask the agent the next, uh, question is what's the business impact of this issue actually occurring. And how many customers are affected and what's the revenue at risk from um this this issue. OK, so yeah, this is the next question business impact. So the interesting thing is we also have some non um logs related data that we've given the agent access to so we've given this agent access to our sales uh tables as well so you can see previous sales history. Uh, and in this case it's able to not only associate the, you know, the logs and traces and metrics, but to find the impact it actually queries our sales history to find the normal order volume and the average order amount and the ordered, uh, dollar value so that it can try to estimate based on the number of orders that we were seeing and how many errors we had and the average. Uh, dollar value, what's the potential lost revenue there directly, uh, and that's really helpful if you wanna quickly send leadership update to say, OK, this is how many orders we might have lost because of this, and then potential future, uh, impact of customers churning because they were not able to full complete their checkouts. They would have bought but they didn't, um, and so it kind of goes through all that math right here, right? You could see. Uh, average order value is from the e-commerce data, and there's normal traffic. Uh, incident response as well. I'm trying to see if I can speed this up for you because I think you get the idea, but it's a lot of good data in there uh to see the checkout service um issues etc. OK, so now what we're gonna do is we're going to ask the agent to based on the analysis, what's the recommended remediation step, right? How can you, uh, how can I fix this issue now and then also how can I prevent this issue from occurring in the future again? So that's a different question to kinda ask, uh, after we establish the business impact which you which you can kinda see there so now what I'll do is I'll ask the agent. Tell me, uh, what I can do to fix it, but not just tell me now. Why don't you create a document for me? Create, create a document for me, you know, save it locally, uh, that gives me all the steps I need to do, uh, and then I can share that document with my engineers. I can share with my leadership, and it should give me all the steps that I should take right away and the steps that I need to take in the next week or so, the steps I need to take in the next month, and then, you know, in the next few months, so. Uh, it created, it created a comprehensive remediation plan and it even wrote it out to my local path. You can see it's called Payment incident Runbook and it goes through all, all the things I need to do to, to fix this issue. Um, And then, uh, yeah, I use used the right tool to do this and then also created a different um. A different document which is a run book uh for future issues that are on call engineers might face. So now it's doing that OK. And just similar to the first one I think you get the idea that it creates a a run book it saves it locally. Mm, and I just wanna show you what that uh looks like. So here's the payment processing remediation plan that I've just opened up. I don't know why this monitor is jittering like that, but, uh, it provides even the database alter statements that I need to do to change the connectivity issue that we saw, um, and those are the short term fixes that need to happen right away. And then it also provides some long term fixes, uh, that we, we should see. So it's giving us the actual database commands for the engineers to do. OK, so I think we get the idea, um. That's essentially a quick demo of what can be done of course to productionalize it you can use something like, you know, uh, agent core to to build an agent in production that has similar capabilities. So what we saw today was we saw automatic correlation. The observability agent was able to correlate and connect data from different indexes for logs, metric traces, and sales data without any human input. It was able to explain what it found in in natural language that was easy to understand for humans, uh, as well as data points that it used uh to justify the conclusions and it also suggested preventative measures for future uh events like Black Fridays as well. And in near real time, you know, you could see it's updating it's, uh, database with the memory of this incident so that if something similar happens in the future it can also use the memory to help uh make that uh process faster. Since it can remember this happened in the in the past and this is what we did to fix it so it wouldn't even have to potentially go and do all this research, uh, this next time around, so before the AI agent this would have taken hours and hours and would have cost thousands of dollars. In the meantime where we're trying to figure it out, but with the AI agent now you could see within a few minutes, you know, uh, 8 minute end to end, um. When I ran that was to come up with all the answers to all our questions and for any company this could be, you know, uh, 70-80% in a reduction, uh, in the incidents impact, uh, with the, the short amount of time it takes, right? So thank you all, uh, so much. If you wanna go deeper into this type of, uh, demonstration and get more hands on, get more deeper, we have a chalk talk coming up, uh, later on this week that's A&T 330. You can see that there. Make sure to check that out, uh, and you can ask questions and do the architecture for how to build something like this that's even more in depth and has more capabilities if you wanna learn more about how to use MCP yourself, we have a blog about it and then we also have documentation on all the observability features that we talked about. So make sure you check, check those out. And then also if you wanna learn more and you wanna level up your skills on OpenSearch and other AWS services, check out SkillBuilder. AWS. There's thousands of free resources there and you can, uh, start, you know, scan to start learning. OK, thank you so much for attending our session. Remember to fill out the survey, and if you have any questions we can, uh, be here for a few minutes afterwards. Thank you.