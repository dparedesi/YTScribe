---
video_id: n3M4cVClJls
video_url: https://www.youtube.com/watch?v=n3M4cVClJls
is_generated: False
is_translatable: True
summary: "In this thought-provoking panel session titled \"Security & Compliance in the Agentic AI era\" (AIM350), security leaders from Kyndryl, Air Canada, and AWS convene to tackle the complex challenges of securing autonomous AI agents in enterprise environments. Tony Dubos, Global Consult Leader at Kyndryl, sets the stage by describing the plight of modern CISOs who are battling \"shadow AI\" and \"low-code\" deployments that bypass traditional security reviews. He argues that Agentic AI introduces novel risks, such as \"rogue agents\" that deviate from their programming or the potential for malicious memory manipulation within orchestrators. To combat this, Dubos outlines Kyndryl's \"Digital Trust\" framework, which rests on four pillars: the automated discovery of all agents, rigorous certification through red-teaming, real-time visibility into agent behaviors, and the enforcement of strict policy adherence. John Solja, CISO of Air Canada, brings a critical operational perspective, asserting that safety is paramount. He draws a compelling parallel between AI agents and aircraft autopilots, firmly stating that fully autonomous systems are currently unacceptable for core operations; a \"human-in-the-loop\" is a non-negotiable requirement to ensure customer safety. Solja also highlights the daunting complexity of the \"AI Supply Chain,\" noting that an airline might depend on over 600 vendors touching their data. He argues that the industry must evolve from static vendor questionnaires to \"continuous monitoring\" of third-party risks. Furthermore, Solja champions the idea that \"cyber is everyone's job,\" urging organizations to \"level up\" the technical literacy of the entire workforce—from the C-suite to developers—so they can stop treating AI as a \"black box\" and make informed, risk-based decisions. From the platform perspective, Alexei Ivanov from AWS provides a technical roadmap for \"Responsible AI.\" He introduces the \"Agentic AI Security Scoping Matrix,\" a tool for classifying agents based on their *Agency* (what they can do) and *Autonomy* (how independently they act) to guide progressive deployment. Ivanov emphasizes that governance must precede technology, advocating for the \"Transparency, Accountability, and Human Oversight\" triad. He details how AWS services like Amazon Bedrock Guardrails allow companies to implement \"defense in depth,\" intercepting harmful inputs and outputs, while the AWS Security Reference Architecture provides the blueprint for secure infrastructure. The panel also touches on the impact of emerging regulations like the EU AI Act, agreeing that compliance requires a blend of comprehensive documentation and embedded technical controls. The session concludes with a unified call to action: enterprises must build \"trustworthy\" systems by integrating security and governance into the product development lifecycle from day one, rather than treating them as an afterthought."
keywords: Agentic AI Security, Responsible AI, Governance Frameworks, Human-in-the-Loop, Digital Trust, Supply Chain Risk, AI Supply Chain, AWS Bedrock Guardrails, Security Scoping Matrix, Continuous Monitoring

Good good afternoon everyone, and welcome to this session. And we're going to talk about uh responsible AI and we're going to talk about the security of gentech AI, um, and I will do the first few minutes of introduction and then we have a really interesting panel. It's a lot of questions and at the end you can also ask questions, uh, to this panel. Uh Let me start with introducing myself first. Oh. So my name is uh Tony Dubos and I'm the global consult leader for security resilience at Kindle and of course you might ask uh Kindle, some of you might know it, some of you might know it not so let me explain Kindle. Kindle is a spinoff of IBM and we feel ourselves as a startup, but the startup is 50 billion revenue per year, so. A relatively big startup and we are managing the most critical infrastructure of the biggest clients in the world so you can imagine banking, insurance, healthcare, uh, critical infrastructure, and, and so on. And of course to do that we have also a, a strong security team, security resilience team with more than 7500 people. Um, and to give you an impression on, on, on the scale that we are operating, we are doing more than 70 million identities per year. We manage that, uh, on a yearly basis. And of course all our clients are talking about AI and agentic AI and about how to secure that and the res responsible AI. So that's what we're going to talk about it, uh, today a bit more. And of course we are doing a lot of work with AWS, so we're really happy to be here. Um So my first slide, I think it's a bit of an open door, uh, when I talk to CISOs and I talk to a lot of them, they all are concerned about AI and the security of AI. No, they see this push of business to do more and more of this AI, and now we have agentic AI. We have this low code, no code. We have all this secure, all these software packages that are introducing. Uh, Agentic capabilities, there's fight coding, um, there's shadow AI. So when I talk to security officers, they all, the first thing they say, what, what should we do with AI and how can we protect our organization better about all this push into uh into AI. And I think that makes a lot of sense, you know, because of course there are some risks which are. Existing, we know them, you know, again, if there's the infrastructure you need to secure your infrastructure that is not changing, but AI already introduced new risks. No, we had this LLM certainly we're not completely sure what is happening in an LLM with all those nodes with the millions of billion nodes in an LLM, um, and then we get, get agentic AI and agents and more and more agents. And again this introduces a lot of new risks uh for for for organizations. Um, clearly there is an extended surface with all these agents talk to each other, the APIs, the NCP servers, uh, how does an agent identify himself? How do we know it's a, it's a good agent and not a rogue agent. Are agents not stealing data? Are they not taking data and transfer it to an agent? We should not have the data? Are agents making the wrong decisions? Because of course I hear saying, yeah, but agentic AI and there's still human oversight, and which probably makes sense. But we, but we also know the moment it is working well, we will see that agents and agentic AI solutions will take more and more decisions. Who, who was responsible for that? In that way, Agentic air security is not only a technical problem, but also a business problem. What will happen if an agent is rogue? Does the complete application stop, or when we introduce a hacking agent in the complete stream of all agents. So a lot of new risks that we need to consider. For example, which one I really like is also if you build a more sophisticated sophisticated gentic AI application, you have an orchestrator. And his orchestrator has a memory where he stores the data and uses to make next steps and it determine what needs to happen. But if you can if you're able to manipulate his memory, probably new things or unexpected things or things will happen in your interest. So the reality is a genetic AI is bringing a lot of risks for enterprises. We're going to talk about that more in our panel, but we believe that at least you should do 3 things, 3 simple things in practice, a bit more difficult, but at least 3 things. First of all, work on your foundations because you need to have a policy. Updated, you need to look at your control frameworks. You need to look at your risk management. You probably need to test your agents in a different way. Red teaming is quite effective, for example. How do you implement human oversight? How do you manage and control your agents? You need to change your foundations. Hopefully the foundations are already there to manage security and resilience in a good way, but you need to take it in the level that it supports AI and agentic AI. Secondly, build it smart, and what I mean with that. You need it built in such a way that you build in advance the security components and reuse it as many times as possible, so it's not always the security challenge coming up, but we can leverage as much as possible from the previous implementation. So build it smart. And last but not least, you also need to run it in a secure and resilient way. And if you build it in a secure way, it doesn't mean that it's automatically ruined in that way. Building it means that you did think about the risk, that you mitigated this risk, or that you build scenarios to monitor those risks with cart wheels and see if things go out of out of control, that you have reports around that, but then you need to also put that in the agents in an environment that is working. And First, we introduced for that uh AI agenttic AI digital trust uh with AWS and it consists of four components and we think those components are really key. First of all, You need to discover agents. And we'll talk probably more about it, but some of my clients already have more than 1000 agents, you know, you need to discover them. You need to register them. Probably you need to give them an identity. You know, they are digital workers that will start working for you. Secondly, make sure that every agent that is going to run in your environment. That it is well tested and certified, you know, it's all the different testing methods that you have, for example, red teaming, all the agents or software scanning you need to be implemented so that you are run agents in a secure and a safe way. You need to make sure that you understand what agents are doing because if you have no visibility. You cannot manage it. If there's no feasibility, you don't know whether the wrong actions are taken. If you have no visibility, you don't know that the whole system came to a stop because one agent is not working. And last but not least, we need to make sure that agents are behaving as much as possible. It's not an easy topic. I love to talk about it, but by building the cartwheels, the cartwheels to implement the policies. Applicable for the organizations that might be regulations, but the reality, a lot of organizations have their own policies and the digital workers, the agents that are introduced needs to follow those policy and if they are, uh, uh, doing behavior what is not accepted, we need to understand that and we need to should be taking action on that. And last and not least to really test agents and, and. Probably understanding that we are going to more and more autonomous behavior agents, we are also working with digital twins, a digital twin environment that we can use, how agents are working, and if they proactively come up with actions to, to, for example, reduce the risk or to remove vulnerabilities in the infrastructure that we can test it in a digital twin before we run it into production. Well, it's clear the world is changing. There's a lot to talk about, uh, and instead that I'm going to talk about it for the rest of the hour, we said it's probably better to have a panel and talk about that in a more interactive way, and we have a really interesting lineup and I will invite them to the stage. First of all, Alex. She will be our moderator. Uh, John from Air Canada is in CIO, so he has his mud in his feet in the mud, as we say in the Dutch, and we have a lot of mud in the Netherlands, and last but not least, Alexei from from AWS, which is a lot of technical experience I already noticed in the start of the before we went live into this event today, so. And I will be in the panel, so I give the control back to you, Alex. Perfect, thank you. So hi everybody my name is Alex Della Serra. Um, you might have been expecting Gilson to be here unfortunately he couldn't make it today, so I took over as of Friday of last week. Um, so I'm a senior partner development specialist focused on security at AWS and I'm super excited to be hosting this panel today with Kendra and Air Canada and AWS, and this is a really important conversation for us to be having with. All of these panel speakers because really when you think about securing autonomous systems we think about our secure um you you think about our actual uh shared responsibility model um and how really it's important to not only have AWS securing it but also thinking about how you can leverage partners like Kindle to help you with that as well. And so first we already heard from Tony, so he already introduced himself. So let me turn it over to John to introduce yourself. Thanks, Alex. Uh, so John Solja, um, been CISO for Air Canada for, uh, for just under a year, and, uh, uh, also, glad to be here with my Kinderle friends and, uh, former colleagues. I, uh, I, I spent, uh, a good number of years at IBM and then Kinderle. Uh, so glad to be here on stage with all of you. Hi everyone, my name is Alexei Ivano. I'm a worldwide technical leader of partner security and identity, senior partner solution architect with AWS. Perfect. So I'm gonna start this discussion with some questions, and then at the end I will be opening it up for Q&A. So definitely be thinking about your questions that you have for our wonderful panelists. But first, let me start with John. As a CISO, I'm really curious, how is Air Canada approaching AI agents and what must be true before you trust Agentic AI operationally? Let me reverse the question. Would you get on a plane that didn't have a human pilot? I don't know, probably not today. Yeah and that's that that really you know thinking about it that way highlights for us, you know, it's an extreme example but it's, it's how we think about agentic AI within the organization um. As an airline safety permeates everything in our business and uh that extends to how we deploy new technology in the business, particularly in the core operation of our of our airline, uh, and so that means we're we're particularly reluctant to give full control to an an AI agent so I don't really envision a world. You know, never is a strong word in, in a world that is evolving as quickly as it is here, but, but I, I don't envision a world where, particularly in the operation of the airline, the, the, the things that touch our customers and, and, and their, and their safety that we would actually grant full autonomy to uh agentic AI. Um, there, there, there are massive opportunities, but I think we will continue to leverage a human in the loop model, uh, to, to make sure that we are getting the, the, the human oversight to what's happening, um, but, but you'd ask, well, humans are fallible, so why do I trust them more than I trust the gente. And and it's actually because humans are fallible we have fallible humans and fallible human systems creating agents, um, and, and that that's more of a concern for me than the actual technology. The technology. We, we know what its limitations are and we can work with it, but it's, it's, it's how we build it as humans that is, is really the challenge, um, you know, and, and so it behooves us as organizations to learn more and more about the technology deterministic systems you can treat as a black box. Um, generative and agentic systems, you really actually have to understand what's happening inside the box. You can't just treat it as a black box and know if this input goes in, this output's gonna come out, um, so, so I think, I think it's got amazing promise, but, but I think blindly trusting any technology, uh, particularly a technology as powerful as this, is dangerous. Yeah, definitely, and I think that's important to call out is because when you're thinking about agentic AI you're introducing new risks. So I'm curious, Alexi, what responsible AI principles are most critical for agentic AI? What are you seeing? So when agentic systems, agentic AI systems can automatically trigger work flows, right, and make autonomous decisions, three principles become non-negotiable transparency, accountability. And human oversight. Let me break it down why they are important and how we implement them in production. So first, transparency. When systems, agentic AI systems take autonomous actions, right? It's really important for stakeholders to understand how and why those actions were taken. So think about it, uh, as a. Flight recorder for AI, right? So basically, as every aircraft reports and logs all the critical actions, Agentic AI systems, they need comprehensive audit trails. This is not just nice to have. It's really important for building trust and regulatory compliance. AWS has created AI service cards that document our services capabilities, limitations, and intended use cases. So that level of transparency is required to build trust with customers. Second, accountability, right? Traditional AI has humans in the loop. A genetic AI can make autonomous actions and decisions. So that raises a foundational question, who is responsible when something goes wrong, right? So basically that means 3 things for AI. We need clear escalation paths when AI, a genetic AI, hands off to humans. We also need monitoring capabilities to spot up any anomalous behavior, and critically, we also need the ability to undo any unintended harm. Insert. Human oversight. So agentic AI systems are supposed to assist humans, not to replace human judgment. Think again about it like autopilot in aviation, right? So it handles routine tasks, but pilots remain in full control and can take over at any moment of time. So for agentic AI that means that we need to build human in the loop mechanisms for critical actions, define clear boundaries, what agentic AI can do autonomously. And We also need to maintain intervention capabilities so humans can take over the agentic actions at any time. Let me provide you with a real world example. A Security incident Response Service now supports Agent AI-powered investigation capabilities. So the investigative agent will collect data from multiple sources like Qualtrail, IM, EC2, and Coast Explorer. It will correlate the data and it will provide clear findings with clear summaries. Importantly, This agent will ask questions to clarify any missing details, and all their investigation activities are tracked through the council. That's a great example of transparency, accountability, and human oversight in action, right? So the challenge isn't about building the capable AI agents. It's about building trustworthy AI agents that can be deployed at scale. Well, by customers. And that starts with 3 foundational principles, transparency, accountability, and human oversight. Very interesting. And now Tony, I know you mentioned earlier you're talking to customers about this a lot already. So what are some of the things that you're hearing from customers as the most prominent issue for Agentic AI? Well, first of all, maybe we can talk a little bit later about how the, how the, we're scale, how we are moving in our, uh, what is normal because I heard John say we don't. Fly without a pilot, but we drive cars without a driver, you know, as we know in the US we have multiple examples where you get in and the car is driven and nobody's there. And maybe you have some oversight, but I don't know what is actually happening. So maybe we talk a bit more about that later. So first of all, I, I really envy, uh, uh, you know, I, I really feel sorry for CIO. I mean sorry because there was already a lot on their plate, you know, uh, the complex. of the infrastructure, the legacy where there's not always the money to invest in, uh, the geopolitical environment which is creating even more threats, uh, and now there is this constant push of AI energetic AI, and I understand from the business side. You know, uh, that that this is important, but it's another task for the seesaw which sometimes I feel, and I've been a few times interim seesaw myself, you're like, uh, the fire brigade running from one fire to the other, and that keeps you, uh, you know, awake during the day and, and in the night, uh, unfortunately, and that's also why I'm not sure a lot of seesaws survive in the role so long. So, uh, so again this is an extra task for them. And they're really concerned because they are seeing that the business is pushing this. They are building the agents. There is. Uh, I did not see 5 coding, but, uh, in, in an enterprise environment, but we are seeing their low code, no code, uh, development taking place where security and resilience is not part of that development and, and interesting enough, even, you know, in, in, in the old days what was maybe a year ago, you say hey this is what we need to develop something and this is what we need to do to secure things. Now we're seeing it becomes really easy. To develop things, but to secure it, you still need a, a lot of efforts. That's why I said let's try to build it in a smart way so you can bring it down a little bit. But at the moment we're definitely struggling with some, some of the clients that securing it in a proper way takes a lot of time and effort, and that brings me to my last point. Uh, and it's also ask, uh, uh, and a lot of discussion. I also have a lot with more the CTO, uh, and, and the CIOs of companies. If, if you want to empower your company with a genetic AI which I can understand. You also need decisive actions on creating the trust and the human acceptance of AI, because otherwise you will take a long time to get the value out of AI and ergetic AI and talking about trust, you know that also empower, that means also empowering you see so that is able to really deal with this new risks that the company is facing. Yeah, definitely, and with a GenTech AI governance and compliance is definitely an important consideration when organizations start thinking about the adoption and what they're going to do. Alexi, I'm curious what governance measures help enterprises stay compliant with emerging AI regulations. Well, let me start with. Governance is not a blocker to innovation. It's a driver. And let me elaborate on this. The biggest mistake we see that customers and companies do is starting with technology and then retrofitting governance. It should be the opposite. Before you deploy a GTI to production. You really need to create, define your governance frameworks. Establish all your policies and define what is safe and effective means for your organization. And to understand this risk systematically, AWS has created an agentic AI security scoping matrix. So basically that brings two dimensions. First, agency or the scope of actions the agentic AI system is allowed to take. From read-only actions to really intrusive changing actions. And the second scope is autonomy. That's a degree of independent decision making that really highlights whether all actions need to be approved and initiated by humans, or it can be initiated by the agent itself autonomously. These two dimensions allow you to plan the roadmap, progressive deployment roadmap, starting with limited agency and autonomy and enhancing over time once your security capabilities really mature. provides multiple tools that can help support with this. For example, Amazon bedrock guard rails that allows you to set. Restricted topics, right, and the limitations regarding what your agent AI can discuss with customers, what it can take action on. We also provide security reference architecture, a prescriptive guidance that not only provides you with concrete architectural blocks, but also infrastructure is code repository that you can use right away to deploy. All right. The shared responsibility model on the other hand, defines what AWS managers, as Alex mentioned, and what customers must implement. For example, specifically for a GT AI, customers need to own the governance layer, defining the clear boundaries for a GTKI. Access control, right? and so. And also your governance policies and data policies. So basically another important and critical thing to consider is data governance because HGT AI systems usually require access to sensitive data from multiple systems, right? So you need to implement. Principle of least privilege from day one. What data agents can access and by what time. So basically think of zero trust and just in time access. It's also important to have a comprehensive data classification policy to understand what kind of data you have and what kind of data you are comfortable sharing with the GTA systems. And one more important thing related to data governance is Data lineage tracking. So at any time you should be capable, able to track the decisions made by GTI systems to data sources. And AWS services, as you all know, support multiple regulatory requirements like HIPAA, GDPR, SAC 2, and emerging AI specific regulations, right? However, compliance, it's not just about tools, it's also about documentation, right? So. You need to document your AI gentic AI system's capabilities, intended use cases, and best practices. You need to enable the stakeholders to make informed decisions about engaging with AI. And important business that The companies with robust, responsible AI practices report an 18% average increase in AI-related revenue. This comes from a joint research by Accenture and AWS that interviewed more than 1000 C-level executives. So governance helps to drive business value, not just to mitigate risks. Governance first, technology second. Start with foundation, building this foundation with governance and expand with technology. That's the way to go definitely. So you mentioned something interesting. You mentioned the new global regulations and so I know we have things like the EUAI Act that recently emerged and I'm sure many more coming after that. I'm curious from you, Tony, how do you see these new regulations shaping the conversations that you're having and how are customers beginning to think about this? Uh, it's interesting questions, and clearly I'm from Europe, so, uh, EUA act is definitely on the agenda for CISOs in Europe, but it's not only in, in, in Europe. I was talking to a client from Singapore, and they also have a standard that's not a regulation yet, but we all understood that if something happens they will quickly turn it into regulations, uh. For me in itself, the EO EU AI Act, which is a lot of discussion about it and also some big tech vendors were not really, uh, supporting it, uh, in principle it's really good because it's saying, hey, you have AI which we find unacceptable. You have high risk AI, you have, uh, low risk, uh, AI, and, and there is no risk AI. And and the reality is that we can use AI to create things that we should not really want to have in our society, you know, you can do social scoring and I think some countries it's actually used to do social scoring and we just need to say as a society, hey, we don't want that. And secondly, if you're using. AI for high risk applications, biometrics, uh, credit scoring, which are all sensitive, that you ask for a proper risk assessment and to document that in itself there's nothing wrong. I think this is common practice. Unfortunately, and this is what we've seen with GDPR GDPR too is that in principle is really good but also creates a lot of legal work, you know, if you make a contract, you need to understand what is my role, what is the response. You definitely need to do that, but by the EU AI Act, you need to interpret that and leads to a lot of work for lawyers, you know, and I know not sure. And see cells because they need to talk to the lawyers saying hey tell me what what this means and I'm compliant or not and what do I need to do so it creates also a lot of work so there is a bit of the tendency in Europe to see to make it a bit smaller but the reality is also we as a society we need to to to understand the risk that it can have at large scale and also make sure that we don't use it in the wrong way, you know. Yeah, yeah, exactly. So John, what's your take on this? Because now you're seeing these new regulations emerge. How do you feel about these and, and what's Air Canada doing? I think from a uh a governance framework it it really needs us to come at it very differently, um, you, you, you, you can't have governance over here and product development over here uh it's with the speed that things are moving, you, you really need to embed cyber and, and governance into the product development pipeline, um, you know, a lot of, a lot of what you said Alexi, and, and really, um, it, it. The the difference I see in in organizations going forward is that people need to know each other's business much better. You can't, you can't just be a pure developer or a or a prompt engineer and and not have an understanding of cyber, but on the same, by the same token, I, I spend most of my time encouraging my team to learn more about. The other parts of of our IT and digital organization because then you can actually articulate what the needs are and and you can get at this and go faster um and but but but the governance you know organizations think about governance today in a very monolithic and structured way. Um, you know, you have to have a steering committee. You have to have, uh, operational committees, and, and, you know, I'll, I'll give you an example. As, as we were standing up, our initial take on, on AI governance, and this is even pre-aggentic, uh, we're, we're going through discussions about how we're going to implement, um, we're how we're gonna implement governance, and while we were in those discussions. Our employees started to use over 250 different AI tools and websites, um, and, and of which I think 3 were sanctioned, and I'm sure that was fun for you to work through. Uh, it was a lot of fun. I, I just turned 200 of them off one day, um, but, uh, and, and, and the screaming wasn't too deafening, but. You know, it, it just, it shows how, how much we have to work in a very, very different way in this era. You, you, you can't take years to figure things out, um, and it's just, it's so much more challenging and, um, in, in organizations like ours which are fairly conservative in how we approach technology and how we, how we approach compliance to regulation that, that, that's a, that's a massive challenge and it, and it changes how everybody has to work in that world. Oh yeah, I, I completely understand, and I'm, I'm actually gonna bring in another element to this, and that's the AI supply chain and the risks that we're now seeing from third party applications. So Tony, what would you say some of the key steps are to help manage these third party risks in the AI supply chain? Well, I think first of all it's good to understand that. Third party risk was already number 1 or 2 of of all the hacks that are happening. Third party risk is one of the causes for that. So it was already a problem, and I think it will be only a bigger problem because we're seeing that all all the suppliers are, including AI and the Ge AI, are not always telling it that they are doing it, so it is, is really complex. Clearly we have the more traditional process where you say hey I look at the contract, I look at the risk, then I do a risk assessment. Based on the risk assessment I go deeper and then I accept offender or not. We really need to try to turn that also from a more legal driven into more a technical data driven discussion. So and that also requires vendors to be more open, so we better understand what is happening, so we better can do technical testing, so we better understand what data is being used because for me data, data risk is one of the most important elements to determine the risk of a third party. Um, and, and really more technical testing to understand the risks that are related to, uh, to your fenders or your second fender or third fenders, and that I'm sure you will go to John and we already had a bit of discussion about that. It's also a bit interesting that in in. In IT we still are not that regulated, you know I was involved in building a plane or doing some software and hardware components and building a plane takes a lot of. There's a lot of requirements and regulations to build it in a secure way, which makes a lot of sense because I sit a lot of in the planes and I don't want to die. So, but the reality is that IT becomes so important for us in our daily lives that why are we still so loosely, uh, vendors can do whatever they want and then we take the risk what is coming out of that. So I think it's also. I'm not really a recod guy or fan, but in this area we really need to rethink about how we are working with third parties. Yeah, no, I completely understand. And yeah, to your point, I think John, I'm, I, I wanna know a little bit about how Air Canada is actually evaluating these risks and managing third party AI applications. We actually struggle with that, um, because we do take that more contractual approach to things and, uh, you know I I think we have to evolve our practices to to be more continuous monitoring, uh, because you, you, you. I look at instances today where we're about to buy something and we go through this rigorous AI questionnaire with the vendor, but then this other vendor that we bought the package from last year that didn't have AI but now it does. But nobody ever evaluated it and it won't get evaluated until the next contract renewal and so I think it needs to become more of a continuous monitoring of what's going on, uh, but, but that's an incredibly challenging thing to do in in our industry if I think even just outside of the software um supply chain, the airline industry is so interconnected. I was doing a risk assessment and I went to our procurement folks and said. Can I have a list of all the vendors who can touch my data? The list came back it was almost 600 vendors, um, right down to the catering company, uh, and so. That that that's outside of the software development practices in the software development supply chain. So, so then you know you're assessing risk of those third parties who are providing you services, not just the third parties that are providing you tools for you to build your own services, and then it gets even more complicated. Like I'm sure everybody's heard about the Collins Aerospace incident in Europe a couple of months ago. We have a direct relationship with Collins, but in this case they weren't even our third party. And they shut down our operations in 3 airports, uh, because they're the third party of the airport provider with whom we have an agreement and so that that that that landscape is just. So much to tackle, uh, that it that it becomes very, very challenging and, and, and you really have to, to, to really start to take a very strong risk based posture to it because to evaluate that number of third parties consistently. Airlines don't have the budget to do that, uh, and so you really have to go, OK, where, where are my biggest risks, truly understand them, and then apply the continuous monitoring in those areas and then the other areas you have to just step back and say, OK, I, I understand I'm taking a risk here, but, but I, I can't, I can't apply that same rigor. Yeah, with all that it's just like how do you trust anybody then it makes it how do you, how do you choose a partner to work with that you can truly trust and I think that's a challenge probably any of the partners in the room may have today and if you're a customer you're probably thinking yeah that that is definitely something that we have to be considerate of. So Alexi, you work with partners you work with customers all the time what do you see that builds trust with customers? Trust is built with Consistent transparency, robust security, and genuine partnership. Let me try to break this down, and all of us on this stage actually represent the trust in action, how it works. AWS provides tools and frameworks, services like Amazon bedrock guardrails or Agent Core, right? Security tools and services. Security reference architecture that I mentioned before. Kindrell brings to the table the implementation expertise, operational excellence. And deep knowledge in Jersey. Air Canada brings to the table real world production operational requirements and feedback from production deployments. Everyone knows their role because of the shared responsibility models that brings accountability. A secures infrastructure. Partners like Kindrill help with implementing controls, and customers like Air Canada, they own their data and governance policies. This accountability builds trust. And it's not just about technology actually. It's all about shared shared learning as well, right? So when Air Canada shares their experience with a genetic AI with other enterprises, it builds trust and highlights that it's, we, we're not just talking theory here. We are actually delivering to production together. Then security, as I mentioned, security is stable stakes. At AWS everything starts with security. So comprehensive security controls like identity and access management, data protection, monitoring and auditing, they should, they all should be built in from, from the start, not added later. Regular security assessments, penetration testings, proactive threat detection, and incident response. Customers need to see that you are taking security seriously, not just checking the compliance boxes. So for genetic AI that means Protecting agents identity, monitoring anonymous behavior of agents, right, and having kill switches or circuit breakers where needed. Third, transparency. Customers really would appreciate clear documentation on your agentic system's capabilities and limitations, not just what it can do, also what it cannot do and what it's not supposed to be used for. Customers really need visibility of how data is used and protected. No surprises here. So basically, if customer data is used, it should be audited, locked, audited, and Clearly communicated to customers. That's especially important in privacy regulations like, for example, right? And finally demonstrated commitment. All these. Compliance certifications, AAA service cards, audit reports, they are not marketing materials. They are proof points. Security competency, the biosecurity competency that I run as a technical owner, recognizes partners that have technical proven, validated technical capabilities and customer success to Help our customers in certain security domains. Specifically, we have a category for AI security. That addresses unique challenges for AI workloads with the recent latest addition of agentic AI. That addresses identity management for agents, threat detection for autonomous agents, systems, and monitoring and auditing. So trust is built overnight. It's earned through continuous. Transparency, security, and partnership, and that's what we really demonstrate here on this stage together today. Yeah, that's really helpful, and I think that's something that we can all think about and I just have a few more questions I wanna get through before we open it up for the audience here in a few minutes. So John, I think it's really important to hear from you what advice would you give to CISOs or other people in your position who are trying to drive agenic AI in their organization. I think the biggest thing is. You need to level up everybody's understanding of of how this technology works and and what the capabilities are and what the limitations are um it it it's not a it's not a GRC function it's not a cyber security function it's not product management it's not software development, it's not procurement, it's everybody's job and and everybody has to have a much deeper understanding of, of the domain, you know, executives love to talk about informed acceptance of risk. But we, we underplay the informed part of that and, and if you're making decisions in an architecture review board and, and the architects don't actually understand how agentic works or if you are on, on an incident call because Agente did something strange and, and you listen to people hypothesizing about what happened. And, and in that hypothesizing you realize they really have no idea how the technology works, um, it's, it's, you know, that, that leveling up of skill set I think is, is probably the single biggest recommendation I would give to everybody and it's, it's, it's from the C-suite all the way down to, to, to frontline software developers and, and, and people within the cybersecurity organization. Um, and you know, I, I, I'm sitting up here and I'm listening to myself and I sound like a wet blanket when I'm answering all the questions, uh, and, and, and that's really, really not who I am. Those who know me know me. I've been, I've been on the forefront of technology for, for most of my almost 40 year career, and, uh, but, but with something with this much promise. Also comes a lot of risk that that we have to we have to really think about and, and I think we have to create those safe opportunities for for experimentation for learning for people to develop that knowledge hands on, um, you know, I'm, I, I'm, I'm from Canada, we love our hockey and, and you often see kids playing road hockey, uh, and you know, as a when you're raising kids, you, you think about, OK. Where, where's the limit of, of what's safe and what's not safe? Are, are they playing road hockey at the end of a dead end street, or are they playing on the shoulder of a freeway? And, and I think that, you know, that there's a lot of opportunities for us to be playing on the shoulder of a freeway if we don't really think about what, what, what we're trying to do here and what the risks are of what we're building. Yeah, I love the idea of leveling up the organization because this is such new technology we see it all the time probably within your own companies with other companies that you're talking to the need to really educate others on what this technology is capable of. Alexi, there's a lot of changes that we can probably recommend to enterprises to implement today, but I'm curious, what would you say is one thing that enterprises can start to think about to prepare for responsible and secure innovation in agentic AI? One thing, huh, that's quite a challenge. Let me give you, yes, your top thing. I, I would really start with uh some practical things that you can implement tomorrow in, in, in pitfalls so way because as I said before, guard rails are really important, and you need to define those before deployment, not when you're already in production, right? And don't please start with broad permissions just to make things work. Follow the principle of least privilege. Implement monitoring, right, and alerting. You cannot really manage something that you cannot see. Start with progressive deployments using the AI, uh, AI genic AI scoping matrix, right? Start with like a low agency and autonomy and progress as you have more mature security controls. And most important, start small. Don't really put your business on your first deployment. Start with non-critical deploy uh workloads, right, and then expand once you learn. And scale to more like critical things. However, however, overall I would say expect more scrutiny through regulatory compliance and laws, right, regulations. Expect more advanced attacks on AI systems, and I really think that those companies that will be successful will start with security and governance built in, not the ones who build the fastest with no any guard rails. Yeah, I think that's great. I think we're seeing companies who are thinking about security and governance moving faster. That's a great point. So Tony, from your perspective, what do you see as innovations in AI security that we can expect in the next few years? Alexi already told us that first of all the hackers will use AI and recently we saw cloud which was itself not super innovative because it was just combining a lot of existing tools but it helped the hacker to automate actions, you know, and if they can hack some computer power they could use it and create more, more damage in that. But we definitely will see that hackers will use it in a smart, smart way, as also are we with our clients try to use AI to understand what is happening to test in the environment, whether this is a vulnerability to see in the locking. And the data whether it actually happened and then also automatically, you know, come with a solution to solve, to solve this, you know. So self-healing systems is, is, I think closer. Uh, than we think, you know, I, I, I, AI, I see a bit like we have, uh, it wasn't, it was an intern, it becomes an assistant, but then it will be a colleague and maybe one day our boss, uh, but, but this is, is, I think we're now getting into, uh, uh, an, an, an, an assistant, but in the future level we probably also that it will be our, our colleague, you know, so, uh. And the reality is, although we started in the discussion saying planes are not flying without pilots. But I think we're accepting more and more that AI that AI and technology will do for us. We have the taxis, we have the drones which fly autonomously, so our life will change, and we need to take really actions on training the people because I see this is one of the big issues, but also please look at security resilience of everything we do. Yeah, yeah, exactly, and to be fair, I have taken autonomous taxis, so I guess an airplane isn't too far off, right? So we're, we're all moving in that direction and there's a lot of autopilot there anyway, so, uh, yeah, the, the, the, the difference is 30,000 ft. That's fair. I don't know. I, I'm from New York City, so the idea of a car driving autonomously is still a little scary in New York City. So, all right, so we just have about 10 minutes left, so I wanna open it up to the audience for some questions for our panelists. So we've had a great session with a lot of great insight for our panelists. So I'm gonna come around the room, put your hand up if you have a question that you wanna ask, and I will run over to you as quickly as I can. Um, if you have a question for a specific panelists, please include that in your question so that they can address it as well. So who has a question? And don't let the question be how do we get discounts for Air Canada flights. Uh, my question is probably specifically for John. Um, you, you talked about like we can't take years to figure this out. And then we've also talked about like the idea of not playing on the shoulder of the freeway. How do we quickly get our organizations up to speed? On understanding what agentic AI can do and what it's used for, how it works. So that we can implement accurate guardrails and governance around it. It's it, it, it gets back to just leveling up that skill set across the organization, um, you know, I've, I've, I've got a book sitting on my, uh, on my bookshelf about the, the, the nuts and bolts of actually building LLMs and, and, and the underlying technologies because I, I, I feel I need to, be, be deeper in that and so, you know, encouraging all of your teams to, to dive deeper into that and understand it. Uh, I, I think is, is the only way that, there, there are no shortcuts here, um, and, and the, the only way to get there is, is to get everybody to truly understand it, and I think the, the other thing that, uh, you know, I, I've been in this role since, since April, and the thing that I've been advocating across my organization is if, if, if I build the best cyber organization in the world. I would still fail Um, if I embed cyber in the rest of everything that we do in Air Canada, then I succeed and, and so encouraging that cross pollination, uh, so, so that people can learn from each other as we're going on this journey, uh, not, not, not building barriers but building true collaboration within an organization, uh, that, that, that's really how you get to move faster. Sorry, sorry, no, I, I have no technical answers. I have only people answers. Yeah, but I think it's also a, a concern. You, you need massive scaling, skillings, uh, training programs, but the question is also, is everybody able to make that step? I mean, this is for me really a social concern, you know, because we've seen in every technology in development some people were not able to make a step, and AI will be even bigger, you know, in bigger organizations, uh. We see with clients we really they force almost force it, you know, you need to make use of AI every day. It, it's also in their own benefits because it will be the future unfortunately so. Cool. Um, thanks for sharing. Uh, I'm Reagan, the co-founder of PinAI. Uh, we were doing a personal AI infra. So I got a question for the panelists, like, um, you mentioned about, uh, the trust and the transparency for the um agency itself. Um, I'm just wondering from a very practical standpoint, like from product engineering. On any recommendations or some, uh some best um best practice you guys have. Thank you. Sure. So, it's all about, I think, documentation and transparency. If we start with transparency, you need to really communicate what your agents are doing, right? What are use cases, what are the limitations, what are the best practices, what are the capabilities. Customers need to understand it. They need to understand whether their data is used by agents in any way. You need to really communicate what are the boundaries, what is the level of agency and autonomy for agents, so customers can understand the associated risks. When they use your uh agents, for me it's a two level. So first of all, it's a technical level, we need to unlock the actions of an agents. Sometimes we think it's not important or it's happening by itself and and we'll just see what happens, but we need to understand just as any other IT components, we need to understand what is, what is happening to really unders to really manage it in a proper way. Secondly, there's more, you know, the transparency is Alex she's saying, you know, create the transparency report and try to explain to yourself why you're doing it and why it's good for people to use it, you know, really in that perspective, not because I often see I'm more of security and resilience, so I see the the risks and the concerns, but businesses having an idea and they're so focused on making that happen. It's all about this small end result. But they're not thinking about, you know, how is the end user really accept, uh, uh, perceiving it, how they will, uh, the questions that they will come up, are we able to answer that, you know, try to write it down for yourself on the page, you know, because I think that will be really helpful for everyone understanding, uh, in a way, and sometimes I prefer, uh, John was already talking about agents. Human agents versus in the call center AI agents. By the way, I prefer to use AI agents than human agents often because they are quite accurate, but I'm happy to use them as long as I understand what is the scope of what they do. Be transparent about it. And obviously proof points like certifications, right, compliance certifications and uh clarity on your security mechanisms that you use in the background. So basically it's documentation plus technologies that you can communicate to customers to prove what you're doing. And highlight the several aspects of potential risks and earn trust with customers this way through transparency, accountability, and. Potentially human oversight as well. Perfect. And we just have time for one more question, so here we go. Uh, thank you guys. Um, so if we recognize at the moment we're at the start of the agentic AI curve, if you will, um, do you think we'll get to a point where we reach agentic good enough and then there's a point of diminishing returns? So how will we know when that's been achieved and as a C-suite member I can then look to invest in, in other areas. I, I don't know that we get to a, a, a good enough, um, I, I think there that if we, if we look at other things that we've seen in, in technology, we, we find new, new ways to apply it and, and we, we continually, it just becomes another tool in our tool kit, uh, I, I think there comes a point where. You know, we, we, we, we're gonna go through this massive build now and then it's gonna be more iterative and, and be more part of the toolkit as opposed to the only thing in the toolkit that we're focused on, uh, so, so I think, I think it will. Reach a peak and then start to level off, but I don't, I don't think we ever stop investing in it. I think agents being good enough is a bit of a misleading statement actually. Because we can't really assume that agents will always make correct decisions. They won't. They will make mistakes. And that's why we always need to kind of have those circuit breakers and humans clear escalation paths. Similar to autopilots as we were talking because they are not supposed to replace human judgment, they're just assisting humans with certain actions. So by saying good enough, what, what do you mean? Like it's just good enough to kind of build a new world or live by itself, don't have any human supervision and oversight. I don't think so. I don't think we are on the way. In this direction. I think we get to an assistant level at this moment, but interesting enough what I see is that we're except we have different uh standards in the human world and in the technology world. So if you drive a car. Things go on, you know, we all have accidents, and it's not the intent, but it happens. If then suddenly an autonomous car creates an accident, it's a big issue. The question is, you know, maybe it's better than human, uh, so what do we prefer, but we accept that technology is never making any mistakes and it's always there and it's always working and this is probably also not realistic. So yeah, well, thank you everybody for the questions. That's all the time that we have to take questions, but. Come meet Kendall, talk to AWS they can answer more of these questions. This is what these individuals are here to is to help you on this journey and to kind of debate some of these topics like we did today. So with that, let's give a round of applause for our wonderful panelists. And I'm gonna let Tony close this off. Yeah, thank you all for joining. I hope you enjoyed it. Alex, thanks for stepping in last minute, flying in uh over the weekend to help us. Alexi and John, much appreciated for being here. Uh, we're here, so if you have questions just come to us and we're happy to discuss. Come to our booths or to AWS. Uh, we also did a really interesting white paper, uh, with AWS as our Kindle Institute which also talks about the acceptance of, uh, of AI. Uh, you can, uh, you can download it. It was really interesting read, uh. Uh, which is really about this topic again, thanks for joining and, uh, and it was a pleasure to have you all in our session today. Thank you. Thank you, thank you. Thanks everyone.