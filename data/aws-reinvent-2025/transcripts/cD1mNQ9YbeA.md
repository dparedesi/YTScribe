---
video_id: cD1mNQ9YbeA
video_url: https://www.youtube.com/watch?v=cD1mNQ9YbeA
is_generated: False
is_translatable: True
---

Hello and welcome to a deep dive into the nitro system. My name is Ali Sadi. I'm a distinguished engineer in AWS, and with me today is Filippo Cerroni, who's a senior principal engineer. Today we are going to talk about the nitro system and why we've done things in a certain way and what benefit you get from that. So far in AWS we built chips spanning multiple areas including data center IO, core compute, and machine learning infrastructure. With the nitro system, we have moved functionality away from a traditional hypervisor to our purpose-built nitro chips. This started back in 2013, and they're working with a startup called Annapurna Labs, which we would later acquire, um, a few years later. And this talks in a talk mostly about the nitro system. But I want to talk about a couple of other areas where we're investing in custom silicon. The second one is with Graviton, where we build host CPUs that deliver the best price performance for a wide range of cloud workloads. We have a range of customers running on Graviton, people like Epic Games, who are the makers of Fortnite, Stripe, the payment processing company, DataDog, and even SAP running commercial databases. Some of these customers are running the majority of their compute on Graviton today. In the 3rd place where we've invested in custom silicon is our machine learning accelerators with ranium, and we announcedranium 3 instance availability yesterday. We built purpose-built deep learning accelerators from the ground up to deliver the best price performance for machine learning inference and for training. So why do we build our own hardware? It's not necessarily easy to build hardware, but the answer is kind of simple. We're building our silicon where we can provide. Improved price performance for our customers. And there's a couple of ways we do this. One is through specialization by building our own chips, we get to specialize the hardware for AWS use cases that lets us tailor our designs to the AWS, um. Operating environment and we can tailor the performance and and the design of them to our specific needs and not burden them with many features you might ask why are people burdening chip ever with any features and the reason is it's expensive to build silicon and so when companies build it they want to. Applied to a bunch of different markets and by focusing just on our use cases we get to optimize. So I'll show you today how we tailored the Nitrosystem database infrastructure to improve both the security and the performance of our servers. The second is speed. We get better speed of execution when we own the end to end development process from defining the product all the way through deploying it in our data centers. We get to bring technology to customers faster and shrink the time between concept and delivery by hardware and software co-design. The third is innovation. When we're building our chips, we're building our servers, we're putting them in our data centers, we get to innovate across traditional silos. Um, and, and that can be really powerful and let us optimize in a way that that you can't with those silos. And lastly is security. Nitro provides us a mechanism to enhance the security of our servers through a harder route of trust, verification of firmware on the server, and limiting interactions with each of our servers through our narrow set of authenticated and auditable APIs. So niro is really a fundamental rethink about how virtualization in the cloud should be done. At its lowest level, the Nitro system has our nitro cards. These offer networking, storage, and security functionality. In the host processor you run our nitro hypervisor, and on top of that, your virtual machines and your applications. And I'll talk to you today about this, how the separation has provided better security, um. And also let us bring features to you rapidly, but first let me rewind a little bit and talk about how we got here. It all started with the simple question, after building EC2 for almost a decade, if we applied our learnings, how do we change our server platforms? We got lots of suggestions. There were performance features like improving throughput, simplifying the hypervisor, reducing latency and jitter, and being able to have bare metal instances that looked and felt like our virtual machines. And then there were a number of security features like having transparent encryption, building a harder root of trust, removing operator access from our systems, and having a narrow set of auditable and authenticated APIs. Nitro is a combination of hardware and software. The chips we're building in AWS and we've built 6 generations of these chips now. We introduced nitro back in 2017 with a CN5 instance, but we really started back in 2013 when we started with enhanced networking, moving functionality away from the hypervisor onto special purpose carts. Over time we've expanded the IO that we offer through our nitro cards to add EBS and our local storage and then we built a hypervisor where we got to remove a bunch of functionality. Since 2017, all the EC2 instances that we've launched have been based on the nitrous system. So what is it tangibly? On the left here, I've got one of our nitro chips, and that's integrated into one of our nitro carts that you see on the right. And this is what one of our servers looked like before the Nitro system. We had customer instances, and they were running on top of the hypervisor Zen. Now Zen's great, but it did a lot. It did CPU scheduling, device simulation, network limiters, security group enforcement, packet encapsulation, and quite a bit more. It even has a full-blown Linux user space in this privileged DOM 0. And all that functionality used resources on the host CPU. So we started offloading those capabilities onto our nitrocarts. No. And the first place we did that is networking. So let's dive into networking. We have a family of nitrocs across now 6 generations of chips, but from the networking point of view, they all do a very similar thing. They provide the VPC data plane offload for our instances. This is things like ENI attachments, enforcing security groups, creating flow logs, routing, encapsulating packets, providing you with DHCP and DNS, and all this used to run on our hypervisor and is now offloaded into dedicated hardware on our nitro cards. Most of our nitro cards since the 3rd generation also enable transparent to spit AES encryption of networking packets in transit to other instances without any performance overhead. And the VPC card presents the ENA device that you see in your instances today. The elastic network adapter is is really cool in that it's been very extensible. When we first launched ENA we had 10 gigabits of networking. Since then we went from 25 to 100 to 200 to 300, and today we have instances with over 600 gigabits of networking bandwidth. And it's kind of really powerful that with that same ENA device model, you can have that range of performance. You can stop an instance on one machine, move it to another, and have an order of magnitude, more networking bandwidth if that's what your application needs. This card also provides our elastic fabric adapter. EFA is a network with features geared to HPC and machine learning workloads. These workloads are special. Most workloads either need low latency or they need high bandwidth, but ML workloads and HPC workloads tend to need both at the same time. They want high bandwidth and low latency. And To build our elastic fabric adapter, we had to build our own protocol to transport data across our network. We call this a scalable reliable datagram or SRD. SRD lets us use multiple paths through our network simultaneously to find you low latency and high bandwidth paths. Let's talk a little bit more about that. Traditional network protocols take a single path through our network, and if there's congestion, they're relatively slow to react to our failures. Our networks kind of look like this. They're a cross network, and between any two servers, you can see there's many different paths that your packets can take. And with the knowledge of our networks, we can detect congestion and we can route around it. At data center scale times, not at internet scale times. And this is exactly what we do with our SRD and our nitro cards. SRD provides a foundation for using many paths to our network simultaneously, allowing us to distribute bandwidth across those paths. And react quickly to any congestion, any link issues, and significantly reduce tail latencies. These tail agencies are particularly important in distributed applications where the P50 is not what you're looking for, but you're looking for 4 or 59 latent tail latencies, um. And let's look at how kind of. SRD is used. So the first way we used SRD was with the elastic fabric adapter. Here I'm showing scaling of an HPC workload where you have, as you increase cores, you'd expect to also scale linearly. You'd like to be on the red line. Before EFA we were on that purple line. We scaled pretty well to about 400 cores and then everything tailed off and actually we started scaling negatively. When we added EFA, you can see that scaling trend continues. We've now had 4 generations of EFA. And we launched the most recent one with our P6B200 and GB 200 instances. Now, another place that expects in order packets are TCP TCP flows. TCP uses packet ordering to detect congestion. And so through our network, flows get hashed to a single path. And that single connection, all the packets flow through it. Now, sometimes large flows, even in an overprovisioned network, can end up hashing to the same path, and there they can result in network congestion. That's what I'm showing here in the yellow line. This can result in packet delays and packet drops, which causes TCP to back off and results in some retransmits, reducing total throughput, and everything continues happily. TCB congestion is generally good, but it was built for internet scale. Um, time. Other than congestion, occasionally you can have a failure in the network, like a link failure. And this is what you see with this red circle and um with this X here. In this case, TCP doesn't respond well at all. You have to wait for a timeout, re-establish connection, and try again. So, why not just send this across multiple paths? Well, because TCP has that expectation of in-or delivery. So with our Nitro cards we also built something we call ENA Express. While TCP doesn't handle out of order segments, we've offloaded the functionality of this to our Nitro cards. It allows you to send traffic across multiple paths at once, either TCP or UDP traffic over the SRD protocol, and the Nitroc takes care of assembling the packets into the right order before delivering it to your application. And the benefits can be pretty amazing. The single flow bandwidth goes from 5 gigabits up to 25 gigabits, and we've seen tail latencies reduced by 85%. It's really easy to enable ENA Express. It's a simple configuration. It's available in the same AZ. Excuse me. And it's transparent to PCP and UIB. So when AC2 started, we had um 1 gigabit networks were pretty common, and it was this way for almost a decade. Today we've seen a shift to 25 gigabit to 100 gigabit to 200 gigabit networks. The increasing base of bandwidth has been incredible. And if you look at our instances this year, we announced CMR AI instances that have up to 100 gigabits of bandwidth in our core sets of platforms. And in our network optimized sets of platforms we have the C and RHGN instances that have 2 300 gigabit network interfaces in a single instance, so 600 gigabits of network bandwidth are available to an instance if you need it. Obviously, I was talking about core instances there. If you look at our, our machine learning instances, we've gone from 400 gigabits with the P5 instance, back in 2020, up to 6.4 terabits in a single instance today with our P6 instances. OK, so we talked about networking. Let's take a minute and talk about storage. NVME is a great standard protocol with lots of standardized drivers across different operating systems. And our Nitroc exposes an NVME interface on one side and translates those commands to the EBS data plane on the other side. This lets us use those nitro cards to encrypt your data in transit um with a hardware engine on the nitro cards, and it means you don't need to make a trade-off between performance or or encryption. You get encryption just transparently if you enable encrypted volumes. And you might imagine here too, EBS uses SRD, that protocol I just talked about, to send packets during multiple paths to where your EBS volumes are located to reduce tail latencies. And just like with networking at the easy to instance level, you can see the amazing increase in performance that we've offered over the years. If you go back a decade, we had 2 gigabits of uh EBS bandwidth. Today we offer up to 150 gigabits of EBS bandwidth and up to 720,000 IOPs in a single instance. This lets you run things like demanding databases that. While also having the durability and enterprise features of EBS. Now, EBS is great. It's the answer for most customers, I think. You get durability, you get snapshots, but there are some customers who also want local storage. So let's talk a little bit about our nitro SSDs. And to do that, I need to tell you a little bit about how an SSD works. There are a couple of components in an SSD. The first is the NAndN. This is where the bits are stored, but NAndN has a lot of peculiarities. You can only write it after you erase it, and the chunks that you tend to erase are about 1 megabyte. So even if you want to update just a single bite, you actually need to copy the data to a new location with an update. And and so This ends up Excuse me, needing something that looks a lot like a right logging database. NAN also has a lifetime. If you write it a bunch, write one block a bunch, you'll wear it out. So you need to spread the rights around all that flesh. And to manage this complexity, there's something called a flash translation layer. It's usually a chip on an SSD. It maps the logical addresses that an operating system knows about to the physical blocks where the NAND is. It performs garbage collection. It performs ware leveling. And ultimately you end up looking like doing something like these databases, like a right logging database. Now there's lots of flash manufacturers that produce their own SSDs with their own FTLs. Just like with databases, actually, each of those FTL implementations behaves a bit differently, even though they all offer the same external API. They all do a good job in the average case, but our experience over many years is they have some unpredictable behaviors garbage collection. Decides to kick in just at the worst possible time. And these unexpected behaviors make it hard for us to engineer consistent performance. So how can we get the performance we need? Well, you could probably guess by this point, we integrated the FTL into our nitrocs, and so with this, we have up to 60% lower latencies. Improved reliability And through our nitro cards we can encrypt all your data with an ephemeral key that never leaves the server. So this is what the server now looks like after we moved a bunch of functionality away from the host CPU onto our nitro cards. You can see that Dom 0, that privileged domain in Zen, is significantly offloaded, and most of the functions are now handled by the Nitro system. Now I'm going to turn it over to Filippo. He's going to talk to you about the Nitro Hypervisor and some of the other features we offer in the Nitro System. Thank you, Ali. So let's now look at the NIR advisor. After having offloaded AO to dedicated hardware and purpose-built hardware, what we are left with is The bare minimum that a hypervisor needs to do. This means focusing on CPU memory, and device assignment. With device assignment, I means providing a path for the virtual CPUs to have direct access to the dedicated hardware that we just added to our systems. On top of that, we want to make sure that we strip down the hypervisor, removing every other feature that's not necessary. For example, when hypervisor is involved with networking or storage, we can get rid of the network stack. We can get rid of the storage stack, so no, no more file system support. We can also get rid of additional services like the server to make sure that access to the hypervisor, which is the closest component to customer data, is completely restricted. This results in a very small, lightweight, and secure hypervisor, a hypervisor that is quiescent, and it's only going to execute code whenever the customer instance, asks for hypervisor services. Think about instruction emulation, for example. And when you put this all together, You have a hypervisor that provides very minimal performance overhead such that our virtual machines can have close to bare metal performance. If you take a look at our most recent series of virtual machines and bare metal machines, it's very difficult to find differences in terms of performance between our metal and our virtualized instances. Beyond stripping down the hypervisor to the bare minimum, we also wanted to go one step further. The hypervisor runs at a layer of the CPU where it normally it normally has access to everything. It's the most. Is the highest priority component that normally has access to all the CPU and all the memory in our system. However, this violates one of the main security principles, which is the principle of least access. So we want to make sure with the Nripervisor, we wanted to make sure that the hypervisor was not going to have access to any customer data unless it really needed to. When you look at the commodity hypervisor, It owns the entire outer space of the server. When you start a virtual machine, both the CPU context, which means all the CPU registers, as well as the memory, is part of the hypervisor space, which means that the hypervisor has full access. This holds for all the virtual machines that we start. When we look at the nittrap advisor, the headspace of the nittrap advisor is minimal. We are talking about hundreds of megabytes. They are the bare minimum to run our systems. Whenever we start a virtual machine. Both the CPU context and the memory for that virtual machine are outside of the space of the hypervisor. This gives us great properties because whenever There is a bug in the hypervisor or for example a security problem, we have better properties that allow us to reason about whether customer data would be enriched or not. This is true for all the virtual machines that we start on our server. Each one gets its own address space. And whenever the hypervisor needs to perform services on behalf of a virtual machine, for example, instruction emulation, what happens is the hypervisor maps the bare minimum amount of memory that's necessary to perform its operation, and as soon as that's done, the hypervisor goes back to having its previous other space that does not map any of the customer data. We call This type of memory management Secret hiding since then. Since we started our journey implementing secret hiding by removing memory and context from the hypervisor out of space, this has also been called the design of a secret free hypervisor or in some cases people are referring to inability of hypervisor to access customer data as confidential computing. A few months ago, a research group from a university in Europe published a very interesting piece of research talking about a new security vulnerability called L1TF reloaded. L1TF reloaded is a Transient execution attack that relies on L1TF and what researchers call A specter gadget. So it's one attack that derives from specter meltdown that came in 2018. Through the clever combination of L1 and Half Specter gadgets, the research team was able to show that when running on a commodity Linux and Q Hypervisor, as well as other cloud providers, it was possible to exfiltrate the private keys of a web server from one instance to another. On the other hand, when the same attack was mounted against the two instances running on the Nitrap Advisor. The researchers were not able to extract any of the private keys, and this is thanks to the design of a secret free hypervisor. We recently published a blog post with a deep dive into memory management of the Nightwrap advisor. I invite you to scan the QR code on the screen and take another look at the blog post. Let's continue our journey on the security aspect of the nitro system. The next component in the nitro system that has to do with security is the nitro security chip. The nitro security chip is a piece of silicon that is baked on our motherboard and helps the primary nitro card or the nitro controller establishing a hardware route of trust. If you look at motherboards, they have a series of non-volatile assets, flashes. These flashes contain bio UFI images, or the image of the binary image of the BMC, the BMC being the component that manages the fan speed and collects several information about the system like the temperature, voltages, and so on to guarantee that the system is operating correctly. Through the Nitro security chip we are able to consistently and continuously monitor the content of the flashes on our systems. And we do this for two purposes. First, we want to be able to update them out of band to make sure that we always have the latest software running on our fleet. And on top of that, we are able to validate that the software and the firmware that we are running on our systems is exactly what we intend to run. Beyond that, the nitro security chip keeps monitoring the auxiliary buses on our systems, things like the iC bus or the spy bus to access flashes. OK, we have removed the yo. We have moved the management of security. We basically stripped down dome zero to the bare minimum, potentially removing it. And we designed a new hypervisor, a hypervisor that's quiescent and lightweight. And that gives more resources to customers, translates into higher performance. On top of that, we have a very flexible system in the Nitro system because the nitro system is extremely composable. We can increase the number of nitro cars in our servers to provide more local storage, to provide higher performance for remote storage or higher network performance. If you look at our pace of launches in terms of instance types between 2006 and 2017, we launched 70 instance types. However, after the full nitro system has been developed and put in production in 2017, we have since launched. More than 800 instance types to the point where we are at 1,000+ instance types today and we did stop counting. it. OK, let's continue our journey to The nitro system and keep focusing on the security aspects. So security is the number one priority for AWS. But security is also a shared responsibility. There is security of the cloud and security in the cloud that we need to take care of. AWS is responsible for security of the cloud, and this starts from securing our facilities. And making sure that our hardware is in top shape, the firmware and the software that runs on our hardware is exactly what we intend to run. On the other hand, the customer is responsible for the security in the cloud. This means that the customer is responsible for updating the operating system that runs in their instances, making sure that their applications are using all the very latest libraries, for example, cryptographic libraries that are not subject to timing attacks. And on top of that, customers are responsible for using all the security features that we make available like security groups, and configuring them correctly to prevent instances that are not supposed to talk with one another from communicating. OK, let's, let's continue our journey on the security front and let's talk about confidential computing. Confidential computing is something that popped up a couple of years ago and It tries to specify. What how to protect customer code and customer data. There's been a bit of confusion, and I want to address a few topics on this front. At AWS we really believe that confidential computing is all about making sure that customer data and code is protected, but we need to understand from whom this needs to be protected. We have two dimensions here that we need to consider. The first dimension is protecting customer code and data from the cloud provider itself and from one customer to the next. When it comes to the cloud provider, the nitro system. Provides several several improvements because we removed operator access to the Nitro system and to the Nitro advisor so we make sure that customer data is safe and customer code as well. Now let's recap how the nitro system achieves that. We have encryption for all the communication channels. Where customer data may be in transit. We use secure boot and Measure boot extensively to ensure that our hardware is running the latest firmware and software and most importantly, the firmware and software that we intend to run, and we perform cryptographic validation that this is the case from the very early stages of the boot process. On top of that, we have Periodic deployments of newer versions of firmware and software. To patch our systems to have the latest security fixes, the latest bug fixes. This includes all the firmware and software that we run, including the NITripAdvisor, which we can live update with no interruption of customer workloads. Lastly, as I mentioned before, we got rid of customer of access to our servers. There is no Shell, no SSH to the Nitro system and to the NIR advisor. Beyond that, every piece of software and firmware that we build, it's built by multiple teams that are globally distributed across all the continents, and it is code reviewed by multiple people. Extensively tested and only when all our testing is passing this code is production signed to make sure that the EC 2 deployment service can deploy it following our policy across all the and regions that provides. Now let's take a look at the 2nd dimension of confidential computing. We want to provide options to customers to make sure that The customer code and data is also protected within the customer organization. There are Several levels of trustworthiness of software that we usually deploy that anybody deploys in systems. There are highly trusted pieces of code that, for example, operate on signing keys, and there are less trusted pieces of code that provide public endpoints. We want to make sure that these are decoupled so that a bug in a public in a piece of code that deals with a public endpoint does not result in leaking a sign-in key, for example. One of the solutions that we offer on this front is nitro enclaves. With nitro enclaves, the parent instance is able to donate resources in terms of CPU and memory, and these resources are used by the hypervisor to start a sidecar instance. These sidecar instances or enclaves. Has all the security properties of an instance. It benefits for all the secret hiding that I mentioned that the N hypervisory implements, for example. However, it's also a much more enclosed environment because a nitro enclave does not have persistent storage, does not have a network interface, for example. It only has a thin pipe between itself and the parent instance, so that the parent instance can provide commands for performer PC calls, for example. Nitro enclaves are integrated with AWS and when all the software in the nitro enclaves. is attested, an attestation document can be used together with to unlock content that only the enclave needs to access, and one of the most common use cases for nitro enclaves is to store signing keys, for example. Beyond nitro enclaves, we are also providing features like secure boots. AI or AI provide an immutable way to boot software on an EC2 instance. However, there is a lot of use of UFI secure boot on premise, and customers have been asking us to provide the very same features for EC2 instances. Through UFI secure boot, it is possible for the gas firmware to cryptographically validate that the bootloader and the kernel and the rest of the operating system that's going to be loaded, it is signed with a key that the customer trusts. This allows the customer to protect against the type of malware that may persist across reboots and ensures that an easy to instance will only run the software that the customer intends to run. 9:30 p.m. is the next step on this journey. 9:30PM is a TPM 2.0 implementation that conforms to the standard. And it allows Measurement of software, cryptographic measurement of software, and extending this measurement into the. So that these measurements can be used to, for example, unlock the local storage on an E instance using solutions like Lux orbit locker. Now, Secure boot and nitro and are available for a long while. However, the software stack to make use of them has historically been complicated. Having end to end attestation of a workload is not a simple step, and that is why customers love Nitran enclaves. Nitran claves provide an easy solution to validate that the entire software that's been running the workload is exactly what it's supposed to be. So customers asked us to provide to port this ease of use to instances as well for use cases where nitro enclaves are not enough. That's why we released the EC2 instance attestation. EC2 instance attestation. Built on top of you a high secure boat and the 9:30 p.m. And makes use of what we are calling attestable armies. Attestable armies are armies that can be built over and over, and rebuilding those armies following a recipe will always result in the same bit, bits that we can measure, and again we can use those measurements to provide an attestation document, an attestation document that like with nitro enclaves can be used to work with to unlock secrets. What can easy to instance attestation be used for? One of the use cases that we have in mind is confidential infferencing. Think about the case of a cloud provider. Wanting to make a closed source machine learning model model available to customers. Now the model provider may not want to disclose the model weights because those are the IP of the model provider, and the cloud provider wants to make sure that the customer data, meaning the prompts, And the inferred content will not be available to the model provider. Now two instances attestation allows us to build systems that achieve these guarantees. You see two instances attestation is available on a large range of nitro instances including instances with cranium infra accelerators as well as Nvidia GPUs, and you can start building UFI and nitro enabled attestable armies using Amazon Linux 2023 or Nixos, and this is working for both Graviton, AMD and Intel instances. Yeah A few years back we published a white paper discussing the security design of the AWST system. And where we go in more depth into aspects, security aspects of the design of the nitro cards and the NIR advisor and the nitro security chip. In addition, the NCC Group, which is an independent company, performed a review of the nitro system, including all the APIs that the control plane or any operator can use to work with the nitro system. And has concluded that there is no mechanism for a cloud operator to gain access to the underlying host or to the customer data that is stored in the instant memory, instant storage, or volumes. I invite you to scan the QR code and have a look at the white paper in depth. Now, Let's continue our journey into the nitro system and let's look at a graviton force server. This is a server that can run many virtual machines or one or two bare metal instances. When we run 2 bare metal instances, the life cycle of the two CPUs is decoupled even though they share a single nitro system. When looking at the server, All the high-speed connections, the DRAM. Everything is encrypted. DRAM encryption is something that we started doing with our 6th generation of instance families leading with Graviton 2, and since then all all these 2 instance platforms have used DRAM encryption. With our 8th generation of Nitro instances again leading with Graviton 4 this time, we also started encrypting the links that connect the CPU to the Nitro cards using ID. But with Graviton 4 we went one step further. Graviton 4 is the first Graviton solution where we are supporting multiple sockets, so up to 2 CPUs in a server communicating with one another, and the coherency link between the two CPUs are also encrypted. Now for the cloud to be secure. We have to have certainty of everything that's running and We have to make sure that at any time we have a Cryptographic certainty of all the software and firmware that we are running on our servers, on our servers, that's what we call attestation. However, doing this at the scale of it is a challenging problem. Let's take a look at How an 2 server boots or any server boos for what matters. When the silicon is light up, it loads. Code from the ROM. The ROM will instruct the CPU on how to load firmware, and then the firmware will hand over control to the bootloader, which usually resides on the boot volume, and the bootloader is further concerned with loading the operating system and from there loading the customer application. Now at every time when we hand over control from one piece of firmware or software to the next, There is a chance that something may go wrong, so we have to make sure that every single step of the boot process is validated. But where do we start? We need to start from the very beginning, from the manufacturing plants. And uh We have to follow our hardware from manufacturing through the assembly lines. Transits from the assembly lines to our data centers, installation in our data centers, and power on in our data centers. This is about creating an unbroken chain of Custody and verification of every piece of hardware, firmware and software that runs on our servers. Let's dive specifically into how this works for our most recent graviton 4 servers. And it all starts with a nitro chip. Every nitro chip. Comes Comes with a private and public key. And this couple of private and public keys. Allow us to establish a chain of trust at every step of the boot process. We use this to create a new pair of private and public key, destroying the previous version, the previous pair, and making sure that the next piece of software that we load. is exactly what we intend intend it to be. I With graviton though, we push this one step further. We do not just look at the nitro system itself. What happens is that the same The idea of having a private and a public key extends to the CPUs in our systems and This allows us to make sure that only two CPUs that are two graviton 4 CPUs can talk with one another and we can establish. A cryptographic channel between the two of them whenever we are running coherency between the two CPUs. Also, we can establish a secure link between the nitro system, meaning all the nitro cars that we have in our systems, and the CPUs. Again, relying on. Now let's put everything together and let's look at the life cycle of an instance. Whenever a customer calls for an instance. The run instance call will go through the EC2 control plane. All the necessary resources will be made available by the control plane, but the last mile of the control plane will use the nitro API, which is an authenticated, authorized, encrypted, and most importantly, logged API. That's going to instruct the nitro system on what to do. The first step is for the control plane to tell the nitro controller to allocate an empty shell which only contains CPU and memory. The nitro controller will do so by making RPC to the nrap a visor so that the nitrate advisor can perform the allocation. The next step is for the EC2 control plane to instruct the nitro controller to set up the nitro cars so that we can expose EI through devices and volumes through. The nitro controller will later on instruct the hypervisor to create a channel between the virtual CPUs and the dedicated hardware to make sure that the hypervisor will not be. In the picture whenever there is happening. The last step is for the IC2 control plane to tell the nitro controller to start the virtual machine. At this point, the nitro controller will instruct the hypervisor. The hypervisor will start executing the gas firmware, and from the gas firmware, the boot loader and gas kernel and gas operating system will be loaded again, potentially using a secure boot and nitro and so on. On a graviton force server whenever we are running in noncoherent mode. This is happening. This could happen at any time on any of the two hosts, and most critically, This also works whenever the hypervirus is not in the picture, and this relies on the nitro controller and the nitro security chip holding the CPU in a reset state so that the control plane and the nitro controller and the nitro cards can set up the system so that devices and devices exposing the root volume become available to the CPU so that the CPU can start booting the. Metal instance as soon as it is released from reset. And the very same solution is what enabled AWS to also bring Mac into the cloud. If you are familiar with our offering, we have multiple iterations of Mac instances, and in this case we connect the Nitro system to Apple MacMin using a to Thunderbolt. Connector and this allows us to provide the elasticity, reliability and security of EC2 to Mac users, including EBS volumes, VPC, and every other feature that AWS provides. This is a great option for customers who are building applications for Mac OS, iOS, and they need to perform continuous builds, testing, and signing of their applications. And with this, we are concluding our talk. We are happy to take questions outside of the room. Thank you for your attendance.