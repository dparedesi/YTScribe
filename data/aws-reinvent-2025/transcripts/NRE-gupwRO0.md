---
video_id: NRE-gupwRO0
video_url: https://www.youtube.com/watch?v=NRE-gupwRO0
is_generated: False
is_translatable: True
---

Hello everyone. Welcome. Good afternoon. Hope uh everybody was able to have like a good lunch and be able to enjoy. Hope you, I don't make you too sleep in this session here, OK? Um, one of the interesting things just to break the ice and get everybody like a little bit more engaged, who's here is the first time or like maybe less than 3 times in the reinvent or first time. Wow, quite a bit of people here, that is amazing. Who is maybe more than 10 times in reinvent. Nobody? Really? Uh, why I'm asking this because actually a customer asked me yesterday and uh I was starting reflecting and this is my 12th time in Reinvent, right? Um, I remember when trend came from the first time in the, in the reinvent was the first cybersecurity vendor being participating in the reinvent and being like seeing the evolution, how much like effort and like, uh, everything that is being built in the reinvent is super uh like impressive. Thank you for uh helping me to understand if it's your first time or not, but. Today we are going to dive into the reality of like uh how like AI it is becoming one of the most innovative uh uh environments and most innovative like a driving for organizations but at the same time how AI is transforming and becoming the new battleground for attackers like how they are taking an advantage on this new era, right? As we are evolving, creating more AI infrastructure, applications, utilizing the cloud native services, how is your new stra strategy for protecting those environments? That is one of the most important things that we wanted to discuss today. And when you go here, starting thinking about like uh this is your mission briefing for a modern AI security and how you can starting strate strategizing uh AI in a different way. A lot of people sometimes they think, no, AI is just like uh adding the guard reos or just like using the model safety in the AI application, and it's actually not true. There's multiple layers that you can think about protecting when you are building those AI applications. When you think about like this new battlefield, right, AI is accelerating innovation, but at the same time is expanding the attack surface, right? More like environments are being using AI at the same time you are expanding how attackers can take advantage or getting in in your infrastructure. Every new AI like a workload that you are building, become a new vector for attackers if you don't take the protection layer needed, right? Modern security, it is about embracing innovation while like you are staying ahead of like this adversaries and who is like basically trying to find different uh uh approach and different ways to get in in your environment. The new battleground, it is not just about infrastructure. For many, many years we think about the cloud infrastructure, but now it's about the data, the models, the AI pipelines, all that combination right now, it is really important for the life cycle when you are building the AI applications. And if you don't think in a new approach of like how you're protecting, any new like um. Um, attack vector that you create in, in one of those points here are very high valuable. Why? Why is that? Because once you affect one of the, the infrastructure, it can affect the entire ecosystem of your AI application, OK? And we are going to like in the, in the deeper details of like how attackers can be manipulating those AI applications and taking an advantage of that. When you think of when you think about like how adversaries are exploiting AI applications, you, you can think like as a simple flywheel. The data, the models, AI pipelines and runtime. Attackers treat your AI as their weapon, right? Poisoning data, stealing models, and, um, invading runtime or manipulating inputs. AI is super powerful, but That power also can be hijacked and be utilized for like bad actors for stealing your data, doing data, uh, breach of your information, stealing your intellectual property that your organization has been spending so much time and so much money investing on. The important thing here too in this flywheel, it is your user and when I talk about your user there's two major points. User, it is the user that you are having from your enterprise, your employees utilizing AI. Like everybody here, right, is using in, in a way generative AI for like, uh, chat GPT Gemini, uh. Some kind of like a bedrock, uh, applications that you created for your customers. Like there is some level of like generative AI being used in the applications. How you are monitoring any risks related with those users, that is important. The second part of the users, it is when you build those AI applications, there is like multiple users that you don't have control. You don't know if they are malicious or not. How you are making sure every single prompt that they are interacting with your AI application are safety and how you also sometimes the specific prompts are not like direct attacks. They are not a prompt injection that we call. They are, uh, indirect attacks. What what that means it is, OK, when you request your AI application that maybe will search in a different like source, uh, data that will have like um. Uh, cross-site scripting that was created by an attacker and when processed by your AI application, it will create a data breach. People don't think about that, but the input and the output needs to be validated always as an additional layer protection, OK? That's why user is critical in this main like file wheel here when we were talking about. Data poisoning, right? A lot of people talk about data poisoning and it's one of the most damaging attacks. By injecting bad data into training sets or fine-tuning process, this can manipulate how your models are going to act, or basically creating the response for your users, like employees, as I mentioned, and your user as a customers as one example. Making sure every single data that you are using in the data sets are being like safe, and they, they, they don't have like any kind of like malicious content, it is critical for every single AI models that you are uh training, but also the database vector that you are creating, OK. Motif. This is important because a lot of people forget that like models can be actually stolen in some cases and redeploy it in an infrastructure. One interesting thing about like this session, it is the name of this session was based actually um in a thread research that was made by uh one of the, the trend micro thread research, Alfredo Oliveira and David Fisher Fisher. Um, they find out like that like specifically models can be uh utilized as a way for you to export the model from containers, manipulated the, the, the way how they were being trained, and redeploy again to the container, and the application, it is actually changing the outcome of the responses. Right, that's why when you think about model TI, it is not just like about manipulating, but also attackers right now. They are trying to get your models to resold in the actual bet market, right? It's intellectual property. A lot of customers here, they, they not just use the regular data, but also sensitive data as part of the training, OK. They are kind of like what we call the new crown jews for organizations, right? AI pipelines. There's multiple points in the AI pipelines. There is like, uh, the poisoning that can be in the model build when you are adding or when you are creating that pipeline. There is the. The inject backdoors that can happen through the supply chains from like open source libraries that you may be using or maybe for like uh affecting developer machines that are interacting and adding backdoors to your applications. There is a uh a way to manipulate and alter the outputs from those applications, prompting injection for sure and interference attacks are the most common for like exploitation. And shadowyIs are becoming one of the biggest problems, and when I say about shadowy eyes, it is. Do you have a full visibility of like all the AIs that your organizations are using today? A lot of companies, they don't, right? And it's complex because every single person, every single developer, every single cloud architect, in some ways they are using different tools and having that full visibility for like everything that is like AI technologies, it is important for you to minimize that shadow AI problems that you may have in your organization, OK. I don't know if you saw or you're monitoring a lot of Reddit, who reads Reddit here? OK, half of the room, that's good. Um, I was reading this one and catch my eye and said like I need to use this in my slides this week, right? This was like an organization that was sharing their challenge about like why they fail in AI project, and I love and highlighted this specifically spot here where they are saying our mistake was relying on model safety alone and not like in runtime wired rails. Why highlight about this? Because model safety, it, it was created for one specific outcome for every single model, and every single model has your own way to do model safety. But every single organization, they have your own gra rates that they should be created on top of that. You are not just going to be trusting in the model safety from like every single model that there is outside because you have no validation um in in a lot of cases. There is some, uh, specifically companies that do the testings and all of that, but having your own word radios that you can making sure which kind of protection you have for every single AI agent, agentic, uh, architecture, and AI applications, it is extremely important, OK? Please, making sure you have that. Don't, don't get to the same mistake. What I bring about this because I think sharing, it is important. Bringing that as a highlight for people to remember when they are building that, it is extremely important for all of us. Traditional defenses are very reactive. We know of this, right? For many, many years we have been using specific technologies that are being the same for like protecting workloads, protecting uh servers, or protecting like uh containers in some cases. AI threats evolved too quickly, like what was AI six months ago, it is completely different than right now. Model drifts, pipeline updates constantly, agents making a huge amount of like automation decisions, more and more with agentic approach. It is extremely important for us to start to rethinking, not in like a static approach of protection, but in a, a more proactive approach of like how we are protecting our own agentic uh agentic infrastructure and AI applications, OK. You need to be adaptive, you need to be proactive, and you need to have some kind of like AI awareness that the AI can learn through the process and understand your risk, uh, prioritization for you to mitigate it as soon as possible before going to production. I created like a little lineup of like this session with like the different layers that we should be reflecting about, right? And let's begin at the foundation here, data. Most AI risks originated actually in the data, right? AI systems often process massive amounts of data, and any blind spots, how data can be stored, shared and classified becomes a direct attack vector for like specific attacks. This visibility here, it is a portion of your data that is considered classified data, right? You know what is, what is that data, you know the information usually, um, but that's like just a surface, right? Majority of the data that we actually have are under like that surface, are like unknown data, are unclassified, are like unprotected data that a lot of times people forget about that. They use as part of the training, they use as the the database factory, they use as the DI creation for your AI applications, but they actually forgot to check and validate it if there is any sensitive information before the actual utilization of that. And this is where things can be a little bit tricky here, like, understanding that data and understanding if they are uh sensitive. And you can, for sure you can use for your AI applications, but now, OK, my AI application is utilizing one specific set of like sensitive information. The level of protection on that application needs to be way higher. Because the risk of data exfiltration is extremely high and risky for the organization, OK. When you talk about data leakage, it can happen anywhere, during training, inference logs, uh, through API's communication, even through models outputs, right, the interaction that you have with the models. Attacks exploits uh this weak points to extract to extract any kind of like sensitive information that they may find PII data, uh, information about passports, people, emails, uh, account information, and so on, depending the level of like, what is your organization providing outside, right? And they can do some level of like a reverse engineering depending the, the information that they get. Our response here and how the way how you should be thinking it is like how can you create a data posture security management, right? How you can start in monitoring every single data set before the actual training before the actual utilization of your AI applications and understanding which ones are sensitive, which ones have Like intellectual property from your organization and which ones are like actually no no problem right it's a public information that can be utilized and if you have any attack on this application even it's like compromising the image from your organization, they are not compromising the data that is the most important and the crowd use for a lot of companies. OK, the next one, we move to the stack up and we talk about like uh AI microservices, like supply chains. AI components depend on like a countless number of like libraries, uh, open source, uh applications that is being utilized outside containers and external services, right? Each of like has uh uh indirect way for infiltration by attackers, right? More and more we see supply chain attacks in the organizations, and that's why it's really, really important for all of us to make sure because a single vulnerability, a single dependency issue in your application can be a domino effect, can be starting effect in the entire application and the way how you were building the security strategy for that like environmental workload. When a new component, it is compromised, the entire life cycle will be affected, and not just like the run time, but also any new application that is being utilizing the foundation that you used from from uh for other applications, right? That. That's why it's important to have a continuously monitoring for any vulnerability uh in the pipeline, in the supply chain, secrets that can be forgotten inside the container image, uh, malware that can be embedded by attackers, and so on. And when you talk about like attack vectors, right, there's 3 major ones that I actually got here dependency poisoning, this is probably one of the most common when you are using a lot of the open source libraries and if you are not validating those, uh, unverified image that is very, very important for all of us. I know we talk about shift to left security, we talk about container image scanning. It's still a lot of companies missing that basic or foundation layer protection. Very, very important for all of you to make sure. And when you talk about like uh, it's not just looking for vulnerabilities, but also looking for secrets. Recently, we have been finding more and more container image that are public facing, that in some ways there is a way for you to access secrets inside the container image that can compromise your cloud infrastructure. That could be AWS credentials, it could be applications credentials that are like plain text, forgot inside the actual uh containers, right? Really important for you to make sure you are validated and verifying and maybe creating like a signature for every single image. This way if there is any kind of like a compromise supply chain, the image needs to be validated by your uh security uh controls that you have it. And a pipeline compromise, right? A diversary is trying to affect the systems, not just like the regular, uh, from like a code repository to the CICC pipeline, but from the developer machines. More attacks are being affected the developers that can drive malicious code to the actual supply chain that it needs to be validated. When you talk about this, we are talking about like uh CIT pipeline verification. We are talking about container and code integrity checking for uh vulnerabilities, secrets, malwares, misconfiguration, and all that information being part of like your application visibility. Whatever you deploy in production, how can you have a full visibility from the shift to left to the runtime, OK. Model and AI agents and when like things can go like rogue, right? Models and, and, and agents can be, can become like attack factory themselves. Once they are compromised, they can basically interact with your systems and affecting the way how they are being manipulated or getting information that can be expatriated outside and can be basically affecting data and be harmful for the infrastructure that you build, right? Models can be poisoning, manipulated, or misled through, uh, subtle, uh, adversaries, right, inputs. AI agents which can act autonomously introduce the entire new class of like risks that can affect the entire infrastructure when you are building the agentic approach. When you are talking about like a gentech approach, if you have an AI agent that is going outside their like uh scope, right, this is actually an interesting conversation because yesterday I was talking like with like one of the biggest software organizations in the globe. And one of the biggest challenges that they are facing right now, they are creating more and more agents as part of their agentic approach, but they don't have a way to control the scope of the agent. If the agent is starting acting in a different approach as they created, they don't have a way to remediate it. They don't have a way to mitigate and like uh contain that agent to not affect other parts of the infrastructure, OK. That's really, really important and it's very associated with the unauthorized actions on those agents. Key attacks vectors includes malicious input here, rogue agent behavior, manipulate outputs, attacks try to steer the models into a safe, unsafe actions, right, or bypass the safety components. That's why I was talking about like the model safety. Every attacker have access to your models that is like open source, and they understand really, really well how the model safety works in each model. But the guardrails that you created, they are customized for you. You understand the challenge and the risks in your AI applications. That's why it's so important for you to edit on that, right? And when you talk about like a guardrails, there are two layers here. Because model, it is a very um dedicated and mathematical way to decision. Like you can ask the same question 100 times to the models. The answers and outputs in some cases will be different. You all tested this already, right? Um The point here it is. When you are testing, securing the models, you cannot test just once. You need to keep like continuously testing the models to make sure maybe that attack that didn't work today, maybe it work tomorrow. Because you change something, you're retraining with like a different data sets, and the actual decision making of the model are changing and giving information that they should not, right? That is why it's really, really important for you to make sure that interaction and continuously validation when you are building those applications. And one of the interesting things that we did this week, it is a launching of like a new tech that we call AI scanner. What that means it is, OK. You can point it out AI application, the AI API endpoint. We are going to scan that AI application for sensitive data disclosure, for prompt injections, system prompt leakage, malicious code generation, agent 2 definitions leakage, and many other things associated with the OAS top 10 for LLMs and generative AI. Those informations and those risks are being utilized by our systems in the back end what we call LLM judge. Our LLM judge, it is embedded as part of the guard reus. This way, whatever risks we detect there, it can be automatically protected. I like to say, for, for people who know trend, this is like the new virtual patching for AI applications, right? We have a virtual patching for protecting vulnerabilities for many, many years. This is a new approach of like how we are protecting the application, the AI applications in real time as We identify those risks, uh, right away, OK. This basically extends of like how you were, uh, creating, uh, a, a prevention mechanism, uh, for every single wasps top 10 being, um, analyzed by your scanner as one example. Infrastructure and under siege, right? Um, AI infrastructure, it is extremely important and attackers are trying to take advantage. It could be because the resources exhaustions, it can be by the utilization of the GPUs, it can be by the cloud services infrastructure that they are trying to take advantage for mining or crypto mining, uh, for their own specific gas. This visual here, it's a it's a front line infrastructure right when you think about like how you're protecting like your castle of like you when you are building AI applications right compute privilege access um access coverage from the different like AI applications or containers or microservices all that highlights a a huge impact that can affect the AI applications. But when you, when you think about this, usually a lot of customers are concerned about like how attackers are not being compromised in my infrastructure for resources exhaustion, for increasing the cost of the GPUs utilization that happens in some cases when they have access to your infrastructure, and also data leakage, right? Those are the three most critical things that I usually hear from customers. When infrastructure controls are weak, attackers can hijack workloads and drain resources and take advantage of your cloud accounts. There are companies that went bankrupt because this kind of like attacks back in the day, right? I hope no more as more people are aware about the security elements that they should be adding for every single account. But it's really, really important for all of us to remember that security needs to be created from the scratch, from the moment that you are building your AI infrastructure, OK? When you think about like those different like attack vectors on the infrastructure, a lot of times they are related with the privilege escalation, cloud misconfigurations, and vulnerabilities that can be affected from outside to inside the organizations. A lot of them, they are focusing like a denial of services, right? Uh, basically stopping your systems to work, right? And people won't be able to access. In a lot of cases also, as I mentioned, the cost increase that can create like a massive problem for some organizations, OK? Infrastructure, it is important because you need to think about like the AI security posture management, how you are monitoring for every potential misconfiguration in Bedrock, Sage Maker, or any other infrastructure that you are using from the cloud provider, but also how you tie in that with the new compliances like, uh, it could be, uh, the EU Act from Europe, it can be like a new systems from the CSA in US. It can be from OAS top 10. Um, that like it is being very vocal, like how you should be looking for those risks involving AI applications. All those elements are tied in with the AI secure posture management. The other point that is AI models, resources, controls. When you are having those infrastructures, they usually generate logs, right? Majority of the logs are being basically Collected through like a VPC flow logs in some cases and the cloud trails. What we do in these cases, it is collecting that information and apply threat intelligence on top of that, right? If we see any suspicious activities happening with like a sage maker or Brock, you can basically create a a cloud detection response to mitigate those attacks right away, OK. Network. As more and more organizations are building the AI applications and making that available, there is a new path, right, for manipulation. In the beginning, AI applications was like basically focused of like the internal usage. Now, more and more companies are putting that outside, and it's very, very important for us to validate it, because in some organizations. They have a hybrid infrastructure. I was actually right before here talking with an Israel customer, uh, where they have like a, a full dedicated data centers for some of the applications, but they're also creating some level of like a cloud infrastructure, and they wanna connect with each other. Hybrid, it is a very common element for a lot of companies that we see today. But how they can have the same level of protection for like the API's communications, but also potential vulnerabilities that can exist on those infrastructure, OK? Attackers use vulnerabilities that are like fighting very often. One of the things that trend try to help in this way, it is through what we call zero day uh initiative, ZDI, OK? We are the biggest bug boundary for enterprise, uh, um, uh, vulnerabilities in the market, right? Last year we found 70. 3% of the total vulnerabilities in the market and our goal is like how we can bring that information about the vulnerabilities in the different like uh AI stack applications for you all to be aware of like the issues and the risks involving AI technologies, OK? It can be MF flow, it can be MCP servers, it can be many different like parts of the AI stack that you are using that sometimes people forget that potential vulnerabilities can exist. And a part of this, um, we, we did like a recently launch with AWS on the year AWS Phawa where we can edit our IDs and IPS rules on top of the AWS Firawall for years to protect against those exploits, those vulnerabilities. It's additional layer and clouded, uh, clouded like a native approach with AWS to enhance those network protection. Um, when you talk about like network attack vectors, right, we are talking about like an API interception that are very important, lateral movements that can be moving from one container to another or from one VP, uh, one VPC to another VPC. Data exfiltration when the attackers find the critical assets or the critical data they are trying to exfiltrate it from uh to outside, right? And having that to like layer visibility or that like layer of like a risk, um, with like a different like network sensor, it is really important plus the IDs and IPS visibility. If there is any exploits coming to your cloud infrastructure, how do you see that? How can you not just like detect, but how you can actually prevent and block those exploits right away. Trafficking, tra traffic inspection is really important, not just for the regular traffic, but also the APIs that the people forget about this was, uh, I was checking some data recently and I was actually super uh skeptical and concerned. Because a lot of customers, they are forgetting about you adding a layer of like authentication, authorization in API gateways and API end points. 40% of the total uh APIs that we are monitoring today, they are not having any level of like authorization and verification. This is super concern. What that means it is, if those API endpoints are connected with critical data. They have a freedom. It's kind of like a free highway to get access to that data. Please make sure every single time when you have an API open to the internet, they are being validated. They have some level of authentication as a minimal layer of protection. Plus, if you can add it like some layer of like ideas and IPS to protect that too, OK. Users, the human factor, it's been the biggest problem for, for probably since the Internet has been created, right? Every single organization we have that user that can click and do, uh, all the things that the security teams or the security awareness tell them to not do it, they actually go against, right? Even if we do the, the training awareness and all that. Uh, but how you can make sure the human error, it is not giving like access to infrastructure? How do you make sure every single developer are aware about like the risks involving of like things that they are accessing from outside? It is super, super important to understand the access control, the permissions that the users they have, phishing, social engineering attacks that happens every single day. Those awareness, it is super important for people to starting paying attention to what they are doing, right? More and more scams are being created with AI right now, it's been crazy. Deep fake attacks and more are coming as a big problem when you think about human. One single Click or one single malicious library can open the door for like the attacker inside your organization. That's how critical it is a human in our companies. If they do a mistake, it can be opened the door, and if you don't have a validation after, they will be able to take advantage of like uh going lateral movement, detecting sensitive information and do a couple other things that can compromise your company. Misconfiguration is a good, it's a, it's a problem sometimes, right? Uh, overexposed access for specifically users and deep fake manipulation are increasing heavily, uh, mainly like the deep fake for sure with the usage of like AI on those, uh, technologies. Having ways that you can monitoring, for example, developers today are, uh, creating AI applications in their own desktops because they're getting more GPUs and all, and so on. Having ways that you monitoring those AI applications and making sure attackers are not manipulating the configuration file and those applications are important. Having, like, it's important for you to have this kind of layer of protection for local AI apps. The second one, having Um, a way to detect and protect against deep fakes, OK, that's important, and phishing attacks, for sure, every single organization should have already because it's increasing every single day with AI. OK, as we wrap up here, the mission is, I think it's very clear, right? We, we create like a little, um, a flow of like coming from the data to the, to the users, the, the last point in every single organization. Securing AI requires a connecting every single layer from the data to the users with a full visibility and proactive controls. When you think about this, it is like not just adding one layer of protection. AI security only works when data, models, pipelines, and runtime are unified under a single security lens. Why do I say this? It is. Sometimes if you have like uh disparate technologies for data for AI pipelines or for models, the problem in this case, it is you are not correlating those elementaries. You are not uh correlating the findings that can help us to create like a mitigation and detection response. Super important for all of you to make sure those points. As I mentioned, this session here was actually based in one of the thread research that we found globally. It's called, uh, this, uh, article was, uh, called it the Silent Sabotage Weaponizing A Models in Exposed containers. This case, a malicious actor was able through an exposed container that was public facing, um, um, extracted the model, right? Get the model, manipulated the data of the model. Redeploy or kind of like reimport that model back to the container that's being exposed. And compromising the way the predictions are being solved, right? Why I mention about this because there are technologies like container secure technologies in runtime that you can have this kind of like monitoring like one of the, the rules that I was mentioning here is like uh any kind of like um um external connectivity with the. Containers that are trying to extract files or that they are trying to manipulate files in the container. Containers should be mutable, right? I like should be not changing. If you are changing, you should be changing the pipeline before you push your production, right? But this is really interesting um article for you all to see it and understand a little bit more where this session came from and why we are talking about this layers protection here, OK. 11 of the interesting things we talk about like uh this, this approach of like layer protection came from my actually last session in Rinvent uh where I created what we call AI security blueprint, OK? And that AI security blueprint, we actually not just wanted to create as a, a paper, as an informative information for you, but we bring that to our platform. We brought like every single layer on that one. To the developers, deployment, runtime, AI workloads, AI applications, AI data, and as we identify every single specifically piece of AI infrastructure in your environment, we also map it if you have the layer of protection enabled or not. The idea here is like helping you to identify if every single layer of protection that we have, you are being able to protect and prevent from uh malicious actors on that, OK? When you, when you talk about like those layers of protection, what I did here, uh, for customers to help, it is, OK, data, what are the security challenge? What are the secured, uh, controls that exist in the market and how our platform can help with some technologies, microservices. When you're talking about microservices, it's not just Kubernetes. It could have been NIMS, Nvidia NIMs, right, where you are creating those containers, uh, that are being utilized by, uh, Nvidia technologies. Models, agents, uh, how you are protecting that interaction as you are building those AI applications and retraining those models. Infrastructure, it could be a uh AWS or any other cloud provider plus any video infrastructure if you have some of those, how you are applying AI uh secure posture management, API risk visibility for unauthorized risks, uh, AI detection response, as I mentioned, when you collected those. Information from Bath Rock and SageMaker you are able to apply detection models on top of that to identify any suspicious activities, and AI application security that I mentioned about the AI scanner and AI guard. OK. The last, uh, the two ones here because I know people were taking photos. Um, was like the network layer that's related with the ideas and IPS where we can prevent and protect against the exploitation when you have those applications or those AI stacks, how we talk that, right? And the last one is the user, how you can basically protect on those. There's a, a comprehensive paper on this uh um AI security blueprint that I go in every single detail here and explain more um information on like why you should be protecting the use cases and the level of like the, the layer of protection that you should be thinking when you are creating those strategies, OK? OK, preparing for like the, the next wave. AI borne attacks are being accelerated every single day to stay resilience we must anticipate a threat. We need to think in a different way how we are protecting against those threats, automated defense and evolve, uh, infrastructure as. Fast as adversaries are evolving, the attacks are extremely important for us today, OK. Proactive AI security, it is something that you should be thinking for like this new adaptive way for you protecting not just your AI applications, but every single application that you have in your organization. The velocity, how we are innovating in every single company today, it is unbelievable. Right? We, we release the new applications, we release new versions every couple of minutes in, in those companies here today. OK, we are doing some pretty cool stuff. I don't know if you saw what we call a trend Division 1 vault. Uh, people in the United States can win $10,000 if they crack the code. This session is giving one of the codes for, uh, for you to, to have the, the way to test it and trying to crack the code. The number 3, it is the number that I'm giving from the, uh, the session. I hope some of you will win, OK? We're, we are playing this, uh, since Black Hat and nobody was able to crack, and I hope somebody this time in reinvent will be able to crack the code for the $10,000 OK? Without more, I wanna say thank you so much for joining me today in this session, really appreciate your attention. And if there is any, any, uh, question, I'm open because there is uh actually time here that I can answer some of the questions, but thank you so much everyone, really appreciate it.