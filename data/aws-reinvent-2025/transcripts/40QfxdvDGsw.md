---
video_id: 40QfxdvDGsw
video_url: https://www.youtube.com/watch?v=40QfxdvDGsw
title: AWS re:Invent 2025 - Advanced VPC design and new capabilities (NET340)
author: AWS Events
published_date: 2025-12-04
length_minutes: 60.15
views: 945
description: "Amazon VPC is a foundational service on AWS, giving you control over your virtual networking environment. Every year, AWS makes updates to Amazon VPC to help improve functionality, security, and usability. In this session, learn about the latest updates to Amazon VPC and how you can use them to enhance your current architectures.  Learn more: AWS re:Invent: https://go.aws/reinforce. More AWS events: https://go.aws/3kss9CP   Subscribe: More AWS videos: http://bit.ly/2O3zS75 More AWS events videos..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
summary: "Alex and Andrew, networking specialist solutions architects at AWS, present a comprehensive deep dive into the Advanced VPC design and new capabilities, unpacking over 150 features and innovations launched throughout 2025 that aim to transition networking from a manual, static discipline to a dynamic, policy-driven practice. The session is meticulously structured to cover the entire networking stack, starting with foundational improvements to Amazon VPC. A major highlight is the introduction of the Regional NAT Gateway, a game-changer that replaces the complex, per-Availability Zone deployments of the past with a single regional construct that automatically scales capacity and resilience, thereby eliminating the significant subnet configuration overhead that often slowed down infrastructure teams. Security remains a paramount theme, with the speakers unveiling the Network Firewall Proxy—an explicit proxy solution that enables granular, improved control over outbound traffic. This feature allows for pre-DNS inspection, pre-request filtering, and post-request analysis without requiring cumbersome route table modifications, as clients connect explicitly via HTTP connect protocols. They also introduce VPC Encryption Controls, a governance mechanism offering monitor and enforcement modes to ensure that all traffic, even between non-Nitro instances or across load balancers, is encrypted in transit—effectively blocking the launch of non-compliant resources. Moving up the stack to Application Networking, the presentation details significant advancements in VPC Lattice, which now supports custom DNS for resources. This removes the operational burden of managing private hosted zones in client VPCs and simplifies connectivity for complex architectures, such as Service Network Endpoints that facilitate layer-3 ingress from on-premises environments or cross-region setups where resource gateways are inspected by firewalls before traffic reaches SaaS providers. A substantial portion of the talk is dedicated to global connectivity through Cloud WAN, which has been enhanced with Advanced Routing Controls. These new features grant cloud network engineers capabilities akin to traditional physical routers, including route filtering by prefix, AS path manipulation, and backup path configuration using JSON-defined policies that support instant version rollbacks. The duo also reveals AWS Interconnect Multi-Cloud, a groundbreaking partnership with Google Cloud (with Azure following in 2026) that provides fully managed, encrypted, and highly available private connectivity between clouds, rendering the manual cabling and third-party router management of the past obsolete. Other key announcements include the VPC Route Server for BGP-based routing updates directly to VPC tables—enabling architectures like floating IP failover—and a massive push for IPv6 adoption, with over 75% of AWS services now supporting it. By aggregating these 150+ innovations, Alex and Andrew paint a vivid picture of a future where networking is defined by intent, secured by default through features like security group referencing across Cloud WAN regions, and managed globally through auditable, code-like policies rather than error-prone manual intervention."
keywords: Amazon VPC, Advanced Networking, Regional NAT Gateway, Network Firewall Proxy, VPC Encryption Controls, VPC Lattice, Cloud WAN, AWS Interconnect, Multi-Cloud Connectivity, IPv6, Architectures, VPC Route Server
---

Alright, alright, awesome. Well, thank you so much folks for joining us. Thank you for giving up on your happy hour today, uh, day 1 plus 2 of reinvent. Um, I'm Alex. I'm a networking specialist solutions architect with AWBS, and I'm accompanied today by Andrew. I'm Andrew Graham, a principal solutions architect as well, networking specialist, yeah, and we are, uh, going to be talking to you folks about all the new feature capabilities launches that happened across AWBS networking throughout the past year and all the cool new architectures that you can build with them. But before we begin, this is a 300 level session, so we will go fast we will go deep in some areas we will go wide as well because we do need to talk about all networking services, right? We will let you know when animations end so you can take a photo of the slides. We know that everyone loves taking photos, so, uh, you will see that icon on slides and you'll see, uh, when to, when to take photos and Q&A as I mentioned, is going to be after the session. So keep that in mind, not during, uh, not during our session. Now, as you folks are accustomed to, we're gonna talk a bit about foundations so that everyone has, uh, the, uh, concepts and knowledge about all the things that, uh, make up AWS networking, and we're gonna start with the AWBS backbone, the AWS global backbone made out of regions, availability zones. Uh, these numbers I'm hoping they're still up to date since this morning. Um, we, uh, the, the coolest number there is more than 9 million miles, uh, kilometers of fiber, so. Uh, that's pretty impressive, and this is what powers, uh, or what, uh, what helps from an infrastructure perspective host all your workloads in AWS, right? From a hosting perspective, right, you have your Amazon VPC, which is hosting your, uh, compute, your workloads, uh, your instances. Amazon VPC is a regional construct, um, and within the Amazon VPC you can spin up, uh, subnets that are those, uh, containers for your, uh, workloads. Subnets can be IPV4, can be IPV6, can be dual stack, and they are AZ level constructs, right? So per availability zone. Now, uh, from a security perspective at the VPC level you have network access control lists which are subnet level access controls, stateless, uh, firewall, uh, filtering, and, uh, you have security groups which are your, uh, stateful firewall filtering. Last but not least, you can insert AWS network firewall in the picture to, uh, get you, uh, traffic inspection, decryption, advanced filtering controls for your traffic. Now from connectivity perspective, if we go and we look at VPC connectivity, um, you have Internet, uh, connectivity with the help of the Internet gateway and the egress only Internet gateway and of course you have IPV4 and IPV6, uh, flows and constructs that help serve, uh, each and every one of them from an IPV4 perspective, uh, you have the NAT Gateway, uh, which, uh, is a regional, uh, regionally available construct, AZ level, uh, deployment. Uh, and for IPV6, uh, to accommodate for the same type of egress-only flows, right, that don't accept egress, you have the uh egress only internet gateway. Now if we go towards application scaling, uh, because it's not just about application connectivity but application scaling as well, uh, our elastic load balancer suite encompasses three load balancers application load balancer, network load balancer, and gateway load balancer. We're gonna see those deployment models in a bit more in depth. Now if we are to talk about connectivity to AWS services, uh, we have AWS Private link which gives you uh two types of endpoints in your VPC. Gateway endpoints do not have interfaces in your VPC. Um, they connect you to S3 and Dynamo DB while interface endpoints, they have interfaces in your VPC. They're deployed in a subnet or a set of subnets in multiple availability zones, and they require you to update DNS, right? There's no routing updates as in the case of, uh, gateway endpoints. Now from extended application connectivity perspective it's not just about connectivity to AWS managed services but it's also uh connectivity to your own application so app to app connectivity. Uh, here's where Private Li started, uh, the capability of supporting customer managed services, and you can have this capability both within region and across regions we launched that last year. And if we go towards application connectivity at scale, here's where Amazon VPC Lattice comes into the picture with a, uh, scalable, highly available, managed, uh, service for that, um, which allows you to connect to your service network, uh, services and resources, uh, in an, uh, secure manner using our policies. We're gonna dive a bit deeper into lattice and into uh reference architectures with this now if we step down from the application level and we go towards, uh, connectivity at the metric level, right, we have, uh, various options for you to connect your VPCs on AWS. The first one is VPC peering, and you can have this intraregion and cross region as well. VPC pairing is uh not extremely scalable, so from a routing perspective, uh, you can scale your VPC connectivity in a region using AWS Transit Gateway, which is a regional routing hub that allows you to connect many VPCs in a region up to 5000, uh, public facing quota, uh, and allows you to also, uh, pair multiple transit gateways together intra-region and also cross region. Just keep in mind that uh Transit gateway relies a lot on static routing right so it's not an intelligent um dynamic uh driven router um or routing hub in AWS. Now if we do want that dynamic global connectivity and we do want to expand across multiple regions, AWS Cloud One is here, and we actually got a question last year around, it's still not clear for me what's the difference between Cloud R and Transit gateway. That's not the big difference actually. The cloud one comes as a fully managed service that's globally, uh, extends globally. It's globally available. And that allows you to have this dynamic connectivity across segments that is a fully managed, uh, type connectivity. You don't have to manage static routes. Uh, the intelligence under the hood is attached to core network edges which are like transit gateways, right? So you can choose. Transit gateway is a DIY type service. Do it yourself. You can configure routing and static routes, whereas tra uh, cloud run is your global fully managed service, and we're gonna dive a bit deeper into it today. Now, not all workloads live in AWS, and if you folks are familiar with some of the recent launches, um, you will see that not all workloads, not just that live in, uh, on-prem, but they also live in other clouds, and we're gonna talk a bit about that from a hybrid connectivity perspective. Uh, we have site to site VPN, which is, uh, our, um, IPsec based, uh, VPN connectivity, uh, method. And Direct Connect which allows you, uh, three flavors of virtual interfaces, uh, private virtual interface, transit virtual interface, and public virtual interface, right? Now, From a remote access perspective to give remote access to your uh end clients without ADPN, right, that's the whole point, uh, you can use AWS verified access and you can integrate this with device management device posture, and, uh, identity for your end, uh, for your end workers, for your end users to access your public facing resources on AWS, including that authentication authorization piece, right. Now all of this is foundations and we've talked through them. There's been a lot of innovation in 2025. Andrew, take us away. All right, thank you. So we're gonna break off these uh these innovations in a few different sections. So the first one's gonna be Amazon VPC. So, obviously we do a lot of development work across many different parts of it. So let's start with uh internet connectivity. So we talked a little bit at the start about this, and we have our standard VPC here. We've got some EC2 instances. We've got our NAT gateways. This is how you've been configuring NAT gateways for quite some time inside AWS. We now are launching Nat Gateway in regional availability mode. What does this do for us? It makes configuration and maintenance and supporting these much simpler. So now instead of having one net gateway per AZ, which we call zonal nap, you now have this regional construct. It doesn't need to live in its own subnet. It doesn't need to do any of the additional bits, and it updates itself automatically. So if you go through and you add an additional AZ, regional gateway expands for you. It also has some additional benefits in terms of scalability, the public IPs, so this is the construct that we see people going forward with, especially as you're going through new deployments and that sort of thing. In the EU security front. We have had for quite some time here network firewall. So we've got again our typical architecture here. We've got some EC2 instances and we're pointing them up to the network firewall endpoint. If you haven't worked with network firewall before, it's based on gateway load balancer. So we have the endpoints. They go over to a network firewall object that AWS manages, and then you can send the traffic on out. Here we're showing it with regional gateway. So That's fine, but now we are supporting multiple VPC endpoints. So in the previous diagram we had everything inside one VPC, but now we can have multiple VPCs all come back to the same network firewall. Why would you do this? It makes it so you have one single network firewall policy that controls a number of VPCs, and we recommend customers doing that, but still consider, do you want to split it up by prod, by Dev, by whatever security segmentation that you want to do. And if you don't want to separate out the network firewalls, which you might not want to do if you're in a test environment and you've got thousands of these, you can still reference the individual endpoint inside your policy, so you can still get that segmentation that you may need for smaller grained usage. Network firewall itself has had a number of upgrades and improvements launched in the past year. The active threat defense is pretty interesting. This is where we are enabling some of the AWS intelligence to be brought into your network firewall, so you get to see all that. But you can also pull in partner managed rules. So this is where you can reach out to another security partner and have those rules get automatically updated into your firewall as well. Number of console and monitoring and all sorts of additional features there and feel free to go read the many, many blog posts that we've launched about network firewall on this one. The big one though is proxy, so. Network firewall proxy is exactly what it sounds like. So for the longest time, if customers wanted to have fairly Controlled access to things like external websites, they were deploying their own proxies, whether it's Squid, you know, who knows what. Manage firewall proxy is now where we provide that service for you. So how does this all come together? It looks very similar to a network firewall deployment. We now have a proxy in point. It goes to a proxy instance that gets tied with net gateway, and this is all an explicit proxy. Why do we care that it's an explicit proxy? It means you don't need to change the route tables. You're configuring the clients to use this proxy explicitly, so this makes it a much simpler configuration deployment. Of course this all works with various levels of inspection. So for those who may or may not be familiar with proxy, the client, like I said, will connect up. We can do pre-DNS inspection, which is where we process the rules against something before any kind of connection goes out. Pre-request, we can do post request. And we can also tie this in with PCA and this allows us to have it handle TLS traffic as well. So this way you have the clients trust the route certificate over on PCA and your clients can work with us as well. The overall packet flow on this one, like I mentioned, the proxy, the clients will do its proxy connection set up to the gateway, and you do that HTTP connect that's part of the proxy protocol, and there's where you send in the host that you're connecting to, and this is where we do that first step of inspection. Assuming that passes, then the proxy will make the outbound connection for you, and then your client can do the individual request, and this is where the request policy comes in. So you can do things like say, hey, you're allowed to get to here but maybe not post or whatever you want to do, or you can get slash but not a subdirectory, whatever you want to do. Assuming that passes, proxy will send the request out. We'll get the response back and we can inspect it again so you can do things like say, hey, you know, I, I don't want requests or responses with the word the in it or whatever you wanna do, um, and then assuming that passes, it all comes back to the client. This diagram does assume you've configured the TLS inspection component that we mentioned earlier using PCA. Otherwise, the traffic is encrypted and you lose a couple of those capabilities because we can't see inside the encryption. So, as before, we mentioned each VPC can have its own configuration and endpoints in a distributed environment, but much like what we just did with firewall, you can have distributed endpoints that all come back to the same proxy. So this is very useful if you have a use case where you want to keep your VPCs completely isolated. You don't want them to have any kind of attachments or internet gateways or anything else, but you still want to have just a modicum of external access to get to things like package repositories or update repositories or things along those lines. You can of course make this part of a hybrid environment where you have cloud WAN or Transit gateway providing your interconnectivity services and then you put the proxy firewall in point in a centralized VPC. We see customers using this if you're in a position where you already have a central VPC where you're putting all your networking resources or you just want to keep the client VPCs as clean and simple as you possibly can and let all the messy networking bit happen elsewhere. Next bit is VPC route server. This one is an interesting launch, and what it gives is the ability for instances inside your VPC to make BGP based routing updates to your VPC routing table. Why would you do that? You can do a number of different things with this. Uh, we've seen customers doing any kind of any casting or making their own failovers or anything along those lines with that. Um, this plays in with a couple of the other features that we'll talk about here in a little bit, but it's kind of an interesting service that fits a nice little niche. One of the use cases that I mentioned is the floating IP. So in this particular case, we've got 2 EC2 instances active standby. We're all networking folks. We love active standby. Well, we tolerate active standby. And in this particular case, we've got an IP address that is not part of our VPC, but it is still being routed up to our active instance. Our active instance goes away and we can through route server push out that update and send all the traffic over to our other instance. So we've talked a lot about some of these interconnects and just to come back to this Transit gateway V4 V6, this has been our deployment model for quite some time. You've got a central Transit gateway roundtable. You're sending all your traffic to the various attachments. That's just been the way we've fund things forever. But it does make things a little complicated when you want to start adding in firewalls to it. So here we have two VPCs that are connected together by transit gateway. And we want to add in inspection. Well, the way we've had to do that in the past is we create a new VPC, a security VPC. We attach that to the transit gateway. We set up some route tables, we set up some endpoints, we set up some ENIs, we set up some more route tables, all that good stuff. You don't need to do that anymore. Now you can do network firewall as a native attachment to Transit gateway. This spares you the time and effort to maintain, build, and deploy that separate inspection VPC. You can just tie to network firewall directly and we handle all the bits and pieces inside for you. Next up is on Transit gateway. One of the problems that we frequently have heard from customers is Transit gateways frequently on a sender pays model that works for a lot of cases but not in all of them. So now with Transit gateway you can do flexible cost allocation. What does that mean? So you can do things where the traffic flows that traffic or Transit gateway is doing the data processing for can be billed either to the source, the destination, the transit gateway owner, and this can potentially help settle things like if you're having to do back charging between different departments or anything along those lines where you're trying to understand the true cost of a workload, you can then put all the data processing charges assigned to that workload to make that math easier. VPC security. Alex talked a little bit about this at the start, but we're back here with our private subnet, public subnet. We've got NAles. We've had NAL for a very long time. We've got security groups. So NAles or network access control lists are they're primitive there. They're on the subnet boundary, but they're rather limited in terms of you can only have 40 rules. That's where security groups come in at. Security groups now work or always have worked on an individual ENI basis and allow you up to 1000 rules. So customers use this for application or micro segmentation, any number of reasons. So one thing that we talk about a little bit, and I hit the next, there we go, is security group referencing. So security group referencing, if you haven't used this before, is a feature where you can say instances that are assigned to one security group get referenced in another one. So we recommend customers do this, for example, if you've got your database servers, you can say, hey, everything that's in the security group, that's my web front end is allowed in on this port. That way you don't need to go through and update additional security lists or what have you. It's all propagated automatically through AWS. The other question that we get a lot is encryption in transit. A lot of customers are worried about, hey, what, what is going on actually inside AWS? Is my data in the clear? Is it not? And we're very open about that. You can go to our website, you can read all the policy documents, the standards, all that good stuff. But for here, we're going to simplify this down and focus purely on our instance to instance, and here we've generally said, hey, we've got AWS Nitro. Nitro handles encryption between nitro instances. Great, perfect. This does work in the same region, cross VPC peering, even better. But the problem comes in when we start talking about other things. So, for example, what happens if we have non-nitro instances? What happens with transit gateway? What happens with everything else and load balancers? Now we are giving you a control called VPC encryption controls. This one is really, really nice from a few different aspects. So with VPC encryption controls, what you can do is you can put your VPC into two different modes. There is a monitor and an enforcement mode. So in monitor mode you can turn this on whenever you'd like and inside your VPC, if you go look at the console, it'll tell you everything that's nonconformant, and you can sit there and mitigate. You can say, hey, I'm going to make an exception because this is an internet gateway. It doesn't make sense for it to have the transit encryption or I understand this or what have you. But that monitor mode gives you a chance to dry run it first. In addition, inside your VPC flow logs, we now add in an additional field. We tell you if it's nitro encrypted, if it's application encrypted, or neither, or both. So monitor mode's great, but then as you go through and you mitigate everything, then you can turn on enforcement mode. And what enforcement mode does is it ensures that you can't launch anything inside your VPC that does not support encryption. Fantastic. Makes it so we don't have to worry about these types of questions any longer, like, oh, is this path encrypted? Is this one not? It's all encrypted at that point. Cool, so I've talked a lot about networking at this point. So you wanna talk about applications? For sure, for sure, and um, with application networking on AWS, application owners and network owners and security owners have started to come closer and closer together. Now part of the application, uh, networking suite on AWS are a few services we're gonna start with AWS Private Li and connectivity to AWS managed services. We did go a bit through this at the beginning, uh, where you have private link endpoints, uh, on in the form of gateway endpoints or interface endpoints, uh, and connectivity, uh, to AWS services translates into a DNS change in your, uh, VPC on the Route 53 resolver. Now, uh, from, uh, the gateway endpoints perspective, these don't require any DNS changes. It's only a route in the route table, right. Now very exciting launch. Uh, we've worked with a lot of you with feedback from last year's launch around, uh, support for cross region privately for custom services and now we support cross region, uh, privately for AWS managed services. How does this work? Well, very simple, straightforward, and exactly as you would expect it. Um, you can now create interface endpoints in your VPC in a different region, uh, for AWS services that are, uh, in across regions. You have full control over which services you allow, uh, to be created across regions, and, uh, you can control that at the AWS organization level. Very important. Now the other thing that I'm very, very excited about and some of you have already mentioned, uh, that you're gonna receive stickers only if you adopt IPV6, that's not true, um, it's comprehensive IPV6 support, and we've launched IPV6 support for gateway endpoints for, uh, S3 and Dynamo DB as well as for interface endpoints. So there's no more excuse to not adopt IPV 6, now. The second service in our application networking stock is uh Amazon VPC Labs, and this is purpose built for internal application to application connectivity on AWS and I wanna go through a bit of history here, right? You as developers as application owners, uh, you have your services, right? You have your applications that are deployed on certain compute types. And you can have a bunch of uh various services in your environment, right? We didn't actually use those services, by the way they're just made up we didn't have a lambda function generate our stickers. In addition to services, you can also have data sources, resources, TCP applications, right? And the idea is from your perspective as a developer what you want is certain things to talk to each other, certain things not to talk to each other. Mostly security wants certain things not to talk to each other, right? These applications that you own are hosted either in VPCs or if you run LAMDA, uh, you can, uh, you can host it in LAMDA, not necessarily to have it in a VPC. VPCs are your network level boundaries, but they don't help you that much express that intent for, hey, A needs to talk to B and C needs to talk to D. So here's where Amazon VPC lattice comes in. In VPC lattice you have a couple of constructs that I'm hoping everyone is already familiar with by now. You have EPC lattice services which are your HTTP HTTPS type applications, um, so layer seven application layer. Uh, and you have VPC Lattice resources which are your TCP applications, TCP applications that could be databases, for example, but could also be just simply applications that listen on ATCP board that are not HTP level applications. Now I talked a bit about intent and I talked a bit about saying, OK, how do we bring these things together to talk to each other, right? This is what the VPC lattice service network comes to offer, right? It's a logical boundary that helps you bring together things that need to talk to each other. A super common question that I'm getting is, should I have just one lattice service network? The answer is no. You can have as many as you want. Just design them in uh in a way that makes sense for you, keeping in mind that principle things that need to talk to each other. And how do you express that intent in Lattice? Well, through associations. Everything in Lattice is an association from this perspective, right? And you can, uh, uh, you can associate, uh, services to a service network, right? So you have service associations. You can associate resources to a service network. So you have resource associations. You can associate VPCs to a service network, and you can associate service network end points to a service network, right. Uh, if you go into the console, you'll actually see all of these names as associations. Now these two types of associations for VPCs or service network endpoints and for services and resources give you the path to either be consumed if you're a service or a resource or to consume things right if you are a client in a VPC. Now really interesting and important, uh, are the off policies in VPC Latices which are IEM based and allow you to control granularly exactly that intent, right, to say A should talk to B but B should not talk to C right. These are baselines and at the end of the day from a developer perspective this is what you need, right? You need the ability to have your applications talk to each other. If we are to dive a bit deeper, uh, and to look at some of our services here from a from a client service perspective. Um, The whole communication path starts with a DNS request, right? You as a client service or the client service is trying to access a service by doing a DNS query, and the answer to that is VPC lattice IPs. That's what draws traffic to the service network, right, instead of routing it to, I don't know, your VPC peering or transit gateway or cloud one, right. And in order for this to happen you have to manage DNS, right? So you have those private hosted zones associated with your, uh, VPC, uh, that tell you those custom names and how they're mapped to the VPC lattice generated FQDNs. But very exciting launch to to simplify how DNS management works. uh, we now support custom DNS for resources, which means that when you create a resource, right, you can now define a custom DNS, uh, for that resource, and it means that you don't have to manage private DNS anymore, private hosted zones associated to client VPCs, right? So let's take for example my resource here new features update for reinvent 2025. I've defined this as a custom domain name and now I have the level of control both at the resource association to the service network, at the service network level, and at the client level to propagate that DNS private hosted zone right into the client EPC. So from the client perspective there's nothing that I need to configure anymore. Uh, on the VPC for DNS management, it all works and keep in mind for those of you who are security conscious and uh need to control how these private hosted zones are propagated that, uh, the, um, service network level DNS, uh, manipulation and the VPC client VPC are always under your control as the owner of those resources, right. Cool. Uh, and a second, uh, very, very important improvement in the PC lattice is around configurable, uh, IP addresses for the resource gateways. By default, your resource gateways would take a 28. 0 sorry for that. Um, a 1/28 per availability zone to use for traffic to your resources. Now you can specify how many IP addresses those resource gateways have and um what are those IP addresses. Let's go towards advanced architectures because these are constructs that uh all of you have been familiar with for for quite some time and uh here are some of the uh most interesting ones. First, when do you use SNA versus when do you use SNE SNA Service network association SNE service network endpoint. The difference between them and when you would use them is exactly like the difference between a uh an S3 interface endpoint and an S3 gateway endpoint. The gateway endpoint cannot be accessed from outside of the VPC, same as the Service Network Association. The service network endpoint can be accessed outside of the VPC, so that's when you would make the decision of using that. Now the second one is providing connectivity to applications that are on premises, right? So your applications are on AWS and you want to bring into lot of some applications. How do you do that? Super common scenario is to have a transit VPC, uh, for those of you who've been, uh, around, uh, before 2018 when Transit Gateway was, uh, was created and launched, uh, it's not that transit VPC. There's no VGP here that you need to manage. Uh, it is a transit VPC in the sense that it has IP addresses that are routed towards on-prem, right, so they're non overlapping. Everything to the left of that VPC can be overlapping, right, because lattice accommodates for overlapping IP addresses. So you have to have that transit VPC with a resource gateway that allows you to target these resources on print. How do you bring in, bring them into the service network by means of resource configurations. And on resource configurations we support uh custom DNS names. Hence you can uh give them your custom names that are uh used by your developers for your on prem resources. Now if we are to uh to dive a bit deeper into the other flow, right, you may ask well how do I do the other, the other way around, right? I have a client on prem. How do I get, uh, it to be able to talk to these resources or services that I have on AWS? Well, the service network endpoint provides you with an FQDN for every single resource or service that you have associated with your service network. And if you take that FQDN and you map it into your uh DNS configuration on-prem, um, you could have an inbound resolver endpoint if you wanted to in that VPC or you could have, uh, I don't know, hosted zone delegation to your towards your on-prem DNS servers. Um, you can absolutely make make sure that that DNS name is used by your clients on prem and everyone talks to that service network endpoint. And also keep in mind that both on service network associations and service network endpoints you can configure um security groups right so you can filter what traffic can get to the service network endpoint. Another interesting one is that this surface network endpoint is a layer 3 construct, right? So it has DNIs in your VPC. A super common question we get is, can I put a firewall in front of that? Yes, you can absolutely do that if you want to, but keep in mind that you have the off policies in Latice, right? So make use of those. There's no data processing cost for off policies. There's no hourly cost for off policies, so make use of them. And most importantly, do you need to use SIGV4? So do you need to use IM rolls inside your traffic to be able to use off policies? The answer is no. You can use off policies even if you're not signing your traffic. You won't have the principal ID as a condition key in the off policy, but you can still have condition keys like source IP, right, or source VPC or source account. So keep that in mind. Now another interesting architecture that we've been hearing about a lot and customers have been trying to, uh, to build it is cross-re service to service communication, right? Clattice doesn't currently support native cross-region VP uh services or VPCs or resource attachments, so you do need to have this transit VPC, uh, that brings you layer three connectivity between the regions. And these are the only two VPCs, or if you have many regions, one VPC per region that has to have non-overlapping IP space because these are the, uh, ones that connect to each other in your layer three domain. Now in each region you have the service network endpoint which gives you layer 3 ingress into the service network, and in the opposite uh region you have resource configurations right that are targeting the services or resources in that remote service network that you wanna bring into your region, right? And you can use custom DNS to have everything managed from a DNS perspective. Centralizing private link endpoints, how many of you are doing that today with Transit Gateway? Probably a lot. OK, you can absolutely do that with, uh, VPC lattice, and you can actually have DNS managed for that for all your client VPCs because of, uh, custom DNS, uh, on resources. So every endpoint is actually a resource in VPC lattice. Now How many of you folks are consuming SAS provider end points? Probably a lot of you, uh, right, can you centralize SAS provider access? You can absolutely do that, right? You can have resources that point to VPC end points for SAS providers that you want to offer to your, uh, your clients. And very, very important, can you inspect that traffic, right? Can you inspect the traffic that's going towards your SAS providers? And can you do traffic filtering? The answer is absolutely yes, you can. The resource gateway is a layer 3 construct in your VPC that has elastic network interfaces. So if you put a network firewall, uh, in there and you configure routing appropriately. Your your traffic will be inspected through the firewall right from the resource gateway towards the uh VPC endpoints for your SAS. Pretty, uh, pretty cool architecture and, uh, it's actually used by, uh, by many of you. Now from a seamless integration perspective, uh, I don't know how many of you know, but, uh, Oracle databases at AWBS does, uh, rely on VPC lattice for, uh, for exposing services, uh, and resources to your Oracle database deployments. So that's a cool one, but not everything is about service to service connectivity. So Andrew, elastic love balancing, sure. So what was mentioned earlier, where we have the different load balancers, application load balancer is one that handles all of our layer 7 work, so HEV HTVS, and it has had a number of new launches in the past year. So out of all of these, the ones I like the most are the target optimizer because this has been a frequent request from customers to say, hey, I want my ALB to actually load balance based on the number of concurrent requests. So you may have some requests that are short, some that are long. ALB will now manage between those for you and help keep that load more evenly distributed in those kind of workloads where you've got different response times. The other one, the health check logs, this has been a very frequent ask to dump what is ALB seeing in terms of all your clients. Now you can store that in an S3 bucket, and that is actually free of charge besides the S3 bucket fees, so that's something you can go in and just turn on and then you keep an audit. What happened to each of my targets, what was the response code, all of that, so you can go back and see is everything doing well, did I have problems, that sort of thing. ALB has also introduced some post quantum uh key management so you'll see that is fairly frequent now across a lot of different AWS services, but it's worth mentioning here we always say that we will be ready for post quantum before you folks need to think about it so this is. This is uh this is that. The other load bouncer that we tend to talk about in the same breath is network load bouncer. This is our TCP slash UDP1, and you'll notice there's some similarities here. So access logs can now be vended out. The one I like here though are the weighted target groups. So the use case for this one is you can do things like, hey, I've got a bunch of fairly powerful instances that cover my baseload, but then I want to come up with very, very small ones in my auto scaling groups so you can assign the weights on those lower and have NLB properly manage that. Again, we've got some post quantum stuff here that everybody likes to see, but the bottom one is really interesting as well. NLB now supports Quick, which is sometimes called HTTP-3 or some different acronyms, uh, in pass through mode. So what this does is this leverages an IETF draft that where you encode the server ID inside the Quick connection ID and NLB will use that to guard its or make its sticky session. Decision Amazon API gateway has also gained a very interesting feature called the developer portal. Developer portal basically does automated work to pull in all your various APIs, your services, and put them together in a portal for you. Previously you would have to do this by hand or create your own automation or however you want to do it, but now this is a fully managed solution. It pulls in your APIs. It can do, and it's very, very customizable. You can go in and play with the. Colors or names or certificates or whatever you wanna do. And you can also put in the access control that you've come to expect from most AWS services backed by our normal identity services. From API gateway, we have a couple of other bits and pieces here. The response streaming. This has been an ass in order to improve the time to first bite. That's what TTFB stands for on there, where we've also extended the response timeouts to 15 minutes, payloads larger than 10 megabytes. Uh, people seem to love using API gateway for very, very large things, so this, uh, got implemented. More TLS security policies you can now integrate API gateway with private ALBs, so this helps a lot from a security perspective and. You, you, you promised no AI mention, so I said, I said we're not gonna talk about AI, but you have to mention this, uh, Agent core. There's, there's MCP support in API gateway. We're, we're just. So, let's get out of that and let's talk about global and hybrid connectivity. And as we've talked about a couple of times, the first one is gonna be Cloud WAN. So Cloud WAN, we mentioned earlier, all the foundations, you start with the core network, which is the container for everything, and then you expand that into each of the regions that you're interested in, and Cloud WAN deploys a core network edge. Once you've done that, you can see this all in a JSON policy file, and we're going to keep coming back to this concept because everything in Cloud WA is defined in a JSON policy file. This is great from an auditing perspective, from seeing what the differences are. You can do policy rollbacks inside the UI. And what we found is especially security folks love having a single document that says, hey, these behaviors are what's being actually implemented. You can put some human names in places, things like that, and your entire policy is in one place, very easy to audit, very easy to control, very easy to understand. So as part of that, the next item inside Cloudland are network segments. Network segments you can define to be anything. You can, we've got customers that do it by security zones, by regions, um, some customers are doing it for where they have different policy domains or different legislation that they have to obey, things along those, but it's completely up to you. Cloudland supports a number of these segments, so you know, feel free, you know, we've got customers that are kind of doing every combination you can imagine. What connects into a network segment? Well, you can connect lots of things into a segment. The first one, of course, is going to be VPCs. VPCs attach into a segment, and that is controlled by a tag that you put on the attachment. So again, easy to audit control. You can be assured as to where that is going. So now if you start doing this multi-region, this is where Cloud One's other big feature comes in. It is global, so the same policy applies worldwide. You don't have to worry about any additional replication, anything like that. It's all global. So your VPCs here, you've got the same policy here. We're saying, hey, you know, some of these VPCs attached to segment A, some segment B, they get the same behavior worldwide without you doing anything additional. You can also connect in what we call tunnel-less connect. So we've had a number of features in the past that were tunneled in one way, shape or form, whether it's IPsec or GRE tunnels or what have you, in order to do dynamic routing. You no longer need to do that with Cloud WAN. You can create just instances that are speaking BGP, the cloud WAN, and control the route tables that way. Again, you can use this for things like, you know, uh, different services. I've seen customers using this for logging and other kinds of failover. The sky's kind of the limit here, but it's been a lot of interesting use cases that have come up from this. Then the next thing that you can add in to cloud one is transit gateway. And one of the biggest questions I usually get is does transit gateway have to go into one segment? No, it is determined by the transit gateway route table, not the transit gateway as a whole. You put in an entry in the route table, you say go to the segment. So this means that you can still maintain some control at your transit gateways, whether you're migrating to cloud one or you're adding it in, you're using cloud one for global, transit for local. Use whatever makes sense for you. But this capability makes it so you don't have to worry about, oh, do I have to dump my entire transit gateway into one segment? No. The next thing that you can drop into a segment is of course a direct connect gateway. So this is where obviously your DX links come in and your external connectivity, and this comes in as yet another attachment. I will say I do see customers frequently creating their own segment for DX. Call it external, call it on-prem, call it what you will. Uh, that is very, very common. But again, you gain BGP routing capabilities, so BGP routes get advertised in via your direct connect links, you can work with them inside cloud land. You can of course connect in multiple direct connect gateways, and customers will do this if they've got things like they want to advertise the same blocks or they wanna go, you know, there's a few other advanced architectures where this may come in at. A little more common though is where you have multiple direct connect gateways tied into multiple segments. So this would happen if you've got, for example, a prod and a dev on prem and you want to tie them in their respective segments inside cloudwa. Cloud WAN now supports security group referencing, so we mentioned that earlier. It now works inside Cloud WAN as well. So inside Cloud WAN, you just toggle a switch, it says, hey, you're allowed to do these references, and like I mentioned before, you can have a security group that controls your instances in VPCA and you can reference it in uh what we're calling Security Group B here. Like I said, my use case is microsegmentation, but it's also very, very helpful for if you've got security folks that are like, Oh, I don't want you opening 10/8 or, you know, giant blocks. Now you can say, OK, only the instances that are assigned to the security group can access this. Very helpful from a security perspective. So this is the feature I'm personally most excited about in terms of cloud line. I came from native routing. I worked with routers for a very long time, so advanced routing controls really kind of scratches some of those issues that we've had from customers saying, oh, this is all great, but I need more control. I need more capabilities. I need to be able to do really fine-grained controls with this. So advanced round controls gain you a lot of features. You can do route filtering now. So you can say, hey, you know, this VPC doesn't need these prefixes, or I don't even want this space routed to this VPC. I can, you can do that. You can do, you know, between segments, between this and on-prem. You can configure summarization. You can configure path manipulation, and we'll go through an example of that here in a little bit. But if you're used to your normal routers, your physical devices, this is very similar to route policies or route maps or whatever you wanna call them from your vendor of choice. You now gain a lot of those same capabilities inside Cloud Win. What does that look like? So inside the console here and we're going through the console, but remember you can do it in JSON as well. You create a routing policy. And it'll ask you a few very straightforward things. What assign it a number, assign it a name, is this gonna be inbound or outbound? All fairly straightforward questions. Here we're just defining it as 100 and then once that's done it'll show up inside the console and now you'll be able to add rules. So inside the rules you can sit here and click create a policy rule and this is where this gets pretty interesting are the various rule components. So again, if you're used to route policies, you'll see your normal hey condition, what am I doing with it? OK, fair enough. So what else can we do here? There are a lot of actions you can take. We can manipulate, we can block. We can allow, we can manipulate AS paths. We can do, well, you all can see the entire list up there. So it's kind of your normal set of things that you're used to seeing from traditional routers, but now it's all being brought into cloud. What are the conditions? Again, kind of the same thing you're used to, you know, you can match prefixes, you can match AS paths, you can match meds, you can do all this sort of manipulation here, and now you have the capabilities to do. So many things determine, you know, where are my primary and backups going for remote regions, for example, oh, I don't want paths from this internal AS to come into cloud WA or I only want these or whatever you wanna do. This feature has a lot of interest from the networking engineers that are been used to that and been kind of screaming at us like we need more controls. Well, here you go. So And there's the real example. And like I mentioned, it's all in JSON. So again, the UI will help you create it, but again, once it's in JSON, you can sit here, you can copy and paste, make it a lot easier to make multiples of these. But remember what I said earlier. Cloud one gives you the ability to roll back policy versions. So if you make a change you don't like, you just tell it to roll back to the other one and it changes everything back for you. You don't have to worry about trying to undo things. You don't have to do any of that. You just go back to where you were and everything's good and you can try again. So The other part of this is where the routing policies actually attach at. Yeah, there we go. So inside each attachment is a Sorry, the clicker is acting up a little bit on me there. You define policy tags inside there, uh, you, you also select what region it is available, and you configure the routing label here. So this label here is just another kind of internal name that you're going to be associating. With You create the network policy there. And now you can go through and apply the routing policy label onto the attachments. So this is that final interconnect piece where you're tying in the policies to specific attachments. Once you apply that label, of course, we will tell you, OK, hey, you applied this label, here's the policy that's getting applied, just give you that positive confirmation that it's doing what you expect it to do. So that's great, but how do I know what all Cloud WAN is seeing and getting? Uh, by very frequent and vocal request, we are now giving the rib view. So you can now see all the routes that Cloud WN is learning, where it's learning them from, all the various information, the AS pass, all that other good stuff, uh, that you can now see inside Cloud WAN, including all your backups. So a couple of low minders about the advanced routing controls. There's a few things where you know it doesn't make sense for some of these things to work on other ones, but generally speaking, this works pretty broadly and it gives you the capabilities that you've been looking for for some of these really fine-grained routing decisions. So I've talked a lot about internal routing. Alex, go for a hybrid, right? And we have site to side VPN and a number of very cool launches about it. First one is high throughput tunnels. So now you can have up to 5 gigabits per second tunnels, and you can do ECMP on transit gateway and cloud one. The Ero partnership was just launched where you can have eero devices on your own premises and VPN concentrator for a large scale of small, uh, small throughput VPN tunnels. Really, really important for, uh, folks using site to side VPN and for Direct Connect, um, we've talked a bit about at the beginning around use cases for Direct Connect and how customers, uh, were, uh, are using it for connecting to their hybrid workloads. Very, very, uh, cool launch, uh, that happened on Sunday is AWS Interconnect multi-cloud. There's also another flavor of AWS Interconnect which is called Last Mile. It's a gated, uh, preview with Lumen, so we're really excited about that as well. AWS Interconnect Multi-Cloud is launched in partnership with uh Google, and we will have other cloud providers coming in 2026. The idea is how do we provide private connectivity between AWS and your environments and other clouds. Well, before you had to go, uh, cable all those things up you had to go build your high availability and resilience using either your own router or customer router. Now you just go through the console and it's just a matter of clicks or through automation, by the way, not click ups, right? Uh, let's see how that works. Everything is taken care of for you under the hood. ABS and, uh, Google Cloud, uh, have cabled large amounts of capacity, and what you get is essentially an interconnect that you get to attach to your Google Cloud router on the, uh, Google Cloud side and to the direct connect gateway on the left side. There's a lot of um deep dives and uh reference architectures that we've gone through uh in terms of regional versus global level construct with Cloud One. There's a dedicated session for this rein event that happened this morning, so you're gonna find it on uh on YouTube, um, in about 48 hours. The most important thing here is the collaboration between, uh, between the cloud, so. You interact with each of the clouds. You create an interconnect. AWS and Google Cloud, uh, provision that capacity, and everything is fully managed. Then you go and you accept the interconnect using the activation key that you're provided, and that's that. That's all you need to, uh, you need to do under the hood. Uh, it is highly available, highly scalable, encrypted, um, connections between, uh, the two cloud providers. There's no third party in between. Uh, and we're diving, uh, quite deep in, uh, into, into this in, uh, uh, in the other session that I mentioned. Now, uh, keep in mind that for preview you get for free a one gig connection, uh, when it is going to be GA, uh, you folks are, uh, going to be able to scale up and down capacity to whatever, uh, numbers you want. Uh, and I mentioned that um other call providers are gonna be coming in 2026. We are working closely with Azure on uh, on that. Uh, from an advanced architecture perspective, you remember this, right, re, uh, cloud one, or some sort of layer 3 connectivity with VPC lattice, uh, and your resources living on-prem, your resources could also be living in another cloud, and now through interconnect you have layer 3 connectivity. Now, a couple of um super cool launches that we're gonna breeze through DNS Global Resolver uh from uh from Amazon RA 53 uh this is a launch that just happened I think on Tuesday. Uh, we've launched it. It allows you to not have to manage, uh, inbound resolver endpoints for your workloads and DNS delegation and so on. It gives you any cast IPs and IPV4 and IPV6 that you can configure on your workforce, uh, fleet on your on-prem devices, and, uh, they're fully controlled, fully managed, and secure. So give it a try. From Adina and content delivery perspective, Amazon Cloudfront now supports Mutual TLS for clients. It was a feature that was highly asked, uh, from our customers, and we've released flat rate pricing without overages for a set of, um, services that customers use for content delivery and website and security and content delivery. So we've created these bundles that you can find on the website. And they are fully, uh, fully managed in a, in a, in this bundle, and you get different tiers right from, um, I know, basic to pro, uh, you, you get to pay a monthly fee without averages as I mentioned. From an IP address, uh, management perspective, IPA had a lot of cool launches, uh, this year. One of the ones that I'm most excited about is, uh, the public IPV4 allocation policies. No, it's not IPV6, but still, um, here you can integrate with prefix lists, and you have the ability of specifying certain IPA pools for certain use cases that you want those IP addresses to be allocated. And you can automatically populate prefix lists, for example, to get you, uh, started with IP allow listing for your partners, right, for example, and we have a blog published describing that solution so I would highly recommend you folks go through it. And last but not least, IPV6 adoption. I wanted to call out these super cool stats. Over 75% of AWS services now support IPV6. We are on track to get, uh, to a higher percentage by the end of this year, so stay tuned. Um, and 100+ services launched IPV6, uh, support just this year. So if you had an excuse around, hey, AWS doesn't support IPV6, so that's why I'm not doing IPV6, now is the time to do it, right? Really, really exciting. And for those of you who know how we end, uh, this, uh, this presentation is usually with recapping all the things that we've launched in, uh, throughout the year in AWBS networking, right. I don't know if you folks have looked at the session last year, but this is how we ended 2024, right? Uh, these were the features and launches all on a big slide, uh, that were, uh, were there for, for you folks to keep, uh, to, to take away. Now in 2025 we've launched over 150 features and integrations in the AWS networking space. So are you all curious how this diagram looks in, uh, in December 2025? Well I am too. This is how it looks like um you have a task spot the differences and let us know. Uh, and, uh, just to call out a few of the ones that we've gone through today, uh, network firewall, proxy, VPC encryption controls, cloud one advanced routing policies, um, security group referencing on cloud one, VPC lattice, um, support for custom DNS names and configurable IP addresses on the resource gateways. Um, Route 53, uh, global resolver, um, bundled pricing for, uh, web, uh, and content delivery. I don't even know. Did I, did I capture everything? I, I, I, you said 150 earlier, so you know, because there's 150 plus. Cool. Well, um, that was it. Thank you so much, folks. Um, thank you for sticking around with us. Please do fill in the session survey and please do meet us outside for stickers.