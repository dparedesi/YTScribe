---
video_id: HBzxf6-7GhQ
video_url: https://www.youtube.com/watch?v=HBzxf6-7GhQ
is_generated: False
is_translatable: True
---

Good morning everyone. I'm Brittany Hurst, and I lead the global AWS relationship with Amazon devices and Services. Joining me today are two of my customers, Lou Tran and Sairu Pauguti, two people who were instrumental in the rearchitecting and rebuilding of Alexa, the voice assistant we all know and love, and transforming it into the generative AI powered Alexa Plus. Over the next 45 minutes, we're gonna take you behind the scenes of one of the most challenging engineering problems that we've ever tackled. How do you evolve a voice assistant serving 600 million+ customers from scripted commands to natural conversation without breaking a single existing integration? This isn't a story about building something from scratch. This is about transforming a massive production system that customers depend on every day while maintaining the reliability that they expect. And adding conversational capabilities that they demand in this new world. Here's what we'll cover today. Cy's gonna walk us through Alexa's evolution and a number of the challenges that we face, the customer learnings that we had over having this technology available to customers for the last decade. And then Lou is going to outline our design considerations and the techniques that we use to rearchitect it with generative AI and large language models behind the scenes. You're gonna walk away with lessons learned, battle tested, and hopefully things that you can use in your own projects. Thank you, Brittany. How are you all doing today? Coffee still kicking in, it sounds like. Uh, how many people remember the first Alexa device? We're gonna talk a little bit about the background uh of where we started and I'm Sa Rupangri. I lead the product team for Alexa AI. We first launched Alexa in 2014. Alexa users? All right. At that time, we had about 13 skills, about a handful of things that Alexa could do, all from one developer, Amazon, and that was only in the US when we started. But these were things that were delightful for customers. You wanted to play music while cleaning the house, just ask Alexa. You don't need to get your hands free. You wanted to convert that ounces to grams while you're baking and making dough. You didn't need to clean your hands. And you're feeling a little lazy getting off the couch to turn on the lights, just ask Alexa. Now, all kidding aside, that last bid has been invaluable for our customers living with disabilities. So, customers loved what they were seeing. This, however, remember it's a decade, more than a decade ago at this point. Came with significant challenges, technical challenges at that time. You had to pick up voices from across the room. If anybody remembers that Pringle scan, that blue light would follow you around. That was to make sure that we were getting the right signals from the right places. Once you actually had that, you had to understand customer's intent. Then we actually had to go get that information for the customer and execute the actions. Now, all of this is through voice, so it needed to happen within like a second or two. Otherwise, you're not gonna wait for answers, well, you know, no awkward pauses, it's, it's, it's a difficult thing to uh deal with. Fast forward to today, Alexa has over 600 million customers and devices, I should say. And we work with developers across the world. Alexa is the assistant with the most products and services connected to a billion plus devices. But then, customers still had to work with uh a, a machine. They felt like they were talking to a machine, if you will. Alexa speak. Yes I called in. You had to phrase things a certain way for Alexa to respond properly, much like you had to do with search at the time. Gen AI promised to break through that barrier for us. And how we took Alexa from what it was. To where it, where we wanted it to be is the story that we're gonna talk about. With 600 million devices though. We had many challenges that we had to overcome. We wanted to get, use Gen AI for things that would take Alexa to the next step with Alexa Plus. What that meant was that it had to be more conversational. It had to be much smarter. When you come in in here and say it looks dark in here, most humans would get up and try to find a light switch. If you tell that to LLM right now, it'll go through its thoughts telling you, I think this person thinks it's dark in here. What do I do? Maybe we should look for lights around here and then finally get to the lights and try to turn on a light only to find out it's hallucinating, that light that doesn't exist in her home. We can't have that. Besides these challenges, we also had to make sure that the things that customers had already been using Alexa for, dropping in at home, checking in on your grandma. And how she's doing Using your smart home routines that you've set up and use every day from the morning. All of these had to work seamlessly. Let's take a look at what we had to do to get Alexa to where it is. With Alexa Plus. Wow wow, look at my style. I know you ain't seen it like this in a while. Oh hey there, so we can just like talk now. I'm all ears figuratively speaking. Oh, do you know how to manage my kids' schedules? I noticed a birthday party conflicts with picking up grandma at the airport. Want me to book her a ride? Billie Eilish is in town soon. No way. I can share when tickets are available in your city. Yes, please. Got any spring break ideas? Somewhere not too far, only if there's a beach and nice weather. Santa Barbara is great for everyone. I found a restaurant downtown I think you'd like. What is Santa Barbara known for? It has great upscale shops and oceanfront dining. Can you go whale watching? Absolutely. Want me to book a catamaran tour? What's the next step? Remove the nut holding the cartridge. Should I get bangs? You might only love them for a little while. You're probably right. Make a slideshow of baby tea nuts. Mom, what part am I looking for again? 2-inch washers. Your Uber is 2 minutes away. For real. Wait, did someone let the dog out today? I checked the cameras, and yes, in fact, Mozart was just out. Wow. Pretty awesome, isn't it? How many people have actually tried Alexa Plus here? We'll, that's smaller than I expected. Well, we hope we all, you all will try it pretty soon. Alexa Plus is a step change in every way that we can think of. We have to make it more conversational. When you talk to it, it must feel like you're talking to a human. You can, you can talk about whether it's dark inside and it would understand what you actually meant. It had to be smarter. If you are planning a vacation, Alexa will go find what's the weather there. It might recommend what, what you should wear there or if there's any weather advisories that you need to take care of. And Alexa Plus is very personalized. The more you tell Alexa about the things in your household, the more you can work with it. The more smarter it becomes about what you have in your household, the better it works for you. If you want to shop for a Christmas gifts, Alexa will help brainstorm gifts with you and ship them to you as well. But the most important thing that we think with that we had to do with Alexa Plus was just get things done. Yes, you can plan and brainstorm that vacation all you want, but you also had to book. That vacation. You had to book experiences in that vacation. This is quite often where a lot of other what you see out there stumbles. Once you actually try to make reservations, make Uh, set up experiences or book things or do things in the real world, things start to fall apart. You saw an example, uh, about asking, um, one of, on the video about asking if someone let the dog out. Let's talk about a real example in this case. We have a dog named Daisy. She's a cream golden retriever. And we, when we go to work, we have someone drop by to feed her. I wanted to make sure that she's fed every day, so I have a ring cam pointing in the vicinity of, of the football. All I had to do was ask Alexa to let me know if Daisy is not fed by noon every day. And that's it. I get a notification. Any day that this person that or people that we've requested to help us out doesn't come in by noon. Think about the things that have to go in to make that happen. Alexa first has to understand. What I said Just recognizing the speech itself. It then has to understand. The context of my ask. It has to recognize that we have a golden retriever. That means it has to bring up the personal context. It then has to figure out what I meant by let me know if she doesn't eat, uh, or if she's not fed, which means it's looking, it's gonna go look for ring cameras that, that I have. So the devices, the context of what I have in the household also needs to be ready for the, for the element to respond. It then has to take action based on what based on recognizing that it has to look for a cream golden retriever that is eating during and has eaten anytime before 12 every day and has to do that every day. And once it, if it does not see. A dog eating by that time, it has to send out a notification to me. These are hard things to do for LLMs these days. To do this, we had to push the boundaries of accuracy on LLMs. We had to push the boundaries of latency. Remember, all of that that is done in the background, it still has to come back and acknowledge me, you know, in a couple of seconds, right, max. Otherwise you lose context. We had to push the boundaries of latency, and for this, we had to actually dig through how we best use the right model at the right time. And to talk through the challenges and, and how we work through them, I wanna bring Lou on stage. Thank you, Cy. Good morning, everyone. My name is Liu Tran. I'm one of the engineers who worked on Alexa Plus. And I'm gonna talk to you about 4 of the challenges that we came across, just 4. I guarantee you there were a lot more. Uh, but I think these 4 are gonna be the most relevant to all of you because anyone building a Gen AI application on top of LMs these days, I think are gonna run into these issues. First is accuracy. And this, this is about getting the LLM to do what we want it to do. Second is around latency. And that's about the user perceived latency. There are lots of elements of slowness in the system. But as anyone who's worked on LMs will know, the big hitter in terms of time-consuming processing is the inference cycle of the LM at runtime, handling customer requests. And third is determinism. Uh, obviously, an LM is inherently non-deterministic. So when you have a use case like Alexa, where customers expect it to do the right thing, how do you make sure it's consistent and reliable, but still maintains the creativity that we all have come to know and love about LMs? And then finally, I'll talk about model flexibility. And this is about making sure that we're picking the right model for the job. So, starting with accuracy. As we all know, elms are great at understanding natural language. Listening to customer requests and figuring out what the intent is behind that request. And if you're building a chatbot, that might be good enough. But when you're taking real-world actions like Alexa is, as I mentioned, turning on lights, playing music, booking tickets. It's really important to get accuracy as high as possible. And the stakes are much higher than just a conversational chatbot, when you're taking World War actions. And in a system as complex as Alexa is with lots of steps along the way, the errors compound. And so you really need to drive up accuracy at every step of the way, especially in the LM inference cycle. The first task that we're asking the LM to do is routing. And let's just take that, that example that I mentioned. Let us know when Daisy doesn't get fed by noon. In Alexa, we have integrations with tools and Software we call experts, or possibly other agents that can carry out the real-world actions. And so, we're, we're, the first thing we ask the LLM to do is, of the universe of tools and experts and agents, which one can handle this particular request? And so In the let me know when that might be. Uh, notification expert, uh, reminder expert. Um, it might be a calendar expert, but it's up to the LM to figure out which one of these experts to call on to get the task done. And I'd argue that this is probably the most difficult or important step and difficult. Because if you get this step wrong, it's super hard to recover downstream. And then the next thing we asked the LLM to do is, given that expert, There's a set of APIs that that expert offers to the system. To, for example, create a reminder. And what API name should be invoked? What parameters need to be provided to that API. So in a reminder, for example, you need to know When to fire off the reminder, what conditions, uh, go into that reminder, what frequency, how often the target for the notification, the message of the notification, all those values need to be retrieved at runtime. By the system as Orchestrated by the LLM to Call those APIs And all of this represents complex planning that we're asking the LLM to do. And each selection along the way, from which expert to which API to which parameters, to which values, all represent inference cycles that can add to Or reduce accuracy if you get it wrong. And then, finally, It's, it's not a, it's not a. You know, easy process to go through. You really have to try what works and figure out what doesn't, especially with different models. And what we learned was providing examples or exemplars of how to invoke these APIs was useful in the beginning. Again, this was You know, quite a number of months ago at this point, where the state of the art LMs aren't, weren't as capable as they are today. And we learned that giving uh an element, an utterance and an example API call helped it to understand the use of those APIs and experts. And as we were running into bugs, we kept adding to those examples, and it actually counterintuitively reduced accuracy. It made things worse. Why is that? Well, it turns out, as we all know now, but was new to us at the time, working with LLMs, was that the, you can overload the context and the prompts, go into the LLMs. You can give it too much information, and it ends up overfitting or acting in a way that's too specific to a particular use case. And It, like humans, have limited attention. And so it might, if you overload it with too much, especially irrelevant information. It can get forgetful and do the wrong thing. Certainly, if you have conflicting information. Some bugs might ask for examples that contradict other bugs. It just uh it's, it's a, it's a real tricky situation to get in. And so we ended up having to take out a lot of those examples and exemplars to help improve accuracy. It was a lot like squeezing a balloon, where we would fix a problem in one area, like In smart home use cases, you turn on the lights. It's important for the LM to know, well, which lights are there in the household. So, which light is the customer talking about when they say turn on the lights? But if the customer says, play some music, then. Providing that context about what lights are in the household is irrelevant and actually, again, reduces accuracy for the music playback use case. And, and so we found ourselves. Solving problems in one area, and then causing problems in another area. Refactoring the APIs helped, so making it more obvious so that we wouldn't have to provide. The examples to the LM it would just figure it out. And uh what we also learned was, it was actually harmful to be too obvious. So, if you had an API or expert called create reminder, you don't need an instruction in the prompt that says, use this API to create a reminder. Like it's too obvious and it's actually again harmful in terms of accuracy for those reasons I mentioned before, overfitting and. Forgetfulness or prompt overload. I remember when we first got it working, more often than not, we're so excited. It was like, yes, finally. I mean, this was a complete rearchitecture of the Alexa system. It's uh, uh, an, an architecture that had been around and served customers well on 600 million devices for 10 years. And we had to redo all of it. To get the LLM integrated. Little did we know That We had a lot more work ahead of us. Than behind us. Our next big challenge was around latency. And so we've got accuracy where we wanted it to be. But then everything was so slow. And again, anyone working with LMs will know. If you're using a chatbot and you're typing, it's OK to, you know, have the responses get typed out and you can do some latency masking techniques, uh, you know, the thinking in the chatbot, for example. But, you know, if you're Alexa, the, the customers expect. The lights to come on practically instantaneously. And so we tackled this the way we knew how using traditional latency reduction techniques like parallellyzation, streaming, prefetching. And they worked to a certain extent. Parallellyzation is. Taking APIs that are largely independent and calling them at the same time. And not waiting for one to finish before you call another. Streaming is about getting started as soon as possible. Changing a. Finish to start dependency into a start to start dependency. And so. In an utterance like, let me know when. Alexa doesn't have to wait until it hears the rest of the utterance and request, can just get started picking an expert that can handle use cases that involve. When? Let me know when, like, a reminder expert. And Prefetching is about loading in the right context. So In most cases, when you just say Alexa in the wake word. Alexa is already getting started before you see, say even another single word. Because it can gather information about the device you're talking to, the time zone that that device is configured in, in case you're gonna ask for the weather, for example, it can. Look up Your account for personal personalization, use cases, all those sorts of things happen. In real time and Prefesh before they're needed so that they're ready to go. When the rest of the utterance finishes. But we quickly exhausted all of the traditional techniques. LLMs were new to us at the time. And what we learned that was that there's a big difference between input and output tokens. Tokens are the numerical representation of the words that you say to an LM or the inputs, the images, or whatever it is that you give to the LM. Can anyone guess which takes longer? Processing input tokens or generating output tokens? How many think it's input tokens, takes longer. OK. About 25% of you. How many of you think output token generation is more expensive? So more of you about. 1/2 3/4. So it turned out, with, at least with the models we were using, I think this is universal. That output token generation is literally orders, not one order, but multiple orders of magnitude more expensive in time than processing input tokens. And so while we were really careful with Being efficient on the input token side, as I mentioned, to avoid overfitting and too much context, we were meticulous about output tokens. So, how many of you are familiar with the chain of thought technique? So that that's where you, in your prompt to the LM you say, think out loud. And it will think out loud. It'll, in its output, it'll start saying, I think I understand what the customer wants. I think I need to call this tool. So, the customer says, remind me when Daisy doesn't get fed by noon. And so, I need to get Uh, the APIs for the reminder expert, and so on. And that's great, but it's generating output tokens this whole time. And Uh, it's great because in, in some cases, and there are papers written about how this can help with accuracy. If it's thinking out loud, it has more likelihood of doing the right thing and predicting the next right tokens. To, to take the right actions. So that's good. And certainly, it's great for debugging purposes cause if you're developing and you don't know what's going on inside the LM you can just turn on chain of thought, reasoning, and it'll spit out exactly what it's thinking. But Again, orders of magnitude and latency impact on a running system. You don't wanna keep this on in a production system. It's kinda like turning on trace-level logging in your services and leaving it on in production and flushing to disk on every request. Like, it's just not a good idea. It's great for troubleshooting. It's great for development and figuring out what your system is doing. But you don't want to do that and leave it on in production. And then The, the other thing we had to do with. Token processing was on the input side. Again, the LM inference is one of the most expensive in terms of latency steps of the, the architecture. And we needed to take advantage of past processing, past. Utterances that are very similar. I mean, how many times do customers say, Alexa, stop? Right. Like, we don't wanna have to redo that same exact inference or imagine just like any application, LM application. A lot of the prompt is gonna be the same, right? It doesn't change from an utterance utterance. Right? Here, here's your identity. Here, here's what you can expect customers to do. Here are the tools and experts and agents you have available to you to get the job done. And all those sorts of instructions are not gonna change from utterance, utterance. And so we wanna take advantage of past processing and obviously caching is a great way to do that. And again, back in when we're getting started with this, it wasn't a thing, it wasn't available yet. Today, we kind of take it for granted. It's just amazing how fast this field is moving that something like prompt caching is just out of the box now, pretty much all the models. But when we were working on Alexa Plus. It, it wasn't available, and so we had to invent it. Working with AWS and our model providers. And it turns out, when you're caching in this way, ordering matters, because dipping into the internal state of the LM It's going to take a different path depending on the input tokens that it's processing, right? And so, you have to make sure that the stuff in the beginning is the most stable. And all the stuff that's changing, you push toward the end. There's another problem with that, I'll cover in a minute. But that was some of the invention that we had to go through again, early on. Invention that now is ubiquitous. But it was really important to get latency down. And then the prompts themselves. We went through a lot of effort to improve and optimize the input prompts, including techniques like minimification. And instruction tuning. Minnification is where you take a long string of words or tokens and the input. And you reduce it somehow. Think of it like a compression algorithm. And you have to do it in a way that doesn't affect the behavior of the LM. And so you're looking for elements, for example, take identifiers. They're naturally long because they have to be unique. But it doesn't really matter. To the LM, whether it's This particular value or that particular value. And so you can. Replace it on the way in, and then restore it on the way out, so that the rest of your system operates on the right value. But as far as the LM is concerned, it doesn't matter what it is. It actually helped with caching too, cause you imagine. You don't want Alexa stop and then that cache miss because you have an identifier in there that's different for every customer. You want that same. Inference cycle to give you the same result. So modification helps in that regard as well. You have to be careful with miniification because that, when you do the analysis, you're looking at tokens. And not every model has the same tokenizer. And even different versions of models from the same vendor might change in its tokenization process. So you don't want to get too tightly coupled and dependent on how tokenization works in a model. But when you, when you do use this and take advantage of it, it can really help reduce latency. And then instruction tuning is about Looking at the instructions you're, you're sending into the LM and in, in some cases, using the LM to give you feedback on how you can make it clear, use fewer words to describe the same thing, fewer examples, all things to reduce the number of input tokens going into the system and therefore reduce the amount of processing required and. Reduce latency. And then, finally, there are a lot of model-related techniques that we went through to help them reduce latency, like speculative execution. This is where, you know, I mentioned parallelization and streaming and prefetching as ways of reducing latency, but speculative execution with models also involves understanding that some models. Again, early on, early days, the models we were working with back then weren't as capable as state of the art models today and won't be as capable of, as state of the art models tomorrow. But you can use uh. High recall, potentially low accuracy model with fewer parameters. And again, this is just how the, the state of the world and technology is today could change. But fewer fewer parameters usually translates into faster inference execution time. And so it's gonna come back faster and have a lower latency. Answer when you ask it to say. Route to a particular expert. Pick an API You can get started on that result before you're, you're certain and sure that that's a high accuracy result. And at the same time, you can fire off requests to a higher accuracy model. That potentially has more parameters, therefore, higher latency. And later, if the result is the same, great, you've gotten a head start on calling all those APIs of that expert that you've identified. And if they're different, no worries. Just throw it out. Call the expert in APIs that higher accuracy model predicted. That way, customer gets what they want. And in a lot of cases, The lower latency model was right, cause it was close enough. And you've actually reduced latency. So that's pretty cool. And there, but there's a nuance here because if you get started on something and it turns out to be wrong, you have to make sure that the APIs you're calling. Aren't harmful because if they're wrong. Like you don't wanna turn on the wrong light or play music when someone is asking to turn on a light. Uh, and so, you have to make sure that the APIs you're calling are I dumpetent or, you know, don't have. Unintended side effects. Or in other ways are safe to call or undo if it turns out to be the wrong APIs. By far the most impactful. Part of the process that we found was reducing the number of times that we went to the LM to ask it to do an inference cycle. And here, API refactoring was super important. Because you could take a sequence of. Uh fine-grained APIs and combine them into a single or a small number of coarse-grained APIs that then could be predicted in fewer inference cycles through the LM. And then Fine tuning was about, again, making the foundational models that we got from our vendors into specialized models that were A better fit for our use cases and could operate more quickly given the traffic that we can expect from customers of Alexa. Uh, using in training the, the models using. That data So, balancing between accuracy and latency were, was a very kind of methodical, data-driven process. And Again, you know, our traditional systems before LLMs were really good at doing things reliably and consistency, consistently. But when you throw in a non-deterministic, stochastic, or statistically driven LLM into the mix, Again, they're great at being creative and engaging in conversation and in a chit chatbot. Uh, but when you Absolutely positively have to get it right. You're booking tickets, turning on lights, it's not OK to just be, play the right music on the right speaker most of the time for our customers. That's just not acceptable. It's gotta work all the time. And so balancing, again, between accuracy and latency, we're tuning the system to be more and more efficient, and more, unfortunately, more and more robotic. And it lost some of that. Character, that personality, that again, we've come to know and love in the chatbots and LM driven AIs that we're now used to. And so we had to actually tune things back, dial things back. Reduce a little bit of the. Determinism so that it could re-inject some of its creativity and be less robotic. So, Turn on the lights. Just do it. All the time. If you say, Alexa, I'm bored. Sometimes she might offer to play some music. Sometimes she'll say, hey, you wanna pick up that conversation about the fun travel destinations that we talked about earlier? Right. So, it's hard to, again, build a system that's both deterministic and consistent, reliable for the use cases that are more like using a tool, like a light switch. And also give it the personality and character and creativity that we can come to expect and the LMs are really good at. So balancing that is incredibly challenging as well. And of course, we all know now parametric answers or answers that come from the model itself based on its training. Uh, are only good as the data that was available at the time it was trained. And so current events or updated data sets like Personalization. Knowledge bases. Aren't going to be available in the parametric answers that the model is giving. And so for that, we use the standard techniques of grounding, retrieval, augmented generation. Rag, as is well known. But the challenge here is about how do you balance just giving the answer and, and then, Embellishing it a little bit. So, For example, Alexa might troll the Yankees in an answer if you're a Red Sox fan. Or it might, if you ask it, or say, I'm bored, it might tell the classic dad joke. Hi, bored. I'm Alexa. And so What we learned along the way was it's this, this subset of prompt engineering that we call now context engineering was super important to get this balance right. I mentioned the difference between. Smart home context of what light switches you have and music context. Like, it's not OK for the model to hallucinate and make up that you have 67 speakers in the household when you really only have 5. And not play the music cause it's thinking it's playing it, but it's playing on a non-existent speaker. That's not OK. So you have to really be careful to make sure that it has the context it needs to make the right decisions. Uh, you know, elements like past conversations, so it, it, there's continuity between the utterances and the interactions with, with Alexa and the customer. It was really an iterative process to decide what to include and what to exclude from this context. Summarization, of course, helps. But there's a lot more uh that goes into making sure that there's the right context without impacting negatively latency and accuracy. And we learned also, as many have learned these days, that the, the models. Exhibit this sort of recency bias, where instructions towards the end of the prompt are given a little bit more weight. Just like humans. I tend to remember things I'm just told than things I'm told much earlier. I just forget. It turns out models do that too. And so, Ordering again, in the context, in the prompt matters, not just for caching, as I mentioned earlier. But also getting the accuracy right and the balancing the behavior of the LMs so that We're fight, we're able to give an uh a use case or handle an experience that's both deterministic and creative. And then finally Safety is just non-negotiable for us. It always has to be a safe experience. And then uh the guard rails that we use are truly indispensable. And we took a sort of a belts and suspenders approach. We just didn't trust the models. We didn't trust that everything that went into the model was safe and everything that came out of the model was safe. So we had guardrails all over the place. Of course. We would prompt the model to do things safely. But in case it didn't, We would have other guard rails in place to take care of that. And all this couldn't have been possible without a multi-model architecture. It was absolutely essential. It was an early decision, and it turned out to be the right one, that you can't expect a single model to handle all the use cases, especially in a system as diverse in its customer base and And experiences that Alexa offers. And so as we were balancing accuracy and latency. And making trade-offs with. Personality and creativity. We found that there were a lot of other dimensions that we had to consider, like capacity. GPU cost. And many, many others. And this Multi-model architecture, lucky for us, was built into the system early on. And part of that was out of necessity, cause as I mentioned, the early models we were working with weren't as capable. And so, it was really convenient to be able to swap out and use other models. Uh, but what really helped us was discovering that we don't have to turn this off. We don't have to do this only in development time. We can leave this on in production. And that meant that we didn't need to go looking for a one size fits all model. We could find the right model for the right job, for the right use cases. And Again, working with AWS was amazing because the bedrock service made it super easy for us to just swap out the underlying model on the back end at runtime whenever we needed to. And not every challenge or use case. Is a nail that requires an LLM hammer. Like, you don't need an LM to handle the Alexa stop use case. I mean, you could, but it's kind of overkill. And there's a feature in Alexa Plus where you can email a PDF of, say, your kid's school schedule. And then later ask Alexa about it. When is my son's homework due again? When is that exam? And Alexa will answer, given the information in that PDF. Sure, the LM could handle this use case as well. But that meant You would have to feed that PDF in as part of the input prompts. That's a lot of tokens. Right? And then you have to figure out. When is it relevant? Is the customer asking about this? And if it's not there, then again, it's extra latency. Give that context to the LM. So, in our case, this isn't one of those use cases where uh Specific purpose-built bespoke non-LLM traditional ML model. Could do the job just fine. And so, we train lots of models in this way in the Alexis system. So, not only do we have multiple LLMs to draw on, but we have multiple non-LLM ML models that we use in the system. To handle all of the use cases. And again, here working with AWS and SageMaker makes it really easy and convenient for us to Build up all of these different bespoke models. But then, if you have all these models, now you have a new challenge, which is, how do you choose the right one? Well, one approach is you, just like speculative execution, you could use all of them, all the time. And then pick one answer, maybe the fastest, maybe the most accurate. Maybe the cheapest. But again, Calling all of these models in parallel. Might help with latency, but it also costs you in capacity and. And runtime, GPUs. And so, we found that you have to use a combination of techniques with multiple models. And as we were going through this, it was really like peeling the layers of an onion. Fixing one problem again. Only uncovered yet another set of issues and challenges that we had to overcome. And so I've talked about 4 of them, accuracy. Latency. Determinism and model flexibility. Some we anticipated. Some we didn't But I hope Sharing this with all of you, you can decide for yourself. What you need to anticipate as you're building. Your use cases. Back to you, Brittany. Thanks, Lou. OK, So what are the key takeaways that we want you all to walk away with, to take back to your own projects? So the first, model flexibility. Lou talked a lot about how early design decisions for our models afforded us flexibility later. We think this is a really important layer to how we build Alexa Plus, because what we found is one model doesn't fit all and right sizing models for specific use cases actually delivers better outcome than adhering just to one and hoping that it works through. And then also as you look across that continuum, optimizing for accuracy, speed, and cost is based on the need and the use case that you're trying to deliver on. Second, and this is critical, iterative experimentation is essential. So traditional optimization wasn't enough, and you heard Lou talk a lot about we started with traditional techniques like parallelization and streaming, but they really didn't go the distance. And so we had to bring in new techniques like prompt caching, API refactoring, speculative execution, and it was the blend of those techniques that actually allowed us to create something magical for our customers with generative AI. And also too, what works in theory doesn't always work in production and then it also doesn't work in production at scale, so building in that experimentation into your process and your mental model is critical. Lastly, we took a step progression. We wanted first for it to be right. We wanted it to then be fast, and then we wanted it to be reliable. When you're in your own world, you have to make these trade-offs. It's a balance. And so how do you do those is going to be a component of what the outcome you're looking to deliver on. Most importantly, Daisy the dog did get fed, but it's almost noon, so we're gonna see what happens today. If you want to experience Alexa Plus live, come to the one Amazon Lane activation at Caesar's Forum. This is where you can discover many of the innovations that are powered by AWS across Amazon's uh different lines of business. Thank you so much and please fill out the survey in your mobile app.