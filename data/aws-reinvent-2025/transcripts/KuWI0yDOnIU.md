---
video_id: KuWI0yDOnIU
video_url: https://www.youtube.com/watch?v=KuWI0yDOnIU
is_generated: False
is_translatable: True
summary: "This session, \"How Spice AI operationalizes data lakes for AI using Amazon S3 (STG364),\" features John Mallory from AWS and Luke Kim, CEO of Spice AI, demonstrating how to transform standard data lakes into high-performance AI engines. They showcase **Spice AI**, a platform that simplifies ingesting, indexing, and vectorizing data into **Amazon S3 Vectors** and S3 Tables. The core demonstration involves a live application (Apache Answer) where Spice AI ingests streaming data from Kafka, automatically performs vector embedding and full-text indexing, and enables **hybrid search** (combining semantic and keyword search) via simple SQL queries. Luke highlights how Spice AI abstracts complex distributed underlying tasks—like index sharding, caching, and cross-index querying—allowing developers to federate data from multiple sources (S3, databases) and pipe results directly into foundational models like **Amazon Nova** for tasks like keyword extraction or sentiment analysis, all configured with minimal YAML and SQL."
keywords: Amazon S3 Vectors, Spice AI, Data Lake, AI Operationalization, Hybrid Search, Vector Database, SQL for AI, Data Federation, Amazon Nova, Open Source AI
---

OK, thanks. Thanks everyone for joining us. Uh, I'm John Mallory. I'm a, uh, storage go to market specialist focused on storage for AI here at, uh, AWS. Hi everyone, I'm Luke Kim, founder and CEO of Spice AI. Uh, we were day one launch partners for Amazon S3 vectors, and I'm excited to partner with AWS on, uh, helping turn your data lake into an AI ready, uh, platform. OK, so I'll frame it up a little, um, you know, the reason partners like Spice are so important when it comes to operationalizing data lakes for AI is because if you think of a, uh, how data supports, let's say, an agent, um, in this case, maybe a, um, HR agent that, um, you know, on boards, uh, a new employee. Um, that agent probably has, or that workflow probably has multiple agents, and they're going to need multiple different data sources to all work together. You're going to need probably structured data, you know, from the HR system, um, you know, maybe, um, object data from your object store around documents that you're going to give that person that you're on boarding. Um, and ultimately the agents themselves are going to need semantic memory for long term storage and performant memory for short-term storage. So it's not just build a data lake and you've got everything you need. You have to start to stitch lots of pieces together. And so the more of these capabilities you can bring into your data lake, the better it's going to be and the easier it's going to be to operationalize it. So some of the key challenges to adopt AI are really data fragmentation and silos, you know, per this example I just lightly touched on, you probably have data that today lives in many different systems that are purpose built for that. Um, and along with that you've got to think about privacy and security of how to extend governance over multiple systems. Um, cost effective scaling is key, um, retrieval accuracy if you're building agent workflows, um, you know, both grounding your data with rag typically. Or your foundation model with RG as well as um providing semantic meaning and long term context to the agent itself which leads to integration complexity and ultimately how do you observe all this and include log data in the mix. So, um, so, um, you know, today S3 is the foundation for a lot of this and you know we continue to build out for the AI era. Um, we released S3 tables where you can start to bring iceberg tabular data into a managed table in S3, um, where you can start to bring workloads that might have set in databases or data warehouses directly into the data lake and query that. Um, and then also, um, we just launched S3 vectors as Luke mentioned, because ultimately vectors really are the language of AI, uh, both for things like rag and, uh, data, content, uh, characterization and search at large scale, as well as, uh, long-term memory for agents. And so really, um, SpiceC AI has been a key partner for us to do this and tie all this together. Um, they integrate both with S3 vectors, um, as well as uh S3 tables, as well as data stored in S3, improve the performance, and, um, ultimately help integrate against amongst integrate across all of these different data sources. So with that, I'm gonna turn it over to Luke. Alright, thanks John. So, as John mentioned, S3's really been the foundation for organizations over the last 10 years. And we talked to a whole bunch of customers, some of our customers like Tulio and Barracuda Networks, and we hear from them, they still want the scale, durability, and cost efficiency of S3, uh, but there's additional challenges to make these workloads work for this next era of AI workloads. And, uh, what that requires essentially is you to handle a whole bunch of additional distributed systems problems to actually uh leverage these primitives which are very uh scalable. Uh, it pushes essentially the uh onus up into your application layer to be able to uh turn all these primitives into a large scale data and AI platform. And that's where, uh, Spice can help. So. Spice will provide an AI native interface across all of these primitives, and handle all these problems like ingestion of data. How do you just get data into S3, into S3 tables, into S3 vectors. Uh, it handles things like caching and indexing and shouting, and it can also, as John mentioned, federate data across any other sources as well. And so, You wanna leverage all of these great products, uh, but how do you do that, uh, very fast, faster time to market, uh, and get that, uh, value back into your application. So, if we zoom in, just one of these problems that you have to deal with is, if you wanna use S3 vectors, you might wanna do something like a daily index. And so now you have to start managing indexes, uh, adjust a whole bunch of data, partition it, put it into indexes. You have to manage metadata, push that into uh S3 vectors for filtering. Uh, if you do this at large scale, you now have to, uh, do index striping, cross-index query, and so forth. And so, uh, normally all of that would now get pushed into your application layer that you now have to deal with. Uh, but Spice handle those problems for you, and enables you to use these products in a very short amount of time. I'll show you in a demo here in about 15 minutes, uh, you can get all of this into estuary vectors, couriered out, and into your application with almost no code at all. So, in the demo here, we have a demo application, Apache Answer. It's like a stack Overflow-like uh uh uh application. And we're going to be ingesting questions and answers as they come through, through Kafka streaming, and Spice is going to ingest them. It's gonna index them for full text search, vectorize them for vector search. Cache and store them, partitioned within S3 tables and S3 vectors, including the metadata. Also enabling you to do real-time historical queries across not just structured data in S3 and S3 tables, but also unstructured data and other data systems like Aurora DB. Uh, it's going to use Titan embedding model for the embeddings, but it could, uh, also, uh, load and serve models itself and get all that stuff into AI so that you can actually do a higher level analysis on it. So I'll swap across here to, uh, my application. This is Apacheander, a, a straightforward question answer application. And what we wanna do here is provide essentially a better search. If I, um, just do a search query here, MySQL connection error, then you can see that, one, the search is slow, this is the native search within the application. It's just using postres in the backend. One, it's slow. Uh, and two, you'll see, uh, sometimes it doesn't actually work, um, but will give you irrelevant results. Um, so this is such a large query that sometimes it won't even return results, but normally it'll give you 20,000 irrelevant results. So how can we actually make this better? So. Uh, this is Spice Cloud. Uh, this is a hosted managed version of Spice. But Spice is also open source, so you can, uh, host it yourself anywhere from a Raspberry Pi all the way up to a cluster, uh, and, um, uh, self-host in kind of any, any deployment environment. So the first thing Spice gives you is very fast queries. So what we're doing here is, uh, start a script, and this is gonna start loading questions and answers. Into the actual application, uh, so that we have real-time streaming data. And that streaming data is coming into the Spice platform, and, uh, we can just do query here, and we'll see the, the number's essentially going up. Um, we also have a whole bunch of historical questions here, uh, 250,000, that's in S3 tables. And Spice offers incredibly fast query across this data, um, so that you can. And sometimes the Wi Fi here's a little slow, uh, to, to query across any of these federated sources. But what you really wanna do is make the search better, so. Um, Spice offers, uh, very easy to use, uh, functions. So here I've got this text search function, and as we're ingesting the data in, we're indexing it for BM 25 full-text search. And so with just a couple of lines of SQL, I can now do full BM 25 full-text search over that same dataset as it's being ingested in. But what I really wanna do is provide a better quality search, and that means a hybrid search that uses both text search and vector search. And this vector search function, we've done all the work in the back-end for you to leverage S3 vectors. So when I call this tech search function, uh, we've put all the data into S3 vectors, uh, indexed it for you, and then we'll provide the search across multiple indexes back into your application for you, um. And then we're gonna combine the results of my full text search and my vector search into one re-ranked list to give me a better semantic search across my application. So here I'm gonna run the, run the query here, and we will, the Wi Fi holds up, get a search answer back. And so here I've got a better results set now for my query, my search query. And I wanna show you what's going on behind the scenes here, so if I run and explain on this, which shows you what the database is actually doing in the background, then you'll see there's all this work being done in the background, um, uh, in the, the, the database, and it's kinda hard to see here, but each one of these nodes here you'll see is a query into S3 vectors on your behalf. And the reason why there there's a whole bunch of boxes here is that Spice has done all the work to uh spread all this data across multiple indexes in the background, and being able to give you in just one very concise query, uh, this full large scale petabyte scale search across S3 vectors, combine it with a full text search and a re-ranked list back to your application. Uh, and you didn't have to do any work to actually do that. Um, but what we really wanna do even more than that is we wanna operationalize this. For AI and get it into AI. So, now can I essentially query across this large dataset, uh, get a search query, get the results of that, and feed it into AI. Uh, and we can do that with Spice. We have a AI function here, so very, very simple, just a small function, and we can give it a prompt, extract 3 technology keywords from it, from the answers, and, uh, we're taking full of the the search results here and piping that into Nova. Um, and so I'll run that. And you'll see that it's now it's doing that search across that large corpus of data, re-ranked, hybrid search, and it's and it's feeding it into uh Nova, and will give me back my uh 3 technology keywords, uh, we hope, with Nova. So it looks like Nova might, oh there we go. So it took a little bit longer because of the Wi Fi here, uh, but you can see here I've got these keywords here, uh, and you can imagine what you could do here. This could be a security analysis, it could be a fraud analysis, it could be sentiment analysis on any of these search results that you got across this, you know, large, very large data set, uh, in just a little bit of SQL here. And the great thing about SQL. Is that um LMs can write it. And so you can imagine that you can even use Texas SQL here to do natural language, uh, and actually get these results back, leveraging the full power of S3 and S3 vectors in the background. Um, so, that was a very fast demo, I'm gonna show a lot there, um, so let me come back and just revisit what we did. So, we ingested a whole bunch of um. Questions and answers into the application is streamed through Kafka. We ingested it, we indexed it, we vectorized it, we pushed it down into multiple different indexes in estuary vectors and tables. Uh, we queried it back out again, uh, and then we fed it back into an AI model for higher level analysis, um, very, very fast. So if we come back to my application here, then, uh, we can enable now spy search on it. And uh we'll come back to that same query, uh, which didn't actually work to start with on the native search. And now this will use Spice Search uh in the background, and, um, now give me uh a, a much higher quality search result back, um, that's, uh, much more relevant to my use case, and, uh, if the initial search actually worked, it actually showed like 30,000 results that were irrelevant, uh, and this shows, uh, 20 relevant results here. So, how is all this wired up in the background? Again, I said, there was very little code here. So, the way you configure Spice is through YAML. And to wire up that entire workflow, all we had to do was say, here's my S3 table's glue table. I wanna enable it with for Amazon S3 vectors, using S3 vector's engine. I wanna partition it by year, but it could be like per day, per month. And then I wanna make it super fast query, so we can accelerate it and do tiered caching with DDB uh or other embedded databases. Um, I wanna have filterable metadata that I push down into S3 vectors so I can make the search super fast. And I wanna combine it with BM 25 full-text search. Finally, I want to connect it all over to my nervous set of models, uh, here, and we also support a whole bunch of other different, uh, models, as well as serving and loading your own one as well. So in just a few lines of Yammu code, there's no application code here at all. Again, I was able to ingest a whole bunch of data in real time, query across the historical dataset, index it, search it, and provide it into a model, uh, in a very short amount. of time. So, that's Spice, uh, it's an AI native, uh, database that enables you to turn your S3 data lake and your S3 vectors data lake into an AI native platform very, very easily, providing, uh, SQL, federated SQL security, accelerated data, hybrid search, and inference across all of these great primitives that Amazon has built. Um, here, I'd like to give. You know, to give us a star on GitHub, it's all open source. You can deploy it and run it yourself, uh, and then we also have some t-shirts and things at the back that we'd love to, uh, give you if you, uh, uh, come and see us after the tour. Thank you, so, uh, thank.