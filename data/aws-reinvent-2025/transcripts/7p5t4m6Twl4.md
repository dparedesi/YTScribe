---
video_id: 7p5t4m6Twl4
video_url: https://www.youtube.com/watch?v=7p5t4m6Twl4
is_generated: False
is_translatable: True
---

All right. Hi, everyone. I'm Shanta Uwali. I'm a director of product management at Lambda Test, and I'm really excited here to talk about scaling automation with KNEI and hyper-execute. Now before we begin, what I do want to talk about are the problems with testing strategies today. While all of us do understand that there are many problems, but a few of them that I do want to highlight are a fragmented oversight, there are disconnected testing processes which do lead to blind spots, and there's a lack of centralized governance which causes misalignment. Second one being there are inconsistent testing standards. Which brings variability across teams which actually just goes ahead and weakens quality causes delays and erodes customer trust if something is shipped out, which is not exactly how it was supposed to be shipped out. And finally, a big, big problem that is still out there is brittle unscalable infrastructure. Now legacy systems cannot meet modern testing needs, and at the end they are slowing down innovation. Now, if we dig a little deeper into it as to why testing is breaking right now, we'll be able to find that most of the times that we imagine that the quality engineers or any developer team focusing on testing would be spending on is not actually being done. The test planning and authoring, it's just the tip of the iceberg. What we actually see is most of the time is actually spent in fixing broken and flaky tests. A lot of the time is actually spent in test optimization, test execution and monitoring. And finally, even right now in today's world, a lot of the time is going into triaging and debugging the tests. Now we all know that Gen AI is reshaping engineering in today's world, and over the last couple of years what we've seen is there has been a huge code volume surge and what we see here is that those code volume surge will also need to be tested. So the question to ask here is, is testing ready for it or not? Now that's where lambda test comes in with modern intent driven testing workflow. Essentially what we're doing with KNAI and hyper execute at the center of it all is that we can allow you to plan your test, author your tests, leveraging KNAI, and of course go ahead and execute them as well, uh, by leveraging hyper-execute and give you all the analysis as well now. The major question that all of you might be asking as well is that is it only functional testing? The answer to that is no. It can be functional testing, it can be API testing, it will be accessibility, visual regression, performance testing, because that's how you'll be able to meet your modern testing needs. Now the way it will work and the way it works right now is that you can input any, any requirement that you might have, which could be via media or video and audio file. It could be a complete PRE document that you have that you wanna upload or it could be requirements coming in directly from Jira stories as well that goes into KNAI and KNAI gets all the context and a memory added to it and gives out relevant test cases to you. Now those test cases, while having human in the loop, then are authored autonomously by KAI agents and at the. And all of that can be executed on hyper execute platform. Now the output that I'm talking about, it's actually open source framework and language leveraging selenium, Playwright, APM, all of that to you and going ahead and executing and giving you all the analysis and RCAs that might be helpful for you to figure out what's the best court coverage and for you to be able to finally achieve that in sprint automation that everybody would like to do that. And of course all of that very seamlessly goes back and plugs in your Jira workflows by having that connection and more than 100 integrations already being in place. Now one great part of it is that all of this works on intent. It's completely natural language based which allows you to move at light scale and much, much faster as well. Now what I would like to do is actually show it all to you in action. And uh we're just gonna go ahead and switch to a demo mode. Now what we have here is a Jira story, uh, and I've taken an example of Airbnb right here. Uh, this would be something that a lot of us might get right as when we're developing any, any feature or any requirements that might have come from the product team. Once we have those details available, what KAI can do is that we can just go ahead and add a comment such as lambda test AI cloud, let's say generate test scenarios for me. And as soon as I enter that, Kenny and I will quickly analyze the description, the summary, and any other custom field that you might want, and within a few seconds it will come back to us with a response that, great, hey, I have gone through the entire context that you have provided, and I'm starting to generate the relevant test scenarios that you want. And if you just go ahead and click on that link. We'll see, uh, that all the information that was initially provided to us has been figured out by KNAI and it has started generating test scenarios and multiple test cases within them as well, giving you all the relevant information whether the test case is a must-have, whether the scenarios that have been generated are the positive scenarios, negative scenarios, edge cases, so essentially. Something that you would have spent a couple of days going through figuring out what are the scenarios and test cases you want to automate. KI helps you do that in seconds with all the relevant context of your application. Now one great thing that I also want to talk about is that if you have existing test cases, it also checks for deduplication and does not create duplicate tests every single time. It saves a lot of your time by ensuring that it has existing test cases in context as well. Now, what we can do is we can actually quickly go ahead and see what details have been created, and these are complete information that would be relevant for you to have your test cases ready. Including test steps, outcomes, and of course we have added the flow in a manner that we can have human in the loop. Anyone can ensure that they are able to review it, make edits to it, uh, make any changes to the test steps as well, and go ahead and review those quickly and once all of them do make sense. The goal is not to just be able to go ahead and create these test cases. The goal is to be able to automate them in a singular flow itself. So what I'm gonna do is I'm gonna just select a few of the test cases and I'm gonna go ahead and create an automate. Now the beauty of it all is that. You, it's completely platform agnostic whether you have a web application, whether you have a mobile native application or a hybrid application, or whether it's just a mobile browser app that you wanna go ahead and test. You can choose any of these, upload your relevant apps, whether it's APKs, IPAs, provide us the URL, even if it's a local application that you wanna go ahead and test out, you can go ahead and do that as well. And finally, just let's say I input the application under the test, which would be. Airbnb.com, in my case. And let me just choose a few agents and go ahead and create an automate. Now as soon as I do that, KEI will spin up multiple agents in the background to autonomously author these test cases, and we can see all of them are coming up right now, right here. So what we see is that multiple test cases got authored and they. Anonymously ran within the session and the output that we get at the end is something such as this that they are providing you the entire context, the entire test cases at every and every, every information which might be relevant to you. So for example, it actually went ahead and searched for an accommodation, uh, figured out every step, and all of this is intent natural language driven. Now why I'm highlighting this again and again, why intent driven is necessary because the problems that we were talking about initially when we started out being flaky tests, flaky locators, triaging and debugging, all of that comes into context when you have that information available to you, and that information comes as intent to the system. Now what you can see here is complete details along with screenshots which might be relevant to you highlighting what exactly is KEI doing where it's going ahead and performing the test for you and as an output what you also get uh is the. Complete code of it as well that you get a complete open source code whether it's selenium uh whether it's Playwright, Cyprus WebDriver IO you can choose any open source framework and language which might be relevant to you and your team might be more comfortable with that and then. Another thing that I do want to highlight, as I was mentioning earlier, it's not just about functional test. You can go ahead and have your API testing, curl commands, assertions on nest adjacent values, and all the best practices that are already out there from, from decades of experiences that people have had in testing strategies can actually be leveraged right here itself. Now, What I also want to show you, which is really, really brilliant, is that while authoring the test case, Kenney and I actually figured out that there are a few issues that are already there within the system. So for example, if I just open one of them. What it was expecting was that the map should be present and enabled above the listing. However, if you just open the screenshot quickly, what we see is the map is actually besides the listing uh that we see here. So it has gone ahead, suggested as a bug that the actual outcome was that the map is present but it's not. above the listings page now that is something which only an AI will be able to figure out quickly and give you that result. Now the best part here is that you can directly confirm and raise a bug on Jira and give it to your relevant developers and team members to have an extremely easy smooth flow, uh, within the same system. And of course you can manage your entire test cases, all the details which might be relevant to you now what we've done until now is just seen how multiple test cases are authored and automated, right? We have automation scripts ready for them, but that's not all. That's not what we wanna do. We want to go ahead and bulk them out, maybe create a test suit, maybe create a regression, a sanity suit, and finally integrate them into CI pipelines as well because that's how, that's how development works for us. So what you can do is club all of the test cases together into something called as test runs and if I just open one of the test runs that I've already run very recently, just a few hours ago, we'll see that we get complete results, uh, and all the analysis that might be required for us so we do see that it has a 70. 3% pass rate, the total duration it took, and finally what we give out to you in case of failures is whether it was a new kind of failure that has come up now, whether it's an anomaly in terms of test execution time, maybe a network call was taking much longer than it was expecting to take, or maybe it's a test which is always failing, which might need, need some help. And if we just click on what the issue was, we actually see a complete RCA, a root cause analysis for how many times the error has been repeated, what is the overall error trends, what exactly failed right there, and if needed, any remediation strategies that might be required. Now all of this comes in from hyper-execute by analyzing all your historical data as well and any executions that you might have. Now the great part here is that you can also schedule your test runs apart from integrating these TOCI pipelines as well. So in case you want to just go ahead and have a sanity run every single day, every morning at 9 o'clock, you can go ahead and schedule that. And once you come back to any results, what you'll also see, uh, is a complete report if I just switch to this report section of the historical runs that have happened. So if you see, uh, these are a few runs that are scheduled and are happening recently, we can see. That what is the overall stability of my suit, whether it's running consistently or not, whether there's any decline, uh, what is the pass fail rate overall that it has been running over a period of time, and what are the failure categories, whether it's failing at a particular step, uh, whether it's automation, whether it, it is a product issue or whether it's an environment issue because that's a huge problem when we're trying to debug as to what is going on and what might the concerns be. And what I do want to show you is that if we go ahead into any of the details, let's say something did fail, and this is not enough information for us, we want to dig deep, we want to see more. We can just click on any of uh the test cases that we see right here and be navigated to a complete details for you along with a video uh along with all the command logs all the uh relevant information that you might need in order to debug the test including the network calls and an accessibility report as well. Now this is something which is extremely critical as of right now because accessibility. Is something that everybody is focusing on and needs to focus on right now so you can get your full fledged report using any specific WCAC version, figure out what if there are any critical issues or serious concerns that you need to get fixed quickly and see a full fledged report as well. And of course if you're talking about mobile devices, real devices, you get a complete app performance report as well for your performance testing whether you want to figure out a cold startup time, hot startup time, check how your application is performing over a period of test session, whether it's a peak in CPU usage, whether your application is actually degrading the performance of the device, and eventually giving you slow results on a real device because that's something that you cannot find on simulator remotely orders directly. Memory usage, disk usage, and all of that information which might be relevant to you. All right. So I know that we've gone through a lot of, uh, information, but. What I do want to highlight and just quickly recap is that Lambda test is helping us provide modern intent driven testing and the only way we can move forward is by ensuring that we're able to capture issues much, much faster and hence release faster as well. Now Lambda test has flexible deployment models on AWS and we're listed on AWS Marketplace as well, and I really wanna thank all of you for attending this session. Uh, and, uh, we are available at booth 679, and if you have any questions, I'd be happy to help out and if you want a customized demo we can try out your applications directly on KNEI as well and see how that works out for you. Thank you so much, everyone.