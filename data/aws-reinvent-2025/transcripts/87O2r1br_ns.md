---
video_id: 87O2r1br_ns
video_url: https://www.youtube.com/watch?v=87O2r1br_ns
is_generated: False
is_translatable: True
---

Good afternoon everybody. Welcome to Reinvent 2025. My name is Glenn Buckholz. I'm a principal modernization and migration architect with AWS. I'm joined today with my colleague Harlene Carr and a very special guest, Ahmed Babars from The New York Times. He is their principal engineer. So the reason we're here today is we're going to talk about migrating from Google Cloud platform to AWS now. That sounds like a little bit of an adventure. I have my adventure hat on. Who here is ready for a little bit of an adventure? Show of hands. OK, we have to talk. If as a solutions architect, I came up to you and I said, you're gonna migrate, it's gonna be an adventure. Does that necessarily inspire a lot of confidence in the activity? No. Today we're going to talk about how we made it less of an adventure. So, no adventure hat. We're going to talk about cloud to cloud migrations in general, some of the patterns that we've observed. Why AWS? We're going to talk about. The thing that started the relationship between AWS and The New York Times. Uh, we're going to talk about the New York Times cloud vision. Remember, the New York Times mission is not to run the best or the fastest Kubernetes cluster. They have a mission to deliver media. And while it helps to have the best or fastest Kubernetes cluster, it's about how AWS can help them achieve that mission, and that's the vision they're gonna talk about. Then we have these ideas of paved roads and blessed paths. And first we're gonna talk about paved roads and how that eliminates some complexity with the container portion of the migration. Then we're going to talk about the blessed paths, and that was how we did some of the database migration, and this one is particularly interesting because it involves two dissimilar services, which I'll go into a little bit more depth to in a minute. And then after that, Ahmed will talk about some of the outcomes from the migration and then we'll take questions. Are we ready for our non-adventure? So one of the things that I wanted to talk to you about first is migrations and cloud to cloud migrations. 5 years ago when I started working for AWS and we would talk to customers about migrations, we would talk about 5 major differentiators. And those differentiators were 1, instant global reach. If you want to go somewhere else on the globe, you no longer had to negotiate a COO in another country or something like that. 2. AWS would reduce the undifferentiated heavy lifting that you would have to do to take care of your infrastructure. 3, on demand. Paying only for what you need. 4, scalability. You can scale up and down as you need. And 5, experiment quickly. You want to experiment with something. Now, let me ask you a question. If you're coming from another CSP, are these necessarily differentiators? Probably not, well, maybe the resiliency and we can talk about that one because you'll see it up on the uh slide right there, but. For the rest of them, we have to think about the value proposition differently. And with AWS and the New York Times. There were 3 key reasons that we saw that elevated the value of using AWS. And the first was resiliency and capacity. Remember. The New York Times' mission is to deliver their content and their media to other people. And whether one person is interested in that content, or a million people are interested in that content, they need the ability to deliver it seamlessly. And they thought that AWS was the right partner for them. 2 is partnership. Amazon has their leadership principles, and one of those leadership principles is customer obsession. And my role and Harlene's role as solution architects is kind of the embodiment of that. We are your partner and we're going to try to help you get the value out of your IT spend. And we do that in a variety of different ways. We do that by making sure that your business need is fit to the right technical solution. And we do that by looking for other ways that we can be your partner that might not be technology related, and we have account teams dedicated to that. And also unsung heroes, the technical account managers. So AWS is your partner in more than one way. We don't just provide a service where you can run things or services where you can run things. We provide people to help you get the best value out of your IT dollar. And that can take a lot of different forms. And last, for the New York Times at least. The building blocks of AWS helped them create their cloud vision, and they saw this as the best, most efficient way to do this. We helped them create New York Times services on top of AWS services that allowed them to centralize their compute. Their data Into a platform that was purpose built for accomplishing their mission. So now that we've discussed how the value proposition is different between an on-prem to AWS migration and a cloud to cloud migration, let's talk about some of the blockers of cloud to cloud migration. Now, it can be summed up that the largest blocker is fear of the unknown. When you're working with another CSP. You're familiar with that CSP, all the quirks. You've probably spent a lot of time and education with that CSP and change is difficult. And the fear of the unknown applies, I would say, to two different categories. These are two different things that you're going to have to approach when you're migrating from another cloud provider. The first are what I would call commodity services. These are surfaces that look and feel the same, almost no matter which. Service provider you're on. So for example, a virtual machine. Most of the virtual machines, they're going to have memory, they're going to have block storage, they're going to have a number of CPUs. They're gonna have probably a little play button that starts the virtual machine, and a little stop button that stops the virtual machine or an API call that is somewhat similar. These are commodity services. And One of the ways that we alleviate the fear of the unknown for the commodity services is through education. AWS has a variety of different mechanisms that they can apply, from our training and our learning needs analysis, to immersion days, and jam sessions. Experience-based acceleration. And just the essay gathering the right experts and having conversations with you about it. Additionally, The commodity services usually are shaped that way because they look very similar to on-prem services, and that means not only do we have a way to alleviate the fear of the unknown, but we have a well trodden migration path that was established when we were first bringing the cloud into existence. So we know how to migrate SQL servers from on-prem. To AWS, well, we also have specialized knowledge to migrate manage SQL servers from the cloud providers to AWS. Tooling is pretty much the same. You use DMS or something similar, right? The same with instances and other things. Now because er the rules are a little bit different when you're going cloud to cloud, there are sometimes accelerators, things that you can do faster that you couldn't do if you're on-prem. And we have the knowledge and the expertise to help there as well. So in terms of commodity services, AWS can help get past the fear of the unknown by implementing this knowledge and these services that we already have. Now the next part, the next part is a little bit more difficult. Other cloud providers, they do have services that are specific to them. They may have an analytics suite, or they may have a database suite that's different and as a matter of fact. One of the databases is what Harlene is going to talk about later. And there's no direct 1 to 1 mapping here. You can't just do a database dump and restore. It's not going to work, that service doesn't really exist on AWS. So in order to get past the fear of the unknown for the managed services, AWS's essays, remember, we are your partner, know how to gather the expertise necessary to provide structure for these migrations and allow you to quantify the cost in both engineering effort, time. And resources in order to do this. We will go out and if migration SA like myself doesn't know how to do it, we will pull in the proper experts, uh, in this case maybe a NoSQL expert or an analytics expert or maybe a storage expert. And these people have real-world experiences on how these surfaces are built from the ground up. And maybe even expertise with that service on the other cloud provider. And so it's no longer unknown. It's a path that we can chart together, and something that we can do with a level of confidence. So that we can make sure that the migration is successful and within the bounds of what you've accounted for in your business plan. And lastly, in order to make sure things work. Sometimes we even engage experts to do a proof of concept. On the small scale, maybe essays like myself will do it. On the larger scale, projects have also been commissioned with professional services or partners, so that you don't just have to take our word for it. And that's how we make this less of an adventure. And we remove the fear of the unknown. So with AWS you can do these cloud to cloud migrations. Now How did all of this begin with the New York Times? Well, there was a spark, and to talk about that spark, we have our distinguished guest. Thank you, Glenn. So Glenn has been talking about cloud migration, and now you see Amazon Connect on the slide, so you might be confused. But let me tell you how it all started back in 2017. It was a business problem. It wasn't a technical problem, and we weren't moving from another cloud. The problem was, as you all might have experienced, our subscriber will call our numbers and they will get to our IVR system, and the IVR system will try to solve the problem, but then when the problem is not solved, they will get to a life agent and then they have to repeat the problem again and the. Validate themselves again. So the problem was how to actually cut the handle time and ensure that all context moves along from that initial conversation that they have done with the IVR to the agent so they don't have to repeat that. So we have to transition them from a very frustrating process to a better smooth process and the business value here and the business proposition was like we want to build a better. Customer experience. So, we went in a hunt, trying to find what tools that we can implement into our stack to have something like that implemented into our CRM. And then we found Amazon Connect. It was a big transition. Now we have our telephony system, we have a lot of pieces over there, and then how to move all of that into a managed service. It was like a big ask. So we have done a proof of concept and quickly we identified that Amazon Connect as a managed service actually is a great fit for something like that. So why? Because all of the things that build or you can integrate natively with Amazon Connect. We are not talking just about telephony. We are talking about Lex understanding the conversational. We're talking about databases just like get the customer context, customer information. We're talking about SDKs, which for us was the actual key that we need to integrate with the CRM. So from there now we have a good way of understanding what's. look like and we know what managed service looks like so we start to think about what do we want to do next and that takes us to another level where we look at our cloud vision. So what actually we are trying to achieve if Amazon Connect as a managed service is good for that vertical business side problem, what else can we do based on AWS? So we start to think about we want to build more compute storage. Other tooling, AI, all of that, but we don't want to focus only on this on their own on AWS. We want to focus on how to build that in a unified platform. We want to build standards. We want to have this organized into a way that we don't have to think too much about how to ship new features. So the idea and the goal here is we are not focusing on the. Itself we are focusing on solving the business problem and the business problem we are solving is we want to deliver news, breaking news, we want to serve games to our subscribers, so we want to focus on that. So how to shift all of that and make it, make sure that we have the reliable partners to deliver this on. So from there we know where are we going to build this stuff, but how we're going to build it is interesting because. Back in time and I'm going to dive into that deeper, like we already, some of our services in AWS and some of our services on GCB and there are other places, there are some services on-prem. But they are differentiated from each other. However, when you take a deeper look into the services, they are not completely different. Like they might share some characteristics. However, because they are built across scale of teams, different teams, they don't look the same. So what we wanted to bring here is our opinionated way, our way to how we actually going to deliver. On the service to our customers, so our teams have to less focus on all of the infrastructure built around, and me and my team as a developer platform team, we are also less focused about how we manage all of that. So at the end of the day we want to have our standards and good standards so we can ship this to our product engineering team, which they can. Focus on the differentiated features and the quality products that we wanna ship to our readers. So from here we are focusing on scale and efficiency, and I'm imagining that all of you are trying to do something similar and seeing like what is good look like for your organization. So it's probably a question that you have seen before. So this is, we already designed. Where our service is gonna look like or how we're gonna be opened about it. If we take a step back and look to our product engineering team, I'm in the developer platform, I'm trying to build tooling for my engineers in other missions, and I'm gonna look about, hm, I have a business problem, then as an engineer I'll come with a solution, and then I'll start planning design. and we'll write some code, we'll ship it somewhere and then we'll release it. We'll have a monitor. This is pretty common to any engineering teams that you have seen. So what are we trying to do with that? We're trying to centralize on that. So we're looking at what are the common patterns between all of these that we can centralize on. And we actually identified that the business problem might be unique, the idea and the plan might be unique, but not all of them. The creation of the process. The code itself will be different, but the way that you shape your code, the way you deploy your code could be the same, the way you monitor it could be the same. So what happens if we centralize all of that and then instead of like making you think about all of these steps. You will get them out of the box. So as a platform team, our focus on building centralized CIC, building observability, treating observability as a first class, not afterthought, like making sure that our deployment pipeline is always there. That gives Us the opinionated way and give us exactly the boring way but also the good boring because like when things look the same you don't have to think about too much like the process of being where I going to deploy this is actually like what I want my service to do. So from there we already defined the process. We actually found out what is the problem that we are solving as a platform team, and we have the life cycle. So now to the next action, which is like how are we going to do that? What are the technical pieces that we have to involve this process in. And when we start to look into that, one of the things that like Glenn talked about is cloud to cloud is not a copy and paste. Like there's always unknowns that you encounter through the way because there's something built with specific thought in a different cloud on these primitives. So how to move it to a different cloud or a different provider. It's always a challenge and not everything known beforehand. So. The goal for us wasn't just like we want to migrate, but like we want to build a library, a library of patterns and good standards that we can share across migrations, and from there we decided on Kubernays. We have been using Kubernese for a while across multiple providers. So what if we have a shared Kubernes architecture across the stack that we will deploy all of our services to? So we We will have the team to focus less on any infrastructure, and I will dive into the details for how this project will go through, but the other thing that was important was to focus on conternalization, which makes the applications less also focused on how to build and shape everything, but more focused on containerizing the apps themselves and have a way that we can embed this into our process and ship it to our platform. And the last thing that like we were looking into, we wanna have this to be paralyzed across multiple teams. We don't wanna do one time project of migrating or building a single library like when we take like an initiative, we wanna have all of the teams move to Kubernes and have all the teams to start thinking about quantumization or doing something else because we wanna keep building these libraries and we wanna keep automating the platform. So now we focused on that we paved the road for like this is where we're going, we're going with containers, we already built Kubernets and we already focused on reliability and resiliency. So now we are trying to think through all of the steps necessary in how to get there. Like we already have the platform built, we have all of our tooling in place. Now we're going to focus on actually let's talk about what do we want to do with this platform. So we have all of these steps mapped out. So we focused on building a Kuberne setup. And the setup here is interesting, but let me tell you a couple context points about it. We are already in AWS. We have what we call AWSMA, multi-account architecture. So all of our product engineering teams have their own accounts that they can deploy their resources to. But what we came up with, so what if we start with. Everyone gets a cluster. Uh, that might be problematic. Now we have hundreds, maybe thousands of clusters across your organization. You can automate it, You can operationalize it, but the effort that you're gonna see from something like that, it's gonna take a toll on the team. So what if we started simpler with a shared architecture, so we have a single account across multiple environments and then we have that account focused on resiliency. So out of the box we build. Like multi-region in space, and let me tell you one thing that paid off in the last year's East one outage, we have our critical applications just scale into the west region and that was good, so we didn't, we definitely have some degradation, but it didn't impact us really as we would expected otherwise. And then we focused on what we need in that shared Kuberne, and when we look at how we would isolate it, like you're going to see psyllium here, we use psyllium for network isolation because on like a on a higher scale, like we have to make sure that each tenant from our product engineering team is isolated to our own spaces. We also focused on scaling. Carpenter here played a good role into how we scale faster. Also we can scale with different parameters to fit our needs, to fit our news, to fit our games. We're also looking at SSTA, which is important for our service mesh, and how traffic moves between east to west, and we look at OBA, which is how we actually going to tell someone you are not supposed to do this automatically. We don't have to go to a team and say, hey, don't inject your Ursa into that from a different account. That is done automatically. So this is the way that we set up our shared Kubernes architecture, and that actually helped us. To focus less on as a team, as an organization, on like all of the infrastructure spun across the org, and make the team focus more into actually building the differentiated work. They focus more into like how actually they're going to deliver different things for the news or like a new game or a new recipe or something else that matters to our subscribers. And just to summarize all of this, it's finding the common patterns between all of your steps and migrations and projects. So looking at this applications is a good start. When I look at a lot of applications on a scale, I see like all of them doing something similar, maybe logging, maybe tracing, maybe something else that we can opt for open source or we can build our own wrappers on top of these things and then we look at shared. Can we map these things together? So now we have done that part and we moved to containers. Now all of the applications are containerized in a way that we can ship them from one cloud to another, and actually the process earlier started by shipping applications from being on their own GCE or stuff like this to GKE, which is Kubernese native, and then move them to EKS, which helped us move through the way. So the migration here is actually giving us a tool kit that we can replicate across all of the projects and we can ensure that all of the product engineering team using the same patterns over and over from a compute layer, and that was our first step. But from here we're going to talk about database migration. So moving the computer is one step, moving the data is another step. So for that I'll pass it to Helene to start talking about the database migration. Thank you, Emma. I do have a question though for you. So when you're doing the database migration and moving the applications, how much downtime can we, uh, you know, have? Like for a news company, for a games company, for all of the things that we do, I don't think we can afford downtime. Like I've never seen a banner on the website say sorry, we are down for maintenance, or for the game say we cannot give you stats right now. So I don't believe downtime is an option. Blind downtime is never an option for us. Interruptions happen all the way, but we have to plan for zero downtime when we do any migration. Thank you, thank you, and that was. The problem statement that was given to us that the migration The applications should not be aware that migration is happening underneath and Because The New York Times is the world's one of the most premier media companies, they do not have maintenance windows as Amed mentioned, and it is also the mission of The New York Times to make available the news and whatever is happening in the current affairs, um, whether there's a spike in Internet, anything that is happening, any implementation of any technical changes is happening, it cannot have any downtime. So not only did we have to engineer a migration, but we needed to engineer a migration where. The applications would continue seamless without any issues, so it was not enough just to get. There we also needed to make sure that no one noticed. So that was the challenge that we faced when we planned for the database migration. Let's talk about NoSQL blessed path. So we did create a pattern for which, uh, we had to do the database migration from GCB data store to AWS Dynamo TV. We had to move the application from GCP to AWS with no downtime, as Emma mentioned. Uh, we also had to keep the data in sync between GCP and AWS while the migration was going on. And the data that needed to be migrated between GCB Data Store to AWS Dynamo DB. Had to be in sync and had to be migrated live. We also had to create a mechanism to do a cut over to AWS once the migration ended and not only did we have to do the forward migration, we also had to plan for if supposing, you know, God forbid the migration failed, we had to have a mechanism to do the fallback back to GCP if the migration was not successful and the best of all, the user should not. Be knowing about the migration and should be able to use the application while the migration is going on. So how do we get there from there? So how do we get from the state where the application is running on GCP and is using, uh, the GCP data store? How do we get from that state to the end state where the application is going to run on AWS? The data is going to be on Dynamo DB, and as I remember, there is no downtime thanks Emma. I keep on saying that because it was a very interesting problem that we had to solve. All right, so let's look at the start stage. So as I mentioned, the application is running in, is containerized. It is running on, um, GCP and it is also pumping data into data store for the persistence layer. So double right comes into the picture. That is basically the main hero of our story. So how does this work? So the application that is running on GCP has to write data to both GCP Data Store as well as on the AWS Dynamo DB because the idea is to maintain the current state of the database. The data has to be in sync. All the active transactions should be same both on GCP data store as well as on the AWS Dynamo TV. So the application is written to PubSub, and for the Pub sub, we are, we have created one topic per table for the data store piece, and we are using PubSub to distribute the data to the new target, which is on the AWS site. We have to migrate the data reliably, and we have to do it live, and PubSub actually will guarantee the data reliability and will help us in avoiding the downtime. So what this mechanism does is it syncs the data between GCP as well as AWS and the data is, even though the application is running on GCP, the data is being written both to GCP and AWS simultaneously to avoid any inconsistency or any discrepancy in any of the live application traffic that is coming. And on the AWS side. There's a job running which will take it to AWS Lambda and Lambda is actually going to change or do some processing on the data that is coming from GCP and it's actually going to create it into the format which is compatible with Amazon Dynamo TV and it's going to put the data into Amazon Dynamo TV. So this was the first piece of the Ala of the pie where we are actually writing double rights but on the GCP side to make sure that the current application, the current data, and the current state of the subscriber or whatever data that is being migrated as of now is current and there's no discrepancy and inconsistency between GCP data store as well as AWS Dynamo DB. So now what do we do with the historical data because there is a lot of historical data which is there stored in data store on GCP because the application has been running on GCP so there is a one time bulk upload that is done from data store to AWS Dynamo DV and the way we do is. From, uh, from the GCP data store, the data is actually pushed onto BigQuery. And the reason we push it to BigQuery is because the data is stored in a binary proprietary format. And from BigQuery, there is a job that is going to pull the data and store it in the S3, which is compatible to, uh, which is compatible to the data that is going to fit in the Dynamo DB database model that we have given it. Then what happens is we have a glue job running. Gluejob will do the transformation of the data, and it's going to export the transformed data and it's also going to push it into the Amazon S3 bucket from where the data is populated into Amazon Dynamo DP. So this is how we are going to do the historical data pushback. So at this stage you have the active. Data, the current data, which is already in sync between GCP, uh, data store as well as AWS Dynamo TV, and at the same time, the historical data is being pushed from GCP data store into BigQuery into S3 bucket doing the ETL with glue and is going to be pushed into Dynamo DB. So this historic backfill is a one-time operation. At this stage, once the historical backfill is done, the data should be in sync between GCP data store as well as AWS Dynamo DB. So now we have the application which did the double rights on GCP. We have the data on both GCP, uh, uh, and also AWS. Now, how do we do the roll? How do we do the, the rolling cutover because we want to make sure the application is up and running. So what we do is we start the application on the AWS side. So whatever we did on the GCP side, we are going to replicate the same thing on the AWS side. So once the traffic is moved from GCP to AWS. The, uh, the application on the AWS side is not only writing the data onto the Amazon Dynamo DB, it's also going to push the data back. If you look, it goes and it does the, uh, the replication of the data towards GCP, goes to BigQuery, goes to data store. Making sure that the application is doing a double ride, but it is doing from the AWS site to both Dynamo DB as well as on the GCP data store. And the reason why we are doing this is because if for some reason there is an issue with the migration, if the migration fails, then we have a chance to roll back and we can go back to GCP and uh. The user will not know, uh, so this was an insurance that we kind of did for the failover if it happens, uh, so this was the plan that we did for, uh, the rolling cutover. So now we have the application running on AWS. We have the data being synced from AWS to GCP. The current state of the data is being maintained both on AWS as well as on the GCP site. And here comes the word which I've been struggling with ever since I did that. It's uh qui quis. Thank you, Glen. And he made my life difficult because he will not change the word. It's like, you know, there's the word that everybody stumbles on. So this has been my nemesis QS QS. Sorry for that. So this is like a pause period, like a peaceful time period, like a coup uh period where, you know, you let things settle down. You make sure the transactions are being processed properly on the AWS side. You make sure the migration is successful. You're doing some sanity testing. You are making sure there is no issues being reported by the users. You are making sure your KPIs, your metrics, everything is working. And so this is the QS time and this is what we gave after the migration and the cutover was over. So now with the successful migration of the data between GCP and AWS, now it is time to decommission. So we are going to decommission. The services and the workflows that were there on GCP and because now the application is running and it is running on AWS and it is running. Still running, so. All right, I'm going to hand it back to Amad to conclude the session and uh also share some of the wonderful statistics that we collected. Thank you, Arlene. So again, the migration is complete, but like the migration in its own, it's not what we're looking for. We are actually looking for to build standards and library across the board. So that's why we found like it's really important to focus on like how to paralyze all of that effort and build that library faster enough because teams will innovate. We need them to do that indifferentiated work, so. All of that work that they are going to start to do to build the quality product, we need to like make it move faster with our migration and I'm from a developer platform mission, so my focus is an engineer. So when I look at the outcome for something like that, actually having engineering teams is my reality, is our reality. We want to make sure that the tools we are building is something that they are actually using it helps them. To build all of the stuff that they wanna build and we are gonna focus on the plumbing, but when we start as a platform team focused on the plumbing, who helps us? So that's where we go to a partner like AWS and work with them on all of the things that they deliver. So we can go a couple of ways here. I can go build my Kubernes cluster. I'm a very Kubernese savvy, and then I can start like. Head my head around EtsyD and how to manage it on an EC2 instance and how to keep my file or I can run it on ETS. I can do the same for other services. So should I focus as a platform team on building the things that it matters, on building the tooling that matters for my product engineering team, or should I like actually start have more managed service for AWS to work with and this gives us all. So all of the nines and the availabilities that we need across regions so we don't have to keep doing the work as a platform ourselves and then like focus more into what other innovations that we need to build for our product engineering teams and it's not just about the platform, it's about the process, the governance, the standard, everything that we ship as a whole, so that's how the people come together to build their services. Another thing that we want to focus on is. When we know exactly how we are doing things, it becomes much easier. So one of the things that I have seen before, someone have an idea, they want to go to be a production ready, but they want to be a production ready with all of the standards baked in place. So that would take them time to test all of the processes they built. So our platform gives them all of that out of the box, so they would think about exactly their business logics, what they are shipping. Here we're finding a reduction of 90% of service creation time. So when they start, look, I have the idea, it's ready to go to production safely with all of the standards shipped in and baked into the process and the platform itself. Another thing when like when you work at scale, it's different from you focus on a couple of services. So if I have a service and I'm trying to optimize for like, hm, I wanna. Improve latency on that service and then like I have to improve like 5%. Sometimes the investment doesn't make sense to focus on the small thing because like sure I will spend too much time trying to improve that on a very smaller scale. What I'm gonna achieve from that like 10 milliseconds, 150 milliseconds depends like when we look at scale and then we have many services we look at from our platform. Perspective, if we have done a complete project, for example, a couple of projects we've done here to improve latency, one we've done is cloud interconnectivity between GCB and AWS that cuts all of the traffic, makes it on the internal network that saves us actually not just latency, but also saves us on the cost because now we don't have to regress to the internet. Things like when we have to do double rights, when we have to go from AWS. To GCP or vice versa, all of that is done internally on the internet on the solutions that we build. Another thing like we may look into something like STO or another feature in the platform when we improve on it, just like focus on like removing one core from an application is different from removing one core from 1000 applications, from 100,000 applications. The scales here matter, so. All of this happens when we start to focus on what is exactly are we building as a platform team for our customers and our customers are actually a product engineering teams, so I'm building something for them on that scale with a partner that helps us build that library to make it available for all of our engineers so they can focus on the problem and build more better products and we can deliver the news at any scale at any time. Was that Thank you and if you have any questions, let us know. Thank you. Really quickly before we go to questions, one of the things that I want to point out is that when we went through the solution for the database migration, there was a step that we left out due to time. The how do we get here from here, there from here. One of the things that we left out, and this is where a partnership comes in, is data store is not an exact match to Dynamo DB. So Harlene, what are some of the requirements that the New York Times gave us like transaction rates? Transaction rates were there, uh, there was one other issue. How do you handle large objects? Because one of the large objects, uh, because Dynamo DB does not take in an object which is greater than 1 MB. So there were a couple of uh strategies that we did. Uh, one was flattening it out and just taking the fields that were actually required and that were being mapped to the Dynamo DB. Uh, second was you can always have like a pointer within Dynamo DV, but the actual record is actually stored in S3 bucket for lookup. Or you can actually break it down into chunks, and all those chunks can come together with one particular key. So those are some of the technical challenges that we also faced. Uh, one of the other things was map object was not easily mappable into Dynamo DB we don't have anything called map on Dynamo. So just don't, just so you don't think by looking at the cleanly structured diagrams and the smooth flow of the presentation. That everything just happened. There was a significant amount of planning that went into this, and that is the partnership that we talked about at the beginning. So I just wanted to go under the covers a little bit and expose some of the rough edges that AWS helped smooth over during the process of this migration. All right, and now, for real this time, questions.