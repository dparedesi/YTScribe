---
video_id: sx3mvBQtaX8
video_url: https://www.youtube.com/watch?v=sx3mvBQtaX8
is_generated: False
is_translatable: True
summary: "This presentation by Joel Miller, Staff Software Architect at DraftKings, details how the company engineered its financial ledger to handle extreme scale—specifically, \"a million financial operations in a minute\"—using Amazon Aurora MySQL. The talk focuses on the architectural decisions and performance tuning required to support the massive, bursty traffic patterns of sports betting, daily fantasy, and iGaming.\n\nThe core challenge DraftKings faces is the \"speed of sports.\" Customer engagement mirrors the tempo of live games: pre-game, there is a surge of deposits; once the game starts, debits spike as wagers are placed; during the game, big plays trigger massive read traffic as users check scores and balances; and finally, post-game settlement creates the largest load, with transaction volume jumping 30x as payouts are processed. The system must update customer wallets instantly to keep them engaged for the next bet or contest.\n\nDraftKings' architecture relies on a microservices approach running on Amazon EKS. Synchronous debit requests and asynchronous payout instructions (via a message broker) all converge on the ledger service. To scale the database layer, DraftKings sharded its ledger using consistent hashing on user identifiers across multiple Aurora MySQL clusters. This horizontal scaling provided linear throughput increases that vertical scaling could no longer match at the top end. Surprisingly, this approach also reduced costs (multiple smaller instances were cheaper than one massive 24xlarge instance) and improved data warehouse latency by parallelizing Change Data Capture (CDC) ingestion across five binlogs instead of one.\n\nA critical component of their success is Aurora's read replication. With typically under 15ms of replication lag, DraftKings offloads massive read traffic (balance checks on every screen view) to up to 15 read replicas per cluster. This \"Command Query Separation\" ensures that essential write operations—deposits and payouts—are never blocked by the flood of read requests.\n\nJoel also shares specific performance tuning lessons. He emphasizes utilizing Performance Insights to identify *why* procedures are slow (locks, table scans). A key optimization involved refactoring stored procedures to return scalar values via `OUT` parameters instead of selecting tabular result sets, which significantly improved throughput by reducing temp table overhead. The talk concludes with the imperative advice to strictly separate reads from writes to protect the primary writer and ensure resilience during high-stakes sporting events."\nkeywords: DraftKings, Amazon Aurora MySQL, Financial Ledger, Database Sharding, Read Replicas, High Scale, Command Query Separation, Performance Tuning, Sports Betting Technology, Kubernetes
---

Thanks, Claude. Have you ever considered what it would take to be able to run a million financial operations in a minute? At DraftKings we have to be familiar with this kind of scale and support it on the regular. I'm Joel Miller. I'm a staff software architect at DraftKings, and what I want to share with you for the next few minutes is about how we built the DraftKings financial ledger on Amazon Aurora MySQL and how that enables us to get to the scale that we need. But first, just a very brief bit about DraftKings if you're unfamiliar. DraftKings is a sports entertainment and gaming company, and we operate most notably in online sports book, but we have several other verticals that all interact with our ledger, including i gaming, fantasy sports, and lottery. Our products are available broadly across North America, in many US states and Canadian provinces. In any given month we have millions of paying customers, but. Depending on exactly how big the sporting event can be that is driving customer traffic, we can see millions of paying customers much more quickly than that. We like to say that to be successful, we have to operate at the speed of sports. But what is the speed of sports? It might take An hour or many hours for a game to complete, but big plays happen in the blink of an eye. The interception that no one saw coming. The double play that ends the inning. The Hail Mary that ties the game. These kinds of plays have outsized impact on sport and drive customer engagement, and when fans are engaged, customers come to the DraftKings platform. So The title of this presentation's all about scale, but what does scale really look like to the DraftKings ledger on an NFL Sunday? It comes really in 4 big areas. So the first thing to say is that we operate real money products in order to Engage with the customer. The customer has to bring money on site. What you're looking at right now is a graph of deposit volume in the lead up to early games on NFL. As you can see, the farther out you are, the lower the scale is on a just a transaction by transaction basis. And as you get towards the right side of the graph, you're looking at the very beginning of the games start, and that's when traffic really starts to ramp pre-game. Customers are coming to the site depositing funds. We're working with our payment providers, but ultimately all that has to hit the ledger. We have to update balances. We have to write transactions. So after the money is on site, Where the next thing is you have to be able to use it. This is a graph of debit traffic also in the lead up to game starts. What you can notice here is that this traffic, this graph is highly correlated to deposits. Makes a certain amount of logical sense, right? The way that the after you bring money on site, you simply want to be able to use it. But from the ledger's perspective, we've just compounded the problem, right? When deposits are busy, debits are busy, right? Everyone's trying to get their wager in, they want to enter into a fantasy contest. The ledger has to be able to support that scale pre-match, but surely. Once the game started, we have no problem, right? Well, not exactly. In-game, we have a sawtooth-like pattern problem. What you're looking at here is not write traffic at this point, but now read traffic. Well, I said that big plays have outsized impact and they drive customer traffic to the site. Well, any time a customer comes to the site, it needs to load up their balance. They need to know what's usable to them and what's not usable to them. You can see that in the top here, every single screen, as they move screens, go around the app, you can see that the balance always gets checked. And if you have that highly drafted, very popular quarterback throws a touchdown pass to a highly drafted, very popular receiver, everyone in the country at the same time is opening up their fantasy app, and they want to see, hey, what's the score, right? They also want to be able to see if they've got a wager. Is the wager, you know, part of a leg that's paying out, or is it settled at this point? Postgame we have the biggest spike problem for DraftKings. The last graph you see here is after the game is done, statistics are finalized, and suddenly every every sport related system at DraftKings knows how many people it wants to pay out. And they want to pay them out at the same time. We have very real business metrics about this. We have to be able to get money back into customers' wallets as quickly as possible once an outcome is known. Here you're looking at just one. Of the verticals. Most times for luck perhaps might have it, sports book and fantasy pay out at slightly different minutes than each other just because the fantasy scoring process is a little bit harder and takes a little bit longer. But even with that, you can see that the background level of transactions goes up by about 30x once either one of them wants to start paying out. This in a lot of ways is the biggest scale problem, where at one minute notice, we have to be able to handle all of that volume on the ledger. So enough about the problem. How did we actually build the DraftKings financial ledger? On the left hand side you see the line of business services, right? I put a few of them on there lightly labeled. They're all running in Kubernetes in various locations, mostly on EKS, and they're doing synchronous debit traffic to the ledger in the center. Ledger is another service running on top of EKS. The ledger service is also, if you look at the top of the graph here. Receiving asynchronous payment instructions from all of the payment services themselves. When when I said that fantasy and sports book payout, they don't do so synchronously. They drop messages onto a message broker, and the ledger is also listening to that broker in order to pay out those customers. At the bottom of the graph, you simply see DraftKings customers. Like I said, every time they're in the app, they're loading balance queries, that's also directing traffic to the ledger on EKS. But the real magic here is on the right side of the graph. A ledger is only as good as its data. And the Financial ledger is built on top of Amazon Aurora MySQL. On the far right side you can see any given cluster here, and I'll go back to the clusters in a minute, but very importantly, every cluster has read replication. In order to handle the scale of read traffic, you have to free up your writer to do what it's good at, and you have to make sure that the writer is doing rights and the reads are directed over to read replicas. This is very easy to do with Aurora MySQL. I think you can even scale out read replicas to something like 15. It's incredible and easily distributes your read load so that it doesn't impact your writer. But you might notice the graph looks a little bit weird. There are multiple boxes on the financial ledger. We got great results from a single Aurora cluster, but as the business grew, we had to decide how we were going to continue allowing the ledger to scale to all of our customers on all of our products. To that end, we sharded the ledger. We're still running on Aurora MySQL, but what happens now is we do a consistent hash of a user identifier to be able to direct users to one cluster or another. In order to support the traffic in a distributed fashion, when we were optimizing the ledger to be able to continually handle growth, there are times where we simply decided to scale up vertically the size of the cluster. This is great. This is easy, and it works incredibly well, but ultimately, once you start getting to the edges here, vertical increases in size linearly don't drive linear increases in throughput. What does drive linear increases in throughput are actually more writers. This was done specifically to be able to enhance the throughput, but it had some ancillary benefits for the company. First one being cost reduction. It's actually cheaper to run a bunch of 4 XLs than it was to run a single 24 XL. That gets even a little bit better as you talk about the clusters and read replicas, and if you're sizing all of your read replicas to be the same as your writer, this increases your cost savings. Our second piece, which we were not expecting honestly but really did help, was on the warehousing side. Every piece of data at DraftKings and the ledger is no exception, it is in fact the rule here has to be warehoused, so that we can use it later for analytics purposes and for data validations and other things. Our warehouse during big games could start to get extremely latent. It was doing CDC replication, still is doing CDC replication from our Aurora MySQL clusters. When we decided to go shard the ledger. The speed of data getting to our warehouse also increased significantly, in fact. That was because we suddenly had 5 bin logs instead of 1 bin log to read. We could parallellyze that data ingestion to the warehouse, and the warehousing became faster. But it's very easy to get bin log data out of MySQL, and Aurora MySQL helps us with that. Aurora MySQL helps us with a lot of pieces of this, to be honestly. It's hard to imagine building it on another tech at this point. Aurora MySQL is able to scale in, out, up and down rapidly. If we're in a situation where suddenly read traffic is knocking, knocking us over, latency is going up, our metrics aren't looking good. We can scale out reed replicas in the blink of an eye. They come up quickly and are already synchronized with the dataset because of the underlying storage mechanics of MySQL of Aurora. Beyond that, the other really important piece about the reed replication is that it's extremely low latency. In order of magnitude better than other things that we've tested. Generally, read replica latency on all of our read replicas on our cluster is under 15 milliseconds. There's no perceptible difference if you do any kind of immediate read after write, it has to be really, really immediate in order to throw off for the read replica not to be up to date. From our standpoint, if you deposit, then go to the next page, the deposit confirmation page, and load it up. It's already going to have your balance, even though we're reading that balance from a read replica. It's simply fast enough that we can't beat it. The other important piece here from the managed service side is we don't have to do too much to it. There's no OS patching that we have to deal with. We don't have to expand the storage and. It's all handled for us. Including automatic failovers. Hardware fails. Hardware will always fail. All you can do is react. Previously, before we used this product, our reaction time was good. But it was on the order of minutes. If we had lost a writer and we had to promote a reed replica up. It took time. We had noticeable outage. Automatic failovers from Aurora My SQL. Having made that a seamless process. I'm not going to say you won't notice it happening, but It'll be much quicker than you can do on your own. So what did we learn in this journey? Optimization to be able to hit. 1 million operations a minute. is complicated but involves data. And involves testing. One of the first things that I think is important to know is what's slow. The slow things are probably hurting your throughput, and I don't want to get into an argument about latency versus throughput, but they are correlated. Profiling is key to figure that out. It's one thing to know that your stored procedures are slow. It's in fact another thing to know why they're slow, because that's how you fix it. Performance insights, which is now I think Cloudwatch database insights, was invaluable to us to be able to look into a stored procedure and see what was taking the most time when it wasn't working the way that we thought it was. Was it locking? Was it not using an index, table scanning? Was it simply improperly set up data? These things are available to you and you need to be able to see them in order to optimize. One of I think the most interesting optimizations that we had to do, and I did not expect was getting data out of stored procedures can be done multiple ways, and some ways are better than others. We typically perform selects at the end of stored procedures and return data in a tabular format back to the caller. This ended up actually hurting throughput significantly as opposed to taking scalar data and sending it back throughout parameters. Our best understanding at this point has to do with temp table usage, to be honest with you, but I'm happy to talk about it later. The key thing to remember is that if you've got scalar returns out of a stored procedure, put it in and out parameter. Your stored, the amount of calls that you can make to that stored procedure, the throughput will increase greatly. But the last thing that I really want to hit home about. Is command query separation. I mentioned it once already, and it's really important to drive home to be able to handle scale. You need to separate reeds from rights, and especially with the ease in which you can spin up reed replicas on Aurora MySQL. It's important that you do and that your systems know when they're calling, when they are making a read-only query or when they're trying to mutate data. And when you can direct those read-only queries to read replicas. There's no chance of you bogging down your writer in either CPU capacity or something's throwing down a weird lock that's preventing your updates. It simply can't happen if it's happening on a read replica and not the writer. It's extremely important, and if there's one thing you take away from this presentation, it's separate reads from rights to handle scale. That's what I've got for you. I'm happy to stick around afterwards and answer any questions you have, but thank you for sharing your time with me.
