---
video_id: tMvwM_KXNPA
video_url: https://www.youtube.com/watch?v=tMvwM_KXNPA
is_generated: False
is_translatable: True
---

Excellent. Well, good afternoon, everybody, and welcome to Reinvent. Everyone having a good time so far. Enjoying the keynotes this morning, some cool things. Excellent. Well, welcome to best practices for serverless developers. But you know what, actually forget the serveless part. This is actually for all developers building in the cloud. A surus is really the default way to get the most out of the cloud when building applications. My name is Julie. I'm a developer advocate here for Servus, specializing in Servus at AWS. We've got lovely people in the room. We've got some overflow rooms in other parts of, uh, Las Vegas at other hotels. So hello to you all virtually. And if you're watching this from the YouTube recording, which will be posted shortly, I suppose hello into the future. So thank you everybody for joining me. Today I'm going to take you through a story that actually mirrors the surplus evolution of, I don't know, thousands of companies. And in this talk, we're going to follow Emily's transformation from a single patisserie to international success. And I promise you there's nothing that I like more than a delicious pastry. It's absolutely my weakness and I'm just thinking I should have actually brought one now. So the serviceless best practices I'm gonna teach you is gonna help to enable this incredible growth and understand the real-world scaling challenges that every developer is going to face. And you can see the evolution from just getting something out there to a mature server-less architecture. You can see here, 5 years of growth, both, both, both business growth and architectural growth and evolution. Each phase brought new challenges and also there were obviously lessons. Notice the dramatic scaling from 40 orders to 320,000 orders per month, a huge scale, and the architecture had to support this growth, and we're going to see how serveless patterns help this transformation. So things started, uh, Emily opened during lockdown following her absolute baking passion. A single lambda function handled everything, just 200 lines of code, basic diamondom DB table for object storage, uh, for database storage, uh, for orders, and then manual payment processing via phone call. You know, 40 orders per month from local neighborhood customers, a great sort of minimal viable architecture with basic lambda functions, great way to get started. Obviously, the pay per use model is perfect for uncertain demand and you know, zero infrastructure management during a crisis was perfect as well. 2021, the simple architecture still holds, you know, with some splitting of lambda functions for payment and inventory tracking, a good idea. And then, you know, the first scaling challenges start to emerge. Lambda timeouts during the morning rush, so a performance degradation during peak hours, and, you know, something that's always hard to troubleshoot and team coordination issues too. 2022, more locations, many more orders. Now 42 lambda functions, complex dependency chains and not enough error handling. The morning rush, cascading failures, spiking customer complaints, performance suffering, 6+ 2nd order processing. That's crazy. 3% failure rate during peak hours. Oh no, a breaking point that forces architectural transformation. And so in 20 with customers getting frustrated with the slow service. And in 2024 was the year to get down and fix things. A proper server gradual architecture sort out, bringing in step functions for orchestration, event bridge for events, SQS for reliable processing, and reducing 42 lambda functions down to just 10, 6 seconds down to 400 milliseconds processing time, and a foundation set for some cool international expansion. And it all made a difference for global expansion. 60 locations across Europe and North America, uh, 3.8 million orders per year with great performance and liability. Handling 40 times the holiday traffic spikes seamlessly and a 35% cost reduction despite 3 times the growth. Emily's passion project goes to international success. And so we're gonna follow Emily's transformation through a number of different areas, show the problems, some solutions, and some of the lessons learned. And I'm gonna give you actionable insights that you can apply to your own serviceless applications. Now, I'm gonna warn you there's going to be a lot here. I'm gonna go both broad and deep, so there's gonna be a huge amount of info. And I've included this QR code, which is a link to a handy resources page with the slides from the presentation and plenty of other reference links. And I'm gonna do the QR code at the end, so if you miss it, don't worry. And also if you are watching this on the recording, this is probably not one of those talks that you want to watch at double speed. Um, maybe half speed is gonna be a little easier, but I'll leave that up to you. A lot we're gonna cover. So, let's start with the foundations, how to size and organize your service applications. By 2023, what started as simple order processing had become a monolithic nightmare. A single lambda function handling orders, validation, notifications, analytics, and a whole bunch of more stuff. Deployment time. Stretching to 5 minutes. Debugging is, I suppose, like finding a croissant in a bakery full of pastries and then a holiday rush incident, a single bug in the notification service brought down all the order processing. That is bad. And this crisis forced them to learn proper lambda sizing principles. And there's some universal principles that do apply. Each lambda function should generally have one clear responsibility, avoiding a complex do it all lambda functions. And what you want to do is actually size it for what it needs to do, not just for your convenience. Partly, we'll get to that. But lambda allocates also CPU memory proportionally to memory configuration and so adding more memory improves performance and also may may introduce, may reduce costs. Package size also directly impacts cold start times, so you do want to make sure your function's small and nimble for your performance. But you can also easily swing the pendulum to the opposite extremes with too many functions or reposes or stacks or things to manage. Resist the temptation to create these sort of micro-functions prematurely. Start with cohesive functions and split only when you experience real pain. The 42 function chaos was organic growth really gone probably a bit wrong. And same for also stacks and repos. The goal is a pragmatic approach for your organizational and for your current needs, not your sort of future theoretical perfection. And this is actually probably more an architectural and an organization decision rather than just lines of code. And when you do create clear boundaries, each team can choose a runtime and tooling based on their expertise and needs. And this is unique pretty much to Surplus that you can pick and choose the different kinds of things you want to do. Maybe the orders team is gonna prefer using SAM or Java. Uh, the inventory team uses Python for maybe some great data-heavy workloads, and the customer experience team wants to use TypeScript because they want to use it for both the front end and the back end. And so your infrastructure choices can align with your team's strengths. And this extends also to domain-driven organization, where each domain can also own its own data and own its own functionality and be right size for the team, uh, right size for what a team can effectively manage by itself. And if you're thinking and you're building serverless apps, just use a framework. It doesn't actually really matter which one, pick the one that works for you. There are many third-party ones, or we've got SAM, which is an extension of cloud formation with some simplified syntax, or CDK to create cloud formation in programming languages. Just use a framework, it's just gonna make your life so much easier. Many people also overcomplicate repos too, you know, please don't have a repo for for each function. I beg you, please, that's gonna be a nightmare. And you can easily have a single repo for many services with separation between, uh, you know, some domain-specific and also some shared infrastructure. You know, the decision on size for actual repos is also all about manageability. So Amni's transformation was thinking about sizing, making a function smaller, reducing the cold starts, deployment times, and also test runs. 40% lower compute costs through right sizing. Right sizing is the key. And the key principle being keep it big until it hurts. Right sizing for workloads organized by business domain and you can enable team autonomy in your technological choices. So next up, you know, Emily learned that too much sync and not enough async was killing performance during the morning rush from sort of blocking chains to event-driven flow. 2023, synchronous architecture starting to create some nightmares during peak hours. The morning rush brought cascading system failures and of course frustrated customers. The synchronous chain meant one slow payment API call, for example, blocked everything downstream. Business impact, not good. 23% of orders abandoned, you know, a huge increase in complaints, a revenue loss during the highest traffic periods. This crisis formed forced them to understand that the fundamental problems with synchronous architectures. The sequential weight train, if you look through the mermaid diagram here, means every step must finish before you get a response. Everything had to complete uh in lambda functions before the customer got a confirmation. And so one slow service affected the entire workflow. And um it's all waiting, waiting and waiting. You know, long chains can also um approach API gateway and even lambda's timeout limits, and customers abandon orders while waiting for confirmation. And this map can manifest itself in two ways. You can have, you know, one big lambda function doing absolutely everything, and it becomes brittle and tough to manage, and also you're paying for the waiting, waiting time within a lambda function. Well, actually, I think probably even worse is multiple lambda functions in series, each one a fragile cascading chain. If a third-party payment service has a failure, for example, the rest just stops. A poor user experience and tough to come back from. So Emily and her team did their research and realized an event-driven design would help, using an event bus for service to service calls, stopping direct service dependencies, which allowed for loose coupling, some independent scaling, and means failures in one system don't break the others. And you can still use async callbacks to update the original caller when the work does complete. Which means bringing in some other of our serverless friends like step functions, eventbridge, SQS, and even sync events, which is which is great for the front-end notifications actually. And what's the mantra forerless? You want to use the best service for the job. And so the customer gets an immediate order confirmation instead of waiting for absolutely everything. And then EventBridge decouples all the services. Each service processes asynchronously and publishes its own events well done. And AppSync events provides real-time updates back to the client as work completes and so the customer is gonna see the various progresses, you know, payment confirmed, kitchen preparing and ready for pickups. And you get the best of both worlds. You get that immediate response back to your client plus real-time progress updates. You get resilience, the payment delays don't affect inventory or notifications, scalability, each service can scale independently based on demand and the clear event trail shows exactly um where issues are occurring. And another thing to bring in for asynchronous processing is lambda's event source mapping. One of the best use cases for lambda async processing from a number of event sources was the direct lambda integrations. An event source mapping is a resource that gets events from a source and ultimately sends the event to lambda for processing. Uh, the lambda service polls event sources automatically. Uh, there's some content-based filtering which can reduce invocations for events you don't need to process. Um, the ESM can batch or group records together for efficient processing and then ultimately invoke the lambda function with the batched records. And there's actually an architectural difference between two types of event sources, queues and streams, and queues are great for task processing when each message is independent and messages are deleted after the processing. Streams is more event processing when perhaps you need multiple consumers who need the same data or order matters and messages are retained for replay. And so a key decision you may have is, you know, do you have one consumer doing what work wants, maybe you're going to use a queue, or you've got multiple consumers or need some sort of replay, then you're going to think about doing a stream. And the lambda ESM provides a bunch of functionality across all of these event sources. You've got the filtering, which I mentioned, batch controls, including for streams, being able to split a batch to find a faulty record, and then choosing where to start in a stream, you know, from the beginning, from the latest stream, or at a particular time stamp. There's some retrial and failure handling options built in, some analytics for Kinesis and platform performance configuration options. So a whole bunch just called an ESM, but all of the functionality that is built in. So, there's the performance configuration options. And so Emily needed a message buffer during traffic spikes to prevent system overload. And so SQS provides perfect decoupling between producers and consumers. Lambda ESM automatically pulls SQS and then scales based on the queue depth and messages are then going to be consumed once and then deleted after a successful processing. And SUQS just automatically handles the complexity of any of the message durability and delivery. SQS is an incredible service. And there's a new mode joining Kafka for SQS called provisioned mode. And this allows you to provision event polars in advance to handle sudden spikes in traffic, which control, uh, with you, and you've got controls to be able to optimize the throughput. And there is an additional charge based on what's called an event processing unit. Um, I mean, it doesn't actually need provision mode, but this can be super useful for high throughput, low latency queue polling. So let's look at some of the best practices for SQSQ configuration. You want to set the visibility timeout. I nearly said invisibility timeout, the visibility timeout to at least 6 times the lambda function. So messages are available during processing during retries. I suggest also setting the redrive policy for the SQSQ to, you know, 3 or probably 5 to also give more chances for successful delivery. And then for message retention, make sure it's long enough to handle any possible break in the system. This is not just in your ESM but maybe in your whole company or maybe in the whole region, so messages aren't lost during uh some downtime. And then you must set a DLQ, which is a dead letter Q, so any messages that lambda can't process are sent to this other queue and then are not lost. And then you want also some process to be able to recover these messages from the DLQ and be able to replay them into the main SQSQ. And from the Lambda ESM side, you want to also ensure that you can process messages and also protect downstream sources from a surge. That is the point of having a queue. For processing messages, the lambda on-demand ESM can scale up to 1250 concurrency, concurrent invokes per event source and then use provision mode, as I said, if you want faster scaling and also up to 16 times higher concurrency, 2016 times higher concurrency, that's 20,000. That's an impressive throughput which you can use for your SQSQs with lambda. Filtering also allows you to only process the messages you need. You can use positive filtering to pick which messages you do want to process, also negative filtering to, um, for a flexible query to say which messages you don't want to process. And this is a very powerful thing, saves you money, saves you time, and it also uses the EventBridge syntax for filtering, so it's also consistency. And also mentions obviously this can save you a bunch of money and using this, Emily was actually able to reduce costs for one of the ESMs for 92% by just using filtering. Batch sizes, I recommend just tend to get started. You may as well start with that. It does mean fewer invokes and still decent throughput and because the larger the batch size, the fewer invokes, and so that's one thing you need to think about. Uh, see how you go, but you know, batch sizes can go up to 10,000, which is a huge batch size, and that can be super useful in some of your scenarios. The batch window here is then to improve the latency when you actually don't have much traffic. So you get to process messages before a batch number is formed. So you can use these two, different settings for different use cases. Uh, and then also, um, you've got the partial batch, the report batch item failures. So if you do have a faulty record in your processing, uh, you can return a list of failed records back to SQS and SQS is then gonna delete the process messages, which is more efficient rather than having to retry an entire batch. So then to control the flow control. This is the rate at which lambda consumes the messages, you want to set the max concurrency on the ESM and this is gonna control how many concurrent invocations the ESM attempts to send to lambda, which prevents overwhelming downstream services. These could be things like databases or APIs or any third-party services which can only handle a certain amount of throughput. And this is actually the all-important buffering control system. And then land reserve concurrency is a separate setting which you can set with the lambda function level, and this actually reserves capacity for this function, ensuring it can scale up when needed within the account concurrency. And so you want to rather use max concurrency to manage the buffer, but if you do want to use both together, make sure that your reserve concurrency is higher than the maxic concurrency to prevent throttling. So do two different, two different settings for two different things and you can use them together. And the last bit is for more error handling. So this is configuring lambda on failure destinations for function invoke issues. And then you may be thinking, so, oh, why are there actually two different error handling parts? Well, the SQS DLQ captures messages that fail repeatedly during polling or processing, and that is on the SQSgu, while lambda on failure destinations captures invocation failures like network issues, throttling, or maybe even if you deleted a function or you've got some kind of IM issue. And so both serve different processes and should be used together for comprehensive error handling. So moving to async is a powerful architecture that actually should really be your default choice in my opinion. Avoiding synchronous chains that can go wrong. You wanna stir, you wanna store first, you wanna reply quickly for better user experience and then process things later. And this is a very powerful, resilient, and stable model. Understanding also when to use queues versus streams. And async also means better failure isolation, but you then do need to think about retries and not losing any messages. So next, Emily learned how to avoid unnecessary work through using step functions, orchestration, and some direct service integrations. 2024, Emily had successfully moved over to async architecture but created a new problem. 42 lambda functions, as we said before, including many doing just simple data transformations and routing. That means high compute costs for operations that didn't require any sort of custom business logic, cold starts affecting performance for basically crud database updates and complex failure scenarios across multiple function invocations. And this led to the discovery that configuration as code could replace many lambda functions. You basically want to avoid lambda when you need to use native capabilities in other services, less code to manage. I don't know about your code, my code, I want less code of that to manage. Your code is probably great, so you may be good with that. But for things like simple data transformations, direct service to service calls, only routing data from one place to another, or simple data enrichment, you don't need to use lambda. Uh, if you do have lambda functions that serve only as a proxy, for example, I'm gonna go through some direct integrations between API gateway and downstream service, you can optimize that. So you can see on the right, API gateway can actually connect directly to multiple AWS services such as Dynamo, SQS step functions, and many more. Once again, no need to use lambda just as a proxy, and I understand that VTL is a bit of a pain to test, but also once you do have it configured, it's done forever. And for a simple example like this DynamiDB get an item request, it's easy, cheap, and it's fast. And more great opportunities to remove code. One common pattern that a lot of customers use is using lambda to capture Dynamo DB change events or change events from change data capture events from a lot of other services if they support it. Lambda then passes the events and then sends them to another service, an event bus or API for some other downstream processing. Event bridge pipes, well, this provides built-in transformation capabilities. Jason Path provides a powerful data manipulation and filtering only needed events. You can enrich data as part of the pipe, do data format conversions, transformations. You can also use step functions and you can also use lambda as part of this, um, uh, the step as well. And then, uh, it's gonna batch it up for efficient downstream processing. And this is perfect for simple data transformations that don't necessarily require require complex business logic. This is it just if you're using the pipe by itself without step function event bridge. But yeah, this is, you know, reduces your lambda invocations for basic event processing and it's really simple to set up. But actually, the big winner in Emily's use case is using step functions. Being able to remove a bunch of lambda functions for direct service calls. You can see all of the AW's SDK actions are available. So in effect, there are thousands and thousands of integrations. And here's the configt to update inventory in Dynamo. Just a simple configuration call that you put in a step functions, uh, um, state, and it's really easy to call and there's a whole bunch of stuff around it with no cold states, no cold starts, and a safe safe retries are also built-in. And it's also built-in Json out support with loads of these functions including um some step functions specific ones like generating a UID. So this is super useful. You want to count things, you wanna, you know, do up some numbers, the length and all these kind of things. No need to have a lambda function. You can just use pretty much free Jasonnato expressions to be able to do it. So these are very powerful ways to build up a lot of functionality without having to maintain any of your custom code. In many scenarios, step functions is then used in conjunction with event bridge, and step functions is great probably within a domain microservice, and then the pattern is to finish what the service needs to do, and that's gonna be whether the, whether that is step functions, whether it's even a lambda function, and the pattern is to then emit an event onto the event bus, and this is then gonna enable the event-driven communication between different domains. Very powerful model, allows you to add as many domains as you need without being sort of trampling on each other. And if you didn't know, there are actually two different flavors of step functions. There's standard workflows which can run for up to 1 year and are asynchronous, and express workflows which are fast and furious. And these are specifically built for high throughput. And they can run for a maximum of 5 minutes and they can also be synchronous. So it's super useful for doing that synchronous calling we were talking about earlier. The pricing is different. Standard workflows are priced per state transition and express workflows are then priced by the number of executions and memory consumption. And express workflows also run in memory, so they're super fast. And the best part is, uh, you can and actually you should combine standard and express workflows. You can combine the durability of standard workflows with the speed of express workflows. And so this is perfect for scenarios where most of the process is long running, but some of the parts of your state machine need some real-time responses. And so here we can see the standard workflow is gonna run start execution and orchestrate the complete order processing life cycle with a full audit trail as part of the standard workflows and then it's gonna run the start execution and run the nested express workflow to handle real-time inventory validation in some second time. And what it can do in the Express workflow, it can do other cool step function things like running a parallel state to check the inventory and the pricing rules simultaneously. And then it's also using Dynamo DB direct service integrations. So again, no lambda cold starts. All becomes nice and nice and simple. But actually, as announced in the keynote today, there's a new kid on the block for asynchronous processing, but still within the lambda function. Lambda with durable functions being announced today. And this actually does completely change the way you think about lambda functions. You can now build workflows in your favorite programming language. You're able to use checkpointing to suspend and resume long-running operations with item potency just built in. So this is actually useful when you have a bunch of lambda code in your workflows and you can now use, uh, use the enhanced context object like I'm highlighting over here, and I'm showing you, you can use, you can run steps, you can do parallel processing, and even wait for callback all within an async lambda function, uh, and, um, within the lambda function, and you don't pay for that wait time as well, and they can run for up to a year. So if your mind's going a bit, hang on, this is all a bit strange and different for lambda, I'm with you, but this is a super powerful way, a new capability that we announced today. Step functions, of course, is still great, particularly for those great uh for those direct service integrations. But if your workflow does require a lot of your own code or you prefer just managing your workflow in code with async checkpointing and resume, this is certainly worth taking a look. And my good friends, Michael Gash and Eric Johnson are doing a dedicated talk tomorrow, and that's going to be diving deep into it. I think it may be Mandalay Bay, but I'm not entirely sure. It's CNS 380, so I have that look that hopefully should be, uh, released in the content catalog today. Uh, durable functions is gonna be awesome. Well, it is awesome. So going all async helped Emily, being able to drastically reduce lambda functions and cold starts, improving latency, better timeouts, better error handling due to reliable service integrations, and a monthly saving of $1500 just from eliminating unnecessary lambda functions. So this demonstrates the power of configuration as code over custom compute. Remember, the good old adage, the best lambda function is often the one you don't even have to write. So next up, Aly optimized lambda performance for functions that truly needed the custom logic, because lambda still is a great service when you need to use it. 2023, 35 locations, 180,000 orders monthly. The morning rush created a perfect storm of performance problems. Customers abandoned orders faster than Ammily could even bake croissants, which meant a revenue loss, and that's not good. And this was an existential business threat, not even just a technical problem. And so, quick sidebar for performance and cost, there's also another lambda kid on the block announced this week, lambda managed instances. And this actually gives you the lambda operational simplicity and surplus operational model to go with EC2's full ranges and specificity. We manage the hardware and it runs in your VPC. You can choose specialized instance based on memory or CPU or network, and then lambda is just gonna manage and handling this instance. And there's multi-currency actually built in another mind thing and another change that lambda is gonna be able to support for, for this uh model, uh, but you do need to handle this in your code. There's gonna be a lot of information coming out about how to do this. Uh, the same timeouts do apply, uh, but this can be a big cost saving for at scale workloads where you can use also your ECG pricing, uh, plans, so things like reserved instances. So this isn't for everybody. If you're at high scale and you've got really steady-state functions, it doesn't spin up as quickly and be able to, uh, burst and all that kind of thing, but at least it is an option at at high scale if you want to be able to manage your costs when you grow with lambda. Again, there's another dedicated talk on Thursday from Steven and Oshana that's going to dive deep into it, and it's worth watching to understand how concurrency changes for this model and definitely check for this one and this is in the content catalog. So let's take a look back at the normal lambda function life cycle. Lambda creates a new execution environment or for managed instances, actually, it deploys a container onto the EC2 instances. It then downloads your code or your container image, starts the runtime, and then we have to run your function handler pre-initialization code and your first invoke. And then after that point, your function is warm and ready to run the event that's been sent to it. There's actually a separation here between what you can control and what, uh, what we, what we control. And essentially everything up to the in it of the runtime is AWS's problem. And the Lambda team spends an inordinate amount of time over the years here, you know, literally shaving down to nanoseconds, trying to improve performance and trying to make everything that becomes on our side of the line faster and faster and faster. So the parts that you can control are memory allocation for performance, your initialization code, your function handler code, and then the package size. So lambda allocates CPU power proportionally to memory configurations up to 10 gig for on-demand functions and up to 30 gig for managed instance functions. Lambda also proportionally allocates CPU power based on memory, and for managed instances, you can actually configure the ratio of memory to virtual CPU that you actually want for your workload. Uh, for on-demand, once you're allocated more memory past about 1.8 gig, you can actually use more cores, so multi-threading becomes possible. And it's counterintuitive, but actually higher memory often reduces total cost. If your code is CPU bound, adding memory improves the performance and may reduce the cost. And the key is to finding the sweet sweet spot where performance gains justify memory costs. So you add more memory, it adds more CPU oomph, and so things can can be processed quickly, and that can reduce your costs. Now for on-demand, if you want to take advantage of more power with more than one CPU, you can use parallel processing within your lambda function code. So if we say here our processing a batch of records, doing this sequentially is going to take 300 milliseconds for 3 records. And you know, within your code, when you use the process record in your code, this is going to process each record at a time. But you can actually speed this up using multiple calls and running this in parallel. And so it's only gonna take 100 milliseconds for all three because you can use promises to run the code in parallel. And this is a super unlock for batch processing to get the most for your CPUs. Now, um, working out the optimal memory configuration, you know, that could be a manual process, you've got to try all the options, uh, to do it all yourself. And so lambda power tuning is an open-source tool that gives you a data-driven approach to help you visualize and fine tune the memory power of your lambda functions. It runs multiple current, current, multiple, um, current executions of your function at all the different memory allocations you pick and then measures how they perform and it runs in your accounts performing your real function calls, you know, showing the real cost and speed, and this helps you to find the right balance in an automated way. OK, so on to cold starts. Here, Emily has her figures and coal starts affect less than 1% of invocations in her production workload, and they primarily, uh, primarily occur during scaling up or after function updates. And so, you generally want to focus cold start optimization on latency, latency-sensitive user-facing workloads. Async workloads can usually tolerate some cold start latency, unless you may be using something like provision mode and you need to do a whole bunch of throughput processing. But generally, all those async processes, you know, if it takes a few milliseconds or even a second for a payment notification email to come through, uh, you know, nobody's really gonna mind. And so don't over-optimize cold starts for background processing functions. Although, you know, I did mention there's a cost when, uh, there's a cost for doing this, but you know, it isn't the biggest thing that you can do for performance. But there is a bunch of stuff that you can do to make your in it more efficient. Focus on what you can control, the package size, the imports, and the initialization logic. You only want to import specific models that you need. So you want to import, uh, avoid importing huge big SDKs when you're only gonna use a small part of it. Uh, you know, if you're using node or TypeScript or that kind of thing, you can mini miniify production code to reduce the download time, lazy initialization if you do have functions that do some multiple processes, and so can this can defer some heavy operations until they're actually needed for maybe only part of the invokes. You can also think about establishing connections during the inner phase and then reusing them during subsequent invocations when we're talking about warm starts. But of course, you do need to think and you need to make sure that you can handle reconnections in your handler to deal with any of those stale connections. Keepives also maintain persistent connections to other AWS services. And then also, you can cache reusable data, um, but also you need to think about cleaning up sensitive data. If you've got, uh, you know, subsequent invokes and you've got, you know, information from different customers or things like that, or, or secrets that pertain to different IM rolls or things, you know, all those kind of things, you do need to think about that to make your functions safe. Now, there are also a lot of code optimization strategies. I'm not gonna go through all of these, um, but, uh, because each runtime has specific optimization techniques. And really, there's nothing groundbreaking here. You know, these are just often, a lot of them are just normal code-based practices. You know, Java can benefit from using Snapstart and proper SDK usage. JavaScript or TypeScript can use modular SDKs and specifically using the version 3 of the AWS SDK and also things like tree shaking. Uh, .NET can leverage AOT, uh, compilation for faster startup, and then Python optimization is often, uh, all about your import strategy and the package size. Uh, but remember, you know, for all of these connection reuseers critical across all the runtimes to save you time. There are some native compilation options that can provide significant performance benefits. Uh, GralVM compiles Java to native executables with sub-second cold start times. .NET AOT does a similar kind of thing for .NET applications. Now, obviously, the trade-offs with this approach. Um, there may be some longer build times and some runtime limitations and a different Sort of uh runtime that you maybe need to use, but this is a useful thing that you can look at and this is, you know, often best for CPU intensive workloads with predictable execution patterns, so you can run the, run these. It does obviously require some code changes and, uh, you know, the framework compatibility considerations, but more and more frameworks are supporting them. And so this is most effective for functions that also frequently invoked or also are cold start sensitive. And we also have some platform features like provision on currency and snapstart to eliminate cold starts, uh, without any code changes, uh, without any code changes, and we'll get into that. So provision concurrency is available for all languages. You configure it then for a certain value, and then lambda effectively goes ahead and pre-warms those execution environments, runs that initialization process in advance, and then we're gonna keep those execution environments around for you. As they age out, we're gonna replace them automatically. And see, here I've turned on provision concurrency for this function for version 10. You can see the inits are running in advance and then when the invokes come in, they land on those already warmed execution environments. And so you won't see a cold start in front of these environments. And so this is helpful just before that sort of 8:00 p.m. patisserie rush, uh, from the morning rush that Emily would need it. Two other things, don't actually pay for provision currency you don't use. There's a cloud auto metric to monitor this, so don't waste any of your money. Uh, and I pushed over too fast. And then, um, also, you must use the function version alias, not dollar latest that trips people out when they wonder why is my provision concurrency not doing anything. So when to use it. This is, this is for applications that need less than 100 milliseconds of startup latency. So, uh, you need to configure the value for provision concurrency, so it's best for predictable traffic patterns which justify the cost of keeping those warm environments up and running. So, obviously, it's gonna be good for mission-critical APIs where any latency variable is unacceptable or some high traffic applications needing consistent performance during heavy periods. Now for, for implementation, I suggest you sort of need to do some research and understand your execution patterns, what times of the day do you need to pre-provision the provision concurrency, and you can just start with a static figure and then maybe evolve it into something dynamic, and configuring it using application or de-scaling, uh, for just when you need it. And that can be very efficient to be able to ramp up beforehand and ramp down afterwards. And with lambda's new, um, more recent, I suppose a year ago now, uh, scalability things, it's really quick to get provision concurrency up and running in advance. So then lambda snapstart for Java, Python, or .NET is another platform feature that runs the cold start process when you publish a function in advance, so not actually just before an invoke. And so when you need to invoke the function, it's gonna resume the snapshot and it's generally gonna do this in under a second and so it makes it perfect for cost sensitive applications with unpredictable traffic. Uh, there's no additional cost, which makes it a great workload where provision concurrency would be expensive, uh, functions with heavy runtime startup or loading heavy dependencies, they're gonna be the ones that see the biggest benefit. Java is gonna see great performance gains to, you know, having to start up the JVM. Python is gonna benefit significantly when loading big ML frameworks and .net gains from eliminating the legit compilation delays. And remember again to target a function version and you also cannot use both snapstart and provision concurrency at the same time. So, uh, what can you do to actually make that snapshot process as efficient and as possible? For Java, what you can use the before checkpoint hook to preload dependencies and initialize resources using the inner phase. And this actually uses the, uh, cracked runtime hooks, which is from an open-source project. And there are two ways, two ways to do this. You can actually use invoke priming, which does have the highest Performance, but make sure that you then uset data and your code must be item potent as you're actually running real live invokes for your lambda function to set this up. Or you can choose to use class priming and this is then gonna load classes without mix without actually the method of execution. So two different ways, uh, but you want to run as much as you can in this init process so that's gonna optimize your invokes. And for .NET, uh, it's actually one on 11 on one, wants to run multiple invokes as part of the in it using the register before snapshot, um, which is what makes .NET snapshot really effective. And, and literally you wanna run probably, you know, maybe even 10 or 20 times and this signals the net runtime to perform tiered compilation aggressively. It's sort of I suppose think of saying to the runtime, this code is hot, optimize it as aggressively as you can. And again, you know, use dummy or stub data for, uh, um, avoid side effects during warm-up because you are running real invokes. And this, you know, works with many frameworks as well. So there's some universal best practices for Snapstart. You want to re-establish network connections after restore. State isn't gonna be guaranteed. You wanna generate, generate unique IDs and secrets after the initialization, and then you obviously want to refresh credentials and timestamps then in your handler. And of course, only executed in a code paths are captured. So you want to use those runtime hooks we spoke about to warm those critical paths. And then again, you must use function versions, not the the latest, and obviously you want to test with your realistic workloads. And Cloudwatch metrics can be your friend to validate the performance improvement with your business impact. Snapshot is actually a really great, uh, you know, quick and easy win to be able to improve cold start performance. Now another optimization tip, just upgrade your runtime. Seriously, it's often one of the simplest things to do. You know, in this example, upgrading Python had a dramatic increase in latency, which improves performance and reduces cost. Uh, you know, languages are continually evolving and help keeping up to date with runtime means you also get the best performance for your buck and of course, there's a um security implication for this as well. So a systematic approach delivered dramatic improvements starting with memory right sizing, the main kind of thing, the biggest impact, the easiest implementation. Code optimization provided additional gains through better resource utilization. Cold start mitigation ensured consistent performance during traffic spikes, and native compilation provided that last little performance boost for CPU intensive workloads. So next up, how Emily built resilient systems through proper failure handling and error recovery. You know, lots of locations, 35 of them processing thousands of orders daily. That holiday rush I spoke about earlier created this sort of perfect storm of cascading failures, payment issues, a crashed order system, no notifications, manual recovery. Obviously, this is a big business impact. Lost revenue, angry customers, a confused kitchen, what's going on? So, of course, this crisis forced them to understand that resilience is also a business imperative. So with all of this, the most important thing to think about is item potency. Item potency ensures operations have the same effect whether executed once or multiple times, and this is critical for distributed systems where retries, duplicates, and out of order processing occur. Basically, I don't want to be charged twice, so please build all your item potency um operations. Um, but you do then need to think about your item potency, uh, what that item potency key is, that sort of unique transaction identifier, you know, is it gonna be an order ID or something that you can use for your item potency. And many AWS services provide built-in item potency, which you may not know about. Dynamic DB you can use conditional rights and so this is gonna be prevent duplicate operations. SQS FIFOQs can use a message duplication ID for automatically duplication. And did you actually know for step functions, you can just specify an execution name to prevent duplicates. So you've got a step functions workflow, just provide the name. If the same name has happened before and you can use that, uh, you know, maybe an order ID or that kind of thing, step functions won't run it. So simple and a lot of people don't know. And of course, lambda durable functions is another way because item potency is just part of the checkpointing part of it. And for your code, uh, Power tools for AWS Lambda, this provides, please, this is a, a, uh, a, a tool and library for .NET, for, uh, Python, for Java, and for Node and TypeScript. Um, just use this for your functions. It has a huge amount of functionality. Uh, item potency is included, um, and it actually just uses dynamo D behind the scenes. Uh, you just set it up in your code and item potency is handled for you. Super useful. So we already covered SQS DLQs and landed destinations. This is all part of your failure handling toolkit. And remember, uh, retries all are all built into step functions when you're using step functions set as configuration, part of your failure and error, um, error handling. And you have retry, you've got a catch dates, you've got choice and parallel states, and this can give you a whole lot of error handling goodness. If you're building saga patterns or other kinds of things are bunch of functionality. And so Emily's order processing workflow has comprehensive error handling at each step, um, to preventing single failures from bringing down the entire order process. And if you prefer the full code approach, you know, we've got new options now with lamb with durable functions. So next, Emily gained complete visibility into their service serviceless systems through advanced observability. You know, 35 locations, system visibility gaps across services, difficult to trace orders, gonna identify bottlenecks causing the slow processing where they're coming from, or trying to connect actually some of the technical metrics also to the business impact. The breaking point was an undetected payment issue which happened for 2 hours, resulted in, you know, hundreds of lost orders and $12,000 in revenue loss during the dinner rush. Now that's not a good thing that any that any fledgling business wants to run. No one wants to fly blind. Now, observability is a massive topic, uh, and it's in a fast-moving place and could, there are many breakouts here at Reinvent, which is all about observability. So I'm just gonna give you some of the new things that you can explore to find out, to help you out. Um, I, I love Charity Majors' work. She works for Honeycomb and she talks, uh, a lot more about more than just the pillars of observability, but a bunch of different signals, um, of data which you can use for unified observability data. And seriously, uh, read up charity Majos, uh, read everything she writes about, um, and, and talks about observability. Uh, she's an absolute guru and, uh, and amazing. So, you've got things like metrics and logs and traces and events and profiling, and they can all be part of these, these signals and, you know, other kind of business things that can come in. Observability is a big thing. Uh, but the thing I want you to take away is you want to really focus on embedding business context in all of the signals for rapid troubleshooting. So this isn't just a technical monitoring thing. This is observability and the business is very much part of it. So this shifts your thinking from not sort of only monitoring failures to being able to answer questions about individual things to understand your business. Again, Power Tools is your friend here for uh observability. It just makes things so much easier. Uh, you can import the libraries really quickly, configure up your service names, uh, and then it's easy to, to set up, uh, the logging and the metrics as well. And this is then gonna use structured logging behind the scenes and using the embedded metrics format. To create metrics from the logs entries automatically. A single log right creates both searchable logs and cloudWatch metrics, and this is also faster and cheaper. This is a great way then to incorporate that business context I was talking about like an order ID for the logs without having it as a metric dimension, which is gonna be, uh, uh, which is gonna be pricey. So here we have, we see both entries written to Cloudwatch logs in a single log stream. The regular, regular logs are queriable with CloudWatch insights for debugging and then the EMF entry lets Cloudwatch automatically um uh extract the metrics from it, a powerful 2 for 1. CloudWch Metrics also has a bunch of new stuff like powerful tag-based automation with filtering on alarming instead of configuring individual function alarm tags, and this makes it way easier. And it's cheaper. You've got one alarm instead of, you know, 50, uh, and individual alarms, uh, uh, individual alarms reduce the cloudWatch costs. Uh, there's really fancy querying capabilities is now available and a cool new dynamic dashboard where you can actually, um, build up your dashboard and then you can just select your environment for the drop-down selection and it changes all in real-time. The query generator converts natural language to CloudWatch Metrics SQL and allows you to refine existing queries, which I found so much easier. The metrics history has also been extended from 3 hours all the way up to 2 weeks for historical analysis. CloudWatch application signals is also something to look at a new observability solution which is based on open telemetry, and this can also automatically discover services and it's got a more advanced service map and a whole bunch of other things. So I can't do cloudwatch justice here and I don't want to, uh, you know, flip, flick over kind of things. So the awesome Joe Alioto has uh done a great video on all the new things and they're gonna be new things announced this week as well. So I just really suggest you take as a look cause there's a huge amount going on that's gone on in observability this year, um, which can really help you with your, with your business. So talking about Otel for uh lambda. Well, the X-ray SDK is coming to an end. X-ray is a service continues, but it's moving to Otel for instrumentation using the AWS Distro for open telemetry. Um, if you want to start looking at this for lambda, there is an A dot layer. We actually recommend using the application signals lambda layer. It's actually an advanced, uh, enhanced version of AOT specifically optimized for. Lambda, with better performance and lower resource optima um consumption. Uh, you can also configure the application signals to use only the hotel libraries for instrumentation without all the other features for, um, um, for application signals, uh, by just setting an environment variable. So, uh, moving to hotel at the moment does mean a little bit more money and a small cold start penalty, but if Hotel is your thing, um, at least we've got some good options. And of course, we love partnering with others so you can use your favorite third-party tools with lambda to get the observability you need. And so Emily did a lot of observability work, more than I can cover here in the time, but it all pays off, a transformation from disaster-prone to a highly reliable system, better reliability, recovery times, less tail chasing. Uh, you know, Black Friday 2024 had zero outages during a peak traffic and, you know, far less customer complaints to better error handling. Next, let's talk about costs and Emily, uh, how Emily optimize costs, uh, using design while scaling. Patiri Ani started the service journey with excitement but faced unexpected cost challenges. This can happen to you. Monthly AWS bills jumping from $200 to $2000 without much warning. During a growth phase, lambda invocations spiking crazily, maybe overprovisioned lambda functions with too much memory, cloud watch logs accumulating gigabytes of data daily. Each new restaurant adding more and more infrastructure costs and data transfer and storage costs all adding up. And Emily realized that in Servius architecture decisions rarely impact costs. Understanding surplus pricing is crucial for cost-effective architectural decisions. Lambda is on-demand. Uh, it uses pay per use model with charges for requests and then also your compute time. So any time is charged from this year to remember, and this was to avoid complexity with some in its charging and not some in its not being charged. So it's all across the board, uh, units are charged and then per millisecond billing is super powerful with each invoke and of course is in its charged separately. And so this is cost-effective when there are gaps in work to do. And you can also consider lambda managed instances for bigger steady-state workloads where you can run lambda at scale in a cost-effective manner. Now, well Fargate uh uses time-based model with chargers for CPU and memory per hour. You pay for what you provision. Now, I didn't have enough time today. There was enough to talk about to go into all the Fargate best practices, but it is a super great solution for many workloads when you don't want to use an event-driven model. And it can also be cheaper than lambda at scale. Continuous processing or long-running predictive workloads are gonna favor Fargate and sporadic event-driven processing is gonna favor lambda. Even though lambda can scale up to super high, uh, super high workloads, this is cost, uh, costing that you can work out. For step functions, express workflows can also be significantly cheaper. Emily's using both, but you can see our pricing can be cheaper. Uh, many more invocations for 1/20 of the cost in this example, using express workflows. So also take a look at your existing state machines and see whether you can potentially save some money and then consider using express workflows first when you're building your next applications. And optimizing your logging costs can be an easy win. AWs rolled out tiered pricing for logs this year. Basically, the more you log, the cheaper it gets per gigabyte. Uh, the best part, it's automatic. Uh, you don't have to do anything. It's just gonna be cheaper. Use structured logging with smart log levels. Maybe you're gonna set info as your default. Maybe you're gonna sample debug logs sparingly and use lambda power tools for efficient logging without the custom code overhead. You also want to match your retention to reality. Maybe you need 30 days for dev, 90 for staging, 3 to 6 phones for production. I don't know, it's gonna depend on your workload, but basically, don't stop paying for logs you're never gonna use. Also, don't log what you don't need to. Skip massive payloads in your logs, just log key identifiers instead. You can also archive to S3 now for long-term storage or log directly to data fire hose when you want when you want high throughput and maybe you want to send the logs to some other destinations. And lastly, monitor those costs. You wanna set cloud watch billing alerts and use Cost Explorer to identify your biggest log spenders. So there's lots that Emily did to, uh, which reduced costs, which also be covered in some other kind of sections. Right sizing was the biggest flex to not pay unnecessarily. Filtering is obvious to not just have to work on things that you don't need to, but it's gonna be able to, you know, avoid a whole bunch of costs unnecessarily. And also using complete savings plans. Emily is looking to explore that as well. She hasn't quite done that. That's another kind of option. And so with some simple optimizations, you can make a big difference on costs. Every architecture decision has a direct cost impact. Understand your pricing models, monitor your usage patterns, and then optimize based on your actual requirements. So next up, uh, integrating AI capabilities into their server architectures. This is the year of 2025. I don't think anybody at any conference in IT in the world is unable to talk about Gen AI, um, but it is an important and I think an interesting part that we can, that we can look at. By this year, Emily's 60 global locations faced challenges, uh, traditional automation just couldn't solve. If you've got 12,000 monthly inquiries that can overwhelm your support team, you know, if you need to particularly handle multiple languages. You know, 4.3 million orders with minimal personalization. Maybe you can do some more with that too, you're missing some revenue opportunities and manual decision making for pricing and inventory. Am starts to look at this, uh, to AI to help with this and maybe take their business to the next level. But Emily also knows that AI is a journey to also work out what's real and what's hype. And I know we're all probably struggling here at the moment trying to work out what the best use cases in your business. Um, and of course, all of AI could be a whole another session, but I'm pretty sure here at Reinvent this year, there are gonna be a few other Gen AI sessions, uh, to help with whatever Gen AI things you're gonna be looking at. So the first thing they realized in the investigations is that Gen AI is just another workload. The same rules apply. What do you need to do? You need to protect endpoints, you need to think about quotas, you need to handle security, you need to think about performance. Everything you know and love about Serus still applies. There's less infrastructure to manage, variable compute demands, pay per use are all good. Using async event-driven processing allows AI to respond to business events like orders, inquiries, and inventory changes. Automatic scaling handles traffic spikes without capacity planning, and your existing server skills transfer directly. Event-driven architectures, cost optimization, and observability. And Bedrock as an example is a server service. You know, the pricing is based on input output tokens, automatic scaling to demand, uh, with enterprise security and compliance, uh, built-in. And in fact, if you look around the industry, um, at the GEI landscape, most of it is just good old, uh, good old fashioned serverless. Any kind of model you're using, it's all token-based pricing, all paper use. Uh, so yeah, serverless, you know, has, is Definitely very applicable in the AI world. And of course, you can integrate um Bedrock with all other server services. For compute, there are plenty of different options and step functions is great if you want to do some complex multi-step AI workflows. Event routing, uh, and response distribution are needed with Eventbridge and to help and Dynamo DB and estuary can also be used for storage, um, of many, many things. And then we want to understand the sort of main two parts of AI that are relevant. Agents, the orchestrators, and tools as the executives. Agents run in an orchestrated loop to do some, uh, something you're basically told it to do. It calls the tools to do the things and figures out what the next stage is to satisfy your request. And they're often long-running, needing to maintain state, the agents. And so we could decide whether agents are short or long-running and pick the best server server to do this, whether that's gonna be lambda or Fargate. But what do we actually learned from a decade worth of Servius is rather use a service that is purpose built for, for the job. And so Bedrock Agent Core is built for running agents and it abstracts a lot of the heavy lifting, so you can head up the stack for a managed service, the trueerless way. Agent Care's got a bunch of capabilities, a secure managed runtime to run your long-running agents with pay per use, and that actually runs a lambda firecracker instance for you all abstracted and behind the scenes. So we're taking in the learnings from other server services like lambda and applying them to more things. An MCP gateway to connect to existing APIs and tools, conversation memory, inbound and outbound identity. Agent Core is also framework and protocol agnostic. And the cool thing is it's a bunch of capabilities you don't need to build yourself. And also you don't have to use them all. You can just use one, use two, use all of them, doesn't matter. It's built, it's built very specifically that you can pick and choose, uh, the useful things that you're gonna do. So for identity, for example, sure, you can spin up a Dynamo DB instance, store your conversation history and manage that all. You could just use identity and it's all built-in with semantic search as part of the package. And then tools. And these are what um uh the agents call to actually do something. And the tools are actually what you likely already have. Maybe that's something behind an API that reads or writes data from a database, does some action, connects to your private data, something on-prem, something within your own VPC. Actually, the tools are the deterministic part within Gen AI. You know exactly how they work, what functions they can perform, and they're gonna, avoiding hallucinations. And in fact, I think you actually want to focus more on the tool side. Even if you don't have robust APIs yet, but with MCP or something like that in front of them, you've actually got more freedom to be able to experiment as APIs are forever, but because MCP abstracts them, you can um have things internal that you can expose externally without having to worry too much about your APIs. You want to worry about your APIs, but having this MCP intermediary layer can be super powerful. And of course, lambda is great for tools, stand-alone or behind an API because they've got fast startup, they can connect to anything, things in your VPC things anywhere, uh, your data, your VPC and they are super secure. So just two examples I've got here. What Emily built a new customer service AI agent to integrate with existing functionality. The customer inquiry goes into an agent using MCP. Agent Core manages the AI agent runtime with Bedrock while using the gateway to connect to APIs and then to connect to lambda functions as, as, um, status tools. And so agent core is gonna handle the orchestration, the state management, and the observability automatically, and the, and the team could then focus on building the actual tools and not having to worry too much about building the agents. Another intelligent recommendation was a good idea they had. Um, uh, this is based on an order-created event which spurs Eventbridge into action. Uh, Lambda processes context. Bedrock is then gonna provide some AI analysis and DynamoDB is gonna store the profiles. Recommendations Adapt during sessions. So this is basically when someone does a payment, it's gonna pop up. Oh, do you also want to buy this? And so it can use time or weather or location or cross location insights. Contextual personalized upselling is really good for customers and also for their business. And so far, Emily's AI's results are a good start and they are welcome. 70% customer service automation using Agent Core and Lambda tools, 12% revenue growth by using the upselling with what's in your cart, and also a faster development, you know, focusing on the tools and not on the infrastructure and cost efficiency, pay per use for the agents and the tools. And haven't got time as well, but there's not even including the AI coding assistance that the team is taking on. Using Qiro for spectrum development, which is literally speeding up everything and building really good production grade applications, and then using the service MCP server which you can connect into Kiro and in fact any MCP client, and this really uplifts your serviceless best practices. You can use natural language, you can query about MCPs, you can query about what infrastructures, code tools, lambda optimization, step functions, all these kind of things built into that MCP server. I really suggest you take a look. So after all that, I'd said I'd cover a lot and I did. Let's recap to see what Emily's journey revealed. Is everybody OK? You're hanging in there. Good. Servalus works at any scale from 40 orders to 3,320,000. Service architectures had to evolve to support this growth, but Servalus, of course, was the best way to achieve this. We have some core principles. Here's the QR code again for the presentation and more handy jumping off links, right sizing, matching the function repo and stack size to your workload. Async over sync, event bridge and queues for resiliency with still faster customer updates. Avoiding work if you can, using direct service integrations, step functions, filtering early, using batch operations, performance optimization, optimizing those cold starts, function code optimization, platform functionality with provision concurrency and snapstart. Failure handwriting, got retries, DLQs, circuit breakers, a whole bunch of things. Um, we've got observability, logs, metrics, signals with business context and lots of new cloudwatch goodness and cost being architecture, every decision has a price tag. And Gen AI just another workload, your server skills apply. And these principles also apply at any scale. So what could be your next steps? Well, this week, maybe next week, I think you're gonna be broken from reinvent, but pick one lambda function to optimize. Add some structured logging, including some business context, implement proper error handling using retries and DLQs if using SQS. Check function sizing, make sure you correctly size your memory and use EventBridge, not direct invocations. And then once you start there, you can build some momentum. Migrate one sync workload to as sync, build robust uh workflows considering step functions or lambda durable functions, implement some event filtering, save yourself some money, review and optimize those costs, and then also document your patterns to be able to scale out your efforts. There are lots of other sessions going on this week, uh, uh, lambda managed instances and durable functions. I mentioned in the talk as well. There's also a leadership session, building the future with AWS Servius and a shameless plug. I'll be back tomorrow with a lambda principal engineer, and we're gonna be talking in detail all about how events travel through lambda, specifically with polling, and that's gonna be back here tomorrow. This link also provides a bunch of additional information to continuous service learning. It's got links to power tools, a lot a bunch of announcements, service land patterns. If you've ever used that, that is infrastructures code and um function code examples for, you know, all the server services and you'll just be able to do them and a whole bunch of them you can use directly in your ID as well. Super useful. Uh, Serlessland.com as well is the best website to keep up to date with all things service on AWS. Lots of cool stuff that you can read over there. But most importantly, thank you for joining me today. I appreciate you, appreciate you taking out the time. If you've had to move across Vegas and come to another venue, uh, thank you so much. I also really appreciate if you could fill in the session survey that really, uh, if you're hungry for, you know, more deep technical contents, uh, more broad, and more kind of things, this really lets us know, uh, the kind of, uh, things you'd be interested in. So I would appreciate, of course, uh, a, a, a, a nice 5 in that as well, but also be honest with what you think. So thank you very much. I hope you enjoy the rest of your day here at Reinvent. Plenty more to learn, plenty more people to uh connect to. I encourage you to use the hallway track as well. Um, sit down with somebody at lunch and, uh, find out and talk about your uh kind of things. Talk about Servius and enjoy the rest of your week. Thank you very much.