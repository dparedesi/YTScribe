---
video_id: M3KAmqX-0d8
video_url: https://www.youtube.com/watch?v=M3KAmqX-0d8
is_generated: False
is_translatable: True
---

How many of you know MCP? How many of you have actually deployed MCP server in local or remote somewhere? 2. Not bad. Nice. And everybody knows Cerveles. What? No surveillance? You guys is what? Severalis lambda APA gateway, are you guys familiar? OK, thank you. So I think the session is going to cover 3 main concepts, which we are going to show how to code it. The first concept is around surless architecture. We are going to use that to host our agents, our tools. And actually show you if you have any Python code running, how can you make a strand agent out of it. The second portion is going to be showing you an MCP server, which is already pre-built. Because it takes around 15 minutes for us to deploy. And then the remaining part, which is a strand agent, we're going to code it live and deploy it live. Hopefully, we get all the tokens for Bedrock today, while we deploy this. So from the agenda standpoint, we're just going to quickly walk through what is agentic AI and why is it being so prominent. Because LLMs are quite powerful and they can give you what you want. But what is driving this whole agentic AI and what is the heart and center of agentic AI? The 3 things we talked about is MCP strands and Serveus. Those are kind of the stars of the show. Everything that we have configured in the code is based on those 3 key components. And the last thing we want to show is like how this entire architecture can come together. And I'll use the word plug and play where you can bring Your own tools, your own MCP servers, and start integrating. The key focus here is going to be how do you seamlessly integrate all the things that you have today using MCP instruments. So the first concept I want to just get it clear, and I was just thinking today morning, trying to explain to my own self, like, why do we need agent AKI? Anybody in the room, Has a use case that they think Agen A AI has been used. Yeah, why not traditional AI basically? Why not just any chat interface? Why agentic AI? Exactly. One of the things, as you can see here, is a multi-step reasoning and decision making. Yeah. So the way we looked at this is like I have GPS in my car. I put direction. And it actually saves me how to take the path and go, right? That is what an LLM is. You ask a question, it gives you a response. The ANTKI is more like your self-driving car. It has capabilities to connect to all these tools, your sensor, the weather map data, the light traffic data that is coming in. And it can use that information to feed in. The LLM to make dynamic responses. So the whole idea of the self-driving car analogy is with agentic AI you can invoke any model you want. Try to attach the tools that you can bring to the table, and we're going to bring a couple of tools today as a part of the demonstration. And then finally, you can dynamically change the response and adapt it based on what's happening. And that's one of the fundamental reasons. Agency care has been kind of driving the wave of G AI because a lot of people wants to make active decisions and take actions based on information. So the whole thing about agent AI if you want to just learn one thing around here is that agent loop. It's the heart and center of any agentic AI framework. How many of you have heard about Agent Luke? Three of them. So I'll just quickly go through what agent loop has. So the key here is that what we see in that agent loop, which is basically a large language model. Your tools that you can connect to, and the interaction that happens between your agent and these tools and LNM model. The key here is that all the event loop communication and the formatting is basically taken care by this agent loop. So it's a process which actually the strands agent also uses. In order to invoke models and use MCP tools. So gentic loop is becoming kind of a priming factor in order to create these agentic air systems. And there's a whole GitHub repository which explains the components of agent loop, how does the user messages and the assistant messages come together, and the formatting of messages that happens back and forth in order to make a decision is quite important. And that loop is recursive, right? So in order to solve complex problem, the loop is constantly running. So in your self-driving car, when the weather data changes, that loop runs again in the back end to give you a new direction map and say, speed up or speed down based on the weather condition. So all those actions that happen in real-time is basically driven through this agentic loop. And the single thing that invoke that agentic loop is basically users like us. When we invoke a prompt, it basically starts an agent, right? As soon as I get in my car and turn it on. The agenttic loop starts and the actions and decisions are made. So that's what we'll show you as a part of demonstration, where we'll basically invoke a system prompt and start running this agenttic loop for you. And one more thing to add here is agent just does not call tools, it can also call other agents. So with strands, it also supports A2A, which is agent to agent protocol as well, which we are not demoing today, but it is possible with strands. So I think this is one of the biggest things that we are going to quote live today, which is trans agent. The reason it's getting a lot of traction is it's an open source agent. That allows you to communicate with multiple tools. It has also got in-built tools that comes along with it, and it can also interact with your MCP servers and other things. The biggest reason why strand agents has been used today is because it's a Python SDK. And it's the most simple way to build an agent. So as a part of that demonstration, we are going to build the code using strand agent by showing you how simple is it with few lines of code to bring up an agent. I'll pass over to you at this point. So before we start looking at the code, we give you a sneak peek. Uh, before we go there, what do you think? There are 3 main components. It's a kind of pop quiz. There are 3 main components to build an agent. Any guesses and wrong answers only, please. Funny answers will be great. What, what do you think the agent needs to operate? Access is a great one. Access to Salesforce data source. That is definitely not a wrong answer, but very close. What else? LLM, yes. Yeah Prompt. Yes, these are all correct answers. Uh, so, so very, at the very basic, you can just in 3 lines, as you can see, you can create AI agent using strands. So Just import agent. It's an open source SDK. Create an agent with System prompt. And then you just invoke it. How can you help me? But as you can see, there's no model definition of which model I'm using. There is no tools access. It's still an agent, intelligent one, but doesn't have that fancy tools to do magic. So, why, we are not just going to show you the code, we'll actually code it, but this is just for you to get some idea of what we'll be building today. Um, and also we cannot show all the capabilities in life code, so these are kind of out of possibility, uh, with the agents. This one, as you can see, uh, sorry. This one, as you can see, it's similar to the last slide, but here you can configure the model. Here we are saying, OK, use NA model, but by default if you don't use model in the previous slide as you saw, uh, you will use anthropic cloud 4 model that strands uses by default, but you have a choice of which model you want to use. It does not have to be Bedrock-based or Amazon-based model. It can be OpenAI model, it can be a meta model model of your choice. And that's the power of what we are trying to show here with the Strands SDK because it's open source, choice is yours. So what about the tools? We saw the prompt, we saw the model, and not just the model, but also you see how you can configure the hyperparametters, like what is the temperature, what is the max token, what is top A, top K. All those things are configurable based on your use case. And as I was saying, you, you can use OpenAI model as well. Uh, this is a very, very sample code. We'll not be coding this one today, uh, but yeah, this is possible, as I was mentioning. So you can just configure which model you want to use and the params. Of course, if there is an API key you need to pass to access this model in the open AI space, you can do that too. And then basically pass the system prompt. And model here and then get the response. We are still not talking about the tools which will be going next. Tools is the actual thing which is happening in real world in production today, which is giving AI model, sorry, AI agent power, right? So what are tools? Any, anybody? What are tools? It MCP is the right answer, but what is the main function of a tool? How does it help AI? It Give you some task. it's To do the actions, OK. So it's basically, think of it as a normal function, which LLM can understand. Before LLM you write an API, you have a code, uh, and that Basically your business logic to make it. To make AI assistant or agent understand that kind of code, you expose that as a tool. It's ultimately some, some code behind the scene, which you'll see. So the key thing for a tool is that it extends the capabilities of an agent, right? So if you have written your Python codes or anything, You can actually use these decorators which will go through and make those as your own tools, right? So the key idea about tools is it makes your AI agent capabilities much better using those tools. And then the agentic loop basically takes care of tool selection, tool execution processes, which you as a user do not have to take care of, right? So tools are important. But how do you integrate with those tools and have those LLMs called the right tool is what that agentic loop comes into play for it. Just dive a little bit more deeper here, as you can see in the example. Strands agents do come with out of the box utility kind of tools, 20+ tool. It comes, it gives you a right, you don't have to code it at all. So in this example, the second line, this calculator tool is basically built in the strands, so you can just, just use it. There are more which we'll use, for example, one which we'll use today in the real code is STTP tool, which basically goes and browses the internet. The second tool which you can see here is your custom code, weather API. Uh, you can of course go to the external websites and get this information, but the idea is any function, if you forget this decorator for a second, this is your code. Any code which you have and you want to expose it as a tool so that AI agent can understand how to interact with it, you put this decorator. Any idea why we need decorator? Why can't just AI. Talk to any code. What kind of problems it might have. We don't use decorators. Short answer is it makes sure it doesn't go in the loop. It doesn't. Call the same code again and again because LLMs need to know how to call, but it can go into that loop. So what strand's agent gives you is the stool decorator. What tool decorator also does is not just. Helps AAIA understand this code, but also security around it, observability around it, all the guard rails basically it provides under the hood. So just with that one line, you make your function work as a tool. And the biggest part when you put that at the redecorator tool, what you're telling that agentic loop is. I want you to record the conversation states. Because when you're calling multiple LLMs and multiple tools, you need to record the conversation state of your loop. So putting the decorator tells that I already called that tool in that conversation state, so it doesn't go in that infinite loop, right? You don't want your supercharger or your self-driving car to continuously go back and forth on the same road, right? And that's the concept of the tool decorator there. Yep So how do you pass these tools? So when you create an agent, in the previous slide you saw you pass a system prompt, and then you also give model configuration which we have skipped here. But the tools is basically an array of all the tools it has access to. It can be dynamic. Right now we are just making it hardcoded so that people know how it is used. Once you initialize the agent like that, OK, you are a math calculator and predict the weather. You can do these two things, and for that you have these two tools. Basically you're saying you have these two powers, you figure out in what order you have to use it to give me the answer to this question. And then the agent goes into that loop. Agent talks to LLM saying, Hey, this is the question from the end customer. And these are the tools I have access to. LLM will tell, OK, I think for whether you should call this tool for calculation, you would call these tools, and then agent actually executes. LLM does not execute. LLM will give you the to do list, the plan. Agent then executes this plan. I need to call tool 1 and then tool 2 and then tool 3, and then ultimately gives the response back. So before we go into this final tool integration, which is like MCP based tool, uh, because MCP server provides a lot of tools, uh, we'll quickly, very quickly we'll go over what MCP is, and then we can talk about the last integration before we start coding. So before we go to what MCP is, let's figure out if MCP did not exist. What will be the problem today, right? So if you do not have MCP, let's just wipe out a year and a half ago. The way we used to write this agentic loop is, if I want to invoke 5 models, And I'm using like 5 different tools, weather tool, calendar tool, and so on. I'll basically have 5 x 5, which is 25 integrations that the developer will have to write down and get everything working. And if we just keep on that scalability problem because there's a new LLM coming out every day now. And there are thousands of MCP servers that you can find on GitHub repository. The N cross N problem becomes not easy to tackle. So what MC MCP provides is, it's an open standard to allow you integrating your tools with your AI applications. Just like your REST API, it's kind of an API for your AI applications, like your chatbot and others that can integrate with the tools. So it converts that MN problem into MM + N problem. So it makes it much more easier. And the key aspect here is that developers can build fast. But if you reduce the time to integrate multiple systems, they can basically have a standardized way to connect your AI applications to your tools and other systems. And there's a patent called uh agent tool pattern. So even strand agent can be one of the tools that you can call, right? So giving this capability of MCP to standardize and open-source way to connect to multiple tools from your applications basically solves this MN problem, which existed like a year and a half ago. I think these are some of the concepts about uh MCP. It's an open standard, so everybody likes to use it. But the key reason why is it important for us in this particular session, because Strand's agent has a native integration with MCP. So it basically runs an MCP client on it. So you don't have to do all the mumbo jumbo heavy lifting of building the client and bringing things on it. So using strands, again, an open source agent allows you to connect. To different tools using MCP which is natively integrated. And from the schematic standpoint, this is what it looks like. You got on the left-hand side, all your AI applications, and on the right-hand side, you've got all your tools. And I categorize these tools in 3 different categories from what I've seen in capital markets. You got your data sources, which are your database, your S3 repos, and everything else. You've got your Python codes, which are your functions that you want to convert them into tools. And then you have some customized things that are written up, which is kind of your proprietary functions that you want to play with. So if you want to bring all those information, Into your AI agent to extend the capabilities. You basically have that MCP open source to connect all this together. And it saves, so from the lens of developer, it saves a huge amount of time in the integration logic. From the lens of AI applications and agents, it basically saves a lot of time in connecting to external resources because just by invoking MCPI can connect to it. And from the end user system, like I have a 15 year old daughter. She likes that lovable website and she can create things, which I could not do when I was 15. So it gives you that extension, not only for the AI agents, but for the population to go and start developing things in a simple way. So the key here is like, how do we make things simple? Because there's going to be lots of tools and lots of agents coming out. And the key is if you have a framework that you can follow with open source, it can make it happen for you. All right, coming back to the. MCP tool integration. So, we talked about built-in tools like calculator. We talked about custom tool like whether API, your your logic basically. So how do we now integrate tools which are exposed by your ISP provider, let's say a third party provider which are hosted somewhere else. It could be on your on-premise, it could be on your lambda, it could be on your AC2 machine. It can run anywhere, that that MCB server, as long as it is exposing that URL or load balance or API gateway URL. In front of it Strand's agent can connect to that and figure out all the tools it has access to. So in this example, as you can see, streamable STTP client we are using, strands do provide support for all three types of clients like STDIO, SSE, and streamable STTP, which is standard for MCP connecting to MCP tools, MCP server. Right now we are using streamable STTP because our MCP server, which we'll be using today, is hosted on Lambda as a separate application. So now it is a remote server which you want to talk to, so we need to use streamable HTTP client in this case. Again, with MCP client, you basically get all the tools which this server gives you. In our example today, we have a very complex MCP server. What it does is it just rolls the dice. So that is one of the tools it exposes. So once you have those tools, just like any other tool, you can pass those tools to the agent, and now it has access to. Roll the dice. Any questions? Don't use those numbers to roll the dice a day in the evening, OK? You're in Vegas. You have to roll the dice. Go ahead, sorry. As the element to run it. On foreign, so your previous example, you had a you had a. Yes. So when the LM come up with a plan, so what it happens, I think you create that, uh, Jason, uh, the tool aspect sending it to the L. It invokes the better bond, it executes the better bond, it gets the sunny, then you pass again all the tool aspects. The LLM, it goes to the calculator, through the calculation comes back. So what I'm asking, can you do it in one shot? Thank you so much for that question, and it was not a planned one. We are exactly going to demo that today, so we will. We'll see how the agent remembers that context. Based on the question, it will know which tool to call first, remember the answer, call the second tool with that answer, figure out the second answer until it finds the final answer which the user is asking. So we'll see that in action today if you can, yeah, just let's but it can't call it in parallel. Is that the question? Like instead of calling 1 and 2, can I just do 1 and 2 straight? No, because you have to wait for the loop to send the response back saying that. I got the weather right now, right? And that weather response goes back in and say, hey, based on this weather, should I go in a car or should I go using a bike, right? So that tool is a different tool. The concept key, what is happening is The pricing comes into play because when you upload that cloud.json or MCP.json, those tokens are getting burnt off. So there are ways to optimize the loading of those, uh, tools for minimizing the use of tokens. So there are some techniques to do that and we can talk after the session. And there's a wonderful article on how to do that. But today you can't send two requests at the same time because it has to wait for the loop to basically execute the first one. Yeah, I mean, there are other architecture patterns using step functions, specifically, if you're talking about surveillance, we can discuss that after the session. So quickly we'll go through why Seris. We can use EC2 computer of your choice, but we are using Servolis because When your, when your agent is not working, you don't pay for it. Uh, it, it scales. Let's say if it is Black Friday kind of AI agent who helps you do the shopping, it, it scales up and down without you having to manage all that infrastructure for you. Security and observability comes out of the box with surve as architectures, as you might know, most of you. So that is the key reason why we chose Sales, and it's a very unique way of doing it. Very few people are doing it. Even though it's so easy, so we wanted to promote that as well as tell you how easy it is. And this is the key part. So today we'll be building this. This is the high-level architecture. We have 2 blue boxes. The small blue box is basically your MCP server, who has one tool exposed, which is rolling a dice. That is already deployed. We are not coding this. We have deployed this because it takes time. The first blue box, which is the bigger one, is basically we will be building AI agent using strands. What this AI agent does actually overall is it Predicts the weather. Based on the weather, it will tell you how you should commute today in Las Vegas, whether you should bike, whether you should walk, whether you should stay on. Uh, and the third one, it will just, because you're in Vegas, uh, you basically have to roll the dice, not related to the first rules. Just we are not trying to show the full 3 tier application which you can deploy in production. It's just the concepts, how you connect different things. So one quick thing, right? So the key we talked is 3 things, right? We want to show how strands can integrate with in-built tools. Which is the weather too MCP server, which is a dice tool. And the third one is basically the custom tool, which is how should I go? So we are going to show all three of them. As a part of this whole demonstration, yes, so number one is HTTP request built-in tool, as Bawin said. Second one is a commute advisory based on the weather, how should I commute, which is your own logic, business logic. And then the 3rd 1, again, dice roll tool, which is the MCP server hosted on lambda, another lambda altogether, which will connect through APA gateway. Super last slide. I promise no more slides. This is the example workflow you should keep in mind before we dive into the code. So this is how. To your question, If you ask one question in one line, which should involve all three tools, this is how it would look like. How should I commute today? I'm not asking, what is the weather today? I'm just asking how should I commute today. But based on the tool, logic and the system prompt, AI agent is given the instruction that whenever somebody asks for the commute. Just first go and check the weather. If it is raining, you don't want to say, OK, just keep walking. So that is the logic which AI agent understands, based on the system prompt. And also the second one is altogether separate tool it has to call, which I'm not asking in the user input. So A agent has to figure out in what order I should call and which tool. So when agent gets this request from the user, the end user, what it does is it calls LLM to come up with a plan. What tool, these are the tools I have, I have access to, and this is the end user query. What should I do? How should I operate? How should I execute it? So what LLM gives is, OK, I think you should call tool one first. So AI agent does that. It will call STTP request tool and figure out, OK, it's a good weather today. Then AI agent, because of that loop, agentic AI loop, it figures out I'm not done with the end user's answer. End user is asking for the commute. I still don't have that answer, so it will call the second tool. OK, this is the weather information. I think it's good for walking. It got some more answers to the original question. Still one more question is pending, the dice roll part. Then it calls the third tool. It's like, OK, I will roll 2 dice because you said 2 dice. It can be 3 dice, 30 dice. And the sum is 10. Once it has answers to all the questions, it will summarize. That is a part of the EA agent. So final answer, it will summarize and very in a coherent manner, it will reply back saying, OK, today is a good weather, you should just go biking, and maybe your lucky number is 10 today if you want to do some gambling. With that, Let's dive into the code. Any questions? We need 2 volunteers to come on the stage to build it. OK. We need to put the all right. Any questions, last minute questions before we dive into the code? Yeah. About reasoning so. Sorry, come again. Reasoning, yeah, a good commute, yeah. Are you assuming the model is going Yeah, we can model can do whatever we ask it to do. So that is part of the system prompt. So there are two types of prompts just to be clear. System prompt is basically defining the persona of the LLM. You can say you are the travel agent, you are the HR, you are any persona. You are that system prompt and. User prompt is basically the end user query, how should I commute? But the agent knows that I'm the commute advisor agent, so I know how to reason these two things together. That we will define in the system prompt when we build this agent. We'll see this in the court. Any questions before we get started? Are you connected to the web? No, this is not the one. This is the this is the one, OK. All right, so I think you see the screen. This is the small blue box MCP server code. We are not building this, just want to show you what it looks like. This is already deployed, which we'll be using as part of the agent, when we build the agent. So the first thing we do is the system prompt, right? So I can tell. And a number of ways, like roll one dice, roll a dice, roll a couple of dice. Or roll 2 dice. So I'm giving it a prompt. Because the whole reason we require LLM2s in the situation for MCP is, once the prompt is entered, the LLM is going to return a number. So if I say roll a dice, it means 1. If I say roll a couple of dice, it means 2. Or I can just be very specific, roll 2 dices, right? I don't have to parse the string where that number 2 is. So Bedrock LLM is what we're using, anthropic here. And the whole The purpose of this anthropic module is to take that prompt and give me a number, like how many dice are we going to roll today? And this is also showing how you can also call bedrock from lambda. Yeah, that is correct. So this is just a lambda function, right, a Python code that all of us have written thousands of times. So any Python code you've written, you basically can call in system prompt and put a model ID number to call to get those numbers out. This is a decorator, which makes this an MCP server at the rate MCP.tool. And when we go in the deployment, we'll show you how, where to input that parameter in order to avoid all the cold starts. This makes it an MCP server. This is just my Python function. It does nothing, takes the number of dice I'm rolling. Generates a random number. If I'm rolling 2 dice, it generates 2 random numbers, sums it up, and gives me the sum total. And again, this is lambda erruli, so there's no state information stored, right? It just creates and goes away. So what we are doing here is we said, let's just create a Dynamo DB table to store that information, whatever that LLM is generating. So all the dice rolls that are happening, we just do a set item into the Dynamo DB table and store it. This, we won't go in detail, but this is basically our way of using lambda web adapter. So this is basically running web server on lambda in order to take that HTTP API event and basically pass it to an agent. So that's all you need for your MCP server, a decorator. The function that it needs to do. And Rahul also showed that security is becoming kind of A blocker or a resistance for deploying a lot of this agentic AI things. So what we also did is like, look, lambda has an authorizer, MCP is running as a lambda. Let's just put a simple authorizer function, which is just checking for a token. And I think we might have time where we can show you with the inspector. So if you pass a good token, It will allow you to execute this MCP tool. If you pass anything which is not a good token, it will basically reject it, right? So this is an easiest way to use a lambda authorizer to basically allow and deny things if you don't want MCP tools to be invoked. So that's all the MCP tools. This is the only thing that we are coded and deployed. We're going to go now into the strengths part of it. Yeah. One more thing I would like to say from the practical implementation, the Dynamo DB thing which Bawin talked about, for now we are storing the dice roll results, but as you can imagine, lambda only runs for 15 minutes. So if there is some kind of state management you need to do, you can put it in Dynamo DB. So when back and forth conversation happens and you want to do such kind of state management but still use surveillance and not run your EC2 machine for 8 hours, you can externalize that kind of state management in Dynamodivi. That's one of the key things which you wanted to show as well. Any questions on MCP? Well, the concept of uh. Yeah health insurance. So we have talked about it earlier before this session. So there's a whole agent core thing coming at you from AWS. So this is, if you don't want to use agent core gateways and all this, this is how you can use strands and MCP with your Python function. But you're absolutely correct. If you have agent core runtime gateway, You can basically have a list of MCP servers and that agent that agent core gateway can actually do IDP authentication as well. So based on your IDP settings in your SEML, you can pass those tokens and get end to end authentication authorizations. So that is what agent core gateway brings to the table, but this is pure vanilla thing. Like if you are just a developer, you don't want to spend any money. I just want to go and write Python code and get it running. This is the easiest way to get it up and running. Yeah, you had a question. What is like the most fun. We can, Because what throws me off is lambda just there like someone has. But if I'm doing a data intensive or if I'm actually using NCB2, which is an output of another model or things like that. So yeah, 15 minutes is the hard limit. Uh, if you think your use case is beyond that, let's say you're doing some research, kind of, in your case, you're doing some research kind of LLM AI agent, you might need to choose some other kind of compute option. That's, of course, is the, that's why we have so many choices. But if you think your 15 minutes is good enough, uh, any web-based application, no user, you don't want to go to any website where you have to wait for 3 minutes or. 30 seconds also. So those kind of use cases where you have integrated AI agent. Sales is the perfect choice. Context back to the element we are The In the past But how is this actually enhancing the That we'll see when we build agent. And I think the last thing I want to call out is we don't know the real answer, but there was an announcement where lambda can run on EC2 now. So I don't know if that changes the timeout or something. So that will also help you, but we are not, so we have been working all these days, getting up and running. So we'll go back and look at the announcement that I just read through coming in here that lambda can run on EC2 now. So that might be a nirvana part to get to. The longer timeouts. Yeah. But go serverless if you can because that we are not managing a lot of information. Yes, please. Yeah, you can do that. So layer, no, usually when the layer is used, typically when you have to have common dependencies, that's the best practice. Uh, of course you can use some extensions, which is deployed as a layer. In this case, You can put it as a layer, but usually that is for libraries, uh, if you want. Yes, you can still do it. Technically it's possible, but what's a use case? What are you thinking? a couple of agents. Yeah, that's a good use case. Totally you can do that. So if you have multiple agents talking to the same MCP server and you want to share that code, uh, rather than having multiple API gateway URLs, you can do that. I start to. So, while usually like any application building best practices, right, when you build a code, you want to test it before you expose it to others, right? Uh, you don't want to give it to somebody if it is not working. So just to test the MCP part, let's see if we can. You can see the screen right Uh You can see there's an Anthropy provides a testing tool. It's called MCP Inspector. Um, it's not the inspector service from AWS. So what it does is it helps you test your MCP server locally, um, and then when it is working, you can basically say, expose it to other agents. Was that Zoom in. Zoom in. Better. Is it good? All right, so what we have done is we have exposed, we are connecting to a URL which is basically the front end of our API. Gateway URL behind which the MCP server is running, and we are using streamable STTP as you can see in the code what we, what we showed earlier. Let's try to connect and see if we can roll the dice before even we expose it to strand's agent. And I did not. Because it says connection error, uh, check if your MCP server is running and the proxy token is correct. If you go back to the logs. You can see STTP 30403 error. Any guesses why it is failing? What's that? Yeah, but why is it failing? Why is it failing? You don't have your your to. There you go. So $25 Starbucks here. In the authentication, what this inspector tool provides is also other configuration. If you go to this authentication tool and if I see I'm providing a bad token, so any. User of your MCP server cannot access your MCP server unless you want them to. So this is how you secure your MCP server. It can be API key, it can be a bearer token, it can be a jot, it can be anything else. So just as you saw in the code, if you pass the good token, and hopefully it should work. Try to connect again. Now you can see. Resources Templates, uh, basically you can see resources, you can see some prompts, and you can see tools. Tools is the one which we'll focus today. So when you say list of tools, what it will do is show me all the tools. Which you support MCP server. And then it says, OK, I only support dice roll too. OK, how do you execute it? Let me test it. If I say, all 2 dices, just make it simple and 2 is. OK Demo gods are sending tokens right now. They're coming. Interesting. It could be lambda gold stars. Oh yeah, it is. Alright, it came, it rolled two dice. One is 61 is 2, and the sum is 8. So this is the, the blue box, we completed that part. This is how we will, right now we use inspector, but this is how AI agents will talk to this MCP server when we do the actual strand agent code. So let's dive into the code for Strand's agent. One thing which I would like to highlight here is the system prompt, which is very important, and as you are mentioning, how it will do the reasoning. This is how it will do the reasoning. So what you say is you are, you define the persona of the AI agent. You say you're the weather commute and ice rolling assistant. And to do that, you have these tools access. You have weather request, tool access, you have commute tool access and dice request, and basically how to use them. Also, it tells you when you call the weather, return not the huge gigantic text, but only return a few things. Return only the conditions, extract the conditions attribute, temperature attribute, wind attribute, and precipitation attribute. I am only interested in these three to go to my next step. From the weather tool once I have this information. I can make a call on commute, so once I can recommend the commute advisory based on. These 3 attributes of the API. And just for no good reason, you can also call dice rolls. It's not connected to the first two. But be very explicit, and this is very important. Be very explicit in the system prompt what AI agent is supposed to do. So here you are again explicitly saying exact tool name. What you're saying is STTP_request is your in-built tool to do this. Commute advisory is your custom tool to recommend the commute, and dice_ro is the exact name of the tool which you should be looking for. And this is the agent which will pass this information to the LLM saying, hey, this is my end user request and this is your system prompt. Now LLM help me with the plan, what tool I should execute in what order. Once you have this, we start with the base tool. Base tool is basically HTTP request and commute. Why, why not MCP? Because as a best practice, MCP server is running somewhere else. Sometimes it might happen that that is down, it's not working, right, because somebody else is managing that. In that case, you don't want your AI agent to get stuck. You still want to operate with all the other tools it has access to, whatever it can with a limited capacity. So in that case, We tried to call the server, but if it fails, If it connects, yes, good, base tool plus MCP tool, now we have 3 tools access, but if not, Stick to the base tool, and this is the part where we are saying this is the best practice, and if you see, it is outside the handler of the lambda method. What is outside the handler of the lambda method? Can somebody tell me? Why did we not put everything inside the lambda itself, handler itself? Thanks. Yes, perfectly. So this is one of the best practice when you write your lambda function, any IO operation, any external initialization, database connections, third-party tool provider, uh, these kind of access, if you put it outside the handler. Multiple execution environment can reuse those connections. You don't want to have this connection on every single request. Lambda runs for 15 minutes. If first request is solved in, let's say, 1 minute, second request comes in, at least this part will not be executed again. It caches that connection to the MCP server. That way it is helping you reduce the cold start. So that is the part which we wanted to show. There's one of the lambda-based practices which you should keep in mind always, especially database connections. Always keep it outside the handler. Once you have this, now connecting back to what we talked about in the deck, there are 3 steps which we need to do. We need to initialize the agent, but first we need to figure out which model to use. So we'll initialize the model. Then we'll create an agent with the system prompt, which we already saw, and then we'll try to do some testing. We'll call the prompt and see how it works. Makes sense? All right, of course we'll do the deployment to see if it actually works. And I hope it does. And Because nobody is doing the coding from scratch these days, we'll also leverage the VIP coding just to show how Amazon Q developer integrates very well with the VS code. And because I have commented this line, I want to initialize Bedrock NoA Pro model. And if I hit enter, It will tell me Not this model, of course. I wanted to show. I need to see what exactly the model ID is, but basically what it says is you can say. Model, I think there's a difference in tax. Yeah. You don't need streaming APIs, you can say. Temperature 30% and actual model information we can pick up from The documentation, uh, so what we can say is for No a pro model, I want to use this model instead. Yeah. You can change the model as we said, and that is exactly what we are trying to show here. So either you can use the bedrock model whole code or you can just type it with your PA assistant Uh, bedrock, yep. Now, once we have the model ready, what we want to do is initialize the agent with this model and the system prompt. So what we're seeing is agent, uh, passing the system from what you saw here at the top. LLM is bedrock model. I want to use this model to do this agentic loop, and then all the tools what earlier I showed access to base tool as well as the MCP tool which you are passing. So now with these three in place, let's try to deploy this, and while we deploy, we'll go over how we deploying it because deployment takes 2 or 3 minutes. So what we do is. So we have built this, uh, you can use terraform, you can use cloud formation, you can use CDK, you can use SAM template. What we are using here is SAM template. SAM template is. Again, the I would say wrapper on top of wrapper is not a good word, but wrapper on top of cloud formation. It makes 10 lines of SAM template gets converted into, let's say 100 lines of cloud formation. So if you have not used cloud SAM template before and if you're building wireless applications, I would highly recommend go and check this. This makes your life super easy. Just to give you an example of how it looks like. For this particular application, this is how the SAM template would look like. What it shows here is. You are creating these resources, one API gateway, which is like type API. Uh, all the definition, what goes inside that API. So you're saying the endpoint is agent, it supports post method, things like that. You can also create lambda function, which is basically where our agent will run. And all these are again declarative, but you can also use CDKA to define such resources. So once it deploys, and we have created a wrapper just to save some time using the make file, so what make file I said is make deploy. It basically is building and then deploying. So it is calling Sabu and then Sam deploy using the same template file. Any, any questions so far while the build is happening? Before we start the testing. Uh, question you mentioned uh. I Yes. So since we have. Yes. I meet with this Correct. That. Revisit our system by doing that in order to change the definition of the. But you see Uh, external tool has some more capabilities that the Correct. You can also see the problem. Now, uh, with the expanded the group. to make sure that you have. So typically you Externalize this outside of your code. This has to be part of the configuration. So you just update the configuration, uh, and you inject from your environment variable or system prompts. That way you get all the latest and greatest functionality. But if I show you in the code. What it does is This lambda function when you call the tools. It calls this line, line number 57, if you notice. What it does is, if you have more tools added, it will list all the tools here in MCP_tools. You're not hard coding that you want to use dice roll here, if you see in this code we are not seeing that. That is a system problem because we want our agent to work as that kind of assistant. If you don't provide, it can do more things, but not very confidently, you know. If you want to make an agent. To answer a question, when you let this upgrade a Python code to do like 5 more things, right? And it's the same MCP server. The key would be like you will have a second kind of a tool that you want to call for another agent because anytime you're developing lines of code, you're developing for some business logic. So for that business logic you just create another agent and let that agent call that MCP server. Don't mumbo jumbo all the system prompt in one place in one agent, right? So like if your kids go to class, they're like math, English, science, you'll create 3 agents, right? They can call the one MCP, but everybody will have a different system prompt. So, you can write the code and create all these things, but create multiple kind of agents to call the same tool, each one having different system prompts. I'll say, let's quickly look into the custom tool which you talked about. So this is basically the commute advisory tool, which is just your plain simple Python code. There's no AI part to it except for this tool decorator, what we saw on the deck. Any business logic, it can simply be, you can just say. Written, uh. Let's say Walk always. That can be as simple as that. OK, so it is what you write basically. This is your custom code, uh, but how to make it available as a tool, you just put this tool decorator here. So now the the deployment is done. Sure. This one or is it just not does just get the tool description. It will basically get from this file, the system prompt. So whenever you a tool. You also give some knowledge about this tool, what this tool does. Otherwise, LLM will not know when to call this tool. LLM will not put that in the plan. So for LLM to understand, OK, I don't read the code, of course it can read the code, but if it, if you give the instructions like this, then while making a plan for the AI agent, it will tell you, OK, you need this tool in this order. So you can provide that description. Yes, the tool. Usually it is always at the tool level. So once the deployment is done, how much time do we have left? OK, 11 minutes. So let's do some testing and see if it actually works. The shooting. So we have also created some uh test shortcuts. Basically, it's a wrap around the curl commands. What we are saying is let's first test whether tool itself, not the full end to end tool, just to see if individual things are working. What it is saying is the prompt it is asking right now to the agent is, how is the weather in Las Vegas? And of course it failed. LLM. Chindela. Let me see. Oh, the ID ID and not the LLM. My bad, typo. I have to do the deployment again. Hopefully it works quickly. This is how the court talk goes, so we can have more questions and answers while we are waiting. Any questions? Is it the agent. Yes. So there are, if you remember the architecture, the lambda function is also running AI agent and lambda function is also running MCP server. They both are fronted with APA gateways. So this agent is calling the API gateway of the MCP server. That way, yeah, you can get that access. Is it clear so far any Confusions. So our strands and MCP, they're both lambda, right, that at the tool decorator makes it strands, and at the rate MCP tool makes it an MCP server, but everything is front end with an API gateway. You can do that, yes. So, that is a good question. Uh, for this, uh, one, we had to increase the timeout. Default timeout is 29 seconds, but as you can see from maybe 6 months back, we said you can also increase it to 5 minutes. So I had to increase it to 1 minute for this use case. But yeah, 1 minute, by the way, is auto-approved. If today you go home and just in your AWS account you request. 60 seconds or 1 minute, you get auto approved. Um, so 29 seconds is not valid anymore. For 5 minutes and stuff, you have to raise the support ticket. Yeah. Any other questions? Lambda, APA gateway step functions. MCP. If you were just staying within the database ecosystem like how this architecture is, could you, instead of an MCP, could you just provide a tool that To keep the the land. Yeah, we can do that, but that's not a best practice, I would say directly calling lambda functions from the lambda functions, but technically possible. Because you'll have to do all the error handling in case things go wrong. So now you'll have to have a step function which causes an error routine. So it makes it complicated. You can do a lot of things, but the goal is to simplify, like with 4 lines of code, get an agent up and running, and then simplify as much as you can with best practices. Still have a few minutes. I am really hoping that we can. What's up? Uh, how do you support like multiple user sessions and memory? Like they ask, what's the weather in Las Vegas and then should I drive? So The idea of this architecture is Uh, because lambda goes away, as long as the session is there, it will remember if you want to do external state management where you can open a new terminal window and you want to ask, you can, what you can do is you get the information from the Dynamo DB or any kind of state management to figure out who that user is and what the context so far is, and then you can pass it along with the system prompt to the LLM. That's the best way to do it in lambda. Otherwise, if you use agent core and other. Advanced features where you can, it gives you the memory management out of the box, you can use those too. So agent code has a long short term memory where you can put the state information in and pass it in the context. Let's take 3 minutes of it. All right, it's done. Just final test. If it doesn't work, I'm sorry, you can come back on Thursday. We have a repeat as well, and we'll make sure it works then. Oops, no, no, no, no, no, no, no, no, no, no. All right. Oh You have questions? No, I think we got an error again. Go ahead. Is it? The question was how Come in in Las Vegas. There are Based on the size. Oh fuck, that's that one it will work that is. We will take a lot, so I talk to. That your That The way the tools are called. So in the system prompt, we basically define each tool and their function, right? So when an agent gives that information to an LLM that are the 3 tools I've got access to, the LLM will make a plan for it, saying, call this tool 1 to get the weather report, call this tool #2 to get how you're going to commute. So the plan is done and the agent aggregates that plan and gets the answer for you. Oh, so, like, you mean like, Sorry, but you get the idea, right, what's happening, so stick around. Give me 2 more minutes, please. Oh, he got it up If this doesn't work, I give up. Oh, You said my 5 minutes. I think It's the final, final version. It's always a final, final. Any ideas what you're going to use uh as a, when you go home? Any thoughts, ideas on agent, what you're trying to build? We got an error in the make file. Oh my God. We can bring our lunch in here and we'll make it work. And this is a prop question. Sure, just thinking about, so if you're willing to 8 and this is props. part of that, right. So I feel like any change to the changes, changes to that, you wanna kind of see, OK, what did that impact. What did might change. Do my age and now it's out in the wild again. Is there any best practice of like how do you like almost version the the system pro if anything changes or anything like that? Any thoughts on that? I mean, yeah, I didn't get, I guess every time, you know, it didn't get, you can see the changes that the tracking. OK, while I was running I was using this synthesis prompt, right? And now, now I'm seeing some new things. I know we changed it. You having any thoughts around that. I don't have an answer to that one, actually, because that's an interesting one. How do you System does not have a versioning on it. The gate has a I don't know the answer. I'll find it out for you. Alright, I think the plate is stuck for some reason in cloud formation. I don't know what to do at this point. But yeah, so it has to just roll back before we do the next deployment. That's what it is saying. But how do we change from the previous update. Uh, and that's why it is not letting us deploy, but I swear it works. Just trust me. Uh, one last request from our side, uh, if you can please go to the app and just give us feedback, except for the, this last part. Please, I would appreciate that. Yeah. So individual tool axes and then one final prompt which you saw on the workflow and it just works, trust me on this. The code is in the GitHub. I think we can share it later if we get approved, but the entire code is in the GitHub. Please, please provide us the survey. Uh, that really helps. We have not published the GitHub yet, right? Not published yet. We will be doing it after this, after. make sure your slide Can you bring up the slide with our contact info if you want to send us an email? Oh. Are we gonna get notification that you are putting somewhere that this is the case that you are. You, that's, uh, I think it's in both of our reports. So if can you bring us over, uh, information about how to contact us, the email. You can just send an email and we'll send you the GitHub report link. Because the question is like, it's not approved by AWS yet. It's a private wrapper. So we'll put all the disclaimers in the email before we send out, but you can use it. More than happy to just shoot us an email and we'll provide you the they need our email information. Do we have that not there in the slide because it's a standard slide, so all right, it's not there. I'll. There's a whiteboard. It's a Ah, it's Bavin, B H A V I N. CS Charlie M. At Amazon.com. CS Charlie, Sam. You can just type it up there, Rahul. That's what I'm doing. It's typing up there so you'll have it. At Amazon.com. So this is the email you can just, oh, and that's his his email pop Rahul at amazon.com right there. Hope it worked. Take care. Thank you, thank you so much for.