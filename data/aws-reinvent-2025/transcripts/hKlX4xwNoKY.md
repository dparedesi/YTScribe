---
video_id: hKlX4xwNoKY
video_url: https://www.youtube.com/watch?v=hKlX4xwNoKY
is_generated: False
is_translatable: True
---

Yeah, hi everyone. So I'm Sergio Merchant. So I'm the global CIO for Iberola, and I will, I will explain to you who are we, oh, and why we are leveraging the earliest technology. My name is Unai. I'm global AI expert engineer. I will show you how we are architecting agents at Verdora today. Great, and I'm Joseph Santamaria. I'm responsible for the technical teams within, uh, AWS that work with, uh, with energy customers in our energy vertical. Um, today we'll talk a little bit about. Um, you know, we'll start with an overview, a refresher on the AWI solutions for, for agentic and AI in general with an emphasis in agentic, uh, and from there we'll talk, uh, you know, Sergio will talk a little bit about, uh, why is generative AI important energy, what's Iberdrola doing, their strategy, and Dan and I will dive into actually what they're doing, uh, the use cases, what they've learned. Um, what, what they're doing, why they're doing it, how they're doing it, and one of the things that I've asked them, this is a learning conference is make sure that we share with the audience what you guys learned, what worked, what didn't work, and they'll do that so I think we'll have a, we'll have a, a great session. Um, so let's talk about what's, you know, what's agentic AI. We've been, you know, we've been in our AI journey for, for decades, and we're now at the stage of the era of agentic AI, which is really the ability to create, you know, autonomous systems that are based on agents that allow you to automate and, and perform workflows in a way very similar to the way humans would do it, right? You can plan the agents will plan activities. The agents will, you know, we have some logic. They'll be able to adapt a little bit to changing conditions, um, they'll be able to reason and, and that is gonna have the ability to really automate, uh, very complex systems and, uh, workflows that up to now have not really been able, uh, or accessible with, uh, with more traditional, um, AI and this is gonna impact every. You know, frankly, every process, every, uh, customer interaction in your companies, every product, every service you offer is gonna be impacted by agentic, and this, you know, revolution transformation because it's really what it is, it will happen. It will first happen progressively, but it will also happen quickly and progressively will happen because, you know, we have to earn trust that the agents can are controlled. They are governed. Iberdrola will talk a little bit about how they're doing that, um. And we're seeing now companies start to take agents to automate simple tasks, um, you know, a little bit narrow focus, very specific interactions with humans, quite a bit of human oversight going on, but now, now as companies' ambitions grow, and again Iberdro is a great example of that, they're moving towards the, towards your right of the chart here where they start to take workflows, specific business problems that can be solved with agents or accelerated with agents or the cost can be lower with agents. And um and they're starting to automate that and eventually what we're gonna get to is where you have systems and swarms of agents that either that from a deterministically or through other agentic coordination they get orchestrated to really solve very complex problems and end to end solutions um as I also and this will you know the the reason I also mentioned this will happen quickly and the reason that will happen quickly is because the price is enormous. I mean we're seeing we're seeing. Uh, companies are adopting genti workflows, things that used to take years, now take months, maybe at times weeks. Things that used to cost millions now cost thousands. Um, so you know we talk about within a Y Matt mentioned yesterday that we're seeing a 10x productivity gain. So the price to get this done is enormous, and that's why this is gonna happen quickly because companies are gonna be, you know, trying to drive and realize that value. Um, there's no surprise that Gardner talks about 33%, anticipates that 33% of all applications will will include agentic AI in the next, you know, 2 to 3 years by 2028, and that 15% of all the decisions that are made in a company, uh, will be automated through agents also with the same with the same time frame by 2028. Um, our aspiration, our goal here at AWS is to make that possible, um, and to help all of you build the most useful, well managed, controlled, secure, governed, and cost effective agents in, you know, in, in the cloud, and to do that, you know, we obviously have an AWS AI stack that is composed of three tiers. Uh, the top tier we have our applications, and they have a broad range. We have applications that will help you like, uh, Kiro, she's, uh, you know, code assistant developer will help you accelerate your SDLC. Um, we have Transform that help you modernize whether it's, you know, .net to Java or modernize within Java. There's a bunch of transformations that will help your, you know, help you keep your, your tech, your tech stack modern and up to date. And then we also have business solutions like Connect, which is a call center solution that helps you, uh. Uh, you know, bring. You know, manage customer interactions in different channels from voice to to chat and so forth and embed a number of agentic capabilities already um for those of you who wanna build some agents but you wanna do that in a managed service, then we offer Bedrock and our core product within Bedrock for agentic uh building is uh. Is Agent Core which we have, uh, you know, we released about a couple of months back and it's generally available and it has really great capabilities to, to, you know, to manage memory of those agents to have observability of what the agents are doing to manage how the agents are interacting with other tools through a gateway, um, it has a runtime that is framework independent, so you don't have to worry about what framework, what genic framework you're using for development. As a bunch of first party tools so we're bringing, you know, we're you'll hear today how Erola is using agent core and how is using this, each one of these components to bring these agentic solutions, um, to, to, you know, bearing fruit. And then at the bottom we also have infrastructure. What we're seeing is that more and more customers are willing and eager to unlock, you know, they have, they sit on decades of data. It could be great data. It could be asset data. It could be subsurface data. They're sitting on decades of data. And generic LLMMs, while they're very useful, they don't have that, you know, they don't have the benefit of that information, so they're starting to fine tune, um, they're starting to be more ambitious in terms of how they embed beyond rag or a knowledge base, how they embed that data into their work flows, and for that we're seeing more and more customers are starting to train, and we give you the infrastructure to do that with, uh, you know, we announced yesterday. Uh, forge, and Nova Forge that is, uh, gives you the ability to embed your own data, different pre-training, post-training, and training at different steps in the when we build, uh, when you build a Nova conditional model. So we give you the ability to, OK, I'm, I'm ready to train as a, a, a small language model, a specialty language model on AWS in a, in a, in a cost effective and, uh, in a very effi uh, you know, effective and efficient way, um, through the different epochs. So with that, let's just dive into a little bit Gen AI in the energy sector and what's going on at Riverrola. So I'm gonna invite Sergio to, to come up, OK, thank you, Joseph. Uh, hi everyone. Uh, now we are gonna explain how we are, uh, enabling also hope we can enable all this technology, all these capilities to support our transformation because I think you said, you mentioned about, OK, the challenges that we're having in the energy sector. So basically, as you can imagine is all the growth in terms of energy is all this electrification so it's putting a lot of let's say uh challenges to to the energy companies including Iberdola no so Iberola. Who are we? OK, so this is more on your slice, I think, no. So the, the challenges, but, OK, the challenge I was just mentioning, so it's electrification. So it's growth in terms of the energy, uh, consumption. So it's putting a lot of pressure in the energy sector, including lala. We want, we need to create a more sustainable energy, let's say a more cleaner energy, and that's why in order to tackle that challenges, OK, we need technology in order just to transform the way we are operating today, you know. So we need to be very excellent. In every single piece of our value chain, so excellent in our operations, excellent in the way we generate, create energy, excellent in the way we, we manage our customers, excellent in the way we manage our corporate activities. So that's why we need technology in this case enabled by LES, no, but let me just explain a little bit who are we, no. So Vedrola is the largest energy company in Europe and it is the second largest company in the worldwide, so by capitalization, so â‚¬120,000 million of capitalization, we serve more than 100 million customers. OK, we have an asset base of 160,000 million. OK, so and we invest $150 billion the last two decades. OK, so we are a very, very big company. We cover the full change from generation, transmission, distribution, and retail. And of course we have presence in many, many parts of the of the of the of the globe. OK, so we have presence in Spain, we have presence in the UK, we have presence in US, we have presence in Brazil, and we also presence in other countries in Europe and outside Europe, like for example, Japan, Australia, so we have a strong presence. So you can imagine. The challenge that we have in order just to respond to this kind of electrification, energy demand, and so on in order to create this cleaner and sustainable energy worldwide. OK, so we have a strong challenge to respond to. So how are we doing that? And this is very interesting, so. Our ambition is very clear. OK, our ambition is to put AI at the center, OK, and, and, and enabling AI capabilities to transform the way we are operating today, to transform the way we are, uh, working today across the life cycle, as I mentioned, and not only just this kind of approach, very incremental. So in terms of we try to identify specific use cases and specific processes that allow us just to capture some incremental value, but we are moving. On the full transformation approach, so we really get one process and we just imagine the process. We transform the process, we rethink the process, OK, so we imagine, complete the process using AI as the centric enabler for transforming the process and for that we will see that OK we are just basing on a specific model that is bringing value. Or is the way of the vehicle that we are put in place to capture the massive amount value of the technology, you know, but that's the approach, OK, so it's not just incremental, it's just real, real transformation, no full transformation of the processes across the value chain so we are seeing the capabilities of AI across the value chain. So the way we generate, for example, we are using AI to help us, you know, and just to define where to be positioning our, let's say. Wind turbines in order just to optimize the creation of energy of the generation of energy also through the operation, so we are also putting in place agents or assistants in this case in order to help our field workforce, OK, in order just to operate, do inspections, maintain the turbines as well, so help them, OK, in order just to do a quicker, let's say resolution, OK, improving the downtime and improving the way we create and generate energy. And in fact here we we have one of the main brains of of these use cases, but also we have use cases on retail on the on the call center area as you can imagine helping with this kind of assistance to our call agents in order just to be capable just to provide an answer quicker and with higher quality and the same agents across the corporate area. As our functions, OK, don't forget about corporate areas like, OK, legal, internal audit, administration, control, financing. We have capabilities just to deploy agents across the organization, and this is the way we see we need to transform in order just to be better prepared to respond to the future of the energy future. OK, what are the models that we put in place, OK, and as you can imagine, one of the main key success factors is going to be the platforms. It's going to be the technology, and I will explain that. Now, in detail, no, so we are moving from traditional assistance, OK, AI assistance to the agentic AI now that is gonna be the next wave, OK, and we are building our best in class platform and agentic framework leveraging AWES services, but we also have another key success factor that is just our competence centers. So we just, we have been building competent centers across the group in a happypoke model, and one of the things that we are doing is just to standardize our method. To deliver solutions to deliver products to our business areas, OK, you show about our, let's say, country-based, let's say that we have, we have too many we serve in so many countries. We have too many businesses that we need to respond to. So the first thing that we put in place is let's try to standardize. Let's try just to build something that is standard, common for everyone as much as we can, OK, not just as well to accelerate and create this kind of speed and time to market. But again, so we build a center of competence, including in this competence center all the capabilities that we need to deploy the solution. So not just moving to one area to another area. So I need the architects, I need the engineers, I need the project manager. I need the operations guys, the security guys. We just put everything together into the center. So we just create this one stop shop, so this kind of factory concept. So we just deliver solution end to end from the engineium for the discovery, the engineering, the architecture, the operations, security, everything. The second point is platforms, and this important point is also one of the key success factors AWS. EWS is not just providing capability in terms of the AI's resources or services. We just create a platform with all the different building blocks to embed this kind of framework. On top of that, so when we deliver an AI solution, this AI solution is secure, is observable, so we can monitor that solution. We can ensure that we are delivering something that is secure, is protected in terms of data, and is responsible. So we create all the framework that we need in your, in your to, to deliver something to our business areas, OK, that is creating security, is putting, let's say, uh, obserability, is putting. Uh, monitoring, it's putting auditing, it's putting logging, it's putting everything in, so that's platforms again, another success factor, and the last one, as I was just mentioning. Coverity centers, OK, so we have a Habspoke model where we just deliver the same concept across the organization no matter where we are. We just operate in the same way, OK? We just leverage the same method. We leverage the same teams, the same capabilities, the same layers of services, the same platforms across the group. No, I think that's another area of, of, of value, no, because at the end with this concept we can create. Uh, let's say solutions that are are quite standards, are flexible, are modular, are secure, and in a very, let's say time to market way, no, and also, OK, being this kind of partners that we want to be with the business just to become this technology partner. No, no, just look at IT as the standard, let's say business as usual activity, but to be this kind of technology leaders, no. And now, OK, I just now now you can come here and explain the way we are delivering gentic capabilities, no, in the company. Thank you, Thjil. So today I will walk you through the global deployment of agents with Amazon Pedrocay and Corati Verdola, and we will see some use cases later. Um, This diagram represents the way we are scaling AI INT initiatives across the entire river rolla group. If you turn your attention to the left side of the screen, you will see our global footprint. We are deploying Amazon Pedrocaencore in our four core major markets UK, Scottish Power, US Avangrid, Brazil, Leonerjia. And Spain of course with this uh regionally uh deployed platform we all we ensure data sovereignty and we also uh ensure that the agents stay where they belong, uh, complying with local regulations and standards and reduce some some networking costs of course um but the interesting part maybe is on the on the on the on the right, uh. This is how we are, uh, structuring the platform. We have a multi-count strategy in which we have 3 environments production, pre-production, and development. And for each of those environments, uh, we are, we have a strict verticalization by business. Uh, we have an account for retail, for networks, for corporations, for renewables, and for IT. And with these regions, companies, and environments we ensure three things mainly. First of all, we guarantee scalability in each region. This means that where there is a surge in smart meter data in Leonarchia, for example, or a new commercial campaign in Spain, we scale independently. This means that we have our agents isolated, scaling independently with the powerful approach of serverless approach of Amazon Pedro Caenco. We will see that, uh, later, how we do it, um. Second, we, we, we ensure business isolation by design. This means that we are securing its business line and ensuring that and limiting the visibility and sharing of agents, uh. On place. And third for me, uh, probably the most important thing is that we are ensuring also reusability and discovery within the same business. This means that for example, if the networks teams in Spain builds a great agent, by design, we are allowing them to share it, to reuse it, uh. Because of the platform, uh, deployment strategy. So the good news here is that we are not just building the platform, we are also the first adopters of it in IT. So, uh, let's see, let's jump to the next, to the first three use cases of A and Quarti Bertola. Um We, we chose IT operations as our first battleground. Uh, we have 3 use cases, simple use cases that are delivering value now, uh, in our in our operations. The first one is the ion support, uh, change requests. Uh, we are working in service now and improving, uh, the way users work with changes, uh. In an utility company like ours, uh, making changes to systems is high stakes. You need to accomplish with a lot of compliance and standardization, and this often brings a cycle of rejection, delays because of the petitioner not fulfilling correctly the drafts. We, we now have a group of orchestrated agents that act as gatekeepers and help the user fulfilling those drafts. We will see details later, but that's the main idea, reducing delays and resource waste. Uh, the second use case is about enrichment, enrichment of networking incidents. The same rationale, we want to reduce manual effort and improve time to market by boosting tagging, assignment, and resolution speed, uh, having a swarm of agents that are specialized in specific things, uh, of the request. And third, probably the most simple one is related to the first one. it's a chatbot-like agent that is connected to a corporate knowledge database that allows the user to pick the right template for the change, uh, in ServiceNow. We have more than 200 change templates, so it's not difficult, it's not easy to pick the right one. Now we have this, uh, rack architecture to help the users, uh, doing so. We'll see later the details, but that's the main idea. This is architecture, so you came here to see technology, so there you have, um. Uh, first of all, I would like to thank Accenture and our partners, uh, necessary partners and areas in Iberdrola for achieving this milestone in record time. Uh, we all know that we are not only delivering value in these business processes, but we also somehow building the, we are building the foundations for the future, preparing the platform for what is coming next. So, uh, we will review the architecture at high level now, and then we will do a zoom in on the use cases and see how the agents are working under the hood. Um, first of all, I would like to highlight that we are Uh, we have one agent called runtime for each of the use cases. This means that we have decouple the costs, the development life cycle, and the escalation by, by, by splitting the use cases, uh, in runtimes. Each of the runtimes, of course, have a docket image in the ECR, so we can deploy with a continuous integration pipelines and continuous deployment pipelines to it. Um, these runtimes are running in Landgraft agents that are consuming, uh, models in, in Bedrock through little LLM. This little LLM is a layer of code that allows us to balance requests between models in case of an abilities or, or quota cannibalization. And we have of course some guard guards in place to ensure that the models generate what they have to generate and not and nothing else. Um, the second thing I would like, I would like to highlight is that we are deploying the MCP servers, uh, also in AM core run time. This means that the use cases that are using, uh, the database, uh, the RDS postgrass SQL, uh, database are, are consuming it through, through this, uh, MCP server. So we have that in place, um. And we are just using also AN core gateway for for reaching ServiceNow and writing the results of the agents where where they are needed. Um, I didn't explain that, but we, our front and in this case is ServiceNow so it's ServiceNow who is calling uh EWS Cloud to to perform the entic workloads. Um, we have also, if you, if you see a micro gateway there to ensure authentication and routing the the request to the To the particular run time. So this is the high level perspective of the architecture. Now, uh we will review the first use case, how, how, how is the functional details and the technical details. So, we call it As assist for change. I was telling before that um Making changes to systems is difficult, and this is why we have a strict process to make changes to systems in Iberdola, as more as most companies do. Um In our case, each change request passes through 66 phases uh from draft where the requester has to fill out a change model to ensure that the information is in place to close when the change is done and and the teams start working on it. We are deploying agents at the draft phase. This means that we are speeding up the the change management process by assisting requesters. um. These templates that has to be filled has several fields and several rules, so we have experts agents on each of the of the fields and each of the parts of of of the templates, so we can first validate and also propose uh fixed proposals so we we we can speed up the process. Um, let's see How it looks, OK? So the requester does this change request draft and pushes the button of the start change uh when when the information is filled, and then a workflow of agents is triggered, uh, we, we are building small agents, OK? So we have a first agent that is determining which fields and contents must be checked. Using LLMs and tools available. Once these rules that we know that has to be checked, the content validator takes this this input and basically validates fields and content based on the rule structure output, simpler, and in parallel, there is a third specialized agent called a model agent analyst that validates the change model selection based on the justification. This is the main point that the third use case is solving of checking if the change model that is being selected it's appropriate to the change that the User want to do. And if all is in place, uh, there is a 4th agent in land graph that is going to, uh, propose fixes in the case there is something missing or automatically transition the phase. So we are somehow putting an intelligent layer in this service now front end powered by, uh, encore. So, I will show you um 3 diagrams like this. In the Top part of the slide, uh, we have the technical flow. Our service now is calling the micro gateway, how the micro gateway is routing that request to the runtime, and the runtime has these land graph agents being run, uh, using, uh, cloth and using the EDS RDS MCP server. So that's, that's pretty much of it. If you, if you turn your attention to the, to the land graph agents, you see that, uh, we have a graph structure in which we are running the agents sequentially, nothing special there. First, the rule structure is triggered, the content valid validator, uh, takes that, uh, output. Validates the contents and at the same time we are, we are saying as as I was saying before, uh, validating the aim of the model. If that is in place, the phase transition, uh, is done. In the bottom we have the ETLs, OK? We are putting ServiceNow. Templates and change models into S3 bucket and this S3 bucket is being used by a pot in Kubernetes. To build the gold layer of the data that the agents are consuming through the MCP server. Uh, a good thing to know here is that we are using postgrad SQLPE vector for the vectorization of the data, and we also have some data models in, in place to ensure that we are versioning prompts, uh, to ensure that we are versioning embeddings, and to ensure that we have all this life cycle of the, uh, data, uh, in place. The second use case is the same structure, the same strategy, but simpler. Uh, to help the user selecting the right model, we are putting a simple agent. In, in, in the runtime. So ServiceNow again calls the micro gateway in this case with a different, uh, with a different uh data, so the micro gateway can route the request to the appropriate. Uh, runtime and this runtime is going to help the user in a chat-based interaction to pick uh the right model. We also have the same thing. We have the ETLs uh that are going in a regular way uh being loaded to the to the database. The third use cases is probably the most interesting one because it's uh the most complex one. we have the same strategy again we service now calling the micro gateway, the microg we call the runtime, but in this case the runtime has a swarm of agents that in telling uh somehow uh uh we have an orchestrator that is able to know which agents has to be triggered when, uh, depending on the information available in the in the incident. So, uh, if the networking incident, um, Has to be tagged. The orchestrator will know it and it will trigger the tag classifier. If the, if we want to uh perform incident similarity, we will have the incident similarity agent in place and so on and so on. So this is the probably the most complex, uh, land graph um. Mm. Architecture that uh we are we are deploying in these three use cases we of course have again these ETLs from the S3 to the RDS to the golden layer and the post-gray SOQ LP vector uh. To to be able to consume the data. Um I didn't say that, but we are using cloud models for for the for the use cases. Uh, we are going with the 4.5 sonnet, but we are, we are flexible, uh, depending on the, on the needs. So, yeah, that's that's important too. Um, but why have we chose, uh, encore runtime and not other compute resource? Um, first of all, it's really for us it was really important, uh, the fact that an inquiry is framework agnostic, so we didn't have to upskill the teams to learn a new framework or to learn to code in, in, in, in, in another framework. So you just can put a land graph code. Expose it, uh, uh, seamlessly deploy it with MCP protocol. So you just need to ensure that you have this fast, fast MCP, uh, this decorator of MCP tool and MCP run. Uh, you can also deploy the agent with, with HTTTPS. We decided to do it with MCP, so if we want to reuse it in another use case, we have that in place, but you can do it, of course, with HTTPS. Also, runtime is really cost effective, uh, compute resource, uh, you are only being charged by actual CPU consumption and we all know that agents, uh, pass more or less between 30 and 70% of the time calling LLMs and calling other tools. So this time they are waiting, we are not being charged. This is really, really, really good to to economies of a scale. Uh, thirdly, of course, ANcore give us the session isolation so we can pause the session, make the agent stay full, and, and, and empower this, this human in the loop loops and, and wait for, for agents and other agents or tools. In our case we are not using that so much because the EU's cases are simple, but it's good to know that we have that available. Also, it's a low latency architecture. This means that there is no gold start. Maybe you come you come on Monday, the agent has been sleeping all the all the weekend and you will have no, no, no gold start. You will have that ready in in in about 200 milliseconds. And also it auto scales. If one of the agents or architectures are going to be are are are receiving more demand than expected, you don't have to worry about uh scaling that yourself or managing the infrastructure. The auto scales like magic. Also, uh, it's really interesting for us that if we are running the agents in those runtime we have available so, uh, so many managed services for for this kind of projects. The memory, for example, to make the agents have soft memory so they can remember what is going on in the session, but also long-term memory so they can learn and then can improve over time. Also we have the gateway for the tools. We we can put our tools into the gateway to to enable discovery and to enable uh also uh uh roles for for the access. So it's really interesting and we believe that uh it's difficult to find uh similar thing uh to this as easy as this uh er in some alternatives. Um, so assuming in the code, you just need, as I was saying, uh, to deploy the the the agents, you just need to ensure that you expose them with MCP or with HTTPS and that's all. You can focus on the programming of the agents. As is and and and forget about infrastructure. Um, Just to give you a, you know, a code snippet of what is going on inside, this is for the Uh, Swarmofayenza I was talking before this orchestrator that enriches the incidents. Um, so we are using land graph, and land graph is all about graphs. Uh, we have a planner node that is going to decide which science has to be run. And we have a note of front next that depending on that output will run the the the agents in sequence. We could parallellyze that too, but We didn't do that. And once the agents has been run, uh, we have another node to assemble the result. So as easy as that. The difficult thing here is that you have to, to program what is inside the nodes, of course, and don't have, I don't have a code snippets for that, but you could imagine that it, they are all calls to the LLMs and some, some business logic. Um, yeah, so that's pretty much of it. You can program in strands, land graph, Google ADK, whatever you want, put that on the run time. And it works. Uh, what's next? Um, this is for me the most important part of the presentation because these use cases, OK, we are delivering value to these business processes. We are preparing the platform, uh, so, so businesses can, can, can work with AN Core, but which is our maturity tire, which is also our role as IT to, to transform, um. We want, we foresee some challenges of non-reusing the agents or not thinking big when Building an agent, so. If we build a great agent and we cannot reuse it, we have to program it again, develop it again to, to put that on another use case in another area, we of course face increased costs due to the need to develop existing capabilities, but also we duplicate business logic which can lead to unpredictability, unexpected behaviors, and we also have this risk of duplication of sources of truth. This happens also in microservices. This also happens in traditional software, but with agencies especially because, um. We, we have these A uh these these capabilities of of generalization that maybe traditional software don't. So, so we have an opportunity to fix. These 3 things. Uh, we believe that Amazon Pedro Cayenco will be Part of the solution or the solution to these problems, but it's also a cultural transformation of how we Think about agents and and and transform the business processes. Um, for me as a technical guy, what are 3 things I want to do? To use an agent. Um, First of all, we have the first scenario, right, in which we have a good agent that is successful, we want to reuse it across the organization maybe in other business processes or maybe by other people, and the first scenario is when the original owner retains the ownership and has to authorize new users or new areas to use it or to integrate it in in other business processes, um. This part is easy, right? You just need to put some network in there, maybe some roles and and and you are ready to go. But there are two other use cases that for me are probably common to or will be common to when you have an agent, we normally want to introduce tweaks to it because we know it works. It has access to some tools, it has access to some data, and it works fine, but for my personal business case, I need something else, some extra detail. So for that we need to Work with persons, right? Uh, the second use case or the second scenario is easy, is when we have the owner keeping the ownership of the agent and we ask the owner. To tweak or to improve that agent so it works for me too. So that's about versioning, that's about uh governing agents in a registry. Uh, but has this problem of making the owner work, work, making the owner do something for you, right? And that's not easy sometimes. Uh, so we need also the possibility to for these agents to be able to share the code, to serve the tools, to serve the infrastructure behind so we can ourselves with our budget, with our development team to adapt it to the specific use case, uh, using the original agent as the base. So these are Uh, the three things that we want to achieve, uh. We have plenty of time now for, for questions, for, for details if you want to ask something, and yeah, I think that's Pretty much of it thank you.