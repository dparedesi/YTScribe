---
video_id: NKF4qKnOaCE
video_url: https://www.youtube.com/watch?v=NKF4qKnOaCE
is_generated: False
is_translatable: True
---

Hello, everyone. Thanks for joining today's session. For enterprises that operate in regulated industries like financial services, compliance is not just important, it's existential. My name is Hassan Tariq. I'm a principal solutions architect with AWS. And I work closely with customers like Stripe on their AIML initiatives. And joining me today is Chrissy and Christopher from Stripe, who led the development of this solution. I'll let them introduce themselves when they take the stage. So, here is the agenda for today. Um, we'll start with covering some background context of, uh, why agents can be a good fit for, uh, compliance use cases and then go over some background and challenges of, uh, the actual use case that, uh, compliance teams faced at Stripe. And then what solutions they implemented and um you know the benefits they achieved from it and ultimately we'll end up by discussing what's next and some lessons learned along the way. So before I go into the details or, you know, we start talking about how agents can be beneficial here, uh, let's take a look at what's at stake here. So, according to a research study conducted by Forrester. Enterprises spend around $206 billion per year globally on operations related to financial crimes. With North America alone, spending around $61 billion to this total. Now this is a very massive spend. But it reflects the complexity and resource intensity of meeting regulatory requirements. And this is not just a static number, um, it's growing. According to some compliance teams based out of Europe, they shared that the compliance regulations and the demands that they're getting from the authorities is increasing by up to 35% year over year. So this means. They are Working through these different cases and have to produce a lot of documentation and spend a lot of extra time doing that. And that's where when Experian conducted a survey on with these compliance teams trying to figure out what type of tasks they perform, um, what type of documentations they need to do, they discovered that almost 1/3, up to 1/3 of these tasks can be automated. So just imagine giving back 8 to 12 hours per week. To these compliance analysts so they can focus more on strategic analysis and decision making. And that's exactly where the agents can play a big role here. Um, think about the best compliance analyst that you work with. Um, they're really good at asking follow-up questions and connecting the dots among different various different, uh, data sources. So if agents can do the initial investigation and data analysis, these analysts can focus more on the complex decision making and, you know, nuanced judgment calls. Um, when these agents can do the heavy lifting of the analysis, initial research, it frees up a lot of the analyst's time, and they can scale up. They can perhaps review a lot more cases than they are regularly doing. Um, it's like giving each analyst a dedicated and tireless research assistant. The, uh, this whole process brings consistency to this analysis also, which means you're ensured that every case will go through this rigorous step process and none of the data sources will get missed. And while doing all of this whole process through agents, um, when the analyst is focusing on making the decisions, the agents are doing the documentation and preparing the audit trail for those decisions. So in short, analysts focus on the why, whereas the agents focus on the what and how. So this is all great, but you need a very solid foundation for these agents to operate and we will hear shortly from uh Stripe team how Bedrock provides this foundation. So now I will pass on to Chrissy to go over some background and discuss about the challenges that they faced. Thank you, Hassan. How are you doing? Happy holidays and happy Cyber Monday. Um, if you care about shopping or payments. So, before I dive into our AI use cases, let's take a step back and talk about Stripe as a company. So many people know Stripe as a payment processor. Last year alone, we processed 1.4 trillion in volume with a big letter T. And that is 38% up year over year. And take a guess. So that number constitutes 1.38% of the global GDP. Wow. So we are great in payment processing. And we bring good vibes to our developers. These are all great. Over the years, we are also evolving into this comprehensive financial infrastructure platform. So, our internet, which is uh was invented in the early 90s, uh and it wasn't designed with a native protocol for payments. So Stripe build the financial infrastructure on top of that. And that is what's enabling the emerging protocols, such as agentic commerce protocol, ACP. If you follow us closely, uh, on Twitter, now called X, um. So Uh, our platform is very modular. And you can choose exactly what you need. Um, may it be global payments, moving money. Or platformization of your money via stripe Connect. Or automating back office tasks like tax and billing. We handle the heavy lifting by integrating with local banks across 135+ local currencies. And while doing all these, we're maintaining 5/9 of time and on average, we're processing over 500 million requests per day and that number might just slightly went higher during this BFCM. If you're interested, uh, please go to BFCM.strip.com to look into further information. So, definitely we're solving very interesting engineering challenges. That is just a means to a bigger end. So you ask, what is a bigger end that uh. The bigger end is our North Star, growing the GDP of the internet. Sounds amazing, right? So to realize this macroeconomic impact, we realized as a company very early on that we need the solutions to scale with our explosive growth without sacrificing safety and compliance. And that, my friends, bring us to the heart of our challenge that we're discussing today. So when you ask people, what is FinTech compliance? Most people will say, well, just following the laws. But at Stripe, our answer is, it is a foundation of trust. It is a mechanism that ensures that our platform um Uh to ensure that our, our, our platform. Um, Ensures our innovative technologies follow the rules of global finance. We view trust as a unified responsibilities with two critical dimensions. One is ecosystem integrity. We enforce know your customer, know your business, and the money and anti-money laundering. This is to ensure our platform will never become a haven for crime. So think about this. If a sanctioned entity tries to use a fake identity to onboard to stripe, our compliance engine would be able to block that instantly. You might think it's easy, but it is actually pretty hard in reality, because, you know, these bad actors are smart, they're constantly evolving as well. And an equally important dimension is to protect our users. At Stripe, users first, users first is one of our leading operating principles. And that's one thing I really love about Stripe. I've been with Stripe for almost 5 years. So I think we really, really care about our users, may be adhering to GDPR for privacy or UDAAP for fairness. We ensure our users are never exploited or compromised. We realized that if we don't protect the people that rely on the system, building a perfect system does not have a lot of meanings. So, to build up this very effective defense, there are 4 factors that shaped our approach. 1, growth. We're constantly growing into new industries and markets. That means, uh, onboarding new users, fast and safe is paramount for us. And second thing is the scale required from a compliance perspective. So, we need to do enhanced due diligence reviews at scale. So short form is uh EDD and we do this, so we can manage users' risk effectively. Third, uh, we are always striving to reduce the time to do these reviews, to raise the bar for our operational excellence. Um, the last, technology innovation. So this is where Agentic comes in. We want to completely transform the manual compliance review process. And To build a solution to make it faster, more consistent, and infinitely more scalable. So What is the core of our business challenge? Due to regulatory compliance requirement, we need to do extensive reviews for high risk users. So think about this almost as a detective, right? So they're looking into some suspicious, uh, you know, like, uh, individuals, they're trying to figure out, are they actually suspicious? So as you can imagine, this requires a lot of the investigation time. However, we're doing this every day. We cannot take a long time. So doing this in a timely fashion is critical for us. So to help you visualize the manual review process by completely done by human a little bit better. So, I summarize, there are mainly two blockers. One is from the manual process. We realized a lot of times, Our expert reviewers are acting as navigators. They're just like going through all these different systems trying to gather and locate information instead of the analysts that they're supposed to be. So they're not really using their time to making high value decisions. And the second thing is scalability and the jurisdiction hurdle. So the biggest bottleneck is the cognitive overload when switching between different jurisdictions. So let me give you an example of this context switching. Say an expert analyst can move from assessing an entity in California, where the retrieval process is rather straightforward. To uh evaluating a complex corporate entity in UAE or Singapore. So for each jurisdiction region, it requires a completely different mindset to decide what constitutes as a risk. Making it more fun. So the threshold of risk is a variable. The definition of a safe business is not static. One example for that is regulatory compliance. So for like say for example, ownership transparency varies widely between high risk and low risk jurisdictions. So thinking about this, right? So your expert analyst is basically applying this ever shifting rule sets constantly. And this is a very demanding and complex task. Sometimes can even be error prone. Just maybe a little bit like more analogy to help you visualize this, right? So every year I go through this pain once during tax season. Like you need to go through a lot of rules. And I'm only doing it for the federal and California. So imagine how much pain these reviewers are going through. So, uh, bottom up, we are trying to maintain the operational excellence for regulatory compliance by navigating through this fragmented jurisdiction map. Just linearly scaling up our workforces with the complexity is simply just not the solution. So you might ask, what is the solution? If this is a quiz in college, I think you guys will all get perfect scores because Hassan kind of gave the answers away already. So to solve this, our solution is to build an NLM powered AI research agent to do autonomous investigations. Here, the agent does have a lifting in 3 ways. So, number one, async workflow oxidation. Uh, this enables parallel processing. So the agent can look into different aspects of the business in parallel, instead of sequentially, like a human would do. Second, because we build this, this plugs directly into the existing compliance schooling. So it works nicely, fits naturally into the compliance workflow that our reviewers are used to. You don't need to train them to the new thing. And the last bit, very exciting, agents, they never sleep. They don't need to rest. Think about this feeling, right? If you have a Roomba at home, you turn a Roomba on and you go out and hang out with your friends, and then you would come home, the home is already clean. That's an amazing feeling, right? So having an AI agent to do these analysis before you even open the ticket is exactly the same thing. It will do the investigation and summarize the insights before you even open the tickets. So this is a game changer. And now we have the AI agent to assist and the humans to maintain the final authority. So, Before I hand over to Christopher, to do the deep dive into our solution, I want all of you to leave this room with with at least 3 takeaways. And obviously, if you have more takeaways, that's a bonus for you. Kudos to you. Um, so the number 1 is oversight. We use, we use a human centric validation. Here, we're not trying to use AI agent to replace the decision making. Instead, we're using agents to empower that. And all the expert reviewers have configurable approval workflows. So the agents are there to assist and the humans make the final call. And the second thing is, we, uh, like Hassan mentioned, we keep a complete audit trail. Um, and we record every single decision and the rationale behind it. This makes sure that we will have enough data to serve as a compliance grid evidence. Um, the last bit. So with all these agentic stuff we're doing, we supercharge our reviews to enable our expert analysts to go deeper, faster. With all these pre-fat analysis. So, with all these things, this is ensuring that we're not just building for speed, but we're building trustworthy, auditable, agentic compliance solutions at scale. Now with no further ado, let me welcome Christopher to the stage to dive deep into our solutions. I. Thanks, Chrissy. Hey everybody, my name's Chris. Uh, I'm a data scientist at Stripe. I'm gonna walk you through a little bit of the technical implementation, um, and. Let's just kinda roll through with what we're gonna go through, so. We'll start with kind of back to the problem setup of exactly what we're kind of trying to solve, why we're using agents, that kind of thing. We'll jump into the agent mechanics, kind of how it works under the hood. We'll talk about how we actually plug into the LM, kind of the brain of the agent in this case. Um, next we'll talk about kind of an interesting service that we discovered we needed, um, as part of this journey. Um, we'll finally zoom out, talk about the, the kind of striped ecosystem of agents. And then we'll get into the business side of the journey, the results, that kind of thing, so. Let's jump in. So let's start with the problem we had this basically very, very long manual review. And I think the first thing we wanted to think about doing was just boy what if we just threw an agent at it and effectively just automated the whole thing and that's uh kind of a nice fairy tale um but in our case it wasn't really gonna work um. The problem is this if you just took an agent and you threw it at this workflow, it'll probably spend a lot of time on things you really don't even need it to spend on it'll kind of rabbit hole, and then it won't spend really any time on things that you needed to look at. I mean this is has regulatory reasons for existing, so we have things we need to look at in many cases. And so we realized early on it it kind of made sense to have rails for the agents and so there's a, a little diagram on the slide there it's, it's kind of meant to show you that we effectively were able to decompose this really complicated workflow into effectively a, a DA and the DA is effectively these, these rails that we can use for these agents. Um, now we happen to use react agents. Um, one nice thing about this kind of DA deconstruction is the tasks become effectively bite size enough to actually fit in the agent's kind of working memory. They do have a kind of a limited amount of, you know, thought processing just the way we do. I can't keep everything in, in my head, um, and the reason we're even using agents in the first place, and I think this is important, is, you know, why not just use like LLMs or something like that. Well, I think the reason, um, agents work well is because they can call tools and I actually think that's the main value um I have a very biased perspective maybe on agents which I'll share a little bit later, but, um, I think that's why agents are even practical here and we'll talk about that a little bit more. Um, one interesting thing, and this, this goes back to the fact that yeah it would've been nice to just throw an agent at it and have it be done, but we actually keep humans at in the driver's seat in this case they're making all the decisions. Um, agents are just there to effectively inform our reviewers to make the best possible decision. They can use it or not. That's up to them, but, uh, this is very much, uh, still a human driven work flow, and we, we still found, um, a good way to make that work. So who wants to see the agent? Actually, I think I'm. I think I'm getting a call right now. What in the world? I think, I think he just finished a case. What's that? You want me to tell him about Amazon Bedrock? You know what, I, I'm doing the presentation here, all right? All right, we'll get, we'll get to it. Agents. All right, so we're using a react agent. Let's, let's talk a little bit about what that is. I, I kind of think agents think a little bit like I might if I have a complicated problem. Someone might give me a question, we'll call it a query in this case, OK? Let's say it's like 10 divided by pi, OK? I'm not gonna know that offhand. Some, some of you might actually know that, but I don't. So I'm gonna call an action to use a calculator, and I'm gonna wait for that calculator to come back as my observation. And at that point, I'm going to think if I now have the answer, yes, I do. So therefore I'm gonna progress to this final answer stage. So really what we care about in our review is ultimately this this final answer that's really all we care about what it does in the meantime is kind of an implementation detail to to our uh project I guess you could say now in reality and the reason we even use agents in this this problem is that it's we have much more complicated queries than something like that and so. Look, I'm a data scientist, so you know maybe I get like an analytics ask or something like that. I might need a a few queries. I, I have an idea of an initial query I'm gonna do that might be close to what I needed, but maybe I see some issues. Maybe I'm gonna rerun that query a few times and so you can imagine I'm looping through this thought, action, observation loop. As many times potentially as I need. You can imagine and this diagram doesn't depict it all that well, but you can imagine this could actually loop around forever and then it would maybe be true AI whatever you wanna say but at the end of the day LLMs have a finite context window and so this effectively becomes a bit of a challenge for us. So In our investigation, really the reason we're using agents is because we need it to rabbit hole sometimes, sometimes we don't. We always need it to cover all the bases. And so when we're basically going deep down in some of these rabbit holes, these prompts can get very long. So, one thing I like about this slide is it kind of depicts this kind of 1 + 2 + 3 + 4 type of kind of behavior. So I'm not like a 10 year old gals, but that looks kind of quadratic to me. So, effectively what that means is that as the agent needs to dig deeper and deeper and deeper to answer a question. It's effectively getting a longer and longer prompt that you effectively are paying for every single time. And it's gonna be actually a quadratic cost over those, those number of turns I believe. If you think about the entire cost of your, you know, agent operation. You're paying for input tokens, you're paying for output tokens, and you can kind of tell here, probably you're gonna be dominated mostly by the input tokens in this case. There is a solution, however, to this. I, I believe that solution today is in the form of caching. And so why is that a solution? Because. If I don't need to reread that prompt every single time, it's almost as if I'm paying for that final prompt once, which is more of a linear situation, which is superior. And that's where we're gonna get back to bedrock. So Bedrock is our provider that we can utilize effectively for this kind of prompt caching, but that's not all I'm gonna talk about in the slide. We do use Bedrock, um, for our LLM provider. Um, we also have our own internal Stripe services as well. So, um, we use this, we'll call it the LLM proxy service for this presentation. I think one way it really helps out a lot I think on this uh project is that we have a lot of people using LLMs at Stripe. And so you know what could happen is I don't know, let's say another team's doing some testing or maybe they're scaling up. That could crowd out my bandwidth on a potential LLM provider and so by centralizing this proxy service we can effectually solve this noisy neighbor problem, um, to constrain basically those neighbors from hogging all the bandwidth I might need, uh, you know, for this, for this project, let's say. Now there's other benefits too, obviously we, we have, you know, guarantees because we're doing it ourselves around kind of the availability of this service, etc. we can do authorization we can make sure the right LLM is being used for the right use case, um, maybe there's some LLMs we feel like is too sensitive to, to run certain data on, etc. and so this allows us to kind of funnel those use cases to the right place, um. Potentially this service allows us to also configure things like, you know, model fallbacks as well if, um, one of the model providers isn't, you know, was out of bandwidth or something like that. So why is Stripe using Bedrock and why, you know, we're using AWS in this situation? I think one of the biggest benefits I see, uh, for this project is, is actually probably around the standardization of the privacy and the security, so. I don't know. I don't know if you've like worked at a big company before. Maybe you wanna use this like new LM provider for your project, um, but then that's not gonna work because it needs to be vetted by security. And one really nice thing about Amazon for us is that we effectively, um, do that vetting once to get on that platform and then it's effectively standardized there so. Um, Bedrock provides us access to multiple vendors and models, um, and we get that kind of standardization of that security around that. The second point I want to say is more on kind of the bells and whistles that come along with it. So we talked a little bit about the prompt caching already, um, that can save you a lot of money if you're, you know, doing some sort of eugenic flow where you're doing multiple dives deeper into the data. Um, I also think fine tuning is a really interesting one too, and I know like. The vendors have been providing really good models lately so maybe it's kind of fell off the radar for a lot of people but um I see a big opportunity for this project mainly because. The vendor model comes with kind of unforeseen like deprecation schedules let's say like let's say we wanna be focusing on adding new questions, but oh boy, now this new model that we, you know, we're having great success with is gonna get deprecated well I prefer not to have to focus on old questions. I wanna focus on new questions and so by fine tuning maybe I could claw back some performance. I think the bigger thing is just that we can now deprecate on our own schedule, um, and that that seems amazing. So I love the bells and whistles that come along with, with Bedrock, um, that's, that's been very helpful. I, I believe there's more as well. The last point I'll make is, um, that it's really one API but you're getting many models so this kinda comes back to the standardized privacy and security thing where you plug into it once you don't have to have that overhead of, uh, you know, basically in this case setting it up, connecting it, doing the integrations, etc. Another thing I'm gonna talk about here is. When this project started, we didn't have this service called this agent service. So why do we have it now? Well, because we didn't have it first, basically. What happened was. We had this kind of classic ML inference system across Stripe. And uh the original plan was to basically hack in uh this kind of agentic workflow into this kind of ML inference system. Thinking it's close enough it's gonna it's gonna get us there and it was shot down quickly for uh very good reasons and um those reasons are mainly that it really requires a totally different set of uh hardware, um, among, among other things so you can imagine with really traditional ML you're you're really compute bound if you think about it, you know you. Let's say you're running an LM you're gonna need a GPU or maybe at least one GPU. Um, if you're running something like XG boost, you might want a lot of CPUs, but that is what you're waiting on. Um, you're probably getting a pretty consistent latency with whatever that computation is, um, so you have very short timeouts, for example, um, you also have very deterministic control flow. The model always runs exactly the same way. Um, and you're, you're basically paying a lot for these machines. They're hard to set up. You're trying to minimize the amount of machines you need, etc. Um, agents are kind of the opposite in all those aspects, so. Agents, particularly if you're using a vendor model, you're just waiting for it to come back you're waiting for the LLM to respond to you, um, and so you're actually not doing a lot of computer at all you're just like waiting around you need a a lot of threads, a lot of lanes basically to wait around for all the different agents that might be running on the same machine. Um, you can expect very long time outs in some cases. You can imagine like what if a tool call needs to call a database. Could be minutes. I remember, uh, when we had engineers like first spinning out the service and they're asking me. How, how long of a timeout do you need? Like, is 30 seconds OK? And I was like, I, I think we're gonna need like 5, 10 minutes. So very, very different profile of, of compute, um, also very nondeterministic. I mean some answers it might know very quickly it only needs one tool call. Some may need to loop around, um, in that, you know, thought, action, observation loop for quite a while. So, you don't know exactly how long it's going to take. Um, tiny machines might be true and might be false, but, like, quite honestly, you really just need a lot of lanes to wait, uh, for, for network calls. So we'll talk a little bit uh about the history of this agent service. Um, we'll start with like there's almost like beginning of Q1 which isn't in here which, which is like it didn't even exist, OK? And we didn't even think we necessarily needed it. We tried to hack it into our kind of traditional ML service again that got shot down. Um, but we realized it was gonna be very impactful for this project. And we saw that it was gonna be something that was gonna generalize pretty well, um, and so kind of miraculous, but we got this thing spun up in about a month after, uh, deciding it was, it was really gonna be useful, um, and we spun that up to, to quickly by just making it a, a little monolith, um, with a very kind of pri let's say a primitive API, um, that, that kind of reflected the way we were hacking it into the ML inference system. So you can imagine an ML inference system you're gonna have some sort of classic like predict end point let's say um in this case we had this kind of um synchronous you're gonna have to wait for it um if we go back to the react agent, let's just say you're giving it the query and then the output is you're just gonna wait until it provides that final answer. And that was enough for this project. Um, it wasn't, it wasn't tremendously hard, I think, to get going, but I, I think it was amazing we got it up in a, in about a month, so we started there in Q1. In Q2, uh, we started polishing it a little bit, um, so we added ways to do evaluation for agents, um, we added in tracing so that we could, you know, more easily debug what's happening behind the scenes within maybe that thought action observation block in this case. And we had an interesting uh developer in Stripe that decided to actually spin up this like no code agents, uh, kind of UI where you could people could just build their own agents, uh, whatever tools they wanted, um, and that enabled this kind of mass proliferation that we ended up seeing, um, in Q3, um. So Q3 interesting enough is is where we started really pushing the boundaries of this project as well and so we started hitting kind of the capacity limits let's say of of this this entire project and so. We ended up taking what was originally this kind of monolithic design and then allowing every single use case to spin up their own, um, service, so they, that solved that kind of noisy neighbor problem within the agent service itself, um. Q4 has been uh a kind of interesting development on the functionality and so you can imagine maybe this product requires more of a uh ML inspired API just kind of that predict on point, but there's a lot of use cases around just chatbots and chatbots you wanna be able to interact with them, you wanna be able to see kind of their thought process you wanna be able to see all the different, um, logs of the chat, let's say. And so this, this new API kind of allows some of these kind of stateful um streamed updates so it really enables that. And what's interesting is. This may be part of, you know, kind of the, the no code agents approach, but what we have, I, I, I believe like over 100 agents at Stripe. We did a, I did a recent change. We, they're all over the place, um. I might rabbit hole a little bit here, but. I don't know if you actually need like 100 agents to be successful. I, I'll say like from my biased perspective of, of working on, you know, this project, I will say. You could probably get by with like a shallow react agent, uh, maybe like a deep react agent in some sense, maybe like a to do list that can spawn, uh, sub agents, but. I would say the 100 agents may be an artifact of the fact that, you know, we, we allow people to really move quickly at Stripe and um there may be a lot of prompts baked in, in my opinion, to the, to the agents to get to that to that number, but. I don't know, that's just my opinion. All right, so let's, uh, let's zoom out a little bit, um, talk about kind of the full ecosystem at Stripe. We'll start back with the reviewers. So remember that we talked a lot about agents, but actually the human reviewer is really, you know, the star here, and they're in the loop. They effectively interface with this whole system through their, you know, web UI, the review tooling, etc. And uh we talked about this, you know, big dag in the beginning. Well that review toolling is effectively the orchestrator of this entire DA flow and so, uh, you can imagine that maybe before the review even begins we know we can kick off a lot of these agents that kind of front run research, um, for some of these questions. But as the reviewer progresses through the review you can imagine what context may become available to actually answer deeper into that review and so you can imagine that this kind of. Applicant this review tool and application is what determines or what triggers effectively this, this agent to, to begin front running some of this research. Now the rest won't come as too much of a surprise because we've already kind of gone deep on all these little kind of pillars in here. Um, the react agent basically interacts with the rest of the ecosystem through, through two ways. Uh, the first is this LLM client which will hit our LLM proxy that we've talked about before that'll handle things like, you know, model fallbacks, um. Potential of token caching. Solving that noisy neighbor problem, those kinds of things that that makes sure that this system is actually robust. The third is how does it actually get the signals that we use in our investigations? Well, that's where the react agent comes in. It's able to call tools that could be an MCP client. I know that's been a big thing, um, could be Python tools, um. Whatever works, really. Um, something that we do not show here is we do have to QA this as well, so this is a sensitive space. Um, regulators need to know that, you know, this is working soundly, and so, you know, part of this is keeping humans in the driver's seat in the first place, but the second part is like we have a very rigorous, uh, QA process that's not actually depicted here and, um. Boy, there, there's a lot of craze of having like LLMs be judges, you know, on many different use cases for this project though, we really wanna keep, um, everything needs to pass the human quality bar that means we need to have humans basically in the QA phase as well. So, um, I'd like to see it become more systematic, but it's not really something that fits well, uh, into this diagram here. So but it does exist. All right, so let's talk a little bit about the whole journey here now that we're kind of through all the implementation stuff. Um, This really started as kind of a moonshot idea. And. Interestingly, you know, we spent all this time talking about agents. We knew agents were gonna work for this, uh, probably about less than a month, you know, we, we set up a quick notebook, um, to kind of demo how it might look on a, on a few particular questions. Uh, very impressive, I think, uh, right away, and so we knew that this was gonna be applicable. Quite frankly, I, I think that's what it really allowed us to kind of get the push to actually stand up this new agentic service which was really necessary to, to, to actually make this happen. So we took, you know, we went from this small notebook demo, fail fast if you will, but it succeeded, um, and eventually are getting to this full enterprise system, totally, you know, their own services stood up, etc. um, really, really was miraculous, um. We also did this by having humans completely in a loop not just during the review again but also during this kind of really rigorous QA process to make sure that you know the agent's not hallucinating let's say or um is providing sensible answers or can be depended on enough. Um, and we were able to finally get our initial questions out by the end of the half. Sorry about that. So let's look at another timeline here. Um, we started with those notebook scripts in early Q1 that allowed us to know that this would actually work and was worth really investing in as a full, full on product. Um, we bootstrapped that agent service very quickly with that monolith. Just the bare minimum of kind of what we needed to get going and, and we took it as far as we could, um, and it did well, um, hooked that up with all the enterprise LMs, etc. By Q2 we finally got that first question in, which was huge, um, and I wanna say, you know, a lot of it was really actually working with the ops team. To understand exactly how they were really gonna get the best use of this as well, so that's a big component of this. It's not just the agent unfortunately. I wish it was that easy, but, um, really getting it to the bar of quality where you know a a human reviewer is actually gonna depend on it. Let's say it's like helpful 20% of the time, uh, they will never look at it and then you're just spending money on LLMs and getting absolutely nothing out of it. Maybe even 80% you could get that kind of, um, reaction and so it took a while for us to kind of work with QA, make sure this was done on rails, um, we didn't want them just, you know, like going to a deep research like interface asking anything, you know, they wanted because you don't know what it's even good at answering, right? So that was actually a big part of Q2, um. So we got that first question out. Q3 we started really trying to scale it up so we started launching a lot more questions, um, looking into things like caching or you know, now we care about costs, um, and. Q4 is where we've really already got a lot of the easier questions, maybe even a lot of the questions like before the review even starts that we can get. But now if we're gonna continue making gains, we need to actually be able to use context within the review, um, to kind of build upon, uh, what we have. And so that DA orchestration makes this a little more complicated, uh, and that's, you know, something we've been to really plug it into in Q4. So we're really just scratching the surface with this, honestly. So what do we get for it? Well, one nice thing we have is, um, we do have a little telemetry from the reviewer so they can tell us if it's helpful or not and what we're seeing is they're finding it about 96% helpful. So remember I talked a little bit before about people need to trust this system to use it. So like if this thing's good, I don't know, 30% of the time, 40% of the time, maybe even up to 80% of the time in my opinion. Um, they'll just learn that they are not gonna get any use of it. They'll do all the research themselves, and you're saving no time. You spend, you spun up this very complicated system, um, and so like getting this kind of quality bar, um, was a challenging process, um. And it took a lot of kind of human reviewer involvement, um, talking to the control owners getting a sense of like what even human reviewers struggle with, you know, when when they're running these reviews and trying to build around that um so there's a lot of prompt iterations you can imagine to to get there. Um, another thing is that we, you know, I've talked about this a few times, but we managed to keep human reviewers in the loop. You know, out outcomes of these, um. Reviews they carry weight and so you wanna make sure that it's it's made in a sound way um keeping a human in control of these at all times is I think a, a really good way to to move along here, um. And doing this in a way where we can show a regulator exactly what the agent has found, how it found it, uh, what tool calls it decided to make, um, what the result of those tool calls were, so we have complete auditability, uh, across, you know, anything the agent touches here. Um, which is, which is helpful, but all of this really comes to, to this number, which is that ultimately we were able to make this review wildly more efficient. So we measure uh the performance of the reviews kind of like average handling time how long does this thing take? So with just these initial questions that we're kind of front running the research on. Um, a human reviewer can spend 26% less time on average on these reviews as a result. So again, agents are not doing the work, um, they're not making the decisions. They're not automating really anything, but they're still able to see these massive efficiencies, and that is gonna allow Stripe to keep up with user growth, maybe changes, um, in requirements for, for the review as, as regulations change, etc. So that's been massive. So what are we going to do next? Well, we're working right now on a kind of expanding that orchestration. We wanna get deeper into this very complicated review. Um, another thing is. I don't know how far we're gonna get with this, but I would love for like the kind of evaluations of how well the system is working to be a little bit more streamlined. Um, I definitely think LLM judges per se could be a good way to like fail really bad models really quickly. Um, I still suspect you wanna have humans kind of in the loop in order to determine, um, if it's really good enough, uh, to, to ship. Um, the 26% reduction is, is massive, um, but it's just scratching the surface again as we get deeper into this review, we can, we can imagine this is gonna get a lot bigger and, uh, fine tuning is really exciting. I, I don't know if we're gonna get improved quality with fine tuning. I definitely think we're just gonna have more control of what we wanna spend our time on, um, and not be surprised by deprecation schedules, etc. um. RL is a fascinating one too, um, mainly because the answers here are verifiable and so you can imagine like this end to end kind of training loop where you're actually learning like a superior brain to the agent, um, that could maybe have less tool calls in the future which saves context, etc. um, so that's a really interesting area too. All right, so what did we learn as part of this? Um, probably the first lesson we learned is really just like splitting these very complicated reviews into these more bite sized tasks that can really fit well into memory, um, that are very easy to judge if it's consistently good or bad. Remember we have, um, humans doing kind of the evaluations of if it's good enough, so keeping the task small enough is kind of. Part of the fight to kind of get there, um, the bite size task was absolutely critical to make really any progress here, um, to, to be anything more than a demo. Um, and those bite size tasks allow us to do this kind of orchestration, so it allows us to actually start building upon context that's actually gonna come up inside of that review, uh, to be able to go deeper and, you know, to get to a point where we're getting more than just 26% reduction. Um, another thing was, yeah, infrastructure is, has been a, a big help. Um, certainly we, we could not have done this probably without the agent service we had. It just was not gonna be hacked into, uh, this ML inference system. It would have been a, um, complete abuse of resources, um, and so definitely, you know, don't be afraid of, of doing that. So what do I want want you to take away from this, um. I would say don't try to like automate everything right away. I, I think that's like a natural instinct. Everyone thinks agents are just gonna like completely replace everything everywhere, uh, tomorrow and that's just not how it's gonna work, um. It's, it's great to keep humans in the driver's seat. You should use these things as tools, um, they may not be actually even good enough to completely automate it. Who knows? I mean, the only way you're gonna find out is to have a tractable way to incrementally attack some of these very long complex problems into these kind of bite size tasks in my opinion, um, and. You know agents are great, um, and, and they're really amazing when you have almost an infinite amount of like signals you could be baking into your answer and you don't even know which ones to be looking at, um, really the tool calling is I think what makes the agents useful maybe generally but definitely for this this project, um. You still need rails for them though because again you don't want an agent um that's just spending all of its time on something you don't even care about that much um and not spending any time on the things you need to care about and so that's why I think those rails are kind of critical to this kind of like incremental approach um to to those efficiencies. Um, and the last thing is, yeah, do not be afraid to, to build new infrastructure, particularly for agents. Stripe's seen a lot of success, you know, we, we have so many agents, you can imagine just how much we're using this now across Stripe. Um, it didn't, it doesn't have to take that long to build either. Like we had something in a month. Um, and, and you could do if, if you don't have it yet. Maybe you do. Um, so that's all I have for you today. So I hope this was a good learning session for all of you and you'll be able to take some of these learnings and implement in your own organizations. So we'll wrap up this session. Uh, Stripe team will be here to answer any of your questions that have been left unanswered and one final thought, uh, is Chrissy initially mentioned about the Stripe's Agentic Commerce protocol ACP if anyone of you are interested in learning more about it. There is a session, uh, I think it's, uh, uh, on Wednesday around 2:30, so you can look that up and, uh, come there and meet some of the other Stripe folks who have been working on this, uh, protocol and in the end, please make sure to, uh, give feedback in the app. That's how we improve. Thank you very much.