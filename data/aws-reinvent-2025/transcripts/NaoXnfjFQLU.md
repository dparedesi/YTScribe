---
video_id: NaoXnfjFQLU
video_url: https://www.youtube.com/watch?v=NaoXnfjFQLU
is_generated: False
is_translatable: True
---

Hello My name is Carl Summers, and I'm a principal engineer with the S3 team. And I'm joined today with By Ron, who is a senior specialist solutions architect. You all are in STG 351. That's inside S3. Lessons from our exabyte scale data lake modernization efforts. That's the wrong button. That's the right way. All right. Let me start by painting a somewhat trite picture for you. If you printed out just one hour of Amazon S3's internal log data. The stack of paper taller than Mount Everest, and of course it's growing year over year. Remember, that's just one hour. But here's the thing, these are not just logs. Every single one of these entries represents a customer interaction with Amazon S3. It's a family photo being uploaded, a medical record being stored. A machine learning model being trained or climate research data being shared. You and millions of customers like you put your trust in Amazon S3. So these aren't just logs. There are stories, there are businesses, and there are our lives. And buried within those exabytes are the answers to some pretty critical questions. Why did a specific request fail? What's causing this particular latency spike? Which features are customers actually using? How do we prepare for the unexpected? Now our challenge here is not generating or storing this amount of data. After all, we have been doing that for nearly 20 years. Our challenge is making that data useful. And I think it's really exemplified by 3 different questions. We asked ourselves, are our engineers spending more time finding data than actually analyzing it? We have these brilliant engineers that are spending hours just getting to the data they need, not using that data to solve problems. How much faster could we be resolving issues with better insights into our logs? In operational and in business decision making, seconds and minutes count. Hours or days spent retrieving the data you need is time lost doing or building the right thing. What business questions remain unanswered or even worse unasked because the data is simply too difficult to access. How many opportunities are we missing out on? How many insights are buried in that Mount Everest of data simply out of reach. Now it's these 3 questions that have guided the direction of my work for the last 2 years. To really understand why it was necessary, I'd like to tell you a personal story from my time here at S3. And it's a story about what unlikely really means. And it starts back in 2013 when I was a new S3 engineer who thought I understood what it meant. Now I started in S3 on the team that owned the front-end components, think it's load balancers, front-end fleets, and much of the business logic for S3. And about a month after I started, I began working on my very first feature. As part of that feature, I needed to retrieve some information, some configuration rather, that had been applied to the customer's bucket. That meant I needed to make a network call to the service that owned the bucket configuration data. Now that client declared that it might throw a handful of pretty standard straightforward exceptions, things like IO IO errors, buckets not found, etc. But it also declared this kind of really odd looking one that I didn't understand at first. After reading through the client code and talking to some of the people that owned the service I was calling. I figured it was just never going to happen. So I wrapped it and I threw an exception that resulted in the customer getting a 500, their SDK retrying the request, and likely succeeding. So I finished up some other what I felt was probably more important work, and I sent the team of PR. And obviously I'm telling this story for a reason, so you can imagine I got some feedback on that code review. And we're talking about it in stand up the next day and this far more senior tenured engineer asked me, Carl, do you think there's enough context here to be able to figure out what happened? To which I replied, I don't think it matters. This particular exception seems really unlikely. And thankfully they were not satisfied with that answer, and they pressed me a bit, asking me what unlikely meant to me. To which I replied, there's effectively no chance of this happening. The chances of this happening are like 1 in a billion. Now that engineer put their head down, looking at the ground, doing a little bit of mental math. And they came back and they said, so this is happening once every 3 minutes. This was a mind blowing moment for me, right? And it taught me my first lesson of working at S3 scale. Exceedingly unlikely things happen exceedingly often. And in fact, the bulk of the work that we do at S3. is thinking about and planning for when things don't go the way they're supposed to. Now it turns out that at S3's request rate back then, their mental math was a little bit off, but their point was very valid. This is going to happen far more frequently than I think. Do I have enough context to understand what went wrong? And I think it's pretty clear that I didn't. And so I did what any one of you probably does all the time, and I added logs and I added metrics, and by the time I was done, my code looked a little bit more like this. For every line of logic, there's one, maybe even two others capturing context, right, illuminating what's happening inside of my system. I'm capturing whether I was able to fetch the configuration. Whether we hit the cash and if we did, how stale the entry was. If we didn't, how many attempts did it take to actually fetch. And finally, when that unlikely thing happens, I'm capturing what the remote service was able to tell me. So now, now I'm prepared to understand what happened when this unlikely thing occurred, right? Maybe. Let's have a look at where these log entries end up. This is a somewhat representative log entry at S3. It's in multi-line format, primarily a set of key value pairs. Some of those pairs are nested to allow us to define counters and timers, and others are used to capture arbitrary context, things like what API was called, through what interface, by what and how we responded. I want to take a moment to clarify. Today we're discussing S3's internal operational logs. They are what we use to run, monitor, and operate the service itself, and they're fundamentally different from server access logs or cloud trail logs, which are designed specifically for you to monitor activity. Sorry, to audit and analyze your own bucket activity. I also want to take a moment to pause and to talk about a tenant that the principal engineering community at at AWS holds dear. And that's that we respect what came before. I think you and I can probably agree that if we were to sit down today and design a structured log format, it wouldn't look like that one. Right? But that one works It's simple, it's readable, it's eminently extensible, and anyone with about a month or two of programming experience can write a parser for it. But most importantly, it works, and it has been working for the better part of 20 years. And that's a theme we've taken to heart through the journey. It's that, well, yes, we can build it better. For some definition, stronger and faster. We're building it to replace something. That has worked And we can't lose sight of why it's worked and the value that it's delivered. And finally, I want to point out that there's a lot more than just my context in this log, right? Every feature, configuration, logical branch, cash hit and miss is captured here. And by the time we're done, we're looking at about 5 kilobytes on average per request and tens of thousands of unique entries. Elements rather. Great. My log injuries are in my services logs, but where do they go from here? Well, frankly, log ingestion, like anything in S3 when viewed in isolation, is actually very simple. Services typically write their logs to disk, and on some cadence, usually an hour, sometimes more frequently, those logs are compressed and uploaded to. Well, they're uploaded to S3. Uh, but the existence of that log file is registered in a centralized service that is indexing the time range and the source service for that log. But so far, I've just talked about the one microservice that I was working on. S3 is composed of multiple cooperating microservices operated by hundreds of engineers. Many of these are logging at very high volumes. And so I hope you can begin to imagine the scale of data that we're generating. But great, I've done my upfront work, captured and stored the context necessary to help me understand what went wrong when that unlikely thing occurred. I'm finally prepared to figure out what happened. Actually, let's do a little bit of arithmetic first. When I wrote this code in 2013, S3 received around about a million requests per second. My service, like I said, is logging on average about 5 kilobytes per log entry, and last I checked, there's around 3600 seconds in an hour. If we do that little bit of math, it means that at that time my service was logging before compression. Teabytes of logs per hour. And today that same service is logging nearly petabytes. And of course, logs are stored for variable time periods, some on the order of days, some for months, and some can even be forever. And so the total log volume stored by S3 is somewhere around. A whole lot, exabytes, in fact. So can I figure out what went wrong? Yeah, All I have to do is download multiple terabytes of logs to my Mac and grab through. I don't know about you, but I didn't have a multi-terabyte drive back in 2013, and I certainly don't have a petabyte drive today. Grapping through my logs isn't going to cut it. So about 11 years ago we did what any self-respecting engineering team does. And we had the intern Rhule. That tool was written with many of the same constraints we have today. It was built to handle S3 scale. And it was built with Esri's foundational nature in mind. That meant it took as few dependencies as possible and still work. It was also built with a singular use case in mind. Find in the billions of uninteresting log entries. The one that matters to me, the S3 engineer, owner, and operator. And its interface to do that was very simple. You could give it a string like a request ID. And do a contained style search. If you needed to look for a counter above or below a certain value, you could pass it to Regics. And if you really need it, you could pass it some JavaScript. And it would execute this on every log entry in your requested time frame. Pass it the right input, and in a short bit you end up with exactly the log entries you need. And somewhat obviously it turned out that this last capability, this ability to pass an arbitrary bit. Of logic representing a complex question that turned out to be this tool's key feature. And man, did we abuse it Do you need to find the logs for requests that hit the primary caches but still took more than 50 milliseconds to return the first bite? Write some JavaScript. You're a product manager and you want to know which customers are using a very specific combination of features. You write some JavaScript. No, I'm kidding. What product manager writes JavaScript? They get an engineer to write some JavaScript. You want to analyze the trend of a specific features usage by customer segment over the course of the last 3 months. You write some. Well, actually it turned out that that wasn't as easy. At least you are gonna have to write some JavaScript, but then you're gonna have to find some way to post process the terabytes or even sometimes petabytes of results of your query. And so it turns out that this tool was very good at the thing we built it to do, not so good at the thing we needed to do. Now metrics and traces are powerful observability tools, and we use them across S3. But nothing comes close to the level of understanding that your raw logs can provide. In many cases, they're the only path we have to understand, sorry, to ask arbitrary questions about the current and historical behavior of our customers and our systems. So what are the outcomes that we're trying to achieve with this tool? Well, we will always need to be able to service that foundational use case. S3's log volume has increased dramatically. But the importance of a single request has not changed. We're always going to be able to need to always going to need to find that needle in the mountain of logs. But we also want a tool that can be used easily by customer support engineers when helping our external customers. S3, as I mentioned, is a conglomerate of cooperating microservices. So we want to be able to join across those services logs and understand complicated inter-service interactions. And we don't want to stop at our logs. We want to be able to join arbitrary data sets with them, things like hardware information, so we can do things like track performance or failure rates of hard drives and chips. We also wanted that tool to be accessible and usable by product managers and other business owners that they can use it to inform their own future proposals, road maps, and planning needs. Thereby returning the engineering time back to building and operating those services. And so to recap. What are some of the challenges that we faced and that I reckon you may face in a similar endeavor? Well, I've talked at length about the scale of S3 systems. But scale is relative This problem was hard for us 12 years ago when we were at the terabyte scale. It's hard for us today. Whatever scale means for you, that problem is growing. It's going to get harder. None of my systems are legacies, they are heritage systems. They've been delivering value, doing the things the way they do them for years on years, and we want to interrupt that as little as possible. S3 is a foundational service. It needs its operational tooling, observability tooling to work even when everything else isn't. And finally, to break at least one rule and throw in a new item during the recap slide. My engineers have been writing JavaScript. For the past 11 years to answer their queries. There are literally hundreds of wiki pages and scribbled notes with the right spell to make the tool do the right thing. And what that means is that what works with today's system has to work with tomorrow's. Does that mean I want them to continue to be able to write JavaScript? Really, really, no, no, it does not. But it does mean that whatever they're using the tool for today has to be straightforward and simple to do tomorrow. Which brings me to the 2nd tenant that we held during this project. And that's to meet our users where they are. We had to understand deeply the types of questions our engineers were asking the current system. We had 11 years roughly of JavaScript. We were able to get at 7 of them, so we analyzed those 7 years of queries to make sure that the majority of them were easy or even better yet easier to do. Like I said, it's more than just logs. S3 has dozens of internal bespoke formats for organizing our internal data sets, most of which were built with an equally bespoke software system in mind. Definitely not analytic queries. So we have to have something that can bridge the gap between those analytic style queries and our custom formats that our teams have developed to serve their customers' needs. And while I'm pretty sure that SQL is the right interface for such a system, it's actually not a great deal easier to work with than JavaScript, and the last thing that you want to be doing at 2 a.m. Is writing it. And so we want a system that lets our engineers write these queries in the middle of their afternoon and register them. Making them easy to discover so that they can be found and executed quickly when necessary. So I spent the past few minutes giving you context on the problem that we had in front of us. And what we were looking for out of our solution. At this point, I'm going to invite Wren on stage to talk about how we achieve that. Thank you, Carla. Thank you so much. So First of all, it's so great to share the stage with Carl today. As Carl noted, we've been struggling with the queries and thinking about queries for so long, and we've made a shift where we actually want to look at rather than asking the right questions to getting answers as quickly as possible. We want to shift from something that feels and looks more like reactive troubleshooting into proactive intelligence, right. Now, The question is how do we get users answers to the questions as quickly as possible. And as Carl noted, we sat with our engineers and asked them about the questions they ask, the data they need. And also the questions they never thought of asking, they never thought they could get the answers to. And when we looked at this process, and that's kind of a off start for this modernization process was breaking it down into a workflow, and we discovered a couple of distinct phases that we could optimize, so. First comes an exploration and learning phase. I have a question in mind. What happened during a specific request or how much does a certain type of operation take. Who is using a combination of features like our last? So this phase is about discovering what data exists. But also about figuring out how to access it. Crafting the right query. In order to unlock my answer. The next phase is around actually collecting the data. And it is at this phase where we are either waiting for something to happen, data is being collected, or we actually need to do something in order to collect the data. And finally, there's some post processing phase where we actually get the results, we aggregate them, we put them in some human readable format so we can actually use them. Each of these phases represents an opportunity for optimization. So let me show you how we kind of tackle those one by one. The first part of getting customers answers or users as quickly as possible, we call them the internal customers, is making data discoverable. This sounds simple, but at estuary scale it's actually pretty challenging. Starting from scratch, how would I answer a question? Well, historically this has been solved through some kind of institutional knowledge like you would ask the engineer on the desk next to you, right? That scales well. Second, you might look at some wiki page that was a little bit wrong when you wrote it, or when it was written, and then completely outdated the next day. And this doesn't scale. And it certainly doesn't help when you're a critical issue. So we needed something that's going to be better, and we focused on making data discoverable through proper catalog. But here's a key insight Every entry in the catalog has to be useful. And while we want everything in the catalog, we need to make sure the perfect amount of friction in the process of getting the data into the catalog. And what do I mean by that? Well, we want datasets to be actually interesting enough and useful enough so people will be willing to do some non-trivial work to get them in and maintain their quality. We want to ensure that it is fri friction free as possible, and that's a delicate balance in order to keep them up to date. And the last part is that we don't want to become centralized owners of the data. This doesn't scale as well. Instead, we kind of position ourselves as brokers and providers of tooling. And we want to engender a culture where people are motivated to be the owners of their data and their quality. OK? But having data discoverable is only half of this phase that we've just talked about. Once you find the data, You need to be able to access it. Now When we were interviewing users of the system, on the query system, we discovered a repeatable pattern. And the pattern, as Carl noted, was around getting data, downloading it. Let me walk you through an example, OK. Put life cycle requests by bucket. Historically, the process would have looked something like this retrieve all the life cycle policy requests for all customers. That would generate an amount of logs. Download the first hour. Then use yourE to extract the data you need, count and sort, so I get the results I need per bucket, save it to a temporary file, download the next hour. And so forth and so forth until I'm done. This really not very effective, right, and reducing this work a primary focus for us to improve time to answer. And we ask ourselves how can we make engineers crystallize the questions and what they are looking for and what is interesting in the results. What if the query instead looked like this, right? Sequel. Probably all of you agree that it's a simple way to specify what you're looking for, what information matters, and how to aggregate the results. And SQL is powerful, but again, Writing a SQL query midday under stress or in the middle of the night can be challenging. So as Carol mentioned, we catalog the SQL queries and we made sure that they're alongside the data sets and they are maintained using code changes techniques so that the engineers can get used to and start using them. No. We wanted to do more optimization. While we are scanning for data, remember Carl needs to scan. Petabytes of data just for that single API call, and finding a 5 kilobyte element buried under petabytes of data, uh is very challenging. So we wanted to optimize our query engine to scan less data. And return only the relevant results a petabyte scale without restructuring an entire data estate of logs right now. We don't want to boil the ocean just yet. So we have SQL that helps us craft the answer what we are looking for, what information matters, and we implemented. A push down optimization technique. Some of you might be familiar with that. We have a SQL query. So we can inspect the quarry and realize what coal is interesting for us is the put life cycle policy in our case. Which value is interesting in the result, which is the bucket, and how to aggregate the result. So we implemented that in our search where we use two kinds of optimization, the predicate and the projection. So the predicate pushdown is for filtering at scan time. So as we scan through the logs in a kind of a linear fashion, we look for a certain type of operation. If the operation doesn't match, we don't materialize the hole. We don't need it. If the operation does match, we use the projection push down, which essentially tells us which piece of information we actually need from the record, and we only fetch that piece of information. And this way we actually get Results faster. Because keep in mind we're scanning. Less data, so this optimization dramatically improves performance by reducing both the number of rows we process and the amount of data we extract from each row. So it's delivering faster queries with less resource consumption. OK, so we've started where, where most people or organizations start, which is optimizing the quarry engine itself. And we have data more discoverable. And more accessible right now. And we implemented the SQL queries and the push down optimization, and these techniques gave us significant improvements. And valuable. But we quickly hit the ceiling. Because you can only optimize so much when the underlying data structure has inherent limitations, right? Text logs have fundamental constraints that no amount of query cleverness can overcome. So instead of walking around the limitations of our data format, we decided to transform the data layout itself. And it may sound kind of trivial at this point in our conversation here today, but we've been working and delivering value for a long time, and it's a lot of data. So transforming data is crucial, but however, by transforming data itself, changing the physical structure, because we want queries to become natural and not heroic. So this transformation is going to require us to choose. The the right data platform, something that supports columnel storage. Efficient partitioning, ski evolution, transactional consistency, and do all that at exabyte scale. And we decided that this platform would be Apache iceberg. Why iceberg, you ask yourself? Well, because we know it works at estuary scale. And it has features that give us transactional updates for data consistency, something we didn't have. We are able to time travel, schema and partition evolution, we'll we'll talk about all these things. And as our data volume grows, we need a modern foundation that can handle that kind of growth. Remember we are migrating a 15-year-old system. To the one that's going to be built for likely the next 15 to 20 years. And we believe Apache iceberg is the new foundation, and this is where the old world will meet the new. Because in the past you used to dump parquet files in a bucket and you call it a table, but then there was no scheme evolution without any major pain, inconsistent views during updates, and definitely no transactionality. So Iceberg solves that by adding this metadata layer on top of the data files that gives us database-like guarantees. On top of the data files. So how does it actually work? Well, let me give you a little bit of iceberg one on one in terms of the iceberg table anatomy. At the bottom of an iceberg table there are data files. And this can be parquet, OLC, Avo, different types of file format support that. This actually holds, contain the data of the table. But the cleverness in terms of iceberg comes in the next layer, which is the metadata layer. So anytime I ingest data into my table. I create multiple files, parquet files, Avo files, OC files. So I need something that will aggregate those files and say, hey, this belongs to a certain commit to my database, and that's a manifest file. So a manifest file is a pointer to data files, but it's more than just a list of files. It also has some statistics and column information, so it actually knows a little bit more and has more intelligence on what's well in the data files. But then I make more than one ingestion into my table, right, so any time I make an ingestion, I may have more. Manifest file, so I need something to basically create a table view, and that's the manifest list. So the manifest list is an aggregator of manifest files, basically creating my you know backy propagation going up, and now I have an entry point into my table view. But the real cleverness actually comes from the metadata file which has a lot of information on the underlying table like the schema. Some information about versions and this is how we manage snapshots and we can point back in time because a snapshot is essentially a pointer. To a manifest list, which is an entry point to a certain point in time, view of the table. Now on top of that, you will obviously always have A catalog Which basically maps table names to metadata files, so we can, sorry, so we can find an entry into the table. That's iceberg one on one. Now keep in mind that iceberg. Is not a server you run. It's a specification and collection of libraries that basically defines this layout structure of data files and metadata files and those manifest files, giving you a database like semantics with schema management, consistency, and transactional data leak storage. However, as Carlos said, we have Tens of thousands of microservices we can't have those start writing parquet files into an iceberg table just like that. It's just not practical, right? So because we're going to stream logs. And we want column-based statistics. That's kind of where we're aiming. And we need to carefully think about file formats. Now There are different file formats that are supported in Iceberg and generally when you collect any type of data, and I'm only mentioning two here like Avo and Parquet. Avo is a role-based structure. It's very good for log kind of collection, sounds like a good fit for us. But we are actually interested in parquet style columnal format because we're interested more in the analytics layer on the user side. And I would just like to point out that from query inspections we usually are interested in something like 10 to maybe 20. Fields or columns out of thousands of potential columns in the quarry. So while Avo seems like the right thing for ingestion, Parquet wins for analytics. The key here would be intelligent partitioning. To make it work and also some intelligent ingestion that will transform the data efficiently. Let's start with partitioning. And partitioning is foundational, especially with the volume of the logs in S3. Why? Because it's the gate of the difference between scanning petabytes of data versus terabytes or maybe even gigabytes of data. Get it wrong And no query cleverness will save you. And iceberg lets you solve that by Letting you evolve partition strategies. Without actually rewriting data, and it's critical where no single approach optimizes all queries. Now, the magic happens by what we call two-level pruning. So manifest file pruning eliminates entire data sets, right? So I know which files I don't need to touch. And then column statistics, those little data that's hiding in the manifest files, help me skip irrelevant files within partitions. And this cuts scanning data by orders of magnitude. But there's no perfect partitioning. For example, think about S3. If I would take, you know, bucket, uh, time-based versus like a account-based, for example, those pull in different directions in terms of partitioning, so we need to start with some common pattern and evolve from there. Though the key thing here is that partitioning only gets you to the right files. What happens inside those files is equally important, and that's where sorting helps us and makes a difference, especially in the context of logs when we're looking for some for specific pieces of data. Let me show you that by using a concrete example. Let's say I have a table and I'm looking for request 8D7, OK? Now in the unslaughtered case, if you look at my table, Every file in my current structure, in my current table holds the range from 0 to 15, so as I'm looking for request ID7, I need to scan every single. iceberg can't skip a single one of them. But in the sorted case, When every file has a distinct range, when Iceberg looks at the table. It realizes it only needs to touch. One file out of all those files because that request ID is within the range of that specific file. And this difference isn't trivial. Again, partitioning will reduce the amount of data you scan, and if you are sorting properly. That's another potential order of magnitude of less data to scan. And This directly transfers again to faster queries. And lower charges of processing as well. So of course the magic only works if you sort on the right columns. We didn't guess. We actually Had conversations and analyzed thousands of queries. To find what sort keys we think are going to be the right sort keys as we build these tables and build this process. So sorting and partitioning. Define how we physically store the data on the disk. But we still need to make decisions around the logical schema. And how the data is structured and accessed. So let me show you how we, how we thought about the mental model, the schema. So, let's define the problem, the problem with, with our log system, and if you're dealing with logs, you may find this very similar. Logs are messy, they're variable, they're ever-changing, and we need to balance certain structure and flexibility, right? So queries that are already cataloged will continue working. But at the same time we need that flexibility to prepare for the unknown. Flexibility can be for known things, but also for unknowns. So that's the challenge we are facing, and we define certain goals that we want to maintain. One is minutes instead of hours to get answers to queries. That's crucial for us. Second is easy to understand and maintain. The schema has to be easy. Complexity. Kills adoption. If it's hard to understand, it won't work. So the schema has to be easy to understand, and it needs to be cost effective instead of in terms of storage and processing. Obviously at exabyte scale, storage doesn't come um storage isn't free. So every design choice has a real cost implication that we need to take in mind. And after analyzing our quarry patterns, we came up with a 3 layer kind of schema for our logs, and you may find it useful as well. So the first layer is what we call the identity layer, the who, what, where, and when. These are core identifiers that appear in nearly every query that we make. The second layer is measurements and counters. These are numerical data about the request behavior. And performance. And the 3rd level is the context level, that's everything else, debug information, service context, um, anything that doesn't fit in metrics or identity. OK. Now, I want to give you a little bit more details on those layers, so you kind of have a deeper understanding of our skin schema mental model. Identity fields, it's flat and it's very simple. It holds the most common filter fields. Request ID, time stamps, servicing for user accounts. These appear nearly in every war or joint clause of queries that we make, and it's making them also natural partition candidates and sort keys as well. So that's the identity layer. Second is the metrics layer, and that's actually a nested structure. Because it organizes thousands of metrics and it's performance data, volume statistics, resource utilization, they are all logically grouped and text to perquet column of format. We only read the specific fields we are interested in. Leaving the rest behind on disk. And the third is the context layer which holds everything else that will. That we want to have on the table but doesn't break. Existing queries around the first two layers, OK, but it helps us evolve and be able to add more context into the logs as we need and as we progress. The result is structure where we need it and flexibility where we need to be able to adapt. OK, cool. So this is what it might look like and if you know SQL, this is a fairly simple query. I'm going to get some data for a certain day. I want to group it by operation and get some requests and order them by total requests and at exabytes scale, getting here is anything but trivial. We're query billions of log entries and we are accessing identity and metrics layers and we're doing aggregations and we're getting results in minutes versus hours. And with this architecture, iceberg layout, the manifest files and the partitioning and the sort. Basically make the SQL queries skip irrelevant data automatically. So now the data structure itself takes care of the optimization, and we can get answers to queries in minutes instead of hours. So we now have a data construct that makes data discoverable, accessible, and queries are no longer heroic. And all this data layout. I optimized and tuned for for S3 scale. OK, But how do we transform existing text logs into structured poque files? Do we change? Everything that we have, or do we just ingest new data into poque files? This isn't just a technical question. It's a strategic one because How do we introduce it in S3 scale without actually creating any disruption? Because again, as we said, we can't have servers write directly perque files into Iceberg. It's just not practical with the amount of microservices we have, and we need to meet the systems and the users where they are. Like I said about the tenants earlier. As we move forward the goal, we need to meet them where they are, and we need something in the middle that's going to bridge the legacy text logging and the percephite. It will pass, it will structure the data, it will compress the data and create those really nice structured percaphites that we all come to love and appreciate, right? So we need something that is. A. Now Keep in mind that S3 is so foundational and operates at such a unique scale. That we've built our own transcoder because it was more efficient for us than adopting existing batching tools or techniques. The transcoder works with our partition logic. And it's doing that during the conversion itself. It's not just changing formats, and our compression team did amazing work in order to optimize this transcoder. We're able to compress 1 hour worth of logs in just 3 minutes. And constantly work away to improve that. OK? But technology itself here is not enough. Now that we have data layout, we have a tool that can convert text logs into formats. We need a migration strategy. So we need to decide how we're going to introduce this change into the S3 logging system. So we've built a tool that can convert to parquet and push it down to an iceberg. And we have this system that's been running for over 15 years, always on. And we want to introduce this change, and this is where engineering discipline. Meets operational reality. And what I mean by that, well, the first one is to meet the log agents where they are. We provide them a legacy compatible interface. So from the app from the application perspective, nothing changes. Still, except the same format, same API. But underneath the transcoder does its magic. Taking those text logs, converting them to poquet files. And Placing them into ice. This is a critical part. We never compromise on what's already working, and we always maintain a rollback plan. As we gradually introduce change, so we deliberately split the data. To the new transcoder, but also to the legacy system. Yes, there's temporary duplication, and that's a trade-off we consciously took, that we can always revert to a non-working state. From our perspective, this isn't wasteful. This is insurance, OK, because a rollback strategy, a rollout, sorry, a rollout strategy is methodological, right. Server by server, microservice by microservice. Availability zone by avail zone, region by region. Why? Because we, we want to monitor the behavior closely. And we want at each stage to query the performance. We want to make sure the data is consistent, storage costs, system load, everything matters. We want to catch those 1 in a billion early on, the 1 in a billion that Carlo was dealing with. Years ago And here's the beauty of this approach. Once we've completed this implementation, and we evaluate that it's actually working, we can remove the duplication, but we no longer need it. While maintaining the safety and the reliability. That S3 demands OK, We're getting close to the end of the presentation and we now want. To actually have the logos themselves kind of write directly, well, not really directly, but we want to change the logos themselves at some point. We want to change the logos. But here's a key architectural decision that we need to make again. We're not going to have them write directly to parquet. This is simply not practical to Iceberg, right? We need to have a way. To use some collectors or aggregators, so the log agents will essentially send the files to a certain collection point or a collector, and we're actually thinking that this would be Avro files because those are very well tuned for logs, but then those aggregators or collectors will actually Do this different kind of transcoding, converging them into large, well structured, beautiful parquet files that's going to be then pushed into the iceberg table and they are analytic query ready and we can actually use them. Now We've talked about the process on how to make data discoverable and accessible through proper cataloging and SQL. We made it more accessible with our existing legacy, heritage system by doing the pushdown optimization that helped us, but eventually we realized we needed to transform the data layout itself. And We've transitioned the data itself, the data layout into Apache iceberg, but at the end of it comes the question. What have we learned through the process? And at this point, I would like to invite Carl for the takeaways and the lessons learned. Carl. OK. Thank you so much. I Thank you, Wren. So we have talked through an awful lot of technical detail from discovering what data we have. To the post processing of our custom transcoder. And Wren covered iceberg tables, parquet optimizations, sorting strategies, and even parallel migration. And frankly, I could easily talk for another hour about some of the technical outcomes that we achieved. I would love to tell you about the number of queries we've run or the bytes that it's processed, but frankly, you don't care. And actually those are just inputs into the actual outcomes that we were trying to achieve, right? In fact, through this process, we have been able to return thousands of engineering hours back to building and operating our services. Additionally, our engineers can now run those arbitrary queries over very fresh data, oftentimes 5 minutes less, 5 minutes old, or less. And finally, our product managers and our applied scientists can access vast amounts of historical data, magnitudes, orders of magnitudes faster than before. And that enables them to answer questions about things like placement policy updates and changes or that feature usage. Without an engineer in the loop. Now in giving a talk like this, it's actually very hard to not make it look like some sort of linear path. Right where we started over here where everything was terrible, we made a bunch of great decisions and then end up over here where everything is golden and beautiful. In truth, that journey is much more like a random walk at times than a linear path. But I do think that there were a handful of things that kept us moving in the right direction that I think are worth highlighting. Now for S3, it was our logs that represented one of these many internal data sets for which retrieving data was often just too hard to be worth it. And that was despite recognizing the immense value that they contain. Now I can easily say that for the overwhelming majority of you, I believe that cloud Watch logs is the right place to put your logs. Nearly every feature that we've been building in our system exists there out of the box. But I also bet that if you look around your organizations, you have one of these, right? One of these systems where there is something of value inside of it and it's just too hard to get to. Maybe that's because it's stuck behind a custom format or behind a custom interface. But that's where you're going to find the biggest opportunity to apply all of these principles. Now once you've identified one of these high value but high barrier data sets or systems. Working backwards from what your users are doing. But importantly, what they wish they could do. Or what they never even thought to ask for. Now for our users, most of their time was spent identifying what data existed, figuring out how to query it, and finally surmounting those scale issues associated with accessing it. But Rand talked us through building out our catalog. Importantly, the mental model for applying a schema to semi or unstructured data. And some of the technical improvements that you can make. In your connector code to improve performance while your data remains in its natural form. And I say natural form intentionally. Because importantly, at many of the stages in this journey, we hadn't required any changes from the systems generating the data at all. They continued to work exactly as they had. But because of those connectors, because of that meeting the systems where they were, we were able to prove out the value of their data sets to the teams themselves and to other consumers in the company. And once that happened, it meant that the owners of those systems became intrinsically motivated to make changes to them. In our case, to change how they emitted their logs so that we could land the data in its optimized physical form. We walked through that optimized form, in particular our usage of iceberg as a table format. And calling out the partitioning and sorting as key elements of that performance strategy. And so finally, I just want to say thank you very much for attending our session today. If you found portions of this valuable and you'd like to dive deeper into Iceberg and some of the ways that we advise or work with customers to build scalable data lakes in S3. Recommend you take a photo, scan some of the QR slash uh. Scan some of the QRs. These are, uh, recommended sessions that are happening throughout the week to give you additional information on how to build scalable data lakes on Apache iceberg on S3. Thank you again and have a great week.