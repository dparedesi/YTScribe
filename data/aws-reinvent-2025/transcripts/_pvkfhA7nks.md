---
video_id: _pvkfhA7nks
video_url: https://www.youtube.com/watch?v=_pvkfhA7nks
is_generated: False
is_translatable: True
---

Anyway. Good afternoon, everyone. Hope you're enjoying Reinvent, and thank you for joining us today. My name is Richard Chester. I work in financial services for AWS um, I'm a principal solution architect, and I'm based in London. I'm delighted to be joined on stage today by Dean Brighton, head of cloud Engineering for the London Stock Exchange Group. So, for the purposes of everybody here, London Stock Exchange Group, LSEG, that's what we use for short. OK, so London Stock Exchange Group is well advanced in its journey, uh, in public cloud, with over 1300 NWS accounts. In addition to a significant presence on Azurea and GCP. And a sizable on-premise footprint. So like many AWS customers, LSEG are therefore a multi-cloud organization. Today we'd like to start by giving you a little bit of background about LSEG, what it is and what it does. Then we basically, um, then the next thing is Dean will take over and he'll talk you through the multi-cloud strategy, um, and then what we'll, we'll conclude with a couple of interesting um and quite different use cases that show the placement strategy of workloads on LSEG. OK, we've been collaborating on these for the past 18 months. So founded as a regulated exchange in 1801, um, and with a rich history stretch stretching back through coffee houses and the Royal Exchange. EEG is a global provider of market data and financial infrastructure headquartered in London. Hm, among a number of trading platforms, it operates, as the name would suggest, the London Stock Exchange, and manages international equity, fixed income, and derivative markets. The group though has more than exchanges in its portfolio. Um, it also offers real-time and reference data products. Through FTSE Russell, curates category defining indices. Develops capital market software used by other exchanges. Provides extensive post-trade services. And helps fight financial crime through the risk intelligence business. Now, executing a business transformation through a single cloud provider would be an interesting challenge. To avoid making this an overwhelming prospect, EEG have implemented a structure which Dean will now explain. Dean, cheers, Rich, thank you. So, when people talk about multi-cloud, you might often think that it's a deliberate strategy that they're taking, right, but the reality is, and you may or may not agree with me, that this is often most organizations end up there through, by mistake sometimes, right, acquisitions, commercial agreements. Maybe there's some regulatory requirement around exit strategy and things like that. And when you start doing this, you start realizing that, running multiple cloud providers isn't really more of the same, it just introduces a whole load of more complexity. So what challenges do we have with multi-cloud, right, so In regulated industries, We often have a requirement to meet regulations around exit strategy, concentration risk, um, diversity, so use like utilizing multi-cloud can help us meet some of those requirements. On the technology side, um, without clear direction, teams build unique IM, logging, monitoring, golden images, various stuff kind of uniquely across different cloud providers, that causes some constraints as well. I thought it was working then, there you go, it's working. Um, from an, from an operational side, right, running one well secured, governed and regulated cloud provider is pretty difficult. Doing that operationally across lots of CSPs is introducing a whole load of operational complexity. And then there's people. Generally, people are either an AWS engineer or an Azure engineer, it's quite rare to get people that understand both cloud providers, or GCP or, I mean, you've got a unicorn if you've got someone who understands those to the depth that you need to run a regulated cloud platform. And inertia's real as well, right? So people generally stay with what they know, right, so. So at Else, instead of fighting these challenges, we took a bit of a step back and we thought, how can we make multi-cloud seem deliberate. How can we build a framework that helps us have multi-cloud in a way where we can leverage the multiple cloud providers where it makes sense, but we can still get the best out of the, the benefits that each of the cloud providers have for us? So what does that look like? So at EEG we've built a multi-cloud product framework. It starts at the bottom with our cloud capability framework. This is a common foundation of capabilities that we define that says what a good cloud environment should look like. It sets standards and principles for things like IM, networking, policy, etc. This means that when we build on top of this with multiple cloud providers, they can be built on a set of common primitives and features, and our developers in the business know that they get somewhat feature parity across the cloud providers. We store this in Git, it's version controlled, it's constantly updated, it's a living framework, um, that we, that we build our cloud platform on top of. And sitting on top of that are our core cloud, public, public cloud platforms. These are our landing zones. We have one in AWS, one in Azure, and one in GCP. These give us standard APIs for things like IM, networking, account vending, typical kind of landing zone operations you would have in a core platform. And when workloads need to remain on premises, we've decided to use AWS outposts for distributed or hybrid cloud. This gives us the ability to have a consistent set of APIs and interface that our developers are familiar with to deploy it on premises when the need is there. Oh, click, and the click has gone again. OK. Oh, it's Turkin now, there you go. Oh, back one. But what we quickly found was that building cross CSP capabilities, there was lots of common things, and as I said earlier, we were starting to build things that were that were independently different across the cloud providers. So some examples here are things like policy. We've decided to standardize on a standard policy engine that is uniform across all our cloud providers. We do golden images centrally, some integration, some of our security tooling, things like fins, service management, incident management are things that are common and make sense to do across cloud providers, and we're starting to do some AI tooling in this space as well, because you can't do a talk at Reinvent this year without talking about AI. I promise you that's the last time I'll mention it, so. But what this is like really the crux of some of the decisions that we have to make in our organization, and I have to make as er the head of cloud engineering. So, one of the most difficult decisions is when we stay with the native services in the cloud provider, and when we decide to go agnostic. And the principle that we stand by is that, where possible, we will always go native as much as possible for application teams. Unless there's kind of a reason for us to kind of go cross-platform. There's a few examples of that, things like some of our, um, unified observability across platforms where we need a single view across these core platforms. It makes sense for us to use some agnostic tooling for that. But if you're an application team that solely runs on AWS, it might make sense to use more native services for observability there for the application team. And then we only really make those decisions to go across CSP when it kind of makes a ton of sense, like, and it's really obvious, or there's regulatory or compliance reasons for us to do so. And then on top of that, we've built unified orchestration and management on top, we've, from an infrastructures code perspective, we've standardized on Terraform as our tool of choice for provisioning our cloud resources and our platform resources, so we build our platforms with this as well. But we don't really treat er reform just as a provisioning tool. We kind of see it as an extendable, extensible kind of API for our developers to consume our services. As a regulated organization, we build verified modules that comply to our security frameworks, to our policies, and they have opinionated defaults built in to make it easy for our developers to deploy. And we standardize in a product we call the cloud product framework, which is those modules that we provide out to our developers across all of our underlying cloud platforms. Similarly, when you build landing zones, you have privileged operations, things that you wouldn't want your developers to do, but they still, you'd still like them to self-service. This is things like account vending, maybe IM access packages and role provisioning, maybe some networking operations, so to do that we've built a custom orchestrator, a provider based orchestrator that's extensible um for doing those privileged operations in a self-service API driven way for our developers. And then at the top level is where we really meet the developers in our organization. This is where our, um, we've stolen the term from Werner from last year of simplexity, and we like to use that internally quite a lot. Building these landing zones in cloud platforms is quite complex, so we abstract the essential complexity um behind kind of clean APIs and interfaces for our developers. So what benefits have we got from this? Our developers are more productive. We can now get develop, we can now get applications to cloud within the last year of doing some of this stuff almost 80 80% faster in some cases. We've bolstered our security and compliance because we've got a standardized framework. So it's a lot easier for us to be consistent, have a better security posture across all of our cloud providers. We've got enhanced collaboration, my platform engineering team, they're collaborating more, there's reuse, they're working together in some of those cross cloud capabilities, and we've saved costs, both in kind of resource costs, but license costs and operational costs as well, so there's a number of benefits that we've got from doing this. So I'm gonna hand back to Rich who's gonna talk about some specific use cases here at EEG that we've been doing that are utilizing multi and hybrid cloud. Thank you. OK. OK, please work. So, thank you Dean. Um, so we've come to the first of our use cases, er, this is the uplift of FX Price stream system. So FX Price stream er is a global dealer to client streaming court service that offers disclosed liquidity. From across more than 150 currency pairs by linking liquidity providers, the makers, with the taker clients foreign exchange trading. Globally it serves over 100 liquidity providers, more than 1200 taker clients, and while it's not a regulatory requirement, all are served using fair and equal access. So what are the challenges? We'll take a quick look now at the challenges of the existing system. So the existing system, they fall into three buckets. So firstly, we have modernization. So while LSEG FX training infrastructure requires periodic modernization, it does create opportunities to look at new technologies and ways of different ways of doing things. For example, you know, they might use kernel bypass to offload some of the high performance capabilities so they can use like high performance nicks. Next comes. Sorry. Right, latency and performance. OK, so, um, the venue, the venues have two different kinds of latency. There's a macro latency performance, which means that the venue needs to be close to the liquidity providers geographically, but there's also a micro latency performance as well, which means the traffic needs to flow from front to back as quickly as possible. So the thinking is if you can improve performance, you can reduce trade rejection rates, you should be able to drive your volumes up, narrow margins, increase market share, essentially make more money. And finally, and finally, infrastructure and cost. So, um, as the currency shifts from primary markets to secondary markets, this causes fragmentation, it increases competition, and what that means is there's a high, high upfront cost with Capex, so it's, it's gonna be difficult to basically keep setting up the same infrastructure globally because you have essentially starting from scratch as technology moves on. Um, so if, if you're unable to use a cloud provider in region due to the macro latency. And you're unwilling to endlessly cycle through ubiquity hardware, else I chose a different route, and that's to use ultra low latency outposts. OK. It worked. So, um, the solution leverages AWS ultra low latency outposts. So these have optimized bare metal compute, deterministic performance. They have a physically separate network that features ultra low latency Nicks. The racks come with equal distance cables and layer to multicast, which isn't available in region to ensure fair and equal access. So the description that I'm giving is essentially what shows the the data flow from left to right in the diagram now. The top to bottom direction is another AWBS differentiator. So AWBS's proprietary Nitro technology provides superior performance through dedicated hardware. Um, you can maintain your security through encrypted connections, built-in tamper proof net tamper detection. And the other thing as well is, um, because it's, it's a, it's a service-based approach. Uh, EA doesn't need to manage things like firmware upgrades and that kind of thing. So Uh, results. So, price stream application on ultra low latency outposts, um, operates at the, in the 49th percentile for um. At 1 millisecond versus 2 milliseconds on premise, so a twofold increase, the median, the median is 181 microseconds at a peak rate of 165,000 messages per second, and that's compared with like 1.1 milliseconds on premise in production. So that's a sixfold improvement, but also at 3.3 times the prod peak on premise. So, um, and the third one is, if, if we, if we look at the fanite across sustained, where we're looking at double the, the peak for longer periods, it's twice as quick. So. All told, um, it's, it's faster across all percentages, it's faster at a much higher peak, um, and when it fans out, sustained, it's also much faster, so faster by every measure that we've tried. OK, so we're, I'm gonna hand back to Dean, we're gonna just discuss one more use case which looks at different, um, a, a different, er, I don't know, factor, feature, um, of, of ultra low latency outposts. Cheers, Rich. Tazri said we. ESEG is a group of companies, we've got lots of different organizations within LSEG. Um, we've learnt about some of the FX work that we've been doing on outposts. I'll talk about a slightly different use case now about our LCH. So the London Clearing House is er majority owned by LSEG. Uh, it's a global financial markets infrastructure organization. It acts as a central counterparty to clear and settle a wide range of financial transactions, swaps, equities, repos, etc. And it reduces risk for its members by acting as an intermediary between buyers and sellers in a trade, um, giving them kind of confidence and ensuring that trades are completed successfully, even if one of the parties in the trade defaults. So the clearing workloads that run inside of LCH, as I refer to it, the London Clearing House, um, they're in the AWS region, but they are mostly migrated from on premises rather than developed in the cloud in AWS. And as with many on-premises workloads. The LCH services have challenges which are pretty well suited to the public cloud, so they need things like scalability for elasticity. They need innovation, experimentation. We need to move fast and keep up with the market. And again, on the innovation thing, we need to reduce the time to market. And then what's really important, the single most important factor, is that our LCH workloads are built for resilience. And I'll, I'll tell you why that's really important. So the London clearinghouse workloads are considered as critical national infrastructure in the UK. This means they're under the most highest, the most scrutiny around for our regulators from a resiliency perspective. So meeting changing regulatory requirements can be difficult, um, but in our case, there's been some interesting requirements, specifically around critical national infrastructure that make this a little bit, well, I think at least, a little bit more interesting. I hope you do too. So the Bank of England in March to 2023 announced a new, a new directive, and they had to explore options for a total cloud service provider outage effectively. We had to be able to operate completely independently of the cloud service provider going down. So we looked at this as a team and we thought, how can we meet these requirements. And we had a couple of options, right, we could have stayed on premises. We could have replicated to another cloud provider, but when we took a step back and looked at it, we thought that, that kind of, didn't really make a ton of sense, because, as we spoke about earlier, that introduces operational complexity, right, and the last thing you want in a major CSP outage is more operational complexity, right? That's the kind of thing we want to avoid. So resiliency is really the key thing for us here. So the solution that the LCH team built, it runs in predominantly in region. So in the primary region we're running Amazon EKS, we're using Oracle and RDS S3 for storage, and we're using. Wait for it, this is a mouthful. Amazon FSX for NetApp on tap, right, I'll need to read that one, but effectively this is our storage solution, so being involved in a NetApp ecosystem gives us some flexibility that you'll see in a moment. So to first address resilience, we replicate into a secondary region, right, this is pretty bog standard multi-region kind of cloud architecture, and the NetApp solution gives us the ability to replicate our data out between regions. But then to meet the Bank of England requirement, we've built a tertiary solution sitting on AWS ultra low latency outposts, which gives us the ability to meet that 5 day disconnected requirement that the Bank of England have mandated for us. So on the outposts we run a, obviously we go via Direct Connect to a local gateway, but again we run EKS on outposts, we run Oracle on, on, on EC2 on outposts, and then, um, because we don't have RDS on there unfortunately for the low latency outposts, and then we also use a NetApp device on premises, on premises, so we can replicate the data out to on premises as well. And then for some final kind of resiliency and kind of an absolute terrible scenario, we replicate the data out to Azure as well. So, oh, wrong way. So we've actually proven this. We've tested the 5 day disconnect with with the outposts. We've disconnected. We've proven to the Bank of England that we can meet their requirements for 5 day disconnect successfully. We've done the failover and the foul back, including migrating our database over to the outposts solution as well. So, it really proves that we're able to meet these requirements, while reducing those operational complexities and staying within the predominantly, apart from the Azure kind of NetApp environment, uh, within the AWS ecosystem, and using the AWS environment, which makes it really easy for our developers. We have a common API within AWS Outposts, and it's a familiar environment for them to work with. But really, this is really kind of, um, Groundbreaking really in the way that regulators treat critical national infrastructure and financial markets infrastructure, and it gives us the ability to, um, it's given us the ability to kind of sell to them the benefits of running this critical national infrastructure in a cloud ecosystem like we have demonstrated in this architecture here. So with that, I'm gonna pass back to Rich, who's gonna sum up. Thank you. OK, uh, unfortunately we're out of time because we had problems with the clicker, so I'll, I'll have to go really, very quickly indeed. So take a structured approach to multi-cloud, balanced cloud native and cloud agnostic. Um, ultra low latency outposts can be used for low latency like requirements, things like low latency trading, but also like extreme resilience. As a call to action, um, we've got, we, we, we'll be basically participating in the multi-cloud village on Thursday morning at 10 o'clock, Dean and I. And also if you want to like learn a lot more about ultra low latency outposts, we would recommend HMC 4002. Um, that's a, that's a really great session. Uh, I guess all that really remains is to say thank you very much for bearing with us during the clicking problem. Um, we appreciate it very much, and yeah, thanks for your time. Thank you. Cheers.