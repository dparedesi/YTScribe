---
video_id: hSkElcxjdfs
video_url: https://www.youtube.com/watch?v=hSkElcxjdfs
is_generated: False
is_translatable: True
---

Thank you all so much for coming this early in the morning. I really appreciate it. And how many of you, or for how many of you, this is the very first session of their very first reinvent ever. wow, a lot of first timers here. Well, you're in for a treat and quite a bit of walking, as you might have already noticed. Right. Imagine that you have built this simple surveless architecture in the cloud. You've picked the services you want to use, you connect them together, and everything just works like magic. So much so that you, of course, decide to go to production with it. And all of a sudden hell breaks loose and things start to fail in all possible shapes and forms. Now, you're convinced that you personally have done absolutely nothing wrong, well, except for maybe not reading all the extensive documentation about all the services and all the components that you are using, but let's be real, who does that nowadays anyway, right? So what's the next logical step? Maybe you swear off using the architecture or services again, ever again because well, they just don't work. And honestly, what's wrong with the cloud? Why doesn't it work? Maybe serverless is just another expensive fad. Maybe we should just all go back on prem and use good old monoliths. But then again, purely statistically speaking, we should consider that maybe the problem not in the cloud or the cloud providers, maybe it's not even us or the services that we picked, not even the architecture. So what is it then? As Murphy's law says, anything that can go wrong will go wrong. Though I personally prefer the more extended version of it that says, anything that can go wrong will go wrong, and at the worst possible time. My name is Anahiid. I'm cloud architecture and engineering lead at FSecure. This is a global cybersecurity and privacy company with over 35 years of experience in the field. And I'm also an AWS hero and there's a funny thing I noticed after becoming an AWS hero several years back that people started to come with me or to me with these smirks on their faces and saying, so tell us now what's wrong with the cloud? Why doesn't it just work? Well, today I'm finally here to answer that. Well, maybe not exactly that, but I want us to look together at something that we as humans don't usually feel comfortable looking at. Failures And I hope that this talk helps you to become a bit more aware and curious to spot patterns that others don't necessarily see. To have the tools and to ask the questions, to make conscious critical decisions, rather than believing in magic, taking controls in your own hands. And finally, becoming a little bit paranoid. But in a good way. Because to borrow words of Martin Kleppmann in distributed systems, suspicion, pessimism, and paranoia pay off. Now, before we start talking about distributed systems and failures any further, let's briefly go back to our fictional story about failing surveillance architecture. It actually had a prequel to it. So, once upon a time, you were a developer who started developing software that was probably supposed to run on a single machine somewhere in non-prem data center. So all you cared about were the so-called functional requirements, so that your code works, does exactly what it's supposed to, and of course has as little bugs and failures as possible. That was your definition of reliability. Now, there could have been some occasional hardware failures, but usually you didn't didn't worry about them too much. Things either worked or they didn't. Everything was nicely deterministic. Next thing you know, you find yourself in the cloud, maybe using virtual machines. And maybe you also start developing software that requires you to think about certain levels of so-called non-functional requirements. So certain levels of availability and scalability, also reliability and resilience get a whole new meaning. Now, you still need to take care of your functional requirements, make sure that your code works and has as little failures as possible, but the complexity level just went up a notch, and now you need to worry about so much more. And failures are also becoming a bit more pronounced and a bit less deterministic. Welcome to the dark side. The wonderful world of distributed systems where with great power comes great responsibility. But things didn't stop there, and before you know it, you jump over to the surveillance world. The things start to look simple again. You just pick services. You connect them together. And everything works like magic. You don't really see any machines around anymore, and the term servals actually suggests that you don't need to be looking for any machines, you don't need to be caring about any machines. And the cloud providers take care of this uh ilities for you, so reliability, scalability, availability, and you are back to just worrying about your own code working properly and not having a worry in the world. Of course we do know that's not exactly how things went. Because everything that can go wrong will go wrong. So what is it that can go wrong exactly? To set the stage, let's talk about cloud, distributed systems, serverless in very simplified terms. And if the things I'm gonna cover for the next couple of minutes are obvious to you, just bear with me and maybe enjoy the animations. Uh, but in really simplified terms, this is the distributed system is just a bunch of machines. Connected by a network. And while it provides a lot of new and exciting ways to build solutions and solve problems, it also provides a lot of new and exciting ways for things to go, to go wrong. Because resources you are using are not limited to a single machine anymore. They are spread around multiple servers, data centers, maybe even geolocations, and instead of just one machine that can fail, now you have plenty. And all those failures can happen on different levels. It can be operating system, maybe the CPU, GPU, memory, load balancers, you name it. And all of those failures can happen completely independently from each other in the most non-deterministic way possible. But the worst thing here is that those machines are talking to each other over the network. The network is known for one thing in particular. Wherever any communication happens over the network, it will eventually fail. Now, any cloud is built on top of such distributed systems, that's where their superpowers come from. The cloud providers take care of the most difficult part of handling the underlying complexity of the distributed architecture, abstracting that complexity away from you, while giving you access to this vast pool of resources that no individual user could ever achieve. But especially on the bigger scale. If something has a tiny little chance of happening, it most certainly will. Now, surveless, managed services are a step up in the abstraction ladder. They make the underlying infrastructure seem almost invisible. Almost magical So much so that we might even forget it's there. But by using the surveillance services and, and fully managed services, we didn't just magically teleport to a different reality. We are still living in the very same messy physical world with all its underlying complexities. And this higher level of abstraction with servalus definitely makes a lot of things easier, just like a higher level programming language does. But it also comes with a danger, being seemingly simple to use, it can also give us this false sense of security. Which will make spotting those potential issues that much harder because after all they are also abstracted away from us. And the reality is those failures didn't go anywhere. They are still there embedded in the very same distributed system, underlying distributed system, just waiting to show up. And as Leslie Lampert said it already, 1987. But the distributed system is one in which the failure of a computer you didn't even know existed can render your own computer unusable. And we could rephrase it for erless. So a servalless architecture is one in which the failure of the computer you definitely didn't know was there, can render your entire architecture unusable. But with surus, rather than seeing those hardware failures, maybe blue screens, we see failures manifest in a different, somewhat less obvious ways. Now, let's climb one last step in our abstraction ladder, and let's focus on building distributed applications on top of servales and fully managed services. So in essence, we are splitting the problem that we are fixing into smaller pieces. For each piece, we are picking a service or a resource, and then we are connecting all of them with things like events, uh, messages, HTP requests, all of which are using the network in some shape or form. So in essence, distributed architectures are actually mirroring the underlying distributed systems. They also give us this great power of building applications in a completely different ways. But just like the underlying distributed systems, they are susceptible to the very same trade-offs. The architectures that you are going to build are likely going to be complex. Every piece can fail at any given moment in a completely nondeterministic way. And whenever there is any communication happening over the network, it will eventually fail. Now, a special case of these distributed architectures are so-called data applications or, uh, with data applications, we are basically dealing with large volumes of data we want to store, collect and process them. And the data itself can really be anything, log data, website click streams, gaming data, and you name it. The volume is what matters. And on one hand, that volume makes it somewhat easier to spot those issues. After all, on a bigger scale, if things have a chance of happening, they will. But also with data applications, usually the failures are somewhat less obvious than let's say with client facing applications. And quite often if there is a failure while processing the incoming data, Nobody is probably going to resend that data to you. Once it's gone, it's gone. So how do we deal with all that failures in distributed architectures? How do we make our architectures such as data applications more resilient? Now, we are mirroring the underlying distributed systems. So let's take a look at how the cloud providers are dealing with all those failures of distributed systems. Now of course there's a lot of complex algorithms and mechanisms at play when we talk about the cloud resilience. But surprisingly, two of the most effective tools for better resilience. Are also seemingly simple There are timeouts. And retries. And those are also the things that we absolutely need to be aware when we are building our distributed applications. I call them these hidden superpowers. Because while they can be extremely powerful with helping us with the resiliency, They can also backfire if we are not careful of how we use them, and we will soon see what I mean. Now you might have noticed that so far I haven't been mentioning any cloud providers, any services at all, because those things are universal to all of them. But now it's time to finally move on from our fictional service about the failing survivalist architecture, and it's also time for me to confess that the story wasn't that fictional after all. It's actually something that happened to me, at least to some extent, several years ago. So I was working on this simple, powerful data architecture for near real-time data streaming, or a pretty large scale. So there we had a producer that received our data that we wanted to store and collect. And for that purpose, we took Amazon Kinesis data streams. On one end, we connected it to our producer, and on the other end, we connected AWS lambda to it. Just like that, we got ourselves a simple, powerful data processing pipeline. We're extremely happy with it. It was working perfectly. Until one day we realized that we were actually losing data in the pipeline, and the best part was we had no idea it was happening. Thanks to the higher level of obstruction. So, what exactly was going on there? Now, briefly, what is Amazon Kinne's data streams? It's a fully managed and massively scalable service to stream data on AWS. After you write your data to the stream, it appears there within milliseconds, and it's available for you to read for up to 24 hours or up to a year if you configure it to be so. And during that time, you can read, process the data, replay it in any way that you want, as many times as you want, but you cannot delete the data from the stream. Once it gets there, it stays there for at least 24 hours. Now, Kinesis is a very powerful tool. It doesn't have any servers or clusters for you to manage, and also scales massively. And to achieve that massive scalability, Kinesis uses a concept of a shard. And you can just think of it as an ordered queue within your stream, and then your stream will be composed of multiple such ques, multiple shards. And the chart will come with capacity limitations, so you can only write 1 megabyte or 1000 records of data per second in each chart. Though each shard comes with limited capacity, but the number of shards you can have in the stream is virtually unlimited. So you can add as many shards as you want to stream as much data as you need. Now, to get all that data to Kinesis, there are essentially two different API calls. You can either write individual records or you can batch up to 500 records and send them as an individual put records request. Matching in general is a more powerful and more resource effective way of making calls, especially in these data intensive applications where the number of individual requests can get really high really quickly. Well, once again, with great power comes great responsibility, and we will see very soon what it means in the context of patching. Now, we have established by now those that failures in distributed systems and architectures are pretty much inevitable. Especially at a bigger scale, but how do those failures manifest at this higher level of abstraction with services like Kinesis, for example? It's actually pretty straightforward. When you interact with the service from your code, you're usually making API calls, and every API call can fail. Now, the good news is that if you are using AWS SDK to make those API calls, it will take care of most of those failures for you. After all, AWS knows firsthand that the failures will happen. So they have built into the SDK this essential tool for better resiliency or uh superpower as we know it. The rich rise. Now, the trouble with the rich rise in general is that They have a potential of turning a small intermittent problem like a network glitch into a massive one. Because rich rise can have unexpected blast radius. They can cause this ripple effect of cascading failure through your system and ultimately bring the entire system down. Because retries are inherently selfish. Just like hitting that refresh button in the browser, we all know we shouldn't do that, but we do that anyway. We tries to imply that our request is more valuable, more important than anything else. And we are willing to put resources to use extra capacity to maybe incur extra costs on the downstream system just to make sure that our request goes through. But the trials are not even always effective, neither are they safe. To start with, which failures are we even retrying? If the downstream system is under a lot of load, and for example it's an API or a database that is heavily overloaded and that's what's causing the failure, well, retrying might just make matters worse. Or if there was a timeout, but you are not really ready to wait for the retry to complete, for example, you have your own SLA requirements, then retrying is plain selfish. It's just like hitting that refresh button, closing your window altogether. You just wasted wasted resources for nothing. And what if the underlying system also has retries built in, maybe even on various levels of the architecture. Well, that can amplify and multiply the retries and make things even more dangerous. Especially If the underlying system is under a lot of load and you are starting to retry right away without giving it a chance to recover first. And what if the operation that you're retrying has side effects, like database update? Well then, retry can have unexpected side effects and unexpected results. So there are many different considerations here, but the bottom line is we need to be very mindful about how we use this superpower, the retries. Now, AWSSDK comes with this built-in safety measures. So if a request to service like Kinesis or uh other services failed for some reason, will handle only the so-called retryable errors, transient failures like service unavailable, other 500 errors, timeouts. And for those errors, it will retry the failed requests on your behalf behind the scenes, but it will stop after a certain number of attempts. And in between those attempts, it will use the so-called exponential backoff, where the time between retry attempts is increasing exponentially. And those are seemingly simple things. But they are crucial if we want to make sure that retries are serving us as a tool for better resiliency. Because they can actually do the opposite. So we only want to retry. If it can help the situation, so only the transient failures, and when we do retry, we want to stop when it doesn't help the situation anymore to avoid the ripple effect of cascading failures and avoid killing the underlying system. And we also wanted to spread retry attempts as uniformly as possible, not to overwhelm the underlying system, not to send those bursts of retries right away. And with AWSSDK you are given those safety measures, but you also have a possibility to configure some of those parameters. And here's an example of how you would do that with JavaScript SDK. Now every language will have their own ways to configure those parameters and different defaults, but all of them will give you to configure some of them just as they will give you the chance to configure the other superpower, the timeout related values. Now, if the timeout doesn't sound like a superpower at all, and you think that it's not a big deal, I have bad news for you again. Because in distributed systems, timeouts are pretty much a given. So let's once again briefly glance under the hood in simplified terms. When we interact with services from our code, we use API calls that are abstracted away as SDK method calls. And those SDK method calls look exactly like any local method invocation would. But let's not let that fool us because we know that the network is still there. It's just abstracted away from us. And Any request that goes over the network, like API call to Kinesis can fail for many various different reasons. Moreover, it's almost impossible to tell if the request actually went through or not, because the failure can happen on many different levels. Maybe sending the request failed, maybe receiving or processing the request failed, or maybe the request was processed, but then you never got the response back. Or maybe your request is just waiting in a queue because the downstream system is overwhelmed. In any case, the result is still the same. You might be stuck waiting for something that might never happen. So, this can happen to any service, no matter how frivolous or not it is. And to prevent this from happening, to prevent waiting forever, AWS has built into the SDK this other tool for better resiliency, the timeouts. And the ability to configure those timeouts is a superpower that is given to us. But just like with the retries, we need to be extremely careful of how we use this superpower. Because picking the right timeout value. It's not an easy task at all. Just like any decision in your application, in your architecture, it will come with trade-offs. If you pick too long time outs, they might be ineffective and consume resources. Too short timeouts might mean you are starting to retry too early without giving the original request a chance to complete. Moreover, the appropriate timeout values will depend on the service or the code that you are using. They will be different for all of them. Now, for the longest time, I've been scaring people by telling that for all the requests, AWS SDK, uh JavaScript SDK in particular. We'll wait for 2 entire minutes before timing out the request. Just think about it for a second. We are usually dealing with low latency systems like Kinesis or maybe Dynamo DB. We are expecting a response within milliseconds. And here we are stuck for 2 minutes just waiting for it to time out. But since then, things have changed, things have evolved, and JavaScript SDK has transitioned from version 2 to version 3. We have seen some changes to the time, our default values as well. So nowadays, the default timeout value is Infinite. So either be prepared to be stuck for a really long time waiting for something that might not actually happen. Or much better take control into your own hands and configure those timeout values yourself. Now, here we finally get to the first reason. Of losing data in my story. Not configuring the timeouts, just blindly going with the default, waiting for too long for the requested timeout, can actually exhaust the resources of your producer application and make it not be capable to process new incoming data, which ultimately means losing data. And well, that's exactly what we saw in our story. So, because of likely a temporary glitch with a service or a network that caused a timeout to begin with, we ended up with a complete system outage. Which is the opposite from what a resilient system should be. We need to mask the intermittent failures instead of amplifying them. We need to be able to recover from them. Now, obviously this doesn't sound too good, but if your first thought is, let's just set them, the timeouts to a really low value, well, again, I have bad news for you because in my opinion, too short timeouts can be even more dangerous. Because having two short timeouts, the request might be retried too early before it had a chance to complete, which inevitably will add extra load on the underlying system, which can cause all sorts of fun things like increase the load, increase the costs, increased latencies, but ultimately causing that ripple effect of failures and bringing the entire system down. And again, if our goal is to build a resilient system, we should do the exact opposite. We should recover from individual failures. And this is especially true when timeouts are paired with the retries. We need to be extremely careful. So This is where wrongly configured time out and retries value can actually become a match made in hell. And instead of being a better tool for better resiliency, they can do the exact opposite. So once again, even though timeouts and retries are extremely powerful, we need to be very very mindful about how we use them, and we should never just blindly go with the defaults, because defaults are dangerous. So when you go back to your code, please go ahead, check all the requests that go over the network, maybe it's SDK calls, maybe some other API calls. Make sure that you know what those timeout values are, make sure that you are controlling them. Don't just blindly trust it and go with the default. Especially if those timeouts are paired with the retries. Now, so far I've been talking about those failures that are inherent to distributed systems in general, but there is another kind of failures that are actually caused by the cloud providers on purpose. And those are failures that are related to service limits and throttling. So this can be especially confusing in the surveless world because we are promised scalability. Somehow, when we hear scalability, we tend to assume infinite scalability. And of course, if something is too good to be true, is that just that? Because no resource is scaling infinitely. And sooner or later we better face the reality. And the reality is we don't have the entire cloud at our disposal. We are sharing the underlying resources with everybody else. Which of course has its trade-offs. So on one hand, we do have this vast pool of resources to use like compute, storage, network, but this also means that an individual user can try to monopolize resources, and this will inevitably cause degradation of services for others. So service limits are actually there to prevent that from happening, and Throtlink is a tool to enforce those service limits. So if you remember in the case of Kines, we had this short level. Limits which were 1 megabyte or 1000 records per second and once you hit that limit. You start to get throttled, so your requests start to fail. And this brings us to the 2nd reason for losing data in my story. So if you remember, I said that ASD takes care of most of the failures for you. But of course, there's a catch, because in case of batch operations like we have put records here, instead of just handling the failure of the entire request. We should also handle the so-called partial failures. Because you see, those batch operations, they are not atomic. They are not either all succeeds or all fails. Part of your batch might go through while the other part fails, but you are still getting the success response back from the SDK. So it's your responsibility to detect those failures and to handle them. And the main reason for those failures is exceeding service limit, for example, because of a traffic spike, so maybe. You could write the start of that batch to the stream successfully, but then the limit was hit and the rest of the batch failed. Now, luckily, we already know that there is this wonderful superpower for better resilience that we can use, that can help us with those transient errors, such as intermittent spike in traffic, and that's, of course, the retries. And we also know that when implementing retries for any failures, we should be very mindful, and there are 3 key things that we need to keep in mind. So let's go over them one more time. So we only want to be retrying if it can actually help the situation. We always want to set an upper limit of retries, to stop retrying where it doesn't help anymore. And we want to use exponential backoff between retry attempts. Or even better so, use exponential back off and jitter. And jitter is just this random component that you add to the exponential back off to spread the retry attempts more uniformly. And what you're trying to achieve is that you don't want to put an extra load on the underlying system, because if you're hitting a service limit, you're probably already quite close to the edge, so you don't want to be pushing the system over the edge. So you want to spread those retry attempts. And the jitter, or exponential back of a jitter in particular, is a very simple trick that can actually dramatically increase the success rate of your retries. And this is in fact something that SDK uses under the hood. I just didn't ment mention the jitter in the previous slides. So, very simple trick, very powerful. So if, so far from this talk, you remember anything at all, let it be timeout. Partial failures of batch operations. And retries with exponential back off and jitter. Those things can save you a lot of headaches in all sorts of unexpected situations. And to borrow one of my favorite quotes from Gregor Hopper. Retriess have brought more distributed systems down than all the other causes together. Of course, this doesn't mean that we shouldn't retry, but again, it means that we need to be very mindful about how we do this, not to kill the system that we are trying to fix. Speaking of which, let's now take a look at another example of what can happen if we just let the matter slide and go with the good old defaults. And this one is from the other end of our architecture. So, if you remember, we had the lambda function there, reading from our stream. And it turns out that things can escalate pretty quickly on that end as well. So let's take a look. The alumda itself is actually a prime representative of distributed archi architectures. It's made of many hidden components that work together behind the scenes to make it so very powerful. And one of those hidden components is called the event source mapping. The chances are high, you have never heard about it before. And the event source mapping is hidden well under the lambda obstruction layer. But when you are using lambda with event sources like Kinesis, for example, or Dynamo DB streams or some others, it's in fact event source mapping that will be attached to that stream and that will be reading records from the stream, and it will be batching those records and invoking your lambda function with that batch for you. Now the event source mapping reads records from the stream, from all the charts in the stream in parallel. So you would have the same amount of landers reading concurrently from your stream as you have charts. Unless you're using one of the features that event source mapping provides, which is called polarization factor, where you can actually have up to 10 lambdas reading from each shard in the stream. Now, this is a great example of the power of parallel processing where we can just speed things up by throwing more lambdas at each chart. But of course we all know what comes with great power. And We need to remember that no resource scales infinitely, not even lambda, and here we have a chance of hitting one such important service limit called lambda concurrency limit. And this one basically says that or means that we can have a limited number of lambda invocations, concurrent lambda invocations in the same account in the same region. And by default this number is set to 1000, though I've heard stories that new accounts only get around 100, which is extremely low for lambda, and this is a soft limit. You can increase it by making a request, but there still is going to be a limit. And once you hit that limit, all the new land vacations will be throttled. They will fail. So imagine you have a Kinesi stream with 100 shards, and then you set the parallelization factor to 10, and here you have 1000 lambdas just reading from your stream at all times, which probably is not a problem at all until somewhere else in your account, in your region, some other lambda that has nothing to do with the stream starts to fail for very unclear reasons. And the reason is that your stream, and your stream consumer has used the entire lambda concurrency limit for that account in that region. So this is a limit that can actually have a blast radius well beyond your own architecture and your own system, so you need to be extremely mindful about it. Now let's go back to reading the records from the stream. So what happens if lambda fails to read a record from your Kinesi stream? Once again, I come with good news and bad news. So, the good news is that the events are mapping comes with extensive air handling capabilities. Now, to use them, you need to be aware of them and you need to know where to look. So the bad news is that you're very likely to just blindly go with the defaults, and we know by now that defaults can be very dangerous. Excuse me, went the other way. Uh, so what happens by default if lambda fails to process a batch of records? Let's say there was a bad record with some corrupted data, lambda could process it, we didn't catch it, lambda throws an error. What happens next? So by default, even though in this situation, no amount of retries will actually help anything because there's a bad record, you can't process it, but by default, Lambda will be retrying the bad uh batch of records over and over and over and over again until it either succeeds, which again we know is not gonna happen, or until the records in the batch expire. which in the case of kinesis happens in at least 24 hours. So it's at least one day of absolutely useless retries from Lambda. And you can imagine all those useless invocations, and mind you, they are not free. You are still paying for them. But that's not the only problem there because all those retries will likely cause reprocessing of the same data. Because you see, from perspective event source mapping here, either the entire batch succeeds or the entire batch fails. So even if the lambda succeeded to process some part of that batch and then it failed, the entire batch will be failed by default, and the entire batch will be retried over and over again. Like records 123 there, you will probably reprocess them a bunch of times. But that's not it. There's more. Things get worse, because while all those useless retries are happening, no other records are being processed from that chart. That chart is technically stuck. It's just waiting for something that will never happen. So, because of one bad record, now we have a chart that is completely non-functional. Which is why it's often referred to as a poison pill record. Now, After 24 hours passed. Yo lambda finally can go on. The data is expiring from the stream. Of course, part of the batch is leaving the stream unprocessed, but that's life. What we can do about it? Nothing at this point. At least Lambda can now catch up. It can continue processing data from the chard. Now, the problem here is that by that point in time, your short is probably filled with records that were written to the stream around the same time as the expired records. Which means that they also expire around the same time. Which means that Yolanda might not even have a chance to catch up. The records will keep expiring and expiring, and you will keep losing data. So I often bring this overflowing sink analogy. When we pour water too quickly, we can't drain it quick enough, so the water just overflows, and that happens to our data. So even though we started with just one bad record. We might end up losing a lot of valid and valuable data. Once again, the opposite of what a resilient system should be. And you might have guessed this is exactly what we saw in my story. Just like with those poorly configured timeouts and retries earlier, we turned a small localized problem, one bad record, into a full-blown outage. And in the process, we ended up losing data, added latencies, incurring duplicates, incurring extra costs, spending resources, and all of that for absolutely nothing, leading abs absolutely nowhere. And all of that because we didn't know any better and we just blindly went with good old defaults. And here again, the wonderful quote by Gregor reminds us to be very careful with the retries. Luckily, there are many simple ways to be more mindful about failures with lambda. And first and foremost, we need to configure those parameters and those capabilities that event source mapping comes with. Now we know by now that the most important thing we can do is to configure timeouts and set limits for retries. And you can actually do both of that with events source mapping. But both of them are set to -1 by default, which means no limits. And this is precisely what we saw in our story, so make sure you configure those defaults. There are also a lot of other very helpful, uh, very important settings. I'm not gonna go through them today, but knowing about them, knowing how to use them can be extremely helpful and can save you a lot of great hair, great hair in the future. And you can combine those settings in any way that you want and use them in any combination that you want. Just whatever you do, please do not go with the defaults. So we have seen today that sometimes we can cause more problems while trying to fix them. And this is especially true if we don't make conscious, critical, informed decisions about handling failures. And things like timeouts and retries can be extremely powerful if we put a thought into it. But if we just let the matter slide. They can turn against us and can make our architecture less resilient instead of doing the opposite. So, next time you build a distributed application, I encourage you to be brave. To face the messy reality of the real world, to take control into your own hands rather than believing in magic, because things are not magical, and that's a good thing. And distributed systems and architectures are extremely powerful, but they are also complex, which doesn't make them neither inherently good nor bad. And the cloud, especially servalus, abstracts away a lot of that complexity from us, but that doesn't mean that complexity is not there anymore, which again, doesn't make them either good or bad. And while we absolutely don't need to know every single detail about every single service, that's pretty much impossible, there are some fundamental things that are inherent to the cloud and distributed systems in general, so things like service limits, partial failures, timeouts, retries, back offs, and those are all the fundamentals that we absolutely need to understand. Otherwise we are just moving in the dark with our eyes closed and hoping that everything will be fine. Now, on an even more philosophical note, distributed systems and architectures are hard, but they can also teach us a very valuable skill. To embrace the chaos of the real world. Because each failure is an opportunity to do things better, to make our systems even more resilient. And even though we can never build something that never fails, despite our best effort, there's one thing that we can do. We can learn and grow from each failure. And as Doctor Bernard Bernard Vogels likes to remind us, everything fails all the time. That's just the reality of things. So, either in life in general, or with AWS services in particular, I'd say that the best thing that we can do is to keep calm and be prepared when those failures happen. And that's it from me for today. Thank you all so much for listening, and I hope I will get feedback from you very soon. Thank you.