---
video_id: fXB6wftmigU
video_url: https://www.youtube.com/watch?v=fXB6wftmigU
is_generated: False
is_translatable: True
---

Good morning. Welcome to Reinvent Oh yeah, uh definitely put on the, actually you don't have to whatever you want, uh, if you do put on the headphones and I'm speaking too loud or too softly, you have a volume button on the right side, I think. Uh, so, um, let's, let's say we're building a game. All of us are starting a new gaming startup, and we're going to create a massive online multiplayer game that if we're successful, is going to support hundreds of thousands of users. So we're going to need massive scale, probably millions of requests per second. And we're going to have to maintain very low latency for our users to have a consistently responsive game, otherwise they probably won't play our game. We're going to use Elastica cerveus for Vki. Because it provides predictably low latency and supports the massive scale that we need. Now, my name is Ela. This is Jaron. We both work for Elasticash. And Elastica Serveles does take away many concerns that you have as a developer of DevOps. For instance, you don't have to worry about security patching. You don't have to worry about version upgrades, but most importantly, you don't have to worry about sizing your cluster and you don't have to scale it when your application load increases. Uh, this session is going to talk about optimizations that are relevant for very, very high performance applications, and optimizations are going to be relevant both for Elastic Serers, also for self-design clusters, or even if you're self-managing your own fleet of open source or Valy. Valy, in case you're, you're not familiar with Valky, it's an open source in-memory database. It was forked out of Reddis when they changed their license, and Valy has been adopted by the Linux Foundation, and it's, it's maintained by a very big community, very active community, including some industry leaders and a few cloud providers, including AWS. Let's go back to our game. So, let's start with a PLC of our game, right? We're just in our labs now. Uh, we're taking a few, a couple of clients, connecting them to an OJS server, right? No JS, that's easy, that's, uh, that's fun. So, um, we'll probably just store all the state of our game in memory, right, that would include things like where our users are, where the monsters are, maybe their health, things like that. And we're going to use some disk if you want to store some more persistent resources. For instance, our game maps, our castle maps, things like that, that don't change that often. And so Um Our users on the PLC, our users are probably going to have terrific, uh, just terrific latency, right? We're in the lab, uh, what would we expect in the lab for latency? Any ideas? OK, so usually when latency, networking is very, is not a concern, latency, you would see latencies of around 100 microseconds round trips. Most of this latency would come from the operating system, the network stack, the kernel communicating with user mode, right? So it's not really network latency here, it's mostly just the operating system. And when we go and access memory for our state management, reading state, changing state, we're probably going to have access times of a few tens of micro nanoseconds, nanoseconds. So that means that we can actually go to the go to memory many, many times before it actually impacts significant latency. We could do it hundreds of times on a single user request, maybe even thousands of times without it impacting latency. This is very different when we talk about these latencies, right? Even if you have the best, the best SSD, it's going to be a little less than one millisecond probably. But it's going to vary, right? Why is it going to vary? Because when you read an object from disk, you're not just doing a single axis. Sometimes you're doing multiple axises because disks store objects on different blocks, right? So disc latency is usually very variable. Which we probably want in our application, yes, no. No, probably no, right? We don't want this variability, and so, I mean, for our for our game POC we can probably just load all the maps when we start, keep them in memory, and then just use them there, right? But when we take this to production, It's going to be a little bit different. So now our users are on the internet. We're using multiple instances of our application because we want scale and we also want high availability. We'll probably use a bunch of availability zones to get that, and our resources instead of this are going to be naturally placed in some database. So, uh But when we read the database, latency is going to be again very variable, somewhere probably typically 1 millisecond to 100 milliseconds, and that's because traditional databases, whether it's a relational database or a document database, I mean those usually use a combination of both memory and disk to deliver queries, and it's very hard to control where your queries are serviced from. It's hard to say, hey, everything must come from memory, and I guess that's where ValKy becomes very, very handy because ValKy uses exclusively memory to serve your queries. It provides you with predictable latency, and I think that's what we want for our game. By the way, for any application, it doesn't have to be a game, right? We chose the game because games are fun. But any application that needs millions and millions of requests per second on strict latency requirements, VALKy is very useful again because of the low variability. I mean, Valky uses memory, so most of the latency you'll see. It's not even coming from VALKy usually. The most dominant factor with VALKy latency is probably going to be the network, right? And so, uh, if you're on the same AZ as your vial key node that you're accessing, you'll probably have latencies far below 1 millisecond, sub millisecond latency. Uh, now it, it may be that your application needs to go from another AZ and access and bulky node on a different AZ, and then latency increases, uh, increases slightly, and I, I'll dive deeper into these latencies in a minute. But what if, what if we could manage our application such that we're reading from a replica node that's on the same AZ that way we can maintain, at least for our reads, we can maintain low latency for all of our application instances, lowering our P50 and our average latencies for application. Sorry. Let's dive deeper into latencies. So we're going to talk about, we're we're talking about milliseconds, we're going to talk about microseconds, and we're talking about nanoseconds. That's the world we live in. Let's start off with your users. When you have users coming over the internet connecting to your application, usually or typically you'll see latencies somewhere between 20 milliseconds up to 100 milliseconds, maybe even 150. It really depends on when your users are, right? They could be very close to the data center with an excellent internet connection, and they could be very far away. They could be on another continent. So that's a variability we'll see. When we continue into the into the data center, if you're communicating between two servers in the same data center, Lancies are going to be very, very low. It's very similar to the lab we discussed before. So network latency is going to be nanoseconds usually, and it's going to be around 75 to 100 microseconds round trips again, usually because of operating systems, right? You can optimize that using tools like DPDK maybe that optimize this kernel to use a more latency, but usually that's what you'll see, and these are very low latencies anyway. Now when we go from one data center to another on the same AZ. We can expect latencies of up to 1 millisecond. In case you're not familiar with this, uh, AZs usually have one or more data centers. They're not just a single data center, and these data centers can be miles and miles apart. So latency is slightly higher. And when you go, when you go over the AZ to another AZ. Those data centers are even more far apart, right? And then latency can in some regions, oh sorry, can be. Can be milliseconds. Now the most dominant factor here is distance, right? We have to carry this signal over fiber optics from point A to point B and back, right? So we're bound by the speed of light. That's the, that's the thing. So if they're close, it's going to be good latency. If they if they're farther apart, it's going to be higher. Uh, let me show you some real, real life examples. I hope you can read this. I, I'll try and read this aloud. What we're looking at are actual latencies from US 3 East 1 region. I took this a few days ago when I prepared the slides, and these are cross AZ latencies, latencies from AZA to AZB, AZ1 to AZ2, AZ2 to AZ1 to AZ3, just a few measurements. And you can see if you are unable to read this, most of these latencies are far below 700 microseconds. Right, these are the quasi ones. So we said they could be milliseconds. In this case, in this particular region, and they're far lower. Let's zoom in now. What happens inside IE? Lancies that are coming that are comparing inside each AZ. So there's a latency from two points inside AZ1 and other latency from two points, 2 machines in AZ B. These latencies are much lower, as you can probably spot, right, and most of them are around or below 100 microseconds. So, excellent latency in this case. Uh There is some variance here. It depends on which region you use and which AZs, where they are, where they where they placed the distance between them. What you could do, uh, these, these charts are not something internal to AWS. You can all view them. I see nodding heads, you know this, so you can, you can check out something called the Network Manager. It's available on your AWS console, of course, the APIs, and you can actually take a look at all the latencies between different regions, different diseases in those regions, and then you can, you can look at your application and consider the factors that around latency here. What latency can we expect between AZ? What latency can we expect between regions, any use case that you have. Moving on. er, so. Uh You folks have tasked me with creating our first microservice for the game. And I didn't do an amazing, amazing job, spoilers, and I created this microservice for getting user profiles. Now this is not a good practice. See if you can spot my mistake here. OK, I see some smiley faces. Don't laugh at me. Right, uh, so let's, let's go over what we're doing here, right? This is a very simplified, uh, HTP server. Whenever there's a new request, the first thing that it does, it goes and creates a new bulky client called Connect. And then does it get for the profile that we actually need now. The problem here you can probably is whenever we have a request we're doing all these things just to get a single, a single key, right? We're getting a new connection that means DNS query. It means TCP handshake, TLS initialization. It means cluster discovery, doing cluster slots, understanding where everything is placed on the cluster, right? And then finally, what we actually wanted, which was getting the key, doing the get command. So, uh, so I'm sure I'm sure most of you have uh already know this, but the the good practice is to use persistent connections, and that means just taking this connection initialization part, taking it outside, oh, sorry about that, uh. Now I'm sure you know this, but persistent connections are extremely important, and you can actually do them even if your compute is lambda. Some people don't realize it, but lambdas, I mean, they don't actually get torn down after each call, right? They, they persist. If you have more calls, if they remain hot. So persistent are definitely useful both for EC2, ECS, Kubernetes, or underus cases, but Uh Another important point about connections. is to understand what happens when we do this good practice, right. So, pipelining is another important factor that can influence performance, especially when you're when you have a high performance application. Let me explain. Pipiplining, by the way, it means sending out multiple requests on the same TCP connection. Without waiting for responses. And this is exactly what's going to happen in our web server, because take a look, what happens when a user, user one send uh sends a request and the server read it, reads it. It heard the request. It's going to shoot out a get command to Orvalki right now. This is a synchronous, right? We're doing an await here. That means that execution waits, but the server is not blocked, right? It can continue servicing more requests in the meantime. So another request is read from user 2. And what happens, it gets sent as well, and user 3, and it gets sent as well. So now we have a pipeline, even though I haven't written the word pipeline anywhere on this code, right? It's kind of an implicit pipeline. And that's what happened. And don't get me wrong, pipelines are great. Piplining is an excellent way to increase latency with Valy or open source re because it's kind of if you send things in batches, the server has to do less work in order to service them, right, less IO. Maybe they can read all the requests in a single IO operation. I think the problem is when these pipelines, when you have very high performance and our game is going to have millions of requests, and the concurrency is going to be very high for each of these application web server, and then what happens if this is not 3 requests at the same time, but 1000. And that's when things get a little bit dicey. Let's zoom into the connection. So we had request number 1, user number 1, request for user number 2, request for user number 3. Now the rest protocol for both Valy and open source is ordered. That means that responses must arrive in the same order as the request that we send. So very susceptible to head of line blocking. What happens if user one requested a very, very large profile? We have to read all the response for user one before we read the response for user 2 and 3. And so That response for user one is literally holding the head of the line for us. And again, with small numbers it's not significant. with large numbers more so. And there's another problem. What happens if you run into packet loss? Should we even, I mean, we're using TCP, I mean, should we even worry about pet loss? No, yes. So I mean does guarantee delivery. The only problem is that retransmissions in TCP take at least 200 microseconds to happen, right? That's hard coded on the Linux kernel. It's not something you can do without rebuilding it yourself or something. So if we are having these large requests or or or or just a request that's not large. And there's 1000, 1000 requests waiting behind it, and there's a packet loss. All these requests, all these users are going to have a major impact. Now, packet loss is a fact of life, when you have a distributed system, right? We cannot avoid it, but we definitely want to reduce the impact when it happens. And the way to do that is to use more connections. Right? We can use more connections from the same application, thus limiting the concurrency on each of these connections. And that helps us minimize the blast radius whenever there's a large response or something like that. I mean, we can even think about things like, hey, if I know this type of request is going to be slower, I can maybe place it on a dedicated connection. That's another idea you can consider, consider. By the way, another, another very easy way to reduce concurrency on each connection. is just to scale up the application, right? The more application instances we have, the less work each of them needs to do concurrently. Of course that comes with a cost. Let's talk about connection pooling. So, connection pooling means using multiple connections from the same application instance. Some uh Valky and Reddik clients have uh connection pools built in. But many of these clients, especially the asynchronous ones, don't. They actually use a single connection and kind of multiplex request in in in that connection. And so what we could do if we actually need high concurrency, right, if you're talking about millions of requests for application and we have high concurrency of a single connection. We can actually build them ourselves. And for our game, we can build something like this. Notice what I'm doing here? Instead of creating one persistent connection, I've just looped and created a bunch of them. And whenever I want to use a connection, I need to pick one, right? I can choose. So in my case, if you'll notice here, I've done a module over the user ID just to kind of load balance between these connections, but you can do a random randomization as well. You can do one robin. You can module over something else that makes sense for the application. OK. So, everything is fine. Our game runs perfectly. Our cluster is very healthy. It has a lot of total resources, right? We have enough total memory, enough total CPU, enough total network bandwidth. Everything is peachy and then suddenly it's not right, even though we have enough resources, we have an alarm, our latency is spiking, our throughput is reducing, and to scramble to figure out what happened and a lot of these, a lot of cases, what happens is that we can run into hot spots. Hotspots mean that a lot of your traffic are hitting 11 particular node in the cluster, right, and that's usually because you have hot keys, so you have a key that's very popular and you need to read it a lot of times, but with Valy and Reddit, it doesn't matter if it's surveillance or not surveillance. Each shard actually owns, each key is owned by a single shard. And so I mean if this weren't, if we're diving into the metrics to understand what the problem is, let's say we all went through the metrics, we reached the network bandwidth case, we see all of the shards, all of the shards do kind of moderate bandwidth network, network bandwidth, and one of them is significantly higher, and that's probably a good indication of a hot spot. Now what happens here? When we all of us dove into and debugged our situation, we found that we have a castle map in the game that's 100 kilobytes in size, so not very huge, right? And it's very popular. Users love it. That's a good thing, and we have to read it a lot of times. But the problem is that say we didn't choose Alasique Serre for this particular example. We chose an in for our nodes which has 937 megabits per second bandwidth, right? If you calculate and you divide it by 100 kilobytes, you can only do around 1000 requests per second for this object. I mean that's extremely low for Valky, right? Just 1000 requests per second. And And so the most obvious thing that we can do in these cases is probably to just scale up our nodes, right? We take the nodes, we replace them with larger instance types, let's say twice as large, so we get twice the bandwidth, and we can do instead of 1000, we can do 2000. But of course this comes with a cost. And also, by the way, with Serves, you don't have to worry about this particular about scale scale up because Servalless automatically scales your instances up in place and your own is going to dive deeper into that in a minute. Uh, but I mean, even with servers, what if you don't want X2? What if you want X4? What if you want, actually, what if you want X10 or 50 or X100? We're not going to find instance types with that much network, right? Just doesn't exist. So what can we do? 11 nice mitigation that we can try is we can duplicate our object, right? We can take our castle map or all of our castle maps, and we can create copies of them. Now because each copy is a different key, it gets placed in a different chart, and now whenever we read a Castle FO application, we can choose which copy to get and now we're spreading the load, the kind of load balancing the load of reading these these objects between different charts. Now think about it. With 7S, it automatically scales, scales, sorry, and it can reach hundreds of nodes. That means that if you do 100 copies of your object, if you need, if you need 100 copies, you'll get 100, 100 times your throughput. Another another thing that we of course can do, we can read from replicas. If we can, if we can read from replicas, we have more nodes participating in our load balancing. Let me show you a very naive example. of of code for this, we have our castle cash. And when we read, what we need to do is to choose one of our copies, right? So we're choosing a copy, and here to module over the the application worker ID. It helps me maybe with data consistency, but you can randomize, you can do round robin, you can do whatever business logic makes sense for your applications. Do you folks already see the caveat here? What happens when we want to update the castle map? We have to go and update all the copies, right? So if there's 10 copies or 100 copies, we have to do 100 rights. So, and, and, and this really works great for situations when you have a lot of reads but uh not as, not as many rights that that's a very common use case, right? This is, this is super helpful uh or it can be uh. But if you do have to do a lot of reads and writes at the same kind of frequencies, this won't work for you. So we have another mitigation that we can try. It's a little bit more work on our application. But we can actually split, break apart our objects into smaller ones. So for our game, we can take each castle and maybe break it down into rooms or areas. And now when we want to read them, we can actually pipeline. Multiple reads, get all the parts for our castle, and then assemble them on the application side. And now see how the load is again spread over more shards. Kind of mitigating the hot spot and if we can do further optimizations for application, we can maybe maybe we can down the line right logic that we don't need to get all the rooms for a particular for a particular user we can just get a subset of things like that and of course you can use your imagination outside of this game example to see how you can break apart your your object and your application. Uh, right now, uh, let me turn over to Jeon, who is going to tell you how we designed the Elasticque Cerveles, how we're doing all this automatic scaling. Thank you. Thank you a lot. So a moment before I dive deep into the surveillance and the way that we build all the technology behind the scenes, Elad mentioned multiple challenges that we need to. To take care of when we build an application, specifically if we build them for high scale, so I want to tell you with elastic surveillance, you don't need to worry about capacity planning or right sizing your cluster. You can still achieve millions requests per second with a sub-millisecond response time just using by elastic surve. Now when you create an endpoint to Elastica surveillance, automatically we are distributing all the infrastructure of the surus across multiple availability zones. We do that first for high resiliency, high availability, but we also do that to achieve a very good performance and to reduce the latency. I will show you in a moment how it's happening. So once you create your endpoint to Elastic surveillance, you connect from your VPC using VPC endpoint to the Elastica surveillance VPC. At the beginning, you're going to reach the NLB, and the LLB are responsible to balance the traffic across the surveillance proxy. Now I'm going to talk about the proxy in the next few slides and to explain why we built the proxy, but overall, the main job of the proxy is to route your request to the correct cache node. Now we also know to locate your local proxy that sits on the same availability zone so you can reach the minimum latency while you connect to elasticaerus. So as I mentioned, the proxy responsible to route the request to the correct cache node, and in order to do it very, very effectively. The reason that in order to do it effectively. is because we are using a multiplexing technology, multiplexing protocol. We actually use a single TCP channel to connect the cache node while we are moving on that channel multiple different connections of clients, and in that way we are reducing the number of connections and we are also reducing the number of system calls upon every network transaction. I want to move back to the cash note itself. We are running on a multi-tenant environment. We are running on a physical host. That we are constantly monitoring all the time. We have a service that we call it heat management, and the job of the heat management is to monitor the cash load, all the physical loads across our fleet. Now because we are running on a multi-talented environment, we are sharing resources like CPU, memory, and networking. And the cash nodes running inside the VM within the physical host, and as you can see, each cash node has a different size. And the reason for that is because each cache load has a different workload to digest. Now our job as a surves is to make sure that you have enough extra capacity to run your workload. So I mentioned before that we have the heat management process that run and consolidate all the physical host status across our fleet. And because we are constantly monitoring and making sure that we don't, you do have enough resources to run your workloads, we are in some situations we can find physical laws that reach to some threshold. For example, in here what you see, we reach to some threshold of memory and network. And we need to start to think how to move to evict some of the cash load from this physical host to a more available physical host. So we are running, we are using the algorithm of the power of the two random choices. We are actually picking the two most hottest cache nodes, and we choose to move only the second one. The reason that we are choosing to move only the second one is because we don't want to interrupt the most busy one that is probably going under heavy load and start to. With maybe scaling operations, so we want to let them succeed, but we still want to move the one that will release enough resources once we move him to a more available cash node. Elad before that explain you why it's so important to have a persistent connection. There is a cost when you create a new connection, and with the proxy we promise you while all the changes happening behind the scenes to store your connection. And to have it persistent to the elastica cervus. Of course, the proxy will also responsible to move the connection to the new created cache node, the one that we move to a different physical host, and in that way we are keeping all the fleets very balanced with all the enough resources that is required. Now there are some situations that you cannot wait for the heat management to free enough resources for your workload, and you need something more drastically. You need to handle a burst of a workload that kicks in because of an event or something that now is happening in your business. And for that we build a very nice technology in the elastic surveillance to support that. We are actually using a platform technology. That does not have a fixed memory or CPU footprint and can be can be rescaled up and down very instantly. We are using the technology that this technology is based on a multi-talented environment so we can keep the cost very minimal. So this technology allows us actually to scale up very quickly and to use the more calls and memory that we have on the physical host and actually we can support 8 times more throughput on demand while once kicks in into the surveillance technology, surveillance endpoints. Now Because we start with scale up, now we have time to start the scale horizontally, scale out, and the way we do that, there are 3 main stages. We have the detection, which is happening very quickly. We also have the ability to project the incoming and the upcoming workloads because we are constantly monitoring your workloads and we know to predict what would be the workload in the next coming minutes so we can pre-scale your cluster behind the scenes and be ready for the workloads that can come now. The second stage is the provisioning. To provision a new node is very costly, but we are using. A worm pool worm pool is a list of cache note predefined, pre-installed, that waiting to be attached very quickly to the cluster. Now once we use the worm pool, we're attaching them to the cluster, and the next, the last phase that now is required is to move the data from the original shards to the new shards that we just attached. So for that we are doing, we are monitoring the data within the slot, and we are capable to determine which of the slots are considered as hot and which of them are cold, and based on that we can decide which slot we want to move to which target. We can do that in parallel within the slot so we can move them very quickly and we can do it in parallel between the different targets so we can actually assign multiple targets to our cluster. And to transfer the data in parallel so the scale out happens so quickly so we can double the cluster throughput every 2 minutes. I promise that I will speak to the proxy and the reason why we build it. First we decided to build to have a single logical entry point to the surveillance cluster. And for that we build a proxy. The proxy main job is to route the request from the client application to the cache node. So it actually encapsulates all the underlying cluster topology, everything that is related to failover, disconnection, scaling, everything is happening behind the scenes. So your application can consist with a single connection to the proxy. While the proxy will maintain thousands of connections behind the scene to the cluster, and you don't need to be careful about that because you can scale your throughput. Very instantly using this proxy technology and all the scaling that I mentioned, so you can jump from 0 to 5 million requests per second in only 12 minutes. Now if your application is latency sensitive, And you don't have a strong consistency requirement, uh, you can use the reed form. That we also have it on Elastica cerveus. Every time that you create Without We create one primary and 2 replicas as the beginning of the show. Now if you want to connect to the replica, the only thing that you need to do is to flag the connection that you want to read the data from the replica, and the proxy will do it for you. Another thing that the proxy will always make sure to do is to read always from the local availability zone. So regardless if it's primary or replica, the proxy will make sure to read the data from the local so you can achieve the microseconds response time. Now let's take a look how to build the cord so we can see how, how simple is it uh uh to work with elastic servalus. So here in my code, I'm just connecting to my uh elastic erus. I'm using TLS enabled because always when we're running on surveless we support only TLS connection. And here I'm flagging on that specific connection. I'm telling to the proxy, I want this connection also to read the data from the replication, so I can scale the retroput, and I can also achieve the sub-milliseconds response time. Here I have 3 very simple block of codes. The first one, just populate a few keys to my cache notes, to my cache server list. The second one is a for loop that I'm fetching the data for my uh cache node. And the 3rd 1 is pretty the same just with pipeline. I'm doing a batch of multiple keys that are running all together now. As simple as that is that we are going through the proxy now. The proxy will make sure that we to spread the request to the correct child behind the scene. He will also promise me that if there is any disconnection, scaling that happened in parallel, failover, nothing will disrupt my connection. Everything is happening because the proxy will take care of that. And the most important is that the proxy will promise me to read the data from my my local availability zone. As as you can see, the beautiful thing here is that my code stays clean. I don't need to worry about that. I don't need to put any special code to to do that. Everything is happening and doing and managed by the proxy. Now I want to go back to Valky. I talked about VALKy. All the examples, all the technology that we build here is based on the Valky, and I want to show you how we leverage the Valky technology even to improve the surveillance scaling that we offer today for our customers. So I want to go back in time and to see how Valky started. Valky designed as a single threaded process. And the main reason for that was simplicity. Because there is no risk condition, no synchronization is needed, and you can still scale out and to use the share nothing architecture and to improve the cash currency. With this technology, with this technology, we can achieve around 1900 requests per 2nd, and 50% of the time we're going to be busy on the aisle, on the read and write. Now to simulate that, we have the main thread. Every time that another workload comes in, it's weight on the pipeline for the main threads to finish the processing of the workloads on the queue, which eventually what's happening, what's happening is that we have kind of head of line blocking that Elad showed you before. Later, Valky introduced the IO threads, and the main idea behind the IO threads was to offload all the IO requests to the IO threads itself. Simple idea, does the dub, double the performance, almost close to 400 requests per second, and this time we are now more busy on the process command. 50% of the time the process itself was busy on that stage. Last year, specifically, AWS and my team uh contribute to Valy, the new uh IOTres architecture. The idea is very simple. We have the main thread that orchestrates all the jobs that span the IOT rates. Now we ensure that there is no race condition and the number of IRT rates can be adjusted according to the workloads that are running on the harbor itself. Now despite the dynamic nature of the higher threads, the main threads are also responsible for the affinity, and it also always makes sure that the data is assigned to the specific thread so we can also enjoy from cache locality and to improve the performance. So with this ability we can achieve even more than 1 million requests per second for a single instance. That was a huge win for Valcke. So just to simulate that, now we have the main thread running, and every time that we need to spawn another thread when a workload is coming in, we just spawn it out. We can process the workloads in parallel, and we can remove some of the threads and to release some of the computes once the data, once the workload is relaxed. Now let's see how everything is playing all together. Uh, we talked about the worm pool, we talked about the proxy itself. We talked about the physical laws that we're running on a multi-talent environment. And here in my example, I have a cash note running with two calls, 2 higher threads, because we're running a very steady state of 300 requests per second, this is the throughput. But now let's assume that we have some events that kicks in. And our customers expecting us to grow our instances very instantly. So immediately we can scale our throughput by 88 times because we assign more calls. We are spawning more higher threads to the cash node, and in that way we can scale up in parallel in the mid. Meanwhile we are just assigning more cash node to the existing cluster because we are using the worm pooling. The proxy automatically going to detect that and all that happening in parallel because it encapsulated from our application, we started to move the data from the old chart to the new shard and in that way we can scale the throughput, double the cluster capacity every 2 minutes, and achieve a very seamless, surveless experience. Now, I have a, I have here a pre-record demo, nice demo that we started with uh uh adding more workloads to the cache surveillance nodes. As you can see, I started with 250 care requests per second, and I'm going to add more and more clients and workloads to my cash surveillance nodes. Now, in the meanwhile, while you see that the throughput is growing, my P50 latency is staying below the milliseconds response time, which means we are still having the microseconds response time. Even when we pushing more workloads to the cash node, as you can see, I'm already achieved the 100 million, more than 1 million requests per second, and the P99 is going to jump to 3 milliseconds, and this is because we started to move things. We are using more res. We are using more hardware resources, so it might impact the. P99 latency, but the P50 will stay behind below the microseconds, and the P99 will saturate it while we finish with all the scaling process. So the demo will run until we reach 3 million, but we don't need to wait until that. I want to recap and to talk about a few best practices that we learned for today. So we talked about connection pools. Persistent connection every time that we establish a new connection is in order of magnitude more expensive compared to simple get and get and sect command of Aki. So using a connection pool, using a persistent connection, long lived connection is very important. We talked about reading from replica to achieve to scale their read throughput, to achieve high availability and resiliency. And specifically for surveillance, it will benefit you to achieve the microsecond response time. We talked about hotspots. We have two mitigations for that. Specifically, we talked about Elad mentioned about the key duplication. That you can duplicate the keys and read and use all the hardware resources that you have on your cluster. We talked about the key splitting that you can split specifically across your shards and to read even more throughput on that specific key and related to that, we also as a key principle we say limit your size of your object because eventually if you have a large, very large object, it will consume processing time and CPU. And also the payload itself, it can also reach you to a situation that you're going to exceed the network baseline. Because we are talking about caching, one of the most important principles is how you can save your memory more freshness, more close to the original without the stale data, and also specifically with surveillance, it can also help you to reduce the cost. So time to leave expiry is one of the ways to do that. You explicitly assign them to a key or a set of keys. And you can use a relative time or absolute time. But one of the most important, if you're going to use it, try to use the random jitter when you delete them, because once they're all going to be deleted, we don't want them to be deleted on the same period of time, make your Vy very busy with that. We want to spread it across time window. That was all today. I hope you enjoyed the session. Thank you very much. Uh, me and Elad will stay around. You can talk with us and ask questions, and I hope you enjoyed that. Please fill a survey and tell us how you enjoy it for the session. Thank you.