---
video_id: 7nY-BifCorI
video_url: https://www.youtube.com/watch?v=7nY-BifCorI
is_generated: False
is_translatable: True
---

Hey, good morning everybody. Great to see a full house here. My name is, uh, Mike Diamond. I'm the principal product lead for Responsible AI for AWS and I'll be joined today by, uh, Louis Baker who's the senior data science manager and, uh, head of AI for for Indeed. And we're going to be talking about scaling AI responsibly. OK. Every AI system incorporates a specific posture towards responsible AI, by which I mean is built upon a set of decisions and positions in regard to how it should operate responsibly and minimize risks, and I think what I'm saying will become clear if I walk through a couple of use case examples. Let's take for the first example a real estate company. Let's say they are building a system to output property descriptions of condominiums for buyers and sellers. They have to consider does the generated text welcome all buyers, demographic groups equally. Are the property features accurately captured or there hallucinations in the descriptions of the properties? Have any private details leaked in the descriptions from previous owners and occupants? And is the content that they're using, the images that they present, does it contain unsafe, uh, or illegal content? Let's quickly think through another example here. Here's one that many folks I know are working on today, a shopping agent for an e-commerce site. As you're building, as this e-commerce company is building the site, they have to consider, can the agent provide specific recommendations to individuals that are meaningful for that individual but also equitable across the different demographic users of that site? Is it susceptible to accumulating unauthorized charges? Or exceeding budget, does it appropriately share any PII information across the payment rails for, um, and is it vulnerable to manipulation attacks designed to do things like, uh, like, uh, trigger unauthorized refunds at scale? Each of these questions across these two use cases represent inherent technical properties of AI systems, whether the builders of those AI systems address them intentionally or not. And of course, there are consequences of not addressing them intentionally. The uh OECD, which is the Organization of Economic and uh Cooperative Development, Has an AI monitor that they use to track incidents and, and hazards um over a number of years. Uh, uh, incident is defined as uh a harm harmful event, an AI, uh, event that has harmful impact upon individuals and a hazard is one in which potentially could have caused harm to individuals, um, but may have caused reputational damage. And as you can see in the chart that I'm sharing here behind me is that in October of 2025. The, um, it hit the highest point, um, with 509, which is a 95% increase over the same time from last year. And of course this rise during this time period corresponds to the expanded use of AI that we've seen over the past couple of years with generative AI. So at AWS we define responsible AI across these 8 categories that you see here. And when I say that there are earlier when I said that there are technical properties that are inherent to an AI system, these are the types of properties I'm talking about. And traditionally, folks who are trained in machine learning are, are trained in order to optimize the pre the the predictions towards accuracy against the loss function. Responsible AI is the art of maximizing the benefits of an AI system and minimizing the risks across these eight dimensions, including the trade-offs that are required between them. Uh, very quickly, uh, these dimensions, uh, these properties that, that we track, uh, here at AWS controllability is the property around having mechanisms to monitor and steer the behavior of the AI system. Privacy and security is around appropriately obtaining and using the underlying data and models that power that system. Safety is the property around preventing harmful misuse around things like scams. Fairness considers the impact of the outputs of the system upon different stakeholders and, and groups. Veracity and robustness are the properties around achieving correct outputs even with unexpected, uh, uh, or adversarial inputs. Explainability is around understanding and evaluating the outputs. Transparency is similar to explainability, but it's around, uh, guiding and the stakeholders and enabling to make informed choices about the use of that AI system and governance incorporates best practices for these technical properties of AI systems across, uh, uh, and I'll be talking more about that as I go. So, in my role as a principal product lead for Responsible AI I get to meet with many of you folks, um, and customers and they share with, with me sometimes some of the, the challenges around addressing responsible AI at scale in their organization. So one of them is, is that each of these technical properties requires a certain amount of uh of. Uh, expertise. There's a science to each of these properties to, to, um, addressing fairness. There's multiple ways that you can measure fairness, for example, for looking at, at robustness, there are multiple ways you could do that as well. So there's a uh an expertise that's required. The second kind of a challenge that I hear as I talk to customers is there are many tools that are provided in the open source community by AWS, by third party vendors, um, but kind of piecing these tooling together into an, uh, into a holistic solution that you could, you could give to your builder teams across the AI life cycle, um, is much, is more challenging. In some organizations, investing in these responsible AI is seen as a bottleneck to innovation. We're gonna talk more about that. And then I'm on the uh responsible AI team here at AWS and I've seen this personally, um, but many of the responsible AI experts within organizations feel overwhelmed as the number of AI use cases they need to support has increased so much over the last couple of years with those teams staying constant. One healthcare company I work with, I remember, told me that they have a backlog for the RAI expert, responsible AI expert team of over 1000 use cases, um, to review. And then of course, um, responsible AI involves compliance, um, depending on the, the jurisdiction that you're operating in within Europe, uh, within Colorado, within California, as well as management standards like NS and ISO 42001 and builder teams have to deal with, uh, outputting evidence in, in the right format to comply with those regulations. So how do we address these challenges? OK. So, the first thing I wanna say and maybe this is, is going to be controversial, maybe not, um, but that um across all of the RAI technical properties or dimensions that we just discussed, um, when you look into mitigating risks, there are 3 overarching strategies you could use, um, to address any of the risks and I'm calling them here baking, filtering, and guiding. So what are they? So baking involves building into the AI system itself the desired behavior that you want. If you, if you're looking to mitigate risks of unwanted bias, you may, may do this by building out data sets with distributions of demographic groupings according to how the system will be used or if you're looking to mitigate risks of hallucinations, going back to that use case we were discussing, you may have a rag pipeline that would ground the answers um in, in data. Uh, filtering is around blocking, um, blocking both inputs into, uh, the AI system as well as outputs that come out of it. Again, going back to our use case with the condominium description, we may want to put in guard rails that would filter or block out the PII or personally identifiable information from, uh, um, from entering into the AI system or the output and as that system integrates with APIs or uh return. To the user you may be filtering out so that's filtering and then guiding is around steering your users, uh, the users of the system which could be human users or they could be, uh, agents or other other systems, um, around the proper use of that system, the intended use being transparent about the limitations of the system through things like data cards, model cards, AI system cards. So those are the three overarching techniques that we use to address responsible AI. Maybe by a show of hands here, how many folks are familiar with the three lines of defense model that's used in regulated industries to address risks? OK, I see a good number. That's good. Um, what I've done here in this slide is applied that model which I'll, I'll describe, um, to AI risk management. So if you start all the way over on the left, that's your first line. Those are your builder teams and they're responsible for building the actual safeguards and controls into the AI system itself. Your second line is that group that I talked about, the group of, uh, uh, of AI experts that support the, the first line. There'll be multiple second line teams and organizations. You may have a cloud security second line team. You may have, uh, compliant, uh, more, uh, money laundering or other forms of risk management that you're working with that a second line team will work closely and guide that first line team and help them and set up, uh, practices. And then all the way on the right you have the third line that's ultimately your um internal audit uh and your independent assurance roles and that's um. And these teams uh interface with the external auditors on the right that you see there, as well as with different agencies that, uh, and consulting agencies, um, and they provide the internal, the, uh, uh, security, the overall assurance, um, to the organization. Now, when organizations talk about sort of responsible AI being a bottleneck, often what I see is that responsible AI is being addressed in that handover between the 1st and 2nd line to the third line, and if you're addressing your responsible AI there, it's too late because now you have to rear. Architect and you have to start over again and just as in security we solve this by the notion of secure by design is a concept many folks are familiar with there's, uh, we use a concept called res responsible by design and it's um taking the policies ultimately that are set in by the third line. That relate to the various management standards and the regulations translate them into a set of best practices by that second line and push those best practices now to your builder team and that eliminates a lot of the bottlenecks and can accelerate your development practices. So, um, that's what we've done here at AWS. We have defined what we call our responsible AI best practice framework, which is represented here on the slide. Um, and this is, this is the framework that's used by our builder teams as they build the AI services that you'll hear about over the next couple of days. Um, or that you're using currently. Um, and there's a, uh, a couple of benefits of doing it this way. Uh, the first benefit we just discussed, which is the more you could shift left your, your policies, um, into your builder team so that they're building correctly the first time, the more you could accelerate, uh, uh, your innovation. The second, and I, I should have made a slide on this, I think it really would have helped, um, but the, um, if you think about on the right, you have all of your, you have your management standards, you have your policies that your organization is setting, you have your, um, uh, uh, regulations that like we discussed, and then all the way on the left, you have all your tools, and there are new tools that come out every week in the open source community by vendors that address different properties of responsible AI. So those are two very dynamic layers. And what uh in the if you put within that in the middle of it a best practice framework like I like I'm uh positioning here, it gives stability and robustness. You can map and onboard new toolings, map it onto that best practice framework, and the outputs can be in a standardized format that can meet multiple regulations on the right size, the right side. So, what is, what are the best practice framework? And I'm gonna go through it at a very high level. You can see it here that it's um on the slide, you see that it spans across the AI uh NML life cycle design, develop, and operate. Um, the first set of best practices that we've defined for our builders teams, um, is, uh, around, uh, narrowly defining the use case, the intended use case, and this is a very common problem that I've seen with builder teams, which is if you define it, uh, very widely, very broadly, you're increasing your risk exposure significantly. Narrow the use case properly and you narrow the risks right up, right upfront. Then for that use case we've um defined a set of best practices to help them identify the inherent risks of that use case. So consider the actual stakeholders individually and then go through a framework like the slide I had with the 8 dimensions on it and consider how for each of the stakeholders what are the potential risks and then rank those risks, according to their impact, according to the likelihood or frequency with which they'll happen. Um, and then, uh, so that you could address them. So, setting up best practices around, around identifying risks for builder teams. And then the third is, is really a working backwards mentality. So, initially upfront, even during the design phase, um, establish release criteria, metric-based criteria for how you're going to measure the, the highest risks that you're, you're concerned about from the, from the previous exercise and then, uh, uh, define the metrics and the thresholds for those metrics. Um, as release criteria. Now this is represented intentionally as a circle because this will be iterative and of course as you learn more about your AI use case, you may want to adjust those as you start to work with the data, but it's important to set those goals up front as design principles and then work backwards from the metrics. So the next set of best practices that we've defined um are during the develop phase. So for each of the release criteria that you that the teams establish and for each of the risks you need a way to test it. And so even before we design the system, we designed the testing sets that will test the individual risks. So just make sure you have, you know, based on your understanding of the inputs and outputs, um, if you're testing for fairness, do you have the right demographic groupings within your data that are in a statistically valid way. So develop data sets and you could use one data set across multiple risks, but make sure you have data sets for testing all the risks. And then design and build the AI system and this is where the previous slide which we talked about baking, filtering, uh, and guiding. Or steering the users, uh, where, where that comes into play. Using those three overarching strategies, uh, design the system to, to address those, those risks and then actually test it, which is run your evaluation with your evaluation suites. And then you'll have at the end of that you may have risks that are still present, in which case that's why again this is a circle and iterative, or you could accept, accept those risks but document them and that's really they're on the next set of best practices which is our teams will build out. About, um, guidance tools which really are again data cards, model cards, AI system cards which we provide to, to users of AWS, um, which guide them and are as transparent as possible around the limitations of the AI system itself. Uh, and then when you get into the monitoring phase, then again, define a set of metrics that relate to your specific release criteria and make sure you're in an ongoing manner now monitoring the same set of, of, uh, of metrics. OK. Um, I don't know if folks saw this, but, uh, last, last week we, uh, actually published a version of the responsible AI framework that I just discussed that we use internally and, um, if this is inside of the well-architected tool. So previously within the well-architected tool there was a machine learning lens and there was a generative AI lens, and now there is a responsible AI lens that's based on the AWS, uh, responsible AI best practice framework that I just discussed. Um, and the, the focus areas here in this pie are, uh, the same, the same, uh, focus areas for the lens that you see there on the left, um, and that you use, um, for each of the focus areas, you could see there are, um, between 1 and 5 questions that, that you answer. Um, and then for each of the, and this is showing one of those questions, how to define the specific problem you're trying to solve, and then for each of the questions there's 1 to 5 set of best practices for fulfilling that. And then on the right side there is the, uh, a paragraph which, um, tells you about that best practice with a link which opens up a guidance paper, uh, which is I think very valuable, which will give you more information about the implementation steps for implementing that best practice. I should say also that this um this framework is also available on GitHub. We published it there too. Um, and then the well architected tool, once you complete it and you answer all the questions, um, at the end of it. You get an assessment, so you'll get a list of high, medium, and low risks, um, to consider, and you'll get an improvement plan which is shown there on the bottom, which will be specific to your, how you've answered the questions. So I encourage folks to see that that's our, our first, uh, publication of the uh AWS responsible AI framework and we'll be updating that um over time. OK, so, um, the last point I want to make before handing it over to Louis. Um Is that um once you've defined your best practices um in, in a framework, it's not just your human builders that benefit from this process, but um, But agents and as you go as we all are, are investing more in agent-driven development and specification-driven development, agents can read natural language and can read your, your, your best practices just as your builders can. Um, Gardner talks about guardian, uh, agents and using them, uh, using them for, for trust and safety. Um, this is that concept, um, defining your responsibility practices as a set, as a set of best practices for your agent. So let me show you a little bit about what I mean by that. Um, so, this is Quiro, folks familiar with Quiro? Good nodding heads on this one. Good. Um, so Kiro, for anybody who hasn't, uh, tried it or unfamiliar with it, it's an integrated, uh, development environment, um, and it has within it an agent, and one of the novelties of Quiro is that it incorporates the idea of specification-driven development, um, and, uh, I should say it does not come with the AWS responsible AI framework in Quiro. I've added it here. Um, it's not hard to do. I just, um. Uh, did it, um, and I'll explain how, how I use the Quiro constructs to do that. So, um, Quiro distinguishes between two types of specification files. One is, um, is, uh, steering files, and those are more of your organizational policies and best practices. So the responsible AI best practice framework fits, fits, uh, very neatly into that concept there, and that's teaching the agent, um, about how, how to guide the builders. And then they'll create, once a builder starts a, a project, they describe the project to, to Quiro in natural language and then Quiro will output uh use case specification files which are at the top. So those are, this is the same condominium description use case we discussed earlier. Quiro has read the best practice framework and starts creating specification files to address the different parts of it. Um, which your, uh, builders now should review before Quiro starts creating code, but then ultimately they'll be creating code according to, to, to the best practices. OK, Lewis. Thanks. Hi everybody, uh, my name is Louis Baker. Uh, I am a data scientist and a manager and the head of responsible AI at Indeed. Uh, so Mike and I have been talking for quite some time, uh, just as mutual information sharing, uh, product usage, uh, about responsible AI, and he, he asked me to come up here and tell you a little bit about, uh, a practical application of responsible AI at Indeed. So, Uh, for those of you who are unfamiliar with Indeed, um, you might know us as the job website. Um, what you might not know immediately is the sheer scale that Indeed operates. Uh, Indeed has 635 million job seeker profiles, um, that is resume data from individuals looking for jobs, and we have 3.3 million employers who are trying to sort through all of those people looking for jobs. Um, a lot of the time when I talk to people, um, within the industry, but more often at social gatherings and I tell them that I do responsible AI Indeed, they're like, oh, I didn't realize that Indeed used AI, uh, and. The reality is that AI is the only way that you can sift through that. um, Indeed is, is not a job board. Indeed is a search engine, um, so it starts all the way at the top, you know, with the jobs having all the jobs in one place. You need more data about those jobs. You need to understand what it is that people actively need in order to do that work. Um, you need to advertise those jobs to get more traffic to the website. Once you have more job seekers, you need to understand what exactly they're looking for, um, what skills they have, what skills they don't have, what skills they list. Does anybody put Microsoft Word on their resume anymore? That's a question you need to figure out. Uh, and as you identify the jobs that people are applying to, the skills required, you're able to identify what leads to success that gets you more employers and the cycle continues. Um, I go through all of this to tell you that like Indeed is an AI company and AI is deeply embedded in every single thing that we do. So let me give you a specific example that we can talk through uh about how to do this responsibly um, so this is the Indeed Career Scout and Career Scout is uh a fairly straightforward chatbot experience um you come in, uh, you have a, a list of recommended options that you can go through or you can just have an open-ended conversation about your career. Uh, some of the highlighted tools are to build up your resume, to just do a job search, or to search for jobs outside your current field. And if you go down that first, um, you know, you can have an open-ended conversation about, you know, uh, I've been working as a receptionist for quite some time. I've been interested in the creative field, I'd like to try that out, and Career scout will guide you through what skills you have, what skills you need, what sort of options might be available for you to make a lateral career move, and afterwards it'll give you a series of jobs that, you know, you can say, yeah, is, is this what you're looking for? and eventually. You can apply to those jobs. So it's conceptually, it's very simple. You're just having a conversation, you're just trying to find a job, and, uh, with any luck, we'll help you get one. So there's challenges with this. Um, every simple system has a million ways to be used incorrectly, simply. Um, if you're at this talk, you probably have heard of things going wrong before, so I'll, I'll speed up through it. But for example, um, less than 2 years ago, a, uh, Google AI overview, uh, if, if you asked it how many rocks a person should eat, it recommended about 1 to get your, you know, daily nutritional value. Um, that's not great. It's not super harmful, but, you know, you get the point. Um, later than that, uh, if you asked it about smoking while pregnant, it would say, you know, 2 to 3 cigarettes a day is great. That's Target. It's not for the record, um, but, uh, you know, again, not, not good. Now, I've purposefully given you some fairly mundane ones. You might be thinking like, oh, what's the big deal? And. Uh, let me tell you, I've seen some truly terrible things, awful things, um, and that I, I, I won't share with you here, um, but if you know, the, the safety of, of other people or, uh, you know, concerns of people who should be informed not to follow the advice to eat rocks don't concern you, then, you know, perhaps it might concern your bottom line, uh, when in this, this very real circumstance and an early, to be fair, very early version of chat GPT. When embedded as a chatbot into a car sales dealership, um, not only sold someone a Chevy Tahoe for $1 it said that that was a legally binding statement, um, and I guarantee you that harm for the company is real and is a thing that can come out of agents. So, uh, the question here is how do you keep AI on the rails, and Mike walked you through like a high level framework, uh, and some specific tools within Quiro to do spec driven development, and I'd like to go into specifically how Indeed has done this. Um, going back to the AI dimensions that Mike talked about, um, there's, there's a whole landscape of things to worry about out there, um, for Indeed specifically, uh, we have a very robust security team, privacy teams, governance teams, uh, so for responsible AI, I'm gonna concentrate on these four things, not to say I don't care about the other things, but we have to get out of this room eventually. Um, so we're gonna focus on safety, fairness, transparency, and veracity. So how do we make sure that things are truthful, fair, they make sense, and they are safe for use. Our general strategy is to follow this flywheel, um, to anticipate what could possibly go wrong beforehand and use this to inform things at the design stage. This is what Mike was talking about when it comes to spec driven development. You need to center having something safe as a design requirement and you need to have tangible metrics in order to get there. Uh, once something's out in the world, you cannot just trust that things are going to go the way your developers thought they would, uh, and you need to put in guardrails in place, and I'll talk through a little bit about what some real-time guardrails are here, but generally any sort of moderation system can be viewed as a guardrail. Uh, and then lastly you need to observe I am probably speaking to the choir, but if you don't log it, it didn't exist. Um, you need to know exactly what happened. You need to be able to learn from that experience so that you can anticipate future problems and just in general make your product better. Um, so, uh, this entire thing, uh, as a, as an umbrella term is known as AI alignment, um, and the idea of AI alignment is that you want your open-ended AI system to be aligned with your values. Kinda makes sense up front, uh, when you dig into that concept a little bit more, the question then becomes what are my values? Uh, so I, I know, right, that like my job at Indeed is to help people get jobs, right? I want whatever I do to help with that and I know in general I don't want people to have a bad experience, um. But then there's all the stuff in the middle, right? Like I obviously don't want, uh, my chatbot to be making threats of violence. I don't want it to be using hate speech. Um, what kind of tone do I want my chatbot to have? Do I need it to be very, very rigid and stuffy? Do I want it to make jokes? Do I want it to stay specifically on track with helping people to find a job through the job search? Do I want it to be able to do other things like if somebody asks me to do. Their math homework, am I going to let the chatbot do that? These are all things that need to be settled way, way, way before you hand this problem to your responsible AI team. This is a problem of human alignment. Uh, and the way that we handled this at Indeed is we got all my lovely peers from a whole bunch of different departments in a room for like 3 hours and we hashed out exactly what it was from each of our perspectives that a good product looked like, um, so I can speak of course for responsible AI, but you know, trust and safety, security, they have a very specific posture that they want the, uh, their agent to also uphold. We put all these things down into an AI constitution and It is exactly like it sounds. It is a big old document where people just said thou shalt and thou shalt not. We want to help people get jobs. We do not want to pose any sort of privacy, uh, event. We don't wanna have any sort of security threats. Um, we do want the experience to be relatively seamless. We do want to keep people, uh, on the website to do their searches, etc. etc. Um, and from this, this is where you get your spec. This is where every time a new product is developed, people will reference the AI constitution and they are able to align it by design. They are able to say confidently what sort of tone their chatbot should have, what sort of data it should have access to, what sort of data should be deleted after a session is over. We then further go into this by saying, great, now that you've built it to this, let's build guardrails that check to make sure that it works. Um, guardrails are as simple as having a parallel prompt or maybe even something in the system prompt that just says make sure that whatever is said follows X, Y, and Z standards. And then at the very end you observe, you make sure that X, Y, and Z standards are followed, um, you make sure that you label anything that got flagged by your guardrails so you can monitor it so you can see, great, we flagged this as a harm, let's look into it, let's see what kind of harm. You look into it, you see what kinds of events have happened, you look into it and you identify things that you missed. This is how you go from big human alignment to AI alignment. I guarantee you it's harder than one slide would state, um, so let's look specifically at how we did alignment for career scout. So again, career scout, opening conversation what you want your career to be. Well, to anticipate the issues that might happen. We sat down and we created a series of AI red teaming events with specific rubrics that would calibrate what was and was not acceptable. And so before anything got out the door, we created an adversarial AI agent. An attacker LLM and we loaded it with a prompt. Uh, that prompt could be, uh, try to discriminate, uh, perhaps try to uh get somebody's Social Security number, perhaps try to engage someone with a scam. The adversarial LLM then goes, it talks to our chatbot over and over again, and every single iteration we have a series of rubrics to try to identify, all right, did a scam successfully get past our guardrails or not? Um, we rinse and repeat this. We do this several, several 1000 times with several different agent personas to see how robust our guardrails actually are. And now when things go into production, we have anticipated a great range of potential harms that we can guard rail against. Um, the most basic of this is standard content moderation, even that's very complicated, but in general, uh one of our values is we don't want our chatbots to swear at people, so we have filters to make sure that there's no swearing on our platform. Um, things get more complicated, things get more contextual, so we also have contextual guardrails. These are secondary system prompts that evaluate whether something is going along our terms and conditions. These are the sort of things that allow you to detect whether something is a harm or not. So if you ask the agent to tell somebody to kick rocks, that's not a very nice thing to say. You can use your imagination to escalate the sort of language, please. Um, we have a guardrail prompt that says do not allow the following types of speech. Also give it this flag. This one is flagged as a harm. It says do not facilitate hateful or harmful topics of conversation. An LLM as a judge has rated this above our threshold limit and has flagged this as harm. And so instead of generating a response, the AI now generates the response, I'm sorry, I can't tell people to kick rocks. Um, so that is functionally what a guardrail looks like. Then after you have your product, you've tried to anticipate how it could go wrong, you build guardrails to keep it from going wrong, you then observe to see what actually happened. Um, a major part of this is just logging events. You don't necessarily have to log them forever, but you do need to log them to know what happens, um. Anomaly detection is a big thing. Is there any event that's causing your moderation system to get tripped more regularly than not? It could be an adversarial attack. It could be that something's trending on TikTok. It could be that something's broken in your system. Either way, you need to be able to detect it. Um, and then there's something that we lovingly refer to as our unknown unknown analysis, which is that you take a very large sample of events that were not flagged. And you press it through a bunch of experimental things to see if there's anything you might have missed. You perform cluster analysis to see if there's any conversations that are on the edge. You know, it could be that 99. floating degrees are people trying to search for a job, and there's always, you know, a fraction of people out there trying to figure out how to, uh, build a car from scratch, and you want to figure out what's going on there and how it got past your guardrails. I say all of this because I really want to drive home, uh, a point that Mike made upfront. Um, every single AI system has a responsible AI posture. If you are driving responsible AI from a policy perspective, you are too late. Responsible AI begins before a single line of code is written, before a single wire frame is drawn up. Responsible AI is an infrastructure investment. Responsible AI at Indeed is part of our AI infrastructure organization. We are a necessary part of R&D. To have that give you some demonstrated things, we currently have 17 active guard rails. Um, we have them propped up across every single AI interface at this company. And we are currently processing 10.6 million AI responses a month. Um, if we were a policy team, we would not be able to do all that by hand. Um, we can't just go to every single team, ask them, pretty please follow the rules. I wrote it down, thank you. What we've done is we've created a series of tools, we have created a series of checks, and people cannot, cannot release their model into production until they have passed those checks. The takeaways that I want to give to you all right now. Is that first and foremost. Principal part of responsible AI is knowing what your values are, getting alignment behind them. You need to know what your company does and does not want. You need to know what is and is not possible, and you need to tell your developers that because they're going to assume if you don't. You need to embed responsibility at every single stage of development. Every single system has an RAI posture, whether you like it or not. There's a whole mess of ways that something can be used. There's a whole mess of regulations that you might not even know about. You need to anticipate those. You need to build up guardrails for them, and then you need to monitor what actually happens. At the very end, find a path towards platformization. Mike gave you several wonderful options earlier. You can build this stuff into your development. You can build checks in. You can do things responsibly so your developers don't have to. Um, so that's, that's everything. I'll, uh, I'll have Mike come back up to close this off. just Oh OK, great. Thank you. So just some, uh, some links if you want to scan the QR codes there. If you want to learn more about, um, responsible AI at um AWS or Indeed, those are the first top two boxes that'll take you to the to the websites. Um, the responsible AI lens that I showed, which is available in the well architected tool, um, that's the QR code that'll get you there and then of course Qiro if you're interested in using that, that's the link there and then a few more. Um, if you're learning, uh, interested in learning more about, uh, um, AI at, at AWS, um, here are some, uh, courses that, uh, uh, to consider. And your learning journey.