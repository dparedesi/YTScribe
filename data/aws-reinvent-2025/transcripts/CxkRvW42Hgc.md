---
video_id: CxkRvW42Hgc
video_url: https://www.youtube.com/watch?v=CxkRvW42Hgc
is_generated: False
is_translatable: True
summary: This comprehensive session explores how AWS Hybrid and Edge services enable organizations to maintain digital sovereignty while harnessing cloud innovation, presented by Ben Lavasani, hybrid cloud specialist, and Mallory, addressing the critical reality that 82 percent of companies are considering sovereign solutions in response to increasingly stringent regulatory requirements from GDPR to industry-specific regulations across healthcare, telecom, and financial services sectors. Digital sovereignty encompasses two fundamental pillars: data sovereignty addressing data residency with geographic control over storage and processing locations, compliance with local data protection laws, operator access restrictions through granular control mechanisms, role-based access policies, audit trails, and encryption; and operational sovereignty ensuring systems resilience against disruptions and disasters, redundancy of critical systems, disaster recovery solutions eliminating single points of failure, independence and transparency providing clear visibility into technology stacks and dependencies, and the ability to maintain and modify systems independently. AWS addresses these sovereignty challenges through a continuum of services delivering consistent infrastructure, APIs, and tools across AWS regions, local zones deployed in large metro areas like Las Vegas extending regional capabilities, dedicated local zones for exclusive customer use in customer-specified data centers operated by locally-based AWS personnel with multi-tenancy features, and AWS Outposts extending cloud capabilities on-premises in customer data centers or co-location facilities, all built on the foundational Nitro system since 2018 that reimagines cloud infrastructure delivery through custom silicon offloading virtualization functions to dedicated hardware achieving near bare-metal performance while maintaining unprecedented security standards. The Nitro system ensures every communication channel is encrypted, employs secure boot processes cryptographically validating security keys with hardware, enables continuous runtime security patching without workload disruption, and crucially provides no remote operator access from AWS or any external party as validated by independent cybersecurity firm NCC, while the Nitro Security Key serves as the root of trust for at-rest data enabling cryptographic shredding compliant with NIST media sanitization standard 800-88 when removed during maintenance operations. The session highlights Judea, a Saudi Arabian payment solutions provider holding 75 percent market share of point-of-sale business while serving over 150,000 merchants and providing more than 700,000 payment terminals across Saudi Arabia, Egypt, and UAE, navigating strict financial regulations from the Saudi Arabian Monetary Authority requiring data to remain in-country by deploying a common stack across UAE regions and outposts in Saudi Arabia and Egypt, demonstrating practical sovereignty implementation while migrating to cloud-native architectures. AWS's digital sovereignty pledge commits to four principles: controlling data location through regions, availability zones, local zones, and outposts; providing verifiable control over data access through Nitro system hardware and software eliminating administrator access and human error potential; encrypting everything everywhere at rest, in transit, and in memory across all AWS services; and ensuring cloud resilience through multiple availability zones and cross-region partitioning capabilities. Practical implementation guidance covers data perimeter establishment using AWS Control Tower's centralized governance with over 1,000 pre-built controls applying preventative, detective, and proactive policies through service control policies, resource control policies, and VPC endpoint policies, while services like S3 on Outposts maintain separate IAM namespaces, default encryption, VPC-only accessibility, and always-enabled blocked public access, with comprehensive monitoring through CloudWatch metrics, CloudTrail audit logging, EventBridge failure notifications, and support for advanced capabilities including local replication for disaster recovery, Amazon EMR for big data analytics processing, Mountpoint for S3 file system access, and CloudFormation infrastructure-as-code deployment, all managed through a unified AWS Management Console providing consistent operational experience across cloud and on-premises environments with outpost ARN columns clearly identifying physical asset locations.
keywords: digital sovereignty, AWS Outposts, Nitro system, data residency, hybrid cloud
---

Hello and good afternoon everyone. Headsets on please so you can hear me. Can I get a thumbs up if you can hear me? Loud and clear, brilliant. Thank you all for joining and welcome to this session. I'm really excited to to see you all here today, um. And er imagine waking up tomorrow and discovering that your organisation's critical data is suddenly subject to laws that you never agreed to. This isn't hypothetical, it's a reality many organizations face today in our interconnected digital world. And that's why digital sovereignty has become one of the most crucial conversations in cloud computing. So much so that 82% of companies are considering sovereign solutions. They're responding to a fundamental truth that organizations need both the transformative power of the cloud. And the ability to maintain complete control over their data. But digital sovereignty is not just about where your data resides. It's about who has access to it, how it's processed and maintaining operational autonomy while still innovating at the pace of the cloud. From the European GDPR standard to industry specific regulations, the requirements are intensifying daily. So today, along with my friend Mallory, we will show you how AWS Hybrid and Edge services can help you maintain sovereignty without sacrificing innovation. We're going to explore practical solutions. That let you harness the full power of AWS whilst keeping absolute control over your data's location, access, and processing. My name is Ben Lavasani. I'm a hybrid cloud specialist with AWS. Let's get to it. So I'm gonna start off by talking a little bit more about digital sovereignty and some of the regulatory challenges. And then I'll introduce some hybrid and edge services to you. I'll be speaking on a customer's behalf today uh representing Judea, who are based in Saudi Arabia. And then I'm going to hand over to Mallory, who's going to go into another layer of detail about specific implementation best practices and controls and building at the edge. So digital sovereignty represents an an organization's ability to maintain complete control over their digital footprint, data operations, their technology stack. And then there are two concepts within the broader topic. First of all, data sovereignty, the pillar on the left. Within that we need to consider data residency, this is geographic control of where data is stored and processed. And compliance with local data protection and laws and regulations within that jurisdiction where you're deploying. We also need to think about operator access restrictions. So this would be things like having granular control of who has access to data and systems, as well as implementation of role-based access control policies and audit trails and encryption, and all of that needs to be considered in this pillar. On the right side, thinking about operational sovereignty. We need systems to be resilient and survive disruptions and disasters, redundancy of critical systems as well as um disaster recovery solutions to make sure that you're protected against any single points of failure in your deployment. And then finally, independence and transparency. Clear visibility into your technology stack and dependencies within it are really important to understand. And the ability to maintain and and uh modify systems independently is all very important. So at AWS um we've been working closely with customers to kind of understand more about some of the digital sovereignty challenges that they that they're seeing trying to be compliant in their markets. For compliance specifically, um, you know, the regulatory landscape is constantly evolving. There are new laws and requirements emerging globally. And organizations must navigate complex cross-border data transfer restrictions such as GDPR in Europe. On top of that, there are also many industry specific uh regulations that need to be considered. Whether you're in a regulated industry, telecom, healthcare, financial services, there's often a very complex landscape about about how you can be compliant with that specific regulation in that market. And that complete, that creates a lot of complexity. Secondly, security is another challenge we see uh with customers that have to balance implementing robust security and data access controls but still maintaining usability. It's all good building a very secure system, but if no one's able to utilize it and be productive with it, it's a it's potentially a waste of time. Um, also, kind of encryption standards are becoming increasingly more complex, and there's challenges around that as well. And audit trails and monitoring also fall into this, you know, how do you check, access, and, and monitor what's happening within your environments. Finally, we see operational challenges as well. Now this is maintaining a consistent, so it's it's can be very difficult to maintain a consistent uh management across IT environments. Whether you're on premises, whether you're running in the cloud, or you're somewhere in the at the at the near edge. It that can be often very challenging as well uh to do, uh especially if your workloads are highly distributed around the world in in different markets. And of course. You being um ensuring that you have business continuity while also preserving sovereignty. We see situations in some markets where all data must be stored locally. We see it sometimes when sometimes primary copy needs to be local, backup can be in a region outside of country. Or where there's no policy and it's much more freedom. And it's, it's trying to figure out some of that that landscape operationally can be very challenging for our customers. So at AWS we've been striving to build a continuum of services where we can bring the cloud to where you need it. The idea is that we offer the same infrastructure, services and APIs and tools so you can have that consistent developer experience, whether you're running in our regions or the way out to the far edge. It all starts with regions, and we will continue to innovate and build regions and also. New regions such as European sovereign Cloud that are uh have have been announced. But some customers need to get close, need their workloads uh to be running closer. So we're expanding with uh our local zones as well as Cloudfront to bring to edge locations and new metros around the world. And then we can bring AWS services on premises with services such as outposts, and then in the far edge IOT services and EKS and ECS anywhere. And the idea is that we wanna do that in a very consistent way so you can leverage the productivity of AWS whether you're, It running in the cloud or or at the edge. This approach has been commended by third parties, so I'm, I'm really pleased to share that Gartner's 2025 Magic Quadrant report, AWS has been recognized as a leader in distributed hybrid infrastructure across critical use cases for edge computing, as well as container deployments and AI and AI workloads. And this recognition from Gartner really validates our commitment to providing comprehensive hybrid infrastructure. That meets our customers' evolving needs. So let's take a look at some of the specific services. I've touched on a few of them, but I'm gonna dive into a few few for you today. AWS local zones, uh AWS deployed and operated infrastructure that we're building in large metro areas. For example, there's one right here in Las Vegas that's extending the Oregon region to the city. It's with local zones, you're able to access on-demand elastic services, pay as you go, and you can do this in the same way and the same developer experience that you're already used to using in the cloud today. So services like EC2 are available and if you're running workloads in a region on EC-2, the experience is exactly the same when you're deploying here in Las Vegas. It's integrated with the regional services through our backbone, so you can also run hybrid workloads as well where you might wanna push certain parts of your workload to one of the regions and we have many customers that are doing that. Here's a layout of what's happening with the local zones er launch. We started in the US er all of the locations on the map in purple are already generally available today. And then everything in white is announced and and planned. Um, we've also expanded into Europe, the Middle East and Africa, as well as Asia Pacific, Australia, New Zealand, and South America. Now we're really interested to hear back from you, any feedback. If there are any specific locations that are of interest from a data sovereignty point of view, a digital sovereignty point of view. We're really excited to learn about this and see whether we can help support you in those, in those locations. Dedicated local zones are another service I touched on. These are for organizations with more stringent sovereignty requirements. We are able to offer a dedicated local zone or a DLZ. They are built for the exclusive use of a customer or a community of customers. And they're placed in custom specified data centers, sorry, customer specified data centers. So that could be your own location or an AWS data center that we have local to you. Um, and it's dedicated for your, for that customer's specific use. It can even be operated by AWS personnel, personnel that are located in that specific sovereign territory. We have multi-tenancy features er built into dedicated local zones, so some customers might, for example, a local government might have multiple um agencies or entities within it and they want to share the same common dedicated infrastructure but have a level of multi-tenancy and and segmentation between them. This type of thing is possible in a DLZ environment. Some of the benefits, well, with, with dedicated local zones we can isolate sensitive workload on dedicated cloud infrastructure. Um, it's possible to build fault tolerance and have multiple locations. And ultimately reduce operational complexity and increase the speed of your innovation. We take care of managing this infrastructure on your behalf and you can deploy sovereign workloads locally. Now the third service I want to touch on, AWS outposts are an extension of AWS cloud on premises. So in this situation, we're actually able to run this in your data center or a local co-location environment where again we're able to extend our cloud into the, into the uh into the on-prem. With that consistent experience, and you have a single pane of management, whether you're operating in a region, local zone, dedicated local zone, or outpost. Um, with outpost you effectively see a pool of capacity in a region that's dedicated to you with a, with a different identifier. Within the family of outposts, we have uh both racks as well as outpost servers. The outpost rack is a 42U data center rack, um, again bringing those same services and APIs on prem. Um, we have a number of core services, Mallory will dive into them a bit more later on, but. EC2, EKS, EBS, S3, RDS, these are all available locally in the outpost. And you might see one rack there, but don't think of it as being limited to just one rack. We can have logical outposts that across multiple racks. And we handle the intra and inter-rack networking on your behalf. You just see this again as a pool of capacity in the console where you can push your workloads. Really makes it very simple and abstract for you. For smaller deployments, um, we can do uh outpost servers, both 2U and 1U. And again these can extend a subset of AWS Cloud into smaller locations. Uh, and we run both uh Intel-backed instances on these and as well as Graviton in the, in the, in the 1U. So I wanna spend a few minutes uh talking about the nitrous system cos I think in in my section of the presentation we're talking a lot about the the the underlying infrastructure. And this is really a foundational element to how we build in the cloud. It's been available in all modern EC2 instances since 2018. And it underpins everything we're doing in our hybrid edge. So everything I've talked about from local zones, dedicated local zones and outposts is all built foundationally on top of nitro. The Nitro system reimagines how we deliver cloud infrastructure. We can offer unprecedented benefits across security, performance, and innovation. At the bottom of the stack, we have the Nitro hardware layer, which basically offloads the virtualization functions that you would normally run on a software hypervisor into dedicated hardware. These are custom chips produced by AWS custom silicon, er sorry, built, designed by AWS and uh the system contains security chips er that offers continuous verification of hardware integrity and the specific nitro cards for networking, storage and security functions. Above this sits the Nitro Hypervisor software layer. This is a very lightweight virtualization layer that will deliver near bare metal performance. The the many customers will see discernible performance from running in bare metal with all of the benefits of virtualization, and the reason for this is because we offload the uh core like hypervisor uh functions to the nitro hardware layer. And then above that we have obviously application layer, sorry, the incidence OS layer, application layer where you'll be deploying. The Nitro system is really a prime example of how we continue to innovate at the infrastructure level to deliver better outcomes for our customers. It's able to offer the highest security standards in the industry, and as I said, performance that's discernible from running in bare metal. The other thing it brings is we can very quickly iterate on new instance families with Nitro. You, you will see this week, there's gonna be more and more incidents launched and they're all built on top of Nitro with the latest generation hardware from AWS as well as our, our partners. Looking at more detail within security, well, first of all, encryption is really foundational to the Nitro system. Every single communication channel within Nitro is encrypted. This means data flowing between the components is all protected from potential threats. Second, there's a secure boot process in Nitro that basically. Um, during the boot sequence, it will cryptographically validate all of the security keys of the entire system with the hardware. And then third, we can continuously patch the nitro security at runtime. So we don't disrupt any of your workloads while we're maintaining our EC2 fleet on nitro, whether it be on-prem in your outpost or or another location. So you don't get any downtime when we're maintaining our nitro security layer. And the final point is we have no remote access to this. Um, it's crucially important that this is, this, this is the case. And it's an underlying design principle of how Nitro has been built, is we cannot see inside your environment. There is no operator access from AWS or anyone else outside of your organization to your environment that runs in the cloud. And security really is our number one priority. And these 4 pillars of nitrous security demonstrate our commitment to providing the most secure cloud infrastructure for our customers. I don't just need to say this. Third parties have validated the Nitro system, such as the global cybersecurity consulting firm NCC have published an independent report on the security design of Nitro and have confirmed that there is no operator access to nitro as a matter of design. I said I was talking about hardware, let me show you the inside of what's happening on some of these. So this is uh one of the uh EC2 servers that would be potentially running in an outpost. Um, one of the things that was really interesting about how we do operations in the field for these on-prem environments, we have a concept called the Nitro Security key or NSK. It's in the top right corner up there. I talked about everything in nitro being encrypted in transit. We also encrypt at rest. The nitrous security key is the root of trust for anything that's at rest on the outpost. If you pull the security key, the outpost is cryptographically shredded. You can't access anything that's on the outpost. So in situations when customers uh maybe we're doing maintenance and we're upgrading or replacing components, we can hand the customer the security key. If they break that key Everything is sanitized on the nitro server, we can take that away with us, we leave you with the key. And this is compliant with the NIST media sanitization standard 800-88. So it's, it's, you know, the, the, the, the, the thing that historically you would have to shred this hardware, physically shred it, we can do cryptographically, which is really neat operationally. So what I've learned, um, I've been working on hybrid with AWS now for over 5 years, and what I've learned engaging with customers is they want that same experience, it's really important, um. And our goal, and we continue to, you know, offer this, is to bring you the same reliable infrastructure, same performance, same operational consistency, APIs and tools, and ultimately let you leverage the same pace of innovation of the cloud, whether you're running in our regions all the way out to the edge. Speaking of innovation, I just wanna touch on um a customer story. uh and this is a customer I've worked very closely with, so I'm I'm actually based in the UK and I'm covering the Emir region. Um and Judea is based in Saudi Arabia. Unfortunately they were not able to make it, so I'm, I'm going to present on their behalf as well as I can. I really hope that uh at coming Reinvent we can get them on stage. Um, they really are an incredible team and have been a great partner for us in the region. Judea are offering payment solutions in the cloud, built on AWS. They have 75% market share of the POS business in Saudi Arabia, and they're also expanding into Egypt and the UAE, the United Arab Emirates. They say they serve over 150,000 merchants and also provide more than 700,000 payment terminals. Now, um, one of the Judea's goal is to their mission, sorry, is to basically empower businesses of any size with the tools to start, manage and grow their business. Um, so that could be a, a start-up restaurant that just wants a, a simple booking system or EPOS system. They're able to deliver this. And one of the challenges we have in the, in the region is the, the financial regulations are particularly strict. So in Saudi Arabia, for example, there's a. The, it's called SAMA, the uh the Saudi Arabian Monetary Authority, or everyone just calls it Central Bank. Um, they're strict about what data can leave Saudi Arabia, and Judea had to innovate in order to launch it ahead. Now we have announced a region there, but they wanted to move quickly. And we've been supporting them with building a common stack across, um, across the different environments that they're, they're building in. So this is a mix of. The UAE region as well as outposts in Saudi and in Egypt where they are able to build and deploy a common stack, and they're in a phased approach where right now they've moved some of the Uh back end of the systems into the into the region and then they're now starting to scale and then move to more cloud native architectures across that whole fleet. So it's been a great partner for us in the region and uh really thankful for the team and hopefully you'll get to see them at an upcoming reinvent. So, I'm gonna hand over to Mallory at this stage, but thanks for your attention, I'll be around at the end and look forward to to meeting you all. Thanks Ben let's do one more check if everyone can hear me OK great thank you. It's great to see how Judea is innovating with AWS outposts. So far today we've learned how AWS is designed to help meet customers at the edge. Next we'll dive into practical ways you can use these tools and services to help meet your security objectives. OK, so what are the security objectives? Let me briefly recap those. We hear how customers in regulated industries. Need complete control over their networking and data encryption requirements. Ultimately they want to store data in a specific sovereign location. Oftentimes that's in a location without an AWS region nearby. At AWS we always work backwards from customers and that's exactly what we've done with the AWS digital sovereignty pledge. This pledge represents our commitment to offering customers the most advanced set of different digital sovereign controls in the cloud. We fully believe you don't need to make the choice between having the full power of AWS or a sovereign limited solution. With the pledge we're committed to innovating with new tools and services to help protect customers' data. This is a 4 part pledge, and it starts with giving you the ability to control the location of your data, where at AWS you've always been able to do this. You have the choice of running your applications in a specific region, in a single availability zone, in metro areas with local zones, or on premise with AWS outposts. Second is giving you verifiable control over data access. We designed and delivered the first of its kind, the AWS Nitro system that Ben spoke about earlier as the foundation for our computing service. It's designed with specialized hardware and software to enforce. Data standards As Ben mentioned, there's no administrator access, so this removes the ability for human error and tampering. And third is giving you the ability to encrypt everything everywhere whether you want to encrypt. At rest, in transit, or in memory, we offer tools and services to do that. All AWBS services already support encryption today. And finally, our commitment to the resilience of the cloud. Digital sovereignty simply cannot be met without having resilience. Customers need control over their networking, and that becomes table stakes during events like natural disasters and connection interruption. That's why all AWS regions are built with multiple availability zones that are fully isolated. Customers can petition their applications across multiple availability zones and across regions, and at the edge you can actually home your AWS outpost to different availability zones in the same parent region, or you can home them to different regions altogether. Building to meet your security objectives often starts with understanding what is your data perimeter. Now it wouldn't be a reinvent session unless I pulled the audience, so I'm curious, who knows what a data perimeter is? OK, all right, uh, a data perimeter is a set of permission guard rails in your ADbus environment to ensure that only your trusted identities access your trusted resources from your expected networks. These organizational wide guardrails serve as always on boundaries to help protect your data across a broad set of AWS accounts and resources. Now they don't replace your existing fine-grain access controls. Rather, they work to ensure. That all your IM users roles and resources meet your defined security standards. They also work alongside the AWS, well architected framework design principles to strengthen your overall security posture. Ultimately you implement a data perimeter through a combination of three policies that's your VPC endpoint policy and resource control policies, or RCPs, as well as service control policies, SCPs that I'll be speaking about today. So let's talk about a fully automated way that you can implement your data perimeter and your governance objectives at scale. And that's AWS control tower, a fully managed AWS service to govern and establish your multi AWS account environment. You can think of control tower like your command center to implement your security guard rails and governance at scale, whether that's in the cloud or on-prem in your own environment. Control tower establishes secure landing zones. That set up the identity governance and logging of all your accounts based on AWS best practices. It enables governance using pre-built controls that you select from a control catalog. Let's explore what's inside that catalog. The control catalog is a centralized inventory of over 1000 AWS authored controls based on common use cases across availability, cost. Security and other application operations, these controls are preset governance rules written in plain text that you can apply either enterprise-wide or to select accounts within your groups. Each control is also written in plain text and enforces a policy that is either preventative, detective, or proactive, and you can make these either mandatory or optional. Let's dive deeper on that concept. So preventative controls block noncompliant actions before they happen using IM service and resource control policies. So for example, say that you want to prevent cloud trail logs from being turned off. When a user attempts to turn off cloud trail logs, it's automatically blocked by the IM. SCP policy This is a good choice when enabling logging is table stakes and a non-negotiable compliance requirement that you can't risk turning off for even a moment. Detective controls identify actions after they have occurred that are noncompliant using AWS config rules. This is a good choice when you need visibility into the compliance status, but you want to offer more flexibility into how that resource gets created. In this case, if you wanted to say detect whether or not all your EC2 capacity reservations had tags on them, ADBS config rules would identify reservations without tags, flag them, and notify you and alert you. And finally, proactive controls validate compliance during resource deployment time using AWS cloud formation hooks. This can help shift compliance left in your CIDC pipelines, so this helps you identify them before detective, but it's less restrictive than a preventative control. In this case, if you wanted to proactively require that tags are applied during EC-2 capacity reservations, if you have a developer that's deploying a cloud formation. Template, it would fail if it was missing a tag during deployment time. OK, so let's take a look what that would look like on an outpost rack. On the left hand side you have your parent region that your outpost rack is home to, and your landing zone is deployed across both any assets you have in the region as well as your on on-premise data center. If you have a client that attempts to turn off a service control policy, it is automatically blocked by control tower. Let's take a look at what that control is doing. Here we see a governance that's been selected from the control catalog. This preventative control ensures the cloud trail logs cannot be altered. What's powerful and easy about control tower is how it's written in plain text. So this means every user can understand exactly what's being enforced without having to read complicated syntax or know the underlying structure of the policy. However, we always believe in the power of choice, so the policy is also available from the artifacts section of control tower. Let's take a look at that. And this is the SCP policy that's been applied to your organizational unit across your landing zone both on prem and in the cloud. OK, so we've talked about how control tower is a fully managed approach to governance, and you get to select from an inventory of over 1000. However, many customers also want to implement their own custom rules. Let's take a look at an example of a custom SEP for another specific governance objective. Let's go back to that outpost and let's say that it's located in country one and it's home to an AWS parent region that's in a different physical location in a different country and you want to prevent copies from the outpost back to the parent region. Here is a snippet of the service control policy to deny that regional copy. I have a QR code in the upper right hand corner that links the full blog post and full SEP policy on this. We see in the first part of the statement. That there's a variety of denies based on each service name. In the beginning we see how Images, volumes, and snapshots are denied, copied back to the region. As the policy goes on, it repeats this pattern for every service name. Now let's go ahead and let's zoom in on the S3 section. Once we zoom in, we see that all S3 puts are denied. So you might be thinking, well, I actually don't wanna deny all my S3 puts since my local puts to Esther and Alpost, those are OK. I only wanna deny the regional ones. How would I do that? And that's exactly why we designed S3 on Outpost to have a separate IM name space from regional S3. So let's talk about the benefits of that. By having separate IM name spaces, it creates a clear service boundary between what's your cloud-based S3 and what's your on-prem S3. So when you see S3 on outpost, you instantly know, OK, this is my permissions for S3 on outpost and vice versa. So this gives you the benefit of writing modular policies the same way, but those are least privileged, and you don't have to worry about managing at a complex lower level relationship like splitting out your bucket names. EC2 also has a similar design pattern to this with a separate action name for EC2 outpost arm. OK, so now we've covered how you'll establish your guardrails and you're gonna create your data perimeter for your trusted identities. Next we're gonna shift gears and talk about how you'll build and run those applications. Earlier Ben talked about how customers want the same experience building at the Edge, so let's dive into how we're delivering on that promise. On outposts we have 14 services running locally across compute, storage and networking. Additionally, given outposts or home to an AWS parent region through an AWS service link, this adds flexibility so you can easily connect back to all AWS services in the parent region. Starting with compute, you can run your compute workloads locally using the same EC2 instances you know and love back in region, whether you need general purpose instance, compute intensive or memory intensive, they're available for you on your outpost rack so that you can process data right where it lives. You also have Amazon EBS which gives you the volumes that you need for persistent storage for those EC2 instances. You can use GP2 volumes on all outpost type, and you can use GP 3 volumes on the second generation instance rack of outpost that was launched earlier this year. Just like in regions you can attach and detach volumes to your instances and you can even copy your AMI, your Amazon machine images from the region to your outposts so you can launch local instances using the same configurations that you've perfected back in region. So this way you can build your AMI once and deploy it where you need it. This ensures consistency across your deployment. Now these EBS volumes work as both boot and data volumes, and you can increase the volume size on the fly with no performance impact. You can also take snapshots locally. And you can also use local snapshots or AMIs backed by local snapshots to quickly provision new volumes. So this means you can create, manage, and restore snapshots entirely locally without leaving the outpost, and, and security is built in by default, so all these volumes and snapshots are encrypted by default. OK, so let's talk about where we're storing these snapshots, and those are in S3 on outposts which is bringing. Local object storage to your on-premise environment with S3 on outpost using S3 APIs, we make it easy to store, retrieve, version, tag, and control access to the objects on your outpost. When we designed S3 on Outpost, we focused on delivering key security and management features that we knew would be important to our digital sovereignty customers on prem for controlling access to your data that starts with a separate IM name space that we, we just spoke about. And we also have encryption enabled by default so every object on your outpost is encrypted and you can choose from encryption types of SSE SC, and or SC. This gives you flexibility based on your security requirements. Second, all these objects on your outpost are only accessible through VPC-based endpoints, so this means access is contained to your network perimeter either accessible from your VPC or through your on-premise. Network endpoints but nowhere else. Third, blocked public access is always enabled by default and cannot be changed for audit and compliance. You have visibility into all S3 APIs using cloud trail at both the object and bucket level that we'll dive deeper into later, and we're also integrated with cloud formation, which I'll show in just a bit. OK, so let's talk about how we separate what's back in the parent region versus what stays locally to meet your data perimeter objectives. Back in the parent region we store all of your non-object data, so that's creating and managing your buckets, your bucket metadata, and all of your policies, so your IM policies, access point policies, and bucket policies. This also includes telemetry data like your cloud watch metrics and your cloud trail logs. Then locally we always keep the object data and object tag information. By default, an object will never leave an outpost unless you explicitly move it. Optionally you can choose to copy it down locally, copy it back to the region, or move it to another outpost yourself. OK, so let's talk about how we differentiate between how you route requests back to the parent region and how requests stay locally. We see a view here where there's a client on an instance on an outpost subnet in your VPC in your on-premise environment. When you want to create a new bucket in turquoise, that create bucket request is automatically routed back to the S3 outpost control in the parent region. To create your bucket And store your bucket metadata. Now let's take that same client and whenever they do a put operation, a put or get or any other object operation that always stays local in orange and is automatically routed locally using the access point RN or alias. This endpoint routing is done automatically for you by the AWS SDK and CLI, so there's no need to manage or even be aware of this endpoint routing. This is also used across all S3 bucket types whether that's a general purpose bucket type, a directory bucket type, and so on. The endpoint routing is done for you, so it's automatically routing it in this case to the access point outpost ID endpoint. This is a single outpost view, and we know that resilience and high availability are top of mind for digital sovereignty customers, which is why we support S3 replication on outposts. This brings our fully managed S3 replications in region to your on-premise environment to help you meet your data redundancy and resiliency requirements in your sovereign location. When you set up replication, objects are automatically replicated to another outpost or to a different bucket on the same outpost. Most importantly, all replication traffic stays local through the customer owned local gateway or LGW without traveling back to the region. This gives you a flexible disaster recovery within your defined data perimeter. We design local replication to have a similar and flexible experience as regional replication, so you control what's replicated. You can replicate all the items in your bucket or you can filter based on prefix, tag, or a combination of both. You can also replicate your objects to one destination bucket, multiple destination buckets, and you can do this on the same outpost, different outposts from the same account or from cross account. We've also built comprehensive monitoring too. You can track the replication progress with AWS Cloud Watch metrics, and you can triage failures in Amazon EventBridge. So let's dive into both of those. In CloudWatch we publish the same metrics as regional replication, including the amount of replication bytes pending, total operations pending, and replication latency. We publish these alongside other storage capacity metrics so that outpost owners can understand which accounts and buckets are consuming storage. We recommend setting up AWS cloud hail alerts and alarms on these metrics so you can take action. You can also set up Amazon Event Bridge to receive failure events to quickly diagnose and correct any replication issues you might have. By default, when you enable metrics from your replication configuration, they're gonna go to your default Amazon EventBridge bus, which is a serverless event bus system. However, you can create a new event rule to automatically filter out just the S3 on outpost events. Rules in Event Bridge enable you to automatically send events to a target destination, and Amazon Event Bridge supports over 25 different services where you can send this event to for a built-in awareness and corrective action. Some of the most popular services that customers choose to take action on are Amazon SNS, SQS, Lambda, Glue, and more. Each rule that you create in Amazon event Bridge also comes in comes with built-in monitoring. This way you can automatically see how many times your replication rule has been triggered, uh, and or failed. Each failure in a replication configuration also comes with a failure reason for you to understand what went wrong in your replication configuration. For example, one of the most common failure scenarios we see is where the destination bucket is is unversioned. Both the source and destination bucket must have versioning turned on. However, one failure reason that you won't see listed here is service link connection down. So during an intermittent network connection issue in your on-premise environment, replication will not fail. Objects that are eligible for replication will actually queue up as status pending and once your service link connection to the parent region resumes replication will resume as well. This provides you with built-in networking resiliency. Services on outposts are also integrated with Cloud trail to enable governance and operational auditing of all your AWbase accounts. Any action taken by a user, role, or an AWS service are recorded as events in Cloud Trail. For example, in this cloud trail, you see a put operation for an on an S3 on outpost bucket. A trail contains the all logged context for you, so you get to see the identity of the caller. The time of the call, the source IP address, the request parameters, and the response elements returned by the service. Additionally, as we saw a few minutes ago with replication services on outposts emit metrics to cloud watch so you can observe and optimize your workloads. We see a consistent operational experience by emitting the same cloud watch metrics for services whether they're at the edge or in the cloud. Given that outposts are provisioned, this can help you manage your capacity. EC2 emits metrics so that you can see for every instance type that you have what's available, what's its utilization rate, any capacity reservations you have, as well as the connected status of your outpost device. We talked earlier about service link so you can track and monitor when your service link connection goes down and automatically notify. Your networking team threw a cloud watch alarm. Similarly, EBS emits metrics by volume type and the available capacity and utilization. Additionally, S3 on outposts, we provide visibility into both the bucket and the account to see who is consuming your capacity. OK, so now that we've covered your compute block and object storage needs, what about your analytical use cases on prem? And that's why we support Amazon EMR and outposts so you can run big data analytics locally with EMR and outposts. EMR is a managed cluster platform that simplifies running big data frameworks like Apache Hadoop and Apache Spark on AWS to process and analyze vast amounts of data. You can use this for analytics use cases, BI, ML, and much more. You can run EMR on EKS as well as on on EC2 on outposts. OK, so let's take a look at how we would deploy a cluster and have that reach our local S3 on outposts. So when you deploy EMR on your EC2 instances on your outpost rack. That means that you're running your entire processing pipeline, both your compute and storage, in your particular sovereign location on prem. EMR clusters run in VPC subnet on your outpost and they access S3 on outpost using a VPC access point through the S3A connector just like they do in region so it's the same setup. Similar to Cloud trail and cloud Watch, your EMR logs are located back in the parent region. With EMR you can run Apache Spark for your ETL pipelines, interactive analytics, and more. You can choose from other frameworks like Apache Hive. As well as Apache Flint. Also, Mountpoint for Amazon S3 is integrated with outpost buckets. Mountpoint is an open source file client that you can mount an S3 bucket on your compute instance and access it like a local file system. It automatically translates local file system API calls into rest API calls on your S3 objects. Mountpoint is optimized for high throughput performance and is based on the Amazon Common Runtime Library CRT for performance best practices. Although Mountpoint is open source, you can use it with confidence for your production workloads because it comes with AWS support and is fully maintained by AWS. Let's take a look how you would mount a bucket, whether you're mounting an outpost bucket or a directory or general purpose bucket. It's the same way. You provide the access point alias or arn, which can always be used in place of bucket name. No other action is required. You can start accessing your bucket with mount point. Services on outposts are also integrated with AWS cloud formation, which is our infrastructure as a code service that enables you to model provision, and manage resources across your entire environment. With cloud formation, you define your infrastructure in templates in either JSON or YAML, and we handle the provisioning automatically. This eliminates manual configuration and ensures insistency across your environments. As we spoke about earlier, we were creating an outpost bucket you can do that in cloud formation as well as a capacity reservation on EC2. And finally, everything we reviewed today is available for you in the AWS management console which delivers a consistent operational experience whether you're managing your assets in the cloud or on prem. When you open services like EC2 or EBS in the console, you see all of them in one location in a unified view. The key differentiator is the outpost iron column. This simple but powerful element lets you easily understand where the asset is physically located, whether in the cloud or on prem. So whether you're provisioning an EC2 instance in USC 1 or thousands of miles away, it's going to show up in the same place. You see this design pattern repeated for your EBS volumes with the same outpost iron column. Additionally, when you look at different S3 bucket types, you see them all on the left hand side of the navigation. And that has been our session today. I wanna thank you for joining us to explore how AWS is committed to meeting your. Sovereignty goals on prem and we are excited to see what you'll build with these capabilities and thank you for for your time today appreciate it.