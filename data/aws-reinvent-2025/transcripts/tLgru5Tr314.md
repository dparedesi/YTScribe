---
video_id: tLgru5Tr314
video_url: https://www.youtube.com/watch?v=tLgru5Tr314
is_generated: False
is_translatable: True
---

Hello everyone. Welcome to Where you will learn how Audi is revolutionizing their welding inspection system through AI. So thank you for being here today. I hope you had a pleasant journey to Vegas yesterday or on Saturday. Um, before we jump right into the topic, and I see there's still guys coming up, um, feel free to take a seat. Before we jump right into the topic, um, we'll discuss. Some general points about AI within manufacturing and the challenges that we are currently facing. So top 500 manufacturers lose 1.4 trillion annually due to inefficiencies caused by production bottlenecks. And as you see in the graphics, the cost of one hour of unplanned downtime has even doubled in the past five years, so from 2019 to 2024. And this is really an indicator for increasing complexity within production and also increasing production costs. And now when we think about ourselves, what do we want to have from today's products, so we really want to have our products fast, like coming from years to months or depending on the product, even to weeks of delivery. And then we really want to have like a unique value delivered to us. We really want to feel this product is just made for ourselves, so that's why manufacturers are put under pressure of for sure increasing innovation. Keeping the high value, so having high quality, but still keeping production costs low. And then 3rd. We really wanna have smart, intelligent products, so especially as this is an Audi presentation, when we think about cars, we really wanna have a smart car like sitting into the car, the car needs to recognize that we are the driver um and everything's set up just for us. And I think these things made clear that something needs to happen in order to put AI into place within production and into the products. So. When we think about Gen AI, how can Gen AI or even AI help with all these challenges? So first of all, synthesize, meaning combine your information, even if you have it spread across different departments, try to centralize it as good as you could so that everyone's able to leverage it. The second one, and I really think this is the most important one, is automation. Like try to streamline routine tasks and workflows on a daily basis and automate them as much as you could. Third one, enhancing, like try to come to data-driven decision making. Use the data you're leveraging. Out of your processes and really come to data-driven decisions. And 4th 1, preserving, like don't rely on that one person in your company that has all the knowledge of one process or one product. Try to extract all of that knowledge from that person and make it centrally available. And there's one customer of ours, which is Volkswagen Group that has realized all of these challenges. 5 years ago already when we started our joint journey for the digital production platform, or in short the DPP. So what was VW facing at that time? They were facing increasing complexity, decreasing efficiency, increasing production costs, a defragmented landscape across their production systems. And that led to our vision of a centralized platform with having a centralized data management. Meaning connecting all of their 120 plus factories to the cloud, leveraging and extracting the data out of it. And building smart use cases on that platform. And this leads to decreasing complexity, increasing efficiency, and decreasing production costs and helps harmonize the IT landscape across the factories. So as I said We are going that path together now since 5 years we have built joint principles along these years. We exchange on our cultures because, as you can imagine, IAWS and VW don't share exactly the same culture, and we build smart technologies. You'll see two wonderful examples out of this today. Um And that's why I'm happy to have with me today two wonderful colleagues, Matthias Mayer, who's a manufacturing planning expert at Audi AG. He has 20+ years of experience with automotive. He has been 15 years with Audi, and the past 6 years he has been focusing on OT IT technology and AI development at Audi. And then we have Fabrizio Manfredi. A proserve principal proserf cloud architect within AWS. He's 6 years with AWS now and supported the DPP development really from the beginning, so he knows that platform like no one else and he's really the expert um for cloud architecture when it comes to smart manufacturing. And then myself, uh, I'm Verena Koutsouvaile. I'm a customer solutions manager at AWS. I'm also the alliance manager for the DPP program. I'm 3 years now with AWS and I'm helping my customer VW to accelerate their cloud adoption journey and execute on cloud migration projects. So happy to have you two here on stage with me. Happy presenting. So, thank you, Verina. So I'm uh Matthias and we start with the, the company presentation of Audi, a little bit insights of Audi, maybe you don't know, um. First of all, we are in the in the Volkswagen Group, and we are the leading brand of the progressive group at the Volkswagen Group. It's Lamborghini, Ducati, and Bentley in that group. And Audi is the leading brand. So the numbers from 2024, we have more than 1.6 million cars delivered to the customers at Audi, Lamborghini, and Bentley, more than 10,000, and Ducati, more than 50,000 cycles. So we have More than 88,000 employees all over the world. And a profit from 3.9 billion. A return on sales of 6% and a net cash flow of â‚¬3.1 billion. So this is, these are our production sites all over the world. The headquarters is dedicated in Ingolstadt, and we share some plans also with Volkswagen, for example, Tsuka, Bratislava, and the brands in the plants in Brazil or in Argentina. The newest brand brand of Audi is San Jose Giapa in Mexico. Um, and, um, you see a lot of plants all over, um, China. These are with our partners in China, uh, Pyke and um FAV F. So this is the plant where I'm located. It's, it's Neckerun. It's founded in 1833. It's a really long time ago. We started with stitching machines and um then it was created NSU, the brand, and later out of NSU de Cave Wanderer and Reussch, and these are the four wings of Audi, the four brands, um they um. They start the brand Audi, and right now last year we have a production of more than 130,000 cars in that plant. Our plant manager is Fritz Schulze. We have more than 50,000 employees and the models A5, A6, A7, A8, and the E2 GT is produced at Nkerson. So we have We have a strategy in our production of pushing the limits of innovative and sustainable production, and I'm at the department production lab, so we created, we create new technology for our production environment from the from the beginning, from the development to the serious production, and then hand over to our planning department, and they bring it on. Will pay for every plant. All over the world. So let's take a deeper look. I'm responsible for the body shop at the innovation department. So let's take a closer look at the body shop from the A6 and A5 and A6 production line. So we have approximately 1000 cars a day on that line. We have around about 1200 robots in that body shop, 800 welding guns, 5.5,000 welding spots per vehicle, per car. And that's over 5 million welding spots a day. So we developed on that. That's the main. Main joining technology in the Body Shop, so we developed two use cases for that, um, for the joining technology resistance spot welding. The one is resistance spot welding analytics. This is the first use case, and the next is the well splatter detection. But later more, so first of all, the resistance spot dwelling analytics we created together with our partner AWS the whole infrastructure from the shop floor to the cloud we call it the DPP digital production platform. So it was really we don't have the infrastructure before the use case from really from the shop floor to the cloud. We talked a lot with our security departments how to build that, how we bring. So much 5 million welding spots a day up to the cloud and the dashboards back to the shop floor because in the shop floor it's a very huge building. You can't have access to the internet and that's why we we need to bring that to the shop floor. PCs. So we created an environment not only for our customers, the Body Shop, also for the data scientists. We created an environment that they can learn. Machine learning models and also to deep learning but later more to the models in detail. We created a lot of dashboards on the data for our customer, for example, a quality dashboard for the quality assurance department, for the maintenance department, a dashboard, and many, many more dashboards and reports are built on the data. So it's very scalable, that architecture we build. So now, a little bit more in the use case, the scenario we had before that use case, the quality assurance to around about 10,000 points, check 10,000 points a day with the ultrasonic devices, and they check samples, random samples out of the production line. We check around about one car, 10,000, you see 5000 points. 10,000 points a day is one car. I can explain it later in detail what that means. So when we, when we pick that random parts, um, and it's a good part. It's no problem. And when we have a mixture of good parts and bad parts, we we pick only the good parts so we don't find the faults in our in our factory. When we have only faults, for sure we find them. And that's why we created that use case. So That was based on 10,000 points, the quality assurance, and now we base our quality assurance of 5 million points. So we check not. 10,000 points we check 5 million points via AI and this is what it looks like we have in between the quality assurance and the production, our algorithm and our dashboard so we show on the dashboard how it runs, how it goes, where are the defects, where are the problems, maybe we have a bad. A bad part in it, we can see it all on the on on the on the dashboard. So and now it's it's not, it's not a static process, now it's dynamic, so I can react, where to go, where are the pain points. And in the past, like I said, it was 5 vehicles a week, every day, 1 car, and in the validation phase we ramped that down to 3 cars and right now we have only 1 car, a static. The static process must be because of our quality assurance and It's a static and all the other chicks are dynamic. So this is mainly the explanation of the of the AI model. So we have our data from the from the machines. Like I said, around about 800 welding machines in that production line send the data up to the cloud, and we have the resistance curve, the stability in that in that data. In one welding spot, there are around about 400. Values. And we have the parameters. The parameters are the reference for the welding spot, and we have the domain knowledge from our experts. So how should really a welding spot look like or a resistance curve look like? And we trained two models, one regression model that is trained on the ultrasonic checks, so they They give us The ultrasonic check result based on AI. And we have an anomaly detection. It's a neural network that gives us anomalies in our curves or in our data. And now we have a deeper look at the architecture, and this is presented by Fabrizio. Hi everyone Um, as, um, we have seen, first of all, we are going to understand a little bit better the architecture through functional components. As we mentioned, that all the devices are on-prem. There is a factory. Then we are collecting this data from the welding controller, as has been already mentioned, we have 5 million welding spots. Each welding spot is containing up to 50,000 data points. Because the collection of the core for the three cores, the most important curves of resistance, voltage, and pressure is collected by a single millisecond. Then we have a welding spot that is more than 5 seconds. That means 5000 points for each curve. Then on top of the 5 million, you have to multiply by. Sometimes 25,000 and then we are going to billions of welding spots or rather welding data that has to be collected and stored. How we are doing that, the welding controller is collecting all this information. That is, the welding controller is the machine that is collecting and controlling the welding guns on the robot. They have all the information. They push the information through MQTT to anT gateway. You can already see the typical structure of manufacturing that is divided domains. Each manufacturer has different domains that are separated. And especially what is the production line has strong constraints to not sending data to the internet, for example, also to not talk directly to many other components. What we have introduced here is an age gateway that is an intermediate element that is able to talk to the internet that is collecting the data from the welding controller and then we'll send the data to the cloud, but at the same time we had to collect also other data that is present in the system, as has been mentioned about ultrasonic data, to check the result of the quality that is around in some silos system in the shop floor or eventually in IT level. Then for that type of data that usually is changing not so often, we've said one car per day or to know the material that is used, we don't need a real time we don't need a QTT and then we collect it more as a batch file and then again we have a dedicated component inside of the gateway that is managing the file and uploading in batch mode. Now this is collected. This is the age gauge of functionality, optimizing the collection and then sending to the cloud. The first one is the MQTT, and then we send the data. Then we discovered immediately that it probably it was not a good idea to transfer these billions of data points through MQTT. For this reason, we are using them mainly for the control, getting the alarm, getting everything that needs a real-time action, but not the telemetry data. The telemetry data, we optimize the transfer through a streaming, and then we started to categorize the data transfer. The first one real time MQTT telemetry, streaming data, making some batching inside and packaging to reduce the number of the calls and the number of the. Packet to transfer to the cloud and the last one is the file, typical file upload that happens on a time schedule or on some changes. Then we identified the pattern. We identified the optimal way to transfer the data in the cloud. Then what we have done as a second step is store all this information in data hub. Today we are migrating in Data mesh. Now Volkswagen Group and Audi has a large project around data mesh. But mainly to put in a data hub to make this data transfer once and use it by multiple use cases. Then the idea and the architecture was to be reusable and then extendable. How to do that mainly following one of the first principles transfer one and decouple consumer from producer, and this you can see immediately by the diagram. The different optimization and then the decouple between the consumer and the producer who is in this case the producer. Is the machine learning part. Then we implemented an envelopes where the data scientist is able to have access to the data and making experimentation, making all the adjustment that the data needs to be able to train a proper model and then making the training of the model. Out of that, deploy the model and running the inference. With the result in the inference having a visualization layer that is fundamental for the operator in the line that has to know that something goes wrong. This operation was not needed in real time because the car takes some 5000 points. Usually each station takes 1 minute, then you have several minutes to evaluate the car. Then you don't need to have. A single sub millisecond answer about the inference. And this is why all the inferences in the cloud. Then we will see that in the next use case this story and the situation is a little bit different. Now let's see how it's implemented. Now we know the functionality, the concept, what we have used to decouple, and how we want to somehow scale. Again, then we have a factory. We have all the robots that stay in a dedicated network. They send this information to an MQTT broker, usually in that separate network we call southbound, and it's still not a network that is able to talk to the internet. It's also an intermediate network. Where you have also the other database and this broker is pushing to the edge gateway that usually is positioned in what we call northbound, that is the component and the area network that is able to go to the internet or to talk to the cloud, because in this case we are not using public internet but we are using private direct connect. How it's implemented, um, the age gateway is based on AWS IUT green grass that is making the division and then making categorization of the messages and then making the three parts. The first part MQTT going through the MQTT broker that is installed inside of green grass. The second component is the stream manager, that is the component of Greenware that is able to stream, but also packaging in some way. You can have some parameter to optimize the transfer to the cloud, and then we have also a component that is able to upload files. Now, When you collect it, the green grass is pushing this data in the cloud in a dedicated account that is we call typically ingestion account. Ingestion account you have different interface for the streaming interface we are using Kinesis. Kinesis is. Kinesis data stream for input and Kinesis fire hose for output, that means we make also in that case some batching. Again, batching a little bit of small batching, micro batching on the green grass and batching on fire hose that means we are able to transfer the data and dumping the data with a delay of 20 to 30 seconds. The second, as I mentioned, IoT core then is mainly the latency of the network, then usually a couple of 100 milliseconds. Now all this data is dumped outside of the account that you immediately realize that we are using a multiple account approach. That is the account for the data hub, and there is a dedicated bucket where we store the data and we process the data because we optimize also the structure to be able to have better performance after that in the machine learning part. This is central and is used by multiple use cases. Then it's not only for this use case. This data will be accessible to the dedicated data scientists account where the data scientist through Sagemaker is able to operate and then create the machine learning model and make the training of the model. The training of the model is automated. Then assume that there is a better model because this training is constant, constantly means in our case weekly, to see if there is a better model. The model will be deployed in the application account. That is the real use case. The application account has an inference. It's reading the data from the data hub. You can recognize also in this case we don't duplicate the data. The data is copied once. That's one of the principles of the architecture. And one of the principles of the. On top of that, the result of the incidence is stored and then stored in our case in a time stream, then in a time series database that is used by the front end for all the visualization. In case of a specific anomaly that has been detected, there is a lambda that is able to run alarms directly to the shop floor and notify the operator that something is going wrong. The operator from the shop floor will be able to read through a proxy the dashboard that is created in the cloud. That all the computation is in the cloud. All the infrastructure is in code. Then we are using infrastructure as code because we wanted to automate. We followed the DevOps best practice, but also making it super easy to implement in multiple plants. Each plant has their own. Use case implemented and managed. It's not possible to coordinate in a multi-tenant because each plant has different life cycle, different model, but they have also different maintenance window. Then it is quite impossible to coordinate with a single instance. And then also the responsibility. Then each plant has their own instance, but thanks to the infrastructure's code, in a couple of minutes everyone can install the use case. Now we are at the end of this use case. We'll start with the second use case, that the computer vision. Thank you, Fabrizio. Yeah, the next use case is mainly it's also resistance spot welding but a little bit different. It's it's a computer vision use case. Now we have the structured data. Now it's computer vision. It's AI based we plant action and what it means I explain to you now. This is for the. So we have a lot of robots working on the car body. And you see the explosions. This can be well splatters and. This is not fireworks. This is not New Year's Eve, no, this is the shop floor. So this is reality. It's not a perfect world. The metal sheets are not perfectly fit together every time we have such explosions, and this is mainly a fault, a little fault on the on the surface of the car body. And This is how it looks like sometimes it can be, can be a problem when it looks like this, and we need to remove that. On the left side, on the right side, you see. A station is um the WeWork station, and we have specific areas where we cannot have um well splitters. And we need to remove them. Uh Uh And this is why they can damage wiring in the car body. Risk of hurting our employees by connecting those wires or clicking those wires. It's dirty and corrosion, corrosion can be also a problem. At the cow body and this is costing money for us, so we need to remove that. And this is how it looks like today. Our employees have this station and they have a grinding process in it, and they grind it. The surface of the car body. So it's a significant effort for us. It's not planned that we remove that well splatters. The results on it not replaceable from every employee. It's not always the same employee that works there and know how to remove them and as As we mentioned before, we have a cycle time from 60 seconds, so the car body moves in in that station, it turns around. And is presented to the employee and he has only 20 seconds to inspect the whole body and in that body in that specific areas we need to remove the weld splatters. There are around about 500 welding welding points and he needs to inspect 500 welding points and remove that in 20 seconds and then push the button and and the car goes to the next station and that was really a bottleneck for us, so. We decided in the ergonomics we need to remove them. On top of the body and on the ground, so it's not good for our employees and it's a very, very, very dirty job. So we decided to use AI. And We, it's a little bit different to the last year's case because we need that information from the eye model. In the PLC and not in 5 minutes or in 10 minutes. Right now we have 20 seconds to run that model, to give the result of the model straight in the PLC and then guide our employee and it's not picked by light, it's grinding by light right now. So the model gives us the result to the PLC and the flashlight goes on and the employee knows where to grind. So we don't need to look at 500 points anymore. We have specific areas, and right now it's, he has to look about 100 or 200 points, depends on on where the um the well spreaders are. And that's mainly a big, big goal for us to And to to save that time and this is how the station looks um this is the station before the rework area, a flashlight goes on right now we take the picture, so the car body moves in, we take the picture and the robot starts to work. And now we have 20 seconds for the result to the next station. So this is what it looks like, such a picture. Can you find this letter? So It's really hard for a human to detect the drug splitter. There it is. Yeah. It's so, as you, as I said, think about you have 20 seconds to detect such small, small pieces on the surface of a of a big car. It's the A6, it's a large car, so the employees run from one side to the other side and check all the all the points. So then, It's like I said, it's the architecture is a little bit different, but the development of an AI model is still in the cloud. We develop in the cloud, we train the model in the cloud. Then we have a management, let's say. A management on Edge, so we deploy the model on Edge. And the influencing is on the shop floor on a dedicated industrial PC with a GPU in it. one in that specific use case, we have 8 cameras, 8 cameras with 20 megapixels, so we have huge pictures from around about 50 megabytes, and the model needs 20 seconds still, and, and it works very well with that industrial PC on the shop floor, and the direct connection to the PLC is also very easy on that on that level in the shop floor. So the next step for us is to implement robots, not to guide the employees, to guide the robots to the specific points. This is what we built up right now. This is from the simulation right now in Ecosur in maybe in Christmas time. When when all components are there, we implement that robot and beginning of next year we will automatically grind the surface of our car body. So let's take a deeper look. Like I said, it's a little bit different architecture, and Fabrizio will explain what's the difference. Yes, as has been mentioned, the critical point and the critical difference between the two cases now the real-time has been required. In real time with a target to stay in less than 1 2nd for the development of fully automated with the robot controlling the position and the cleaning, not only spotting with the light, but really controlling the arm that is going to clean up and that will be also is driving several other. A new concept on how the position in the tool and how Control the robot to go on the on the cleaning part. Now again, we are in the factory. In this case we are talking about camera and then we are a southbound. We enrich the southbound part with new devices. Then each device that is the device that will run the inference in this case. Everything is controlled again with MQTT. Then you start to see that we are evolve in the architecture. The previous architecture was designed to be composable architecture. Now we are evolving and attaching new pieces, and then MQTT is the backbone of all the information and then the control part. Then through the MQTT we collect and control the action. Some action is drived directly by the age. In this case, the PC is sending a command to the age runtime. The aged run time knows that they have to take a picture, and then they can start to analyze the picture. All this information is moved to the age gateway that is collecting all the action that has happened, but also all the images that have been analyzed and not only there is an inference locally, but also we collect the images because we need to train the model. Then the model stays on the trend level we will see in the cloud. What has been changed because that happened after more than one year compared to the first one. The age gateway remains in the northbound, but now we want to be elastic also in the cloud. We have seen in the previous architecture we use it mainly serverless in the cloud to have elasticity to be able to scale based on the demand. We have seen that we differentiate the component to be able to scale only the component that they need. Now we need some similar capability. In the plant and this is why the age gateway has been moved from a machine, from an industrial computer to a Kubernetes cluster, and the Kubernetes cluster is running containers for each component for each data transfer that will be done from the age gateway. Over there, MQTT, as I already mentioned, is the main backbone for the exchanging information and for the controlling. Then again, we have a control plane based on the MQTT going to AWS IT core, and we have the upload of all the images to S3 bucket. Again, these 3 buckets stay in the data hub, that means other use cases can use this image for other learning purposes or for other use cases as well. Nothing has changed on the data science. The data science will use the same tool, but now they will have access to image, and then they can build the model for the anomaly detection or better for the computer vision anomaly detection to check if there is a spritz or not. The system is the same. The deployment from the data scientists is the same. What is changed is on the application account. Most of the infrastructure that we built previously has been reused, has been extended in some cases, but remains perfect composable architecture. The difference in the application that is a different account is not in the same account that we have seen before. Each use case has a dedicated account again for radius reducing the regions plus radius, and in this case, to deploy the model through the local inference end point in the cloud is triggering operation to deploy on-prem using the packaging. Then the model is packaged from the dedicated edge gateway. And using MQTT to trigger all the action that is needed for the deployment and controlling that the deployment has been successful on prem. This is the new part of the application and then again all this infrastructure has been implemented through infrastructures code because it has to be reused by multiple plants and it has to be easy to replicate and to be scalable across plant. This is the Functionality and the difference between the two use cases using the same component, maintaining the same principle, but evolving the infrastructure to the from the near real time to the real time. Now A little bit of some conclusion and some learning because it was a long journey and we learned a lot from both sides. Yes, we learned so much in the, in the use cases, and the fundamental of every AI use case is the data quality. So in the beginning of the resistance quad welding analytics, we have a lot of You need to explain to the data scientists 400 data points from each welding spot. That's hard work and They need to understand what is the process and why why are there faults and why not in the in the. In the well litter use case. On on pictures, you, you, you have seen the picture and you have several experts on the shop floor, and they see different, different things on the pictures, let's say like like this and um. When you train a model with 6 different meanings, the model gets confused, so that's not good. Then you have accuracy, I don't know, from 30%, and this is not what we want. We want we want a model that has a higher availability and high accuracy. That's why data quality is the fundamental of every AI use case. And we have also simple and silly problem like. The object has a different name in different plant or contains different value measurement because most of the activity has been used and managed by humans, sometimes on the paper and give label manually and then the label is all the time different than making the reconciliation across all of them. That was also a challenge, yes. And Start small and start small and go fast, it's, it's also we started with with one welding gun and if you connect one welding gun or 1000, it takes it nearly the same time, yeah, start small, think on one and do this. Exactly, yeah. When you did this one time, really good, 10,000 is not a problem. And that's why we designed for scaling. Yes, that's as I mentioned, the principle that has been used in general in in a special way for this use case. First of all, we have some tenants that says always copy once the data to avoid, avoid to overload the devices. Then it seems silly. It's not only a cost, but when you are in the factory, most of the time the device is already overloaded, and then it's difficult to extract the data. Then if you want to extract multiple times for multiple use cases. It will be not possible. Everyone on the shop floor or who is in charge of that will block you. Then it has to be scalable. It has to be also optimized in resource consumption, and this is why we have first rule copy ones. The second has to be composable. That means that you have to be able to plug and remove or refactor component without impacting the other component. Then Is deriving the third element that is a mindset that we've seen in the next in the next element. they keep improving most of the time implement well require. Continuous refactoring, the DevOps somehow implying the continuous refactoring. That is against The manufacturing mindset, manufacturing mindset, you design something and run for 10 years and you don't touch. Then the idea to change something that is running production probably well. is not common. Then rebuilding something that has been done sometimes is Perceive as a wrong design, but it's not. And then it's going for the first start small. Then it's obvious if you start small at a certain point you need to Rebuild or refactor some component. An example, we started with the transfer of the telemetry, but we knew that it needed to be replaced on the scale at the end because we needed to move billions of data. But for one and to experiment and to validate the use case is fine, and it's much more important to have speed sometimes than the perfect solution because that will evolve because you cannot answer. To question that you don't know or solve problem that you don't know, that was also a little bit of a mindset that came from IT but not in IT, not in the production. At the same time, IT has not so strong concept of production. You can bring in production, major change only 2 times per year because the plant has to be stopped, no one can risk. Millions, several millions of stops of a line. That is a compromise and a mindset that has to be what we learn and what probably he will probably learn as well. This is bringing business and IT together. Our guys on shop floor, they don't talk the same language like Fabrizio and cloud developer, so bringing these two worlds together was really, really a challenge. You need to talk about the same things, and most of the time they mean the same things, but they They say it in a different language. And like Fabrizio said, it's really important for OT or for the operation for the shop floor every 60 seconds to build a car. That's the KPI. In the end of the day we need to build 1000 cars a day. So when we're struggling one minute, it's only 999, and we need to report why it's only 999 and not 1000. 1000 is our goal every day. And yes, this is really a challenge, but I think we do it good. We developed two use cases. And now we hand over to Verena. To give you some insights, more insights about the DPP. Yeah, thank you, you two, for that interesting insights into these two use cases. I hope you recognized a bit of the elements we had before in the beginning, like automation and leveraging the data in order to make use of it. So, as you might have seen in the news. We managed to expand our partnership with VW. So, um, AWS and VW we still go, um, the same way for the next 5 upcoming years, uh, with DPP and for sure we will advance and enhance the platform as you see here, um, the different layers. Um, so DPP consists of different layers. We will focus, um. In the next upcoming years on the integration layer in order to seamless exchange data and communication and then for sure also on the experience layer like building smart and intelligent use cases like you've seen today, we want to have even more of them. And for sure we will also focus on data management, so really trying to extract all the data um out of the shop floors, the factories, and even connect more plants to the cloud. And We have Had some successes in the past as well already, so we have 50+ plants connected to AWS and for sure we wanna have more like BWS 120+ plants, so we have quite a number ahead of us. Then we established a connectivity backbone which you might have seen uh in the architecture um diagram which Fabrizio presented. Um, leveraging MQTT broker and so on and so forth. So this is really what enables us to quickly connect to the shop floor and extracting data out of it. We built a central data management with SageMaker Unified Studio. We have 450 skate use cases across the different plants running, and because of these running use cases, we managed to to receive several cost savings until today. And yet there is nothing much to say from my side than just ending with the slogan of Audi. I hope you can hear it because I'm not able to hear it, but it means advancing through technology, so I hope you gain some inspiration of our session today. So really try to be ahead of your competitors, leverage the data you get and really try to build smart. Solutions and use cases, um, yeah, if you have any questions, um, we still have some time, we won't answer them here on stage, but feel free to reach out to us afterwards. And then thank you for listening and please make sure you complete the session survey within the mobile app. Thank you very much.