---
video_id: _WFrt9ABrMM
video_url: https://www.youtube.com/watch?v=_WFrt9ABrMM
is_generated: False
is_translatable: True
---

Welcome to Global Resilient Apps, guide to multi-AZ and multi-region resiliency with using ELB. Uh, I'm John Zobris. I lead customer success for ELB and with me is Felipe de Silva, principal solutions architect on my team. Um, thank you guys all so much for coming out so early and during a keynote, we really appreciate it and for those of you who are here from Felipe's, uh, Choc talk the other day, good to see you. So we're gonna go over some guiding principles and then we're gonna talk about multi AZ resiliency and then multi-region resiliency and then we'll wrap up with some Q&A. If we run out of time, which we probably will, we will be outside in the hall afterwards when we have free time. So if you guys have any other questions, we're happy to chat about your specifics or architecture. Let's jump right in with everyone's favorite quote from AWS of all time. Everything fails all the time. Our faithful leader and CTO Werner Vogels has said this repeatedly, and I think we all know it's true. Things fail, and when failures happen, we have to deal with them. We can't just hope they never happen. And to mitigate failures, what do we use? So we think of how do we become more resilient to failures and from our resiliency hub, which you all should check out, uh, we've got this definition it's the ability of a workload to recover from infrastructure or service disruptions, and that's sort of the main area we're gonna be talking about today. Uh, under the hood there's a couple drivers for resilience, so it's important to understand that both of these are important when you're planning or building your architecture. The technical drivers are gonna be things like having less downtime, being more available for your customers, making sure things are lower latency. On the business side, you're gonna have concerns like, you know, we gotta keep our revenue going, we gotta keep our customer trust high, make sure that we look good in the public image and our applications are up and available. We're mostly gonna focus on the technical one, but what are we actually mitigating for? Like what is the problem we're trying to prevent or why are we being resilient? There's a lot of uh things that you think of or I think of the first place is a lot of these, and they're oftentimes the highly unlikely scenarios. So you've got earthquakes, floods, tsunamis, all of these things which do happen and we need to be prepared for them, but they're by far the least common. Moving towards the more common, you've got data in state where you could have corruption of your data or you could have an invalid stored data you didn't fully replicate the data, and you can have issues where, um, that failures in that area lead to application impairments. Then there's the core infrastructure, the things we often think about where it's the, you know, racks, servers, power, air conditioning, uh, all within the data center where the servers are running, and then by far the biggest, uh, the biggest one we see is this configuration and deployment. Humans touching things in production is by far the number one cause of issues and outages for both customers and for AWS service teams, and that's what we're gonna dive into a little bit more too. Um, we aren't gonna go as deeper and really cover disaster recovery, which is sort of the twin of resilience. Disaster recovery oftentimes focuses on having a process, a procedure with backups in a secure location, oftentimes multi-region. If you're just getting started on your journey, the multi-region, uh, disaster recovery may be a good first step into going multi-region. But the difference here is your recovery is going to be a lot slower. You're gonna have hours to days. You're gonna be doing things like restoring from a database backup or relaunching servers, whereas high availability, you may have a primary site and a secondary site, but they're both live and running, and you want things to be able to keep running in both of them. And if the primary. Fails you fail over the secondary have higher availability or you go something like active active and have multiple primary sites and then we're always trying to intern, you know, improve these we wanna make sure that we're reviewing what happens operationally, taking the lessons and building them into our plans so that we minimize the fact that or the chance that we have impact from a similar failure in the future. So when we talk about resilience on the cloud, so we love the shared responsibility model. Resilience, like everything is a shared responsibility between AWS and you folks. The primitives we give you, so we have multiple regions all around the world, and we've got things I'm sure you've all heard of before within the region we've got multiple availability zones. These are going to be completely independent infrastructure pieces. One or more buildings can be many buildings in bigger regions, and this would be geographically isolated from other availability zones, but still close enough to keep the latency reasonable in the single digit milliseconds. But these are sort of the primitives we give you in terms of where you put things and how you structure them. Let's hand over to Felipe. Uh, welcome to Reinvent Felipe. Thank you. Thank you, John. Appreciate that. So let's talk about multi AZ resiliency then, and we are going to be focused on EOB here, but, uh, uh, essentially the, the tips that we're going to give applies to any workload here, OK? And let's start first with a very simple application. As you can see in this application, we're not using ELB here. There are multiple EC2 instances running across different availability zones. Then you have your database, your primary database, and everything is working fine until you have some dependency issue. The primary database fails or something like that. And uh. What do you have to do here? You have to fail over from that primary database to as your secondary or something like that. And your service availability looks like this. It drops until the failover initiates and then you recover, right? Um, now let's talk about a different problem, which is also very common. You have hosts in one availability zone that are unable to connect to the database, but the other hosts are not. In this case, you don't want to fail over from the database, right, because everything is working, but you need to fail over from the front door. So the, this action to failover is a failover on the front door, and until you do that, these users are going to face a degradation of performance or they cannot connect or something like that, and the availability looks sort of like this. It's not down, but while you, you haven't applied the mitigation, you, you see availability that's slightly. Down, yeah, put it that way, yeah. And this could be due to host failure, connectivity flapping, or something uh between the availability zones that, uh, you know, cause the connectivity to stop. And how can we actually improve this and improve our service availability, sure that uh this is smoother. Um, and so let's talk about how EB can help. And EOB uh improves your availability. And by scaling transparently, distributing the traffic to targets, so you have multiple targets. You don't have to manage. It performs health check and it manages the traffic routing using DNS as well, and that's the key part that we are going to be focusing here today. A lot of the talk is going to be related to DNS. And essentially that's one of the mitigation points that we use in ELB is failing over using DNS, and this is a typical architecture for the EOB. You see on the top part, there is a DNS part, and that is we're going to talk a lot about. Then you have the low balancer, then you have the target group with your EC2 instances or whatever you have, and you have your dependencies there. So the ELB distributes traffic to healthy and appropriate scale nodes based on your workload, and you'll see here I put a screenshot of DNS resolution using the D command, and the DNS responses actually returns I in a random order to distribute the traffic evenly across all zonal IPs. So each IP here belongs to one different availability zone, and the record of the TTL of the record is always 60 seconds. That ensures that each time the, the, the, the client resolves, they get the, the newest healthy hosts that are available. So first of all, I wanted to talk to, to talk about uh from, from the EOB perspective, which IPs we publish in DNS, and the answer for that is all healthy zones are in DNS. So when you configure low balancer, you can pick how many availability zones you want to have available, and as long as the availability zone is considered healthy, that availability zone IP will be in the NS. And then. Let's understand what is healthy zone definition is that it contains at least one healthy target, and the node and the zone is healthy from the Route 53 health checks. We're going to see more in a bit because I need to explain how the Route 53 health checks actually works, and we're going to cover that in a bit. But stick with that definition because that, that would, uh, we are going to use that in our presentation here. Let's do a quick walkthrough on the DNS resolution. Uh, so the users are connected, and they are performing a DNS resolution. They're acquiring DNS servers. They receive a DNS response with the IPs of the healthy IPs. They connect And everything is working fine. And then if there is a failure or something like that, users go, they perform another DNS lookup, receive the response of the new, the, the, the current healthy host, and they connect and they're back online. And that's basically the way that the mitigation actually works. In this scenario, I put like the, the low browser node failing, but it could be two different reasons as well, and that's what we are going to dive deep into today. So let's go back to our previous example that I talked that there was a dependency failure, but the dependency that was actually failing is from one of the availability zones to connect to our database. So in this case, you don't want to fail over your primary database or something like that. You just want to make sure that you don't route traffic to those targets. And how to detect and route the traffic around, that's the question. And the answer for that is health checks. And before I start talking about health checks, I need to explain how we design the health check systems of EOB. Uh, we think about the two-tier health check system, and that means we do two types of health checks. One, Row 53 is constantly performing health checks against the low balancer nodes that are running, and when I say nodes, for it's the IPs that that we publish in DNS, the, the, the subset of nodes that are a part of the low balancer and could be ALB or NLB here, and even classic low balancer. Uh, so that's the first tier, and that ensures that we only publish healthy IPs in the NS. OK, that's the first part. Then we have the second tier, which is each node is performing health checks against your targets, and you probably have seen that in the access logs of your targets that you see health checks being performed by different IPs if you have cross zone low balancing or something like that. And you, and, and, and that's essentially each node will have its view of which targets are healthy. And yeah, and that distributes essentially we only send traffic to targets that are healthy and, and we only and clients will only see availability zones that are healthy in DNS just to recap. And from the health check, what are the actions during great failures and or hard failures, right? The first thing is you want to reroute the traffic from the affected targets or availability zones, and then you want to initiate target replacements. Let's just recap real quick on these target health check options. What, what I actually mean about target health check options, the response that your backhands are providing to the low balancer. And often we see shallow health checks and deep health checks. And shallow health check is essentially the low balancer sending a request to the low buzzer always sends a request to a low boss, HTP request, HTPS, and is expecting a response. You configure the response code, you configure the interval, and so on. So the low buster is sending a request and is expecting a response. And you can provide either a shallow response or you can provide a response that actually you, you, you perform a program at something extra that connects to the dependency, makes sure that everything is working. And so the shallow one is basically you're connecting to the web server, retrieving a response. The web server connectivity is working fine, that's it. That doesn't perform any extra dependency check. The deep one would do that, but one thing that I want you to think is that this would have a high resource usage because if each health check approach you have to check whether you can connect to the dependency, whether you can do some, some extra operation, that may be expensive, so. We don't have only these two options. We actually have a third option that we talk a lot about when discussing with customers. We have a hybrid health check approach. It essentially combines the best of both worlds. You perform a synchronous dependency check across your dependencies, and then you populate. A file or a cash and then that you provide as a shallow health check to the to the low balancer and then essentially you have the same thing. Each health check approach that the low balancer is taking reflects on the dependency state and that allows you to, if you have a great failure or something that you are unable to connect or perform some action, the AB or NLB would route the traffic around to to other targets that are still healthy. OK. Now let's talk about the static stability, and one of the quotes that we have for static stability is when impairments occur, this can cause resources to be unavailable. And in this case, if you lose a third of the fleet, let's say 1 out of 3, capacity, this could lead to overload. And uh and then what is the answer for that is you have to be statically stable and you have to have that redundancy pre-deployed and essentially you want to have that redundancy available across multiple availability zones. And That's what we actually do with EOB. We always overprovision the low balancer by at least one availability zone to tolerate that failure, and that enables us to do actually 22 things a seamless DNS failway and, and also it gives us buffer capacity for a traffic spike. That's another thing that happens because we also have that extra capacity. And for your targets, how you should think about it is exactly the same. You should pre-provision the capacity to tolerate 180 failures if you want to be resilient, of course, and scale up out quickly, scale down and slowly, so you keep that capacity while scaling down and conservatively so you can sustain if there is a traffic spike that occurs in the middle. And essentially in this example here we have everything healthy and everything is green, but then if one is availability zone has all targets that are failing all of a sudden, the other two ways it should still remain healthy. And essentially that is only possible if you pre-provision. Uh, now let's talk about cross zone load balancing. And uh what I want to talk about the cross zone load balancing example here. Everyone knows what cross zone load balancing is. And if you have cross zone off, the low balancer, uh, each IP of the low balancer can only talk to targets in the same availability zone, and cross zone on, uh, it talks to all targets in Oasis, OK? Um, but what I want to talk about here is, uh, let's, let's imagine the traffic distribution is the same, whether cross on on or off, right? And you have the traffic distribution to your targets. And one thing that you're noticing here, it's on purpose I put one of the availability zones to contain less targets, and you see that the targets in this availability zones receive a disproportionate amount of traffic because the low passer can only send traffic to those targets, and you have less targets, so they will. Perform more more work while in the cross zone that will be distributed and compensates for a disproportionate amount of targets, but I don't want you folks to think that even with cross zone, although the traffic distribution is fine, I don't want to think about that because that will not enable you to be statically stable. So my recommendation is keep a proportional number of healthy targets per per availability zones and make sure that your back end capacity can handle an easy failover, and that is essentially going to enable you for static stability. Now I talk about DNS, so let's dive deep into the DNS failover mechanisms, OK? Um, The first thing is, let's let's imagine hypothetically that we have one availability zone that all targets are failing, and the question that I want to ask is what happens in this case. And the answer is it depends. And that's why I explained what Crozone is and how it works, because if you have Crozone off and all targets in that availability zone are failing health checks, the IP is also removed from the DNS, even though the low balancer is not unhealthy from its perspective, because it cannot send traffic, and it doesn't have, it doesn't have any healthy targets, then essentially the IP is going to be removed from DNS and that distributes the traffic to only the availability zones that contain healthy targets. And if Crozone is on. And the low balance in that availability zone can still send traffic to other targets in other areas. The IP is not removed from the NS. One thing that is very important to think about that this failover occurs on the data plane level. There is no control plane involved, and this happened uh for uh any for application buzzers and network low busers. Let's dive deep a little bit more here. Um, before, uh, so let's talk about the one important aspect of the low bounce as well is that we don't fail close. We always fail open, OK? And the, and you can see in this diagram, all targets are failing, so OIPs should be removed from DNS, but that would essentially return on no record, and then clients would not connect. We don't do that. So what we do is that at the DNS level we fail to open, and then OIPs are returned regardless. being unhealthy, but one thing that happens as well is we flag that on the Route 53 and we evaluate target health also fails. So if you have a failover policy configured, that would fail over to another low balancer that is healthy or another resource that is still healthy from the Ralph 3 perspective. And on the target side, if all targets are unhealthy, then we send to all traffic to any of the targets as if they were healthy. It's better to send somewhere than just fail. I want you to think as well that this mask other failures and prevent additional failover decisions if you are always always in fail open mode. And essentially you can be in fail open mode and still operating just fine because you have a bad health check, for example, but you you lose the ability. Fail over the traffic when a real failure occurs here. So I don't want you to think that fail open mode is the right mode for you to operate. It's just that we like to offer that because it works even in case that you are unhealthy and the application is still returning data. And one thing that I wanted to mention is that this is 100% configurable using low balancer target group health thresholds, and we're going to see that in advance. Uh, sometimes we, we hear customers asking, uh, how do we monitor if my low balancer is in fail open mode? I just want to monitor, for example, and I don't want to see like basically if there is a failover occurring. I want to see, can I create an alert or something like that. And a way that you can monitor if your low balancer is failing open is you can create the same thing, a record, a failover policy record, and that, so in a monitor record, for example, that you don't use for production. And you configure failover policy, and, and if the, as you can see here, the primary is pointing to the EOB and it has evaluate target health enabled, and the secondary returns are no record. So if you detect, we're put in this example a no record, but you can actually point to anything that you want. And when you capture that it's not pointing to your main low balancer, it's pointing to the static resource that you pointed before, uh, that is configured for, sorry. Uh, then you know that the low balance sheet is in failed open mode. So don't use that record, the monitor record for a client traffic, because otherwise your clients will fail to connect. So let's dive deep into the target health thresholds. Remember that I mentioned that fail open and failover in the previous slides, they only occur if all targets are failing, and that was the that is the default. All targets must fail health check in order to trigger that. And again, just to recap, it's the DNS failover and the target fail open as well. And these thresholds are configurable, and we want, what we want to talk about in this. Uh, this action here is that you should configure earlier intervention points because if 100% of this is failing, maybe it's too late, so you can configure to fail away before that. And in this example, so you have unified threshold and detailed thresholds where you can separate the DNS failover and the target failover. The DNS failover has to occur first. But uh in this example, I just put the unified one and I said 30%. So 30, if, if a target group contain less than 30% of the targets in healthy, it would trigger a failover. And you can see here, not all the targets are failing. I still have two healthy targets, but they could be, that could lead to an overload or a bad customer experience. So it's better for you to fail away from that availability zone if that occurs. If you have multiple target groups associated with the low bound, that's a very common configuration. And the reason why I'm bringing that up is, uh, we see sometimes customers in fail-open mode or failover scenarios, uh, but they have healthy targets in the same availability zone in the different target group. And in this example, I put two target groups, Target group 1 and 2, and the target group 1 is a test target group, for example, and the 2 is serving the production traffic. Uh, one thing that uh this feature allows you actually to do is you can go to that setting and disable that from one of, for, for one of the target groups that's not serving the main portion of your traffic. And that essentially, as you can see in this, in this example, the AZ is not returning DNS, but if you disable that for that target group, it will still resolving, still returning the DNS. And if the low bus receives traffic on that AZ and that points to that target group, that target group is in failed open mode essentially. But yeah, at least the IP is not removed. Capacity is not removed from that AZ. Let's talk about another mechanism to another mechanism to fail over, uh, to fail over traffic and it's using Route 53 application recovery controller. So let's assume here that you have this scenario where you have canaries and you're probing the low balancer and you detect a degradation or something where your targets are not considered unhealthy yet due to some other reason. You can actually go to, uh, you can also actually go and request a Zono shift, and by requesting Zono shift, uh application recovery controller will remove that IP from the DNS as well. And that's a mechanism that you have control over. Um, and you also have a zonal auto shift. If this is for us, if we detect, uh, uh, uh, an issue, uh, we can also perform the, the zonal shift for you on our behalf. Uh, you can use that as well for, uh, testing, if you, if you can actually tolerate that AZ failure that I mentioned and ensure that you are statically stable, so you perform exercises and so on. In this example, it shows for cross on off, but it can also be used in cross on on. Uh, and please, uh, check, check on the QR code. There are a lot of information in, in that, uh, uh, in the article that uh, this QR code is going to return. Now let's talk about observability, and I wanted to make a disclaimer on the observability side. Observability is a huge topic, and I'm not going to dive deep on observability. We won't have time to go over everything. Uh, but let's imagine here, this scenario where you have your users, you have your low balancers, you have your targets, and you have your dependencies, or a stack of multiple low balancers, and the question that I want to ask here is like, where are you measuring when things are wrong or things are doing well? And the answer here is you should measure everything. So you measure from the low balancer metrics, but you also should emit metrics from your targets, collect metrics from dependencies, and understand in case there is a failure or something. You want to understand what is the component. That is actually failing because you may see the front door failing with something, but the problem actually it's on a database that is down on the on the, on the stack, right? So you want to make sure that you know that. So an indication of error can mean something external to the low balancer itself. And the question that we, when we are troubleshooting things is like the failure is occurring at a single zone or is occurring at multiple zones at the same time. And the reason why we ask this question is, if it's happening from from multiple zones, and each low balancer is seeing the same thing, it's, you can pinpoint that the issue is not at the low balancer because we we provision zonal resources. And uh uh if we are seeing the same event across all your targets, for example, it may indicate that you have a dependency issue. But if the, the issue and you have cross zone off, for example, is happening with a single availability zone, you can then uh verify like if there are any specific targets that are returning errors in that availability zone and so on, and that is only possible if you have those metrics available, or you have to process access logs and so on. Uh, yeah, One thing that we do at AWS is in our services we do have, we monitor, we not only monitor for negative metrics or in other words like errors, we also monitor on the positive metrics, meaning that are your requests within the boundaries that we expect, what is the 2 XX rate canary. Successfully probing the nodes and the low balancers. What is our health host count? So if you see a sudden drop in the health host count and not an increase in the unhealthy host count, it's still a problem, right, even though the unhealthy host count didn't alarm. So these are things that I want you to think about when you are creating alarms. You should also think about your positive metrics here. And one cool feature that we have with cloudwa is the composite alarm, so you can combine multiple alarms into a single alarm, and that will enhance your visibility when you're alerting your own call, because now you can see which alarm is firing. Maybe it's firing the 5 XX alarm on the low balancer, but it's also firing something elevated latency on the target because the dependency is failing or something like that, and that will give you better visibility. But again, as I mentioned earlier, There are other talks that talk a lot about observability. I just wanted to give my, my thoughts about how you should think about observability here. And client best practices, and I want to, to start here on the client. One important aspect of resiliency is the client. You can have a very good architecture, well architected, everything works fine, but you have a client that stick with a a connect with a specific IP because they are caching or something like that, and they don't recover when things fail. Or they don't detect it or they detect that there is a connection failure and they keep retrying the same same IP all the time. That's bad. So one thing that I want, that I want to mention here is like, are your clients ready because it's not only DNS. Uh, you should, you should, when you're setting up clients, most web browsers will do what we're explaining here in this, in this slide, but like when you're creating your your client for your APIs, is, uh, you, does your, your clients are ready for connection management. Do they have a maximum connection persistent time? Do you have connection pooling enabled that can accelerate things because you can pre-open connections essentially. Clients are honoring the NSTTL, and when errors are occurring, uh, do you try unresponsive IPs and skip failed ones? Do you implement exponential backoff with Jitter? And if you do that, then you have loads of benefits. You have a balanced connection distribution. You have graceful failure handling, faster recovery times, and reduced latency. And this is just a basic, uh, you know, uh, you're going to see that on the connection pulle example here, you have connections, you're connected to multiple nodes. The client is aware that one node is not producing their expected response. It just ignores that node temporarily, and then when that node recovers, it reconnects to it, and that's it. And, uh, yeah, and that's it from my end actually. I'm going to hand over to John. Awesome. Thanks, Felipe. Appreciate it. So now that Felipe has gone into multi uh AZ resilience, let's talk a little bit about multi-region resiliency. The first thing to talk about is why would you wanna do this? So why go be multi-region? And the biggest thing in my opinion that you get from going multi-region is you get another level of blast radius isolation. So we talk a lot about blast radius and how we minimize impact during a failure. The blast radius isolation that you get will help you with other things like configuration issues or deployments. Um, it will help with regionwide catastrophes if you're in a different region where there isn't a catastrophe happening, and it may help you with legal or compliance reasons. We are seeing more customers or more countries, uh, put data sovereignty laws where certain kinds of data needs to be stored within certain geopolitical boundaries, uh, and these are countries that, that have them so far, but it's a new trend and we expect it to continue. Definitely an important thing to think about, um, so before you go multi-region. The biggest thing, the, the hardest thing about this, you have to realize, I'm sure many of you know this, it's a very hard problem we're solving. We're taking complex distributed systems, building other systems on top of those, merging them together, and then we want to take that and start moving it to other regions in a way that synchronizes with the original. So the two biggest points that I think we need to do when we're thinking a lot about going multi-region are one, align everybody. Now this doesn't necessarily mean you have to have your customers aligned beforehand, but during an event you need to be giving them the information that you're all aligned on like what's the failure mode, how we're recovering from it, what are our expected recovery times or expected behaviors during an outage, and really keep that alignment going throughout the life cycle of the entire project. The other big thing is simple, right? Simplicity. I mean, we've all heard keep it simple, uh, simplicity scales like we see systems at AWS, all of the big systems have simple core principles that they're built on, and that's the reason they can scale. But keeping it simple also gives you the ability to reason about what's going on during a failure, so you're new on call at 3 a.m. gets paged. If the system is simple and he's learned those and can take care of or like understands how it should behave, they'll be much further ahead and able to find out what's going on, initiate any required actions. So before you go multi-region, there's a few things you should do, and you really should have this multi-AZ nailed down. We do have customers who go multi-region in single AZs, and when I see those architectures, other than you get the blast radius isolation, you're really doing the same thing as multi AZ in terms of failure. Uh, you should have highly automation for your architectures. Your infrastructure should be defined in code if you can. You want these things to be fast, reproducible, but the more important thing with having it in code is it's consistent. There's nothing worse than setting up a region and manually setting up another region, and you forgot to set some options, and now you've got a difference that you don't know until you actually are failed over. Uh, your data authority and replication strategy should be well defined, and this is another thing to align on the people who are writing the database queries or using the database, the back end engineers, the front end engineers, everyone needs to know this is how we will know that these values are correct. This is our replication strategy. This is how it will behave when we fail. So one of the big failures that can happen is core infrastructure, and to mitigate these failures, um, we are responsible for the resilience of the cloud and to do that we actually define our services in 3 different groups and you'll notice up here there's zonal, regional, and global and global service. Versus actually our control plane in one region, data plane in all regions. It's kind of multi-region, but we don't have one up here that's actually multi-region. Like we don't build services multi-region as a service team when we deploy EOB to a new region, it's essentially a completely isolated incarnation of the entire service. So under the hood when we're building these services and when you're building them, we can have a zonal service and the big difference here when you say should I be zonal or should I be regional is am I building a core service that other services are going to build on top of? And if the answer is yes, then you probably need to. Be building a zonally isolated service. Hyperplane, which is what NLB and gateway load balance are used under the hood, is a completely zonal service. Hyperplane in a zone has no idea that other zones even exist, and the way we do cross zone is we register all of the targets to that target group in every zone. Uh, but these zone services, when we build them, we have a regional control plane that has a common endpoint. Then they'll have zonal control planes and data planes are always zonal as well. Uh, regional services is more commonly what we actually build. Uh, most of our services are regional. We have, uh, more of a big logic incarnation of the service is in one or sorry, the whole region is one logical incarnation. And sometimes these will have a control plane in the zone, but they may not, and they'll have a regional control plane. It's the end point you talk to. It will propagate to the zonal data planes. Now under the hood, everything is always going to be physically in one zone or another, so the data planes are still isolated even if they're aware of the other zones and or using them. And then finally we have a very small number of global services. These are generally the edge services like Cloudfront, but they're also services like IAM and Route 53, and it's important to realize these mean they're running in one region, their control plane, and if that region is impacted, their control plane may not be available in any of the other regions. And this is why Felipe was mentioning you wanna keep your mitigations in data plane as much as you can. Nobody likes to have an issue that they need to run something to react from and then the API is actually down. We build our control planes to be resilient and highly available, but nowhere near the same level of effort is spent as compared to data planes. So the other category of errors that we can run into is really data and state, and when you're planning for this, you need to consider and keep in mind the cap theorem. So I'm sure everyone has seen this in some form or another. We'll just do a quick refresher. Partition tolerance is the ability to survive a network segmentation where one component of your application cannot work with or talk to the other components. In network apps we're everything's networking, you're ready you'll be talk, so you guys are dealing with networking, uh, you have to choose this, right? You can't not have partition tolerance if a user is on their desktop at home and your application is there working locally and the data is consistent and available, it's probably not the same, right? They need to be able to connect to something. So we have to choose partition tolerance, and that means that we have to tug a war between consistency or availability. Now what are these actually? Consistency is the ability, or not the ability, but the requirement to always return the correct answer. So the correct answer could mean the latest version of a file. It could mean the current balance of a ledger, or it could mean something else that requires additional state syncing before returning and saying yes, we have this. It does not mean that it will always return a response. Availability means it will always return a response even if it's wrong and wrong here could be out of date. It could be again like the ledger you could have a time from earlier when you have the latest value that you're now showing. Most people pick availability and then want consistency anyways, and this is something that when you're planning you need to be honest and say like we need to be clear that we can't have all of these things all of the time. Now there's strategies you can do to mitigate problems, and we'll talk through some of those. We just wanna kind of cover, um, cap theorem again. Now I mentioned earlier the disaster recovery planning. This is an important part of resiliency or an important part of availability, but it's not really what we're focusing right now. If you're not having any multi-region for your applications or data and you wanna start, this is a good place to say let's just ship our data somewhere, our backups into S3 in another region, and then. It's less effort. It'll be less cost overall, and the downside is the recovery could be hours to days instead of hopefully seconds in a more highly available resilient system. So data replication is something that is a challenge, and it will vary a lot based on how you or what your data is and what your users' expectations are. You can have multiple options in terms of maybe you need to write your changes to one region and maybe you need to replicate to another region before returning that right as a success. You may have that kind of consistency requirement. You may just write to the region and then have it lazy replicate or use another system to replicate, but this will help determine whether you need to have active active in multiple regions active at the same time or active passive where you have write regions and read regions. We have a handful of services that actually give you global features. Dynamo DB, DocumentDB, and Aurora Global Database are all good options. These will let you determine or configure how your data will replicate between regions, as well as monitor it and have metrics and views and insights into what's actually happening. So when we're talking about a failure and it's a regional size failure we need to fail over somehow. You've heard Felipe mention that we're doing DNS a lot. We actually use DNS for everything. I think everyone knows, you know, it's always DNS, um, but when you're. When you're, when you're actually shifting traffic, we're gonna be, we're gonna be actually changing DNS records or updating them or letting the health checks dynamically change them. And when you fail between regions or you're sending traffic between regions, you're going to be using the AWS backbone. The backbone has multiple layers of encryption. It's DDoS resilient. A lot of the hardware we designed and built ourselves, a lot of the fiber we laid ourselves, uh, and it is highly scalable and has a lot of good features, and you'll be shipping traffic between regions using this backbone. So when you're getting your traffic into the ELB, you have a few different options. And this. And Route 53 being the first one, so you always have Route 53 with ELB. It's because we're, we're a Route 53 customer. We pay them for health checks. They help check every single IP we have globally all the time, and that carries over to your DNS records when you have an ELB DNS record. You can also use Cloudfront, which will give you things like edge caching, WAF at the edge, lambda at the edge, other features that can help get you into your load balancer, or Global Accelerator. Global Accelerator's real advantages are it has static IPs. You get two IPs. They are in different, completely different infrastructure under the hood, so there's no overlap. It's another good level of blast radius isolation. They're any cast, so you can configure multiple regions as targets and let AGA or Global Accelerator figure out how to route the traffic. As well as um The static IPs that you don't have to worry about changing. So when you're actually having a failure and you need to shift traffic over, you want this again in the data plane, and you can do this with Route 53 records if you choose the failover type record. And in this example we've got a primary region and a backup region, and our primary region, as long as it's healthy, we will have traffic routing 100% to it. But if the primary region becomes unhealthy, Route 53 will detect that, flip the DNS record to unhealthy, and that will cause the traffic to start failing over to the failover region. If you have an active, active workload, you can use a weighted record, which will actually let you put in different numbers for weights for each region, and you could send in this case, 50% to the first region and 50% to the second region. It does the same thing when things fail. So if the first region goes unhealthy, it's still gonna fail traffic over, so it's 100% to the other region, even though they're both primaries. Now I put 45% of impact here because this setting is what Felipe was talking about on your target group where you can change the threshold. So you could say 50% of the hosts need to be healthy and this failover would trigger exactly like this. The default again is 100% need to be unhealthy, and we don't think that's what most folks should configure, but we hate changing defaults. So definitely go spend some time and look at changing that setting. The simple version is use the unified one and decide on what level of failure you're willing to tolerate in your application and set that. But what happens if when you shift this traffic over your new region gets overloaded? We all know that when errors happen, clients sometimes go into connection storms, reconnecting rapidly, sending more and more connections, increasing load, and making the outage worse. If you're having an outage and you fail over a bunch of traffic from one region to another, or even if you're just getting too much traffic that you might be entering a state of congestive collapse where the problem creates more of the same problem. In this case, our backup region is still healthy, but it's not 100% healthy, and we know it's because of load, so we need to shift some traffic away. You can use DNS load shedding. So if you use a weighted record, you still get those health checks if you check, evaluate target health, and in this case we've got EB1 record, uh, it's showing you'll be 1, but you would replace that with the same thing. The EOB 2 record is now a new load shedding record. It's essentially going to have its own weights, and it's going to say somewhere, in this case a null route to 00. is weight 0 and the main site weight 100 and then if that main site crosses its unhealthy threshold or if you want to go change it so that it is going to happen sooner, you're going to manually configure it and say shift traffic to this. In this example we're going to send 20% of traffic to a null route. Clients get this, they won't be able to connect to your site, but your site can then have a reduced load and preserve the experience for some of your customers. It's not always the best option, but in a lot of cases this is what we can get you out of. OK, we're having a problem, we're in congestive collapse. We need back pressure or load shedding. And then after your region goes healthy again, you can update the record and change it back, so you're 0%, and in this case we're still shifted over to the backup region because their primary region is still unhealthy. These are all Route 53 records that you can configure, and there's more of them, including latency or geo records to say that people from this geography will go to this place. These records are very useful when you're creating your global infrastructure because you're doing multi-region. Your clients are gonna be closer to some regions than others. You wanna configure your records to ensure they're sending traffic to the right region. So let's go to the big thing that we've talked about. Most failures are caused by humans changing things in production, so we need to try to be as careful as we can. What does that mean? What are the things we can actually do to mitigate these configuration changes and deployments? The biggest thing is testing. Now I'm sure everybody has good testing hopefully, um, but this includes integration tests. It includes the unit tests that run when you check things in, and it includes automatically running this test at each stage of your deployment or of your environment. Uh, the other big thing is change management. So we do a lot of automation and I'm sure you guys do a lot of infrastructure automation. Those kinds of things are allowed to continue on their own because they have their own systems for detecting failures and mitigating things. But when a human's gonna change production, we require to go through a strict change management process. You're gonna. Document everything that you're going to do and then you're going to run it step by step and that increases visibility and that increases the ability to say when we're manually changing things we're not doing it without asking a bunch of people to review what we're going to do and getting more people so we're not in the heat of the moment trying to change production. What does that actually look like when you're deploying? So you've got, um, code pipeline is, um, basically the, the, the thing we use. You've got a pipeline of deployments where you're gonna start with some pre-production environments. So someone's gonna write the code and check it in. It's going to get built by a system and pushed to the first stage. At each stage as it deploys, you wanna have multiple things that are checking and allowing you to forward that deployment to the next stage or do the deployment itself. That includes all your testing, so we run integration tests in every environment in every region before any deployment separately from the previous runs, and that'll detect things like somebody changed the value in that region or there's a different thing that was built differently when the region was originally built. You wanna make sure that you have a rollback alarm that's going to, if that triggers you automatically roll things back. You wanna make sure that's green before you deploy. You wanna make sure your dependencies are all green, and you wanna make sure that the time of the deployment is OK within a schedule. So a lot of folks. A lot of folks schedule that so they have deployments in the middle of the night or on the weekend. But we don't really have a weekend or a middle of the night when our customers aren't using our services, so what we actually do is we say we want you to deploy in the region during the daylight hours. We want the people who get paged when the human change breaks something to be awake, alert, at their desks, maybe already ready to go and able to start investigating and not page somebody at 3 a.m. to wake them up and then they have to wake up and figure out what to do and we wanna make sure that um they're prepared for that. So at each of these phases we're gonna run those tests and then propagate to the next phase after the deployment completes. Alpha and gamma, we generally think of as like an alpha is usually just the team's changes with production from every other service that you use, and then gamma is usually the the services integration where you've got all the changes that your whole service is making. Sometimes there's more pre-production stages. Um, once you get to your one box stage and one box is just a name, it doesn't always mean one box. If you have a fleet of tens of thousands, one box may be dozens to hundreds of actual application instances that get updated as part of this. It needs to be statistically significant enough that you can detect the vast majority of problems with. The majority of your users, so look at how much traffic your application sends and pick a, pick a number here to say that's our one box. A good answer is one if you wanna know. Uh, and then when you go to the production, we start zonally, so we'll do 1 AZ. And we'll go to the next zone and then as we grow this out it just scales we can start touching multiple zones or multiple regions in the same day, but this is another place where having alignment is a big deal, like having a rule that says you will not deploy to two AZs in the same region in the same day means that you can look at this, or it's even more crazy version this, which is not the smallest pipeline I've seen at AWS. But you get, you get a simple understanding of, OK, I know that none of those columns, even though the labels will actually say wrong because I didn't edit every one of them, but, um, the labels will actually, or not the labels, the, uh, you know that each of those deployment stages won't be the same region twice in a day. And if you see that, you wouldn't, you hopefully nobody would put it into the pipeline, but if you see it, you know immediately that's a problem with my configuration and planned deployments. But this scales and we do deployments where we fan out after we get more and more confidence that we are detecting or we would be detecting any issues that are occurring from our deployments. So another concept that's very useful is graceful degradation. When you've got a failure and you're mitigating it or you're reacting to it, you want your application components to continue to perform a subset of the core functions, even if the dependencies become available. A good example of this is the Dogs of Amazon page. I'm sure if you guys have heard there's this website, Amazon.com. You can go buy things on there. Um, and if you run into an error, the graceful degradation that the application sometimes does is returns you a picture of somebody's dog who brings it to work, and first of all, you get a cute dog usually and say, you know, this is, aren't they adorable? Um, you also get instructions what do you do next? Go back and try again. You also get clarification that this is on our side. We had a problem. We got a 5XX instead of just giving you a generic error where you don't know what to do, we've given you some instructions what to do. There are many other kinds of graceful degradations you can use, but the key concept is you wanna have. Part of your functionality or some level of information to help the users or whoever is interacting with what's failing to have a way out of it without having to panic or open a support case or complain maybe they just retry every time I've seen a dog of Amazon. Uh, a retry has fixed it, but if it doesn't, you'll still keep getting the same error, and that error is a big part of what's gracefully degrading, right? The website didn't just return nothing, it returned something, even if it's not what you were actually looking for, but you, you have a step or you have an action that you can do something. Another example of this is you actually scale in the services you're running, so there are customers who have multi-region, but the backup region is not running the full application, and it's not expected to. They're expecting that when they fail over, they're going to have a subset of functions available. So if you're a bank and that's the ledger and you want. Say we always want to have the right version or the right balance for it. You're gonna make that right, go to the first region and then to the second region before saying yes, we accept the right. That way if you fail over to the second region, you can have strong consistency knowing we have the value because we wouldn't have accepted it if it wasn't already replicated. But other features may not be there in the second region, right? Maybe I can't go make a payment or maybe I can't go and, you know, file something else like there's a bunch of things you can not have in your backup region as you build into this more and more. Now, of course you can replicate everything, run it in. Other region and have everything fail over smoothly and have all of the features it's just this saves you some money as well as complexity if you know that during a failure this is how the system's going to behave and everyone's aligned and agrees that's the right thing to do, it's gonna help you out a lot and save some money, uh, but when you're thinking about this, you're gonna be prioritizing, you know, what, what are our users really care about what's the critical service, and make sure that they're informed. So an example for our availability in our cap theorem, if you have a ledger and you have the latest value, you could show the time where you have that value from, and you could have that cached on the client. You could have that in the target that gets it from the database but doesn't have it in the failover database and giving that information to your users. can help them understand, OK, I know that that transaction isn't there yet, and the time shows me, oh, we don't have the latest version, and you probably also want to say something about we're experiencing a failure or we're running in a degraded state, but it's something that gracefully you can help your users have a better experience or continue to maintain function during a failure. Um, one thing that that a lot of customers do, but honestly the majority don't, and I think it's undervalued, is toggles in circuit breakers. So toggles in your features would be we're deploying a new feature, we deploy the things that make it work, and then we watch to make sure it's going to work the way we want, but we can turn it on and off without having to go and do a full deployment. Toggles can help you disable a feature that maybe it's dependency is having an issue. Um, again, prioritize business functions, resource intensive features under load, maybe you don't wanna do that extra widget that costs a whole bunch of CPU when you're running in your other region. Those are the kinds of things you wanna think about and remove from your part of your failover. Queuing also works well, um, serving cache content. So if you're. In a standard application and your client has code that you own, you can go change this and and add things you wanna make sure you're following all the TCP best practice Felipe talked about, but you could also have local versions of, say, an error page, something like the Dogs of Amazon page, or the latest version of the values that you're going to request, just cached and then display them. So you can gracefully degrade right there on the client if you own it. At the target group, the health check configuration we talked about, you can use that to fail away sooner and make sure that your, uh, target, your, your experience is preserved for your users, whether that's failing open and sending traffic at the target group or failing away. Load shedding, uh, is a useful thing that gets you out of congestive collapse, um, something that definitely is a very useful thing to have happen or to, to be ready for in case you get a surge in traffic, uh, and then rerouting traffic to alternate regions is another way you can gracefully degrade and have your lower experience or your reduced experience application or your full full application. So let's go back over to our checklist, um, multi AZ resiliency, so use multiple availability zones, pre-provision, so you can have static stability. Maintain headroom at least one AZ worth of capacity. Use DNS everywhere at your client. Make sure you're honoring DNS TTLs. Make sure when your clients are having errors they're reconnecting to an IP that didn't have the error and think deeply about your health checks. Configure them so that you'll fail away when you don't want to send when the resource that's in the health check would have, you would, sorry, the resource in the health check that you're actually health checking, should it be in there or not, you determine that by saying if it's failed and I cannot respond. Would I want that response to go to the, the customer, and if the answer is no, I wouldn't, then that should be in the health check. And as that gets more expensive, you're gonna want to use strategies like the health check separate asynchronously from it getting served directly to the health checker. On the, on the multi-region side. Again, the biggest things, keep it simple and align everyone are the two main things that I really want people to think about. When you're doing these things and you're making these decisions, you should not be doing it isolated. You should be doing it as a group so that everyone understands this is the expectation during a failure. That's it. Thank you all so much for coming, really appreciate the early morning rally and uh don't forget to fill out the survey in the app.