---
video_id: ZHF0oNwNhig
video_url: https://www.youtube.com/watch?v=ZHF0oNwNhig
is_generated: False
is_translatable: True
summary: "This session, titled \"Building scalable applications with text and multimodal understanding\" (AIM375), unveils the transformative capabilities of the newly launched Amazon Nova 2.0 foundation models, designed to help enterprises unlock the massive potential of their unstructured \"dark\" data. Dinesh Rajput, Principal Product Manager at Amazon AGI, opens by addressing a critical enterprise bottleneck: while organizations possess vast repositories of images, videos, audio, and complex documents, they typically only leverage text and structured data due to the high cost and complexity of stitching together fragmented, single-modality models. The Nova 2.0 family—comprising Lite, Pro, Omni, and Sonic—shatters this paradigm by treating all modalities as native, first-class citizens. Rajput highlights three core optimizations for document intelligence: \"robust real-world OCR\" capable of handling tilted or handwritten scans; \"mixed context understanding\" that seamlessly interprets charts and tables alongside text; and \"schema-driven extraction\" that enforces strict JSON outputs for immediate database integration. He emphasizes the models' massive 1-million-token context window, allowing for simultaneous processing of up to 90 minutes of video or hundreds of document pages. Brandon Nayer, Senior Product Manager, then takes the stage to explore Nova's breakthrough visual and temporal reasoning capabilities. He demonstrates \"Visual Perception\" through a living room analysis where the model accurately identifies and bounds objects even when partially obscured, and introduces \"Temporal Understanding\" as a solution to the complex challenge of video analysis. Unlike traditional methods that rely on unscalable human annotation or disjointed frame extraction, Nova processes video natively, understanding the causal flow of time. A live demo impressively shows the model pinpointing the exact timestamps (within one second) of specific events, such as CTO Werner Vogels standing on a boat, across a lengthy documentary. The session also introduces \"Amazon Nova Multimodal Embeddings,\" a state-of-the-art model that generates unified vector representations for text, images, and video in the same semantic space, enabling powerful cross-modal search (e.g., text-to-video retrieval). Tyan, representing Box, concludes with a compelling customer success story. She details how Nova's embeddings have revolutionized Box's ability to index and search petabytes of unstructured client data—ranging from 80-page engineering reports to decade-long video files. She shares specific use cases, such as an architecture firm automating project summaries from complex mixed-media reports, and a media studio solving continuity errors by instantly searching video footage for the precise placement of a coffee cup on a set, effectively proving that the \"Electric Age\" of multimodal AI has arrived."
keywords: Amazon Nova 2.0, Multimodal Understanding, Temporal Reasoning, Visual Perception, Multimodal Embeddings, Document Intelligence, OCR, Cross-Modal Search, Unstructured Data, Video Analysis
---

I know. All right, let me start again. Good morning, everyone. Um, first of all, thank you for being here. Um, as a very quick introduction, uh, I'm Dinesh Rajput. Uh, I'm a principal product manager with Amazon AGI. AGI stands for Artificial General Intelligence. Uh, it's an organization within Amazon which builds first party foundation models called Amazon Nova. Today's session we'll talk about how you can utilize data beyond just text. Things like images, documents, videos, audio, call recordings to build really accurate, context-aware applications using Amazon Nova Foundation models. I'm also joined by my colleague Brandon Nayer here who will talk a little bit more about image video understanding. And we also have one of our customers, Tan, who represents Box, who will talk about how they use Amazon Nova models to improve their AI workflows. Let's get started. So we'll, this is the broad agenda for today. So first I think we'll just talk about what are the enterprise needs today when it comes to multi-modal data, what are the different challenges that customers face. Then we'll talk about a quick overview of Amazon Nova, two models that we just introduced yesterday. Then we'll do a deep dive on how we have optimized these models for document intelligence use cases as well as visual reasoning use cases. Finally, we'll talk about multimodal embeddings and how they can help you search and retrieve all of these multimodal data across your enterprise. And finally we'll have a customer success story of how Box is using Amazon Nova models to power its AI workflows. So let's talk about data first. Today, organizations have immense amounts of data, text, structured data, contracts in shared drives, videos, call recordings of customers. But if we are honest, we use a very small sliver of that data today. It's mostly either text or some sort of structured data that you have in your database tables. This is the only thing that we practically use in most of our AI workflows. A lot of these other multimodal data. Somehow gets unused and it's not really contributing to our AI applications. But multimodal foundation models are really changing that. They let you see what's inside an image, what are what's happening within the different frames of a video sequence, what is the customer saying or feeling within a support call, and you can use all of this data together to reason over all of this data to deliver customer insights and improve your AI workflows. We've been working with customers all along, but when they try to use multimodal data, there are 3 key challenges that they face. First, there are many separate models and tools. What I mean by this is you need one model to process text, you need another model to process structured data. You need a third model to process images, and then you might need a 4th model to process videos. This leads to a multitude of problems. First is you are forced to stitch together these different sorts of tools, which makes this entire process very costly and complicated. The second key problem that happens is because you have these different set of models across different modalities, it's very difficult to put together all of that context and reason over all of these modalities together and actually deliver customer insights. So think about what you read in a document and a customer call and putting that together to solve the real customer problem because of these different models, that's not really feasible today. And the third thing is a lot of these models are not super accurate, which forces you to have a human in the loop, and you know this manual checking doesn't really scale, which, which leads to a lot of cost issues and efficiency issues when you're sort of trying to deploy these deploy these AI workflows. We launched Amazon Nova 1.0 models at reinvent last year. We've been working with a lot of our customers over the last year. We have had amazing response with tens of thousands of customers using us. We have heard some similar feedback from all of our customers as well, and with this to solve these customer challenges, we have introduced Amazon Nova 2.0 models that were just launched yesterday. One of the key fundamental things that we designed for Amazon Nova 2.0 models are for is to treat all the modalities as a first-class citizen. So when we designed Amazon Nova 2.0 models, we made them natively multi-modal, and they are able to process text, images, videos, audio, speech, as well as generate text and images. We have a variety of models to cater to your different cost, latency, and accuracy profiles. So we have Amazon Nova 2 Lite, which is our fast, cost-effective reasoning model for most of your AI workloads. We have Nova 2 Pro, which is our most intelligent model for your highly complex tasks. We have Nova 2 Omni, which is our unified model which can not only understand text images, videos, audios, but can also generate text as well as images. And then we have Nova 2 Sonic, which is a conversational speech to speech low latency model. And then finally we have Nova multimodal embeddings. This is really a model that can create embeddings across all of your modalities so that you are able to implement any sort of search or retrieval on all of your enterprise data. Very quickly going over some of the salient aspects of these models. So the first thing that we did is we designed these models to have 1 million of context window. Just to put that in perspective, it means it can process 90 minutes of video, hours of audio, and hundreds of pages of documents all in one go. We have made these models to be multilingual in nature so they can process 200+ languages, and on the speech side we have optimized them for 10+ languages so that your solution really scales globally. And one final part is we have also included reasoning within these models. So Nova Lite, Pro, and Omni all come with reasoning enabled so that you can reason over all of the data, whether it's text or multimodal data together. And then we have also optimized these models for some sort of tool calling so that you can implement agentic workflows using all of your multimodal data. When launching these models, we made sure that they are frontier intelligence on multimodal tasks. So here are some benchmarks that we have presented. These are mostly multimodal benchmarks. So TripleMU Pro is a complex visual reasoning on images. We have document OCR, whether you're doing optical character recognition or extracting documents of handwritten text, slides, what have you. We have real KIE, which is a key information extraction. So any sort of tables and you want to extract values out of it. We have QB highlights, which is our video understanding benchmark, and then we have Screenpot, which is essentially any kind of UI browser tasks. So as you can see, Nova Light and Omni perform at the frontier compared to their peers in this category. And then Nova 2 Pro is also an extremely competitive model that we have introduced in preview. Um, we will, we will potentially GA this, and this also shows extremely competitive performance on multi-modal tasks as compared to its peers. Awesome. Now, let's get into the deep dive around document intelligence. So when it comes to document intelligence, we have heard from a lot of our customers that the two primitives that they care about are optical character recognition and key information extraction. And we have made sure that Amazon Nova multimodal models deliver absolutely the best performance on these two key primitives so that then all of the developers as well as enterprises can build on top of them to enable their different AI workflows. So here is a quick example of a Nova output from an OCR text. So as you can see, we have optimized these models around 3 key parameters. One is robust real world OCR. What I mean by robust real world OCR is a lot of documents that you get in enterprises may be handwritten. The scan quality is not great. Sometimes the documents are a little tilted, so we've made sure that we have optimized around all of these real world scenarios so that the information that you are able to extract from documents is super accurate and you need the minimal amount of work. Second is mixed context understanding. Um, it's very simple for models to just extract a chart or a table or a text separately, but most of our real world documents have all of these things put together, so we've made sure that the model also excels in, in, in understanding these different set of context, whether it's charts, tables, or any sort of text so that you are able to deliver this performance together. And finally structured output. We've also made sure that our model is able to produce the right structured output whether it's in JSON, HTML or XML format, and so that it's machine readable and then you can sort of extract all of this and put it in your databases so that you can further process this. So these are the three key things that we made sure that our models really excel at. And then the second task is key information extraction. Here we have tried to optimize the models around 3 key things. First, schema-driven extraction. So any sort of schema that you specify to the model, um, maybe you want to extract only 5 rows, or maybe you want to extract all the rows, or maybe you want to label certain rows in a very specific manner. We have made sure that the model does instruction following around what is the schema that you want to define, what sort of indentation you want, and what is the output you want. Second is layout of our text extraction. We have seen in a lot of use cases that the layouts, especially in complex layouts, there is a lot of interpretation that the model needs to do. So we have made sure that we are covering all of these long tail use cases of really complex real world tables which require a lot of human interpretation to actually extract the data and make sense of it. And finally, as I was talking about, all of our models also have reasoning, reasoning capabilities within them, so you could also extract all of this information. Um, from these documents and sort of reason over these documents to see if, if the model has extracted it correctly or even fundamentally if the data is correct or not. As a very quick example, you might extract zip code from a document, but you need some model intelligence to know that zip code should be 5 digits and not 4 digits, and that might be an error in the form. So you just don't want extraction, but you want intelligent extraction within those documents. With that, um, thank you so much. I'm gonna hand it over to my colleague Brandon, who will talk a little bit more about image as well as video understanding. Thank you. Not working. It's working fine. Hey folks, how's it going? Um, My name is Brandon. My name is Brandon Naya, and I am a senior product manager on Amazon AGI similar to the team that Dinesh is from. Dinesh has given us a pretty great deep dive on how Nova 2 models can be applied for understanding use cases, but I'd like to expand that a little bit further and showcase how those capabilities can also solve for image and video understanding use cases. So vision understanding is a foundational capability that enables language models to really Understand the world not just in the form of pixels, but similar to the way that humans do. So, really understanding the context and the meaning behind those images and the video. Nova 2 models process all images as well as video content in their uh in their unstructured format so it processes it natively and it converts it into a structured output that could be a text written as in a JSON format. And that text can be used by customers to easily search, to categorize, as well as to generate rich business insights from that visual content. And so some examples of what these insights could look like could be identifying key objects in uh an image or a video. It could be around um identifying and extracting text overlays um in a video uh through optical character recognition. It could be around understanding, um, you know, temporal relationships or causal relationships within a video, or it could be around generating rich captions that can be used by a customer to really understand what is the content that is contained within that particular video asset. And so We are super excited about the Nova 2 models that have come out, and from a vision understanding perspective, you can break these down into the improvements that we are bringing through with the Nova 2 models. You can break those down into 3 Uber capabilities that we are trying to solve for based off our customer needs. The first is on vision perception. The second is on reasoning and scene semantics. And thirdly, we have temporal understanding, or really the understanding of how time influences the understanding of a video. And so let's dive through those, um, one by one. Visual perception can really be seen as, you know, the model's eyesight, right? How adept is a model at taking in a particular image or a video and understanding all the elements within those videos or that image. Understanding attributes associated with each of those elements, such as the color or the shape or the count, as well as understanding geo-relationships, geospatial relationships between the different elements. And so, vision perception forms the basis of your common computer vision tasks, um, such as object detection. Object detection, um, and I'll showcase an example in just a few, but object detection is essentially a, a way that you prompt a model to identify a particular object and potentially extract out the coordinates of that object that can be passed down into a downstream system for further processing. Example of this could be logo detection, where you're identifying a logo, you're passing it downstream in order to do an add attribution use case. In the example that I have on screen, we have just a standard living room, and I've prompted Nova 2 to identify, to detect plants, cushions, table, and a TV. And in the next slide I've actually overlaid the bounding box coordinates corresponding to each of those particular objects and so as you can see, firstly, Nova has identified all the objects in the screen, even the little plants that are in the bookshelf in the back, even the black TV which. Literally is represented as a black rectangle. The model has an understanding of what to expect in a living room space and is able to detect that as a TV. The second finding from this particular image is you can also see how tight these bounding boxes are. It gives you a sense of how well the model is able to look to the particular object that you are trying to detect, um, and so it is a measure of just how well the model is able to, to, um, accurately detect those kind of objects. Visual perception is really a mechanism to test how well the model can visualize and so that capability can be extended further to, for example, having the model to captions of what it's actually seeing and in this example it could be something like modern living room, it could be bright open spaces with natural sunlight coming through. The second capability that I'd like to talk through is the model has, um, as Dinesh called out, the model, each of the Nova 2 models has reasoning capabilities, and so this allows the model to extend its uses beyond just visual perception, identifying what it can see, but to actually contribute different elements together to try to make a logical deduction or to make an inference of what is actually happening. The model has consumes reasoning tokens in order to generate this kind of thinking, and within the API we provide parameters to developers that allow them to control the depth or the budget of the degree of thinking in order to solve for their particular use case. It's just another way to try to prioritize what makes the most sense for your use case. The third big upgrade that we have is around video and temporal understanding. Now, video understanding is a pretty critical use case for a capability for a bunch of use cases from everything from media asset management to semantic search to personalization and recommendations to contextual ad placements to VQA. Detection, the list kind of goes on. The challenge, however, is that videos are a really complex asset or really complex modality to try to deal with as you can see it has frames, it has shots, it has scenes, it has chapters, you may have audio transcripts in there, um, you have textual screen overlays in there. It's a really complex multimodal asset. But this is really compounded by the fact that we also need to consider the time dimension. Time is super important to understanding the context of a particular video, what's actually transpiring in a scene and what's actually taking place. And so current solutions have. Um, basically two options or currently have two options that they can utilize to try to get a video understanding, um, uh, use case solved. The first option is to have a manual annotation of the videos, and so here you can think about a team of people watching a video and uh literally just noting down metadata that is annotated to the video. This just based off the manual nature is really unscalable. Um, it depends on the degree or the depth at which someone actually takes down these annotations and furthermore, because there's a human in the loop, you get a degree of variability that can, um, differ from teammate to teammate. The second option that customers can pursue is to extract out frames from the video and to send those frames to a vision language model and ultimately try to generate metadata through that way. But this option too is flawed. Firstly, it requires customers to build out complex pipelines in order to preprocess their images, their videos into frames or images, and then plumb them into a vision language model. Secondly, just the fact that you are extracting frames and processing them separately, when you're thinking about the sheer volume of video archive you might want to go through, this could really start to become cost inefficient. And the third aspect of just processing frames is that you don't really have that element of temporal understanding which we've already covered that it's a really important way to understand what's actually transpiring in a video. Nova 2 models support video natively, and we've trained the model to actually understand the temporal, sorry, the temporal aspects of the video so it understands what's happening across the length of of frames in order to have a deeper understanding of what's happening on the on the particular video. Temporal understanding is super important because now with temporal understanding, you also get a sense of being able to ask the model to, for example, generate chapters where it could say it could provide a description from time A to time B indicating what's what's transpired within those two time frames. Or you could ask it to actually process a video and extract out the time stamps that correspond to a particular event that might be interesting to you. And so I have a demo here which actually showcases this. So this is um. This is a video, it's a sped up version of a documentary that has Werner Vogels, the CTO of um AWS, but in this video, we have a number of occurrences where someone is standing on a boat. And There's actually 4 occurrences in this particular video. I've provided that video to the Nova 2 model, and I've prompted it to extract out each of the time stamps that correspond to when someone is standing on a boat. Nova 2 was able to identify all four of the particular events when someone was standing on a boat, and not only that, but it was also able to localize that time stamp, to localize that event to within 1 2nd of when the start and end time was, which is a pretty powerful capability in order to identify different events happening within your video. I'm gonna switch gears a little bit and now talk about um Amazon Nova model embeddings. Amazon Nova Multimodal embeddings is a separate model from the Nova 2 models. It takes in text, images, documents, video, and audio as inputs, and it outputs an embedding that represents any of the components that are passed in as an input. And so before we go any further, let's define what's an embedding. An embedding, simply put, is a representation of the input that you provided to the model. So you can think about this uh this lovely picture of this Labrador. The Labrador is sitting on a beach, it has uh an ocean in the background, um, it has this blue handkerchief that's tied around its neck. All of those data, all of those elements within that image, is important to understand the overall context. And when you convert this image into an embedding, you're really trying to represent it in a machine readable format which we call an embedding that captures all of those intricate details, all of that information represented within the image. This is super important because it helps to enable semantic search applications where you don't have to rely on metadata, you can rely just on the embedding itself to retrieve the correct image. And it also helps for rag applications. So as you're thinking about building out, you know, um, you know, deep AI workflows that manage to um uh retrieve important information that might be be proprietary to your business. With an embedding model, you don't have to have the metadata, you can actually retrieve it just based off the embedding itself. So Amazon Nova multimodal embeddings, a state of the art embedding model that takes in text, images, documents, video and audio, and it outputs an embedding. It is the first model in the industry to process all these different content types and to process them within the same embedding space, so it has the same level of understanding. So in other words, if you have a text string of the word dog, if you have an image of a dog, or if you have a video of a dog, they are all represented in the exact same way. And this allows you to to expand applications to cross modal capabilities such as doing um a text plus image to image search or a video to video search. Or trying to retrieve visual documents that contain both text as well as images. Nova Embeddings offers great price performance at approximately 10 to 20% lower than your other alternative embedding models. Some of the key features of the embedding model. Is that first it does have this unmatched modality coverage. It also provides a very long context length. It provides 8000 tokens for an embedding model which is. Pretty high amounts. It refers to how much context can you bake within a single embedding and still have a meaningful representation of your input. It also provides segmentation capabilities within the API and so if you have longer text or video or audio you can first split those into smaller into smaller manageable pieces and then generate embeddings for each of those pieces. The model comes equipped with both synchronous and asynchronous APIs as well. Synchronous API handling your latency sensitive workflows where it might impact your user experience. You can think about something like someone is searching for a document, you want that to be retrieved pretty quickly. But it also supports asynchronous capabilities and so if you have a very large video file, for example, that you want to process and it's not latency sensitive, you're able to pass it through the asynchronous API and get a response at a later time once the job has been done. Lastly, the model also comes with a choice of 4 embedding dimensions, and this really gives you the option to trade off just the level of compression within an embedding against your storage cost implications. The model is trained with Matrioshka representation learning, which essentially means that it bakes in the most important context earlier into the embedding dimension so that if you truncate the embedding from the native 3000. You can still maintain a very high degree of accuracy, and sure enough in our benchmarks, we see a pretty minimal accuracy drop when we move from a 3000 embedding all the way to a 256 embedding. In the slide, I also talked through some of the benchmarks that we've developed to compare Amazon Nova multimodal embeddings against alternative models. The first thing you should notice is that because Amazon Nova multimodal embeddings is so comprehensive, we have to pick select models to compare it to because other models tend to be more specialized toward a particular modality or particular domain such as images or documents or maybe just videos. And as you can see from the slide across video retrieval, visual documents as well as text, the model does deliver great accuracy in terms of your retrieval tasks and it's a, it's a great model that we've been really proud about and we've started to get some great feedback over the past few weeks. We're excited to have more folks test it out and, you know, give us a sense of how it is um being applied within your business use cases. Now, I will transition over to Tyan, who will take us a little bit more in terms of how uh Amazon Nova Multimodal embeddings is unlocking new use cases at Box. I don't think the click is working. Thanks so much Brandon. Uh, welcome everybody. Uh, excited to talk to you a little bit about how, uh, Box specifically is using these new models. Um, so for those of you who are not familiar with Box, uh, we are the leading intelligent content management company. What does that mean? Um, companies trust us to store, manage, and share their information so that they have the ability to, uh, um. Interact with that information, share it securely with other customers or with other people in their organization, and that they can actually get really useful value out of that information. And over 115,000 organizations trust Box, and these are across many, many different industries including really high. Highly regulated industries like government, finance and life sciences, um, so obviously, uh, it's really critical for Box to be able to, um, be really secure about how we manage that content, but also we need to really be able to make sure that we're providing, um, access to the full breadth of the content. And there's a whole bunch of use cases that our customers have that they want to use this information for just across a wide variety of different verticals and industries all the way from digital asset management, insurance claims management, to real new product design and development. Now one of the big things that's always a challenge with this kind of information is that most of it is in unstructured data, right? We're talking your PDFs, your Word docs, your Excel spreadsheets, all kinds of things, video files, and traditionally it's been really hard to get access to that information. Uh, Previous, uh, you know, sort of, uh, paradigms have really focused on structured data, being able to do database queries. That's not super useful to a lot of our customers because so much of the information that they need is in that 90%. So it's a lot of really untapped value that we want to be able to. Unlock and obviously AI is a is a huge um way to do that so that's where Box AI comes in um and that's the particular product that I work on. So this really is the the platform that we built that uh allows uh Box customers to use AI and apply it to their content um. And uh we've had things out for uh you know a few years now, but we're continually improving and and making sure that we can provide even more value to that content, not just being able to find things, ask questions, but also take that information um that you get out of that content and use it to uh create new assets or power workflows. Right, so a couple of um just key use cases, right? I talked about the very first thing that people like to do is really just being able to generate instant insights from content. Um, how many of you have had a time where you have a really long document, uh, that you probably need to read before a meeting and you just need to know a few things? You don't have time to read the 50 page document, uh, being able to just ask questions really quickly on that is super, super helpful. Hang on, there we go, um, and then obviously once you have that being able to get that information out of there, um, and do interesting things with it so for example if you have a long document and you know that you need to be able to um search for it later you can extract that information, save it as metadata, you can save that back to the document or. You can take that information, for example, if you have a loan document, you want to be able to pull that information out, you can then go and take that metadata and put it in another system as well. So really being able to take that information and spread it across not just the box ecosystem but across your entire ecosystem so that you can really get more value. Um, and of course the real thing that we're really focused on is being able to to use that information to automate workflows. It's great to be able to go and read through a document or look through a video and find the information that you want, but what we really want to do is to be able to empower our customers to actually take that information and use it to power new processes so that people don't have to do that manual step and it just is done automatically. OK, so this is not a sales pitch. I just wanted to set some context for what Box is and, and why we care about this stuff. Let's actually get into the details, uh, of what we're trying to do. Um, so let's go back to that, you know, 90%, uh, of content that is unstructured, right? That's a lot of content across many different file sizes and file types. And in order to access all of that, we need to use rag, right? Um. If we have, you know, a 10 hour video, there's, you know, we can't just plug that into a model and be able to get the information. We need to use rag to do that and especially when we're talking about comparing across different documents as well. The real challenge is current models tend to just be very text focused. There's some models that do images as well, you know, we talked a little bit earlier in the talk about how sometimes you can kind of stitch things together, but it's a real challenge to be able to access anything beyond the text. And so this is where, you know, we're trying to figure out how to solve this problem. For our box customers this is really really important. Um, there's a lot of content think your PDFs, your CAD files, your presentations we can access the text of those things but we can't access necessarily all the embedded images, the charts. The graphs and so we're losing a lot of context because we're only looking at one dimension of that file and these kinds of files make up a huge percentage of the files that our customers store in box so we're losing a lot of information. Um, and then obviously we have a lot of video files, audio presentations, and PDFs. These tend to be very disproportionately large files inbox. Um, we actually have a customer that has a video that is um. More than 10 years long. I don't quite know how they've managed that, um, but as a result, you can imagine we have some very, very large files within Box that we have to manage. The only way to be able to do that and get value out of those really large files is using rag. Right, we've been trying to solve this problem for a while. We've done a couple of different things, right? Obviously the first thing is to convert audio um from your video and your audio files into text. There's very good existing models that can do that, um, and I think this is uh what a lot of sort of um. You know, multimodal models work right now where really all they're doing is extracting text and then they're doing the embedding on the text and that allows them to be able to, uh, you know, really search across um text files, audio files, video files, right? The other thing is human annotation, right? This is a very traditional approach. This is what we did even before AI. If you wanted to get information out of a document, you hired a person, you told them go and find these things and they would go and they would annotate it. So we've tried both of those. But obviously there's some big limitations to those you can imagine, especially when you're talking about human annotation super difficult to scale that it's very, very expensive. You have to hire people who are experts in a particular field, so it's quite challenging and then both approaches are quite slow. Transcription is getting better and it's getting faster, um, but especially human annotation takes a really long long amount of time and no matter what approach you use your potential for search is really limited to whatever keywords you're extracting from human annotation or what is specifically in that that uh transcript that you're getting from the audio or the video. And as a result, really important context is lost as part of that process. So up to this point, even though we've been trying to solve this particular problem, we haven't really found a great solution. Right? And of course the whole goal is to be able to look at more than just text. We want to be able to look at all the information within a document together at the same time. And we want to be able to look across all different file types. I always tell people, uh, and it's really true, my team's goal is to be able to, if you store that inbox, you should be able to use AI on it. And that's not just you can use AI on a file, you should be able to use AI on the entire content of that file, and that's what we're working towards. All right, so this is where we get to the the new Nova multimodal embeddings model, and this really has just been a game changer for us, right? This really is going to allow us to unlock a lot of this stuff that we've been trying to do. We finally have a single multimodal embeddings model that handles all content type, not just text, and it's not pulling, uh, you know, a transcript and doing it. It's. Actually pulling both the text, the image, the video, everything from whatever that file is, so we have all that information and that means we're not losing that critical information that might come from the non-text pieces in that file. So for us that is just huge and that means we really can look at documents that. you know different kind of file types within a document as well as looking across the entire depth and breadth of the files that are stored within box and of course all that's great that unlocks a ton of new use cases that we just couldn't do before and it's an added bonus for us, you know that that's very fast, it's very scalable and very affordable, which is great for us. All right, so I'm gonna give you some real examples here of customers who have actually used things like this. So we have a leading engineering and architecture firm. They do a lot of materials testing. They get these reports every month. They're super long. They're like 80 pages, lots of really in-depth technical information talking about the results of all the tests that they have done. Previously, a customer or you know they had to hire somebody at their company to go through and read through that report and pull out say project specific information for different projects to be able to look at anything that came out of those results that might need. Additional, um, you know, some sort of additional action and so it's super, super time consuming to be able to get the information out of that report. There also are often attached videos or images, for example, if somebody is trying to look at a particular. You know, a room that's been built, you might want to be able to take pictures of, of what things look like and then see does that actually match to the specific requirements that you have and it all required a human to do all of that, but with this new embeddings model, this customer can now create an agent that can go through and it can look across all that. Information and it can really pull out that that the insights that they need in project specific summaries as well as summaries for executives and a list of action items that they need to take based on that report so that they know what to do next and this is huge for them and it just saves days and days of work. You know, just all by being able to do this. Uh, let's talk about a fun one too. Um, we, we definitely have, um, some really cool, uh, media and entertainment use cases here. You can imagine they use a lot of videos, a lot of audio. Um, we have a very large production studio that is a customer of ours, and one of the things that's a real challenge for them is continuity checks. So, uh, you can imagine, uh. It's really complicated when you're filming something uh you have many people uh working on different sets at different times it's quite complex and so you might film a scene or a few scenes at one time you have everything set up um in your location and then for whatever reason you might then need to, um, tear that set down, go someplace else, and then come back to that location for at a different time. You need to set everything up to be exactly like it was before and in the past that meant some poor person had to go through the uh the video from that previous uh you know time that they were on that set and look at where were all the objects, what was all the context. And then go and set the set up to match exactly what it was last time super super time consuming especially because depending on where your camera is, you know, it's not like you can look at the last shot you have to go and look at all the different shots so that you have the whole set set up the correct way. This is great because now they can just go and use this to search the video and really be able to find that complex information right down to find where that coffee coffee cup was located and what direction was the particular writing oriented so that they can get everything set up looking exactly like it was when they finished so this just saves them hours and hours of work, so we're super excited about this one as well. So that's great. We're really excited. So what's next? We're doing a lot of really great stuff with the teams here at Amazon to really start, you know, getting this in the hands of our customers. We've proven that it works. It solves our use cases. So now we just really need to make sure that all our customers have access to this. So that's a big effort for us now is to really start applying this embeddings model at scale. In our production environment, uh, we're also working on testing and integrating those uh Nova 2 models that were just announced. My team is actually actively working on testing them right now and, and looking at them on our use cases so we can see uh where they really do well. And then one thing that's really great is we're now able to extend the kinds of things that Box AI can do to new use cases. And every time we have some new capability that comes out there's always new use cases that come up that we just couldn't do before and so we want to be able to really start figuring out now that we have this new capability with these new embeddings models to be able to look at all this different content, what new kinds of things that can we do and so there's a lot of really active work happening right now to see what new use cases we can apply these models to. All right, uh, and that's it. Thank you so much everybody for joining us. um, we really appreciate you, uh, coming out and listening. um, please make sure that you complete the session survey it's in the mobile app, um, and for those of you who have questions, we'll be off to the side over here to answer questions for a few more minutes. All right, thanks so much everyone.