---
video_id: Gcje6pRGr1g
video_url: https://www.youtube.com/watch?v=Gcje6pRGr1g
is_generated: False
is_translatable: True
summary: "This session, titled \"Improve agent quality in production with Bedrock AgentCore Evaluations,\" introduces a comprehensive, fully managed solution designed to address the critical \"trust gap\" that currently prevents many enterprises from deploying autonomous AI agents into production. Presented by Amanda Lester, Vivek Singh, and Ishaan Singh, the talk begins by acknowledging the transformative potential of agentic AI, where agents can reason, plan, and execute tasks autonomously to solve complex business problems. However, the speakers emphasize that this autonomy brings a non-deterministic nature to the software, creating unique challenges where agents might fail silently—through hallucinations, factual errors, faulty reasoning, poor tool selection, or security vulnerabilities—without triggering traditional operational alarms. Such silent failures can lead to severe consequences, including the erosion of customer trust and potential revenue loss, often only discovered weeks later through user complaints. To combat this, the team unveils Amazon Bedrock AgentCore Evaluations, a service that integrates quality assessment directly into the agent development and operations lifecycle, operating in distinct \"Online\" and \"On-Demand\" modes for continuous surveillance and CI/CD integration respectively. The technical core of the service relies on using Large Language Models (LLMs) as judges, providing 13 built-in evaluators organized across three granular levels: \"Session level\" for overall goal success, \"Trace level\" for response correctness and faithfulness, and \"Span level\" for tool selection accuracy. Beyond these metrics, the platform supports \"Custom Evaluators\" created via structured rubrics, enabling checks for domain-specific criteria like regulatory compliance. A distinguishing feature is that the system provides detailed, natural language explanations for every judgment, enabling developers to understand \"why\" a response was scored a certain way. The session brings these concepts to life with a detailed case study of the \"Wanderlust Travel Platform,\" where the tool identified a critical drop in tool selection accuracy in under five minutes—a task that previously took seven weeks of manual diagnosis. Ishaan Singh’s live demos further illustrate how to set up these pipelines and use the observability dashboard to pinpoint issues using the \"LLM judge\" reasoning. The presentation concludes by outlining best practices, such as defining multi-dimensional success criteria and adopting a mindset of continuous statistical monitoring to ensure agents remain reliable and effective as they scale."
keywords: AgentCore Evaluations, Trust Gap, Silent Failures, LLM Judge, Continuous Monitoring
---

Hello everyone and welcome to Amazon Reinvent. Um, it's great to have you all here. My name is Amanda Lester and I am the worldwide go to market leader for Amazon Bedrock Agent Corp, and I am joined today by two of my esteemed colleagues, uh, Babek Singh, senior technical product manager for Agent Core, and Ishaan Singh, uh, senior Gen AI data scientist here at AWS, and we are incredibly excited, uh, to be able to present to you today, uh. And we're gonna discuss how you can improve the quality of your agents in production with Amazon Bedrock Agent core evaluations, which we just recently uh launched during the keynote. We're incredibly excited to present to you what we have developed for agent evaluations, which we believe is gonna fundamentally help you to improve the way that you do business. And in today's session, you're gonna learn a couple of things. First, you're gonna learn about Amazon Bedrock Agent Corp. We're also gonna discuss some of the key fundamental challenges that are associated with operating your agents at scale in production. Third, we're gonna provide an overview of an introduction of our solution that we've built to uh to address some of those challenges, which is agent core evaluations. Fourth, we're gonna provide a couple demos of our solution. And, uh, finally, 5th, we're gonna provide some best practices and resources that you can use, uh, to be able to get started evaluating agents and get those agents that you've built into production much faster and quicker than ever before. What is incredibly exciting about this year's Amazon reinvent is that we are at the edge of another technological revolution which is unfolding right before our very eyes. Um, this future is happening now. This is not, uh, a dream. It's, it's happening now and developers all around the world are empowered to be able to reimagine customer experience. and also are leveraging agents today to improve their operations more efficiently and effectively and this is happening across all virtually across um every single industry, every size of company from startups to enterprises and all around the world and in order for developers to be able to take advantage of the benefits of agents. Developers need to have the confidence and the right foundational set of services and tools to be able to bring those agents into production, and one of the uh ways that AWS is helping to get those agents into production faster for you is we have built Amazon Bedrock Agent Corp, which is our uh most advanced agentic platform. And it provides developers with everything that you need in order to get your agents into production faster, and this includes a comprehensive set of services to be able to deploy and operate your agents at scale. In a secure manner, and you can leverage that with any framework, and any model. Agent Core includes a foundational set of services that you can use to run your agents at scale securely and uh and uh and at scale. And this includes a set of services that you can use to enhance your agents with tools and memory. It includes purpose-built infrastructure that you can use to deploy your agents at scale, and also controls that you can leverage to gain insights into your agentic operations. We built Amazon Bedrock agent core to be extremely flexible. We understand that as an agent developer, that you want the choice and flexibility to build with open source protocols, such as the model context protocol known as MCP and A2A uh for agent to agent intercommunications. It's also incredibly important that you have the choice to be able to build your agents with the right frameworks of your choice, which is why we've built Agent Core with the flexibility to be able to leverage any agentic framework of your choice. And once you've built your agent with your agentic framework and you're ready to go and deploy your agent into production. Securely and at scale, you can do so with the right confidence and the right set of foundational set of services to be able to uh do so um with trust. But we are not done innovating on your behalf. Agents are non-deterministic by nature, which means that as a developer, there are an entire new set of services that are required to be able to handle the non non-deterministic nature of agents. What do I mean by non-deterministic? Agents can reason and act autonomously. This is incredibly exciting because this means that we agents are fundamentally changing the way that work can get done. We are moving to a world where agents can go off and do work on your behalf. And they can do this to achieve a specific goal, a specific task that you set out for them to be able to do, and they can do this all without direct supervision. They can reason, they can create workflows to solve a problem. And they can make decisions, all without direct supervision. And this is, uh, incredibly exciting and powerful because it means that it can free you up from mundane tasks and it frees you up from this busy work and you can then leverage that uh extra time that you have to work on higher value strategic strategic tasks. But because these agents are autonomous, which is, uh, what makes them so powerful, it is fundamentally critical that you, uh, can trust that the agents are going to perform their jobs correctly. The fundamental question that every single person in this room needs to answer if you are gonna leverage agents is can you trust this powerful new digital workforce to be able to do their job correctly? But the problem that is keeping up developers and CTOs up at night. Is can you trust that these agents, because they're autonomous, and you're gonna hand over mission critical business processes and tasks to these agents, that they will do that job correctly, efficiently, effectively. And will provide a good experience for your customers. Can you trust that the agent is going to go in and address uh and provide an optimal solution? It's not enough for the agent to just go in and produce an answer. That answer needs to be the right answer, the correct answer, and an accurate answer. Because if the agent provides a wrong answer, it may cause more problems for your customers, your users, your company, and your developers. So the fundamental benchmark now for getting an agent into production is trust. Agents have a trust gap right now. If you cannot trust your agents to be reliable, to consistently do their job accurately, effectively, and to produce the right answers in a consistent manner. That may result in a poor customer experience, and that is one of the biggest fundamental uh uh blockers to adoption today for companies around the world and agent developers that want to be able to take advantage of agents. So you need to be able to ask yourself if you're gonna use agents, how can we make sure that you're building agents that are trusted so you can create a good customer experience and you need to be able to bridge this gap. And one of the ways that you can bridge this gap is by making sure that your agents have a job performance review. In other words, and, uh, going in and doing an agent evaluation, and you need to go in and complete this performance review of your agents to be able to evaluate if your agents are making the correct decisions, delivering the uh correct results are efficient and effective at their job, and can be relied on to do that job and execute those tasks and workflows in a consistent manner. When you're going in and creating an agent, uh, evaluation, there are some questions that you need to be able to answer on a regular basis, and that fundamentally includes, uh, you know, answering or whether or not the agent achieved its goal, uh, and it achieved its, uh, task and job, uh, uh, that it was set out to do. Um, is that agent making the right decision? Did it generate the correct response? Uh, are, did the agent go in and select the appropriate tool, be able to do its job, and are the answers that the agent, uh, have generated, are they accurate? Were they polite to your customers or not? And these are what is, uh, fundamental about these types of questions is that they are subjective, which makes it very, uh, difficult to be able to evaluate and measure these agents to be able to complete a job review. But it is uh fundamentally critical and an imperative to be able to assess the performance of, of your agents and whether or not you can rely on them to do their job effectively. And when you're going in and doing these agent evaluations, this is not a one time task that you're doing when you deploy your agents into production. This needs to be done continuously and the reason why. An agent evaluation needs to be done continuously is because this needs to happen in real time. We're not talking about doing this once a month or once a year. You need to be able to identify in real time, monitor if these agents are failing, so you can monitor the behavior, the quality of those agents, so you can address problems proactively and be alerted the very second that these agents fail silently in production. And the reason why agents may fail is because every time that you go in and update a model. Every time that there is a new version of the agent that you're going in and rolling out, every time that you are modifying a source prompt or adding and removing tools for your agents to be able to leverage to solve these tasks and uh do these jobs, that may result in the agent failing and result in the agent not being able to achieve its job or its goal correctly. And unlike traditional software, uh, you really only know if the agent is effective or not, once it's out in the real world in front of your users and customers. And uh you won't be able to understand and uh uh and you need to be able to go in and measure and evaluate those agents continuously to address the agent behavior uh proactively in real time. So what are the, the primary points of failures for an agent? Um, there are 3 primary ones. Uh, first is there are quality failures that may, uh, encounter. Um, uh, that includes hallucinations, factual errors, faulty reasoning and planning, poor, uh, agent tool selection. Which results in inconsistent outputs. And this, these, uh, quality failures will cascade into reliability issues, which may include context loss for the agent, poor handling of errors, and security and vulnerability gaps. And those reliability issues will result in inefficiencies associated with those agents, which may result in higher costs for you and your company, and it may result in higher latency associated with those agents. The net effect of all three of these points of failure is a potential loss of customer trust in the Gen AI application that you have built or your or your product and service. And the worst case scenario is. You've built an agent, you've put it into production, you're super excited about how it's gonna be able to give your customers a great experience, and that agent fails silently. And you only find out if the agent failed after receiving an onslaught of customer complaints. The problem with that is by the time You hear it from the customer, it's too late. You may lose that customer. You may generate a bad reputation for your business, for your company, and for your product, and that may result in the loss of current or future business revenue. And we can't afford that, and you need to be able to address that uh upfront proactively, which is why you need to be able to understand in real time and get alerted the very second that the agent fails so you can proactively take action to address those points of failures well in advance before the customer experiences it. And what we heard from developers all around the world is that they don't have the right tools and mechanisms today to be able to go in and evaluate agents in real time and to be able to monitor those agents. And developers have to go through multiple steps to complete an agent evaluation. This includes going in and finding and creating data sets, selecting the right appropriate measures and dimensions to be able to evaluate that agent on, selecting the right model to judge the outputs of those agents, building and maintaining the infrastructure to serve those evaluations, recording the results, making adjustments, and then continuously monitoring, monitoring and production. And this process can take months and is the difference between coming up with an idea 6 months ago and then having to go through multiple evaluations until you can get a trusted agent into production. And again you have to repeat this process over and over again every time there's a new model, which today seems like it's every, every week, um, every time there's a new, uh, uh, version of that agent and every time there's a new source prompt or tool. And what we've heard from developers is that this process is extremely time consuming and is one of the most challenging aspects and painful aspects of operating agents at scale, and you don't just have to do this for one agent. The reality is you may have 10,000 agents that are off doing and executing mission critical tasks for your organization. And it simply is not feasible to be able to go in and do this at scale, which is why we took a look at this entire end to end problem and we asked ourselves how can we make this process simple and easy for you and provide a way that you can uh evaluate agents faster and easier so you can get agents to production faster in a secure manner and, uh, with confidence. And the good news is, is there is a better way, um, and to tell you a little bit more about the solution that we've built that we think will help you, uh, manage this process better, I'm gonna invite up Vivek Singh, who's our technical, uh, product manager for Amazon Bedrock Agent Corp. Very good. Hi everyone. My name is Vivek I'm a product manager at Amazon Agent Corre. And today, I'm very excited to announce and tell you about Agent Core evaluations. With agents, I'm sure a lot of you, if you deployed an agent, would have faced certain similar problems. You deploy an agent, operational dashboards look green, and then 3 weeks later, you're firefighting, quality issues, wondering what went wrong. That's the problem that we are solving with agent core evaluations. A fully managed solution providing continuous assessment for AI agent quality. Now, the key word there is continuous and fully managed. You can get evaluation frameworks and prompt at a lot of places or build your own, but then you are left managing your LLM infrastructure. You have to manage capacity, you have to manage rate API limits, cost optimization, and infrastructure scaling. With agent core evaluations, we handle all of that. The evaluation models, the infrastructure, the scaling, that's our problem, and we take care of that. You, your teams can solely focus on improving the agent experience for your customers without having to worry about the LLM infrastructure for running evaluations and managing those systems. We provide 13 built-in evaluators across common quality dimensions, correctness, helpfulness, stereotyping, tool usage, and more. And when you need domain-specific assessment, you can create custom evaluators. Everything flows into agent core observ through Cloudwatch. So, you don't have to monitor and manage another monitoring platform. Let me show you how this works across your development cycle. Agent core evaluations operates in two modes of online evaluations and on-demand evaluations. Online evaluations for production environments. This continuously monitors live agent interactions. You configure sampling rules, 1 to 2% of your traffic for baseline monitoring, and we automatically score them in real time. This is how you detect quality degradations and catch those silent failures before they affect your customer experience. The key is continuous, not weekly reviews or monthly audits. Continuous automated assessment of your production traffic. On-demand evaluation is for development workflows. This integrates with your CICD pipelines. Teams run evaluation suites against code changes, test a new prompt, validate new configurations, or compare two models. You can gate deployments when quality scores fall below your certain thresholds. Block that pull request if it degrades helpfulness by more than 10%. Both modes use the same evaluators. So, you're testing against the same quality dimensions throughout your development life cycle. What you test in CICD is what you monitor in production. Now, what can you actually measure? 13 built-in evaluators organized across 3 levels. Session level evaluates the entire conversation. Did the agent achieve its goals? Did it fulfill the user request and did the conversation succeed end to end? Trace level assesses. Individual responses. This is where your quality signals lie. What the response, was the response correct? Was it helpful? Faithful to the provided context, does, did it follow the right instructions. The span-level focuses on tool usage. This is for agents with heavy tool use. Did it select the right tools and did it extract the right parameters from the user's query? Here is the thing, you don't have to select all 13. You pick the 3 or 4 or whatever you want that is specific for your use case. Customer service agent, you probably care about helpfulness, goal success rate, and instruction following. Rack components with your agents, correctness and faithfulness are most useful to you. Tool-heavy agents, tool selection accuracy, and tool parameter correctness. And when these 13 do not cover your particular use case, you build your own. 4 steps, define your criteria. Let's say, purchase intent detection for an e-commerce agent. You write your evaluation prompt with a scoring rubric. You define what high purchase intent means and how that compares to simple browsing. You create that grading for doing that scoring, you select the model to do the evaluation, and you configure the sampling rules. Same flexibility as built-in evaluators. Same automated continuous assessment, just tailored to your requirements and your domain. We have seen customers build evaluators for brand voice consistency, regulatory compliance, conversation flow quality, whatever matters for their business that the built-in evaluators do not cover. Now, some of you must be thinking, OK, you're using LLM to evaluate my agent responses. How does that work? How do I trust that? Fair question. Let me address that now. First, detailed structured rubrics. Every evaluator follows a specific framework with clear criteria. This isn't, like, is this response good? It's, does this response follow the Instructions and use the provided context. Does it meet the defined criteria? Does it address the user's specific question? We have spent months engineering these rubrics to overcome the typical LLM limitations, bias, inconsistency, lack of specificity, grading on a continuous scale, grading on uh different categories. Second, reason for the scoring. The judge provides detailed reasoning before it assigns any score. It has to explain why did that judgment occur, why that particular score. Why the agent choice was right or wrong, and what should have happened, and what the impact is. No scores without justification. This ensures consistency and makes every assessment explainable. Third, Complete context. We give the evaluation model everything. Full conversation history, user intent, tools available to the agent, tools actually used. Parameters passed, execution details from the traces, and the system instructions. It's not giving opinions, it's performing systematic assessment against clearly defined criteria with full visibility into what happened. And you can verify each every judgment with complete visibility into the reasoning. So once the evaluation result tells you that the tool selection was wrong or poor, you can see the reasoning to find out why, which tools should have been used, which was actually used, and why that matters. Let's take a quick look at how this plugs into your existing setup. Your agent runs using the framework that you prefer, la, graph, strands, and you can deploy it on a service of your choice. Agent Core, in this case, um, associated with a memory and a gateway. Traces are being generated using standard open elementary instrumentation. We support popular instrumentations like open inference, open elementary. If you are already doing observability, and you should be, you are already collecting these traces. You configure which evaluations to run, and from there, the service takes over. Using your sampling rules, we pull the traces, perform evaluations, write those results back along with the explanations in your log groups for your traces. You monitor and assist through cloudwash dashboards and agent core observability. All your operational metrics and quality metrics at the same place. Key point here, no changes to your code, no redeployments. You're already collecting traces for observability, we are just adding quality scores to those. The evaluation happens service side, fully managed. Let me now show you what this looks like with an example. Wanderlust Travel Platform. They run a travel search research assistant with multiple specialized tools, climate data, flight information, currency conversion, web search. Here's what started happening. The user query has all the right parameters, destination, duration, and the budget. To invoke the. Calculator budget tool, but the agent instead invokes the generic web research tool and responds with a few generic blog posts. Not a calculation, nothing personalized. User engagement drops, unhelpful feedback increases. And look at the silent failures. Look at the monitoring. Response time, latency, error rate, tool activation all appears healthy and normal. Every operational metric tells you that the system is working fine and still the user experience is degrading. 4 weeks to detect the issue, 3 weeks to diagnose and fix, 7 weeks of degraded experience because their monitoring was solely focused on operational metrics and not quality assessment. This is the gap between the system is running versus the system is working well. They turned to agent core evaluations and got it working in 3 simple steps. First, select the agent, 30 seconds, point and click. You could be using a framework of your choice, strands, land graph, deployed on Agent Core, lambda, EKS and use an instrumentation library of your choice, Open inference, elementary, what you need. Step 2. Select the evaluators. They selected tool selection accuracy for this agent that is, uh, that is heavy on tools, tool parameter accuracy to that tells you whether the agent understand the user's intentions and is extracting the right parameters from the user query and helpfulness. Is the response useful to the user and um providing user value. This is the user experience metric. 3 metrics that cover tool behavior and user value. A good picture for this agent quality. No prompt engineering required. These are built-in, just check boxes. Samppling rules to balance cost with coverage. They selected 2% sampling and got it going. Total setup, less than 5 minutes. Select agents, pick your evaluators and set sampling rules. Done. Let's go over what they found. Baseline for tool selection accuracy, parameter correctness, and helpfulness are over there, 0.91, 0.9, and 0.8. This is what healthy looks like. What they found was the tool selection accuracy just plummeted to 0.3 for this particular use case in this scenario, while tool parameter correctness remained basically stable and helpfulness declined by about 15%. Remember the increase in unhelpful feedback that these metrics are tracking user sentiment. The pattern is clear, and the diagnosis is there. The agent understands what the user wants, but is picking the right, is picking the wrong tools. Let's see what happens when you drill into the faces. On looking at a specific trace, they found the specific scores for that trace, but the key is they also found explanations for why that particular score. The system explained why it's wrong. It tells you the user's query had all the necessary parameters that should trigger the calculatory budget tool, which would provide precise cost breakdown. Using web search introduces latency and accuracy issues. So not just scoring, explaining the logic, which tool should have been used, which was actually used, and what the user is missing. They looked at a few more traces, same pattern, and were quickly able to diagnose and root cause the issue. They realized that a recent prompt modification had emphasized comprehensive information gathering but removed explicit tool selection guidance. They restored the tool selection guidance and provided a few more concrete examples and deployed the changes. Within a few days, the scores returned back to normal for tool selection accuracy as well as helpfulness. OK. Now, let me go over what this means systematically, all of what we just went over. The detection and diagnosis time goes from taking days or weeks to minutes or hours. Review the scores, drill into traces, see the reasoning, and see the pattern. Monitoring, weekly reviews become continuous. You're always watching. Visibility, unknown until investigated, goes to real-time tracking with trace level details. Think about your last quality issue for a moment. How long did it take to detect the issue? How long did it take you to diagnose the actual root cause? That's what automated evaluation changes. What previously required weeks of manual work now happens automatically, continuously at scale. Let's go over and summarize what we just covered. This is what we did, so you don't have to. We spent A lot of time and extensive testing on prompt engineering, designed rubrics that overcome LLM limitations, built infrastructure for capacity and scaling, and created interactive dashboards and visualizations. Multiple quarters of engineering work. And what you get is select evaluators, configure sampling, then go into the dashboards to review the insights. Drill into the traces. Verify improvements after you have um uh acted on the findings. So we have compressed months of infrastructure work into minutes of configuration. You focus on your agent's quality, not on building evaluation systems. That's the value of fully managed. Summarizing, you move from reactive investigation to proactive monitoring, from manual reviews to automated assessment, from weeks of detection to hours of visibility, and from unknown quality to continuous insights. Agent core evaluations is available in preview today. Start monitoring your agents and cache those quality issues before they become customer complaints. Agent core evaluations is meant to help you move from hoping your agents work to knowing that they work. I'll now hand it over to Isan to take us through a technical deep dive and a demo of the service. Thank you, Vivek. Uh, this makes building trustworthy AI agents much easier, much faster. And, uh, today I have built 4 demos for all of us, from getting started to really diving deeper into evaluating agents and like looking at the journey of going to production. Without further ado, let's Let's do the first demo. So here, I'm creating an online pipeline, which is supposed to evaluate my agent continuously. So we are going to look at the setup now. There are 3 main components here. I am choosing a data source. Then I am uh selecting the metrics that I want to use for the evaluator. And then, lastly, I am going to select sampling strategy. So in this case, I'm selecting 15%. And that's it. That's all you need to do in order to continuously evaluate an AI agent across different metrics, across different levels. And we will talk about levels. Vivek has covered um levels in, in the detail, session trace span. So what, what we did here is, firstly, we looked at data source. A data source here could be, one, your agent that is hosted on Agent Gore runtime, or it could also be an agent that you are managing uh the hosting for yourself, maybe in like your own managed infrastructure or elsewhere in AWS and other services. As long as both of these agents are logging their open telemetry traces into Cloud Watch log Group, we can seamlessly fetch those traces, apply evaluation on top of it, and provide you with the scoring and the explanations. Well, we just looked at a demo of how to create that online config. I selected some bunch of metrics there, and then I also selected a custom metric. So in my 4 demos, that was the, like, I recorded it on number 3. So, but I'm showing you uh first. So this is where we create a custom evaluator. If you remember from my previous demo, I had a line item there where I checked one of the custom evaluators. So why do you need a custom evaluator in the first place? So, Built-in evaluators are really good to get started. And you may then start seeing the needs for like more nuanced evaluations, and that's where the custom evaluation comes in. Again, it's based on LLM as such. So let's look at the demo for creating a custom evaluation. Here, we have 4 components. First, we have the load template tool. So from here, you can easily select different types of templates. Um, these basically give you access to the context variables so that you don't have to worry about setting those up, and then you can edit the prompts as you need to. Then, I am configuring the model, the inference parameters. I set the temperature to 0 because I want a little bit reliable results. But, I mean, you, you, you can experiment based on what works for you. Then I'm configuring the uh The type of scale I want, which is the rubric in this case, essentially telling me. What to give score, given, given the context of the agent, how to score it, simply. And then lastly, I'm selecting the level that I want to use. And as I was speaking earlier, Vivek also covered this. So we have 3 levels, session, trace, and span. So, we just looked at 13 evaluators. We also looked at how to create a custom evaluation. Let's dive in, like how the selection of these metrics work. And if you recall, we already looked at the slide. But we just want to quickly cover that how do you go about selecting these metrics? So essentially, you work backwards from the success criteria you have for your agentic use case. The success criteria is often a combination of multiple metrics. And usually, it should at least have 3 components, the quality that the agent is uh providing in terms of the responses. Then the latency at which you are getting those responses and the cost of inference, because at the end of the day, As a business stakeholder, you may be thinking that, hey, I want to give my users' response maybe in like 3 seconds. It starts getting the first token in 3 seconds. The cost of inference should be like roughly, let's say less than $1 for each interaction, and so on and so forth, and I want my user to have good experience, and then you can define what that good experience means for your use case. So, having covered the success criteria. So how do you go about selecting these metrics? Well, it really depends on your use case. So I'm going to talk about 22 real quick. If, let's say, and Vivek also covered this in little detail. So if you have some sort of like a customer facing application, you are really looking forward to whether the responses are grounded in the context that was provided to that, uh, to the application, in this case, agent. And the other example here could be That if your agent is a part of, let's say, some back-end workflow. It is supposed to read in from some, let's say, large text corpus and provide you with a structured response. So in this case, you may be more interested in instruction following, whether or not it's like extracting everything as you want it, exactly in those formats, and so on and so forth. So selecting the metric really depends on Your use case and your success criteria. Ultimately, what you want to do is, what you can measure, you can improve. So, take that as a mantra. Well, this is all great. Let's look at the process of evaluating AI agents now. There are 5 main components here. So, you build an AI agent, that's like, you, you're just getting started with it. You build it for the use case. The next obvious thing that you do there is, you come up with some questions that you are now testing this agent against. As the agent is producing a response, it is also producing traces, and as we wake up. Mention that you should obviously be doing observability. So observability is tracking those traces, storing those for you, then use those traces to score uh based on the metrics that we covered. So you have now an agent, you have some test cases. Test cases here, in this case, may or may not have a ground truth, because, hey, you're just getting started, right? And ground truth data is very expensive, as we all know this. So we have built an agent, uh, we are now, uh, invoking that agent with the test questions. We are getting scores, scores also have explanation with agent core um evaluations. So you're looking at those. Now you're analyzing, hey, is this score somewhere close to what my success criteria is? Yes, no. If no, then you are sort of like getting back into like improving the agent. Maybe you need to break the agent into like sub-agents, specialized agents. Or maybe you're just looking to improve the model, maybe you're testing like with a faster model. Maybe you now want to go to like a more Um, let's say, a higher quality model. Then with these improvements, you put back into the agent and then test again. Every time you saw a failure with your agent. You sort of get that question back into your test set, and then this process sort of continues until You have met your success criteria. So, this is all great, right? What's, what's the process here? And what do you Like, what's the path of building and deploying the trustworthy agents. So let's also look at that and then I'll show you two more demos. So path to production for an AI agent starts with, well, creating a baseline. We just looked at the process where I covered 5 components. So we start with baselining and agent. Baselining of agent again, goes back to the testing. Once you met the success criteria, you feel comfortable, you are able to trust the agent, and finally, that agent can go in production. Now, once it is production, is in production. You may be wondering that, am I done, or do I need to do something else? And as we see the speed at which GAI is evolving, there are new models, new techniques, new protocols coming up literally every other week. And we are also improving the data. The data is changing. Your user usage pattern would be changing as well at the same time. So then, what happens is every time you want to improve anything in the model, or sorry, the agent, or if you are updating a new tool, if you are adding any new functionality, or maybe, you know, like just breaking down the agent into specialized agents, you're going back to that five component loop I was talking about. But since you are now in production, this process looks tiny bit different. Uh, Again, those 5 components still remain there, but you are now also doing offline evaluation. There is also a component of shadow mode, shadow evaluations, which basically means that you already have an agent in production, which is serving your traffic. You have now built the 2nd version of this agent, which is also serving the traffic, but it is not sending the responses to the user. So, you can easily compare these two agents, see if this new agent that you have built is really solving for, is improving for the metrics that you were looking for. Then comes the AB testing stage. And you may be wondering like, I, we already did shadow testing, why do we need AB testing? Well, in the shadow shadow mode. You never collected user feedback on this. Your actual users never interacted with this second agent that you built. With AB testing, you're now going to give a portion of your original traffic onto this agent, you're going to send that, your users are going to interact with this agent, and then you are going to measure whether it's actually solving the problem you are actually trying to improve or not. And once you feel comfortable with that, you have a full rollout. Across both of these stages, one on the left hand side with the POC to broad, then if you are already in the broad and beyond. One thing remains common. You need two types of evaluators. On demand And online. So, you may be thinking that this is all sort of a repetition of a slide. So, yes, we, we want to really reinforce the concept of that you need both on-demand and online evaluations. You would use on-demand evaluation when you are testing your agent. And by on-demand, by no means uh I'm talking here about like a batch process. It is still a real-time API agent core uh evaluation provide both of these as a real-time APIs. So You are testing in your pre-deployment phase. You are using on-demand in your CICD pipelines. Vivek has covered that already, but you can also use on-demand in production, because you may have some nuanced use cases. This is a real-time API. You can really bring together any type of nuanced use case you may have and evaluate using on-demand. And then continuous monitoring is really needed as you go in production, as you put the agents in production, because In case of Asian core evaluation, it is doing the heavy lifting of building the pipeline, and I believe all of us have interacted with data. We have all seen data pipelines are complex. So in this case, uh, Agent Core evaluation does the full heavy lifting of fetching the data from the cloud was log groups, putting that into the format that these APIs accept, and then. Doing the inference, collecting the score, collecting the explanation, and providing it to you. For the online evaluation, you do want dashboards, because you want your subject matter experts, your product experts to look at how your users are interacting, and then also at the same time, looking at the scores of how helpful, for an example, the agent was, or if the agent was faithful, that is, it did not hallucinate. OK. So now, let's dive deeper into how on-demand evaluation for agent core evaluations work. So as we see here. We have a developer who has configured some evaluations. We looked at the first two demos, where, uh, the second one, more specifically, where we created a custom evaluator, and then we also have the 13 built-in metrics. So, on the on-demand side, you first invoke an agent. That agent produces traces. In case of on-demand, you fetch those traces and put those into the on-demand API in the format that the on-demand API accepts. And then this can be a part of, let's say you are working in your IDE. So it can happen there. It can be a part of your CICD pipelines, and as I was speaking earlier, you can also put this into production for a nuanced use case, because it's a real-time API. And then seamlessly on-demand uh API invokes uh or works with the Asian Core Evaluation Service and uh invokes the models that power the uh evaluations and provide you with a score and an explanation. Now, let's look at the demo of the on-demand uh evaluations. So I'm going to go to that side because this screen is uh really small. So what I have here is, uh, we are continuing the example of uh the travel agent that Vivek was speaking about earlier. Um, this is an agent where I'm using Haiku 4.5. This agent has access to 7 tools. And see if it is. So this agent has access to 7 tools. And uh as shown there, uh, it is using Haiku 4.5. This is my baseline, baseline uh implementation. And by baseline, I mean, this is the first version of the agent that I created. And I deployed this agent on agent core runtime. You may ask, why did you deploy if it's a baseline agent? Well, I want to evaluate my agent exactly in the environment where I'm going to finally deploy it once it is in production. So this is the process of like deploying the agent uh with Agent Core um evaluate uh so with agent core development kit, it, it's super easy to deploy any agenttic code there, uh, literally 5 to 7 lines of code, and this just deployed it. Then, I just checked the status, whether it's deployed and ready for me. Probably this whole thing takes about 1 minute or so. Now, I am asking questions to this agent. So there are, there's a question set that I have prepared, and I'll show you in a second. This is what it looks like. In the very first question, it asks like, I am planning my honeymoon to Maldives for 10 days and my budget is 4500. And this agent is supposed to now look through things, as you can see here, uh, it, uh, invoke tools, do all that, generate traces for me. Nowhere in that, uh, entire notebook I was looking at or doing evaluations. My agent has run, here is what I'm doing, the, where I'm doing the evaluations. I'm looking at the session IDs. And uh please look at this closely. So the code that I've highlighted, this is all you need in order to do on-demand evaluations. I just provided the set of metrics that I want to test. I give it the session IDs. We are already collecting those in, uh, Asian core observability. Uh, it is very easy for me to evaluate now. As you can see, all the pipeline, all the data management, everything is taken care of. And here are the scores. This is my baseline version, um, does OK, OK job. Uh, my goal success, uh, is in meeting, uh, and as I also show in the explanation below for one of the cases where my goal success rate is 0, because my agent in this case asked 3 follow-up questions and did not invoke anything. So, the goal success rate, uh, evaluator gave it a 0 score, and it also calls that out. So all of that is good. Then, uh I'm skipping a part of the process, I'll cover that later, which is like how I went to this notebook. So this is my notebook where I'm doing my first experiment, where I improved the System prompt. It is the same agent, using the same model, same config, but my system prompt has changed now, based on the analysis that I have done of the explanation. And as you can see, I'm giving it specific instructions around, hey, be more concise, uh, use tools, and all those things. I'm deploying it again, because, as I was speaking, I want to evaluate my agent in the environment I'm finally going to deploy it in. Then, well, this is my trial one. I run through the same list of questions again. And as you can see, I was doing this yesterday, and I recorded it live. So it is generating those responses there. Um. And then it is supposed to give me the scores and explanations. So now, let's talk about like how I went from my baseline to experiment. What did I change? How did I know what to change? And that's what the key is to like evaluation and iteration and improving these agents. So, if you remember from my uh the explanation that I was showing you in the first place, it said that my agent did not invoke a tool, it just continuously collected information from the user. And essentially, I just analyzed all those different explanations that I got as a response of my first baseline notebook. And then I came up with, hey, these are the top failure points, let's improve those in my system prompts. I could have also taken a route to update the model, but in this case, I did not think that at this time I needed that, because I did not fully exhaust my prompt engineering capabilities. All right. So this is the score lift that I see after I analyzed the explanations, improved my uh agent system prompt. And as you can see, there are significant lifts, especially in uh goal success rate and conciseness. Well, my conciseness score is still low, to be honest, uh, but hey, since this is a demo, I'm, I feel comfortable with deploying this agent to production. But we can see like good improvement between the baseline and just the first experiment. So how do you go about using the explanations and improving the agent? There are two ways. You can one, have your subject matter expert look at these explanations, you know, dig deeper, do some data analysis and see if they can come up with these failure points and uh suggest you prompts. Or you could also use a hybrid approach, uh, and that's exactly what I did. I used LLM to analyze all those explanations, because to be honest, as you scale, let's say you have thousands of test cases, as a human, I don't think I could read those, so I just used NLLM to come up with like uh some sort of extraction of problem. Then I looked through the insights and then came up with a prompt. All right. So as I was saying, since this specifically is a demo, I felt comfortable deploying it. You may want to do more trials. And you would repeat that until it meets your success criteria. Now, let's talk about the online uh evaluation. So online evaluation, in this case, and if you remember from my demo one, It took me about 20 seconds to set up the online config. And that's exactly what online evaluation uses. So every time you invoke an agent, and let's say in this case, agent is deployed on Agent Corps runtime, but as I was speaking to earlier, it could also be an agent that's not on Agent Corps runtime. So in this case, it's on agent core runtime. It seamlessly transmits its traces to our agent core observability uh offering. And from there, Agent Gore evaluation can seamlessly read the traces as the session completes. And then evaluate that session, or trace or span with the metric that you have chosen. To finally produce the score and explanations of like why that score was generated for you. Now, finally, I think this is the last demo, and um this is where I'm going to show you our dashboard. So, since I put my agent in fraud, I set up the online config, the demo demo number one. So when you invoke agent, this is what happens. On your agent core Observatory dashboard, you can also see evaluation scores. You can see that my agent does a decent job. Uh, I have, I think, over 3, 300 something invocations. So these scores are, uh, not with just toy sample, it's a decent sample. You can also filter things down, um, based on what the, um, category or the label, remember, from the second demo, we created some labels, rubric, so you can filter things through that. You can also look into the traces. Trace is a specific question that was asked of the agent. And then you can look at the explanations right there on the dashboard. So if you click on, in this case, I clicked on correctness, it tells me what's wrong with this, because the score is 0.5, and uh there is a mix-up with how the agent used the context from the tool, so it gave the score of 0.5. And you get explanation across all metrics that you have chosen, as shown in demo number one. OK, so, um. Best practices. So how, what are the best practices of evaluating agents? How do you go about evaluation of agents? Firstly, define a multi-dimensional success rate. Success criteria, not success rate. Define a multi-dimensional success criteria. And your success criteria should also have the experience as a part of it, because that's what you're creating with these agents. And experience here could be like, based on like, if it's a customer facing app, it could be an experience. If it's like doing some back-end workflow, it could be like your downstream impact. So get your subject matters uh subject matter experts involved in sooner, so that you can design those metrics, you can uh build human in loops with them, so that you are really targeting this agent to do the task to solve for the use case. Do rigorous testing. And as I was showing you, there's a baseline, then there was trial, and then there could be like number of trials until you meet the success rate, success criteria. So, do that. Uh, I suggest you at least try to define a success criteria as if like, not your final, maybe like, uh, maybe 5, 7% below the threshold. That, that's like a good starting point as you're launching the agent, and then sort of work towards, because it's very easy to go up to 80% and then, you know, there are diminishing returns. Your data also evolves uh with your uh agent and your user pat uh user access patterns. So no for that. Then. Your, um, evaluation framework also should, uh, improve over time. So built-in agent, uh, built-in evaluators are really great to get started, but then as you have more nuanced use cases, you want to build those custom agent, uh, custom evaluators. There is one more uh thing that I want to cover on this. You may also realize as you are uh graduating from your built-in evaluators to a custom evaluator, that sometimes there would be cases when your subject matter expert would be like, yeah, this score of 0.8, I don't agree with it, maybe it's like 0.7, so this is called like a calibration gap, so you also want to do some calibration there. Then monitor, monitor your agents continuously. You need that, because what you can measure, you can only improve that. And uh do rigorous statistical analysis as you are improving your agent from version 1 to version 2 to version X. And with that, This is the end of the session. Um, we have documentation, GitHub samples, workshops, and, um, Agent core SDK for you. So feel free to get, um, take a look at those, get started. We are very excited for you, um, to get started with agent core evaluations. Well, uh, we really hope you are enjoying Reinvent, and, uh, have a great rest of your day.