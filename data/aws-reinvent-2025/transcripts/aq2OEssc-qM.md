---
video_id: aq2OEssc-qM
video_url: https://www.youtube.com/watch?v=aq2OEssc-qM
is_generated: False
is_translatable: True
---

Good evening everyone. I know it's been a long day for all of you, but I hope that invent has started great for all of you, and we do have an exciting topic to cover, uh, as part of this session where we'll be talking about how we can break the legacy barrier and accelerate the modernization using G AI. I'm Anna Bilgaya. I'm a senior specialist partner solution architect for enterprise transformation, and I'm joined by Shri. Hi, uh, good evening, everyone. Uh, my name is Shrikumar Naya. I'm a senior Specialist Solution Architect, Migration and modernization at AWS and I'm based out of Singapore. Thank you, Shri. uh So let's get started. Many of our customers, for them, the digital transformation is a key priority. To support their business growth, right? But the legacy IT applications and the landscape becomes the barrier for their business growth. Now the modernizing these legacy applications can be complex, and they may warrant significant amount of the cost as well as the effort, and that's where we are going to look how we can accelerate the modernization of those legacy application using some of the architectural methodologies, best practices, and. The power of GEI coming all together to transform those legacy applications, right? So a quick agenda, how our next one hour going to look like. So we are going to talk about a use case from monolithic to microservices where we are considering monolithic as a representation of the legacy application and the microservices based cloud native architecture as a to be target state and then we are going. To look upon how we can converge from this legacy architectures in terms of monolithic to the microservices using some of the architectural methodologies and then how we can leverage GEI to accelerate that since this is a level 400 session so we will be covering the different demos right uh toward the different modernization phase as part of this session, right. So let me first introduce a sample application. Which is a unicorn e-commerce store which representing a monolithic legacy application for our session, and we will be. Converging this into the more modernization based architecture throughout our session and in the end we will be having a modernized state of this unicorn store application. Now what are some of the business capabilities this standard e-commerce monothic application have? Just have a look on that. So it's a standard uh. A web-based ecom application. Which allow users to log in, then search the different unicorn icon and then do the check on their specification pricing and then based on their need they can actually select these unicorn icons, add them to the cart. Review the card. And then provide their payment as well as the shipment details and then they can just place the order, a standard flow like what we all used to uh do on any typical e-commerce application, right? So let's look at the functional flow, what we just saw from this unicorn store monothic application, right? So, users logged in. They start search for the products. And then based on their requirement they add those products or items to the cart and then they make the payments and then after the payment they actually place the order right? Now these functionality. Has been built using a standard .NET Core 3.1 um framework and what are the properties this application have which make it kind of a legacy monolithic architecture if we look upon the different modules which support these different business capabilities like cart. Like payments, like check out, like the product catalog, they all are these all modules are built and bundled as part of the single application, right now we have looked this flow from the business perspective. Let's revisit the same application and its property from the architecture perspective as well. So again, as I mentioned, this application comprises of different modules which support the different business capabilities around the car products and the payments and the order. All these products, all these modules are bundled as part of the same application, mostly also leveraging the same common database. Now what's happening there is the single application is doing everything. All the business capabilities are being supported by this application. And that's where it is having a lot of hidden complexities. The different modules, they all are dependent to each other, may be referring to each other implicitly. Now this is, these are some of the properties which make this application monolithic. And now let's look into what are the different challenges this application will have. So one of the challenges around the scalability. Let's say we are running to the Black Friday sale or the Cyber Monday sale where we are anticipating the 2x or 3x amount of orders. Now we want to scale the order module to handle these Cyber Monday or maybe the Black Friday event. What we rather end up scaling this entire application, right, because all other modules like other module is bundled as part of the single application. Now what that causes it causes the operational challenges. It also increases the overall cost of the deployment, right, because you are actually deploying the entire application as a whole, even if you want to just scale the order module. The next is the significant time to change, so it impacts the business agility as well. Let's suppose the different new features which need to get released. Now any feature will warrant. Test this entire application, right? And then deploy this entire application to the production. So it also increases the overall time to market, right? And another challenge which is imposed by this kind of monolithic applications is on if any issue in any module at any part of the application may bog down the entire application. So it also impact your business continuity and apart from that, it also impose a lot of operational challenges and introduce a technical depth in your environment, right. So then we talk about, OK, we understand what are the challenges of the monolithic applications. Now if we want to modernize that, the modernization is not a single step process, right? Modernization is a phased approach which required a careful consideration. To transform this legacy application into the more modern agile cloud native architectures. Now, given the, given a case of this particular application, right, if we have to transform or modernize this application, what could be the possible different phases which. We may have to think through and take to transform uh into the more modern state. Let's have a view on that first. Definitely the first phases or the first step is to do an application analysis. This application analysis is very, very important because this application analysis may give you some of the. Properties around the application which may help you to take the first step toward your modernization. For example, as I mentioned in the previous slide, this particular Unicode store application is built on .NET Core 3.1. Which is an older framework. Now the first thing is to improve the overall performance and the security posture of my application, so it's better to transform this application into the latest version of the framework, right? So those kind of insight I can get via doing the right. Application analysis. Once my application analysis is done, then I want to definitely transform my runtime and the framework so that I can improve those performance and the security posture and make my application more better in terms of then introducing the new capabilities around the modern cloud native architectures. Then I definitely have to build and validate my application with whatever changes I have done architecturally from the framework upgrade standpoint. And then once that is done, I need to perform a decomposition analysis, and we will talk in much detail about the decomposition analysis and what are some of the architectural methodologies to perform this decomposition analysis. Once my decomposition analysis is done, what is what it provides us, it provides us an identification of the different services in which this monolithic application could get transformed or decomposed. And last but not the least, once you have identified those right services, you start refactoring this application, and then you modernize it accordingly with the new architectures, right? So we are going to look upon all these different faces, uh, sorry, we, we define the right architecture and then perform the, you know, the right decomposition and the refactoring of the application, and we are going to look upon all these different 7 phases, how we can, uh, perform these different phases using the different architectural methodologies and the capability of JN AI. So to perform these different uh step um process to modernize a legacy application, we are going to leverage. Some of the key GEI based capabilities provided by AWS. Now let's look upon those very quickly. The first one is the kiro, which is AI-based IDE to accelerate your prototype into the productions. Now there have been mostly the developer leverage the code companion or the AI-based um. IDE to perform the VIP coding now we want to get away from that phase to make instead of doing the VIP coding we want to write a Bible code and then we want to go more into the spec driven uh software development approach where your prompts become. The specs, so that you can perform a right business analysis. Requirement analysis and then from there you go and design your application according to your requirements and then from there you implement those requirements and then finally you execute that particular code so the entrance life cycle of building the software. can be then powered by using the this uh ID AI driven um ID called Quiro. Now it has a different capabilities. The first capability, it can help you to build a new application using the spec driven uh software development approach which I just spoke about. You can also. Modify existing application where you have to adopt the new change request based on your business requirement that can also be done using this particular AI driven IDE kiro. And last but not the least, it also supports the refactoring and modernizing your existing application code base, and we are going to focus on this particular capability, last capacity throughout our session when we go into the different phases of the modernization. The next very important and the key service which we will be covering in our session is AWS Transform, which is the first agentic AI-based service to perform the large scale migration and modernization. This particular service has got 4 different key capabilities. The first one is on our own VMware migration where it can help us to assess the existing VMware environment. Perform the right networking design in the context of migrating those on AWS. Do a right way planning with the intelligent grouping of the application, so effectively migrate these applications on cloud and last but not the least, execute the actual internship. The Sawin capabilities on the full stack window modernization, right where we have released some of the additional and advanced capabilities today itself, right, and our transform has got more powerful to perform your end to end window modernization where along with the .NET modernization means the .NET application transformation, it also supports the modernizing it. Associated databases. So if you have a .NET application, you want to transform the .NET application, let's say, from the Legacy .NET framework to the .NET 8 or .NET 10, along with that, the associated databases of this application from the SQL server can get modernized into the Amazon Postgrace, Aurora Postgrace, the more open source, uh, uh, uh, you know, the, the databases which can significantly improve your overall cost and give you the Windows licensing freedom. The third is on the mainframe modernization where it has got the capability to perform the mainframe application modernization assessment and then you know do a right decomposition analysis based on the domain driven approach and then also we have got a new capability where it can also support all three phases of mainframe application modernization form of reimagining. Replatforming and refactoring the application and also helping to the customer to significantly reduce the testing effort for the mainframe workload and last but not the least, the custom modernization where not only the .NET you can transform the uh you know the application from Java from .NET to Python and even your custom uh application based on your enterprise code. Pattern and the design patterns, architectural pattern, it can adopt and then it can accordingly perform the transformation and we are going to focus on the window moderation capability as part of this session because our unicorn Store application is a .NET based monolithic application. Now with that, uh, let's look into the first phase of that modernization workflow where we want to perform the application analysis of the given application and how AI can help us to do that effectively. So. All right. Uh, thank you, Anand. So, uh, Anand has taken you through the, uh, the different, the seven-phase approach that we are gonna talk about today, uh, as well as the, uh, Unicorn e-commerce application as well, right? Uh, so the next step for us is to use, uh, Quiro, uh, to actually, uh, look at the Unicorn application itself and understand its, uh, current architecture, right? Uh, so, in the next video that you're gonna see, we will open the, uh, Unicorn apps, uh, codebase, uh, in Quiro. Uh, then we'll, using the natural language query, we'll ask Quiro, uh, what is the framework used in the application, what are the current technical and business challenges the application has, and also explain the architecture of the application, right? Um, once that is done, we will use that insights which Quiro has given us to develop a comprehensive modernization strategy for this application. Including, uh, you know, short term quick wins, medium-term, uh, uh, uh, service decomposition as well as long-term architectural transformation. So the idea is after this demo, uh, you should be able to use this methodical approach within your own, uh, applications, um, and break down those into, uh, uh, manageable, uh, scalable services. So let's go into the demo now. All right. So, this is the Quiro interface. You can see the explorer window as well as the chat window on the right side. So, I'm gonna ask Quiro to analyze my code base, determine the .NET version used, and explain what are the business and technical challenges. Uh, so, Quiro is thinking, going through the load. The, the code base, so it's trying to understand what is the current version used. So it says it's .NET 3.0. What is the recommended target? .NET 8 because .NET 3.0 has already gone end of life a few years ago. Current architecture is a classic monolith inter application. It also recommends the key business and uh uh technical challenges, including cloud-lative limitation. It highlights the shared data management issue because it's using a shared database across multiple services. Monolith API, uh, which makes it difficult to uh expand the different business capabilities. What is the immediate recommendation? Upgrade to .NET 8, short-term extract the uh bounded context which we will explain about it later, and medium-term, uh, go into a complete uh microservices architecture. Yeah. So Quiro has analyzed the code base and then provided us with the, uh, recommendations. So next step, so we have seen that it's using .NET 3. 3.5 framework, right? And then it's recommending to upgrade to .NET 8. So the service that we are gonna use is AWS Transform uh to upgrade the code base to .NET 8. Uh, so what is AWS Transform? Uh, uh, Anan uh brief you on what it is, but it is the, uh, agentic AI powered service, uh, developed to accelerate enterprise modernization of .NET, VMware, Windows, uh, mainframe, and more. Uh, so it's based on, uh, 19 years of AWS experience in migration and modernization. Uh, using, uh, AI agents to automate complex tasks like, uh, code analysis, assessment, refactoring, decomposition, uh, dependency mapping, validation, as well as transformation plan, uh, for your applications. Uh, so, uh, in this particular demo, we'll be using the AWS Transform for .NET capability. Um, and then there are two ways that you can, uh, use the AWS Transform for .NET service, either using the web experience or web interface, or using the, uh, Visual Studio A. toolkit extension. Uh, so in this demo, we'll be using the uh Visual Studio toolkit uh extension. So let's see the demo, how, um, uh, AWS Transform does it. Uh, before going that, uh, I also want to expand on, uh, how the, uh, AWS transform for .NET. Uh, uh, follows through the three critical stages, right? So the first step is the analysis phase. So basically what it does is, uh, it looks at the existing Windows dependent application, identify the components which needs to be modified for Linux compatibility. Next, it does go through the transformation phase, right? Where the agents, the AI agents will convert the applications into uh .NET. 8 applications. This is where AWS Transform does the heavy lifting. So it basically, um, uh, uh, handles the framework upgrade as well as uh platforms specific code changes that is required for the application to uh run on Linux, which is .NET 8. So finally, it does a validation, so it ensures that the transform application functions correctly before the deployment, which is key to make sure that The migration and modernization is successful. Uh, so how does it, how it does this key task? So first is it upgrades the language version. So if it finds an outdated C code in your uh in your application, what it does is it replaces it with Linux-compatible C code. Uh, Next, it does upgrade the packages uh from a Windows. Uh, dependent .NET framework to a more Linux compatible, uh, uh, um, more .NET which is compatible with Linux. And third, it rewrites the code, uh, for Linux compatibility. And finally, uh, for, uh, open-ended task where you require human intervention, uh, where the transform is not able to make the changes, it provides you a detailed report of what are the changes that you need to make. Uh, so that you can get the application, uh, build and successfully run on, uh, Linux, right? So like, um, uh, Anand was explaining earlier, uh, by modernizing your .NET applications, you can break free from the Windows dependencies, uh, reduce costs, and gain the flexibility of the, uh, Linux environments. So next, let's see the demo. So you can see that, uh, we have opened up the, uh, the Unicorn store in, uh, the GitHub, um, and then now we're gonna Uh, open the, uh, Visual Studio, uh, code and then go to the AWS toolkit. So you need to make sure that the AWS transform is connected, uh, uh, authenticated using uh IAM. So we have the, uh, project opened on the right side window. Now, we right click on the uh uh on the project itself and then go to port solution with AWS Transform option. Uh, so, here it shows the uh target versions, so you can see.NET 8 and .NET 10 are available. So you click on start. So it opens the AWS Transform hub, uh, which is used to track the uh the entire transformation plan. Uh, so it shows the plan. So you can either customize the plan which it provides, or you can also use the default plan that Transform provides, right? Uh, so customization, you can add more steps. Uh, so it's going through the transformation plan, so essentially it tries to build the application, make sure all the dependencies are available. Um, and then it completes the transformation. So you can see on the top that the transformation has been completed. Now, it, you can download the detailed transformation report. Uh, so in the transformation report, you can see the AP, the packages it has updated, the APIs, it has changed, uh, whether it is a partially successful transformation or a fully successful transformation in partially successful. For transformations, uh, you, you can go down and see the project summary to see are there any build errors, uh, what are the packages that are, uh, uh, updated, uh, so you can see the files that has been changed, uh, to make sure it is uh Linux compatible. You can use a diffie of the files, so you can see what was there and what has been changed, so you can uh view that option as well. Uh, uh, and then again, you can go to the Linux readiness, so it provides a detailed report on the Linux readiness. Uh, again, what are the current framework stack that is being used. Uh, you can download the next steps, so that will actually tell you, uh, what are the Critical errors that you need to resolve to make sure this uh application is ready to run on Linux. Um, you can go through, validate the steps, uh, to make sure you execute it. So it provides all the commands, the changes that is required before you can, uh, uh, do it. Uh, then you can view the diffs on all the files, uh, and then you can select all, uh, and you can do an apply changes which will do an in-place change of all the files, uh, and then now the application is, uh, you can see that it's already on .NET 8, right? So it has been now upgraded to .NET 8 version, which, uh, is, uh, Linux compatible. Now, it has been built, uh, sorry, it has been upgraded uh to .NET 8. Now, we need to make sure that this application runs successfully, right? Uh, so, again, I'm gonna use Kiro to build that application. Uh, so I'll go into Quiro, I'm gonna prompt Kiiro, uh, to, uh, ask what is the .NET version that is being currently used. So, it tells me that now it is .NET 8, right, uh, which, which was our target version that Kiro suggested earlier. Now, I'm gonna ask Kiiro, uh, to build the application and run it locally. Uh, so the Kiiro is actually doing the build. It checks the, uh, dependency, so it checks what .NET version I'm running on the local computer. It finds that it's already running 9. then it tries to restore all the, uh, it runs a .NET restore to, uh, restore all the dependencies, uh, everything. It started building the application, so you can see the build has succeeded. So it has provided me a URL. Uh, saying that it's listening on port 8080. So I click on the URL. It opens the .NET, uh, uh, the application, the Unicorn application. Now I can go in, uh, choose, uh, unicorns that I want to purchase from the store, um, and then click on, uh, uh, submit. So, as you can see, this was the previous application demo that, uh, Uh, that Anand has showed, right? Uh, now it is running on .NET 8. But key thing to remember here is we haven't, uh, decomposed this application. So it is still running as a monolith app, but its version has been upgraded to .NET 8. Uh, now, let me call Anand on stage to explain on the decomposition process, right? Thanks, City. Uh, what we have seen so far that we have upgraded our application to the latest paper and now we need to perform the analysis how we can decompose. The Unicorn store application, right? Uh, So what we have covered from our seven step, uh, uh, process, the step number 12, and 3, we, we have completed, we have done the application analysis, we have done the transformation of the code, and then we have also validated that application is successfully working after the transformation. Now we need to look upon the decomposition, uh, analysis to identify the probable right candidate of the microservices, right? And trust me, folks, this is the toughest part. How do we break a monolithic application into the more modular Decoupled uh application, right? So, How do we break it? There are different ways in which we have observed the, the application team, the application owner, you know, try to decompose the existing monolithic application into the more decoupled microservices based architecture, and they fall into a different pitfall. Now what are those pitfalls? The the very first pitfall is the distributed monolithic. Often the time we have seen the application owners, they converge to the distributed monolithic. How? Because they try to apply a horizontal tiering to the existing monolithic application. What does it mean? It means grouping the related functionally related modules together which are similar or the common component together and try to. Carve out a submodules or the subgroup out of your monolithic applications. You have a monolithic application you are grouping the related components together and what you have is a smaller submodules coming out from that application, but still they are doing multiple things or more than one thing, and they are inheriting the property, the design challenges of monolithic applications, right? The other pitfall which we have seen the application team goes into is a nanoservice. What does it mean? It, it happens due to the vertical tearing of the application. Now what is the vertical tearing of the application means? Let's say you have a UI and the business capability driven by those UI pages. What the application owners try to do, they try to break or split the application functionalities using the different UI screens what they have right now. Imagine the situation if your Monosaic application comprises of the. Hundreds of UI pages now what could eventually end up happening, you are having those many number of nanoservices in your environment, right? And then it imposes a lot of operational challenges because you need to take care of hundreds of services and it also increases the chattiness in your environment, right? So these are the common pitfall and that is where. The right granularity is the key to decompose these existing monolithic applications. Now the question comes, how do I arrive to the right granularity, right? And that is where. The key here is to not only assess or analyze the existing monolithic applications from the implementation from the technology perspective, but also to have the right analysis done from the business domain perspective and then combine the right business analysis and the technical analysis together to arrive to this right granularity, right. How do we achieve that? So there are some architectural methodologies which we are going to talk about which can Help us to arrive to the right granularity. One of them is the domain driven design first book by the Eric Ivan, and what he mentioned that the technology is not important, not the primary foc should not be the primary focus for the software development. Rather, the business features and the capabilities which we want to perform through the technology should be the primary focus. And what does it mean? So the domain driven design help us to arrive. The shared understanding from the business owners and the application owners together so that We can build the right software applying the right granularity and the right architectural patterns, right? This is what the domain driven design providers and how domain driven design works. Let's look into some of some of its key concepts. So, What is domain? The domain driven design helps us to do a right analysis of the given application from the domain perspective and help us to understand the domain and its different boundaries. So domain could be a business for which we want to work and provide the services to our end customer in our use case. For Unicorn store, the retail is a domain because it's an e-commerce application running over the web and providing the different capabilities to the end user. Now this retail could have a different subdomains. For example, the order management module, right? It's a subdomain which helps us to manage the overall orders for our application. The second could be the fulfillment once the order is placed, how the fulfillment of those orders happen, and the shipment, of course, so that you know these products which are purchased by the end users, you know, can be delivered to them. So these could be a subdomains right for a given um uh domain now in that. This subdomain further needs to be categorized into the three different categories. One of them is the core subdomain. What does the core subdomain mean? The core subdomain means. The subdomain which is a differentiator for you, right, which is what you should care about to modernize, which consists of your business logic, right? In our case, it is the order management, fulfill management, and the shipment management. The second subdomain type is the supporting subdomain. Supporting subdomain are required for the core domain to achieve its functionality. Take an example for an order management. You might be running some loyalty program, and that might for that the user information might have to come from the, uh, the CRM system. So, so the supporting subdomains are the domains which sometimes can be built or sometimes can be bought, uh, right, as the external, um, uh, software products. The last is the generic subdomain. For example, if any user has to purchase any product, they need to first authenticate and have a right access to the application, and the generic subdomain could be our identity system, which often be buy bought, not built, right, because that's not the core domain for the retail application, right. So if we just focus on the core core subdomain in the context of our unicorn application, the core subdomain will again have a different context within it, right, which help us to provide the right business capabilities around that subdomain in case of uh order subdomain. The order subdomain have a different context because order required items. Order requires the end users who purchased those items, and then the payment which has been done. So there are different contexts associated with it within that subdomain. Same could be in the case of the fulfillment subdomain as well. So this domain domain design is helping, helping you to assess the right analysis. Of the domain it's subdomain. What is the core subdomain and the core subdomain is comprising of what different contexts and eventually these contexts will help you to define the right boundaries uh around the subdomain and those boundaries will then finally converge into the right services, right? And we will see how this happens, right? But the question comes, OK, the domain driven design is good because it is helping you to come to the common understanding between the business owners and the application owners. But how should I start understanding these subdomains? How should I start getting this information about these different contexts, and that is where. The Alberto Brandolini, he's an inventor of event storming, made a very good quote. What becomes the software is not the expert knowledge but the developer's understanding or sometimes the misunderstanding because the business owners may what understanding business owners have, the application owners may not have the same understanding, and that's where we have to bring both of these parties together, right, so that they understand the domain, uh, in the shared, uh, context, right? And that is where. The event storming as the next architectural methodologies can help to achieve um the business owners and the application owners to have their shared understanding of the domain. How this happened, the event storing is not uh any architectural tool or a design pattern. It's a methodologies. It's a workshop which is which happens between the, the business owners and the application owners, and the intent of the workshop is to understand the domain via identifying the different domain events. And identify these domain events, producers and consumers, and once we identify that, then we can identify the different. Bounded contexts associated with those subdomains and then those bounded contexts eventually become the blueprint for the different services, so. How this event storming happen, event storming has a different, uh, elements, and the intent of the workshop is to the event storming exercise to identify these 6 or 7 elements which are represented by represented by the different multicolor sticky notes. So how did this happen? The first sticky note or the first element is the domain event. We identify what are the different domain events are part of, uh, the given, uh, domain space. So in our case. For example, order created, order canceled, or order placed are these different type of events. Events are always represented in the past tense, means something has happened, uh, in your business context. The second key element to identify under the event storm exercise is the command, like what commands generated these events. For example, somebody would have placed a command, OK, place order or cancel my order. So these commands are eventually generating those events. The third thing to identify who are the actors who are actually issuing those commands in our unicorn store use case. I'm the user. I'm logging to the application and I'm placing those commands. It could be the machine as well. It could be business processes as well who can become as an actor who are issuing the different commands. The 4th, um, key element is aggregate. Aggregate is something who actually understand these commands and execute certain business logic to fulfill those commands. The next one is the business policies. So as part of the event storming, we identify the different business policies, and the business policies could be like when somebody's placing the order, whether the inventory has been validated, whether the payment is successfully done or not before even placing the order successfully. So these are the business validation rule which you will apply, right, for the different business flows. The next is the external system. For example, if I'm placing the order, I need to perform a payment first, and for payment, I might be leveraging the external payment gateway. So what are the external components which are involved in my business processes, right? And last but not the least, the view which represent, uh, which is represented in form of, let's say a. UI screen where the user can then understand the current state um uh of the application or the business and then accordingly take the action like or understand OK whether my order is placed by my my whether my order order is shipped or it has been canceled it could be anything for that matter and if if we identify all these elements right then. We arrive to the domain event driven decomposition approach. So it's the domain events who drive the decomposition. So the applying the domain driven design principle along with the event storming can help us to identify the microservices based architecture. How this happened, we perform the domain analysis. We apply the event storming to further advance the domain analysis with more bounded context specific information, identify the right bounded context, and then from there convert to the right microservice. If I take example of the order management, how the outcome of the event storming for the order, um, processing use case might look like, so I'm an actor. As a user who is performing a command saying that place order, that command produce a domain event like order is created. And when the when when this event is created, there are some business policies which are getting validated like validate the inventory, right? And based on the inventory only you successfully place the order and once that is done, then it could again issue a separate commands and there could be external systems and like a payment gateway, right? They can also produce the events like the payment is successful right now once we, once we do this event storming analysis, do identification of these different sticky notes and different domain event, and then what we do, we try to group the relate the relevant related domain event together and then converge to the right bounded context and that right bounded. Context then represent the different domain services for us. So in this example for the order processing use case, what we saw, the entire event storming analysis help us to identify what are some of the application capabilities in terms of what command it could support the order management flow, what are some of its external systems to which it depends upon like payment, and what are its own repository means the data or the current state which it needs to maintain in terms of the order life cycle. So this is how we can actually perform the the right decomposition for a given monolithic application with the right granularity. And now let's see how GEI can help us to accelerate this entire, uh, event storming and the domain driven design based analysis for our given application. So we again leverage the kiro here. We have already. And done the transformation of our Unisore application to the .NET 8. Now what we use, we ask Kiiro to perform the event storming analysis for us. So we provide a prompt. In the prompt we tell Quiro, do the event storming analysis and identify all these 7 elements of event storming from the domain events to the commands to the actor external system, business policies and. Uh, right, uh, and the aggregates and the keyro then introspect the entire application code base and it tries to identify all these event storming, uh, elements and then try to map what could be the, uh, you know, the probable bounded context which can be fed using, um, uh, this event storming analysis, right. So currently Quiro is performing that analysis and it has done its analysis and now we can see what are the different domain event which has been identified by. Uh, kiro, right, for a given application. So if we highlight on that, can we see, for example, the order management events, order created, order completed, order failed, or the even the payment process, right? In case of shopping cart also there are different type of domain events which has been identified now. The intent here is not to do a complete event storming and domain driven analysis based on the GEI, but this information certainly become the base right on which further the business owner and the application owners can do their own assessment and refine it further, and we are seeing that not only the domain events, the different commands and different aggregates, all of those has been identified by Quiro for a given. Uh, monolithic application, right? Unicorn store in our case. Right, and what it is, uh, what, so not only it's just identifying the domain event, it is going one step beyond and then it is also giving us uh those probable bounded context and also highlighting the probable microservices which could be carved out from this given legacy monolithic application so this. Is, uh, this is important, folks, because this recommendation is now not just coming, uh, from those horizontal tearing or the vertical tearing which we saw. It is coming from the, the, the architectural methodologies like domain driven design and even storming which has been applied and working backward from there. What could be the probable bounded context and the respective microservices, right? Now we have done this analysis. The next thing is, as I mentioned, to actually perform this event storming where the outcome of this kiro become the base for the business owner and the application owner, and they can go into the shared workshop mode to further optimize this analysis and then modify it according to their business. Understanding and the implementation understanding and this event storming workshop can happen in person or it can happen virtually. We are taking a use case where it has to happen virtually. There are some platforms which provides the virtual event storming board. One of such platforms is Miro. So Miro help us to do a virtual event storming exercisevi providing the virtual event storming board, and it supports these different sticky notes what we talk. About so for that, what, what are the prerequisite as a developer you can create your developer account. The API keys are generated and using those API keys you can then, you know, uh, uh, programmatically generate the different, uh, event storming board and exactly the same thing we want to do. And again we are leveraging the GEI capability to perform the same. So what we are asking, uh, Quiro, OK, based on this event storming analysis which has been done. Generate a script. Which then can Create a mirror board for us and apply all this uh understanding what we have got via event storming analysis on a on this unicorn store application so Quiros go ahead goes ahead and actually. Generate the script, right? And with that script, it is going to generate. A virtual event storming board for all of us and the intent would be then we use that virtual board and then the product owner and the application owner can do their further brainstorming and optimize or refine the the different service boundaries and the bounded context accordingly, right? So it has, uh, done that, it has created the board successfully, right? Uh, and then we will quickly see how that board looks like right? with a different bounded context. So it is, it takes us to the mirror platform and then, uh, then, you know, it, uh, we open the mirror board and if we can see. This is how the all the different domain events, it's associated commands it's associated, uh, uh, you know, the business policies external systems, how they have been grouped together in the different context. We are just taking an example of zooming into the order management uh domain if we see how the bounded context for the order management has been built, uh, right via our GEI capabilities to after performing the event storming and. Domain driven design based analysis so we have all the different um uh events uh from the order uh processing flow uh in place we have all its respective commands we have all its respective external system and the other uh element uh part of it now this become the blueprint for us to then build the right micro services with the right boundaries and. The granularity right now for us the next phase is to see now based on this bound these bounded contexts what we have generated, how we can then refine the architecture for our monolithic application and then build these new microservices which we have seen as part of the, the event is even storming outcome in form of those different bounded contexts. So for that I call Shri Shri over to you. Thank you. I also want to explain what uh the MCP is. Uh, so MCP is an open source protocol that standardizes how application talks to large language models, providing them with the right context. Now, if you look at the bottom of the slide, you have the uh MCP. Uh, on the left side, you have the agents. So these agents could be chat boards. Uh, autonomous, autonomous workflows or your application. On the right, you have data sources, which could be your APIs, internal systems, databases, etc. So, MCP sits right in the middle, so it acts as a glue. So instead of an agent integrating with every system in its own custom way, MCP gives you Common consistent way to describe and tools and share data. So this means that the agents become more portable, integrations are reusable, and then you don't have to, you can avoid the one-off plumping that every time that you have to do when a new agent talks to a new source or you want a new capability. Now, um, AWS has uh uh a GitHub rapport where we have provided a lot of reusable MCP servers. So one of the MCP servers that we will, we will be using here is the uh AWS architecture uh diagram MCP server. It's a Python package, uh, and it, it helps you to design architecture diagrams, flow diagrams, sequence diagrams, uh, etc. So we'll be calling that MCP server. So the next step is basically for you to go into Kiiro and configure the uh MCP server. It's a very simple step. You go to uh uh go to GitHub, you get the MCP server, the instructions that you need to follow, uh to add the MCP server into your ID is mentioned over there. So once you configure that, so I have already done it. Now, I'm asking Kiro to generate an architecture. Diagram for me. Uh, so again, by default, it creates a PNG diagram or a JPEG diagram, right? But I want something that I, I want to iterate, I want to modify. So I use a tool called Draw.io, uh, very frequently. So what I'm telling Quiro is to generate that architecture diagram in a draw.io XML format. So then it would be easier for me to start iterating from here. Uh, so, let's go ahead and play the demo. Uh, so, I'm going to ask Kiiro, use the AWS diagram MCP server to create a draw. XML diagram showing AWS Microservices architecture based on the bounded context that has been identified from the previous, uh, uh, previous session. So, it's gonna, uh, call the MCP server. You can see that it is called the MCP tool get diagram, uh, and then it's gonna generate the architecture diagram for me. Uh, so it has actually generated a, uh, uh, PNG diagram. Since I have asked for a draw.io, it's actually going to recreate that file in Draw.io. So, uh, also, if you have noticed on the left-hand side, it has created that uh particular file, right? It has created the architecture diagram, uh, data flow patterns all are defined over there. So I have opened my raw.io, so I'm going to open the diagram that it has created. Yeah, see, this is created by the MCP server directly. So earlier this used to take days for, uh, uh, for an admin to a platform architect or application owner to create it. Now it has already been, uh, done through the, uh, MCP server. Now, uh, this, you can see that it's actually organized based on different. Domains. So you have user management domain, product catalog domain, how the different data flows works, uh, each other. Uh, again, um, uh, you, there is lambda mentioned over there, but it is easily replaceable since it's microservice. You can either replace it with an ECS or any EKS or a container solution. It has also identified the external services where notifications like SES, uh, uh, third-party APIs it needs to call, uh, the monitoring and the logging capability, um, as well in the, uh, architecture diagram. So this is a raw diagram that has been created by uh Quiro. Uh, now, again, you can iterate it based on your requirements. OK. So, next step, the 7th step that we have seen, um, next step is to actually decompose and refactor, uh, this entire application into microservices with Kiro. Now, I'm going to, uh, uh, ask Kiro to actually break down this monolith code into different microservices. So, I'm gonna prompt Kiro now. So, I'm going to ask you to analyze and decompose it to mic uh uh microservices. I'm also asking it to generate uh web APIs, entity framework, models, database, data components, everything separately. So, it, it is, uh, it has done the analysis, it has, uh, completed the extraction of the, uh, microservices. So you can see that it has created a comprehensive project structure, so, uh, independent service folders, uh, shared artifacts, uh, uh, Kuber manifest, and deployment files, right? Uh, so you can, uh, we'll explore the, uh, uh, the, um, the folders, uh, the service folders and shared artifacts on the, uh, left-hand side. So you can see that uh you know the project structure. So the services are organized as catalog, cart, order, user. Um, and if we explore the, uh, folder structure on the left-hand side, you can see the services, the cart. Uh, so it can also identify the service contracts between APIs. It can identify data ownership issues between services. It can identify shared code which it needs to extract as common libraries, so it will identify all those. So you can see that earlier you saw the project files actually. Organized uh under one folder, if you remember the first slide that we showed, right? Now you can see that it has been organized, uh, the, the project files has been organized into different uh uh folder structure. So I'm just running through each of those, uh, you know, uh, uh, folders to make sure that all the files are here. Uh, again, it has created the deployment files as well. Uh, so, uh, depending on the platform that you are asking, so in this, uh, prompt that I gave, I asked it to create a deployment file for, uh, uh, Kubernais or EKS deployment. So it has created me a cloud formation template which I can then use to deploy the infrastructure and the application components uh for that uh uh uh application. All right. OK. So, let me invite Anand also to uh summarize the modernization flow. Hey, uh, thanks, Rie. So, finally, what we have achieved, right? We started with the legacy monolithic application and we have run through the seven-step process to modernize this unicorn store application. So first we did the code analysis leveraging the kiro, then we transformed from .NET uh older framework to .NET 8 for our existing application using AWS Transform and then what we did, we actually did the event storming and domain driven analysis for a given application. Uh, right, and then we generated a different even storming board to represent the different bounded context we could arrive to, right, uh, with the right granularity which then finally converted into the. Right microservices or the right services and then from there and this we did it all through the Quiro and we leverage MCP integration of Kiro to then generate the architectural diagram for these new microservices and then we. Finally decompose our legacy monoheic application and refactor that into the more modern microservices based architecture again using Quiro. So the end to end journey of modernization has been accelerated using our GEI-based capabilities along with the different architectural methodologies and the practices, right? So what are some of the key. Takeaways. One of the key takeaways definitely leveraging the, the domain-driven design and the event storming based approach to do a right analysis of your domain and then get the right shared understanding between the application owner and the business owners to define the right boundaries and the granularities, right, to decompose the legacy applications, sorry. The second is on, uh, you know, leveraging AI-based capabilities like AWS Transform and Quiro, right, to accelerate your modernization and which can significantly reduce your overall effort, time, and the cost to modernize your legacy application stack, right, and you can drive the innovation faster. So that's all, uh, uh, folks, thank you very much, uh, for joining our session, and if you have any queries, me and Shri are down there, uh, right, and we can, you know, take your, uh, queries if you have any. Thank you. Thank you. Thanks everyone.