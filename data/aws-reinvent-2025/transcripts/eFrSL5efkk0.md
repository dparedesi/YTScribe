---
video_id: eFrSL5efkk0
video_url: https://www.youtube.com/watch?v=eFrSL5efkk0
is_generated: False
is_translatable: True
summary: "This session, \"Under the hood: Architecting Amazon EKS for scale and performance (CNS429),\" features Shweta Joshi and Raghavendra from AWS, along with Nova from Anthropic. They detail the engineering behind EKS's ability to run tens of millions of clusters and support massive AI/ML workloads. AWS introduces \"Ultra Scale\" clusters capable of supporting 100,000 nodes, 8 million pods, and 100,000 node objects in a single clusterâ€”driven by a re-architected etcd that offloads consensus to a multi-AZ transaction journal, uses in-memory storage, and partitions keys. A new \"Provisioned Control Plane\" feature allows customers to proactively select high-performance tiers (XL, 2XL, 4XL) for critical events or massive workloads without migration. Nova from Anthropic explains their \"single logical cluster\" strategy for ultra-scale training (thousands of Trainium/GPU pods), utilizing custom Rust-based scheduling (\"Cartographer\") for topology-aware placement, parallel image pulls for 35GB+ images, and vertically scaled CoreDNS. They emphasize the need for a single pane of glass and efficient multi-tenancy to maximize heavily utilized, expensive compute."
keywords: Amazon EKS, Ultra Scale, Kubernetes, Anthropic, Etcd, Provisioned Control Plane, AI/ML Infrastructure, Custom Scheduler, Trainium, Scalability
---

Good morning, everybody. Welcome to CNS 429. Uh, under the hood, Amazon EKS architecting EKS for scale and performance. I am Shita Joshi, principal essay for EKS here at AWS. Have been here, uh, like, you know, coming to a five-year mark. I'm gonna be joined by Ragh in a few minutes here and completing our lineup. Is Nova from Anthropic, who is a, um, a member of technical staff and also head of infrastructure at Anthropic. You know, I just envy her job. She just makes all these cool things possible, you know, uh, training, uh, these large cloud models at Anthropic. Um, I, I, we are just excited to have her here on the stage with us today. Uh As you can see here, uh, basically, um, Cubanities since the launch of it has revolutionized how the applications are built and deployed as well. Uh, in the recent survey of Cubanities, as you can see, 93% of the companies are either running it in production. Yeah, either running it in, uh, uh, production or evaluating or piloting it, right? Um, Cuba it is basically the declarative form of it makes it easier for infrastructure management and became the de facto standard for actually deploying applications and on the cloud native environments. So moving forward, Amazon EKS, uh, while Cuberities is undoubtedly great, but the real challenge lies in managing Cuberities at scale, managing hundreds of clusters in your production environments, right? As a CNC of certified and fully managed Kerity service, Amazon EKS actually uplifts all of that operational burden away from you to basically. Basically focus on your business applications as well. Here at Amazon, we run tens of millions of clusters at scale, and Amazon EKS over the years has become the most trusted way to run Cuberis and enables you to build reliable, scalable, and secure applications. While doing that, it is fully upstream and certified Cuberti conforment as well. So as you can see, it's a quick journey into our Amazon EKS. We are into the eight years of managed Cubanities on AWS. We launched Cuban EKS here in 2018 during the reinvent. Since then, we have launched, like, you know, multiples of the features, starting from managed control plane capability. Into helping you basically manage the compute that is required to run your applications and also launching a lot of the add-ons, whether it is fully managed or the marketplace add-ons that your applications depend on and your cluster depends on as well. In 2022, we launched IPV6 clusters and moving on and. Basically, last year, we launched auto mode and hybrid nodes, making you easier to get started on Amazon EKS and also making it easier to get started with your applications wherever your infrastructure is. And a little bit recap, uh, the one thing that I am most proud of and the entire EKS team as well, is the launch of the Carpenter project 4 years ago. It is. Our version of the cluster autoscaler, and we are really proud to donate it back to CNCF. Carpenter helps you to auto scale and also bin pack your applications efficiently onto the infrastructure as well. This year, we have been focused on scale and performance along with that, providing the managed capabilities for you to run your applications and also manage your clusters at scale too. So let's just get right into it. Today I'm very thrilled to share how EKS is powering innovations while allowing you to architect your cluster for scale. So as you can see, let's start with the foundational truth. Your mission is to deliver the applications and not worry about the infrastructure unless you really require to, and the versatility of this foundation is evident in the incredible diversity of workloads that our customers deploy to Amazon EKS. As you can see, some customers just take whatever they. Run in a legacy environment, containerize it and deploy to EKS and some run web applications at really scale, and some are building the large complex data pipelines and the data frameworks onto EKS as well. And this architectural approach allows organizations like Anthropic to also run the AIML workloads on EKS. So how are our customers actually accelerating AIML with EKS here? Uh, looking at it, these are some of the examples. Basically, what you see here is autonomous vehicle development, and for autonomous vehicle development, EKS provide the scalable infrastructure to process these massive amounts of, uh, sensor data and complex perception models that to be deployed on Cuban models as well. For robotics development, EKS actually orchestrates the training environments that stimulate the real world physics scenarios, allowing companies to safely test and refine the robotic control systems. And moving on, the generative AI, the large language model training on EKS has revolutionized how the models are developed and built, leveraging all the fundamental AWS EC2 capabilities and. EKS actually allows for you to build the scalable infrastructure where you can host your models and provide an inference endpoints. And as you already know, there has been a lot of buzz about Agene, I think, at least in the reinvent. So, in the emerging field of the ANT AI EKS enables teams to run thousands of agents, orchestrate, uh, taking these agents at scale too. So moving on, let's just take a look at what are the AIML workload characteristics, right? These are not like the traditional web apps, but these AIML workloads basically depend on the vast amount of the data, whether it is structured or unstructured data, and they're trained on a vast amount of data as well. And they are compute intensive. They require high bandwidth, low latency networking interconnect, and parallel read-write storage solutions as well. And at the same time, simplified infrastructure management is at the core for efficient scaling, scheduling, and discovery of these infrastructure, and also the repair of this infrastructure as well. Along with that, they depend on a broad set of capabilities, plug-ins, tools and frameworks, and also the guardrails to actually meet the security and the compliance requirements. So this is not ours. This is what Gartner says. As you can see, by 2028, 90%, 95% of the new AI deployments will use Cuban, which is up from less than 30% today. So this is evident, like how customers are actually leveraging Cubanities and EKS for their AIML revolution as well on their side. So, this is all great, but why customers really choose EKS for AIML, right? So, EKS because it's upstream con uh, conforment, it already comes with a tooling ecosystem, right? You can quick start with a vibrant open-source system with all of the tooling integrations. The second critical advantage of EKS is unparalleled customization capabilities that it provides. Organizations have the granular. control over the infrastructure that they can choose from with all of the underlying AWS easy to instance types. And third, of course, is the extensibility. You get integrations with a wide variety of the AWS services from compute storage to networking, and you can run anywhere. We have launched hybrid nodes, and with the support of the hybrid nodes, you can leverage the infrastructure that is on-prem or on URH to connect to. EKS control plane and run your AIML workloads there too. At the same time, it is highly scalable, rapid scale in out and, uh, in and out with the governance control is possible with EKS. At the same time, with the help of the carpenter, you can achieve cost optimization out of the box, and Carpenter supports the right sizing, efficiency management of your compute for performance and scale. So this is great, but all of this innovation depends on building the right foundation as well. So what does EKS provide for you to build that right foundation to begin with? So as you can see, it's just a level set. We're doing a quick recap of Amazon EKS control plane architecture here. As you see, uh, let me walk you through the core architecture. When you create a cluster, you are connecting to the regional endpoint, and you invoke an endpoint and you create a cluster. And when you do that, we do create API. Server instances across two availability zones and HCD, the data stored is spread across 3 availability zones here, right? And security is built into the foundation of EKS as well and the control plane and the SCD component run in a private VPCs providing the VPC level isolation for you. And for cluster access, uh, you create an API server endpoint and we provision a network load balancer underneath it, and you can say whether you want a public or a private or both for the API endpoint as well. So how do we actually handle the resiliency here? How do we respond to AZ failure? Spreading these API server instances and HCD across different availability zones is great, but how do we handle the resiliency? So what do we do here is the deterministic AZ resiliency. We have implemented what we call deterministic for the control plane as well as the HCD. So that really means is predictable outcomes even during the AC failure, seamless API experience and no disruption of humanities. Controller workflow and maintained API latencies throughout as well. And this basically during the AZ failure, we take 4 critical actions. We immediately pause all of the cluster updates to preserve the capacity, redirect all of the API traffic to the healthy availability zone, and relocate leader elected controllers to the healthy AZs. Along with that, we transfer the HCD leadership to maintain the database performance and also the quorum. And it depends on the static stability principles here, right? Our highest priority is what we want to aim is for the static stability, even when there is an A failure and existing cluster continue to run and provide that availability for you to run your applications and to be able to connect to your Cubanities control plane. And we carefully architect around these dependency failure modes as well, and we proactively test thoroughly for this AZ failure scenarios on our side with all of the packet drop scenarios, health check scenarios, and the dependency service disconnects because you're running applications depend on the multiple other AWS services interconnects too. So moving on, how does Amazon EKS scale its control plane, right? Core scaling architecture. So what happens here is basically Amazon EKS acts on multiple signals. We started off with monitoring, CPU and memory, and then we added on the node count, HCD database size, and other API server performance metrics as well. And our recent breakthrough. Through improvement include parallel scaling of the API server and the HCD and blue-green API server, um, basically deployments, optimized bootstrapped with the preloading of all of the files and with the S3, uh, prefatching as well. This actually helps reduce the warm-up times for all of the components, and the scaling process is both intelligent and conservative. As well. Global scaling using the progressively larger instances, and smart cooldown periods, which is reduced to 15 minutes, and automatic QPS and the burst rate adjustment for our controller frameworks and also the scheduler. Basically, this all lets us actually, you know, uh, provide this, um, maintain the promise of 99.95% availability, while actually providing, uh, the scaling, reducing the time from 150 minutes to 10 minutes for you. So moving on from control plane architecture to the data plane, what you see here is from the utmost left, which is providing you the utmost flexibility of managing your own infrastructure, which is self-managed node group, to the utmost right, which is completely managed, um, auto mode node, node pool wherein we basically shift away the operational responsibility from you, and then we manage the instances and the add-ons that are required for your applications to. Or not. As you can see here, uh, we also support the carpenter node pool, and on the right side, you can bring your own infrastructure from your own premises with the help of the hybrid nodes. 4 years ago, as I said, we built a Carpenter, and our portion of the Carpenter is the next generation auto scaler, and we have donated back to CNCF. Carpenter gives you the flexibility of choosing different instance types from on-demand and the spot instances. It also does support the GPU. And also you can specify the capacity reservations for GPU within the carpenter specification. Carpenter helps reduce the cost by constantly rebalancing your workloads and also bin packing those very efficiently as well. So with Amazon EKS hybrid nodes, you can now use your existing infrastructure, as I said, on the edge or on on-premises, connecting back to the control plane that is actually running within the region, thus actually getting, uh, the efficiency and also the scalability out of the box. So the next, here at Amazon EKS we support all of the EC2 instance types from different categories. As you can see, general purpose bustible with the storage high IO and the graphic intensive, moving on to different capabilities with a choice of processors to choose from, whether it is AWS, Intel, and high memory footprint, um, instance types and all. Also the accelerated compute instance type with the instant storage choices of HTDB or the NVME and different sizes, networking, and the bare metal options as well. With EKS you have access to all of the silicon innovation that is happening here at, uh, AWS whether it is powered by nitro or the graviton for the best price performances as well. With all of that, you will be continued to have access to the purchase of different options, whether it is on-demand, savings plan, or the spot instances. So what you see here on the screen are the different accelerated compute instance type, whether it is in media or our own accelerated frameworks as well, including ranium and inferentia here. So the top, as you can see, we do support different GPU instance types, G's and the P5s, and also the latest, latest versions of the Pixes too. So this is what I'm really excited to share with you today. As you can see, our customers use millions of GPU powered instances with Amazon EKS, and that number has only doubled since last year. So this means Amazon EKS has becoming the de facto, uh, you know, choice for most of the customers to run their AIML workloads here at AWS. So how does actually Amazon EKS accelerates the innovations? We talked about building the stronger foundations, but how do we accelerate the innovation here at, uh, AWS? So what you can see here is all of the features that we launched recently with Amazon EKS. So as you can see, we make using EKS easier with the launch of the auto mode and the hybrid nodes. And here, what you see are the features actually help to optimize, uh, the compute. The first one that you see here is a C cable OCI. With that, uh, we launched, uh, lazy loading, um, you know, uh, last year, but, Basically, with the lazy, uh, loading, our customers have not been able to meet the performance that they really were depending on. So we launched parallel, uh, pull that we call it as with the parallel pull. Basically, Amazon EKS accelerates the image loading and also unpacking, uh, with a very memory, um, you know, intensive and the high-performing HTTP range methods. The other thing that you can see here are Amazon EKS, uh, supports capacity block reservations that you have purchased, and the accelerated armies. So the accelerated armies actually combine all of the optimized drivers and the runtime components for GPU and AI accelerator, significantly reducing the setup time for you. And with the launch of The Cubanities DRA, you can actually enable the fine-grained, um, sharing and allocations of the GPU resources across multiple of AI workloads and the pots basically here at AWS and with EKS you can integrate the EC2 Ultra servers very, very easily. And on EKS you can run the different device plug-ins, operators, and the framework. And the last one here, S3 mountpoint drivers. With the S3 mountpoint drivers enable direct access to your training data, model artifacts stored in S3, and S3 treating them as a local file system with the enhanced performance act, uh, providing you the enhanced performance. This capability significantly reduces the data loading time for your, uh, AIML workloads. So, this is all great, but what about operations? Day 2 operations are also critical, right? So we do support node health and auto repair. So what does this really mean? EKS continually monitors the health of these nodes through a sophisticated system that integrates with EC2 health checks and the puberty node conditions. When an unhealthy node is detected, it repairs that node and replaces that node with a healthy node, um, by gracefully shutting down, uh, that node and also gracefully evicting the pots on that node as well. With the container insides, you can provide comprehensive observability into your EKS, um, AIML workloads as well. So what you see here, uh, basically is how our customers are architecting. All these together included, right? So the top part is the AIMM AIML OSS tooling, and the integrations. And as you can see, customers are building the Jupiter notebooks, they are configuring the AIML workflows and AIML job scheduling and building the model registries with the memory optimizations and multi-modal, uh, management and the dynamic batching included. Cubanities as a platform, a layer down. Uh, what we see, our customers already have EKS as the platform. For other applications, and they do extend the same platform to build the data pipelines and do the model development, model training, and inference on the same cluster with all of the infrastructure orchestrations integrating into the other AWS services. Moving on, this is how it looks like for most of the customers in production. As you can see, EKS supports multiple model frameworks and also some of our customers do implement the MLOps workflows on EKS. Um, they do run the analysis, the model development, and use different work, uh, frameworks such as Q flow, Ray, and ML flow, providing, doing the model training and evaluations and the validations as well. And on the, uh, right-hand side, as you can see, customers are also doing the model deployment and serving, leveraging both Nvidia GPUs and the AWS neuron architecture here, and integrating with all of the AWS storage layers and the networking, uh, services included. This is all great, right? But AIML workloads have some key challenges even after all of this building, the stronger foundations and the features that we talked about, um, they have a cri like some of the critical characteristics as we can see here, um, basically, these AIML workloads, especially the training requires a massive coordinated compute, uh, that actually needs the coordination, uh. Um, along thousands of accelerated instances to work as a single coordinated systems with a low latency and high bandwidth needs to. Along with that, the tools in the frameworks that we use to basically orchestrate these AIML workloads do not work well across the clusters, and it is hard to manage frameworks and mapping across different clusters. This also means managing different clusters. is increased operational overhead too. So customers are really looking for reduced operational overhead, simplified cluster management, and also have the shared governance on these clusters, thus allowing them to get to the improved cost efficiency that they're looking for, increasing the resource utilization across different workloads on the same cluster, and sharing the capacity pools that have already purchased. Answer to this, I am very proud to introduce you to what we call and what we launched here recently is Amazon EKS Ultra scale clusters. So what is Amazon EKS ultra ultra scale clusters? I feel it's a groundbreaking innovation here at AWS that I'm very. Proud to announce that our Amazon EKS team has done pushing the boundaries of what's possible with EKS. These clusters support up to 100,000 nodes in a single clusters, enabling you to manage 800,000 Nvidia GPUs and which kind of translates into nearly 1.6 million AWS premium chips. Without further ado, I would like to invite Raghav on the stage. Raghav is an engineering leader here at Amazon EKS. He and his team are responsible for operating those tens of millions of clusters at scale, and. Raghav, please take it away. Thank you. Uh, thanks for the introduction, Sheetel. Alright, so I think as Shiel was discussing, um, Cuban has basically become a key enabler for running large scale AML workloads over the last few years. Um, we, we noticed this trend like around 2 or 3 years ago that larger models were more capable and like we wanted to make sure that we're like, you know, setting up the right, uh, fundamental, uh, fundamentals to, to support those workloads. So I wanna start with. The data store itself, right, like, so we'll dig into like how did we actually enable ultra scale for a humanities architecture. Uh, it all starts with SCD, uh, so SCD is at the heart of every humanities clusters, as most of you know, it's the distributed key value store that stores every piece of configuration, so every pod, every config map, um, here's how it works by default. So for every Amazon EKS cluster we create a 3 node SCD cluster that uses the RAF consensus algorithm to maintain strong consistency across the 3 nodes. So the architecture you're seeing here shows the key components. Uh, you have GRPC SGTP for client communication. The MVCC layer handles the concurrent reads and writes. Uh, you have the bold DB as a database engine and then the writerhe log for durability. So over 8 years we've made this architecture incredibly reliable. We, we scale this cluster on demand in respect, uh, in response to your workloads. Uh, we run tens of millions of clusters using this architecture today. However, HCD was never designed to support a single human cluster with like 100,000 nodes, right? So we had to ask ourselves like how do we take this battery tested infrastructure and basically reimagine it to support 100,000 nodes in a single cluster. So this is probably the most important slide I'll go into today, uh, so bear with me. So in traditional HCD, the RAF consensus algorithm is deeply integrated into the database itself. What you see is every right has to go through the raft that gets coordinated across the three nodes first. Uh, this works great at, you know, regular scale like thousands of nodes, maybe tens of thousands of pods, but that becomes an extreme bottleneck when you're looking at, you know, 100,000 nodes. So first, the first thing that we tackled here was we offloaded consensus to a purpose-built multi-AZ transaction journal within AWS. Um, when SCD is not managing its own consensus and durability, uh, we offer this to a specialized system that's purpose built to do high throughput distributed consensus. This removes one of the largest bottleneck in a human release architecture today. And this will also allowed us to scale SCD horizontally because we're not bound by the quorum requirement anymore so we don't have to like have 3 nodes for SCD or 5 or or 7 we can scale SCD horizontally, which is a really good property to have. Next, uh, traditional SCD also stores all the data on the disk using Bold DB. So when you're looking at millions of objects undergoing read and write operations, Desko becomes the next natural bottleneck. So we leveraged the fact that we had moved durability to the purpose-built system and we could move the cluster state from static EBS volumes to an in-memory database using TEFS. This had helped us unlock like an order of magnitude higher read and write throughput, uh, because we're just reading and writing from the memory itself. Also this change helped us increase the total SCD database size that I'll go into later. Uh, the next, uh, traditional SCD is also a monolithic database. All of the cluster state is stored in a single key value store, so you can imagine like it becomes very hard to scale this architecture horizontally. So we looked when we were scaling up like these workloads we identified around 4 or 5 keys that take most of the traffic. It's your nodes, pods, leases, events. So we intelligently partitioned those keys into their separate dedicated LCD key value store again allowing us to increase the total rewrite throughput for SCD by this change. So these are the three core changes that we did that really helped us support the, the scale and the throughput that we need, uh, to support ultra scale. All right, so let's take a look at the same architecture with the changes I just described. So we created a consensus interface which connects you to multi AZ transaction journal. You have the MV MVCC layer that's now writing to the in-memory database. And then finally we have a partition key space that can allow us to horizontally scale at CD. But the key thing here is we maintain the same API semantics that Cubanities expects. We didn't have to change anything in humanity core while we're doing these three changes. The it it must be it must be resonating with you like these are not incremental improvements like this is like a complete architectural rethink in how human communities can store and retrieve data at scale and this is what basically made, uh, you know, ultra scale possible so I'll show you some of the results in the next slides. So in the first slide, what you're looking at is just the scale of the operations. Uh, what you're seeing here is 3 distinct AIML workloads from our customer deployments and testing scenarios. So I'll start with the first one. We submitted a massive pre-training job using stateful sets on all the 100,000 nodes. This gave us the confidence that we can sustain long training runs over an extended period of time. Next, we submit what I call like the mixed mode workload, multiple parallel fine tuning jobs where each job occupied 10,000 nodes, and then we also served real-time inference using leader worker set on the LAMA 3.2 model. The mixed one workloads are slightly different. They're very bursty, so they, they pose extreme stress on the control pane. I think the key takeaway from this slide is all of these workloads are running in a single ultra scale cluster, so you're not looking to, you know, split these workloads across multiple clusters, trying to coordinate them. And as Sheetel mentioned, like the, the frameworks out of the box do not think about like being able to manage these workloads across multiple clusters. This is like the real part of why we, why we're motivated to solve this problem in a single cluster. All right, so in the next slide. Uh, we're looking at the number of objects of different kinds as we scale from 0 to 100,000 nodes and then back down. So at peak we are managing tens of millions of objects, around 8 million pods, 100,000 node objects obviously representing the cluster infrastructure, 6 million lease objects for doing leader election coordination, and then tens of millions of events representing the cluster activity. Uh, this is clearly unprecedented scale. Uh, traditional SCD was never designed to handle this volume of, uh, this volume of objects. So as a result, we also had to like push the limits on the total SCD database size. Uh, with Ultras cluster we support up to 20 GBs for a single, uh, for a single cluster, uh, which is 2.5x what we get in, in standard EKS today. So we all know like it's not just about the raw storage capacity. What matters is that can we actually serve milliseconds of read and write latency when the working set gets this large? And that's what I'm gonna go into next. All right, so this slide talks about the throughput. On the left you can see that our peak we were able to serve around 7500 read requests per second. Um, the throughput also scales smoothly and we start to add more nodes and more pats to the cluster. You must be wondering why is this important. Like these are, these are graphs. Like it's, it's interesting. Like they're, you know, these are high values. But if you think about it, communities or most of our customers use a lot of controllers operators, right? So at that scale they're all, they're looking for changes in this, uh, in the cluster itself to make decisions such as pot placement, monitor pod health, coordinate workloads. So all of this starts to matter. Uh, in addition to the next generation data stored, I do want to say that we use an upstream change that allowed us to serve consistent, uh, read requests from an APS server cache instead of going to SCD, and that like really, you know, saved like that, uh, additional round trip. On the right you have uh the right throughput. Uh, ride requests are obviously more expensive because they still require consensus, they require persistence. Uh, we were still able to peak at around 8000 to 9000 ride requests per second. Uh, this was primarily driven by the next generation data store, the journal back end that I discussed previously. When you're thinking about having millions of objects, you know, 100,000 nodes, there's always a part that is being created. There's always a part that's being whose configuration is being updated. You're doing scaling operations. All of those go through SCD. Traditional SCD would have bottlenecked at a fraction of this throughput. All right, let's quickly look at the second half of this equation which is latency, right? Because you can have all the throughput in the world, but if you can, if you're taking seconds to serve a request, uh, that will still, you know, impact your workloads. So on the left you can see that we were able to serve read, write, and delete requests between 100 milliseconds to 1 2nd at P99. We achieve this optimal performance by carefully tuning things such as request time out, retry strategies, worker parallelism, uh, throttling rules. Uh, it's worth noting like doing all of these, this work only matters at this specific scale. Um, on the right you have list requests, uh, if you're familiar with list requests, they're very special to the humanities architecture because a single list request can return thousands or in the, in the scale that we're talking about millions of objects in a single request. We were still able to return, uh, respond to this request between 5 to 20 seconds, uh, which is way below the 32nd upstream SLO for list. So one of the ways we achieve this that I wanna highlight is we moved, uh, we, we made an optimization and how the items in the list requests get encoded by default, humanities, you know, I, uh, encodes the objects in the list request in a batch, but these batches were getting so big that we moved to incrementally, uh, encoding these list requests, significantly reducing the memory consumption that it takes to serve one as a result, allowing you to serve a lot more requests in, in parallel. So all of the work that I've described so far uh results in a single high performance, highly responsive humanities cluster. All right, so we just walked through the performance landscape of the control plane, right? But that's kind of like half of the story. It's an important part. It's still half of the story. So to enable AML workloads, we knew we had to optimize every layer of the stack. We started with the control pane, then we got to data plane and networking. So let me just walk through a couple of other innovations that we did to maximize application performance and resiliency. AML workloads run on instance type that have two unique characteristics. They have network bandwidth up to 100 gigabits per second, and they also have EBS volumes that support extremely high IOPs and throughput. And we wanted to make sure we take the full use of these, uh, characteristics. So when we look at these AML workloads, they tend to have very large container images, often exceeding 5 GBs. Pulling these container images can have a pretty large impact to your workload readiness if you think about it like this really determines like how long does it take to serve that first training inference uh so that uh start the training or serve that first inference request. So using the AWS Sochi snapshotter that I think we highlighted previously, we basically paralyzed both the container download and unpacking operations. Next, we saw that the container, uh, the AML workloads also download a lot of data startup, often from S3. So we made a change in our CNI that allows a single port to connect to all the network cards on this instance, giving it access to the entire network bandwidth that I just described. The combination of these two changes reduces the time it takes from a port to go from being scheduled to running to having all the data it needs by 3X. And then finally we have to talk about node scale up, right? So Carpenter has become the standard in how customers are scaling compute in response to pending workloads. Since we own both the compute scaling and the networking plug-in, we made another optimization where we preassigned IP prefixes to pods to nodes during that launch instead of doing it reactively through a CNI after the launch. This further reduce the time it takes for a node to launch, become ready for workloads, and if, if you think about it, if you're scaling from a 0 to 100,000 nodes, every second matters. And I'm really happy to announce like all of these, all of these features capabilities have been available in EKS since, like, you know, the middle of the year. OK, so we, we kind of walked through the company's journey for ultra scale showing you how we kind of reimagine the, the data store, uh, to support this, this massive scale. Uh, these innovations represent a fundamental rethink in how in the community's architecture itself. We have learned a lot in working with customers like Anthropic. Now I want to say, show you how we're bringing those learnings and making them accessible to every other ETS customer. So this is where provision control pain comes in. Uh, not every customer needs 100,000 nodes. We completely recognize that, but customers are looking for high performance control plane, uh, predictable control plane behavior, and think of it this way we took all the learnings from ultra scale, our deep understanding of being able to optimize the control plane performance every layer of the stack, and now we are making them accessible to you through provision control plane. All right, let's just still start with like why did we build provision control pane using provision control pane you can proactively select a tier that matches your business needs instead of having the control pan reactively scale based on demand. So let's say you're doing a major deployment next week or you have a product launch coming up. You can come to EK EKS and say, I want to run this cluster on a, uh, on a high performance tier, so you're guaranteed the capacity is there when you need it. Next, we are also introducing three new performance tiers that offer significantly high performance such as API request concurrency, pot scheduling rate, and database sizes that were not accessible in standard clusters before. So that means that you can run exceptionally, uh, you can run a lot more demand and workloads than you could do, uh, previously. And then finally, even if you're running on standard most of the time, you can temporarily scale up to a higher tier, you know, you, as I said, like you have an event coming up, and then you can scale back down. So this gives you the ability to optimize for both performance and cost in your EKS environments. So starting last week, we give you two control pain scaling operations. I talked about standard, and we have provisioned. Standard control pain continues to be the right choice for most workloads. It's automatic, it's battle tested, and it handles dynamic scaling beautifully. Provision control pain is when you need predictable high performance capacity, that's when provision control pain comes in. Uh, the best sort of analogy I've been able to think about, think, uh, in terms of is standards is like driving a car, uh, where the engine is kind of automatically tuning itself based on how you're driving, right? So it's kind of great for everyday use. Think of it as like a Toyota, uh, provision control pin is like being able to step into a sport mode because you know you're gonna need that performance tier when you're driving. So I wanted to spend some time talking about the standard control plane because this is what most of you are using today, uh, as I mentioned previously, standard control plane scales up and down automatically based on workload demand. We constantly monitor several signals, right? Yeah, CPU memory utilization of the control pan, IO, uh, object counts, the total database size, and when we detect that the cluster needs more capacity. Such as you're deploying a large number of pods or you're seeing an increase in your API traffic, we reactively scale up the control pane in approximately 1010 minutes, and once the demand decreases, we scale the cluster back down. It's worth noting that this dynamic scaling is happening within the standard tier levels. So if you're thinking like when should you stick to uh standard control pain or the standard mode, uh, if your workloads have a predictable pattern. If you're good with automatic scaling or you don't have a critical business event where you need guaranteed capacity, I would recommend continue to use standard. But let's talk about provision control pain. This is where you take control of your own capacity planning. Effectively you can proactively, you can proactively provision specific performance tier to match your business needs. So here's how it works we are providing you three additional tiers Excel, 2XL, and 4XL. These tiers provide significantly higher capacity than standard, more API request contingency, faster pod scheduling, larger database sizes. So when should you use provision control pane? Let's say you have an anticipated high demand event next week, or you have performance critical workloads that need the minimum possible latency from the control pane, or you have massively scalable workloads like the ones we described with AI model training, inference, or data processing. And if you're wondering, like, how do I start to use it, that's what we've made incredibly simple. You can use the cluster create or update APIs that you're familiar with. Add one control, uh, add one parameter, the cluster control pane scaling config, choose your tier and you're done. There's no need for migrations. There's no complex configuration. There's no downtime. You can start to use provision control pane on uh any human community's version starting 11.28 or later. All right, so I wanna dive into what uh capacity each tier provides so you can choose the right tier for your workload. Let's start with the API request concurrency. This is the number of concurrent API requests that your control pane can handle at any given time. Think of it as like the number of, you know, conversation it's having with clients such as Clets, Cubanities operators, controllers. Um, next is the pod scheduling rate. This is the number of pods per second the scheduler can place onto your notes. If you are doing a large deployment with thousands of pods frequently, this parameter will significantly determine how quickly they become ready. And then finally you have the maximum database size. Remember the conversation from Ultra scale? As your cluster grows, so does the database. Each provision tier supports up to 16 GBs in total capacity for SCD, which is double what you get in standard today, giving you significant headroom for growth. So you must be wondering, OK, like I still need some more guidance on how do I select a tier, uh, so if you find yourself running a lot of customs, operators, controllers, I would consider looking at how you're doing with respect to API request concurrency and select a higher tier. If you do frequent deployments with large number of pods, say, like Spark jobs, um, I would start to think of using tiers with higher pot scheduling rates. And if you have a large cluster with many objects, we see this often, like, you know, customers using really large custom resources or config maps, you want to make sure you never exceed the total SCD database capacity. So I encourage you to go check out the the pricing page. um, we try to make the pricing as transparent and predictable as possible for each tier so that you can go and like, you know, balance and optimize for both performance and cost. So here's the next critical question. You've selected it here. How do you know you're effectively utilizing it? And this is where monitoring comes in. We're making several new metrics available to you to monitor your tier utilization in real time. Again, starting with the API request concurrency, this will tell you how many, uh, API requests your control pin is handling currently. So say you're, you're in Excel and you see yourself being close to 1600 or you're around 1600 out of 1700 API request concurrency, you may wanna think about scaling up your control pane. Next is the pot scheduling rate. This will tell you the number of pots per second that the schedulers attempting to place, how many are succeeding, and how many are failing. And then finally, the database size. Each tier has 16 GB in total capacity. If you're running close to the tier limit, I highly encourage start to think about cleaning up your unused objects or start to look at some architectural changes. So here's how this would work in practice, uh, before a major event, say a product launch or a big deployment, check your baseline utilization. If you're already at 70 to 80% of your cus of your current tier's capacity, uh, you should proactively scale up during the event, uh, monitor these metrics in real time. If you see sustained high utilization or a spike getting close to the tier's capacity, you can just call the cluster and update API and upgrade to the next tier on the fly. And then after the event, review your utilization again. Say you upgraded to 4 Excel, but you're only using 30% of that capacity now, you can scale back down to 2 Excel or even Excel. All right, so we just walked through provision control pane as a feature showing you how we're taking the learnings all from ultra scale, the architecture itself, and then making them easily accessible to you as EKS customers to these new 3 new tiers. I'm just going to quickly summarize, um, a couple of points here. So it's incredibly easy to use, right? Like you don't need a new cluster. You don't need to migrate your workloads. You can just call 1 API to get started. Second is the flexibility. You're never locked into a single tier. You can upgrade the, uh, your control pane to higher tiers and you can go back all the way down to standard. And then finally, if you have built this feature with like the Operation Excellence excellence that we acquired using running tens of millions of clusters, you're running effectively the same architecture that you get from ultra scale just in a much more easy to use way. So I'm really excited to see how all of you can like use this feature to optimize for both performance and cost. All right, so on that note, I'm excited to welcome Nova. Uh, Nova is the infrastructure lead at Anthropic. Uh, Nova and her team have been working with us over the last few years as they've scaled up their community environment at EKS. They've been running success. They've been running the machine learning workload successfully, uh, on the ultra scale configuration for the last 6 months. Uh, Noel go into the motivations behind their own architecture. Please, please welcome Noah to the stage. Hey, thank you so much. Uh, it's hard to follow. Um, so let's, uh, talk a little bit about, uh, what we do and, and why we do this. So, um, I work at Anthropic, uh, we do cloud, um, so you can see that on the slide. Um, we recently released Opus 4.5. If you haven't tried it, you should consider it. It's the best in the world at code and also most of the other things. So, I'm pretty proud of that. Um, and that was something. Thing that we built on EKS, so, um, we do a very large amount of ML, um, on, uh, AWS, and as part of that, um, almost all of our compute is done inside of EKS. There are very few cases where we have some raw EC2 instances, but I would say over 99% of our capacity is, is managed by EKS today. Um, and I'm gonna talk a little bit like philosophically about why we do things the way we do, why we push for things like ultra scale clusters, um, because I think that that, it's like an interesting architectural decision because if you look at sort of upstream and you look at how Kubernes has been used historically, there's been a big push to try and have manageable clusters, manageable failure domains, um, and there, and there's like a lot of reasons to do that, so it's, it's like still worth thinking about, um. On the slide we can talk about like rannium, uh, we use a huge amount of ranium today. I think the vast majority of our, uh, workloads, especially around infant, are, are served on trannium, uh, and we use these very large clusters. So, uh, I guess like I one question I ask is what makes a. Cluster a cluster, so it's like not tied for us to a physical domain. This is like the, the big, uh, important thing, uh, that makes choosing when to make a new cluster weird. So, um, if in some case you had, um. If, if, if we had like, uh, an application that was, uh, only in one physical location, so, um, we didn't have distributed training, we didn't have distributed inference, then it would totally make sense to try and align your cluster, uh, control plane with the physical domain because this gives you predictable, um, properties around resiliency. It gives you predictable properties around like maximum cluster size. And it doesn't sort of give you this problem of continuing to scale things forever. Um, but I work at Anthropic and we're a scaling lab and so we want to scale things forever. Um, so, so when do you actually need a new cluster? And I think, um, the way that we think about this is it's when there's a single point of failure that you can't engineer out in some other way. So, um, we talked previously about what the architecture of the control plane looks like, um. There are many different places where there is resiliency within a single cluster. You have to do a lot of things to, uh, protect the control plane from runaway workloads. Like, obviously when you are in a multi-tenant environment, it's harder to think about some of, some of those things, but Kubernetes does give you the tools to do this today with things like APFs, with things like quotas, um, to be able to put multiple and with named spaces to put things inside of the same cluster, um. So I guess like why ultra scale clusters for us? Um, a big one is that we get a single pane of glass for observability. When we think about how we're designing our applications, I really want Kubernetes to sort of work for the application. I don't want to design my application around Kubernetes. Um, so there's like two different axes, uh, and this isn't on the slide. I'm very sorry, uh, there's a scale out and there's a scale up, so a single application should fit inside of a single Kubernetes cluster. That's the thing that we believe, um, pretty strongly here, um, if you're going to take a single application and put it across multiple clusters, then you're just building kind of, uh, Kubernetes on top of another Kubernetes and like. We already had Kubernetes for that, so I'm, I'm not going to do that if I don't have to, um, for us that's driven by like things like the size of our largest training job. Um, our largest training job is huge. It's, uh, and it's also inside of one name space inside of sometimes one stateful set. So this really pushes, uh, us to like get these really big. Clusters working and then everything else um is is what like drives things like the QPS so like there's a database scaling piece and then there's like a QPS piece um and then uh there are reasons to scale out I think that if you actually do have applications that are are are localized then like you might want to use multiple clusters to to meet that need, um and uh the other thing is like this the scaling around, uh, for scaling for our customers and when I say customers here I mean often oftentimes our internal customers, uh, for our for the infrastructure team. Um, and efficient multi-tenancy for ops. Every one of these instances is, uh. It's very expensive, which you all know already, um, and any time that we are spending where we either idle an instance or we are waiting for, uh, an instance to scale up or scale back down again is time that we could be putting more training workloads on it. It's dollars that are out the door. It's like slop that's inside, uh, of our, our overall like. Uh, architecture that I is a KPI for me. It's this thing that I'm gonna try and drive down, um, and so having one logical resource pool, even if it's like across multiple, uh, domains it whether across AZs and things like that, um, makes our job easier. And so like one of the things, uh, we talked about this earlier, um, we have extremely large, uh, images. Our, our images are something in the like 35 gigabyte range, and, um. So we've worked with EKS on, on the streamable OCI stuff to um optimize that in a bunch of different ways. Uh, on the slide you can sort of see, well, we can see something. Um, you can see sort of like we've got these different layers that are different sizes and so when you have large layers, um, those large layers are going to oftentimes be the bottleneck for how fast your whole thing completes. So, um, you wanna think about your application as something that's like it's either IO bound or it's me, you know, it's, uh, CPU bound or, or, or network bound. Um, in, in these cases, uh, we were seeing that we were actually, uh, like CPU bound, and so, so I think that being able to use multiple cores for doing the parallel decode and, uh, or the parallel, uh, unpack and things like that, um, has improved our, uh, P50 time for parallel pole by almost 5 minutes, which is like a huge amount across, um. Thousands of servers and, and for us this means that oftentimes with like a large training run or something like that, um, that is the time, uh, like this sort of like MTBF or sorry MTTR when there is a failure of a node might be determined by the time it takes to pull that image onto a new node. Now, obviously we do things like prefetch that help us with that, but it's, it's just really not, I don't know what's going on with this slide. I'm very sorry guys, um, and, uh. Sorry, it's very distracting. I think that, uh, that's like a thing that we optimize for. It's like trying to optimize the amount of downtime that we have, um, and so the other part of that is like optimizing our scheduling. So we actually use two different schedulers today. One of them is an in-house scheduler, um, that I'm gonna talk a little bit about. We actually. Do use the default scheduler for a lot of things today. We, we do the vast majority of our Damon sets are scheduled via the, uh, the default scheduler. All of our CPU workloads are scheduled via the default scheduler, and EKS has made that work even for extremely large Spark work workloads, tens of thousands of nodes, um. And that's been very exciting for us as well, um, but one thing that we do is we try and scale on something that is a smaller number, um, and so when I say that we've got these like very large training workloads that have thousands of pods in them, I actually don't care about how fast any individual pod schedules. I'm not going to be able to do anything until I have all of them. So, um, in that case, the thing that I'm looking for is something that scales with the number. Number of workloads, not the number of pods. Today, the way that we do this is, um, we have something called cartographer, um, that we're considering, uh, we're considering open sourcing. Please talk to me if that's like an interesting thing to you that scales with the number of workloads. Um, we use an informer and we have like a rust-based thing that, uh, looks at the entire cluster at the time and sort of loops through and schedules a whole workload, does bindings to every node at the same time. Um, with topology awareness for network optimization, especially in large clusters, this is super important for us. We want to be able to co-locate even within a single job, um. Different parts of the job to different network domains. So, uh, the Amazon architecture includes things like spines for networking, which you might be familiar with, um, from things like placement policies. So we want to be able to support a, uh, ML workload being co-located inside of a Kubernetes cluster that is ultimately multi-tenant. Um, while being network optimized, and this can make a difference of like somewhere in the like 2 to 3x range, um, and it can also be the difference between being able to do it or not, um, because I really don't wanna get paged by EC2 next time I do something like this, and I, I think I'm very excited to, to say that this is coming upstream. Soon, um, we're not clear that we're going to use this, but we are hoping to do that. So like we're hoping that Kubernetes will be able to do the kinds of things that we do custom today, um, for everyone because I think that this is like very exciting. There are many kinds of batch workloads where this is the dimension that you wanna scale on. It's, it's not the pods, um. So one thing that we have had to do is we've had to scale our DNS. Uh, DNS is oftentimes the, uh, the bottleneck in, in these things. Um, core DNS has a couple of different things upstream. I actually didn't mention it on this slide, but, um, we're really excited about being able to do multi-core, um, core DNS upstream, which means that you can vertically scale, um. A single core DNS instance at a time. Um, one thing that we do is we've actually been able to scale down the total number of instances that we've had in these large clusters. We've often had had hundreds of different replicas of core DNS, uh, which has like a severe downside which is that a cash hit is much less likely, which means that like sort of pushing through those requests to the upstream. DNS, um, you, you're just putting a lot of load on that, which means that you run into things like network limitations for how many UDP packets you can and, and DNS entries you can shunt out of each of your core DNS instances. And so being able to pack those onto smaller instances means you have warm caches, which means you have ultimately a whole lot less, um. Upstream load. So this is pretty positive for me, uh, a vertical scaling perspective, um, today, uh, like we really want to avoid having a service mesh unless we, unless we really need to, um, today we, we find that basically we can do that service discovery via DNS for almost all of our applications because a lot of our applications just really aren't, um, microservicey. A lot of them are, um. I'm from an academic background. They're, they're cluster applications. They're parallel applications. And so, um, for those we, we really wanna know what pods are inside of the same workload. We don't care as much, uh, going between those workloads. And so, um, that, that's something that we've been able to do without a service me today, which is pretty exciting at this scale. Um, endpoint slices, um, has been like one of our, uh, bugbears though, so that one. Has not scaled for 100,000 services, um, so we're hoping to see some upstream improvements on that, uh, especially going into CB 135, um, uh, along with some, some other optimizations there. Talk to me after the talk, um, I'm gonna give a couple of shout outs to some of the other services inside of Amazon that we use, so we actually don't use parallel file systems today for the vast majority. Uh, or, or like explicit parallel file systems for the vast majority of our workloads, we use a lot of object stores, uh, and I've given this slide roughly before, but the way that we do this is we have an object store, we have a prefetched buffer and a queue basically so we can absorb, um, not arbitrary latency, but seconds of latency at times, um, to the job leader which then broadcast things out to all the workers and this is like. It, it sounds a little obvious when I'm saying it like this, but this has been what's led us, um, scale without having a, a luster, uh, or something else that is like a, a big parallel file system, uh, for some of this stuff, and, and we're looking at the S3 CSI for doing some of that prefetch in an implicit way. Today we use it explicitly, um, and this is the, you know, this is last year's slide. It's. You know, 800 gigabytes per second with no provisioning required, uh, so it's now 5000. So, so we're, we're really pushing S3 very hard on this. Um, I didn't ask them before the slide, hopefully they're OK with, uh, saying this, but it, it is like, uh, it's pretty exciting to see that like we have been able to continue to scale, um, on this object store without necessarily having, uh, other layers. Uh, in there, um, we do use, uh, parallel file systems in one place, which is that, um, we talked about Jupiter. Uh, there's a lot of Jupiter users at Anthropic, and those Jupiter users oftentimes have a multi-pod workflow, um, and the historically the way that we did this is we had something where we are synced the code from one pod to a bunch of other pods, and that works fine. Um, it has bad consistency properties, and EFS has much better consistency properties than that. So that is one place where we've been excited to see multi-attach and, uh, the EFS CSI, um, for that. So, uh, and then like one last slide, uh, that I've got for you guys is, uh, things that we're looking for in the future, um, that we're excited about. We're excited about name space controllers when we're talking about that single point of failure, the KCM, it's great for a small cluster. To have all 30 controllers inside of the same process and for a large cluster it means that when you, uh, have one of those controllers get unhappy, then you have 30 controllers to worry about. So I think, uh, seeing that get names spaced out, getting charted out is gonna give me the failure domains and the properties that I care about, uh, for large scale. Um, there's things like multi-VPC architectures for ultra clusters that, uh, where you start running into the NAU limits and things like that. Um, we're excited to think about carpenter for capacity reservations. Um, this is something that's like fairly new to us, but, um, we're, we're pretty hyped for this. We use a lot of carpenter internally for our CPU workloads, and it's exciting to be able to do that for GPU and ranium workloads, um, to move away from auto scaling groups if possible, um, because it means that we have one diff like one thing to manage. We have one set of, uh, like configuration for our, uh, the way that we, uh. For the way that we provision nodes and, and then finally, um, we said IPV6 came last year. Uh, IPV6 has been coming for 20 years, but, um, where we want to be able to move basically everything that we can to IPV6 for this big scaled flat network, um, to make it so all of our services are able to talk to each other again without a service mesh, um, and with that, um, I'm gonna welcome folks back to the stage and we can, uh, do a little bit of closing thoughts and Q&A. Uh, thank you folks. Right. Uh, thanks, Noah. That was super exciting to see all that work done together. Uh, I think just like as a, as a round up we talked about, you know, how EKS has become the foundation trusted way to run clusters that, you know, battle tested reliability went to ultra scale like our sort of motivations behind why we build that for specialized workloads like the ones that anthropic is running and then really like how we're making all of that available to you through through provision control plan. So please try it out, give us feedback, we're always looking forward to that. Uh, these are some, some of the sessions that are still remaining from EKS, and I wanna highlight the one at the top. Um, that's a chalk talk like, uh, uh, AWS is doing a 500 level series for the first time this year. Uh, if you're interested in this slots left, I highly encourage going to that chalk talk. It's from two principal engineers, one from EKS, uh, Sham, and, and one from the transaction journal team, NAMI, who are going to go into all the dirty detail details that I didn't cover. I'm probably not equipped to cover. Uh, and then some useful links our user guide, uh, we do workshops monthly. You can sign up for them, uh, and then we have an AI on EKS website which is a comprehensive set of tooling blueprints that helps you sort of like, you know, try out some of these things, uh, on, on your own. Uh, we try to keep it updated with, with all the latest and greatest that's happening in, in machine learning right now. So shout out to the best practices guide, and the best practices guide is really, really good. You guys should meet up. Uh, yeah, that's it. We'll, we'll take some questions. Thanks for spending time with us.
