---
video_id: acRBzghbhE4
video_url: https://www.youtube.com/watch?v=acRBzghbhE4
is_generated: False
is_translatable: True
summary: "Brent Everman (AWS), Jean-Paul Bonny (Duolingo), and Seth (AWS) present Duolingo's successful journey to compute savings and improved performance by migrating to AWS Graviton processors and adopting cost-optimization strategies. Seth begins by explaining AWS Graviton's origin: custom-built, ARM-based chips designed from the \"Nitro system\" security chips. Graviton processors (now in their 4th generation) offer up to 60% less energy consumption for the same workloads and significant price-performance benefits. Seth notes that Amazon itself used over 250,000 Graviton chips to power Prime Day 2024, demonstrating their scalability.\n\nBrent, Duolingo's Technical Account Manager (TAM), details how the partnership with Duolingo evolved. He highlights key achievements such as the viral 2024 Super Bowl ad (\"Duolingo's butt\"), where they successfully sent 4 million notifications in 5 seconds without crashing. He also covers Duolingo's migration of their English Test (DET) workload to a separate AWS account for compliance and their ongoing migration from ECS to EKS for better scalability. Brent emphasizes the TAM's role in proactive cost optimization, citing specific wins like a 50% reduction in DynamoDB costs through targeted table deep dives and migrating S3 storage to intelligent tiering, which saved $13,000/month.\n\nJean-Paul \"JP\" Bonny then takes the stage to explain the technical execution of Duolingo's Graviton adoption. They started with managed services (RDS, ElastiCache) because AWS handles the compatibility complexity. By migrating to Graviton-based instances (e.g., R5 to R6g), Duolingo achieved over 75% adoption in managed services with minimal engineering effort. JP also discusses how they optimized their EC2 spot instance pool by moving from expensive NVMe-backed instances to EBS-backed instances after realizing they were only using ~4% of the local disk space. This change alone saved over 10% on EC2 costs while improving spot availability.\n\nFinally, JP describes the challenge of migrating their Python 2 monolith (\"Duolingo 2\") to Graviton. By testing in canaries and slowly rolling out to production, they achieved a 10% reduction in task count on Graviton 2, and a further 20% reduction on Graviton 3, purely due to the chip's efficiency. Today, over 30% of Duolingo's microservices run on Graviton, and they aim for 50% by 2026. The session concludes with a look at their future roadmap, including full migration to EKS, adopting Carpenter for spot management, and implementing multi-architecture builds to seamlessly switch between x86 and ARM based on spot market availability."\nkeywords: AWS Graviton, Duolingo, Cost Optimization, Spot Instances, ECS to EKS Migration, DynamoDB, FinOps, S3 Storage Lens, Nitro System, Technical Account Manager (TAM)\n---

Before we get started, I just want to get a sense of who's joining us in the room today. So by a quick show of hands, how many folks out there have ever looked at their cloud bill and thought there has to be something better? A few hands up for that. Thank you. Um, how many of you maybe worked with a technical account manager, solutions architect, a real business partner, doesn't have to be at Amazon. It's really made a difference in your business. All right, a few hands up for that. That's great, thank you. How many people have heard of graviton processors but have not yet a chance to implement them in your business? All right. A few hands up. All right, keep these things in mind because I think the presentation today is gonna resonate with many of you. So here's the story. It's 2024. Somewhere in Pittsburgh, a Duolingo engineer is looking at their quarterly cloud spend report. You know the feeling, the numbers are growing faster than you expect. But your, your features are growing, your users are expanding, and it feels like if you start cutting costs, you might be cutting corners. Now Duolingo isn't just any company. They've, the app, they're the app that's taught over 500 million people a new language through that persistent green owl that we all know and love. They process billions of learning events, serve personalized content to users around the world, while maintaining a user experience that keeps us coming back day after day. All right, one more question. How many people have a streak going in their Duolingo app right now? All right, and lots of hands. Love it. That's great. But here's the thing about success, it scales. When you're Duolingo, scaling your infrastructure costs scale too. So the question becomes, how do you maintain this growth trajectory while optimizing for efficiency? That's what we're going to talk about today. It started with a conversation between Duolingo's technical account manager, who's someone who knows that Amazon's founding principle isn't just about offering lower prices, but about finding innovative ways to deliver exceptional value. During this session, we'll hear directly from both sides of that conversation. We'll start off with an introduction to AWSS Graviton processors and learn about the methodology that took Duolingo from cost savings analysis to proof of concept to enterprise-wide implementation. You'll also discover how AWS graviton processors. Became not just a cost saving measure for Duolingo, but a competitive advantage. And most importantly, you'll learn how the right partnership can take what feels like a constraint, your cloud budget, and turn it into an opportunity for innovation. So whether you raise your hands to any of those questions or you're simply curious about how two organizations can partner together, this is your story. I'm joined by Brent Everman and Jean-Paul Bonny. Let's dive in. So I get this question a lot. Why do we build our own chips at Amazon? There's definitely choices in the market that we can, that we could use. The idea at Amazon is we work backwards from, from the customer experience and specialization was key. We spent a lot of time operating and building cloud infrastructure and running cloud applications. So we were able to take that specialization and build that into our, into our hardware. Speed and innovation are also key. By having hardware and software engineers under the same roof, we're really able to deliver for customers at a higher pace. And then security. Security is the highest priority at Amazon, and the graviton processors actually came from building security chips, which we'll talk about here in a minute, and we turned them into the general purpose processors that we use today. So those chips came in what we call the nitro system. How many people are familiar with the nitro system? A few hands up for that. That's great. Combination of hardware and software, um, the chips provide a simple hardware route of trust. They measure the contents of the flash devices so we can be sure what's running on the when the firmware is running is what we expect. It monitors all the hardware interfaces and blocks unauthorized modifications. We designed the nitro system. To provide confidentiality between customers as well as confidentiality between customers and AWS. There's no mechanism or system or person that can log in to an EC2 nitro-based uh uh server to access to access customer data. There's no root user, no no no interactive access, just a set of narrow and authenticated APIs that we use to manage the systems. I encourage you to take a look for more details on that on our website. We have some third parties that have validated that. So this is what a nitro system looks like today. When we started, we had customer instances running with a Zen hypervisor. For those who have used AWS for a while, Zen was where we started. And for those of you who run data centers, 20% is pretty normal as an overhead for, for a hypervisor. Uh, Zen was great. It did a lot and took all these functionalities, handling connection, VPC networking, connections out to EBS and so on, but it used those resources on the host CPU. So we want to start offloading that to hardware-based devices. So move IO functionality to nitro cards, also move management and monitoring in the security functions. Then we were able to replace Zen with a lightweight hypervisor. What did that do for us? That allows us to sell almost 100% of that CPU to customers, allowing us to keep our prices low. So I talked about pace of innovation. So you can see sort of the growth of the EC2 instances over the, over the years. Nitro, we can think of nitro as Lego, Lego building block that lives underneath all of AWS servers and it was able to significantly accelerate our engineering pace. So in the 1st 11 years, it took us, uh, it took us 11 years to grow from 1 to 70 instances. And then with nitro once we added that, we're able to go to 950/950 instances in just 8 years. So these 950 instances provide our customers with that flexibility to choose the optimal instance type for their specific use case. So with all those instances, how do you choose? So I'll talk through um uh three of our, our most popular CMs and Rs. These are compute optimized, general purpose, and memory optimized. And what does that mean exactly? It's really a memory to VCPU ratio which you can see on the screen. So, it goes up as, as the, as the instance family changes, but what I'm about to talk through as we decode this will work for all of our instance families. You'll be able to see how this works. So we're looking at a C8GN 2X large. So what does that mean? C tells me the series. So I know this is a compute series instance, which means that for every VCPU I have 2 gigs of RAM. It's the 8th generation, so you can see where it sits in the lineage and the history of, of our instance families. And this instance has two options. GN. So what the G tells us this is a graviton instance, which is what we're gonna talk about today. And N tells me there's some specialized networking for this particular instance, a little different than than standard. Now, it's a 2X large. That tells me how many VCPUs, medium, large, extra large, 2 X large, we're doubling each time. So this is 8 VCPUs. So if we look at this, this is a compute-based instance with 8 VCPUs. That tells me I have 16 gigs of RAM based on the ratio and the networking feature is, excuse me, up to 50 gigs of additional networking and it's based on graviton. This will, like I said, this will work across our instance families. So a little bit of history of graviton. We started in 2018. And we work very closely with ISVs. We know that customers are using third party applications. Maybe you have a security agent or an observability agent that you need to run in your environment with your with your own applications. So we leverage this heavily to get those ready for customers. In 2019, we launched Graviton 2 Performance Increases, and this is where Graviton starts to become more widely adopted. Lots of customers using it, also using it heavily at Amazon. Graviton 3 launches in 2021. Again, compute performance improves, but our customers are telling us they need better floating point, so we improve that for customers as well. That having that software and hardware teams in-house allows us to do that more rapidly. And then in 2023 we launched Graviton 4. 30% better performance, but now our customers are saying, I want to run larger workloads, maybe a database, something that needs more cores. So we improved the number of cores. Improve the memory bandwidth on Graviton 3, from Graviton 3, excuse me, and, and get that out into our customers' hands. How can you use graviton today across EC2? So general purpose compute and memory optimize, we talked about, right? You can see the different options available that are out there, even burstable instances our T series are available. Maybe you need something more specialized. Accelerated computing, you need a graviton processor and an Nvidia GPU. Maybe you're doing something that's storage intensive, so you need some SSDs, local, some fast SSDs, uh, NVME storage locally, things like that. The iSer will help you with that. Then we can get into HPC optimized if that's something you need, we can help with, with that part as well with the HPC 7G. Some other ways to look at adopting Graviton, you hear some of this from Duolingo today. Managed services are a great place to get started. We've done the heavy lifting and got the engines running on Graviton, and you just bring your data. So maybe you've got a database that you're using RDS. Maybe you're using Elastic cash. We take care of getting that running for you and you get the, the price performance benefit of using Graviton across those managed services. Analytics is another great place to start. I talk to customers all the time. They start with EMR. They're already using it in some form. They're able to change that over to graviton and get that price performance benefit very quickly and very easily in their environments. Compute Duolingo is gonna talk today about EKS and ECS, but you can also consume graviton under Fargate and lambda as well. Then maybe you're using Amazon Sage Maker for machine learning, excuse me, for machine learning work, machine learning workloads. Um, you can consume graviton instances under Sage Maker as well. All right, one more question. How many folks maybe have a a sustainability goal for their organizations? A few hands up for that. Great. So this is one of the things about Graviton, 60% less energy to compute the same workloads. So we work with a lot of customers. Sometimes you're looking for that compelling event for customers. We have found sustainability is a key. They want to be able to show that they're more sustainable, and graviton is a way to do that. And my team works with, works with uh customers to help them understand their power consumption today. So last thing before I sort of hand it over to Brent here, but this is how we're using it at Amazon. Uh, Prime Day, uh, through, through the last few years. In 2022, we were able to leverage Graviton to minimize the scale of our, of our compute workloads for Prime Day. So our business was growing. Our prime day workload for 2022 was only 7% larger, right? That's the idea. We're able to maintain the growth while the, while the retail business was growing. In 2023, over 2600 services were powered by graviton processors. In 2024, over 250,000 AWS Graviton chips were used to power Prime Day, and this past October, about 40%, more than 40% actually of the EC2 used by Amazon.com on Prime Day was powered by Graviton. So we're taking advantage of it and getting that scalability and that flexibility for our, for our organization and making the same available to customers. So with that, I'm gonna hand it over to Brent because he's gonna talk about how technical account managers work with customers. All right, thank you Seth and how many people here know what enterprise support is a show of hands. Alright, a few people and so for those of you who don't know what it is, enterprise support is one of our, uh, support offerings from AWS, and with it comes access to a, uh, a technical account manager. And so how do technical account managers or better known as TAMs help enable customers? And so this is the diagram that we use when we talk to customers who are looking to enroll in enterprise support. What is the value of a TAMM? So first and foremost, TAMs are focused on cust helping customers achieve their business outcomes, whether that be user growth, uh, reducing operational overhead, or leveraging Gen AI. TAM's also um help educate customers about how to leverage AWS services and there's a big difference between um building a POC on AWS and then operating that in a mission critical production environment and TAM's leveraged their expertise and best practices to help customers do that. And then we also help elevate customers' voice with service teams in our leadership, especially when there are blockers with adopting or operationalizing a service on or uh a workload on AWS. And lastly, we help customers evolve by proactively learning about their initiative initiatives, discussing findings, and diving deep to drive technical solutions. And so now we're gonna talk about how as have I as a TA, I'm the Tam for Duolingo, helped drive some of their strategic outcomes. So the, the first thing we're gonna talk about is high profile events. TAM's help customers plan and execute these high profile events on AWS. We leverage a structured approach called AWS Countdown, uh, to plan and execute these events on AWS. And the example that we have with Duolingo is the Super Bowl ad they did in 2024. So how many of you here remember that ad or even received the notification that we have a screenshot of on the screen? Alright, a couple people, cool, so you're gonna learn more about this ad, um, and, and the impact that it had for Duolingo. And so Duolingo came to us and they had never done anything at this scale before. They were looking to send 4 million notifications within 5 seconds of the ad airing on, you know, during the Super Bowl. And they, they've never done anything of this scale and we're like came to AWS and myself were like how do we do this? How do we, you get to this scale? They built out an architecture and so I started engaging with them, um, using that structured countdown approach, um, to plan and execute this event on AWS. And so we started by reviewing their architecture, highlighting risking gaps. Um, I was also basically a part of their engineering team attending their weekly engineering team meetings where we discussed status, risk issues, um, uh, for the project. I also helped them secure enough EC2 capacity for the event. As you might imagine, sending that many notifications in 5 seconds took a lot of EC2 capacity to do that. And then also help them outline fallback plans should there be potential issues and lastly I worked alongside of them to conduct test events and I cannot stress the uh the importance of those test events enough. We conducted those test events over Halloween and Thanksgiving and each time we learned something new that we took back to improve the plans and the architecture for the actual event. And so during the event, you know, I joined the War Room Bridge with the Duolingo team just like I was a part of the team, and I remember getting that nervous feeling because we had months of preparation on the line, and I remember, you know, they had built a nice UI where they had a button to send out the notifications. I remember that hitting them hitting that button and those notifications going out and the kind of that relief and so they did successfully send out those 4 million notifications within 5 seconds of the ad airing. And from a business perspective this was really about re-engaging those lapsed users, you know, if, if you use Duolingo at all, you get notifications every day, um, if you don't do your lesson and so they were looking to engage those users who hadn't done their lesson in a while and this was, you know, driving towards their business outcome of user growth and so from a Duolingo perspective this was a very successful event that we, you know, we put months of planning and preparation into. And if you had seen it or if you're on social media at the time, you probably heard about it because it was a viral hit and if anyone didn't see the ad running it was basically um Duolingo's butt, um, blowing up um as you kind of see on the screen over here and so that was even discussed um on this uh late show with Stephen Colbert as well. All right, and so now moving on to um another topic and so for those of you who aren't very familiar with Duolingo, they also have the Duolingo English Test or DET and so this helps to validate a a learner's proficiency in English. And so you know, um, Duolingo was a born in the cloud, uh, cloud startup, and you know they had, um, you know, you know, like many born in the cloud startups built everything in a single AWS account. Everything was running in a single AWS account and that, um, eventually became the management account of the organization. And so this was preventing that that kind of structure was preventing them from achieving some compliance certifications that they needed to expand DET and grow the product. Many of the organizations and universities that they work with wanted these compliance certifications, and so I started working closely with the Duolingo team to look, look through what are the options to help meet some of these compliance certifications. And we determined that migrating to a brand new AWS organization and then breaking out that DET workload into its own AWS account was the best path to help achieve that. So I started working with the with the Duolingo team and creating a step by step migration plan to migrate to this new AWS organization and help them execute it, um, by their side. One of the things you know we saw an opportunity for improvement around was security in their previous structure, you know, due to having all those workloads running in the management account they weren't able to implement some security best practices so we took this as the opportunity to um implement the security best practices like uh service control policies or SEPs if you're familiar with those. So we were able to execute this migration to the new AWS organization with no impact to their production workloads or downtime at all. And so the next step then was to migrate the DET workload over to its uh uh own AWS account and we took a very similar approach where we created that step by step migration plan. I worked very closely with them to migrate that to its own AWS account and out of that, you know, we were able to do that with minimal impact to, you know, production for the DET product. And then, you know, out of that, so what did this all result in? They were able to, as a part, you know, due to these efforts as well as other efforts they had internally, they were able to achieve these compliance certifications which then opened up the door for growth for its DET product. All right, so how many of you here have been a part of a migration from on-premises to AWS, a show of hands. All right, so a lot of you, you know, may have horror stories from those migrations, and, and that's oftentimes what you think about in terms of migrations, uh, in AWS, uh, from an AWS perspective. But we also support migrations like this one from ECS or Elastic container Service to EKS, which is Elastic Kuberneti Service. So Duolingo, as I mentioned, is a born in the cloud startup, and they started by running everything in ECS because ECS is super simple to use. You build a container, you throw it at ECS, and the ECS does most of the heavy lifting for you. But as Duolingo started to grow in scale, they knew they needed some additional features and the ability to customize their compute platform for their requirements and after thorough analysis on their end, they decided that migrating to EKS was the best path for them for the next evolution of their platform. And so myself and the broader AWS account team started working um with the Duolingo team from the very beginning on this migration. We first started off with enablement and education, and some of you may be uh familiar with immersion days, which is where you can get hands on with an AWS service and so a lot of the Duolingo engineers didn't have any exposure to Kubernetes or EKS prior to this, so we decided to conduct these hands-on immersion days to give them a better understanding of the concepts, how EKS works and how, you know, how to deploy their services on EKS. And one of the things we heard during those sessions was that the Duolingo engineers were somewhat hesitant about this migration. They were wondering, is this the right path for us? Is this the right thing to do for us? And we, so we, um, spoke with the project lead at Duolingo about how to address these concerns, and we decided that we would, uh, present at their internal engineering summit. Duolingo hosts an internal engineering summit every year where they bring all of their engineers together and they present on best practices, new projects they've worked on, new technologies they've been using, and we brought in one of our Kubernetes experts to talk about what is the value of Kubernetes and what it can unlock for the organization. I remember coming up or um one of the engineers in the crowd coming up to us after. The the talk is, uh, to myself, the Kubernes expert and the project lead in saying something along the lines of I was very hesitant about this migration. I wasn't sure if it's the right thing for us, but now I see the value and I wanna be one of the first services to migrate to EKS and so a lot of these enablement efforts helped instill confidence in the broader Duolingo organization that this was the right move for them. And so then as Duolingo started building out this new EKS platform. We conducted uh targeted deep dives, so we, uh, we knew they wanted to use Argo CD and Carpenter and so we conducted deep dives on those to ensure they were properly set up and configured in their environment for their requirements. And AWS enterprise support in TAMSs can provide proactive guidance on many AWS services, and we knew that with the size of Duolingo's ECS environment there could be potential challenges with scaling with EKS, you know, especially since that, you know, the EKS team recommends keeping the number of pods and nodes within a cluster below a certain threshold, although some of that has changed with some of the recent announcements. But they, um, we wanted to, you know, get out ahead of that and ensure that they were, um, not only scaling the nodes in the pods, um, in, in an optimized manner, but also we dove into other areas like observability and core DNS to ensure they were set up in an optimized manner. And one of the other things that we worked with them on was a multi-cluster platform. So you know, um, Duolingo wanted to design this multi-cluster platform. You know, uh, for a couple of reasons. The first one was, you know, should they encounter these thresholds, they wanted to be able to move workloads between clusters, and they wanted the ability to do maintenance if they had maintenance or upgrades they needed to do on a cluster. They didn't wanna have to have production downtime for those. And so we worked closely with the Duolingo team to design this multi-cluster platform that has been a, a really big win for them. And then lastly on the networking setup side, um, you know, for those of you, um, IPV6 has been out there a while in the community, but I would say it's not often been used and so there's not a lot of expertise in IPV6 and one of the things that the EKS team recommends is using IPV6 because it can help with things like IP exhaustion that can occur with IPV4. And so I worked closely with the Duolingo team to ensure their networking with IPV6 was set up not in an optimized manner, not only from a cost perspective but also a security perspective and what did all these efforts result in? So Duolingo has built out their new EKS platform. And so they have started migrating workloads in 2025. They've even migrated some of their most critical services over to EKS that shows the trust that they have in the platform, and we are continuing to work with them in into 2026 to complete this migration. So TAMs and enterprise support are expert in cloud cloud optimization. So we want to ensure your environment is optimized for the business outcomes that you are trying to achieve. There are 3 main pillars that we, um, we look at from an optimization perspective cost, security, and resilience. And I'm gonna talk about a few examples of how I've worked with the Duolingo team to optimize their environment. The first one we have here is is around S3 storage lens, um, and so I started working closely with the Duolingo team 3 years ago and one of the things I noticed is that they didn't have a lot of visibility into their S3 buckets. Costs kept growing month over month, but they weren't sure what was driving that. Was it the increased user growth or was it something else driving those increased costs? And so I, I started to dive deep into it and recommended a newish service at that time called S3 storage lens which can provide visibility across your S3 buckets at your at the AWS organization level and can even provide visibility down to the prefix level within a bucket. So after setting this up we started to dive into the data that it provided and so these are, you know, this has been, you know, used for many different use cases examples across Duolingo but I just wanted to highlight a few of the wins that we, you know, we were able to achieve by leveraging S3 storage lens. The first one was we identified a bucket that was using S3 standard storage and we're able to uh migrate that to use intelligent tiering thus saving $13,000 a month. Next, we, you know, due to the, the visibility in the metrics that it provides, we were able to identify an unused 1 petabyte bucket and delete that bucket. So you know that was significant cost savings as well. And then lastly during the DET migration we were able to leverage the visibility metrics it provided when we had to migrate S3 buckets from the old account over to the new account to ensure that migration was uh fully complete. Now moving on to cloud front. One of the things that TAMS can do for their customers is conduct architecture reviews. When I was conducting the architecture review for Duolingo, I noticed that their mobile app was directly connecting to their application load balancer or ALB. While that's a perfectly acceptable architecture, I knew that with Duolingo's global user base they could benefit from having cloud front sit in between there with all of the, you know, the global cloud front edge locations they could reduce user latency by having them connect to those edge locations route over the AWS backbone instead of routing over the public Internet. And so I worked closely with the Duolingo team to um introduce and implement Cloudfront between their mobile app and their application load balancer and while that was a big win, it reduced latency for their user, it also opened up additional optimization on the security and the cost front. So we have uh projects planned in 2026 where um we're gonna look to improve security by leveraging the AWS WAAF or web application firewall where we're looking to block malicious bots at the edge. And then we're also looking at improving caching within Cloudfront, which will reduce hits to the origin and thus optimize on cost as well. Right, and now moving on to Dynamo DB. So Duolingo was heavily using Dynamo DB, but costs kept growing month over month, and it was the 2nd highest line item on their bill. Engineering teams when talking with the Duolingo engineering teams, they weren't very confident that they were using Dynamo DB correctly and if they were using it in an optimized manner. And, and there was also some hesitancy internally to expand the adoption of of Dynamo DB and they sometimes stuck with relational databases like RDS even when they knew Dynamo DB was a better fit for the use case. I saw this as a as an opportunity. Um, to conduct enablement sessions, so similar to what we did for EKS, we conducted those hands-on enablement sessions with the Duolingo engineers to give them a better understanding of Dynamo DB concepts, um, how to optimize for Dynamo DB, and then also, you know, how it works under the hood to give them a, a much better understanding of Dynamo DB and how to work with it in an optimized manner. During those sessions, one of the things we heard is that they, they were sometimes not sure how to model their tables correctly within Dynamo DB. They had a lot of uh experience doing this in a relational world, but they not a lot of experience with single table design. And so in talking with Duolingo engineering leaders we decided to conduct another session at their internal engineering summit on best practices for modeling tables at with Dynamo DB with one of our Dynamo DB experts and so we conducted that session to a standing room only crowd at their internal engineering summit which had rave reviews and so that kind of concluded our enablement piece for Dynamo DB and next we wanted to focus on optimization. And so there were a handful of tables that were cost drivers for the organization. And so what we decided to do was conduct targeted table deep dives on those and better understand the use case, their access patterns, and then make recommendations for optimization and help the Duolingo engineers implement those optimizations. And what did this all result in? And so you can see with the graph up here, uh, within 3 months from when this program began, they realized a 50% reduction in their, their dynamo DB cost, and that came from those targeted table deep dives and the optimizations that were implemented as part of it. We also optimized their reserve capacity for their tables that were using provision throughput. And lastly, we converted some tables that were write heavy, read less to infrequent access, access storage. And I, I'll say even the even more telling success of this was, you know, you kinda see how, uh, usage increases over the months after that that wasn't due to Duolingo engineers going back to their old ways. It was actually because Duolingo engineers felt much more confident in using Dynamo DB because it helped accelerate their development and it became the database of choice for Duolingo and I sometimes joke that Dynamo DB is Duolingo's favorite AWS service. Right, and the last one I'm gonna touch on is around EC2 instance pool optimization. And so, as I mentioned earlier, Duolingo, you know, runs everything on ECS and now is in the process of migrating to EKS. So all of their containers run on EC-2 and optimizing their EC2 instance pool can have huge impacts on not only performance but also cost. And John Paul, who's coming up next, is gonna dive into a lot more details on this, but I wanted to cover this at a very high level first. You know, I've helped them migrate to and adopt the latest version of Graviton Graviton 4, helped them migrate additional services, um, from just using Intel over to using Graviton. We've helped eliminate some of the older generation of instances that they had in their instance pool that were less cost performant and lastly they had been only using NVME backed instances and so I helped them introduce EBS backed instances. All right, and now I'm gonna welcome JP on the stage, um, who's gonna dive into more details on this. Thanks, Brent. So I'm gonna talk about our graviton journey in a few steps. First, how we utilize managed services, uh, with Graviton at Duolingo. Then I'm gonna talk about how we accelerate that graviton adoption by optimizing, expanding our EC2 instance pool. Finally moving actual microservices onto Graviton. And then what's next for us in the world of compute? So if managed services of Graviton, once we first heard about this, we were really excited to start utilizing these for the immediate cost savings, but also given how much work we'd have to do. I mean, AWS handled all the crazy compatibility issues, so all we'd have to do is just upgrade our instances, right? Well, there's a little more to it than that. We had to come up with migration plans to make sure we could move our managed services over with little to no interruptions for our learners. So Seth mentioned a lot of services that have managed services that have Graviton support, but I'm gonna focus on Alasa Cash and RDS. So for Alaska Cash, this is actually a pretty boring story for us, which is what makes it very exciting for me to talk about today. You never wanna have like a crazy exciting story, right? That would take too long. So thankfully, because of like all the work AWS did with all the uh compatibility details and all the other issues, we could just focus on upgrading our caches like we would any other time. So we wound up migrating a lot of R55 instances to R6G's and let's talk about the different engines that we had to deal with when moving lots of cash. So let's uh start with Mem cash on the left here. So essentially we'd have our market sources that run with Memcash should have of course the cache. And in order to mitigate risk, we actually wound up creating a new graviton cache. So we actually had our services writing data to both, to both caches. And we did this kind of like, like I said, to mitigate risk. So while we were still having data being written to both caches, we were slowly shifting. We were, and once we felt once we felt comfortable with all the data in our graviton cache, we started to slowly shift reads over to that new cache away from the old one. And then once everything looked good and nothing was crazy on our, on our graphs, we would then start to spin down connections to the old X86 cash, decommission it, and we could counter cost savings and be on our way. Now looking at Reddis, this is a little bit of a different story since ideally it should be as simple as Memcache, but if you're running a couple of old like Reddis clusters, they might not have the latest version, you could uh introduce having some downtime. And we wanted to naturally mitigate that. So some of the ways we did this was looking at our applications, seeing if we had solid default values for these clusters, and also looking at trying to upgrade these when we had lower volumes of traffic. So once we felt comfortable with all the mitigations in place, we then went to the console, changed our instances from those R5s to R 6Gs. Our data is intact, we had a little reboot and then things were fine and we had our graviton cash. So now RDS is a similar story to Elastic cash, but it can be a little more complex due to the auto-scaling readers, writers, and probably just those like crazy applications you can have on RDS. So it can be scary if you especially have to do like a failover or a database upgrade for something critical, but we still approach this as we wanted to, of course, minimize downtime and make this easier to do in the future. So a show of hands, uh, who has a service running on RDS? Anyone? OK, yeah, a lot of you. So in this case, uh, this flow chart behind you might look a little familiar, but if not, let's, uh, let's talk about it. So thanks to, you know, RDS being a managed graviton service, it was as simple as just doing a typical like RDS upgrade where we would add our new instances to our RDS cluster, and then we'd have to, of course, like wait for us to get online data syncing, so we, we might be like waiting a little bit depending on how big your cluster is. And once we and once that's available, we can then start moving those old instances. Everything's pretty good so far. And then finally we get to kind of the scary part doing that writer failover. Now sometimes even upgrading a couple of databases, I still get a little scared doing this, but once we're able to do that failover, of course mitigating risk with those applications, our writer will now change. Things are fine, looking good, and then we can deploy our service, clean up our references to the old RDS, and just make sure things are running smoothly again. So we wound up applying applying this methodology to both our Aurora and post-grass workloads, and after doing this a few times for a couple of upgrades, we thought to ourselves, hey, let's try and minimize the amount of engineering effort required, and we created a temporal workflow to help, help other engineers and service owners to do this without having platform support. And we wound up doing that at least 10 times to mainly accelerate that graph that managed services adoption and even resizing instances as our traffic grows. So with all that said, we now have over 75% of our managed services running on Graviton just because of all the planning and work that we've done, uh, in our org. And just given how easy Graviton is to use whenever new service owners or teams want to spin up new managed service, uh, managed services, Graviton is the default option going forward. So now we're saving a little money with our managed services on Graviton, but we want to step it up to get some bigger, uh, cost savings for our workloads. So of course enter EC2. It's the largest item on our bill, and we use it for a lot of things. So like Brent said at Duolingo, we were running a lot of, we were running most of our EC-2 on NVME instances, and at the time that was because of all the data we had. But over the years as we've been migrating workloads around, we've been reducing logging, improving reservability. We kinda had this thought to ourselves, do we really need all this storage? And then we also have this other goal of wanting to be able to adopt newer EC2 instances quicker. So instead of waiting for like that NVME specific flag on that decoder ring, we want to just run on a wider variety of instances. So essentially we wanted to take that letter D out of our instance type thing and expand our EC2, our EC2 pool. So for some context for us, uh, our EC2 setup isn't just simply using stuff on demand. We do use a good amount of the, the spot, uh, the spot market for powering our workloads. And we also work with AWS TA as well as several third parties to analyze our EC2 usage to make sure we're diversifying our workloads across spot, on-demand, reserve instances, savings plans, all the typical EC2 things. So with all of this said, how do we go about trying to uh tackle this instance pooled instance type challenge while still staying on the spot market and utilizing our cost savings? So first, let's actually just talk about the spot market for a little bit. So like I said, at Dylan Goat we're actually leveraging it pretty heavily, so we're essentially using spare EC2 capacity to power most of our microservice workloads. And these come with a very significant benefit where it's up to 90% off the on-demand price. Now that sounds pretty good to me, but there is a trade-off we have to consider, and that is when, and that's spot interruptions. So whenever the on-demand pool is seeing issues with, uh, their EC2, they will reclaim the spot instances. So you essentially have 2 minutes to vacate your workloads, and then that machine will get reclaimed. So how, how do we at Duolingo deal with, uh, deal with this, uh, trade-off? So we've designed most of our microservices to support, uh, something called graceful shutdown and of course auto scaling. So graceful shutdown essentially it means whenever we get that underlying sig term signal from an EC2 instance, our ECS tasks will see that signal and then they will reject incoming requests from the load balancer, and those will get routed to another task behind it. And but it'll still process in-flight requests, so ideally our learners aren't gonna see any disruption while we're still processing their requests for saying a shriek or doing a lesson. And while that's also happening, and while that task is shutting down, we're spinning up another task on another machine that's hopefully not getting interrupted. So that's actually one way we've been able to stay on, on the, on the spot market while having little to no interruption. And then also another point about being on the spot market is just diversifying your workloads, of course running your EC2 and multiple availability zones, but also having a a breadth of instance types. So with that said, let's actually start talking about how we went about adding more instance types to our our pool. So with EBS we had to have a couple considerations. First off being this space. I know I mentioned this a couple of times, but like how much space do we really need? I mean these are really big like NVME drives. The second one being spot interruptions, we don't wanna spend our time spinning up new EC2 machines and we wanna make sure these aren't gonna be reclaimed in case of interruptions. And then finally rolling this out, how do we roll this out and in worst case, how do we roll back if it all kind of goes wrong? So first looking at this space, of course we have a, so as you can see from the screenshot we have a lot of room in these EC2 instances and we first wanted to kind of get a feel for how our microsources were using these. So naturally I decided to just uh take an EC2 machine SSH in and now we're doing doing a disk screen and looking at it. So the first thing, uh, so this, this is a C5D 18X large. So there's several workloads utilizing the CPPU and memory before I bin packing, but as you can see, like, look at the purple rectangle, we're not really using that disk space. There's a lot more room for other things that could be there. So we're only using 4% of that NVME drive and then about 6% for that EBS volume. And then looking at this gave me the confidence to kind of look at the rest of our fleet, and we saw similar patterns for most of our microservices. Now we still had some workloads that still would need that NVME drive, whether they're reading or writing from disk, or they just have a lot of files they're storing, but thanks for that. But, but thanks to using ECS we were able to take it, we were able to utilize task placement constraints where those workloads could specify the type of machine they needed. So in that case they could still deploy, get those NVME machines while most of our other workloads were more compute agnostic and they could just potentially use other EC2 machines. Next were interruption rates on these instances. So thankfully AWS has a pretty good website with the Spot Instance Advisor where we're able to look at general interruption rates for these machines. So we're able to, so look at that website, we had, we had an idea that these EBS machines will be interrupted less or about the same as the NVME machines. And and also here's a picture of us graphing our own spot data for a C7G2X large or graviton 3 processor. The dark blue is the NVME option and the teal is the EBS, and we can see that the EBS is interrupted a lot less, which means it's more available in the spot market compared to the NVME counterpart. And then the reason it goes down a bit is as we were rolling this out for this one specific instance, we found that they were about the same with the EBS being ever so slightly more available. So seeing this for this one instance type gave us more confidence that this was the right way and the right way to expand our pool and even improve reliability. Finally, how do we go about rolling this all out in production? I mean, as much as I'd love to just roll this out in production, we want, we focused on doing small changes on and using small clusters to test this out. So when I first saw this, we, we, we first deployed one microservice with a mix of EBS and NVME instances, and we were able to observe interruption rates and saw that nothing was changing, which is great. And then after that we worked on moving up to slightly bigger ECS clusters like a dev cluster, a staging. And then a few different versions of production and again thanks to using task placement constraints we felt pretty confident that workloads that still needed those those those discs could still have access to them while the rest of our services would utilize whatever is in our pool mainly with the EBS. So now after all that said, uh, what were some of the results of this? So we were able to see an over 10% savings on our on our EC2 costs, which is huge without changing any application logic. So most service owners didn't really know where their machines were running, and they were still running the same. And being on the fin ops team, this was a really great number to see since I care about cost efficiency. But more importantly than costs, we're all this also positioned us better to adopt newer instances without waiting for specific options and also helped improve our, our pool flexibility. So you can say this is one of the few times where we have a cost savings win and a reliability win come together. Now we talked about moving moving to graviton with managed services and of course the work we did to expand our pool. So let's talk about microservices. So essentially we started early with ARM and we had to pick a target. Finally update our CICD and then how do we actually accelerate that to see to be more in graviton. So first, uh, starting early, so we at Dulingo were, were lucky enough to start working on Graviton early back in 2022 a little bit. So when Apple first moved their laptops over to uh ARM from Intel, we used this as an opportunity to try and help dog food this amongst our developers, since who didn't want to get a new laptop to try out a new technology. And thankfully, thanks to our uh dog foodie and using this technology, we noticed a lot of different compatibility issues early back in the day. A lot of things when started up, DACA might be defaulting to AMD 64 instead of ARM 64. Your favorite package might be deprecated or just not support ARM. We ran into a lot of issues, but thanks to just people working together, we were able to help uh solidify our local development story for this new architecture. And we also saw meeting benefits there. First, things were running a little more efficiently and laptop fans were pretty quiet, so that made meetings a lot smoother. And so this is all great for a local story and eventually trying to get on the graviton, but we first need to pick a service to move to Graviton that would show people that this is something that's worth investing in. So naturally you pick the hardest thing you can, a monolith. So a show of hands, uh, who has a monolith where it's worked with the monolith in their company. OK, I, I see a couple of hands here. I know I have. I'm talking about it. So our monolith of Duolingo is called Dulingo 2. I don't know what happened to Duolingo 1. That'd probably be another talk, um, but so like, like most monolith things, our monolith for Duolingo 2 was written in Python 2. It's pretty critical. It's a lot of traffic. Not many engineers know all the exact ins and outs of it. It's just your typical monolith things. But by picking this as a target, and we wanted to we wanted to prove to our engineering org that this is technology we want to invest in and this could have a big impact for us for cost efficiency. So how do we go about doing this? First you wanna build it locally and thankfully we had the investment to do that locally and then when you run into that, of course you have to update some packages, remove things, really look at some parts of your code and wonder why did we do this. But that's only half the battle is running it locally. How do we actually get it out into production on Graviton? So naturally, here's another flow chart of the uh the process we wound up taking. So with our model, if we had a specific configuration for deploying it, so we naturally need to update that to support Graviton. And some more often than not you might see some build failures and some other packets might need to be removed here and there, but eventually we're able to get it built and then naturally when you get it built, you want to deploy it, but not to all of production. So Dulinga, we believe in using canaries and multiple environments to slowly phase these changes in. So when we were deploy when we were deploying our graviton monolith, we wanted to just try in our canary, I'll see if there were any weird runtime errors or code paths we couldn't cover locally. And when we did, we worked on rolling back and then looking at those logs and then redeploying and then redeveloping locally and kind of like going through this loop a couple times. So we started with the canary, then a smaller production environment, and it's kind of kept growing until we eventually reached all of production. And we had some pretty big, uh, very great benefits there. We saw a 10% task count reduction, which meant obviously your code, your code's deploying quicker and we're also spending less money. But we didn't want to stop with just gravi scales of graviton 2. So we immediately wanted to utilize newer graviton instances, so we wound up adding graviton 3 or C7Gs to our pool on top of those graviton 2 instances, and we saw something even more magical happen where when we deployed it again without changing any of our Python 2 code, we saw our task count, our task counts dropping by 20%. And at first we were kind of scared, like latency is going down, task counts are going down, what's happening? But I think that just speaks to the efficiency of the graviton chips or from generational leaps. And then with lower task counts we also had lowered, uh, shorter deployment times. So instead of taking 2 hours to deploy a model that feels a little over 100 minutes. I don't know about you, but like I love getting time back to do, to do other things and not watch a deployment go out. So this was pretty much pretty huge for us. And thanks to all the investment we did with ARM and Graviton, we were able to get this advantage. We're able to utilize this newer generational upgrade essentially for free. Which I I said, being a fin-offs person, that is, that's some cost efficiency right there. Now I wanna say we just immediately started moving more things over to Graviton, but unfortunately due to the high demand of Graviton instances on the spot market, the availability wasn't great, so we had to pause our Graviton journey. But not completely. We still were working on improving the tooling to make sure that other service teams could still move over their workloads to Graviton without having to involve more of the platform teams. So we want, we, we wanted to make sure developers could still choose an architecture between X86 or ARM. And they and and they can still deploy to one target or another. So here is a picture of our CIC a little bit where uh Dylan go we use GitHub for hosting our code and ideally if a if a team has chosen Armor Graviton they can essentially pick one of two paths. So if a developer does a pool request, we'll then build that image on Jenkins and then we'll run their tests whether unit tests or integration tests for that for that specific architecture, make sure it's working. And then once we're and then of course once it's approved we'll then push it out, we'll build, we'll build that specific image and then you also using UCS task placement constraints we'll make sure that the graviton image goes to our graviton uh instances XA6XA6 since if they, if they crossed wires that would just be a pretty sad error. So even though it's not the most seamless, but we still want to empower developers to choose their journey and make it easy for them to be able to to be able to build without having to think too much about it. Now after establishing this ecosystem and moving a few and moving a few work workloads over, our FinOps team wanted to move that cost savings needle even further and accelerate that graviton adoption. So here is a bar graph of our EC2 spend, uh, and, and, and, and, you know, anonymized, of course, and here's a couple of services, uh, that were, that were larger than others. So naturally we're like, we're looking at these graphs for a while we saw that these same five services were driving up 25% of our EC2 bill. Naturally we're thinking, oh this would be, these would be great targets to move for Graviton for cost optimization. But even more to it than that, these are also a good mix of different workloads like Python 3, Java Catlin, and we also thought this would be another great example for other engineers. So if we could show that we're moving more modern services on the Graviton, it would hopefully inspire them to move their services, so it'll hopefully have a compounding effect. And then thanks to all the improvements to our tooling we were making while we were slowing for the spot market to catch up, these migrations were pretty easy and pretty seamless. We just worked with service teams to just say, hey, we're gonna move this over to Graviton to not interrupt their production pushes, and then we were able to move these over staging service first, and then finally production. And that some of the results that we got were pretty great from that. So first we have, we, we got the 20% savings, uh, for those specific things, but as an aggregate we now have over 30% of our of our microservices are running on graviton. And then we also have this really cool declining VCP per hour cost graph. So you can probably guess where we started, uh, really accelerating that that journey from, from June to July. And it's not that we were using less EC2 instances, but I think we're being more cost efficient about our workloads. So we're able to scale with our business and not and be and just be more cost optimized about it going forward. So here are some of the results to date just to sum it up. So we have that 30% on Graviton, but we also have a new goal in 2026 where we want to increase that number to 50%. And we also wound up getting our 10 10% EC2 savings coupling that with EBS and then also the 10% faster deployments from just how efficient the architecture is. And then on the managed services side we have that 75% number and it's growing, but we also have a nice 10% RDS savings as well as the 25% Alaska cash savings. Now what's next for us, uh, at Duolingo in terms of compute like Brent alluded to, we're, we're doing an ECS to EKS migration and that's gonna lock some cool things for us like using Carpenter to interact with the spot market, ephemeral staging servers, so you're not gonna have multiple teams fighting over like one server, and then maybe down the line a cool service mesh. And then the next one would be multi-arch. So we do love Graviton, but sometimes the spot market might also love it too much too, so we might have to fall back to Intel. So this way we can we can position ourselves to build one image, simplify our CICD, and that way most developers can just say, hey, I'm building my thing, and we just deploy it, whether it's X86 or Graviton. And then finally more graviton. You can, you can probably tell that I do love graviton and we wanna get, we wanna get onto more graviton for more cost savings and even that power efficiency as well. Now I wanna hand it back to Seth to close this out. Awesome. Thanks, Jean Paul. Thanks, Brent. Um, For those of you that want to figure out where to get started, some resources that are available. So we have a graphvion technical guide available on GitHub. Big repository. We keep it up to date. You can get a lot of technical information and guidance on how to migrate different applications, whatever programming languages, Java, Ruby, Python, there's C, C++, whatever you need. Resources are out there so you can get started in your organizations today. If you wanna learn more about enterprise support, um, you can take a look. Those QR codes will take you out to those links and learn how to work with an AWS technical account manager. I think they're one of the great resources inside our organization. Maybe uh you're looking at how to measure performance. I think that's a, a key differentiator, right? Looking at how to see the difference between the performance on these instances. APR is a tool that we've open sourced to help customers gather the right data so they can make the right decisions about their performance for their applications. And maybe you need help from partners. AWS ProServe has a graviton migration accelerator program. We also have, uh, systems integrators that can help as well. Um, as well as the software solutions, some examples up there, if you come across a software solution that doesn't work on Graviton today, please reach out. We have a team that works with ISVs to make that happen. And then again, certified third parties, uh, of solutions of, of, of system integrators that, that you can work with. You can see some of them on the screen that's growing every day. Thank you very much for, thank you everyone for joining us today. We appreciate you taking your time. Enjoy the rest of the week.