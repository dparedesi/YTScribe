---
video_id: 2yK1IUC5bi0
video_url: https://www.youtube.com/watch?v=2yK1IUC5bi0
is_generated: False
is_translatable: True
---

Welcome everyone to session AMZ 306. Elevate streaming quality with AI Prime Video's innovative approach. So before we start, how many of you watch Prime Video? It's awesome. So you might have seen the latest shows like Fallout and Lasso or like watched NBA or TNF on there. So this session will be really interesting for you to learn how Prime Video uses AI, particularly Gen AI, to improve their artwork quality, to be able to improve the streaming quality so that you can watch uninterrupted movies, uninterrupted sports on your favorite channel, Prime Video. So to start off with, I'm Tulip Gupta. I'm a principal solutions architect with AWS, and I support the strategic accounts under AWS, um, primarily the media and entertainments under Amazon, and Prime Video, uh, is one of my customers, and I have with me Brian and Mona from Prime Video. Brian is a principal SD at Prime Video, and Mona, a senior manager of data engineering, and they are going to introduce themselves later on when they present their use cases. So we are going to start off with an introduction, go through all of the use cases that Brian is gonna talk about artwork quality moderation that he uses Gen AI with, and then Mona is gonna elaborate on the streaming quality, how they improve that using Gen AI agents, and we're gonna talk about the challenges that they faced in, in their individual journeys as well as the solution and journey, uh, on AWS and also like talk about the demonstration and benefits at the end. So when we talk about Prime Video, Prime Video is global. Prime Video has millions of titles in its catalog that you can stream on hundreds of devices. They support over 30 different languages, operate in more than 240+ countries and territories, and support over 200 million Prime members worldwide. So nearly anywhere around the world you can log in onto Prime Video and enjoy your favorite content. So let's say you're traveling to Europe or India or anywhere, like you'll be able to log in and watch your favorite show out there. Here's the prob, uh, like, basically like there's 315 million data points that Prime Video has a global ad supported reach for over like 300 million monthly viewers. And this is how Prime Video has grown linearly over time. Uh, so they started off like in 2016 with the Late Show and then as you can see as they have moved from like 2018 to 2020 to 2022, they have, uh, increased the number of streaming that they have done, number of, uh, shows that they have produced, so it's, it's a lot that's going on and so as they're scaling, they also want to be able to scale how they are, how they are able to like use it for the streaming quality and how they're able to like reach it out to like millions of customers out there. So it's a huge accomplishment and as they continue to grow bigger and bigger, scaling efficiently does impact their ability to deliver these events over swim team. And the lining up their biggest slate of theatrical movies ever and with Prime Video customers they can customize their viewing experience and find the favorite movies, series, and live events, including Amazon, MGM Studios produced series and the movies fallout. And so in 2025, they also added, you know, like the Peacom Fox One and Peacock Premium Plus as well. So it's been kind of a journey as they've grown over time. I saw a little trivia out here. So in 2025, what was the peak audience for the Commanders versus Packers Thursday Night Football game on Prime Video? Can anyone answer the question out here? Any guesses? 18. We see 18. So you can see like how massive it was like with like 18 million people logging in at the same time and watching the TNF game and so to operate at that scale they also want to be able to scale out the infrastructure primarily on AWS to be able to meet that demands. And so to kind of like talk about what the use cases were, you know, so they wanted to be able to improve and moderate the streaming quality that Mona's gonna touch about and then like also to be able to moderate artwork quality that Brian's gonna talk about at scale. Like when you talk about the scale that we looked at like in its goes of linearly to operate at this scale, they depended on, uh, solutions like G AI tools for evaluation and performance, and that's how they were able to scale it out. So like for example for um like for Brian's use case they get content from like you know Peacock from like Stars, you know PBS for NFL and and uh and MLB and NBA and they are able to moderate it as uh by using Gen AI tools and so, uh, if folks are aware that some of the Gen AI tools out there, I'm also gonna cover some of the Gen AI tools in the coming slides. So when we talk about G AI tools, you might have heard about AI agents particularly. So what are AI agents, and it's a tectonic shift of how we build, deploy, and interact. And so for an AWS we have what we call our agents, which is like, uh, and it's kind of a G AI agents that have come out there. These are like autonomous or semi-autonomous software systems that can reason, plan, and act to accomplish goals in digital or physical environments and so. So these, they, you know, we had the foundation models which can do one task and so, but we, when we talk about agents, they're able to do a task independently. They're able to call the foundation model. They're able to access database. They're able to go and call tools, so and they're able to perform that task on their own. And so these can leverage AI to reason and plan and accomplish them, the, uh, the, uh, the plan, the task, uh, on their own. And so that brings us to what we think about the evolution of agentic AI. And so when we moved from left to right, there was more human oversight when we first started with they had like generative AI assistance, and you might have heard about it 2 years ago when it came out, they follow a set of rules. They automate repetitive tasks. And as we move towards the right, then we have the GAI agents we touched about in the previous slide, and they are able to do a singular task, uh, very well. And then we moved on to the right is agent AKI systems, which are like fully autonomous multi-agents, and they can orchestrate between them to accomplish a set of tasks and so they can mimic human logic and reasoning. And as we move from left to right, there's less and less human oversight and there's increasing autonomy. And so we then we talk about AWS agent care portfolio and for some of you it might be familiar with something called Sage Maker. So Sagemaker essentially it gives you the compute power, it gives you the model where you can customize your own model. You can build, you train, and you host your own models, and we also provide, uh, computer stuff like Tranium and inferentia. Then we have um the middle layer, which is AI and agent development software and services, and in this we have Amazon Bedrock. How many of you are familiar with Amazon Bedrock out here? Cool. So for folks who are not, this is the managed service. Basically it gives you a flexible, comprehensive service for an AI application and agent development. It offers access to like foundation models. You can customize uh models and you can application with your data and apply safety guard rules. So let's say you're building an assistant that provides you movie recommendation and you say like, hey, you don't. Ask me questions about politics they shouldn't be able to do that. That's what we mean by guardrails and also like, uh, it provides you Agent Core, which is one of the new services we added, uh, with essentially like helps you scale deploy your services operate at scale on AWS. So if you build an application and you want to scale it out to like millions and hundreds of users, Agent Core is what we'll be using. And this is just briefly touching about the services, uh, and then we have SDK for agents like some of the one, the one with Nova Act which helps you is designed to like take action within the browser, and then we have strands agents which Mona and Brand are gonna talk about it in deep when they talk about the use cases and how they have been leveraging strands and a strand agents is another open source um SDK. That AWS has developed and it basically is a lightweight source SDK that helps you develop agents very quickly and be able to uh leverage and call multiple tools to be able to accomplish that task. And on the top we have applications. So these, as you move on from Ram, like, obviously it gets, you know, there, uh, it gets more and more managed. There's, you know, and you don't have to manage your own infrastructure. Like for Bedrock, you do not have to manage your infrastructure and the applications. You get like a whole applications out there that you can like play around with. So for example, like Hiro, you can, it acts as a coding assistant if you wanna develop an app like a, I want streamlet app to act like a travel agent. It'll be able to quickly follow, design it, and. Build it out for you. Then we have Amazon Que Developer for accelerating software development, Que Business for your data and to answer questions, transform to accelerate enterprise modernization of .NET and mainframe and VMware clouds, and then Amazon Connect, which is to speed customer service to delight customers. And then lastly, marketplace where we can go and access a lot of agents out there and just a quick brief overview for you to see all the portfolio out there. And that brings us strands agents. So we briefly covered what strand agents is, uh, in the previous slide. So again, like I said, open source SDK. So in the past, like LLMs, um, were a, you know, we basically would provide agents a template of of how to do stuff. And right now LLMs are getting smarter and smarter, and strand agents provides a lightweight model where it's more intuitive development and it can figure it out, uh, to call the LLM and the tools on its own. And it can get started in minutes instead of hours, and it provides you robust capabilities like native tools, MCP servers, and you can also like extend the support for custom mobile providers, custom tools and MCP so it helps you like with rapid development and prototyping. And that's what the key uh thing that Mona's and Mona and Brand's team leverage to be able to experiment quickly, to be able to use new services and be able to like iterate on what they're developing to develop that robust evaluation loop. And with that I'll hand it over to Brian to talk about his use case and the artwork quality. Right. Thanks Mona or thanks Tulip. Um, so my name is Brian Breck, and I'm a principal engineer with the partner experience team within Prime Video. Uh, we work with our content creators like major studios and independent filmmakers to ingest their content and prepare it for streaming customers. That content includes not only the streamable assets but also artwork, metadata, trailers, and, uh, bonus material um as a part of that data set. Uh, so just to get us started, uh, quick question for everybody, um, how many of you raise your hands, stream with more than one device? Probably most of you. It looks like most of you, um, so I stream with my phone, my laptop, my TV, my tablet, and that creates a lot of complexity at Prime Video. We have to support, um. A number of form factors, backgrounds on top of 30+ languages in over 200 territories. Now what we're gonna be talking about today uh is artwork quality um we use artwork to represent movies, TV shows, channels, uh, carousels, and those, and that artwork can show up in the streaming site as well as in marketing material and advertising. Now, the artwork is provided by our partners, um, and the artwork may be beautiful, but it may not meet the needs of Prime Video. So for example, um we need to be able to crop artwork depending on uh what form factor we're going to be showing it um we also may need to overlay a logo or a uh or an action button. And so we need a safe zone on the perimeter of the artwork so that we can work within those parameters. So I've highlighted the safe zone in purple, in purple. In the first example, we can see on the left there's plenty of text on the side of Lioness. There's plenty of space on the side of Lioness, and on the right, only the shoulder of the last actor is being cut off. So this is a perfectly acceptable safe zone. Now in the second example we can see that um a head is being cut off for one for one of the characters and there's some text that's completely obscured, so we could not use this for Prime Video. Now this is just one type of defect that we're looking for, uh, we're looking for other things like mature content, pixelation, um, issues with localization as well as accessibility for things like uh color blindness. So we have a number of challenges in this space. Um, one of the biggest is that a lot of this work is, uh, traditionally done manually. So our partners provide us a piece of artwork. Uh, it needs to get into a manual evaluator's queue. They need to, uh, provide their feedback, and then that feedback needs to get back to the partner, and there may be multiple iterations of this, so the entire process can take multiple days. Now our solution to this generally has been to create an ML model that we train with artwork uh examples um and then we use those examples to find defects, but that's a time consuming process and we have over 30 defects that we're looking for today in our artwork and that number is only growing. Now another problem that we have is data. We have been collecting data from our manual evaluators on acceptable and not acceptable content. However, they don't always follow the same SOP standard operating procedure, and so what may pass one evaluator may not pass another, and that data can leak into our our data sets and make it more difficult to use. And then once we have an evaluation once we have a solution, the evaluation can take a while. We can take several days to run it over a data set and then figure out what changes need to be made next for the next iteration so that we can improve the system. Now when we're talking about evaluation, what we generally do is we take a few 1000 images and we have a human annotator that goes through and provides what the correct result should be, and that's what we see with the ground truth column. Then we run our automated solution and then we look for uh discrepancies in the results. So in this example, uh, in the second image, we can see that Ground Truth says that this should have passed, but the model that we were using is going to fail that piece of artwork. Um, maybe it thought that Ryan Reynolds' hair was too close to the edge, and so now we need to provide more artwork as training data to solve for this problem or create some additional instruction in our algorithm to allow this to pass. But we're only looking at 4 images here. Imagine if we're looking at thousands of images with results in S3 buckets that we've got to pour over. Um, it can just take a lot of time to evaluate the results. Now one of the things that uh we noticed is that with LLMs and their multimodal counterparts um we can detect certain defects um with those foundational models um we've seen anecdotes of it being used in other places and so we wanted to try it out for our use case. So what we ended up doing is using QCLI, which is now folded into Quiro, to generate a few algorithms for us and use a few different foundational models to try out some results. We wanted to move quickly. We wanted to make sure that this was going to work for us. Um, when we ran the results, um, we anecdotally saw that it was promising, um, so then we wanted to go and perform one of the evaluations that we just took a look at, um, we saw that our precision wasn't high enough, um, and we knew that we could improve it, um, but we also knew that it was going to take a few iterations, so we wanted to move a lot faster than we had in the past. So what we ended up doing is creating an evaluation framework that we use for defect detection. Uh, we take as input data sets with ground truth, um, as well as some initial configuration, and then as output we get the, uh, results as well as feedback on how it could be improved. Um, instead of diving into S3 buckets, uh, we can see views of the artwork, um, so for example, with Uncharted, we can see a mobile view versus a web view and how that's going to look, uh, in the different use cases. And then we get some benchmarking statistics based off of how the the ground how it compared to Ground Truth as well as how it compared to previous runs and then we also have the ability to dive in and take a look at some of the issues at the individual artwork level. So here we're taking a look at the back end architecture for our evaluation system. It is a system that is orchestrated by strands with individual agents that perform the defect detection. They perform the uh uh evaluation of the results, and then also agents for uh suggesting improvements to the process. So the way that it works Is that a user will submit an initial request through our cloud front and load balancer instances and hit our API. Um, that API will store the data in our config table and that will include an initial prompt used for defect detection. It will include a link to a data set along with the ground truth data. And then it will also include some initial configuration such as which model to use and something some things like temperature to determine how to use that model. Uh, next, the orchestrator will pick up that configuration and delegate each piece of artwork to our eval subject. Uh, agent that will actually perform the defect detection and once we've gotten through all of those, uh, pieces of artwork, uh, we write the results to our S3 results bucket. Once the results are written, we use our results calculator to generate the statistics that we saw in the previous slide. And we also use that data to determine some next steps. Finally, we will. Send that data on both the uh results from the defect detection as well as the results that were calculated um and send that to our prompt improver. the prompt improver agent will take a look at all of that data and make. Determination on what should be done next, that could be um making changes to the prompt, it could be suggesting different models to use or different configurations for using those models. Now, once we've gotten through that process, we can that that data is written back to our config table and then can be used for the next run. I'll take a step back real quick, um, so. Strands is doing a few things for us. Um, first, it's simplifying our interaction with LLMs. Um, second, it is allowing us to easily create relationships with between agents, and it also provides some out of the box tools that we've been able to take advantage of. So when we're talking about tools, um, there's a few built in tools that we use on a regular basis, um, one would be the image reader which allows us to, uh, prepare the artwork for the LLM, um, when it's initially being called or the LMM. I, we use file read and write so that we can do, uh, intermediate manipulation of the images as a a part of certain processes. And then also agents as tools to create those relationships between the agents and be able to call them explicitly. On top of that we've also created some custom tools, so like the safe zone example that we talked about, we used, we have a crop image custom tool we have also a transparency check tool for readability and accessibility, and that the set of tools that we have available to us now has has grown significantly. Now, unlike the safe zone defect detection process, not everything can be a pass fail, or can we use that information as pass fail in order to improve the system. Sometimes we need qualitative results, as well as the quantitative results in order to decide what our next steps are. So what we do is we create a judge that takes a look at the evaluation performance and provides some additional context for why things failed, um, what could be done better on that individual artwork basis. And so what we do is we provide some initial configuration um to our Dynamo DB table for the judge. The judge configurator agent reads that information and prepares it for the judge itself. And then the the judge will then take a look at the results of the evaluation and provide additional context that can then be used as a part of the prompt improver step. Now we could use a judge for all of our defects, uh, but it's expensive and so we only add that configuration and step when it's absolutely necessary, um, but it has been critical to get some of our defect detection mechanisms in place. Now overall, um, strands has greatly simplified a lot of what we're trying to do. Um, it has made it so that we don't have to move images around. um, all of our interfaces are text-based, we can access the images uh centrally. And we are also able to use the system to run regression tests. So if we want to change the model or if we want to change the configuration or a prompt, we can validate that we haven't made things worse. This has been so successful that we've also started using it for things like text. So like I mentioned, we receive metadata and so we have to validate the synopsis and so we have a bunch of defects that we look for in a synopsis that is running through the same process. So like I was saying before, um, we generally use about 2000 images and we have humans run, uh, run through them and provide the ground truth result. When we initially started this process, we were running into some local maximums with precision. We just weren't hitting the values that that we thought that we should be able to reach, so we're running into situations where when we would fix one false positive or false negative, we would cause another one. And what we realized is that as we started digging into the data that the manual evaluators were using inconsistent criteria, um, some of them would pass something that others would fail and vice versa. And so as a part of this process we ended up creating a standard operating procedure for for the manual evaluators that would also be shared with the automated system and so we could have consistent results and those consistent results led to a better ground truth data set. And that better ground truth data set allowed us to run that loop that that I was showing where we could run a run an evaluation data set we could look for ways to improve it and then run another evaluation set, um, basically creating a auto tuning mechanism where it was completely hands off the wheel and we could just watch the the system improve itself. And so that actually simplified our runtime solution. So we take the configuration that we were, uh, generating in our evaluation phase and we load that into a an app config instance. Then we allow our partners to upload their artwork through our portal, uh, goes through API gateway and then is delegated to. Modules each res representing a particular defect, uh, so we can run defect detection in parallel, um, and for each defect detection mechanism we are reading the configuration uh from the uh app config. In combination with providing the artwork to Bedrock and we're able to generate those results um almost real time so where it would take several days for the partners to get the results back we're actually providing that within a minute. So When it comes so. The solution isn't perfect, um, and so when the partner gets a result that they don't agree with, they are, uh, allowed to override that result um so what that means is, um, we, we recommend that they make some particular update. And um they think the artwork is fine, so it goes into a manual evaluator's queue and we'll use the old process. But the great thing about this solution is that where we were reviewing 100% of the artwork that was provided to us, now we're only reviewing say 10-12% of the artwork and that has been a huge time saver for our manual evaluators. So, uh, through this process, um, we starting with the QCLI, um, solution, we were only at about 37% precision. We were able to get that number to 78% for the, uh, safe zone. Um, we were able to reduce false positives and negatives by 70% and we were also able to, uh, reduce the amount of time it takes to get results from. Uh, several days to less than an hour for certain circumstances, um. But most importantly, we were able to reduce that manual effort by 88%. So we learned a few things along the way. Um, first was don't try to do too much at once. Um, the context windows have been growing recently and we initially tried to take advantage of that. Uh, we tried to run defect detection for multiple types of issues at once and found. That that was just not the way to go. So what we ended up doing was we broke the problem down into the individual defect types, ranked them by how often they occurred and how much effort it was to manually perform the detection. And tackled them one at a time. The next thing that we learned is that we can really use generative AI throughout the life cycle. So we started off with our initial proof of concept being generated by QCLI. Then we used the we use generative AI to create our system design and then for development and then for evaluation and a lot of our monitoring. Um, so we estimate that we used generative AI for about 85% of what I showed for both the evaluation framework and the production system, and so it was a huge time saver, saved months of engineering work. We also found that LLMs are effective at improving their own prompts, so we used cloud to take a look at prompts that we were providing to cloud, and it was effective at telling us where we could improve things that were specific to that particular model. Um, we also, um, found it very helpful to establish that robust evaluation loop, so being able to just iterate quickly, make even when the changes that we were making were manual, we could just kick the process off again, see how it worked, and, um, not spend so much time in the investigation phase was, was critical for our success. And last, um, we learned that, uh, manual evaluation is hard, um, and error prone and so it was worth it to, uh, take the time to generate a high quality data set so that we can, um, make sure that our automated processes are successful. Um, so, uh, we started off with the, uh, safe zone. Um, we've since moved to logo and text placement, um, offensive and mature content, uh, we're taking a look at text legibility, localization, and accessibility. Um, all of these things are either in production, it, uh, being able to be used by our partners, or they are able to, um, or they will be there by the end of the year. Um, so, uh, that's, uh, the presentation, um, uh, it was great to be able to share that with you, and I'm gonna hand it back to Tulip, um, who is going to continue. Thank you Brian. And so you heard from Brian like how they were able to use Agent TKI. They were, how they were able to use Strad agents in their framework to be able to moderate their artwork quality and be able to place it at the right place. And so that, you know, I want to talk about Agent Tech AI a little bit more before I hand it over to Mona to talk about her story about Strand agents and Gen AI. So we, when we think about the two flavors, one, you know, is basically accelerating software development where we talked about the applications like Hero and Q DeveloperI helping you to be the code assistant and helping you code and develop apps. And the second person is like reimagining business workflows with custom agents, and this is where Mona's use case becomes really important. So they were able to use a mud, uh, you know, a few agents out there that were able to orchestrate between them. And created that custom workflow to be able to accomplish that goal that they need, which was improving the streaming quality. And so this this is the one that we're gonna focus on in the next use case that Mona is gonna talk about. And so briefly about when we talk about enterprise agentic AI applications when we have an LLM in the center, that's the brain that basically reasons that basically understand and gives you the output. But it needs help. Like when we talk about LMs, it probably doesn't have the latest information. So if you go and ask any of the LLMs out there like what's the weather in New York, it wouldn't be able to answer it. Because it doesn't know what date it is, it doesn't know what the weather is, so we want to give it some data points. And so the tools out there help it call maybe like a weather API or the current time, that some of the database can maybe it shows you like where New York is, things like that, the information it needs to be able to give you, uh, to be able to correctly reason and give you that information. It also probably needs memory to be able to understand the current conform information, current context and conversation. And take that action and give you the correct prompt, give you the correct answer like, oh, it's 56 °F in New York. I'm not sure if it's 56 °F right now, but I'm just like saying it. But, and also like the, the another important thing is observation and guardrails where we want to ensure that if you're asking it for weather information it doesn't answer me like about politics answering me about like who the current president of the United States is so we want to be able to restrict it to be able to answer only for the prompt or the context that we're asking it for. And so with Amazon Bedrock you're able to do all of that. It provides you the ability to access models. It provides you able to call tools with MCP, which is Model contest protocol, that's been introduced by anthropic and A2A as well, agent to agent, like how your multiple agents can orchestrate, provides you frameworks like strand agents that we talked about QI and Landgraf to be. Able to call build that agentDKR framework agent code to deploy your agents at a scale on AWS with your runtime gateway memory and observability, and obviously it provides you the ability to customize and fine tune your models. And with that I'll hand it over to Mona to talk about her HNTKI use case, how they improve streaming quality. All right, thanks Tulip. So quick show of hands, how many of you have been on call trying to use a whole bunch of data to detect, localize, mitigate issues? Alright, I see a few hands here. Well, I'm very excited to talk about our journey of building an energetic workflow to detect, localize, root cause and mitigate streaming quality issues. I'm Mona and I'm a senior manager at Amazon Prime Video. Prime Video is a global platform. We stream high scale live events, video on demand, and linear content for our customers. Ensuring our customers are able to watch their favorite content, whether it is the Patriots scoring a touchdown, Go Pats, or Barcelona scoring a goal, we want to be able to obsess over our customers being able to take in that moment. Sometimes what can happen is, while streaming to millions of customers, even an interruption for a brief moment can mean that thousands or millions of customers are interrupted from their viewing experience. Traditional operational approaches such as manual monitoring of metrics or alternatively reactive root causing just do not cut it at our scale. We asked ourselves the question, what must we fundamentally build a system that's not just monitoring these metrics, but actively understanding these metrics, learning from these metrics, and able to autonomously take action towards these metrics, and that's exactly what we sought out to build. Now a few challenges that we had kind of kept in mind and used as the guiding principle while working through these systems. First off, we wanted our system to be able to have access to a multi multimodal sort of set of data. So this can include things like time series, uh, infrastructure logs, player logs, uh, graphs, so on and so forth. We also wanted the system to not just have access to this data, but to actually understand this data and almost build an intuition behind it. And finally, we wanted to be able to have the system accessible to pretty much anyone in the engineering teams, so this did not require special expertise or domain expertise when it came to understanding our data. With that in mind, we went ahead and built an AI agentic system that sort of put together multiple agents that were orchestrated using strand to be able to accomplish these tasks. One of the qualities of this system is that it can reason through complex tasks, break them down into multiple simple tasks, and then chain them together in a sequence to accomplish the set task. We also made sure that the system was not just a one and done, but was constantly learning from all of the data around it, keeping the most current operational snapshot of the data, and also be able to sort of actively learn from any past mistakes or feedback that it received. So with that, let's take an overall 30,000 ft view of what the architecture looks like. This system was built as an AWS native as well as an AI native system. It's sort of orchestrated with strands, uses AWS lambda for things like authentication, as well as things like orchestrating across the different agents, as well as uses Athena both for querying as a data store, as well as Dynamo DB for some of the global state management. This system is the foundational backend system that can be used for a multitude of different frontend interfaces, so it can be used, for example, as a chatbot interface where somebody can put in a natural language question and be able to get an answer. It can also be used to sort of autonomously be triggered by a different system that maybe has detected an issue, so really a bunch of different sort of use cases that can be facilitated with the same backend system. So diving into the components of this system. Uh, as I said, this is a multi-agent system. We have a bunch of different agents as well as sub-agents kind of working together to be able to accomplish the task. First off, we start by looking at the request handler. The request handler is sort of the front gate of the system. What the request handler does is once it receives a request, it will first go ahead and authenticate the request, then it will validate the request. And then it will go ahead and start to decompose this request into simpler tasks. So for example, if the question asked was what was the rebuffering rate on iPhone devices in Germany over the past week, the request handler will sort of break this down to understand, OK, this ask involves a metric ask, it involves a trend analysis ask, and then it also involves an ask for a specific cohort, cohort being devices, geographic information, and time periods. The request handler also has a guard rail agent, as you heard from Tulip previously, that's sort of able to validate and make sure that the request is compliant with what the system is supposed to support. Now that we have kind of talked about the request handler, let's look at what happens after that. Suppose the request handler is the routing agent. You can think of the routing agent as an intelligent orchestrator or sort of a traffic controller that based on the request that it got from the handler, sort of tries to understand what are the different capabilities that need to be invoked to be able to service this request. So the routing agent kind of understands what those capabilities are and sort of passes that along as a signal to invoke other subagents, agents, tools, data sources, and so on. So it can be really thought as the brain of the operation once it gets that decomposed request. The routing agent also uses the chain of thought process in terms of breaking down a complex task, kind of reasoning through it like a human would, and then finally understanding what are the capabilities that it requires. From the routing agent, we then have the integrator subagent. The integrator subagent can be thought of as some sort of a traffic controller. It's sort of, you know, once it has got the request from the routing agent, it knows what specific tools and data sources it needs to connect to. This integrator subagent sort of works through MCP, which is Model context protocol, and is able to talk through a host of different tools as well as data sources and is able to work through things like different access patterns, APIs, access formats, so on and so forth. It's also able to kind of combine a bunch of these data together by knowing the right joint conditions and so on. The other thing about the integrator subagent is that it also sort of serves as a data quality check and makes sure that it's only the right kind of data and the right quality of data that is accessed by the system. Post the integrator subagent, we then have what is the analysis subagent. The analysis subagent can really be thought of as a data scientist in a box. The analysis subagent is primarily based out on Bedrock, and it has a whole bunch of both large language models as well as small language models that it can access in order to be able to service a specific request. You can really think of this as sitting in an ensemble of different models and sort of leveraging the right models per the use case and per the capability that's needed. Now once we have talked about the analysis subagent, the next thing that we have is the reasoning agent. So the reasoning agent sort of takes all of this input that it has received from sort of the prior agents and sub-agents, and what it does is it kind of uses business context that it has access to to be able to determine if the sort of analysis that it has been provided is sort of, you know, pertinent and is sort of relevant with all of the business context that it has. So this can be thought of really as, you know, sort of it it uses an iterative approach and uses LLMs as a judge to be able to use an independent LLM to kind of validate the responses that it has received from the previous analysis agent, for example, and using that business context is able to tell, OK, is this really the sort of, you know, expected answer that I would have, or would it be something else? The reasoning agent also has the ability to sort of have an iterative loop to kind of go back and request a different capability or invoke a different data source based on what it might have gotten from the LLM as a judge. From the reasoning agent we then have the response handler. So the response handler kind of takes all of the different input that it has received from the routing agent, the reasoning agent, as well as if at all you had to run multiple iterations of the loop that we have just talked about. And really packages all of this information together into the expected output format. So this could be things like the response to a natural language question. It could be generation of a complex sequel, or alternatively it could also be an autonomous action or a mitigation lever that can be pulled in the response to a certain trigger. The response handler also interacts with the guard rail agent again to sort of make sure that the response is compliant with sort of, you know, the required uh sort of data sharing and uh you know, other such sort of activities. Separately from that, the response handler can also sort of goes ahead and logs all of these decisions that it has made similar to what all of the other agents have, so it kind of takes all of these logs which can then be used for sort of reflective analysis in terms of improving some of the decision making. So taking a step back and kind of looking at the system, um, you know, overall, what we talked about, you know, first off was the request handler that takes in a request, sort of validates it, makes sure it's the right sort of format, uh, decomposes it into the sort of simpler tasks, moves that along to the routing agent that then sort of, you know, knows which capabilities it's going to need to invoke. From there it goes to the integrator agent which can orchestrate a bunch of different tools and data sources, followed by the analysis agent, which is really the data scientist in a box ensemble of LLMs that can be used. Finally, the reasoning agent that kind of uses business context along with what's given from the analysis agent to make sure that it is a sort of an acceptable and reliable answer. Along with that, it can also trigger reiterations of the loop invoking other capabilities and invoking other data sources as needed, and finally, the response handler that packages all of this and makes it available either as some sort of an output or a mitigation lever or an autonomous action. As you can see, we also have both an online evaluation system, so similar to the LLM as a judge that I mentioned, as well as an offline evaluation system that kind of takes all of these logs of decision making and can be used sort of iteratively in terms of understanding and improving the system. All right, so now that we have kind of talked through uh overall components of the system and how it works in real time to be able to sort of, you know, help detect, localize, as well as root cause issues, taking a step into some of the lessons that we learned through the process of developing the system. First off, you know, data quality always beats data quantity. When you're trying to develop such a system, it can be sort of, you know, there can be a whole lot of things that you might want to add, things like infrastructure logs, metrics, past incidents, you know, tickets, so on and so. Forth, but you really want to make the most efficient use of your context window and make sure that you're giving the right set of data and data that will actually get you to the right outcomes. So being judicious in terms of the data that you sort of have as part of the system is especially important. The next thing is building feedback loops early and often. This really helps in the efficiency of development as well as the efficiency of getting your system to the levels of accuracy and levels of reliability that you're looking for. The other one is planning for failure modes. You know, Systems do fail. We want to have an autonomous system, but there are going to be times when you might want to have sort of human evaluation or when you might want to have times that the system is just sort of exposed to a brand new situation. So you do want to have safe ways that the system can fail and trigger the right sort of human involvement as needed. And finally, continuing to use AI to amplify human expertise, whether that is understanding the business context better, data better, so on and so forth, you always want to have AI amplifying human expertise throughout the SDLC software development life cycle process. In terms of what's next for our system. We want to continue to build more mitigation levels, sort of have more and more autonomous actions that the system is able to perform by itself, continuing to use AI through the software development cycle in terms of accelerating our development, in terms of having quick prototyping, using things like SageMaker, as well as strands for quick orchestration and quick proof of concepts, as well as using AI in the deployment, as well as the overall maintenance of the system. And finally having more and more safety mechanisms so that you continue to not only build but also keep trust in your system and sort of have it leverage more and more autonomous actions. So with that, I will hand it over to Tulip. Thank you, Mona. So let's see how many of you were paying attention. What was the peak audience during the Commanders and the Packers game in 2025? Does anyone remember? That's right, yes, so you heard from Mona and Brian about how they were able to use Gen AI in their solutions in their use cases respectively and how they were able to, if you remember that HND AI evolution from like going from. Uh, more human oversight, less human oversight, and be able to automate a lot of those tasks. And so both of them wanted to be able to for the use cases where their main thing was to deliver premium video and artwork to scale to millions of customers across thousands of different devices. And obviously when you're trying to do that, there's millions of customers globally and you're streaming content 24/7. And you want to be able to, uh, and it can impact thousands or even millions of viewers instantly if you're not doing the right job. So they wanted to ensure like nothing, none of their customers or Prime Videos are impacted while they're trying to experiment, while they're trying to like see how it is being delivered and so that that's where automation using AI helped them like it was more they were able to do it more precisely with increased productivity because there was less human oversight. They were able to make those agents do the work and be able to experiment it faster and faster and be able to get the output that they need. And so their first key takeaway is more automation. They reduce their evaluation time, and you heard from Brian. It took them days before, and they were able to like reduce it to 15 minutes and and maintain performance without human intervention. And so like going from again that slide from like going from right to left with more human oversight to the less human oversight. They were able to scale smart, so what they did is break out the problem into small chunks and then scale out. So if you have a bigger use case like they want to achieve, you break it out into smaller chunks, iterate on it, work on it, use agents, AI agents, and then build a multitude of agents to do the orchestrated across and then be able to accomplish the goal that you need. And have a robust evaluation loop and so whatever you build and whatever you want to do, even for Mona's and Brian's use cases, they were able to enable rapid duration and continuous improvement through all of their process because they had a robust evolution loop. What they were building, they were able to validate the agent's outcome, see if it's the right one, and then basically improve their agents as as they went on with their experimentation. So I'll leave with this quote from Andy Jesse who recently said like what makes this agentT future so compelling for Amazon is that these agents are going to change the scope and speed at which we can innovate for customers, and that's true. We are working on it. That's the H&T future that we look at at Amazon and that's the scope that we look at when we talk about it. So if you're here around right on the corner, there's the one Amazon lane as well where you can see some of the other use cases that we have out there from Prime Video. So we have the extra recap, we have uh rapid recap and a NASCAR bon bar which kind of like shows you some of the sport innovations that we have done as well as like extra repair if you watch Prime Video, uh, when you watch a movie. Movie or you watch any TV episode, it allows you to basically kind of like show you uh what the scene is about, and you can go and recap and summarizes it. So that's out there in the demo at them on Amazon Lane on top of that other demos as well like the Zeus robotaxi. So if you're out here, I would highly recommend just passing by it and checking out from some of the cool demos from Amazon, including some of the ones from Prime Video. And with that, um, I'll end the session, and if you have, you know, if you have enjoyed the session, uh, you know, please complete the session survey in the mobile app. Uh, we look forward to your feedback and we obviously work off your feedback to improve our sessions as we go. So thank you so much for attending the session.