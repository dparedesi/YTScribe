---
video_id: wxnQPj9b-gM
video_url: https://www.youtube.com/watch?v=wxnQPj9b-gM
is_generated: False
is_translatable: True
---

So welcome everybody to, I think it's DVT 36, where we're gonna be talking about customizing llama models for code. If that's a session you expect to be in, that's the session you're at. If it's not, then hopefully you're able to find the other one you wanna be at. Otherwise, stick around. Uh, my name's Issa Jamil. I'm a, uh, AI partner engineer at Meta, and, uh, hopefully this is informative and helpful for everybody. Thanks also for just showing up and being ready to learn, hopefully. So for today's agenda, uh, we'll be going over basically ways to empower developers to build, um, basically build with Lama, uh, using basically tools that we have that'll help you enhance coding capabilities for productivity use cases and all of those types of things. Um, hopefully some takeaways you'll get from the session include learning basically the challenges you're gonna face when adapting LLMs for coding tasks, understanding data prep, model training, and, uh, evalves. Um, and then also discovering lamb deployment strategies on AWS whether it be Bedrock, Sagedemaker, EKS, um, even EC2, whatever you end up doing for your use cases, and hopefully we'll explore a little bit into real world, uh, use cases as we progress here. So before we kick into things, we do have a pre-presentation anonymous survey. If you guys have a minute to scan the code, I'll keep the slide up. It takes 5 seconds to complete. We'd super appreciate it. So I'll go and just be quiet for a minute. All right. So let's go ahead and jump in. We're gonna start with a little bit of an introduction to llama. Most of you probably have already messed around with llama play, deployed it. Um, nevertheless, we'll do a little bit of an introduction, tell you a little bit of a background story on the progression of llama, and then we'll dive in a little bit more into some of the, um, technicalities and challenges you'll face when fine tuning for coding tasks. And then at the end, we'll go through, um, a couple of example, basically notebooks, which are from our llama recipes repo where we publish like end end use cases, examples, stuff like that. Uh, but yeah, we'll go ahead and just jump on in here. So. Looking at this slide, uh, Lama's really the idea behind it is it's open source it's a great foundation to really unlock any sort of like use cases you're trying to build for, uh, we see a lot of organizations building like we have the examples here, uh, for language translation use cases, personal assistants, chatbots, agents, uh, content creation. It's a wide gamut of topics and use cases and that's something which we are super excited to see the last few years with Llama 2 release, Llama 3, and even Llama 4 as it's released where we love hearing about what people are doing and that's, uh, that's kind of a high level there. So talking about a little bit of the history of Llama. It's crazy to think Llama 1 came out just a couple of years ago. Early February, Lama 1 came out, research purpose mainly. We then came out, um, later that year with LA 2 with the ability to use it for commercial use cases and, um, later on that year we had code llama. We released Purple Llama kind of built around safety and content moderation, input output filtering, and you could see the family of models just kept on kept on evolving. We had Llama 3 release that came with a number of awesome models, um, in particular, everybody loved Llama 3.18B. Uh, and then we had 3.2, which introduced, um, some multi-modal models, so you have the ability to do, um, input with, uh, text or image, and then you also had our small models. If you were using anything that really needed fast inference or you wanted really small, uh, basically ability to deploy a small model, we released a 1B and a 3B model at that time. Uh, we also released LamaStack, uh, basically a, a framework for building with Llama, and then we released LA 3.3. That's where we released an updated version of 70B, which was a great model all around. And then in 2025, we released LA 4, which came with Scout and Maverick. So these stats aren't completely up to date, but just from earlier this year, I think this aligns with when we had our, uh, LamaCon event, we had over a billion hugging face downloads, which is amazing for us to see how many people are interacting and actually trying to build with our with our, uh, models, and that's what our intention is. And over 200,000 derivative models existed at that time that we could see, and that was mainly from checking hugging face and just the access we could see ourselves. We don't know what everybody else is building. We don't know what's being deployed in private or being developed within VPCs, so this is just data we had at the time. So it's amazing to see. And just in general I mean open source is huge and it's a great area we we're very uh much always gung ho about um it allows for deployment flexibility if you wanna go on prem, you wanna go within your own VPC, you wanna go however you want it's an option. You can fine tune the models, take them, do whatever you want, customize all of that good stuff. And then there's the model distillation capabilities where whether you're using something like Lama 3.370B or you go over to something like 405B, all of those give you great options to go and look at options for distillation, so. Just diving a little bit more into exploring that family of models. You can see here I talked a little bit about 3.18B and I just mentioned 405B. You could see some recommendations here for when you would use them versus other times. 8B has become a super powerful model for customization. It's fast to fine tune. It's relatively cost effective. And it runs fast. A lot of you have customers or your company in general is engaging in a solution that needs to be able to do quick inference, and you don't really need to be running a 70B model. You might just fine tune 8B and run with that. Um, we also talked about 70B, uh, performance, super good on that one. Uh, you're able to go customize that model as well, and you're seeing a lot of conversational type stuff, a lot of deep context type use cases being built there, and then we had Lama 4 earlier this year. This was when we released our first two big multimodal models. Again, image and text input, text output. And they allowed for the first time our multimodal models allowing multi multi-image uh to be really capable. And Scout and Maverick were just two different size parameter models with uh slightly different capabilities and context lengths. So if you wanted 10 million context length, you could have gone with Scout. If you wanted something like, uh, Maverick, it would have a slightly less context length on it. So let's talk about how can you actually use these models, how can you go and deploy and build for your solutions. So one of the ways that you can do it is via AWS. We're all here for reinvent. Everybody, I assume knows some of this, but just in case I'll go ahead and just kinda talk a little bit about this. So we have Amazon bedrock here at the top. And with Bedrock you've got the ability to go build solutions quickly using just a model as a service hosted API endpoint, right? You're not gonna run any infrastructure, you're not gonna handle any of the deployment. You have some options to go and customize a little bit, but really you're just getting an endpoint and you're sending in your prompts and getting out, but there's a bunch of features built in relatively. Hands off. You don't have to go build that yourself, so you don't have to orchestrate as much yourself. That's the idea there. All of the lot of llama models, um, have been on Bedrock for the most part, uh, except for 1B and 3B, and I think the main models that I'm seeing people use right now 8B, 70B, those models are definitely available on Bedrock. So that's your fast get up and go. You want a prototype, go build. Don't worry about it. A lot of times you see people when they wanna go to production, you're looking for more throughput, lower latency. Maybe that's when you progress to using something like Sage Maker, right? And, um, you can go to using the actual like optimized chip sets whether it be in French here or terranium, um. Or if you're gonna go and you wanna deploy on GPU, uh, then you could go Sagemaker, you can go EKS, um, you can even do EC2 if you know how to manage everything, but, um, those are also really powerful in general when you're thinking about SageMaker. If you're gonna go into fine tuning, if you're gonna do clusters, uh, there's options there like Hyperpod to go ahead and do a lot of that management without you having to think about it. So progressing on a little bit more now on Llama. So with Llama, something that we did this year is we began releasing tools to help you build and the reason I'm talking about this when we're in a session about fine tuning for coding use cases is because a lot of people are looking at moving from other models or from older llama models to newer models and one of the tools that we developed was basically llama prop tops. It's a way for you to go and automatically optimize your prompts for the llama models. It says you're optimizing prompts for llama models and it doesn't say you have to be on a llama model to optimize for llama. If you're coming from another model, that is an option you can go and utilize in your work stream. It's just a simple Python package, pretty lightweight to run, but you do have to download, deploy and manage that yourself at this time. So some of the benefits there are listed above, um, I mean you can imagine. Less trial and error. You're not gonna be manually telling your data science team or some uh prompt engineer folks to go and manually tweak your prompts, uh, to make it work. Instead you can go take your entire data set, uh, all your prompts, throw them through this, and have that conversion happen for you, right? Um, helps accelerate optimizations, uh, you get data driven improvement type learning from this, so a lot of benefits overall. And you can kind of see a little bit about how the actual workflow is set up, pretty simple, straightforward, but this is one of the things that we kicked off this year, uh, as a tool in addition to a few others which we'll mention a little later. So now let's dive into customizing the actual llama models and the idea here being, let's think about customization, customization for coding tasks when we're trying to go and train for that, right? So the llama models as they are, can do some coding uh capabilities within them. If you're just using the, the general models, it's not necessarily the best, it's not what you're gonna be wanting for very specific use cases and with specific languages you're gonna probably need to do a lot more fine tuning and customization work to ensure it's working for what you need. So let's talk about actually before I move forward, yeah, let's go ahead and talk about, uh, some of the reasons why adapting the LLMs in general, not just lama, is challenging. So we all probably have worked with programming languages before. They all have different syntaxes depending on which versions you're using. Things change, uh, that creates a lot of complication. Models can get it. If you use something like Code Lama, uh, then it did a pretty good job at figuring this out with the main languages it was trained on, but there's a lot of complication behind recognizing and generating code that really adheres to a specific language. Next, data quality. We all know if you're training models, uh, data sets are super important and we're in a completely different world right now than we were even a year ago with regards to data sets that are available, data sets you can purchase, data sets that you are curating yourselves from your internal actual data, uh, completely different capability of tools are available to you and that is. Still a very challenging thing for coding and the reason for that is coding data sets are generally smaller they're a little harder to come by. They are becoming a lot more available you're finding a lot of stuff available, um, open source, which is great, but then that doesn't really tailor specifically to what you need or your specific company's standards, right? And everybody has those so that's an extremely, uh, difficult and challenging task when you're thinking about coding as well. And then evaluations evals I think have been a pretty hot topic because everybody went the last couple of years from experimenting and prototyping to figuring out, OK, these are cool applications we developed with LLMs, but now we wanna go to production. How do we ensure this is actually working and how do we ensure we're able to properly, uh, validate and eval these things? Coding assessing it is complicated. It's uh just not straightforward to your code base kind of like data sets aren't straightforward always with it and there's not a lot out there so just some challenges to talk about there. So let's think about how to get started with llama models, um, if you're thinking about fine tuning here, really high level steps, but this is just kind of a little flow you would imagine as you're thinking about any sort of fine tuning you're gonna do, not only coding, right? So in any sort of fine tuning you're gonna be doing, you're going to be doing that data ingestion. You've gotta go ahead and uh get your data set up so you can actually do that, uh, fine tuning pass or whatever training you're doing. So you're gonna be looking at code and programming resources, your internal data sets, your internal code repos, that's all gonna be raw data to an extent unless you can find stuff that's curated and already preprocessed. So the next thing you're gonna go into is that step of preparing that code by preprocessing, cleaning it up, ensuring you don't have, um. Let's imagine comments and a whole bunch of really custom flares sitting in code which can exist depending on your engineers, depending on the teams, depending on the resources that you're actually downloading this from. You go into optimizing that code, uh, optimizing that for code generation task and training, right? And you're looking at how do I set this up in a way which you've got a data set now from all this code and the resources you've developed, you've preprocessed and you're ensuring it's set up in a way that can begin helping with your actual training passes. And then you're gonna go through your eval steps. You're gonna ensure that the quality, all of that stuff from a high level, um, standpoint. Then you go to your deployment. So super high level steps, this kind of a workflow that you're gonna run through. So let's talk a little bit about data prep. And some of these slides here will have a little bit of duplication between them because during these initial stages where you're setting up for fine tuning and doing your training passes, a lot of times you're gonna be doing iteration on your data and you're gonna be figuring out how to structure things in uh better ways and then how to optimize your configuration for. Training all of that. So when we think about data prep, really what you're doing is you're organizing and putting together your code data sets so you can begin processing. The first step always you're gonna be processing all your data sets or, uh, basically acquiring it and then going and setting up the pre-processing and then going into the training. So then we talk about you've now acquired the data. Now we've gotta actually process this. And do some cleaning, right? So what do you end up doing in this? Um, you're gonna go in and you need to refine and secure your code for whatever your use case is, whether it be as a coding assistant that you're gonna have running or you have something specific to like frame, uh, mainframes or, um, let's say low resource languages or low resource coding, uh, tasks, those are all gonna be opportunities there for you to possibly be setting your, your actual training material up for. And that might include, uh, kind of a handful of these tasks again this is a pretty high level um run through of the things you're gonna need to consider, but you might have to go and segment and filter the code by language you're gonna probably have multiple languages in your repos, um, all of that good stuff so you're gonna go and set that up, uh, make sure that things are properly, uh, preprocessed and filtered out by, um, languages. And different versionings you're gonna sanitize things. All of our repos probably have, um, some sort of sensitive data or information that may need to get stripped. You wanna go and make sure that's been taken care of. You're gonna begin looking at tokenization. Do you have proper tokenization? Do you have to have custom tokenizer? Is there something there that is, um, gonna work for your data set? You're gonna have to iterate on that a few times. Then we're moving into, uh, basically ensuring if you have anything that is privacy sensitive you've got to go and kind of ensure the transforms and the transformations that are sitting there are properly occurring and that's gonna require um a little bit of review so you do your pre-processing and cleaning you review your data sets kind of iterate from there. So super simple, like a little example here. I won't even read through this, but just a way to go ahead and do some code clean up, uh, very simple. Uh, then let's go into a little bit of tokenization again. Just some really simple snippets here, just as examples. A lot of this is a lot more descriptive if you go into um llama recipes and you look at a lot of our fine tuning or end to end use case examples. But I figured I'm not gonna walk through it line by line, um, so let's keep it simple here. So let's talk about the actual fine tuning situation. So you've gone, you've got your, you've aggregated the data, you've gone and cleaned it up and processed. Now you're actually getting to fine tuning. Fine tuning is not a one pass and done type of thing. A lot of times you're doing several loops of fine tuning. You're gonna go, you're gonna iterate, you're gonna look at what your loss values are. You're gonna look at what the quality looks like. You're gonna eval things and you're gonna determine, do you need to adjust data sets? Do you need to adjust parameters? It's an involved process. And when you think about optimizing code generation as a particular um kind of example here, we've got some tips here for how you would think about it. So, If you're looking to do it and you're looking to fine tune a specific model, 8B is still a model that people are trying to use, right? 8B is very, it's relatively lean. Uh, it works pretty well. It's fast, and it can solve a lot of specific problems if you go and customize the model. 70B has a lot more capability. A lot of people will start there. Uh, when you're thinking about which model to choose, you'll start at 70B or something like that, and then if it works well, they'll look at how do we maybe compress that down to an 8B model just to save, right? So then you'll As you're running your fine tuning path as you start thinking about getting this running, you're gonna think about how do you actually, um, implement this fine tuning and how do you run this in an efficient way. So you'll look at like different SFT, um, techniques to supervise fine tuning. You'll look at am I going to run. Uh, basically. Full parameter am I gonna run something compressed? Uh, it's totally up to you and a lot of people will try to go and like Pest or Laura or something like that, and that's what they're gonna do in first and then they're gonna kind of evolve from there. You're gonna look at nowadays RL, which is a pretty hot topic, but in general, uh, RL is a pretty like well known thing. People have been doing RLHF for many, many years. Uh, that's definitely a way to improve your actual fine tuning, um, and optimize your actual process here, and then you're gonna look at, can do I wanna actually run and manage all this stuff, or do I wanna use tools that are available that can automate and build out a lot of this process for me? And that's where I have the bullet point there, the check mark around Sagemaker. Uh, that's just an option for you to go and use a robust training infrastructure with the tools and all the, all the actual features built in so you don't have to go and orchestrate this fine tuning pipeline and all the iterations, um, manually. You can just use that. You also have the ability to go to like hyperpod and, uh, if you need to do distributed type training EKS and all of that. So pushing on, just a super easy example of what you're gonna look at if you're gonna set up an actual like uh run of fine tuning here. You're gonna go ahead and set up the code, set up the configuration parameters, and then let it run, right? This is an example running through epochs that doesn't mean much depending on your use case. You're gonna go ahead and have to iterate on that or figure out what's worked for you guys before. Look at what your learning rates are, loss rates, all of that. Understand, am I getting the progress we want? Is this evaling? You're gonna pick obviously instance types, um, so that's a GPU. Many of us were probably using, I think that's the. A 100s. So a lot of times you're gonna either pick with the larger llama models you're gonna be running like a P4D, a P5, something like that. Uh, you could also use tranium for this. It is supported and those are all options there. So eval next big topic evaluation is super critical as you're going to do fine tuning as you're gonna be looking at did this actually work, is this something that's sufficient for my customers' needs, you're probably gonna have an eval data set or you're gonna have to generate one for evaluating and validating that your fine tuned model is performing like you wanted. So you're gonna have to set up, uh, basically a reliable way for you to determine is my code generation now improved after I've done my fine tuning that's gonna help inform you. Did my training data set help? Did my configuration parameters help? Do I need to run more epochs? Do I need to change and pivot things? I'd say at least when I, when I run through training loops, it's more so gonna be at least 5 or more passes on my fine tuning. And that's gonna lead me towards the let's say the eval metrics that I want, but that's 100% dependent on you and what sort of data sets that you have when you go into this. So that's one of the reasons we really emphasize data sets are super critical when you're going into this. Having a really well established 11 that's really well formed, is going to help, uh. Ideally reduce a lot of the complexity and passes you'll have to run, but again that's not necessarily guaranteed, oops. So let, let's just continue through this slide. Um, so I mentioned things, but let's look at the bullet points here. So you're gonna want with evals to understand key metrics, so your customers are gonna have key needs. They want to perform at a certain level of accuracy and confidence. That's gonna be one thing when their coders are using a coding assistant or a code generation, you're gonna wanna make sure that works. You're gonna wanna make sure it's creating compilable code. You're gonna wanna make sure it's doing stuff contextually aware. So that's gonna be something you need to build into your evals, um, that also includes things like human eval and custom internal data sets. You're gonna have, uh, ideally ways to monitor the evaluations continuously and run these things, uh, post deployment, and then there's gonna be generalized metrics that you can also, uh, kind of depend on like blue or, uh, the any other kind of standardized ones that are out there. So let's think about deployment now. So you've gone, you've decided to customize the model, you've gone through your pre-processing, you've fine tuned, you decide this version, this customized model weights are ready to go and deploy. Maybe they're not perfect, maybe they are. You're gonna go deploy it. A lot of options to deploy. We've mentioned them already. Bedrock allows for managed inference but also allows for custom model import which you can take advantage of, especially when you're prototyping you're doing a lot of these loops and you don't wanna go set up all the infrastructure and manage that too. That's, uh, an option. We have things like EKS. A lot of people when they decide to go into production or if you have a mature organization that knows how to manage infrastructure, you're looking at options like EKS, uh, for custom kind of deployment of model weights, right? And then you have um other considerations with code in general. So now you're looking at you're not just deploying to an endpoint and allowing people to prompt it you're looking at integrations with IDEs. You're looking at does this go into VS code? Does it go into my other IDEs that are available to my engineers? Does it go into a web client? A lot of. Options there, um, then a lot of open questions about how does that actually go through our repo updates or, um, our source control and versioning updates. Do we have to keep custom parameters in there that state, uh, where code is being generated versus being manually edited a lot of considerations there. You're also looking at deployment. OK, are you're gonna be setting up custom rest APIs for this, uh, and then you're gonna be looking at code review bots. How do those integrate with things? A lot of us see that within our organizations where you have code review bots and, uh, basically automated systems to help catch these things, catch errors, catch issues before they hit even like a, um, alpha train or something like that, or before it goes into any deployment passes. Uh, but all of these are things you're going to be considering as you're thinking about deployment. So deployment and just another example here using fast API um I won't necessarily go through this but just another option as you're looking at um. Basically quick deployment options and quick quick deployment, um, capabilities. There's a lot of options like this too, but I thought I'd just put up an example. So I spoke a lot in that past 25-30 minutes, uh, best practices for deploying llama models. Let's ensure that data. Is basically de-duped, cleaned up, pre-processed before training, uh, in general they're open source models we like to really create an open community. We love the idea of contributing back as you've learned, maybe that obviously data sets, a lot of that has information that's confidential or private. We understand that that's the power of open source models you can train for your own purposes and use cases, but then contributing back is a big thing. Uh, basically optimizing inference costs by picking the right models is always a critical thing. You look to train a model. You maybe don't wanna be running 70B unless it's a complex task. So you look at how do you minimize that. You look at an option with 8B. That's why we have so many models that we've made available. It gives you that custom customization, um, capability, the, the ability to choose what you want, how much you wanna spend, and what sort of performance you're getting. Um, and then just in general when you're looking at working with llama models, engage with the open source community, engage with us, we, we do obviously look to developers for feedback and to continue growing the llama community and also effectively create better models in the future. Now. Let's think about real world examples for just a minute. So real world examples on the code side you're looking at internal tools you're looking to basically accelerate development boost productivity that's a big reasoning we're using LLMs and that's why we're getting these assistants developed. Automated code reviews, huge use case. A lot of the stuff is happening any time somebody submits a diff and that gets PR'd, those types of things are going through code reviews. Even when a diff gets submit, code reviews kick off. All of those things take time. They take money. They take compute in order to uh, to, to run and accelerate. Not all of that's running obviously via humans being tied in. It's all automated. There's a lot of old ML, uh, ways of doing that now. LLMs are coming in and they're helping to use MLP to be able to do that type of thing too, and it works very well, especially when you customize your model and you orchestrate multi-models for those use cases. And then just streamlining DevOps, so you're talking about customizing models and basically being able to use agents um for the same purposes, but then allowing orchestration of multiple tasks if you're looking at orchestrating agents to go and do your fine tuning from end to end or thinking about data processing, those are all options that people are looking when they're customizing something like LA for coding tasks. So I'm gonna transition to um not necessarily a walkthrough a demo, but a couple of examples here. So what I wanted to show everybody. We're a couple of examples where we have basically built coding assistants and we've published these on our open source llama recipes repo or Lma Cookbooks I believe is what we've named it now. And the idea behind them is these are either getting started type examples they're full end to end POCs, um, or just use case solutions for anybody to take and then use as a frame of reference for building your own kind of clone of that, um, or we actually have some end to end applications and solutions we've built here which we also publish an open source. So I wanted to go through in particular just a quick run of this example we have where basically we've built with a coding assistant, uh, and what we've done is we've gone through a few different tasks where we wanted to migrate basically an OpenAI API to LA API. We went through fine tuning llama models. We built a rag chatbot with Lama and then we did some upgrades because for this particular example we had originally built it with Lama 3 and then we wanted to migrate to LA 4. So it's a mixture of steps here, not necessarily solving an exact coding use case, but in general this is the process you're gonna look at when you're running a a similar use case like a coding assistant, right? So in the first stage here we go through some examples of basically migrating an OpenAI API to LA. We've kind of showed you guys how we thought about this. This is something which easily gets kind of transferred if you go into using prompt opps as an opportunity here. You can go and use that to really streamline this. Next, what we do is we go into fine-tuning llama models, um, and we talk here about how we go through that process. All the code is available on this as well, if you just go up a level. And that's gonna be an example. We talk about actually building that rag chop chatbot with the fine-tuned model. And then we talk about how you think about going from llama 3 to llama 4. Again, example, a perfect example use case of prompt ops. So another one I wanted to kind of share here was generating code based docs because it's related to if we're looking at coding assistance or we're looking at customizing models for code, uh, this is a very common use case where we've built an example here of being able to use Lama to go and build out useful generation of docs and this is something you can go ahead and build out kind of run on a small server or run on a small endpoint and then go ahead and go through your entire code base to begin documenting everything. I won't run through every step of this, but again, these are just examples I wanted to share, to show exactly and then like a lot of the tools and the capabilities that we've already gone and built and we're trying to make accessible. So one thing I'll do is I'll transition over to this. So this is just the GitHub repo. This is one file, which is end to end use cases with a number of examples built in, going across examples requiring fine-tuning, coding, SQL to text, text to SQL. Uh, multimodal type rag use cases, a whole bunch of variety of, um, example cases that we developed and we've put out there as options. We also have within Llama Cookbooks a number of the tools that I mentioned. So I mentioned prop tops. If you go through Lama Cookbooks and you look through our actual read me here, a lot of the links to our tooling and the capabilities are already built into this. But then again this repo has a wealth of just getting started information. Now one thing I should have linked in here is AWS also hosts a repo, which is LA on AWS. That one has a bunch of end to end use cases we've worked with, uh, AWS on and that also go ahead and build this type of stuff out. We actually just worked with them on a agentic fine tuning use case where. Uh, we just did a, a road show across the US and we published those. So if you're looking at building out coding assistance via agents, then that's an option as well that's published on their repo and was recently published out there. So I'm gonna go back to our, our deck here. And Gonna kind of wrap us up here, so we went over a high level of considerations you should make. We didn't go into super deep detail on actually doing a lot of that fine tuning. One thing we'd love to do is we'd love to run one post presentation survey like we ran in the beginning. I'm gonna keep this slide up here for a minute if you can spend a few seconds to do it, that would be amazing. All right, and then we have some next steps for everybody so next steps here are this presentation was really covering tools, thoughts, the way you think about fine tuning and setting up for coding. Uh, I also went into exploring a little bit of the open source content we have on GitHub, but that's something I think, I think is extremely useful for you to jump into, especially if you're trying to solve a problem now rather than spinning the wheels and doing it from scratch. A lot of this has already been kind of worked on and pushed into these repos already for you guys to access. Obviously recommendations, further reading materials, papers, all of that we're always interested in reading new content as well, but a lot of that we post onto the repos as well. Um, AWS services can help automate and build out a lot of stuff quickly, um, and then again you can utilize EKS and all those things as you look to deploy and scale. I'm gonna talk about this slide just really quickly because I mentioned our tools earlier but I didn't get to talk about them a lot in detail. I talked about prompt ops. I want to talk about Datakit. Datakit is useful for your fine tuning passes and any of that you wanna do, um, as well. So basically it's a, it's a, it's a tool for synthetic data, uh, generation. So we've, we built this out earlier this year as well. It goes ahead and builds out and helps you automate that process when you're looking to develop a bunch of synthetic data for. Say you're doing eval data sets. Say you're doing training data sets. The idea is really open and there's a lot of examples on our repo and the Lama on AWS repo which have utilized us to go and train the models. So I wanted to make sure I actually talked about that for a second. So if you haven't seen these tools, check out the general repos. Go to the LA web page. Um, we have links to everything there as well. I'll go ahead and end there, but I'm gonna just kinda I'm gonna mention I'll hang around for a few minutes if you have more specific questions. I'm happy to chat about them, um, but I'll hang out off stage after this. So thank you guys for attending. I appreciate it.