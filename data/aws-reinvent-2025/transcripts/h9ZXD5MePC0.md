---
video_id: h9ZXD5MePC0
video_url: https://www.youtube.com/watch?v=h9ZXD5MePC0
is_generated: False
is_translatable: True
---

My name is Sudesh, and I'm thrilled to have all of you here with me. Uh, before we start on with the session, a quick show of hands. So, how many of you have any live production applications running right now? Almost everyone, right? And you're here at Reinvent, enjoying the sessions, learning. How many of you know what you would do if the application goes down? Oh, fantastic. So you are clear on the step one, what you would do if the application goes down. How many of you have tested that workflow in the last 90 days? Fantastic. Great. Uh, so today I'm going to talk about a very important aspect of any business requirement which is DR, which is business resiliency, right? And with me we also have Zydis, a global pharmaceutical giant. We will walk through their journey of how they implemented disaster recovery for a very complicated and complex suite of applications using AWS disaster recovery system. But uh let me start with first talking to you about the agenda for today. So, for the longest time, backup and disaster recovery was done using tapes. You have your data, copy that to the tapes, save the tapes at a secure location. Anyone does that now? I hope not. OK, great. In today's time we have multiple architecture architectural patterns and solutions to give you much faster DR that too at a much lower cost. Of course, backup and archival still exist, but you also have an active, active setup which is much faster near real time, but that also comes at a cost. You also have multi easy deployments which cloud offers you. Again, there's a different cost angle to that. Right, so DRS gives you the benefit of a much more efficient disaster recovery at a at a lower cost, right? So you get the benefit of near in subseconds RPO and a few minutes of. Without taking the entire cost of active active set up. So today we have Mr. Ravi, chief financial officer for Ziss, who will talk to us about what their applications are, give us a brief overview of the company, and then I'll talk about how DRS helps you solve the DR problem, how do you set up and life cycle of disaster recovery using DRS solution. And finally we have Ashish, our enterprise architect, who will give a short demo of how the service actually works and talk about how we architected the solution for Zyders, not only from the backup perspective, but how we architected. The network. How did we achieve security and resiliency for their environment using AWS? Well, with this, I would like to invite Mr. Ravi to talk about Zs and give us an overview of the problem statement. Hello. Good afternoon, everyone. My name is Ravi Yadavar, as Sudesh said. Thank you, Sudesh, for the introduction. I'm the chief financial officer for ZIDS in US operations. So first two slides I'm going to talk about what ZIAS is about, and, uh, one more slide on the challenges what we face with the current DR solution, what we have. And the last slide before I hand it over to Sudesh, like what is the scope or what is the expectation from AWS to help us with the Uh, DR solution with this we can see on the screen like Zadas is one of the global pharmaceutical company founded in 1952, headquartered in Ahmedabad, Gujarat, like in India, it's India-based pharmaceutical company, having grown into more than 29,000 employees worldwide, and we also have a business operations in multiple countries like India is the main and the second largest is US. We also have a business in Europe, like many countries in Europe, uh, Brazil and Mexico and many other like emerging markets. If you see the. Screen we have business offices in multiple locations in the world and also we have 41 manufacturing plants. Out of 41 manufacturing plants, we have like 16 US FDA approved facilities, 9 formulation, 5 APIs, 1 biologics, and 1 animal health, all catering to the US market. And also we have 8 R&D centers. If you look at this here, the group aspires to transform their lives through pathbreaking discovery. So we are more not only we are a generic company in the US, but globally we are, uh, would like to go into like the discovery-based RNC-based company. That's the way we want to grow. We are, as I said, 29,000 employees. So I will talk about a little bit about the R&D initiatives, innovation, what you have taken in the next slide. So if you look at this slide like we're talking about, we have almost more than 1500 research scientists working across 19 sites developing on many like NCEs, biosimilars, vaccines, and many niche technologies developing into many new products for across the globe. So we can see that we are more focusing on like NCS, peptides, molecular antibodies formulation development, and And the nutraceuticals and uh we recently like published or announced our uh uh our positive results of Positive results on our pivotal phase 2B, phase 2, and 3 clinical trials for Saaglitazo in adult patients in primary for PBC treatment for adult patients, and we expect to submit with our FDA the beginning of next year and we expect the product to be launched in the beginning of 2027. This will be our first NCE product coming out of our research facility which is based in India. So we take pride in saying that we'll be the first company introducing NC from our facility. So having said this, uh, it's a very high level brief, uh, what is about, uh, we'll talk about quickly like what is the critical challenges faced and which drove us to seek for a new disaster recovery solution that we can focus, like we can, we can divide this, um. The, the challenge that we faced in three major like, uh, hurdles or challenges that we faced. The first one is like higher RPO and RTO rate. You may know like RPO. We had RPO of almost, uh, recovery recovery point objective almost 24 hours and RTO of like more than 24 to 48 hours. So this high level of RPRTO type, uh, result in very long recovery time of data in case of any hardware failures. Or any loss of data or accidental deletion or maybe any security events which is not good for any pharma industry. The extended time of getting a shutdown results in delay in production and regulatory compliance issues and any loss of data is negatively looked at or viewed by regulatory authorities and also any delay in manufacturing activities also result in delaying in delivering products to the needy patients. The second hurdle or challenge that we faced is the cost and operational overheads, as we all know, any investment or infrastructure investment, building up a facility is always, uh, we, we invest a lot of amount and also we have to spend a huge amount of money or amount in running the facility, operational cost and operational complexities. And lastly, we can say the security and skill stuff like on-site disaster recovery site is always prone to any cyber threats, cyber cyber incidents, and also we have to hire people, develop skilled people to run the multiple, uh, sites and, uh, keep them like trained for like application hardware center and other plant center locations. So these 3 major challenges which forced us or which drove us to seek to go to AWS and help us with a better disaster recovery solution. So with this we'll just quickly talk about what is it that we wanted, what is the scope of the work, what was the expectation of Ziddas from this team, AWS team to help us. So if you look at this, this whole thing what we are talking about this applications or the servers, everything, this shows how complicated, how complex, uh, the IT infrastructure is for any pharmaceutical company like a big company like ours, for example. So we have 10 mission critical pharma applications running like multiple like applications like. Sorry, so like we have this, uh, uh, like, uh, lab information system, the quality management system or document management system, warehouse execution systems, and warehouse management system, manufacturing execution systems, all this elog book, all this critical mission critical application running on 60+ like virtual, uh, uh, physical virtual servers running on multiple operating system like Windows and Linux and also database like SQL database or Oracle database. But the volume of data you use, for example, we look, if you look at we have almost 80+ terabyte of data in central data center, there's multiple 8 plant locations. These are all not just cold storage data. These are all actual live transactional data, uh, which requires very quick RPO time and RTO time of say RPO time of 15 minutes, RTO time of, uh, less than 3 hours. And also on top of this very quick RPO and RTO time, we have to also. Take care of this compliance, security and compliance issues because for a pharma company like us, regulatory compliance is not just a mandatory, it is regulate it is mandated by regulatory bodies across the world and every aspect of DR solution what we implement should be is easily validated. It is documented and it is audited by the regulatory authorities, so we have to keep in mind it is keeping. Not only is it is helping us to recover the data quickly, no loss of data, plus it is it is complying with all the GXP requirements. Having given all this scope, we approached the team and they helped us to develop a system for us. I lend it out to Sudesh to help us, show us what is the system they have developed for us, how they try to achieve our scope of objective. Thank you very much. Thank you, Sudesh. Hey, thanks a lot, Ravi. Uh, thank you for giving us a brief overview of not only the scale of operations at Zitus, but also, uh, also the business requirements in terms of RPO and RTO. Well, let me just summarize, uh, what, what we just mentioned in terms of business requirements and building this solution at Zitus's scale. Was the toughest part, right? Not only they wanted a minimal RPO and RTO of 15 minutes RPO and less than 3 hours RTO, but also the solution has to be cost efficient, right? The scale at which they operate with 80 plus terabytes of data, multiple plant locations, and these servers of multiple applications. You saw limbs, you saw different management systems. All of these are deployed across different infrastructures. There were physical servers. There were virtual servers. The servers also had multiple operating systems, different technology stacks, right? So the problem was of the scale and diversity of tech stack and the source application. Next is because they have multiple plant locations, we had to establish secure and resilient networking between different plant locations and AWS regions. And of course, as Ravi mentioned, all of their software and applications is regulated by compliance since they are a pharmaceutical company. So we had to make sure we design the entire solution, not only test and operate it, but also keep it compliant. Now, let me talk about uh AWS Elastic Disaster Recovery Service, right? But before I talk about the service, uh, let's just take a step back and understand what exactly is RPO and RTO is very briefly. RPO is recovery point objective, basically your data loss tolerance, how much data your business can afford to lose in case there is a disaster, while RTO is recovery time objective, which is how much downtime your business can afford or your application downtime tolerance. And of course everybody wants RPO and RTO to be minimum, right, but that comes at a cost. So there is a calculated trade-off between how much RPO and RTO you want to achieve based on how much money you're willing to spend. Lower RPO RTO, higher the cost, and vice versa. So elastic disaster recovery gives you a balance of RPO in a few seconds, RTO in a few minutes, and without the extensive cost of running duplicate servers in environments. Certain some key features of the service are, the first one is account isolation, right? The, the account or the or the environment where your data is copied is on AWS, that's your staging environment that is completely isolated from your primary. Your primary could be on-prem, your primary could be a different AWS account or any other cloud. That is completely isolated from your staging environment. So if your primary is down or compromised or impacted, your data still remains safe in your staging environment. The second key feature is immutable snapshots. The data snapshots that DRS takes cannot be overwritten. This is a very strong defense in case there is a ransomware attack or there is a cyberattack. Your data is intact. Your data is not corrupted. The third key feature is it helps you to have test drills and validation. This is not only a best practice, but also a compliance requirement that you need to periodically test the solution by conducting DR drills and validation tests to ensure your data integrity is intact and your DR solution actually works when there is a need for a disaster recovery. And the final point that I want to talk about is it helps you with point in time recovery, which means that you can go back to a particular snapshot, a particular time when your data was intact, that's before the attack or before the disaster happens. So collectively this gives you a very strong control on your disaster recovery that too at a low cost. Now the AWS DRS is a purpose-built solution for all your disaster recovery needs. The first key feature is it is flexible. Your data, your servers could be anywhere on-prem, cloud, hybrid, physical, virtual, anywhere, right? DRS supports that. It supports a wide range of operating systems. Multiple technology stacks. The entire service is reliable, which means that it is nondisruptive. You can perform your drills and your tests without impacting your production live use case, without impacting your production and live environment. You can perform these drills and these testing, and all of this is highly automated. Hence it's easy to use. If there is a disaster. The last thing you want to do is figure out which EC to do I launch or what are my login credentials, or how do I set up my environment. This is where creating launch templates and automating your entire process and testing it multiple times comes really at hand. And overall it helps you to reduce your total cost of ownership of building a disaster recovery solution. And at AWS security and resiliency is a shared responsibility, which means that AWS takes care of security and resiliency of the cloud. Ensuring the data is encrypted in transit, at rest, physical security of all your servers, of your data volumes, and it's customers' responsibility to make sure and ensure security and resiliency in the cloud. What that means is you have to ensure that correct monitoring, observability, and security protocols are in place. You set up and adopt correct backup and DR strategy. As per your application needs, and you test them as well. That is customer's responsibility. And the underlying infrastructure is compliance ready, which gives you a very robust platform to build healthcare and applications which require different GXP compliance. So it helps you during your audits and compliance checks. Now, let me talk about the entire life cycle of disaster recovery service. The step one is set up. During setup stage, you set up a replication agent on your application servers. Again, your servers could be anywhere, any operating system. As long as you can install a replication agent, you're good to go. Once your application agent is set up on your servers, it starts backing up your data to the staging environment. It does this by doing block level replication. Once all your data is copied in the staging environment, your setup is ready and you're good to test. In the testing phase, you simulate a disaster in that you create recovery instances or recovery EC2s through launch templates in your recovery environment, which is again completely separate from your primary environment. Once you test that your recovery workload is up and running, data integrity is intact, that's the time you get into the operating phase. At operate phase, the agent or the replication agent continuously does block level replication of your data. And this is how we ensure that you don't pay for an active active setup. You just pay for the block level storage that you consume to copy your data. If there is an actual disaster, either natural or a cyber attack or a human error, you fail over. During failover, you using your staging data, you create your recovery instances and your business starts operating instead of from primary through staging. And finally, fallback. Fallback is equally important. When your primary site is again up and running post disaster, all your data from your recovery environment is copied back to your primary environment and your business is up and running uh from the primary site. Now let's take a look at how this works by taking a look at the technical architecture. On the left you will see the on-premises set up where all your source VMs exist. Now you have to install a replication agent on these source VMs. All this data will be continuously copied with the help of replication servers. In AWS on your staging subnet. So what our application server does, it copies your data, it reads your disks, uh, compresses it, encrypts it, and sends over TCP to your staging subnet. On staging subnet, all of this is copied into your EBS volumes, so the cost that you bear is only for replication servers, which are again not full scale production servers which you would otherwise use to run your application workloads, but replication servers are low cost servers and your EBS volumes. Both of them are again comparatively much more cost efficient. And you also also set up your EC2 launch templates. So launch template is where you predefine in case there is a disaster what EC2 instance do I launch, how much, what capacity, what configuration, what should be my network configuration, what should be my cider ranges, and all of that. Now let's take a look at what happens when there is a disaster. When there is a disaster, your primary site goes down. That's when the replication servers and check that all your data is actually copied to your EBS volumes. Once the data integrity is tested, then it quickly creates snapshots using your EBS volumes and using the launch template that we have defined earlier. And with snapshots, a new environment is created in your recovery subnet. At this time, all you have to do is point your network DNS to use the recovery subnet instead of your primary. Right now you're operating in a failover mode where your application is entirely running from the recovery subnet running on AWS and not running from the disaster impacted primary source. All of this is automated so that if there is a disaster at 3 a.m. at night. You don't have to actually figure out what to launch, where to launch it, and all of that. All of that is already tested, already automated, and already defined when you do the setup stage for DRS, and DRS abstracts and helps you with building this end to end solution which you can test as often as you want to. Now, this is half of the DR cycle. Now what happens when your, when your primary site is up and running? So when your primary site is up and running, you have to fall back from your, from your recovery subnet to your original primary one. For that, you have failback client installed on your, on your primary site which copies the data back from your AWS environment. To your source environment, once all the data is in sync, you perform a failback and your disaster recovery cycle is complete. You start running your application back from your source environment and and then you can decommission the new instances which you started in your in your recovery subnet. Now Ashishish will give us a walkthrough demo of how this entire cycle works, and then he'll also give us a brief overview of how we architected the network and uh and and talk to us about a few learnings while doing this for Zidda Ashish. Thank you, Sudesh. Uh thank you very much. Good afternoon, everyone. So as we have now learned about Zyrus, their challenges, and also how the AWS disaster recovery overall service architecture and the compliance, let me take you through a journey of Ziddas solution. I mean, where Zidus started, uh, it was not a single month or a year journey, multi-year journey. They started, they wanted to first verify how the solution actually works for their key requirement, which is the minimal RPO. And for that they wanted to understand how the application level RPO and RTO is met with the solution, so we created a disaster recovery service demo, more of a use case demo, where we showcased, and we'll see in a minute, that we take an example of a Windows-based application server and a SQL Server database, pretty normal in the pharmaceutical application workloads. And then what we did was we created an abrupt failure scenario. We recovered that to hundreds of kilometers away in a different region and measured the RPO, RTO, and the data integrity. So this is how we will see in the in the solution demo. This demo required a failover scenario and also verifying the data integrity. It is approximately 20 to 25 minutes, and that's why we have recorded, pre-recorded this particular demo. So let's look into the demo. As you can see here, uh, we have an application server and a database server in a source region, Mumbai, and replicating into the Singapore region, which is more than 4000 kilometers or 2500 miles apart. Let's verify the source environment. This is a CMS application. Uh, let's see in the, uh, server also, it's 80 GB server and running on a .NET 5. Application Let's verify the database server, approximately 150 GB, and to measure the RPO, what we've done is we have created a date script which modifies the date table every second. And so that we can measure the RPO in the overall disaster scenario, and as you can see, it's modifying the script, and we will keep this running. Let's now also Check the application level RPN RTO measurement uh without creating a blog post. Now let's mimic the exact abrupt failure scenario. So both the server, we will skip the operating system shutdown. That means it may corrupt the data or system stability issues may occur. Majorly, generally that's where the disaster whenever occurs. This is the scenario. And immediately we will get to the action. Both the servers simultaneously we will recover. And if you see, we have various snapshots, and but for this particular demo, we will use the latest snapshot to measure the RPO. As you can see, the job has started. Let's also measure the exact time of the disaster from the cloud trail audit API service. During the jobs I mean the recovery job, one of the key steps is the conversion process. So any source, as we have seen, any source machines can be recovered on AWS. So this particular conversion process, what it does is it makes changes into the bootloader so that your source server type, it could be a physical server, virtual server, Hyper-V, VMware, or any virtualization. It can boot natively on AWS. That's the temporary convergive server we spin up. And as you can see now, the complete recovery is completed. And now it's time for verification of the data integrity. We'll check the application server first. As part of the recovery, if you know the Windows operating system, if it have done abrupt failure, it will give you a shutdown event tracker, which because of the abrupt shutdown of the operating system. We'll start the ISS server and uh meantime it boots up. Uh, let's check the SQL database. Same shutdown tracker. Let's now check the date script, how much data we are able to recover from the abrupt failure. So we'll query a date table. As you can see like it's 11:45 48, and we'll summarize in the next slide how much RPO. We are able to recover or how much data we are able to recover. Let's also check the application level RPO and RTO with the data integrity with the blog, so we are able to also see the blog is also available, which was just created before the abrupt scenario. So what we observed, the AWS Disaster Recovery Service was successfully able to recover a Windows-based application and a SQL Server-based database application from an abrupt failure to a completely recovered state which was there in the original state of application. This particular process, we are able to do I mean 9 seconds of an RPO and less than 25 minutes of an RTO. So it was the job took around 21 minutes and then we take the IS boot up time because that's where the application is accessible to the users. Uh, this was, uh, with Mumbai and Singapore region, which is like 4000 kilometers or 2500 miles apart. And distance plays a role in RPO as you may know, right? This is great. I mean, if you see the Zyr scale, it has a data center and multiple plant locations across the country. And for that, how, I mean, what would be the scenario where they can achieve a less RPO? Any guesses, I mean, What could be the important aspect of architecting the solution? Network architecture. You may have guessed it right. So let's look into the network architecture. So this is the core foundation of ZIA's disaster recovery solution, providing resilience, security, and performance all together. If you start from the left, if you see the Zass corporate Data Center, which was already connected with all the Zass plant before we started this particular solution implementation, they already had an active passive network, different network provider connectivity between their data center and all the plants. All the traffic was going from the plant locations. To the central data center locations. What we did was we just extended the existing resilient network architecture to AWS Direct Connect with a distinct AWS Direct Connect locations with the same resilient pattern with Active passive with two different network service providers. Here, the key service is AWS Direct Connect, which is a secure and private connectivity service, connects between your on-premise and the cloud environment. One of the key things or design decisions we had to make was after establishing this particular connectivity, all the, I mean in the existing network pattern, all the disaster recovery traffic will also go to the data center and then will move to the cloud. That would create a bandwidth issues at the data center level. So what we did is we designed the network path such that only the DR replication traffic will move from the plant location directly to the AWS Direct Connect location, and the rest of the plant traffic will move on an existing path of the Jade corporate park. So this was a very key decision, this actually optimized the bandwidth. Uh just imagine in at the greater scale when they add new plants, this would have created a uh exponential issues at the bandwidth. Let's move within the AWS cloud, where if you see the service called AWS Direct Connect Gateway. So Direct Connect Gateway is a service where you can connect to multiple AWS regions. Just imagine, I mean with a single resilient connectivity, Zide is now able to connect to any of the AWS regions. They don't have to mimic the same connectivity for every region, and that is enabled by the AWS Direct Connect gateway. The next key service is the Transit gateway, which is the central router and a network segmentation using hub and spoke topology. This is a very key service which enables you to for isolating your critical workload into a separate AWS accounts and virtual private clouds. If you see here. From the different network. Accounts and the disaster recovery production account and as well as multiple VPCs having network account having disaster recovery staging subnets, recovery subnets, all are isolated for a network segmentation, better security. Point of view. Let's Deep dive into the overall Zy complete disaster recovery solution. Starting from the left. Uh We installed the AWS disaster recovery agent on each. Source server. Which started replicating encrypted and compressed block level data to the staging subnet. If you see here, We have already seen the direct connect and how it is enabled by the networking services. Moving to the recovery subnets, if you see here, the recovery subnet, there is not a single workload which is running 24/7. And that has only been Available during disaster recovery drill or failover scenario. Ziddas followed a 3 step of recovery in the recovery subnet. In the first step, they recovered a primary active directory. It's recovered and configured, along with a third party firewall instance with a pre-configured Amazon machine image which provides additional network security to their existing disaster recovery environment. As you know, your data has to be stable and available before your application can connect. And so in the second phase, all the database servers were recovered and configured. And at last, in the 3rd stage of recovery, all the application servers are recovered and configured. At last, if you see, I mean, this is all the overall network architecture and the solution architecture. If you go into the solution now, let's say what was the implementation phase, right? You must have those questions. So followed a structured and phased approach in their implementation. Starting with establishing a secure and resilient AWS cloud Foundation, we have seen in the previous slides. Then after we started application wise and a location wise, replicating the data on cloud, this was in the first phase. In the 2nd phase, the replication was completed of all the application servers and the database servers, and then it was extensively tested for configurations. This was done in the 2nd phase, followed by a phase called as a hypercare for any to addressing any unplanned issues. Hypercare is a temporary phase, temporary phase where we provide intensive support to address any unplanned issues like a replication lag or our application backlog, or let's say recovery configurations which you may encounter during your extensive testing. The third phase, and this is one of the very critical phase in terms of implementation, where we did a centralized monitoring and automated alerts for key matrices like replication lag, replication backlog, and network state monitoring, which is very important to make sure the overall RPO is minimal. In the same phase, we have also done solution validation. So the solution was validated as per the GXP guidelines to ensure the data integrity and availability. Accordingly, Ziddas teams have completed the GXP compliant solution validation documentation. And at the last phase, the solution, we have done a complete disaster recovery drill with complete failover and failback testing. As you can see Within 6 months of a period, enterprise scale complex solution implementation, as well as validation was completed by the ZA team, and this is one of the outstanding efforts by Zas IT and the business teams together. Here comes one of my favorite sections. Uh, not only, I mean, failed implementation, even the successful implementations like this also teaches us something valuable. And we would like to summarize in three key relations. The first one with the cloud Foundation, where Zido prioritize the secure and resilient cloud Foundation, as we have seen in the phased-wise approach. Why this is important? And if you ask this question, I mean because it ensures that the DR environment stability is there throughout, ensuring the minimal RPO, which was their key requirement for implementing the DR solution. Not only that, that also enabled them to spin up new workloads which are in isolated environments based on the network and the security-based architecture. The second one is where the things I mean things went interesting. So while in the second phase, what we realized was there were 3 database servers which were having consistent replication lag issues. And this was very odd because there were more than 10 database servers, and they are very, very, I mean, critical in terms of the transaction volumes. But only these 3 database servers in their peak times, and other database servers in their peak times also, they were not showing any replication lag issues. So ZA teams, AWS support team, and as well as the partner teams, they actually worked together, deep dive into the issues in multiple levels of diagnosing. What they figured out was there was a very extreme change rate was there on those three database servers. And Just summarizing the resolution which happened in the stages was with configuring their firewall QS policy, the corporate QS firewall policy, then putting a dedicated replication service, and as well as increasing the IOPs and the throughput of those database servers. Now you may have a question. This is a uh a block level replication. And when you have just replicated the same disk or a replicated DR environment, how come IOPs and storage throughput can be changed without changing the storage size? Because storage size is the same because of the block level replication, right? So AWS has a GP 3, which is the latest volumes, where you can increase the IOPs and the throughput without changing the storage size. And this was very key in achieving our solution also overall. Let's move to the last one, lesson learned, monitoring and alerting. So automated monitoring and alerting was set up, you already know as part of the phase wise implementation. What we observed was certain alerts were coming quite often and due to the nature of the networking, which is part of the remote sites, and they were also automatically getting resolved and so. Overall governance point of view, we were getting a lot of alerts without any insight, so it's a little more of an alert fatigue, you can say. So what we did is AWS, AWS partner Jatma Technologies, who have actually helped with the semi-autommated reporting, daily reporting, which made sure only for those set of alerts, also we are not missing the key insights at the end of the day, and it helped actually in overall governance, though I mean this is part of a single sort of or I would say 3 to 4 matrixes. The key learnings from here is like monitoring and alerting, making sure like when you are establishing a complete governance, you're not missing any insights out of that. Certain alerts, if they're not making sense, maybe removing it is a better choice. If they're making sense and having repeated alert like false positives, then you need to make sure you get a better insights with some daily reporting or maybe hourly reporting sort of implementation. Let's move to the, like, summarize the transformative disaster recovery solution implementation by ZIDAS. So as you can see, uh, from 24 hours of an RPO to less than 15 minutes of an RPO. And 24 to 48 hours of RTO to less than 3 hours of an RTO, all this while not having any running any disaster recovery site up and running or any infrastructure up and running. Or without having any operating system or a database licensing, which means no capital investment. Operationally, also a very simplified solution which takes care of your physical servers and the virtual servers, all while having a completely managed and an isolated staging area, and you can do unlimited number of disaster recovery drills without impacting your actually primary environment, which is very critical for your regulatory compliances. So all this was achieved, summarizing the key benefits. I'm sure I mean all these benefits are, all the benefits are equally important for ZIAS. However, if I may ask Mr. Ravi to choose a single benefit which I would like to highlight. Yeah, Ash, see, you're talking about 4 benefits like no capital investment, minimal RPRTO, simplified setup, and security and compliance. See, see, everything is important, right? But if it is a finance person for me, no capital investment or minimum investment with minimal operational cost may be the key, but I will, instead of saying which is better, I will rate it. So I will rate maybe no capital investment is the number one, but for me. Equally important is minimal RP, minimal RTO as well, because that is more important from a data point of view and compliance point of view. Thank you. Thank you very much for your view. And let's now move to the last slide of ours. What's next? So, typical cloud journey starts with a disaster recovery, and ZDS has adopted a very strategic and measured approach in their cloud journey, uh, starting small with disaster recovery, proving the value, building the confidence, and then gradually increasing the the cloud workload. So they are in advanced stage of implementing the cloud-based file servers, having secure and virtually infinite scale storage at their plant locations. They are also on the generative AI front. Z is investing in their technical teams by making them smarter and faster by using Amazon Q developer. And they are also evaluating to move their non non-critical primary workload on cloud. So with this, uh, we come to an end of this session. Uh, thank you very much for joining us and uh take a moment, please, uh, and uh your feedback, valuable feedback will be uh highly appreciated. Uh, please uh uh have that in the AWS events app. Um, thank you again and enjoy the rest of the reinvent. Thank you.