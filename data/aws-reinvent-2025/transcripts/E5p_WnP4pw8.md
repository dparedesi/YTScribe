---
video_id: E5p_WnP4pw8
video_url: https://www.youtube.com/watch?v=E5p_WnP4pw8
is_generated: False
is_translatable: True
---

Hello. Good morning and thank you for joining uh for this session. I hope you all are having a lovely day too and are already raking up a lot of steps walking through the maze that this conference is, but I hope that you continue to have fun and a lot of learning for the rest of the week. My name is Sujay Doshi, and I'm a senior product manager with Amazon guard duty. I'll be approaching, I'll be completing 4 years this February, uh, and I'm joined today by Peter and Mikey. Hi, I'm Peter Ferry. I've been with Amazon for 5.5 years, and this year is my 40th year in doing uh anti-malware research. Thank you. I'm Micaelli. I'm in a bank in almost 3 years and I'm working as a senior security engineer. Right, thank you. So, a quick agenda. We'll start with giving a short overview of what the services and just raise of hands on how many of you today use guard duty and more specifically, one of the malware protection offerings. All right. OK, that's good. Uh, next, we'll talk about the new feature that we launched, a couple of new features that we launched a few weeks ago. Uh, uh, one is a scan on demand API and the other is a fully managed offering to scan your AWS backups for malware. Next, we'll use Peter's extensive experience and then for the first time he'll actually dive deep into the inner workings of the guard duty malware scan scan engine and essentially how we keep up with the ever evolving malware landscape. And then we'll have Micheli share uh on how they use uh one of the offerings or a couple of the offerings from malware Protection from guard duty to drive their security outcomes at new bank. So, guard duty, Amazon guard duty for the, for the ones uh that have, that do not know, it's a native threat detection service from AWS trusted by 95% of our top 2000 AWS customers. Looking at data and consuming logs, essentially, uh, guard duty for across tens of thousands of customers, uh, monitors, millions, hundreds of millions of EC2 instances and millions of S3 buckets to essentially give visibility into account and resource level compromise. The sophistication that it derives is the accessibility to native threat intelligence uh that it collects from various internal threat intel groups, uh, and combining it with the all the different detection algorithms that we use that threat intelligence to detect uh sophisticated cloud threats. One such example is with a system called Mithra that we run internally. It's a domain reputation service. It's a graph-based neural network database that essentially monitors all the DNS traffic across Amazon for the last 7 days and curates a list of malicious and benign, uh, domains. A lot of the times, days, weeks, months sooner than a traditional threat intel provider. And the reason we are able to do this is the accessibility and the telemetry that we are uh looking at to kind of build this intelligence. Being a fully managed service. And being the first party service automatically these IOCs in form of IPs, domains, and file hashes are consumed by guard duty into the threat intelligence platform that is then used to monitor the customer workload in the network traffic to detect those threats. The other example that we share here would be a system called Mtpot that's a globally distributed, uh, system of threat sensors, honey pots that's essentially tracking the attacker behavior, the tactics, the techniques, and the procedures. Which ultimately also provides us the IOCs on and the behavior to understand on how the attackers are targeting the whole of AWS cloud that we can then leverage for specific customer uh workloads and then use it to generate findings. And then from an elasticity perspective or the auto scaling aspect, we're just sharing an anecdote that this year, uh, during Prime Day, which I hope a lot of you uh were able to enjoy Guard duty processed close to 9 trillion events in an hour, goes on to show with a 49% increase year over year on how automatically it will scale, uh, with the data, uh, that it consumes. With all this, uh, intelligence, it essentially relies on the telemetry and the sources that we collect the data from, and you think about when you think about guard duty as an account that has enabled guard duty automatically we'll be monitoring the cloud trail management events and the VPC flow logs and the DNS query logs from your EC2 instances. That's basically what we call as foundational sources. And then everything else, uh, that we have S3 data plane events, lambda monitoring where we monitor the lambda function executions for network threats. All of this, uh, in the runtime agent, all of these essentially are what we call as opt-in protection plans. So as a delegated administrator, you can decide to enable it on specific workloads where you pertain that the threat activity would manifest and you want to be protected against. Once we collect all of this data and a key differentiator, uh, from a fully managed aspect perspective means that US customers you do not need to do any source service side configuration so that we and then collect the data so that we can process it. You don't need to. You know, collect the data specifically for guard duty. It may be for any, uh, investigation or compliance purposes, but guard duty does not rely on customer vended logs. We automatically, if you enable one feature or multiple of these features, we'll automatically pull the data from a back end stream so that you do not have the cost of ownership of collecting the data and then, you know, shipping it to guard duty so that we can process it. And that's a key, uh, value proposition being the first party in a fully managed service. Once we have all of this data, it goes to the analytics engine that essentially comprises of threat intelligence we talked about some heuristics-based TTP detection algorithms, and then we also learn the normal behavior from these logs to essentially generate ML-based anomaly detection findings that gives visibility from the perspective of is it normal for the organization, is it normal for this particular workload? Do we, does the communication happen from. Uh, uh, an ASM that has a bad reputation, so on and so forth. But the major focus of this session would be on malware scanning or malware detection. Historically when GuardDity was launched in 2017 and up until 2022 we had all of these different uh protection plans or features which relied on telemetry data or events but in 2022 we expanded to for the first time we expanded to scan a particular resource uh specifically here to detect malware so uh we'll talk about some of the offerings. For malware protection that we have, starting with in 2022, as I said, we launched for the first time malware protection for EC-2 that delivers an agentless solution to detect malware for your EC-2 and container workloads that have been attached to EBS volume. With a single click and similar to other features, right, uh, the ease of use and holistic coverage across the organization remains paramount. With a single click you can get malware protection coverage for all your accounts and workloads. The centralized monitoring, uh, so you'll have a delegated administrator and the security teams can essentially review the findings with contextual data around the malware family, the, the, the source within the scan engine that we use, and we'll talk about that later in the. Uh, in the session Then there's no agents to a very big uh. Value proposition here is that there's no agents, uh, so you don't have to update, manage, install, uh, in order to specifically detect malware. The container awareness brings in, uh, a key protection against some of your container deployments and if there's a malware that's installed. Prior to this feature, customers had to essentially take a snapshot of their volumes, run it against whatever malware scanner is at their disposal, and then try to stitch it back to a narrative around if you saw in guard duty a network anomaly that indicates that there's some communication to see to server, what would a customer want to do, right? They all, they'll take a snapshot, they'll check whether it's malware or whether a malware was installed as a result of this network communication. But all, all of that with this feature was automated with a single click, so customers who have enabled this feature, uh, automatically will have their EBS volume scanned when we see a preceding network anomaly. Or you can even use scan on demand as part of your investigation or forensics workflow. And then this one's no, there's no performance impact to your live workloads because again, there's no agents that are installed. Next, uh, we expanded the offering to provide, uh, a fully managed solution to secure the data pipelines that you build on S3, specifically helping to solve for untrusted upload application use cases. You have external vendors that's uploading data or you have internal users that's uploading data to S3. In both these scenarios, you did not first want to be. The receiver of something that's malicious so that you want to establish a trustworthiness in the data that's uh that's uploaded to your S3. And also in the second case, you do not want your internal users through some letter supply chain compromise. In the event that they're uploading, let's say an image file, it's actually not a malware, and you do not want to be a distributor of malware. In both these cases, there's a massive need to ensure that there's trustworthiness in the data and you can and you can trust the provenance of the data that's being sent. With this feature on the buckets, you can configure the malware protection plan and we'll automatically monitor for any new objects that are uploaded. We'll scan them. We'll detect if it's a malware generator finding. Use EventBridge. Using EventBridge, there'll be scanned notification that you can consume either in the delegated administrator account or your bucket owner or the member accounts. But once we also complete the scan, we'll attach a tag on the object itself so that you can implement inherent protection and quarantine, uh, use cases using tag-based access control. In the bucket policy, you can ensure that no user or the application can access this particular object, uh, until and unless you find a tag that's clean, let's say. The scan engine, and again we'll dive deeper into what all comprises within the scan engine, but that's automatically updated to keep up with the. Signatures and detection definitions uh that we'll be using to detect malware. So you as a customer, you don't need to manage the infrastructure. There's no management of updating the scan engine to keep up with the latest and the greatest threats, and all of it is managed on your behalf through this feature and the seamless setup with a minimal configuration, uh, on the buckets that you want to protect from untrusted upload applications. A few weeks ago, we expanded uh the same offering. Everything's the same from a scanned infrastructure perspective, you do not need to manage, again, no infrastructure, no scan engine updates, but we added another pattern. Essentially to ensure that you not only scan any new object uploads for malware but you can decide to even scan existing objects within an S3 bucket to actually detect malware and you can seamlessly integrate into your workflows using the API to ensure that uh even for the objects that are already existing you can establish a security baseline for the buckets that are let's say not newly set up and uh a hybrid strategy of combining. Continuous monitoring through new object upload scanning, and then retro scanning through objects that are already present in your S3 bucket to ensure that you have the latest and the greatest security baseline uh for the buckets that's uh in the untrusted upload realm. And again, The orchestration layer, you can use the API depending on what the operating procedures are and the workflow entails. A major focus of this session for the remaining part of at least my talk would be to introduce. The fully managed offering that we expanded in guard duty, malware protection suite of offerings is to scan the backups taken by AWSS backup for malware. We launched this a few weeks ago and this is where. From the AWS backup console, as part of your backup plan, you can ensure continuous monitoring for malware once a backup job completes. We'll scan your EBS backups or snapshots, your EC2 AMIs, and your S3 recovery points automatically for malware once you set up that configuration as part of your backup plan. You can also use on-demand scanning and investigate your past backups, very specifically useful for safe restoration workflows. In addition to be scanning, uh, we have two modes that are available, full and incremental. Incremental scanning here provides cost efficiency wherein we'll only scan for the data that's changed between a backup that I took yesterday and the backup that's that I'm scanning today. So you don't end up paying for the entire scan always, but then the full scan is where you add more security assurance. And it's something that you would want to ensure that you're safely restoring all ways with the latest verification from the latest and the greatest. Malware detection definitions. So, the full scan gives you that opportunity. And again, the, the idea is it does not involve you as part of the backup, uh, processes, uh, for the backup jobs. Once they're complete is when the scan initiates and there's minimal configuration that, uh, you have to do either on guard duty or backup to, to realize this feature. And this eliminates the need for you to do manual scan and restore workflows which are time consuming, delays, delays the business recovery in cases on events like ransomware, and provides you, uh, a seamless integration between these two kind of native AWS services. But you might ask, on why would you scan backup resources for malware if you're already scanning the live workloads either through traditional EDR tools or using one of the offerings from GDD that we talked for EBS volume scanning and your S3 object scanning. And to understand this, we'll, we'll try to look at a scenario on how a malware progresses through a backup. And this is, this timeline is a typical 14-day retention timeline where in each block represents a. Daily backup. So as you can see in the first phase. Which is the green phase from day 1 to 7. We see that the attacker already has a foothold in the environment. And, uh, but again, the malware was planted but it's dormant, it's not yet activated, right? We don't see any system changes, there's no files that are impacted, we don't see encryption or there's no malicious artifacts that got introduced in day 1 to 7. Also, the dormancy of the malware uh ensures that there's no persistence that's created, um, and it's just specific, uh. Malware files that are present, but they're not yet acting upon. Also, the, the reason we call these, uh, backups as clean backups is in a typical restoration workflow, you'll restore, you'll always restore or you want to restore in a clean and isolated environment. What that does is it actually in many cases negates the, the effect of the malware which is uh environment dependent, right? And it also blocks the C2 communication pathways. So. All in all, this represents the safe recovery period and also a point in time before malicious behavior. I surfaced in the environment. The second phase is the yellow phase, day 8 to 10, where we start seeing early stage artifacts uh appearing in the environment. This might be small preparatory changes, but a still low noise that you're. In environment, EDR tools or life scanning workload solutions will actually not have noticeable indicators that are generated because of the low noise and still the dormancy of the malware. But all of these, uh, the, once the malware is activated, actually that slips past into the backups. You can still do some surgical fixes from this malware and, uh, restore just the critical business data, but that is where you still need to, uh, perform the item level recovery capability that AWS backup, uh, inherently provides, ensuring that you are restoring just the clean portion of these backups. And day 11 to 14 represents where we see ransomware event happen, so the entire production system and even your new backups that are taken are actually all compromised and it's unsafe to restore from these backups. If you think about this timeline in a real environment. The timing spans across multiple weeks. For an example, if the initial compromise that we talked about happened 3 weeks ago. Your last clean backup that you know of, uh, was taken 2 weeks ago. The activation happens now. So imagine that by the time the team identifies that there's a breach in their environment. It might be too late because of the retention window that you typically would have configured for your backups, and you start losing on the clean backups. Imagine if you identify the breach one week from now, you're basically losing out on the entire safe recovery window or the backups that are identified and tagged as clean backups. So the key takeaway here is proactive preservation of the clean backups, but more importantly, identification that you have a clean or an infected backup to ensure that your recovery process in the event of events like ransomware, you can safely recover always from a clean copy. So this, this progression actually highlights a natural blind spot, uh, and the early-stage threats that slip past into the backups. Uh, but are not detected by live workload scanning solutions like GARDDD, EBS scanning, or S3 scanning, or even your traditional EDR tools because they are not designed to actually detect some of these dormant behaviors. They'll be monitoring the in environment running workloads, and then we'll look at the behavior in many cases to. To find something malicious has happened. And if the threat is dormant, which in our example was the case, these threats will slip past into the backup. Automatic backup scanning essentially solves or addresses this by continuously scanning the backup data at rest into the backup repositories, abstracting vault-level uh constructs so that it automatically, once the scanning job once the backup job completes, we'll initiate the scan. And it continuously tracks the changes that happens to your backups. And identifying from when a backup moved from clean state to unsafe to actually compromised. And then you can define the last known clean backup is what you want to restore from, uh, in the event of ransomware. Also, the traditional. Recovery solutions actually rely on manuals restore and scan, which is again, as we identified, is time-consuming, adds cost, overhead, and delays essentially the business recovery. So automatic scanning through this feature basically solves for these efficiencies and will continuously provide assurance of the presence of malware as the backups are taken. All in all, bringing this together. Backup scanning essentially provides a second layer of defense to your traditional malware scanning for live workload solutions, and hence they are more like complementary and not competing solutions which are two essential security layers in the entire data protection life cycle. Quickly we'll look into, you know, the periodic importance of periodic full scan and how, uh, you want to ensure that while cost efficiency, uh, is a major factor with incremental scanning, but time and again, and especially for safe restoration, uh, how full scan and more specifically periodic full scans, uh, are important, right? So imagine the first full scan happens on the day 1 when the backup runs. So day 1 to 4. Uh, after the first backup, there's incremental scan, and we'll continue to scan just the data that changed between these days. Let's say a malware, a 0-day malware was planted on day 4. Uh, but guard duty because at that point the detection definitions did not have information about that threat. So what we'll do when we'll do incremental scan, we'll actually tag it as no threats found, even though the threat exists in the backup. When we scan that data, we didn't have, because we didn't have the detection definitions, we'll tag it as no threats. So now let's say after the 5th day is when the malware engine was updated and we now have that information of the 0-day malware. But if you continue to just run incremental scans, because it only scans the data that's different from the past day, we'll actually continue to attach the wrong disposition uh to subsequent backups and we'll continue to tag them as no threats found. So where periodic full scans uh come into play for the same scenario on the uh. If you, if on the day 6 when we identified that there's no threats found, let's say on day 7, you actually ran a full scan. Now, because you already have the information with the latest detection definition and also the definition for the zero-day malware, we'll actually attach the right disposition. We'll say that backup 7 is, uh, it has threats and, and we'll also maintain the lineage. Of all the backups attached or the parent backup from it, then we continue to take uh incremental backups. So once you re-based the baseline through periodic full scan, When we do an incremental scan on the 8th day, even though there's no new file that was added, which was actually malicious, we carried forward the status. So tomorrow, if you decide to actually, uh, Restore from 8th day backup, you, you will have the information that in the lineage there was a backup that was either the same backup or the backup in the lineage has malware. So once you remove, remove that malicious file from the backup source. The subsequent incremental scan automatically will rebase the status. So now what this provides is, is it, it maintains the information and the history of. Through incremental scans it will maintain the history of what files that were detected as malware. Are they still malicious? And if they are, we continue to carry forward the status and if we don't find that malicious file and everything is clean, we'll update the status so customers can use the subsequent backups or the last known clean backup. This is the importance of periodic scanning to ensure assurance every time you want to restore. A sample implementation workflow. Think about using Rard duty, you get to the point where there's an outcome in the form of findings, that is event bridge notification. You can use that and have custom implementation through a lambda function letter that will Tag the backups and use SCP to ensure that you are never restoring from a backup that's tagged as infected. On the other hand, for clean backups, you can safely move them to an isolated environment to ensure that you always restore from these isolated, uh, logical air gap vaults to make sure that the backups you're restoring is safe. How it looks on the console, you simply provide a scanner rule as part of your backup plan, define which resources you want to, um, Perform the backup scanning for and then you select the mode, uh, against the cadence of daily, hourly, weekly, monthly. And then the on-demand scanning is exactly the same when you provide for investigation or forensics, you provide, and you can perform full on incremental, but you provide the on and we'll continue to scan, providing you have the right permissions given in the role. And then on guard duty, you can monitor the results again contextualization like any other finding in guard duty is paramount. We'll give the malware source information, the, the scan engine information, so on and so forth. And again, if you want to restore as part of, you know, your, your backup processes, you go to the backup console and then you look at the backups that are, uh, clean. And then you can define to either run an on-demand scan. We, we actually recommend to run an on-demand scan always at the time you are restoring so that it, it verifies against the latest signature set to give you the assurance that you need. Pricing, uh, straightforward pricing, uh, for all the resource types, that is EBS, S3, and, uh, EC2, it's 5 cents for the GB. If you decide to do incremental scanning, the first scan will certainly be a full scan so that we can then define on the next time a backup is run what has changed. So the 1st, 1st scan will be full and then subsequent will be incremental depending on your configuration, and there's an example on for 100 GB volume, it'll be $5 and then subsequently if there's only 2 GBs that changed, you know, it will be cost efficient, uh, giving you kind of the same results that you'd, you'd reach to or outcomes you'd reach to with full scans. So, some general best practices, um, automated incremittent scanning for cost efficiency, periodic full scans for re-base lining and assurance. And it, like I said, in time of security incidents for restoration, on-demand scans always appreciated. And you don't need to enable foundational guard duty, but we still recommend it, right? Think of different solutions and findings that we have, especially for resiliency and recovery and ransomware detections, which is not always based on files, but fileless behavior you see with, you know, S3 data plane events where we see a lot of encryption. Uh, customer through customer keys or there's a spike or anomalous spike in, you know, get object list object kind of scenarios. All of those signals and findings are available in guard duty through other features. So even though foundational guard duty is not paramount, but we recommend it so that you can combine it with these signals to get higher fidelity findings around your ransomware events. And with, with all the information that's available in Findings and EventBridge, uh, both backup persona and security personnel get the relevant information around the malware. All right. I'd like to invite Peter to talk and dive deep into this. So guard duty has multiple engines that it uses to handle different detection scenarios. The first engine we have is a hash-based one. And we use that engine primarily to suppress false positives, so we can exclude the the one file and and no others. We also Uh, use this engine for responding quickly, for example, if there's an outbreak, and we can, uh, add detection very quickly, and we can respond within minutes instead of hours. The engine itself is quite fast, we've found some ways to accelerate our hashing operation. And the engine itself is resilient to minor changes in the file. And what I mean by that is that typically we consider a file to be a solid block of bytes and so we'll. Uh, hash, just from end to end, uh. Uh, to get a single value, single hash. But there can be cases where a file has a, instead of being a solid block of bytes, there might be some squishy bit right in the middle. It might have a URL or username password combination. And in that case, our engine is differing from the traditional implementation because it can be told to hash around the variable part. Similarly, if a file has. Content at the end that is uh variable, perhaps there's an appended configuration. Then our engine can be told to hash up to the point where the variable, uh, content starts, which means that you can add any number of additional bytes, you can change any values within the variable section, and we'll still have a constant hash. So for the people that, uh, you know, bad actors that that are getting their, their sights taken down quickly and they keep cycling, we'll still have detection for these. Next engine we have is a pattern matching engine. In the event that hashing is not suitable because there might be multiple changes throughout the file, or if a file is very, very large, but the really interesting part is right at the front, then we'll use the pattern matching, pattern matching engine for that. The engine is very flexible, it supports traditional signature-based scanning, just doing straight bike comparisons, but we can do more than that. We can, Perform Conditional evaluations up to a certain point in the file. We can perform frequency analysis of the bites we've seen up to a certain point. And we can even extract bit level characteristics of the file for very fine-grained pattern matching. The engineer's very expressive, so the intent of a detection is very clear for the next engineer who has to perhaps extend an existing detection or if they're the ones having to do the code review for for an existing detection. We use this engine primarily for specific detections, where the code is constant across replicants. And here's the example from actual malware. This code will execute arbitrary commands received remotely without any user interaction. And for us this quite a simple example, it's a single detection. But there also can be cases where we want to use the engine for generic detections where multiple uh variants of the same uh sample uh differ only slightly, but are otherwise functionally equivalent. Here is an example of an actual malware, just a snippet of the code. This malware uses a polymorphic engine to change its appearance from replicant to replicant. And The engine is able to insert random characters in the decrypted body and also change the variable names to random strings. Here's another example where you can see the constant portions are becoming smaller. And here's the extreme example where the constant portions have been reduced to the smallest possible length and all the variable names have been replaced with random strings. And despite the difference between the first example and the last one. There's still a single detection for us. It gives a sense of just how powerful this engine is. The next engine we have is a machine learning engine. And our focus with machine learning is cryptocurrency miners. The reason for that is because within our environment, cryptocurrency miners are a big problem. And you might be wondering how big of a problem it is. If we look at this table here. We can see how quickly a cryptocurrency miner can be installed. We went from a misconfigured state to a compromised state in less than 4.5 days, which means that the, The the compromised instance was found in its its pristine state almost immediately and the miner was installed very, very quickly after that. If we consider uh from a dynamic analysis point of view, over 60% of compromised workloads involve cryptocurrency mining as part of that post-exploitation event. And in fact, cryptocurrency mining is the most common post-compromise activity that we see in a dynamic detection scenario. But coming back to the static analysis. I took one week's worth of scan results that had detections. Of those scan results, I took 8000 samples entirely randomly. And of those 8000 detections we had, 45% of them were cryptocurrency miners. Now not all of those were unique, but even if 90% of them were were identical, that still leaves over 300 unique samples that someone would have to add detection for manually from this subset week over week if we were to do this. One at a time. So that's a huge problem. But fortunately for us, we have a machine learning model and I would love to say it's so awesome that it detects all of these. But it's not, at least not yet. But still it's doing some very heavy lifting for us and for the ones that it doesn't detect, we have our pattern matching engine and the the hashing engine to clean up the rest. The next engine we have is a 3rd party engine. We received this engine as an SDK from the vendor. And it runs as part of our regular scan flow. But we run this engine in an entirely isolated instance. It's not able to connect with the cloud provider, it's not able to connect to the internet at all. The only thing we get out of the instance is the finding results and we are very careful about what uh we'll accept from the finding results to minimize the chance of exfiltration of data. We use this engine primarily for historical detections because the vendor has seen many samples that we might never see. We don't automatically collect samples from customers and we don't share these samples with the vendor. So in the event that a customer is not willing to share a sample with us, then we might never see the sample at all. The SDK also offers heuristic detections and they use a machine learning model for this as well. We are working to develop our own heuristics specific to our environment, but at least for now this these the existing heuristics are a very powerful baseline for us. We also get passes in this SDK for common file formats such as archives, images and documents. Again, we're working on our own passes specific to our environment to get better scanning performance. But we also have file formats that the vendor might never see because we don't share the samples, and so we're working on specific passes for those two. So that covers our static detection for EC2 and S3. And Mikaelli, my, my co-presenter, will talk more about a specific use case for S3. But we have more than just static scanning. We also have a runtime service. It's runtime service, runs on the instance, it has system-wide visibility into. Uh, everything that's that's happening as it happens, we get file system and network events, and there's also container aware so we can see inside EKS and ECS. We collect events from the system into signals. And a collection of signals get correlated into a finding. Here's an example of a finding using our latest extended threat detection, which was presented at an earlier talk this morning. You can see the level of detail that we have here. We have a detection here for a newly downloaded file. There was a file system event when the file was downloaded that triggered a hash calculation and a hash lookup. The hash was found on our denial list and resulted in this finding. Here's what the findings used to look like, and you can see why we changed it. Uh, if you squint in the top right hand corner, you can see a reverse shell was created on the system. We have file system events when the file was created. We had the system events when the process was started, and we had the network events. When the process connected to the website and when it redirected the standard IO to point to a particular port on the system. And that collection of events resulted in this finding. Here we have another detection for a site that tried to connect to a cryptocurrency website. And the network event that uh was intercepted when the process attempted to connect to the site is what resulted in this finding. But for customers that don't want to run a runtime service on their instance. We have a network level scanning as well. And this is not relying on explicit scanning, it's just monitoring the activity as it happens. The network scanning is able to watch for network connections to sites with poor reputation. It's able to detect credential use in anomalous configurations. And it's also looking for data access that is not within the typical behavior patterns on the system. Here's an example of a a finding where a instance tried to connect to a cryptocurrency website. Here's an example of a finding where it's an instance tried to connect to a command and control website. And finally, here is. A finding where an IP address that's on our denial list attempted to call an API. That is commonly used just prior to an exploitation attempt. And we we see how This this shows how all of our engines work together to protect customers and that's how we keep up with the threat landscape. And now I'll pass to Mick Aeli, who can describe a specific use case for S3. Thank you, Pete. So I'm from the Bank's called security team and I'm gonna present a use case from EC2 and 3 of but that is my mojo. But before this, let me start introducing us to you. New Banks was born from the discomfort and the powerless feeling while dealing with bureaucracy in financial services. We started in 2013 in Brazil with a goal of simplifying people's lives and empowering them financially. Since 2013 to today we went from a credit card issuer to the most valuable brand in the country, seeing how successful we are, we begin to search other markets and seeing how similar problems Latin America have we started there and now we have presence in three countries in. Mexico, Colombia, and Brazil. We surpass our 100 million clients go. And we still managed to have a metric of custom-center philosophy to have less than $1 cost to serve each client. The about the challenge they are gonna present here as new bank was scaling and adding more financial products and more countries we began to, to have a problem with hat growth and also different regulators with specific countries. And this year, we had the opportunity to have a single process for PCI because it's basically the same in each country. Uh, the main point about the PCI that I'm going to present here is about marriage detection. And Not only about the requirements that we were gonna attend, we also had the company's requirements. Our security team don't have to impact or add friction to growth. So we, we were searching a tool that could be effortless to integrate in our system. And Guduri was the smooth place and most integrated with our environment. To begin the implementation strategy, I have a main point to, to present here is new bank. In a bank we treat each country as a single organization in AWS. So if we're gonna do the configuration of Guaduri on AWS organizations, we would have administration points for each country. And as a global team, it's simpler to go into with an invitation-based implementation. It showed on the step 3. But in practice, we have the accounts with the resources in the original place and going further to our demonstration point. For the monitoring of findings, we have a dual path. As a cloud security team, we have to enable things from other security teams and have this monitoring constantly the. The main path and the official path for the sincerity and so teams for incident response is this, the one that goes through a bucket and feeds RCM so they can interact. And the other, the other path is serves a a purpose of visibility and also that we can be promptly wary to have any assistance for the security teams and the production teams invited in the incident. Beyond the implementation of the detection. We also managed to, to get ready for specifically for 3. The whole process automated with a first response where we can be a tag-based pipeline once we dri tags uh S3 object. We managed to have a restrictive policy, so only SOC teams have interacting with the file and have the analysis. They, if they mapped uh false positives, they also update and the access is a storage. For the ECU we went from a more conservative approach because in an automated first response could lead to outage so we made available the isolation in the playbook but it's only applied once the team is involved in the incident. So to finalize our our use of guard duty. We, what we begin with uh regulator request we managed to revise our process of Mario analysis in the bank and also improve the process. We Because we use gu dirty in our already integrated environment, we cause no disruption to productive teams. We didn't break anything and we was very seamless. We also achieve a global compliance and we can roll out to any country that we intend to. This changed completely our coverage in the cloud because we didn't have this 23. And also by enabling this we managed to get to productive teams a new feature they can autonomous enable a document triage for third party files. And be more secure. The whole process was advised and enabled in less than 12 months, and we still managed to, to maintain the cost at less than $1 for to serve each camp. So, um, welcome back. So I finalize. Thanks, Mikeli. All right. So, some key takeaways from this session that we learned, the, the fully managed aspect reduces the cost of ownership for you to detect threats from malware, uh, in your workloads and more specifically, aiding in ransomware scenarios with the recently launched feature, um. And again, uh, live workloads, and you can combine now with backup scanning, uh, or for a backup workloads with a single scan engine that kind of learns from all of these different offerings to collectively provide, uh, higher efficacy and higher fidelity to malware-based threats. And complements, as Peter walked us through, it complements other offerings from guard duty by combining and correlating to actually provide visibility into the actual breed scenario, pre-exploitation, compromise, and post-exploitation. And this improves security operations with contextual information around the resources, the malware family, the, the, the scan engine that contributed, and the attacker information. And that's it, uh. Thank you for attending this session. I'd like to invite the fellow speakers to.