---
video_id: -nMiqOgQbHc
video_url: https://www.youtube.com/watch?v=-nMiqOgQbHc
is_generated: False
is_translatable: True
---

Hello, everyone. Uh, good afternoon and uh thanks for coming to our session on Amazon Nova 2 Omni, which is a new frontier in multimodal AI. I'll be joined with my two co-presenters, uh, Ashwin Swamiathan, who's the director of Applied Science, working on Amazon Nova models, as well as, uh, Yamamoto-san, who's the Chief AI Officer in Denssu Digital. For the agenda for today, uh, we're gonna look at Amazon Nova, uh, family of models overall. Then we're gonna touch upon, uh, what was, uh, launched by Matt Garman yesterday in his keynote, which is about Amazon Nova 2 family. Then we're going to deep dive into Amazon Nova, um, especially the Omni model, which is optimized for multi-modal workflows in terms of multimodal understanding, multi-modal generation. We're going to also touch upon some of the examples, demos that that will give you some idea on what are the things that you can create for your business. As well as also touch upon the performance of these models as compared to the best and the sort of models out there. And then we will hear from Yamamoto-san on how using Noah Omni Denssu Digital is transforming their business and helping their customers. So we launched Amazon Nova family of foundation models last year at Reinvent where I was here in this venue actually. And we launched a suite of models. Our primary NA models were launched in two categories. One we call NA understanding models, which is they take in text, they take in images, they take in videos, and we're able to derive metadata out of it, summarize that content. Do a Q&A to generate text. So that's the understanding family of models available in Micro Light Pro, and since then we have launched Premiere, which is the largest model with the best performance in that category. We also launched an image generation model called NoA Canvas and a video generation model called NoA Real. And then, since then, uh, we have launched a speech to speech model, which is for real-time conversational AI applications such as customer support calls, and that is called Amazon Nova Sonic. And very recently, last month, we launched industry's first natively multimodal embedding model, which is great for semantic search and agentic rag applications, especially if you have a lot of unstructured data around documents, images, videos, audio, and text. Um, and that's, that's, um, I mean, we are the pioneers in, in that space in terms of providing the best performance at a much lower latency and cost characteristics and being the first model in the industry to do everything together. Noah is already used by tens of thousands of enterprises and startup customers, and some of those names are listed here. Now, to touch upon what was launched yesterday, right, so. We are introducing 4 Amazon Nova 2 family of models, starting with our Nova 2 Lite, which are, which is our fast, cost-effective reasoning model for everyday workloads. It's our first reasoning model, uh, and it's a hybrid reasoning model, meaning that developers control whether they want reasoning to be enabled. Or and and also to what level of reasoning that they want for a particular task. So if your, if your task involves document understanding with some reasoning on whether the extracted values are incorrect or not, then you might want to use some level of thinking or reasoning with it. For everyday tasks, you can just disable that reasoning and the model will provide you results as is. Again, using less tokens and producing faster results. Then we have also launched in preview our NA 2 Pro, which is similar to the NAA 1 family. It's a higher tier that provides higher performance at a different latency and cost characteristics, and it's our most intelligent reasoning model for highly complex tasks. So think of coding tasks, uh, complex agents, a multi-agent scenario, you would want to use Nova 2 Pro there. Then the third model that we also launched in preview is our NA 2 Omni that we're going to talk about today, and it's our unified model for multimodal reasoning and for image generation. Now, we believe that models in the future will be fully multimodal, and that's how humans interact also, right? Like spoken words, visual. As well as written text. And all the providers will be moving towards that. So this is, this is our first model that can understand any modality, including audio and speech, uh, which is the first one in any of the models available on Bedrock. While also generating high quality images. Um, uh, in the same model. And it's, it's also an industry's first reasoning model that can reason across any modality and generate images in the single model. And then we have a 2nd generation of our Amazon Nova Sonic, which is the Nova 2 Sonic. Um, and it improves the performance as compared to the first generation. It supports more number of languages, and the conversations feel more natural as compared to the first generation. Um, so in this one, we're going to deep dive more on the omni model. This is just a quick overview of what are the capabilities of all of these 4 models. All these 4 models support a large context window of up to 1 million input tokens. They support more than 200 languages for the input text. And pro and Omni also understand audio, so they can understand up to 10 languages for audio slash speech as well. Um, the other thing about Sonic is, um, it supports more voices now, naturally. Uh, so for multiple languages, uh, you can choose between the different male or a female voice, uh, for your workflows. So, um, Omni, as I said before, just to recap, it can take any modality as input to generate text and images. And It feels more natural in the way that, I mean, as I mentioned, humans interact, so the models. Understanding all the content into the same space to then Uh, generate text or generate images provides higher quality outputs than a combination of multiple bespoke models that are task specific. It also means that you don't have to worry about creating complex pipelines that have multiple models and multiple systems in place, and you can unify that with a single model, reducing the build costs, maintenance costs, and faster time to market. Now these are the main properties of what Amazon Nova 2 Omni provides. As I mentioned before, this is a hybrid reasoning model where the developers can control the level of reasoning or whether they want reasoning even enabled or not. This model. is great at instruction following, tool calling, and even for simple text-based natural language tasks such as for NLP tasks, sentiment analysis, classification, those kind of things. It is a state of the art in multimodal perception. So, so this model is is highly optimized for any multi-modal tasks. Which means that input document understanding, input image understanding, video understanding, audio understanding. As well as the new feature of cross model reasoning is all uh that this model really exceeds as compared to the best of the models out there in the market right now and we're gonna show some of those numbers which we have already published on our. technical report as well. Um, audio understanding, as I mentioned before, uh, Nova 2, Pro, and Omni are the first models on Bedrock that supports audio understanding, which means that you can do speech transcription. You can do multi-speaker dialization, which is to understand which speaker is speaking at what point. Um, as well as it supports multiple languages and, and, uh, supports different types of tasks that we're going to dive deeper later. And this model um supports much higher quality image generation and natural language-based image editing. Now backed by a text LLM. It can generate much higher quality of text which is rendered within the images, especially for longer text in the images that has always been a challenge, and we we face that challenge even with Canvas. So with this model we have, we have tried to fix all the remaining gaps that we had in the previous generation of image generation model, and this, this model should be very compelling for image generation. Now to look at the performance, and this is this chart is based on artificial analysis, uh, so it's not done by by us but by a third party that shows in terms of their overall artificial analysis index which measures performance across 10+ benchmarks on how top of the line models perform, right? So 58 is a. Is a consolidated number out of all the multitude of different benchmarks that are meaningful in different categories such as instruction following, tool calling, agentic coding, etc. And as you can see in its tier, which competes with Gemini 2.5 Flash, GPT 5 Mini type of models, this model is very competitive and stands very high on the leaderboard. The ones on the left are the higher tier. Those are the Gemini 2.5 Pro, which is similar to the Nova 2 Pro category. So that's why we're not comparing there, but. Um, these are some of the numbers just to, which are, which are again already public just to give an idea in terms of the language understanding, so knowledge, reasoning, instruction following, tool calling that are critical, um, for that that test the uh how good a model is and also for tool calling for agentic applications, these are the things that matter. And as you can see here, Nova 2 Omni is very competitive on all of these numbers as well. Now, let's look at some of the use cases, uh, especially the multimodal use cases where this model improves a lot upon, uh, the NAA one family of foundation models as well as pretty competitive as compared to the other models out there. So the main use case that we have heard from our customers is document understanding. Pretty much every company has documents. Um, some of the documents are very complex in nature as well in terms of the layout, in terms of there are some handwritten documents, a mix of handwritten plus typed languages in different languages, so. Document understanding is a very complex and, and a very challenging problem, right? Um, so what this model does is it provides much higher accuracy as compared to the previous generation of our models. Um, and, uh, we're gonna show some numbers also in terms of the measurements on the public benchmarks on where we perform as well. Now, some of the key things that we have tried to fix with this is the OCR, the character recognition, as well as the key information extraction. So if you have a, if you have some PDF reports, if you have some receipts, um, then how accurately can the model extract all that information out. In some of the cases, it can also do verification with the built-in tools and expose any inconsistencies in terms of the content that is listed there as well. This is, this is an example of uh OCR where the image on the left, I mean it, it is a combination of Uh, it's, it's a very non-traditional type of a document you can think of, right? Like, uh, there are some text, there are some images in different areas, and this is one of the basic use cases that every company has, like how accurately can you extract the information out here, like do the OCR and create. Um, create this text. You can also tell the model to produce output in a structured manner like JSON, XML, etc. which you can then use in further down the down the lane in terms of uh calling different tools. Um, as you can see on the right-hand side, uh, it extracted all the information in a, in a kind of structured way. I mean, it's not displayed as the JSON here, but, um, that's, that's what the model can accurately do. Um, this is, this is again a pretty complex layout of a document on the left hand side. And um the model is able to pretty accurately extract the information out of this. Uh, as I was mentioning before, um, you can prompt the model to look for any inconsistencies in specific areas, do the calculation. So the model using the built-in tools that it has, say the code interpreter, which can do simple math calculations, etc. it can, it can tell you whether, um, the information presented in the document is correct or not and where are the inconsistencies. Now to look at the audio understanding, which is a new feature of this model. Now We've had AWS transcribed as one of the solutions for ASR or the speech recognition type of use cases. Uh, using this model, you can, you can transcribe speech, you can summarize, um, what was said in the audio file, you can do Q&A, and then you can also do everything and call tools with it, right. Uh, as I mentioned before, um, it supports up to 3 speaker dialization, which is it can segregate which speaker is speaking what part of the audio. Um, it is a very performant model. On the MMAU leaderboard, which is the massive multitask audio Understanding and Reasoning leaderboard, we are overall on #2. Um. And much ahead than the models provided by Google, OpenAI in this space. And it, it measures, uh, accuracy over speech, the soundscapes, which is non-speech audio, as well as music. So the model can understand different types of audio inputs. Let's see how it works. Uh, so in this case, um, we have a short snippet from our Q3 earnings call that was done by our CEO Andy Jesse. And uh we just tested it out to see how the model performs, um, when we provide this input or the different types of tasks that we ask the model. So in this case, this is just an internal tool where we just upload this file and then we start very simply with just a plain transcription. In, in which case we look for, did the model miss for any words? Did the model misquote any words which were not spoken in the input audio, right? And it's hard to read here, but uh we did the analysis, we did the testing ourselves, and the results was pretty accurate. Then you can ask follow up questions like what are the key takeaways, so it can provide a a a bulleted list of main items that were part of this audio file as well. And then you can ask again follow-up questions like outline the, the key accomplishments um that was mentioned by Andy. So, yeah, so you can basically Analyze any type of audio file to do these kind of tasks at a much higher accuracy, right? Yeah, and this is just a summarization as well. So, um, we're going to share the results of this later when we talk about the, the summary of the multimodal perception. Uh, with that, I would like to hand it over to my co-presenter Ashwin, to, uh, talk about the image and video understanding. Thanks, Rohit. Uh, uh, like, uh, Rohit mentioned, I'll talk a little bit about some of the work that we did as part of the NAA 2 Omni model, focused on image and medium understanding use cases, and talk, and, and also extend about the work that, how Omni can solve some of the image generation aspects of things. So if you look at the broad categories of image and video understanding use cases, we can broadly look at it in three specific areas. So one is perception and object detection. So you have a scene, you want to understand what objects are there in the scene, and you want to create bounding boxes. Second scene, the second aspect is Q&A. You want to ask questions to the image and, and figure out what is happening in the image. And the third part of it is temporal understanding. When you have videos, uh, going from images to videos, you want to capture the temporal aspect of the scene and be able to. To identify and ask questions about a particular video, like when did a particular event happen in the video, or when did, uh, uh, did something show up in the video, things like that, which you would generally have for any kind of uh uh video understanding tasks. So let's walk through some examples. So this is an example of a scene. It's a complex scene. It has a lot of components in it. Uh, if I ask a simple question like detect the number of plants, cushions, tables, and TVs on the scene. So, uh, as a human, as we're looking through the image, um, there are things that pop up immediately to you. Like there is this huge, uh, potted plant on the right, uh, right next to the right sofa. And then if you look around, you can find, uh, multiple small plants all through the different scenes. Let's look at what the model produced. So, the model not only detected the big potted plant that's behind the scene, but also all the small plants that are all over the place. So, the plants that are in the cupboard, the plants that you see in, in front of the, uh, in the center table, the one next to the TV, it also detected all the individual cushions, extracted bounding boxes for these cushions, as well as the TV which was provided in the prompt. And the second thing to note is the accuracy of the bounding boxes in terms of the locations and how tight the bounding boxes are. So these are things that we optimized as we built the Nova 2 Omnimo. We want to make sure that we are producing very high quality outputs and at the same time. Producing very accurate bounding boxes that can help you unlock different kinds of applications, whether be it automation or real-time decision making, robotics, different kinds of use cases. So we wanted to try to optimize the model to make sure it works really well for these use cases. Let's look at perception and QA. So if you look at the scene, you want to be able to identify, you can ask a bunch of questions on the scene. Uh, how many teams are on the screen? What is this event? What are they playing here? Um, has the runner, uh, transferred the baton? So if you pass this kind of uh image to the NA Omni model, it determines that this is a relay race. Uh, it has 3 teams. It has Great Britain, uh, along with Swiss teams and Belgium. And it also, also can detect the location of the batons and identify um where, uh who, who has transferred the baton and who has not, and who has come first in the race. So this entire understanding of the scene becomes extremely powerful for supporting a wide range of different kinds of apps. Applications. And this is something that we've been hearing from customers like yourself on how we can uh support these kind of complex understanding use cases, which will enable uh and reduce the human automation part of it and increase the overall uh power of AI for your workloads. A third example is on temporal and multimodal attention. So if you have, uh, want to reason across frames, you want to be able to support long, uh, event detection, identify particular events in the scene. Let's look through an example. So here, uh, I'll play the video in a minute. Um, so the prompt here is to be able to locate all the segments in the video, um, for this particular prompt. And the prompt is the man standing on a boat, and it's asking the model to produce the results in a very structured output fashion. So as we walk through this, uh, I'm just gonna pause so that you see the exact time stamps. So as you walk through this, you see, uh, different kinds of landscapes, different kinds of scenes, and around the 12th, uh, 14 2nd mark, you see the man standing next to, uh, on the boat. And as you roll the video, you see the man is now walking, but is no longer, you still see the person, but not standing next to the boat, so it's not counted as part of the model outputs. So, the model can understand and reason, um, exactly the kind of prompts the cus customer is asking for and provide outputs in a way that it can uh solve complex tasks. And in this case, um, a man standing next to a boat where, you know, it's not about just a person walking or a person standing, but also about how the, the relationship between objects and how these objects can come together towards uh answering the question that the customer is asking for. So this is the kind of complex use cases that the model on the NA 2 Omni can support. In terms of performance, as Rohat mentioned, uh, we have evaluated the model, uh, against a wide range of benchmarks. We have published a very detailed technical report on the Amazon Science website. I'd encourage all of you to, uh, go in and look at all the benchmarks in that. Uh, in the technical report. Here, I'm just pointing out a few of the, uh, benchmarks that we, um, that we propo uh looked at. On the video understanding side, we have benchmarked extensively across a wide range of use cases, including the video MME benchmark. Um, and the video MME benchmark is, uh, interesting in the sense that it can, um, it has general video understanding use cases, but it also has cross-modal understanding. So you can look at how does the video perform, uh, when you input video along with speech, and how, what is the performance improvement because of that. So we see that with, with the combination of video and speech and cross-modal understanding, our performance is much better than other models that are out in the similar category. On document understanding, we have, uh, OCR bench, uh, which is, uh, uh, uh, OCR based benchmark. And again, uh, the, the Nova Omni model, uh, is exceeds state of the art and provides very good results, uh, uh, in, in the wide range of use cases for this particular benchmark. And for speech understanding, as Rohit mentioned, on popular benchmarks like MMAU we are, uh, Number 2 in the leader board. So across the board, you see very good performance in a wide range of tasks. One other thing that we did as we built the Omni model is, uh, we learned from all the feedback that you had from our NA 1 models. And NA 1 models, as you are trying video understanding use cases, uh, you provide a lot of feedback in terms of what are the cases where the models worked and the cases where the models do not work. So, in collaboration with CMU we created a new benchmark called Mavericks, and this benchmark has also been published. Um, and we also evaluated our model based on uh real customer use cases, based on all the feedback that you, you have provided to us in the last year. Uh, in this Mavericks benchmark, again, we are, uh, among the top, uh, in terms of performance, and, uh, uh, it also helps us evaluate how video along with speech or video along with audio can together improve and provide very high quality results for real-world customer use cases. And that's something that I would also encourage all of you to check it out, uh, as part of the work that we've done on benchmarking. So now, let's talk a little bit about uh image generation. Uh, one unique aspect of Omni is it not only understands all the different kinds of content, uh, like text, images, video, audio, speech, uh, but it also can generate images. So this can help you power a lot of complex use cases where you probably use multiple models in the past. So, so NA Omni model can generate high quality realistic images such as people, text rendering, and also spatial understanding. Because we have a, a, uh, a good understanding backbone, which understands the content and images, we can now use that backbone to also generate better quality images and help us to do better quality editing tasks. And we'll see that in some of these examples. To start with, these are some examples of images that we created with the Noah 2 Omni model. This is a text 2 image part of it. So given a text prompt, it generates different kinds of images, and you see some of these images in this uh graph here, in this slide here. And as Rohit was mentioning earlier, we also optimize a lot on visual text rendering. We want to make sure that the models do well, uh, in, in, uh, producing right text outputs, which is a really hard problem for most of the image generation models that are out there today. Uh, in terms of, uh, we evaluate our models in terms of winning rate. So the way we calculated winning rate is, uh, uh, again, based on all the feedback that we've gotten from you as customers over the last year, uh, on our NA Canvas model, we curated a data set of prompts, and we did blind AB testing, comparing our model against other competition models that are out there. Uh, this is a blind AB testing. Uh, the annotators did not know, uh, which image correspond to Noah Omni and which image corresponds to the competitor, and, uh, they just looked at each of the images and provided a win, tie, or a loss rate. And so then we aggregated all the results across all the annotations. Uh, the exact process and the procedure for human evaluation is outlined in our technical report as well. Um, and then we calculated the winning rate. Winning rate essentially is, uh, a win rate plus tie by 2. So then we are looking at how well, uh, so it essentially gives you a quantification that anything above 50 means that our model is equivalent or better than the other competition models. So, looking at, uh, as you can see in the figure, Noah Omni performs, uh, significantly better than Noah Canvas. So it's a step function improvement in terms of performance compared to our last generation model that we launched last year. Um, and it also significantly out performance models like Flux ContextMax or Flux ContextPR, the, and it's comparable to GPT one, and Gemini Nanobanana kind of models. And these are more detailed results looking into specific categories, uh, specific areas that we focused on are improving rendering of people and rendering of visual text and scenes. So this is an area that we also improved as we went from Nova Canvas to Nova Omni, uh, as part of our image generation tasks. Uh, here's one complex example. So the prompt here is an image of Paris, uh, with a crowd of pedestrians all looking at the camera. So, here you're trying to generate not only a, a group of people, which makes it really hard for image generation models, because now you're generating a lot of small spaces all through the entire scene. But it further the prompt also says that they want the people to be looking at the camera. So, so here again, it's, uh, the model is able, uh, the NA Omni model is able to understand the user intent and produce images which are representative of exactly what the user is asking for in the prompt. Uh, so if you look at the scene, a lot of people on the scene, you'll see most of them, around 90% of them are actually looking at the camera, which most of the other competition models also cannot do today. And, uh, and as, as I was mentioning, the advantage of having a, a joint understanding and generation model is the understanding components of the model can learn from the generation side, and the generation components of the model can learn from the understanding side. So that helps us in tasks like editing. The NOAA Omni model can support 9 different kinds of editing operations like adding new objects, altering objects, extracting information about a specific object, replacing, removing, doing background changes, style transfer, and so on. Now I'll go through. Some examples that so that you get a sense of the kind of operations we support. And all this is just with a text prompt. So you can just provide the input image along with the text prompt, and the model understands the user intent and can help you with the different kinds of editing tasks. So here's an example of an ad operation, where you want to add a wooden bench fence facing on the right side of the image. So the model generated the bench, but it also understood the style and the layout of the scene. And as it's creating the bench, it created the, uh, the bench with the exact style of the image. So it's representative and it flows well with the scene and the scene uh layout. Uh, here's another example that you want to add a door number to the, uh, uh, to the image on the left. So you, the model understands the intent and also again, creates the image, uh, with the, uh, uh, numbers in the right location of the door. Here's an example of Alter. So, here the, uh, the prompt, uh, the customers asking for changes the color of the bicycle to red. So it understands the, uh, location of the bicycle, is able to segment out those areas, and then correspondingly convert, uh, those locations only to red color. Here's another example of uh frosting the color to different, uh, changing the frosting color to a different color. And again, the model is able to understand your intent and change things. This is an example of altering the scenes, so changing a scene from a snowy background to a more sandy kind of a background, uh, and extracting information. So, you have an information, think of Amazon.com, you have, uh, images of people wearing dresses. You want to be able to extract the dress and show the dress and represent this information very accurately, so that the user can help making the, make the purchase decisions. So that's an area that the model does well as well. Uh, replacing, uh, objects. Here, the interesting part is, uh, the user's prompt is specifically focused on keeping the hashstyle untouched. So, as you're changing the hat, you also want to make sure that you're not changing the hairstyle so it doesn't affect the overall representation of the scene. The model is able to understand that intent and also generate images where it doesn't touch the hair, but only touches the, um, the hat, so that it generates those images uh from the hat, uh and just updates those images. Here's an example of remove, uh, we're we're removing a polar bear from the scene. Uh, and again, uh, uh, removing the cell phone, uh, from the user's hand. So again, uh, this is also an example of complex editing operations where it has multiple prompts, not only removing the cell phone from the user's hand, but also changing the color of the sweater from pink to blue. It understands both aspects and generates an image that adheres to the prompt that's been described. Uh, here's examples for, uh, background change, style transfer, um, uh, and motion change. This is more about how do I represent the scene, where, like, how does the same person look in a different kind of, uh, uh, expression and be, and create those, uh, uh, expressions so that it looks realistic, while not changing the appearance of the person or changing the attributes of the person or the dress of the person. So that becomes an important aspect as well. And these are hybrid changes where you're not only removing the hat, but again, uh, changing the color of the dress that the person is wearing. So again, these are the kind of complex scenes that we've been, uh, evaluating and testing as we were building the Nova 2 Omni model. And having the capability of understanding tasks helps us also improve the overall quality of uh the generation aspect of things. Uh, we evaluated our model against the image edit benchmark, uh, and this, uh, graph shows the performance of the model. Like, uh, I was describing earlier, uh, again, on image editing tasks, we do better than Flux models and are are at par with the GPT 5, GPT 4.1, and Gemini models as well. Let's look through, walk through uh an example here. We have, uh, different kinds of editing operations that you can do with, uh, with the Nova models. So we want to transform the bytes, uh, the bright Scandinavian style to white colors. Adding a muted minimalistic art. So you can use it for rearranging and creating your own, uh, environments. If you're a property manager, if you're, uh, so you can extend it to similar kind of scenarios and similar kind of use cases for your workloads. And if you wanna create ads, um, you have the object, you can create captions, you can place the object in different kinds of environments, and create different kinds of ad marketing advert uh assets, uh, for your, uh, workloads as well. So we're seeing a lot of customers use the models right now and different beta as part of our beta program. Monks is one such company. Uh, one of the advantages of the Omni model is it can, given the power and given the capabilities of understanding and generation, it can, uh, you don't need to have multiple models to be able to solve the same task, and the model itself can understand and represent the task and also help generate content. So which becomes more powerful and So companies are able to see that they now don't have to have a very powerful, complicated workflow, but they can simplify a lot of the workflows by the power of the omni model. Uh, we have, uh, with that, I'll hand over to Yaomoto-san, who will talk a little also about how DNU is using some of the models for their use cases and workloads. Hey, thank you. Haha, gonna need to, uh, hello, hello, thank you so much. Uh thank you for the introduction and thank you for the great opportunity. Uh, let me introduce myself. I'm Sato Yamamoto, Chief Aerosa at the Dance Digital, and then I will be the also Chief AI, uh, Deputy Chief Aer of Dentsu Japan, and then. Uh, I wanna say, uh, thank you. I really appreciate all the support from AWS. I have a lot of opportunity to present in a lot of events in, uh, related to AWS like, uh, reinvent and, uh, Summit and also Canon Lions. Uh, so I'm quite a big fan of AWS. So today, I really wanna, uh, tell you how AWS AI is powerful, especially Amazon, Nova Omani, right. And then, uh, let me, uh, but before I go to the main topics, let me quite briefly introduce our company, uh, Bentu. Uh, Bentu is, uh, Asia's uh, largest advertising agency, and we are operating our business globally, uh, more than 100 countries, uh, now with over, uh, 7000 employees. Oh, I have to say one thing. I need to say, thank you. It was a good news for me. Uh, thank you so much, uh, because I know, uh, yesterday's keynote session, uh, Matagama CEOs, uh, mentioned, uh, then to also marketing giant. No, no, no, no, we are not so tall as giant. We are just number one in Asia. Thank you so much, Matagama. Like this. By the way, by the way, uh, so we have the solution. Um, named, uh, Mugen AI, and, uh, and this is a kind of a digital marketing uh solution. Out here we're utilizing, uh, AWS AI service a lot. So today I wanna introduce how we're utilizing Amazon, uh, AWS AI, especially, uh, Nova Omony, uh, in this solution. OK, Let's go to our main topic, agenda. Uh, we have 3 topics, at the creative, planning operation, and next generation experience. Uh, this agenda is following the evolution of the generative AI like, uh, uh, multimodal AI and the agent AI and the physical AI. Let's go to first topics. A creation. Had creation. Uh, we have a solution named Me AIA, and, uh, it has three functions like, uh, creative generation, performance prediction, and the improvement suggestion. And thankfully, it's already implemented more than 200 companies, and we succeeded to, uh, achieve an average improvement of more than 150%. I don't, I want, why don't I emphasize. Here is we are utilizing Amazon Nova to make this solution to conversational in order to make our employee um uh to use it as much as possible. But still we have a challenge. It is, uh, video creative, video creative. Uh, for example, uh, we couldn't get the accurate video prediction, but, uh, we can, uh, uh, say that. Uh, we can change the situation with the Amazon Nova Omani. OK, let me explain. Uh, and then traditional approach, what we are doing is, uh, we are changing the original video, uh, to the like more simple d, fragmented information like, uh, uh, keyframe, and, uh, also we, we need to convert keyframe to the, uh, textual summary, and also we need to Uh, gets the information from a sound like this. Everything is separated, but it's really different from how humans watching a video. So, uh, as a result, accuracy is not so high, but on the other hand, on the other hand, Amazon Nova 2 Omni is a free genuine multimodal AI so it can understand the video itself like us. So as a result, we can get a quite high accurate pre uh pre prediction. And also, um, in a traditional way, uh what we need is a quite enormous amount of data. Uh, but, um, because Nobel Oman is already learning, uh, the vast knowledge, uh, background in the back, in his background, so, uh, we don't need such amount of data, we just prepare, uh, the small set of data, uh, with the, uh, high quality data. Right. Let me show you the demo here. What we should do is, 000, come on. Yeah, it's working. Uh, what we should do is just upload the uh video like this, and then also we should select the, uh, how to say the advertisement campaign and then to prepare the training data for the prediction model. And that model will tell us, uh, uh, what, what kind of uh video creative, how's the uh higher performance, and like, like this. And then finally, uh, we can get the predicted value of the uploaded, uh, uploaded. Video. And I, what I emphasize here is uh the uh correlation value uh between the, uh, predicted value and the actual distribution value is quite high, like 0.88. So, uh, we can really um accurately conduct the prediction. Uh, I forgot to say, oh, by the way, this example is from the uh ANA. It's a Japanese largest airline company. I'm here using that airplane. So thank you so much for providing the example. By the way, Uh, not just, uh, utilizing uh Nova Omony for prediction, but we can also use it for, uh, generation, uh, because, uh, Nova Omony is the uh free uh multimodal. So we prepare the brain which can understand the tendency of the which video has a high performance, and then use this brain to generate a storyboard. Because it's uh has a it's a multimodal, it can also draw a picture. So let's check the uh demo like this. So uh we we just ask uh the same AI. OK, now, let's make a storyboard. First, we can get the uh textual storyboard. Uh, we can convert this storyboard, to the visual storyboard, and based on that storyboard, we can quite easily generate uh the uh moving video. Like this. And here I want to emphasize two things. Uh, it's related to the capability of image generation of Omony. Uh, one is that Ommon's listening power is really high, so it's quite easy, uh, to handle and edit the, uh, images with natural language. A second one. One is quality itself. Uh, it's more, it's a kind of competitive to the leading model and also I'm really quite surprised because we can generate Japanese characters really accurately and beautifully. It's almost first time for us. It's really high quality. OK, uh, not just, uh, generating the storyboard and video from scratch. Also, we can, uh, reuse the existing video because of the, uh, the omni is understanding, uh, which part is important. So, uh, let's check the demo again, and it's a bit tired. Instead of me, please listen to. Today we demonstrate how AI accelerates content creation. First, we upload a raw video file. Nova Omni instantly analyzes the footage, generating a detailed metadata of every scene. Next, it acts as a creative director, drafting a viral strategy with specific hooks and scripts. Finally, leveraging the AI's deep understanding of the interview and visuals, we efficiently craft the final video edit. From raw footage to viral ready, streamlined by intelligent analysis. So, like this, using our own money, we can quite easily handle the complex data like video. OK, let's go to the next topic, it's planning and operation. Uh, we have a solution, uh, named, uh, Mugen, 00, sorry, the character is, uh, the latest but it's OK. Uh, we, uh, solution, uh, named the Mugen AI agent, uh, Agent Canvas. It's a kind of a citizen development, uh, solution for non-engineer to build up their agent quite easily. And we already, uh, succeeded to, uh, develop the, uh, solution by marketers like, uh. Strategic planning and the journey planning and the media planning, but we can go more when we utilize Nova Oman for agent. I think, I think this part is interesting. OK. What we did is we tried to uh omini uh make uh mimic human. So, uh, the video left side is uh the human actually operating the UI of the Amazon. As automation UI and then we input this video to Nova Omony, and then Novam understood the workflow and the writes the prompt for NovaAct. NovaAct is the AI which can handle the browser's, uh, browser's operation. So as you can see that we can free automate uh this process uh without uh any kind of human touch, right. And, uh, these, these two pages are not directly related to Omony, uh, but it's a quite nice example. So let me introduce that. So, so far, I uh, I introduced the AI solution and then to Japan, like, uh, uh, creative, uh, prediction and generation as a planning and operation. What we, what we're gonna do is we try to combine every whole solution together in 11 AI agent, utilizing the bedrock agent. Multi-agent collaboration of functionality. OK, let's check the demo. What we're gonna do is, for, uh, sorry, it, it is written in Japanese, but I speak in English. So, OK, let's, uh, make the new product, the sparkling water. How about the, uh, shining sparkling water? Then let's talk to the, uh, AI persona, and they'll ask persona, uh, how do you think about that? And if, uh, uh, taste itself is not so important, if the visual is nice, it's OK. OK, so let's summarize the. Uh, interview, and based on interview, we can, uh, consider the name and the key messages and uh also the key visuals for that. And then I'll summarize this one as the, uh, value proposition. And based on that proportion, uh, we can, uh, prepare the customer journey and for acquisition part, uh, we need the digital banner generation blah blah blah blah. So what I wanna say is now we have already Applied Amazon Nova AI leads for our solution, and we can go, uh, we can, uh, prepare, we can go more uh with the Amazon Nova Omany and we combine them together with Bedrock agent. OK. So, so far I think, uh, so far I explained is also the kind of advanced way of utilizing AI for marketing agent. Uh, but let's, uh, the final topic, let's talk about the next. Generation experience. OK. And it is more like, uh, conversational uh interactive and real and physical. Uh, the first one, first example is, uh, using a chat. Uh, with this solution, we can quite easily prepare uh client chatbot uh utilizing client specific data. Now, of course, it's based on AWS and utilizing Amazon Nova Cities, uh, like this, this one is uh Uh, example of Gulf Coast reservation. But the problem is it's just text to text conversation. It's quite ordinal, but utilizing uh Nova Omony, we can provide more, uh, uh, richer experience like this. He's my friend and he's talking to AI with voice. Oh. I've analyzed your video. Tap the mic to talk. Yeah, first, uh, she's speaking in English and then she, uh, he, he inputs information with Japanese like, uh, it's very interesting. Uh, only speaking in, uh, English and then he's speaking in Japanese, but the conversation is still not broken and here we can also upload the video. AI so we can understand this our video and I can give us advice how to improve the golf swing like this. It's a kind of the, uh, new generation experience uh which is related to real world. And there's uh more example. Uh, this one is, uh, kind of the uh in-store experience. What, what we're doing on the left side video is the, uh, the video from the customer, customer. View, uh, in the store, and, uh, this customer is watching the perfumes and they're waiting for a second, then they, they, oh, maybe she, she's, uh, this customer is talking to, uh, the shop clerk like this da da da da da da and then what we're doing is we are extracting persona of this customers. For example, maybe we can guess from this video, um, this, uh, this customer might be a lady. And also, uh, sees a kind of the gorgeous and uh also the, uh, how to say the style conscious uh office worker like this, we can get uh we can extract the persona. And also we can use this persona. To digital world again. I think it's kind of an interesting example. What we did is uh we use, we use uh this Persona uh to Amazon Nova A. So, um, based on the in-store, how to say that, uh, in, in-store behavior we can guess how this, uh, customer will behave in the AC shop like this. And I think this kind of technology, um, you know, which connects the real store and the, uh, AC store. It is very essential for the era of the VR and AR glasses widely spreaded. OK, so this is almost the end of the presentation. Uh, today, I have presented several applications, uh, based on Amazon Nova to Omani, but I know this is not a marketing session. This is technology session. So, uh, I'll inform you very shocking news. Actually, uh, I am informed that I have a presentation today. Uh, last month, last Tuesday, and we have just 7 days to prepare for presentation. So, but we, uh, uh, I, I introduced the 7 applications. So 1 application, one day, it's a quite rapid development. Actually, uh, the lady sitting here, Suzuki-san develops this solution, uh, by her own alone. So compared to before, the development condition is quite changing. Uh, one It, uh, maybe it takes, uh, it took at least 3 months for one solution, but it's just 1 day, 1 day. So, uh, now we can quite focus on, uh, just, uh, our own business, but I think it's quite the right way of working together with AI. So this is almost, uh, final comment from me. So, uh, thanks to, thanks to AWS AI we are quite free from, uh, concern of development. And then as a marketer, we can focus on, uh, inspiring people's heart and also creating new values and also changing the world. Thank you very much. Finish.