---
video_id: VU50A5rEwDw
video_url: https://www.youtube.com/watch?v=VU50A5rEwDw
is_generated: False
is_translatable: True
summary: "This session presents a compelling philosophical and practical argument for the future of software engineering in an era dominated by Generative AI. Clinton Herget, Field CTO at Snyk, challenges the prevailing narrative that AI will make human developers obsolete. Instead, he frames software development as a \"craft industry\"—a concept borrowed from physicist Freeman Dyson—that will evolve rather than disappear. Herget uses a strong historical analogy of the automotive industry: when Henry Ford introduced automation, automotive engineers did not vanish; instead, their role shifted from hand-crafting individual cars to managing the immense complexity of the assembly line and maintaining the machines. Similarly, as AI commoditizes the actual generation of code, the role of the software engineer must shift from being a scribe of syntax to a guardian of reliability and trust, ensuring that the systems created by AI are robust and secure.

Herget introduces the concept of the \"AI Sorting Hat,\" which he predicts will bifurcate the developer workforce into two distinct paths. \"App Creators\" will move up the stack, leveraging natural language prompting and agentic tools to rapidly build applications without needing deep knowledge of the underlying code, prioritizing speed and feature delivery. Conversely, \"Foundational Engineers\" will move down the stack, becoming akin to platform engineers who manage the \"machinery\" of AI—handling the infrastructure, security, and complex systems that support the App Creators. He argues that while AI can generate code at incredible speeds, it fundamentally lacks understanding and accountability. Being \"stochastic parrots,\" Large Language Models (LLMs) are non-deterministic and prone to hallucinations—what he poetically describes as the \"vapor of nuance.\" Because AI cannot be held accountable for risk, the human engineer's primary value proposition shifts to providing \"AI Trust\"—the unique ability to verify, secure, and take responsibility for the software's behavior in a chaotic environment.

To enable this shift, Herget proposes a new \"AI Trust Platform\" to replace legacy application security tools, which are ill-equipped for the non-deterministic nature of AI. This platform consists of three interconnected engines. The **Fact Engine** focuses on \"knowing the knowable,\" utilizing Software Composition Analysis (SCA) and Software Bill of Materials (SBOMs) to track every dependency and asset. The **Flow Engine** models data behavior through static analysis to identify risky patterns and toxic data flows before deployment. Finally, the **Threat Engine** performs dynamic analysis, red teaming, and fuzzing to test the system against real-world attacks and chaotic inputs. These engines work cyclically to inject \"understanding in the loop,\" allowing engineers to validate AI outputs without becoming bottlenecks. Herget also emphasizes that for this new paradigm to work, Developer Experience (DevX) must be optimized around three pillars: maintaining \"flow state\" by minimizing interruptions, reducing \"cognitive load\" by aggregating context, and shortening \"feedback loops.\" The session concludes by redefining the engineer's core responsibility not as producing lines of code, but as \"condensing fact from the vapor of nuance,\" ensuring that despite the chaos introduced by AI agents, software remains secure, maintainable, and trustworthy."
keywords:
  - Software Craft
  - AI Trust
  - Snyk
  - Developer Experience
  - Agentic AI
  - AI Security
  - Platform Engineering
  - Stochastic Parrots
---

Sneak, uh, which is pronounced like a pair of sneakers or sneaking around. It's also an acronym. It stands for so now you know. Hopefully at the end of the next, you know, 58 minutes and 43 seconds we'll know a little bit more about the future of software craft in the age of AI. Now I do have to apologize for some of the fonts on these slides. It's 2025. We can vibe code entire production enterprise applications with a simple natural language prompt, but we still cannot reliably make fonts portable between different, uh, slideware pieces of software. I take a lot of comfort in that though, and that's actually, uh, one example of one of the concepts we're gonna talk about, uh, in the, uh, in the presentation today. Uh, so we at Sneak, we are a developer, security and AI trust company. Um, that's something we're gonna talk a little bit about as well. What is this concept of AI trust? Why is it important? And how, why and how does that matter to the people whose job it is to build software for a living? Now the title of this talk is actually very intentional and the one word that I wanted to kind of center it around is one that we don't normally associate uh with the act of building software, and that's the word craft. As a matter of fact, this kind of whole idea came from a quote that I picked up from, you know, somebody that that I've read pretty widely, and that is the physicist Freeman Dyson, who said, and this is probably 25 or 30 years ago, software is a craft industry. And the craft of writing software will never become obsolete. And I thought that was so interesting, right, because you listen to these non-technical business types and all they want to talk about is how software writing, software building as a profession is absolutely going to become obsolete, right? They cannot wait to reduce engineering headcount. And I wanted to push back against this. A little bit if you're not aware of Freeman Dyson, by the way, he's the guy who invented the idea of the Dyson sphere, uh, any Star Trek nerds in the room, yeah, the single best episode of Star Trek The Next Generation, Relics is based on this idea. I highly recommend, uh, you watch it. James Doohan's finest moment as an actor, by the way. What do we mean by craft? Craft is really interesting, especially when we juxtapose it with the idea of industry, right? We think about craft beer, right? What a, what a craft beer nerds really appreciate? Well, first of all, they appreciate that that beer is made in small batches. It's not associated with large brands or with multinational corporations, but most importantly, it's not associated with large amounts of automation, right? There's a human element to craft. As a matter of fact, we make this part of a larger world when we talk about craft. people, right? A craftsperson is an artisan, somebody who mixes their labor with certain kinds of technology, but in a unique way that can't be replicated or can't be replaced by more automation. Now you can do that to a certain extent, right? You can certainly make Bud Light or Coors or whatever your, uh, you know, preferred industrial beer is, but somebody who likes craft beer is never going to decide to to drink that as an option. So what is it about craft, uh, that's interesting. Well, I would say it's certainly the human element, but it has something to do with the identity of the craftsperson, of the artisan who is applying a unique set of skills that you can't potentially replace by the act of any kind of machinery. So we are gonna talk about AI eventually. I'm sorry about that, um, but I actually wanna go way back in time. I wanna think about what did it mean to be an engineer 100+ years ago, and I think a lot about this guy. Uh, what I'm gonna call him, I don't know his name. I probably didn't even pay for the rights to this photo, so hopefully this isn't being recorded or shared anywhere. Um, this guy is an automotive engineer, right? Now if we think about the way that cars were constructed in the early part of the 20th century, you had guys like this who are automotive engineers that built each automobile with their hands from scratch, right? They probably fabricated the sheet metal. Uh, they had specialized tools that allowed them to construct an engine block, right. They knew every part of the process. This guy is an artisan, he's a craftsperson. Now, do we still build cars, cars this way? Of course not, right? What happened was famously Henry Ford came along and he said, I'm gonna add automation to this process. I'm gonna drastically speed up the act of building an automobile. So this guy had a choice to make, right? All of a sudden there were machines that could do some portion of his job, do it faster and do it to a degree of reliability that allowed the modern automotive industry to take off. Now, did he lose his job? Because of course we know the same business leader types were salivating over the idea that we can drastically reduce the amount of labor it takes to build an automobile. And to some extent that happened. But the interesting thing is that the corollary, could we reduce the headcount of engineers involved in that process? Well, the answer was no. So what actually happened? We had massive amounts of automation introduced, right? We had factories, we had assembly lines. So all of a sudden the skill set of knowing how to build a car from scratch mattered less or it mattered in a different way. So let's think about this transition, right. We had this pre-automation approach where the craft engineer was sort of the only game in town. You built these bespoke automobiles, one at a time by hand and you need a large set of skills to do it. Automation comes along and again changes the game. Now all of a sudden the job of that engineer is not to exercise every single one of those skills, but they still need to have them, because their job is now to manage the complexity associated with the new automation resulting from the assembly line. So the moral of the story is, of course, engineering headcount didn't drop after the introduction of automation. As a matter of fact, there are more automotive engineers in the world today, many times more than there ever were a century ago. Today we call them mechanics. So what does a mechanic do, right? A mechanic manages the complexity associated with the automated production of automobiles. The engines roll off the assembly line, but that doesn't mean we don't need a human to understand how they work because eventually those engines are going to break down. This is the metaphor that I like to think about when considering this question of what is the future of software development. We didn't move from a world in which we had craft engineers building cars and then all of a sudden every aspect of building and maintaining automobiles could be conducted by machines. As a matter of fact, the craft became even more important precisely because there were so many more cars on the roads that needed to be maintained over time. And you needed to have a human engineering skill set to understand the complexity that was introduced by that kind of automation. So let's keep this example in mind as we walk through how this applies to the current world of AI and software development. What we can learn is that new technologies don't make engineers obsolete. Now again, this is something that the business leader community of the world doesn't necessarily like to understand, right? They don't want to look at the history of technology, which always proves this idea wrong. But what new technologies do is they do change the nature of the craft. They change the relationship between the engineer and the technology that they're working on. And in particular, they changed the way those engineers take pride in their work, and instead of this pride of creation that comes from the idea that, well I built this car with my own two hands and I know every step and every process along the way, they had to shift that mindset to want of taking pride in maintenance. In understanding how every part of that vehicle worked because eventually it was going to break down, because no machine on any factory floor in the world can build an engine block using completely automated technology that is guaranteed to never have a fault, that never requires troubleshooting and bug fixing, and all of these activities that we associate with human engineers. So how can we apply this to the way we think about the, uh, you know, software development landscape in 2025. Now we've had AI code generation, code completion tools, you know, for a while now. We've been talking about how this is massively disruptive to the software developer community, and I don't disagree, right? It almost certainly is. I would consider it kind of the second great revolution that I've experienced, uh, in my career. I started building software in the late 1990s. Now I get to talk about building software for a living, which is a lot easier. Ah, 15, maybe 20 years ago, the way software developers interacted with code was much different, right? You had a lot more proprietary code being written. You had fewer open source dependencies. There was no such thing as containerization or DevOps or cloud or infrastructure as code or anything that anyone at this conference is talking about. You deployed software every few months rather than every few seconds or a few minutes. You did that via very manual processes that were extremely error prone, and we had a revolution as a result. We now have continuous integration, right? We now have all these various ways in which the software life cycle is much more reliable. So we're now poised for this, you know, 2nd revolution of introducing AI, not only using code completion tools, coding assistance, but full-on agentic coding environments like Amazon Kiiro, which we're going to mention here in a moment, um, and that is something that can be supported by something like an MCP framework for agentic AI to introduce the idea of security and trust into that process. But the point here is that this is not the first time that software engineering has been disrupted. As a matter of fact, the further back in time you go, all you see is more and more disruption. Uh, I'm not old enough to remember when software was written on punched cards, but the idea that you could represent the 1s and zeros that were, you know, indicated by chads on a physical piece of paper instead by characters on a screen, on a CRT monitor that. Be manipulated by typing at a QWERTY keyboard was a massive revolution in software development. It just occurred in the 1950s, so we don't tend to think of what came before as meaningfully software at all. It sort of recedes into the past in our in our imagining of this of this world. But it's all been one sort of fluid process of continued disruption. The act of being an engineer is to never get comfortable with the kinds of technologies we're interacting with because something will come along to change them. So we moved from punch cards to assembly language where we could manipulate bits ah using keywords that represented at least something that a a a programmer could recognize, as opposed to having to feed in these punch cards into a mainframe. Then we had higher and higher level languages. We had Java, right? We had object orientation. Then we moved forward into this cloud and DevOps and containerized world we know today. But at no point did the number of engineers decrease. They've always increased throughout the history of technology. So why is that? Let's think a little bit about. This idea of complexity and how every time we introduce automation, every time we abstract away from the technology that's being used today, every time we eliminate the undifferentiated heavy lifting, if you've heard that phrase before at AWS reinvent, what do we do? Well, we actually just make more things to lift. We create more complexity through the effort to manage the existing layers of complexity. So every new layer of technology we add onto this cake introduces more potentially unpredictable behavior that then requires what? Trained engineers to manage. So, the, the idea that we can ever reach a point in which we eliminate engineering headcount is fundamentally flawed if you look at the broad sweep of the history of technology. The definition of risk in technology as a matter of fact, is the existence of unseen complexity. If I can visualize every single line of code in my application. And I know exactly which dependency each of those refers to, and I can compile in my head into bytecode how that will interact with instructions to a CPU and know exactly in a deterministic way what will happen as a result. There's no complexity there. That creates risk. But the moment I abstract away from that, the moment I say I'm going to build a higher level language, I'm going to write a framework to handle this work that I've been doing manually, I introduce the possibility of breakage, right. I then require a human engineer to potentially troubleshoot that issue to fix what went wrong. So we have this idea on the one hand of yes, increased efficiency. We're not arguing here that we should go back to punched cards, but we are saying that that is not correlated with a corresponding reduction in the amount of engineering talent that's required to engage with a particular problem space because we have more unpredictability due to the additional layers of complexity that are inevitably introduced when we when we create abstractions and automation to solve problems. The way I like to think about this is that when a machine replaces your work, you can do one of two things. You can either learn to use the machine to increase your own productivity, or you can learn to fix the machine when it breaks. Now, the second group ends up getting paid more, and this is true again throughout the history of technological innovation. Because if we go back to again that idea of the assembly line, we take our automotive engineer who's about to get crushed by that Model T, uh, and we say, look, you want to continue providing value in your engineering profession, you can do one of two things. You can either produce more cars by going onto the factory floor, learning one particular part of that process, and embracing the the efficiency that's introduced by this new layer of automation, or. You can get even a little bit smarter than that and you can say that fancy new machine they just introduced, I bet eventually that's going to break down and they're going to need someone to fix it. So what am I going to do? I can either move up the stack in terms of embracing the new automation or I can move down the stack and I can learn to manage the complexity associated with the introduction of that new technology. Let's think a little bit more about how this applies to the AI accelerated world of software development today. We can think about this as a spec driven development or as vibe coding or as prompt engineering, and I hate all of these terms, by the way, because fundamentally I think they are. Distraction from the core skill set of software engineering that is in even higher demand than it was previously. But what's happening is the same version of the choice that was given to that automotive engineer 100 years ago is now being given to everyone who builds software for a living today. I call this the AI sorting hat. Um, well, you know, some people call it the AI sorting hat, we'll leave it at that. But we put today's developers in one side of the hat and they come out in one of two directions. On the one hand, we have what I'll call AI powered app creators. These are the people who want to move up the stack. They are the ones who say, I want to interact primarily with this new generation of AI tools because it's gonna drastically increase my productivity. That's what I have every incentive to do, right, is to build more software faster utilizing these new tools, using not only Uh, code completion, coding assistance, but full-on agentic coding environments like Kiro. So my language now that I have to master, it's not Python, it's not C++ or JavaScript, it's natural language, it's prompting. I'm learning to use the machine. So my interaction surface is no longer an IDE, it's no longer the command line, it is the AI chatbot, the AI engine, um, that is then translating those prompts into ideally usable code that satisfies my use cases. This means I can churn out new functionality very quickly, right? I don't think anyone is uh more in a position to doubt that. Your, uh, especially if your first introduction to, uh, AI power coding, it feels kind of magical, right? You can say in a few prompts generate an enterprise grade application to satisfy these use cases and it creates something that more often than not works pretty well. So there is a sense of drastically increased efficiency through embracing this movement up the stack. However, this comes at a cost. What this new generation of app creators that are primarily interacting with AI and you'll notice that I don't use the term developers here, because I think we're in a world now in which the ability to build software has fundamentally been decoupled from the skill set of writing code. And again, this is nothing new. We go back in time 50, 75 years. The skill set of building software was fundamentally associated with operating a punch card machine attached to a mainframe. That changed. We decoupled the skill set from the value, and then there was something new that engineers had to learn. They needed to learn to type assembly language on a keyboard attached to a cathode ray tube monitor. So we're in the middle of that same kind of transition now. The downside is just as that developer who was learning assembly language for the first time could no longer directly verify that the ones and zeros they were manipulating with symbols on the screen accurately represented the bits that they otherwise would have been able to see with their own eyes on a piece of cardboard. So we're in a similar situation now because the abstraction introduced by the new automation. Fundamentally obscures what is happening in the background. And if eventually I lose the skill set of being able to read what's on the punched card, I simply have to trust that the automation is doing its job. But that means that I can't individually ensure the quality or the security of what's being produced. I'm at the mercy of that new machine. So I can increase my efficiency, but because I am moving, I'm transitioning in my skill set, I now have a gap. I now require another kind of engineer, somebody who focuses down the stack. If I have a bug in the translation. layer between the assembly language code that I wrote and what ends up on that punched card or the binary byte code that's being sent to the mainframe, I no longer have the ability to troubleshoot it. I have to go to the person who is writing the compiler to be able to say there's an issue here and I need your help with it. So that's the second group of engineers. So today's developers can either decide to move up the stack, they can move into this prompt engineering world which no longer requires them to understand the code that's being produced, or they can move down the stack. And I would. Call this role a natural evolution of the concept of the platform engineer which itself grew out of the maybe older concept of a DevOps engineer, somebody whose job it is to build the developer experience for the rest of the organization. I've also been using the term foundational engineer here as well, because I think that really gets at the idea that in order to allow those AI powered app creators, the, the, the prompt engineers, the software builders to do their best work, they need a foundation to stand on, right? You can't just type your AI prompts into your command line and expect that that's going to result in useful software. You need a layer in between. You need an enterprise grade suite of AI technology to be able to turn that into useful software for your organization. So these foundational engineers provide and support that AI powered platform, the developer experience, or really the software builder experience for the organization. They interact with the quote unquote bare metal, and of course that's an abstraction of an abstraction at this point, but they're the ones who understand how the inputs into that new automation layer ultimately result into outputs to the next layer of the stack. So they are the ones that the AI powered app creators can turn to to say something went wrong here. Something maybe in the system prompt of the agentic AI development environment that I'm using misinterpreted a prompt. I have a hallucination that I need to track down, but that is no longer in my skill set because I've been decoupled from the understanding of the code that these tools are producing. So that means that the foundational engineers have to understand the complexity of the system that has just been increased, right, by the introduction of this new automation. They primarily have to care not about the functionality, right, so they are not in the business of putting more, you know, use case tickets into production, but they are in the business of ensuring that process flows as smoothly as possible. So their new value is precisely in understanding maintainability and security and long-term management of technical debt, which otherwise those AI powered app creators have no visibility into. So, precisely as our automotive engineer had a choice to make, to either use the new machine or learn how to fix it, today's developers are in the same situation. Do I move up the stack and become a spec-driven developer, a vibe coder, or do I move down it and become a platform engineer who needs to understand all of the ways in which AI can potentially go wrong? And it turns out there are many. Let's talk about that for a minute. AI is a machine, uh, it's not magical, it is a pattern recognition algorithm trained on essentially humanity's entire output of content. But it's a particularly noisy and error-prone machine. Uh, I, I've really taken to using the phrase stochastic parrots to describe large language models because I think it removes us from the sense of any actual intelligence, any human characteristic being involved in what these technologies can produce. They're very, very good at mimicking existing patterns in their training data set. That's what they're designed to do. You throw enough GPU cycles and enough electricity at some of these models and you get really extraordinary results. But again, this comes at a cost. What they are very bad at is understanding why they produced a particular output, right? This is the explainability problem in AI. But it's that very explainability problem that requires human understanding to be in the loop when we think about the value of the engineer and how that's evolving. We think about hallucinations, we think about the non-deterministic nature of machine learning models, because inherently they're black boxes. What's inside the black box? A whole lot of emergent complexity that can't be seen from the outside. I often think of the metaphor of a black hole out in space. We can't actually see what is happening at the singularity in the center. The only thing we have access to is the event horizon, the two dimensional surface that surrounds the black hole. So the way to test, to do any kind of risk assessment against an LLM or against any kind of model is not to analyze the code statically as we would have in the past because there's nothing to analyze, right? It's a black box full of complexity, full of parameters, which are numbers that are fundamentally already compiled, right? We can't decompile that into anything humans can understand. So when we think about non-determinism, we think about the idea that I can use the utilus the same prompt against an LLM two different times and get two different responses. So, this is what creates noise, this is what creates errors within this machine that require this new group of engineers to manage that complexity. The third thing that happens here is that agentic AI, and by the way, there's no such thing as an AI agent. I don't know if you've heard otherwise this week or not. We think about agentic AI, we're really just thinking about stacking up multiple requests to a machine learning model with a persistent context window in between them. But what that means is we're now compounding the non-determinism problem, right, because we're taking the results of one query and we're using it as input for the next one. So the only thing that AI agents do is they potentially inject more chaos, more complexity, more uncertainty into every action of software. I'm particularly interested here because there is a movement afoot to say, well, what if agentic AI is the new API. What if instead of software communicating with itself through REST APIs and JSON, instead we're using natural language, right? We've got uh an LLM layer on top of every service in our organization, which means that we don't have to think about how The schemas are in common. We'll let the LLM figure that out and again. That can drastically increase efficiency, right? How much time do we spend making sure that JSON output of one service is compatible with the input of another and that the curly braces all line up. I mean, raise your hand if you've lost time to that problem in your career, right? So, the natural instinct is to say we can use AI to solve this problem. We can allow natural language to become the new interaction standard, not just between humans and software, but between different kinds of pieces of software, different services in our stack. But that means that we're injecting this complexity into every request, right? So we're compounding the existing problem. So I bring this back to the two fundamental rules of computer science. First of all, garbage in equals garbage out. That hasn't changed. But the second rule is almost everything is garbage, and this is exactly what we're seeing when we think about AI slop, right, and the um uh idea that we're eventually going to end up in a world potentially of model collapse because we are now training models on output of either themselves or other LLMs. So we have all of this noise, we have all of these potential errors, we have all of this chaos in the system. And again this isn't particularly unique to AI. As a matter of fact, there is a whole school of mathematical thinking around this idea of nondeterministic systems around turbulence, around chaos, and that's where I think about this guy. Ian Malcolm, the world's favorite dinosaur snack, he said in that movie Jurassic Park, tiny variations in complex systems lead inevitably to unpredictable results, which is I think the best uh definition, bite-size definition of chaos theory in mathematics. We can't accurately predict turbulence, that is in the definition of the idea. Now we can model it, right? But you can't deterministically say that a particular input is going to result in a particular output. And this is the situation we're in now, right? We spent so much time arguing about whether or not we could, we never thought about whether we should. But this is the world. So, we are in a naturally chaotic universe in which we are layering a non-deterministic requests on top of each other. And we're also asking for some level of reliability in the software that is produced by that combination of non-deter deterministic requests, generating emergent complexity within every piece of software, you know, essentially in existence. So I don't know about you, but I'm a little scared by that. So how should we think about the role then of human engineers in managing this phenomenon? Who watches the Watchmen? Because as we've just seen, we're talking through agentic AI, it's fundamentally impossible to solve the LLM uncertainty problem with more LLMs. Now there's an ongoing debate about the idea of a judge model that can sit between other kinds of models and make these sort of value-based determinations. But again, if we go back to the mathematical. idea here we still can't predict that turbulence, right? So for a given business case we're now left with what is our probabilistic acceptance of a hallucination, right? We have to radically rethink the way we construct concepts like security away from a deterministic response and into a much more probabilistic one. So we have to use the right tool for the job and I think until recently within the past couple of years in the AI discourse, you've seen this phrase human in the loop, right? Well, unfortunately it can't always be a human in the loop because then you sacrifice the vast majority of the efficiency that you're gaining from using AI in the first place. If you are. Using AI to generate your code, but then putting a human at the pull request to do all of your code reviews, you still have a bottleneck, right? So we can't simply say that AI is useful as long as we're always checking it and we're, you know, sacrificing 80 80% of the efficiency we would otherwise gain for it. But way more important than that is the idea that what you actually want is understanding in the loop. What is understanding? Well, it certainly can come from a human, but maybe it doesn't have to exclusively come from a human, and that's where I think there's some potential uh room to, to think about how does the role of the human engineer evolve in this new world. So potentially other kinds of AI have a role to play here. As a matter of fact, when we think about LLMs, we think about machine learning in general, in particular supervised learning, um, what you realize is this is actually just one kind of very small, uh, descendant of the overall history of AI development, which actually does go back to the 1950s when you think about the perceptron and the original idea of the neural network, right? There's an entire other school of thought which is around symbolic AI which is sort of the inverse of supervised learning. Rather than starting with a bunch of data and extracting the rules from it, you create a set of symbols and you begin to build intelligent judgments around that symbolic vocabulary. And that can backstop some of the chaos that's introduced by this sort of semantic world in which we're relying on LLMs and the various weights of words within a stream of language, whether that's code or poetry or English or anything else. So this is actually great news for engineers, right? We have potential tools in our toolkit that we can utilize to inject understanding in the loop without saying we have to babysit every single pull request on every piece of software in our estate to ensure against hallucinations and report up to the board that we're not actually getting much efficiency out of our AI. But it does mean we need a new way to think about the craft of building software, right? We can no longer simply assume determinism. So what does this mean for engineers? How should we think about the evolving role of understanding and how we inject it into these loops associated with our software development workflows. Well, I would suggest that the primary responsibility of every software engineer is changing. It used to be what, to write code, to produce functionality. Well, guess what, Machines are relatively good at that now. So we can no longer attach our value as engineers to this idea that we can write code faster because we can't. And we also can't necessarily attach it to the idea that we can write it with higher quality because for some subset of cases, maybe even the majority of them, AI is going to beat us there as well. As long as somebody has built a similar kind of application and that exists somewhere. The training data for the LLM to learn patterns from, we're still stuck because it is high quality code that is coming out. It satisfies the use cases and every business leader in the world is going to say, I want to do the machine rather than the human because it's much, much cheaper. So we have to have a new definition of the value we are providing as engineers, exactly as our friend the automotive engineer did. When he decided he was going to learn how to fix those machines that were producing all of these new engine blocks. So I'd like to call that AI trust as a concept that can now unify us as an engineering community. Because we are the ones that can inject understanding back into that loop. We can identify the right tools to backstop the emergent complexity that is coming out and at an increasing velocity from these AI cogenerators, from these agentic development environments. And I would say that the primary goal then for a developer is now security. Because the risk associated with that emerging complexity is precisely what our leaders of our organizations want to avoid. So security is now the main value add of the human developer because it can't be writing code, it can't even necessarily be producing higher quality code, but if we attach it to the idea of risk reduction, all of a sudden we start to open up a new value space. No matter how good these LLMs get, no matter how much we advance in the world of agentic software development, the one thing AI will never be able to do is take accountability for the risk that it creates. So human software developers are going to be needed to accept that accountability. But in order to do that, we're going to need to understand and trust the output of the new machine that we've just introduced. And that requires context. I actually need to understand not just what code is produced by the LLM. I need to understand how it fits in with potentially all of the other code in my estate, how it interacts with the different services in my microservice universe. I need to understand how uh many layers of infrastructure are sitting. Between that service and the outside world, I need to understand the business requirements associated with that piece of software. I need to understand what are the existential risks, what is the threat model associated with that piece of software, all context which is very difficult to cram into a single window to make that request to any kind of machine learning model. So what this means is that when we think about competition among engineers, and we think about how do we begin to hire the next generation of software engineers, well, we certainly can't give them a coding test, right? We can't even really say, uh, you're gonna compete based on the quality of the prompts that you're, you know, injecting in a particular LLM because that's sort of a fool's errand. The system guardrails themselves could potentially uh, you know, mitigate any kind of difference, um, and they're also always constantly evolving. So, so prompt engineering as a definition or as a, as a kind of occupation can't really scale in a linear fashion. But what we can do is create a value system in which engineers compete with each other based on their competence at securing the results of their AI coding assistance, right? The understanding and the gathering of context and the application of that context to making better decisions, especially when that software has to be maintained. Every piece of software in the world breaks eventually, right? Bit rod is real. It is a measurable phenomenon. There is no static, uh, you know, nature to a piece of software because everything around it is constantly changing. You let something run for long enough. Eventually one of those requests is going to fail and it's going to require the collection of context to understand the problem. Trace it back to what changed and either revert that change or make some kind of workaround in order to allow that service to continue to function. When I started my software engineering career now 25 years ago, uh, my, you know, first boss was explaining the responsibilities of the role. I was a little bit nervous, right? And I was asking about like, well, how many lines of code do you expect me to produce? What, how am I being measured? Um, what is the, what's the metric, right? What's my KPI? And uh this guy is a big science fiction fan, and he pointed me to a quote by Neil Stevenson, and he sort of said, your job is not to write lines of code, your job as a developer is to condense fact from the vapor of nuance. And I love this idea and it's been in my mind the entirety of my career as I've thought through what value am I providing and I think that is even more true now than it has ever been before in the world of software development. Our job is not to produce ones and zeros or even to produce cur. Braces or you know. TF files or anything else. Our job is to condense fact from the vapor of nuance, and that's precisely what AI will never be able to do. All it can do is generate more nuance that it is now our job to understand and create facts out of because fact requires the accountability that AI can never itself provide. So now we have some foundational idea of what the future of software engineering looks like. The value that we can generate over and above these agentic tools as this next generation of engineers. So what do we need? What tools do we need to be available beyond of course these agentic coding environments themselves like Quiro, ah, in order to do our jobs better, in order to embrace this new value proposition of being an engineer. I've got some suggestions. Uh, I would suggest that the most important experience or tool in a developer's toolkit is no longer the IDE and it's not even really the agentic developer environment, it's the AI trust platform. What does this mean? What do I need to ensure AI trust? It's very different than saying I need a command line and I need a a text editor and I need some kind of automation to take my commits and push them through a pipeline and deploy them into production. We've got all that, right? We're, we're at reinvent. We should probably be reasonably certain those, those ingredients exist. Well, I bring this back to the idea of developer experience. What does it mean to actually be productive as a software engineer? Um, this is based on some research in the, uh, Association of Computer Manufacturers Journal from, I think 2023. You see this, the, this triangle repeated in the phrase, uh, or in the area of developer experience research. So you take high performing teams and low performing teams and you say what do they have in common, where are their differences, and what you don't actually find is, well, the high performing teams are better educated or they use better tech or they're moving higher up the stack. What you find is a combination of three things. In order to have productive software developers, number 1, you need to allow engineers to remain in their flow state. If you've ever built software, you've had this experience, whether or not you've had a name for it, right? When you are on, when you are working at your most efficient, is when you are in your flow state. What this primarily means is you're not being interrupted. You're not context switching out of what you're doing into something else. Right, so this is, you know, fundamentally the idea that developers or engineers of any kind need large blocks of time to, to conduct their work. If I have 4 hours uninterrupted, I'm gonna get way more done than if I have 8 half-hour blocks scattered throughout the day, right? Because in order to then answer the phone, to respond to a Slack message, to go to a meeting with my business leader. I have to sacrifice all of the context I've just generated, move into something else, come back to my work, and I have to re-establish all of it. I have to reopen all of my tabs, I have to get all of my files correct, and most importantly, I have to re-establish the mental model of how bits are moving around in this universe I'm creating, represented by code. So flow state means allowing to developer, allowing today's developers to stay within their AI native, their AI accelerated surfaces without pulling them out, requiring them to do other different things that causes them to sacrifice that context and make a very expensive switch. The second aspect of developer experience we call cognitive load. Cognitive load is the idea that all of the information that I need to solve a particular problem needs to be kind of in my own human context window. So it all needs to be aggregated and preprocessed. Any time I have to then switch out to open a new tab to do some research, to ask the question why or what is my next step, this is adding to my cognitive load. So all of the required information to make a series of iterative problem solving choices need to be made available to me. Um, I talked to a lot of security leaders in, in my role, and it's really fascinating to me the idea that people who have never built software have of what it means to be an engineer because they tend to think it's very linear, right? You sit down at your desk, you begin typing, and 8 hours later you have a certain amount of lines of code, right? Nothing could be further from the truth, and I think we all know that who have this experience. Mostly you're staring at your monitor and you're scratching your head going. Why the hell is that behaving that way? And you make a guess and you think, I don't know, maybe it's this, and you cross your fingers and then it's not. But in an ideal world you get a different error message that gives you a clue that maybe it's something else, and then you try again and again and again and again, and at the end of the day, maybe you've written one line of code, but if it's the right one, that is way more productive than a day in which you just sit and type for eight hours straight. So cognitive load is the idea that all of that context needs to be available, otherwise you're just simply iterating over and over again, bashing your head against a problem that you're never going to solve because you don't have the right context. And then finally, the third aspect of developer experience that remains important is the length of the feedback loop. Uh, probably got a lot of XKCD fans in the audience, right? One of the most famous XKDC XKCD comics is the pointy headed boss walks in to two developers who are having a sword fight with pool noodles, and he says, Whoa, what's going on here, right? You're on the clock, and the developers say it's OK, we're compiling, right? So the idea here, and of course the boss says, whew, good, continue, you're doing exactly what you need to be doing. Because of course, while their code is compiling, they can't make another decision. You have to wait for feedback, you have to know if that compilation was successful before you can jump back in. So that length of time between when you begin that compilation and when it ends is totally lost to your productivity. And this is the same for something like waiting for feedback from your security team, right? Reloading a page and waiting that interminable 5 seconds to see if you know you got the CSS change right. All of these things we do as developers are limited by the length of time it takes to get feedback about whether the last decision we made was the right one or not. So we need to make these feedback loops as short as possible for engineers to better understand the impact of their latest choice. So if we have a good developer experience, how can we construct this in the new world of AI trust? How can we create the right visibility by managing the flow state, allow developers to prioritize by minimizing their cognitive load, and allow them to apply the right policies and guardrails through minimizing the feedback loops. What we can't do is rely on legacy security tools to solve this problem, right? We can't just say, and take your existing application security stack, apply it to the new AI accelerated and AI native tools of today, and let your developers run wild. There are 3 key problems with this. There's probably a lot more as well. One of them is just non-determinism itself, right? Security has become probabilistic because. Of the new chaos engines that we're introducing. So we can't rely on tools that were invented in a world in which software had a deterministic outcome because it just doesn't anymore. So AI trust has to mean knowing everything that is knowable about your software estate and making educated but repeatable assumptions about what isn't. That's the only way we'll have any hope to address this problem of nondeterminism. The second problem of legacy security tools is this idea of UDA loops, and UA loop is a concept from military strategy. It basically means any time you're competing or even trying to collaborate with another party, you have certain loops that you go through in your thinking. You have to do 4 things in order to, to, to really make a decision and take action. You have to observe. You have to. Orient you have to decide and then you have to act. You have to do all four of those things in that order every time you want to move forward, either in competition or collaboration, and then you wait for the other party to conduct their uda loop. Now the way you disrupt that, right, whether you're in battle or in competition, is you try to get inside the other party's uda loop. Which means if you are doing all four of those things, but most importantly you're deciding and acting over and over again while the counterparty is still observing, they can't move forward because they have to keep observing, so they're fundamentally stuck, right? So this is a core concept from military theory that's made its way kind of into the broader community. Now what I find really interesting about this is when you apply it to agentic AI you realize that that decision making time has been drastically compressed, and it's actually compressed beyond a human's capacity to do anything but observe the situation. If you just let your agentic AI tool run wild, fundamentally you can't inject your own understanding into it. So what AI trust needs to be able to do is to get inside those Uta loops, not of the human engineer, but of the AI agent without slowing them down. That's a really fundamental challenge that is new to the security community. The third big challenge here is what we'll call secure at inception. We can no longer have this paradigm of reactive security where we find things, we fix them, right? We guess about whether a piece of software has risks, we run a scanning tool against it, whether it's in an automated fashion in a pipeline or some kind of part of, uh, you know, regular audit process. We generate a big list of vulnerabilities, tons and tons of Jira tickets, and we start to burn them down. We never finish, right? The backlog always grows over time. That's probably the 3 fundamental law of computer science. So what AI trust has to do is to provide guardrails that govern the creation process of software, not that just speed up the reaction process when we think about injecting security. So legacy application security does none of these things, and this is why as an application security community we need to be moving toward AI trust and we need to be thinking about what is the new value proposition of the developer and what is the new experience. How do we begin to construct a platform to enable engineers to trust the output of their AI. Well, I think it's going to require 3 different engines. And we can abstract away from the particular kinds of scanners that are at play, the kinds of risks that we're looking for, and begin to conceive of this entire process as a loop, right? That then inevitably will need to be adopted by not just human developers, but their agentic AI counterparts. The first of these is the simplest. This is the fact engine. This is knowing what is knowable about a piece of software. This is things like software composition analysis, right? What are your open source dependencies all the way down that dependency graph of the particular, you know, framework that you're using. This is your software bill of materials, right? This is every piece of software that your service is potentially touching, understanding the implications of those interactions, and it's identifying the assets that are in play, the cloud resources that you're using, the AI models that might be in use, the third parties that you're reaching out to. So we need to drastically kind of expand this concept of the bill of materials to not only include AI but to think about what are all of the knowable aspects of a piece of software that might have any impact on risk that we're introducing. The second engine here is called the flow engine, and this is where we take all of those facts and we begin to make informed assumptions about the risks that they're introducing and most importantly how we remediate them. This is where we think about static analysis, right, where you use code to generate an abstraction of the data flows that are being used by that piece of software and determine if any of those are risky. Do I have potentially untrusted data coming in from a source without being sanitized that has been moving to a sensitive sink. These are also where we think about things like MCP security. How do we visualize data flowing between our different MCP servers within agentic AI framework and are some of those flows potentially toxic. So we are modeling the way data is flowing based on our understanding of the facts of the piece of software that we are analyzing. The 3rd engine that we need is the threat engine, and this is where we think about everything that could potentially go wrong. This is where we broaden our focus to the business context of the piece of software. We're testing the assumptions that were made by the flow engine using all of the context that we've learned about the broader ways that piece of software is being used. This is where we think about dynamic analysis. What does dynamic analysis require? Well, it requires that piece of software to be deployed in a usable form, and it requires an understanding of what are the threats that it's likely to encounter in the wild. You then run a bunch of those attacks and you determine did they succeed or fail. So it's more accurate than the static analysis we're doing through the flow engine. But fundamentally it happens at a different point in the process. It's confirming those assumptions. This is where we also think about red teaming. We think about penetration testing. We think about the non-deterministic testing of large language models that's gonna involve a lot more fuzzing than we would normally associate with traditional dynamic analysis. There's a lot more technologies that fit in under the hood of each of these engines, but these are kind of the broad ways in which we think about what an AI trust platform needs to look like. But this isn't a linear formulation, because actually, once you have the result of the threat engine, you've got your dynamic analysis, your red teaming results, you know which potential threats are actually risks associated with your piece of software, what do you then have? Well, you have more facts about that piece of software that can be fed into your fact engine to then improve the assumptions that are being made by the flow engine, so it can produce better remediation advice, so that the thread engine has an easier job. So all three of these engines play into each other. They can speed each other up and make each other more accurate, and AI has a role in each of them. Uh, in order to ensure the trust of, again, that decision making loop at the core of these agentic AI frameworks. This is just the beginning. Of understanding what an AI trust platform looks like that provides for the needs of the new developer experience that is centered around long-term maintainability and risk management and technical debt management in our new applications. So what have we learned? What are our strong opinions loosely held about the future of software craft in the age of AI? First of all, we don't think engineers are going anywhere. We just think they are moving up the value chain. Some of them are going to move down as well, but they are definitely no longer going to be uh tightly coupled with the skill set of writing code. AI can do that, but what it can't do is to take accountability for the code that is written. It can't manage the long-term security and take all of the context into account, and it can't provide the understanding that we so desperately need in the loop. So we think security becomes much more important to developers, not less. It doesn't become something we can ignore because we can trust it to the AI. It's actually exactly the opposite. We can trust a lot of the functionality development to the AI to the agents, but the security is precisely where we require accountability from a human engineer and that's why their value goes up over time rather than down. What do we need to establish that trust? Well, it comes from understanding. It comes from collecting the right context about software at the right time in order to enable that engineer to make the right decision. And that ultimately is what we want from a new platform associated with AI trust. So again, I'm Clinton. I'm the field CTO at Sneak. Please come and see us at booth 447 if you're interested in anything from LLM red teaming to MCP security to the way we're reinventing static and dynamic analysis and software composition analysis for the new age of AI.