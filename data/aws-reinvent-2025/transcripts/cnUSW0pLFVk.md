---
video_id: cnUSW0pLFVk
video_url: https://www.youtube.com/watch?v=cnUSW0pLFVk
is_generated: False
is_translatable: True
---

So, hello, everyone, and thanks for joining our presentation. I'm Jan Kovski, CTO and co-founder of Pathway, and together with Victor Scherba, our chief commercial officer, we want to show you what's happening in AI beyond Transformers. So what's the uh post-transformer architecture, and what it offers to the enterprise. Um, and let me tell you a little secret. The transformer is on its way out. Its days are numbered. It's been a really wonderful 8 years with it. Um, it's, it's a tremendous invention. It's scaled beyond all the expectations, going from single sentence translation to basically powering the trillion parameter language models, um, and in a sense, going from a coherence of a few words to, to, to a few pages and then to, to, to, to basically um a small book. But it allowed us to dream about the possibilities of AI, and now as we try to implement those with with the transformer, we start to see limitations. And the main limit is when you try to employ it to perform on the long running task. And the truth is no one is really working on it. The frontier models, the foundational models, they are getting smarter and smarter, so you get something really smart, but it doesn't get any better than this. It's not able to basically follow through with a task which is sufficiently long. And in fact what we all, we often need is a coherence over a long running task. So there is this new axis, this horizontal axis of long thinking, which looks to be totally neglected. And yes, you can download a better model every week. Yes, you can switch to it. They all start to feel the same because they basically all take part in this same competition, but then you put it to work. And it doesn't perform better on those long-running tasks. If there is a bug on day one, there will be a bug on day two and there will be a bug on day 3. Nothing really changes, uh, despite the constant feeling of progress. And we have narrowed down this to basically 3 main limitations of the transformer. First, it lacks a long-term memory that it keeps on adapting. In the current blueprint of how to do AI models, it's trained once in the lab on multiple tasks, then released a snapshot, either as a way to download or as a basically a version model behind an API, and it stays like that throughout its lifeline. You can try to hug an external memory into it, but ultimately the base model stays the same. It's a very smart savant. It's extremely brilliant on day one, it's just as brilliant on day two, and feels just as dumb on day 3. There is, there is no progress, there is no change. Um And you cannot solve really long line tasks without learning in between, without any sort of lessons learned and getting better. Then it's actually very inefficient. You can keep on improving the benchmark by scaling, but every incremental improvement on the benchmarks basically means a 10x on the cost, because the model has to get bigger, the data set has to get bigger. If you want to get better on the benchmarks which are hard for humans, you actually have to curate. Namely, label assemble, generate more hard data, which means to get the good data, you have to hire the really smart humans. There is no automation, there is basically rewriting. So yes, the models code better. Yes, you need to hire very good coders to train those models. Um, and the value per token in a sense is getting smaller and smaller because you have to feed the model with even larger data sets just to keep on progressing. As a matter of fact, we ran out of text data on the internet to train the largest models. Taken together, this basically makes the model not suitable for the enterprise. These are one size fits all customer grade models. They are not really customizable to the enterprise needs. They are not getting better with what they do. It's really hard to train them on the limited amount of data the enterprises may have, and it's sort of counterproductive to hire more people to generate more data so that you can automate the process one day. And then you don't really know why the models are failing or why they are responding the way they do. So if you spot a bug with this limited interpretability of the thing, there is no easy way to fix it beyond, well, let's do a 10x on the data. Let's try a smarter model and maybe it will switch from bug failure one to bug failure two. However, we, we know that it's possible to have a learning, a continuous learning, getting better every day, and we know that it's true to do so in a predictable way. In fact, we are all here today just to learn, right? And our brains excel at integrating new information and learning on the go, and we expect smart people to start on a journey, start doing a task, and finish it changed, but they have learned something during it. It's all about the journey, right? It's not so for the models, it's all for the brains, and we know how the brain differs from the current models. Yes, the transformer is a deep neural network. Yes, it's brain inspired, but also, yes, it's a very different kind of neural net than the one we have in the brain. The transformer is basically a very dense network, and what it means in terms of operation. Uh, you have to use all of it for every decision it makes. Every piece of information is mixed together. Everything is squeezed in those fully connected dense layers, which is basically the bulk of what you download off the internet when you download the weights of a model. Um, now the brain is different. It's sparse in many senses of the word. It's sparsely activated, you know the joke that we use only 20% of our brain. It's actually a good thing energy wise, um, because we only use the parts of the brain which need to be used for the task at hand. This also means less crosstalk between tasks, less overwriting the skills for task one when I'm doing task two. It also means energy efficiency. Then the brain is sparsely connected, not all the neurons talk to each other, and this allows it basically to have those little modules, clusters where knowledge is stored. And as far as network science goes, the world of sparse networks, it's a very, very different world from the world of dense networks. So yes, the transformer is an artificial neural network, but we want to say no, it's not the proper network. You need to have this special brain-like structure with the. High dimensionality, sparse activation, sparse connectivity to have the benefits of the brain, to have the inductive biases of the brain to basically learn and behave like a brain. And the good news is we did it. We are introducing the baby dragon hatchling, our AI architecture, which picks the good parts of the brain and basically builds the whole AI around it. So we went back to square one. We thought how an artificial neural network could behave like the brain network and be the age as the result of this quest. We are bringing together this brain inspiration about large sparse networks with GPU friendliness. As you know, the GPUs love dense matrix multiplication. They don't love that much the sparse matrix multiplication. We made the two work together. We inherited the brain's capacity for learning. We actually have a Synapse network, which is sparse and which is evolving as the model is solving tasks. So we do have this memory which keeps on adapting to the task at hand. We inherit the computational advantages of the brain. The model has good information processing locality, which means that certain parts are activated for central concepts, and you only activate what's needed. This also means that this can scale because localized information processing means it's easier to decentralize. It's easier to go from a GPU to a server to a cluster, basically to a data center. Which is the last bullet point. No central coordination basically means supreme scaling and going from small tasks to large tasks. We have published a very rigorous scientific paper which draws the full path from brain. All the way to how the model operates. Finding this brain-like network inside of the AI. And finally, we have models which really work. On the benchmark we have tried, the models much or exceed transformers performance. So we do have the correct inductive biases, we do have the correct energy efficiency. We do have the correct model to build the next AI, to build the post-transformer AI models. Let me now transfer to Victor, who is going to explain to you how this translates to what basically you care about. So how those scientific breakthroughs translate to enterprise ready features. Oh, thank you very much, Jan. So Victor Sherba here. So let's talk a little bit about how this works conceptually. So think about our model a little bit like a darkened high school gymnasium with millions of little LEDs. And they're all turned off and as data is flying into the room, only certain ones of these LEDs kind of light up. These LEDs are literally the components of the building blocks of our model. They're a little bit like weights. They're a little bit like memory. It's kind of a combination of the two. And only a very small portion of these things at any given time with any piece of data kind of gets used and so it says oh this is important to me, this is important to a few of my buddies here, go pass this along and say, hey, listen, you need to know about this thing, but most of the model does nothing. So that is what allows us to keep the model in memory. That is what allows us to keep the model going very quickly, etc. So that's the image that I'm hoping that we could kind of implant inside your head. So what do we get from this? Right now we have a model that as the data is coming in. It's kind of continuously learning from the data itself. It's now, it's no longer a model that is fine tuned because the fine tuning is actually being processed as the data is coming in. Now all of a sudden you have all of these monetary benefits because your model now is much faster and the throughput is much faster and you don't have to use as large a model and everything else. So in the world of efficiency, let's take a little bit of digression here in the world of power in that high school gymnasium, when you're training a model, you literally have to light up every LED in that room at all times. This is why you're using so much power in in so much GPU when you're training models. And in our model, it's only doing a certain small little piece, so it's always kind of available, always super lightweight, much faster, especially on these really, really long reasoning tasks. And the other thing, the 3rd thing is the increase in functionality. So if you think about that diagram that we showed you guys about the functionality and why all of the models out there, the frontier models, all kind of feel the same, kind of because they are, right, they're trained on the same data, same number of parameters, etc. Think about us as almost like another axis, where an axis where we're opening up all this new functionality that you can use for your enterprise type of applications. So let's talk about a couple of things and a couple of examples of things that you could do in a non-transformer model specifically inside of BDH, right? So other than the continuous learning which we've talked about before, let's talk about attention span. So right now the best models, the highest end reasoning models have the attention span of about 2 hours, but what nobody really tells you about, that's with a 50% success rate. If you want to get to an 80% success rate, they can only kind of focus on a task for about 30 minutes. That does not help us in putting together a lot of functionality that we have inside of our corporation into an LLM. Think about a model that could take as complex a process as. Finishing a quarter So now all of a sudden this is 8 departments working for weeks on end coordinating all of the different things that have to happen so a company can release its public results and audit them. This is the kind of attention span that we are talking about with models like this, having a really smart person that remembers the quarter end process and having them there with you the whole time. The second thing that we do is we can learn from very, very thin data sets. A lot of corporations don't go through that many of these processes that many times to get a lot of learning data. When we're people and we're learning things as a kid, how many times do you have to taste soap to remember what it tastes like? Right now, in a world of Mediterranean LLM, you literally have to taste soap thousands of times before the LLM says, ah, this is what soap tastes like. Right, so think about you have something that you're like, oh my God, I want to learn this and internalize this into the model. That's what this long attention span does with a thin data set. And the third thing that happens is in a world where there is highly, highly regulated data in in places where there's a lot of regulations and things like that, we can actually look inside the model to see what's happening. So if you think about that analogy that we painted of all the little small LEDs, we can actually count and see what's going on inside of each of these LEDs and keep track of that. So now all of a sudden you have this observability and you have this auditability and all of a sudden you can now start using this model in places where you would never really use a black box uh type of LLM model. All right. So, What's, what's next? Right now we literally just started making our announcements. We've been kind of peeking out there. Those of you who looked at the Wall Street Journal the other day, they did a nice little story on us. We are just opening up our customer engagement. We are looking for design partners along with our uh partners at AWS. This model will be available sometime in the middle of the year, the first half of the year, and we are looking for the folks that have use cases right now that have not worked using traditional LLMs, whether it's a thin data use case, and whether it is one of these long attention use case or a highly regulated use case, please, please talk to us about that, right. Our model of monetization is actually a kind of a consumption function, so we earn all the tokens that we put through the machine. So in essence what we're doing is we're building this concept of a sticky. Inference where a lot of inferences that you go through the standard models you're gonna probably want to go through whoever's cheapest, right, because it doesn't matter whether the outcome comes from, you know, competitor A or competitor B, the answers are pretty similar so if they're pretty similar, whoever's the cheapest, right? But what about the most strategic nature of a sticky? Is the stickiest thing you have is your internal corporate data. Put that in the model and see the answers that you're gonna get out of the baby dragon. Right, so that is how we those use cases that we're looking for reach out to us. We have uh an email you could talk to us afterwards if you have these use cases. We are literally starting to uh sign up design partners today. And with that, I wanted to say thank you and we will be able to take questions, but the rule was we had to take questions off to the side. We can't do questions in uh this kind of a format. So with that, Jan and I wanted to say thank you very much. Thank you and happy building.