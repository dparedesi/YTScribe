---
video_id: RScWKQQwCKk
video_url: https://www.youtube.com/watch?v=RScWKQQwCKk
is_generated: False
is_translatable: True
---

Alright. Hi, good morning, everyone. Thanks for joining us today. I hope you guys have been enjoying this year's AWS reinvented thus far. Thanks to AWS for giving us the opportunity to speak a little bit about Cla to be here today. And uh of course I would like also to uh express my deepest gratitude to both SAS and Fresh Works both for their continued partnership as well as us taking the time to share a bit of their story with us here today, all of which you are going to hear about just shortly. My name is Felipe and I work as a technical director at CODB. So just to set the scene here real quick, I want to briefly recap the primary challenges chins face when they try to scale their dating days applications. These are applications that require extremely high throughputs with fast and consistent response times, often within single digit milliseconds or lower for their tail latencies. As data intensive applications scale and becomes critical for your business, the primary core challenge is ensuring their consistent performance. As you scale, conventional database systems become increasingly sensitive to traffic fluctuations, which, yes, you can try to mask by throwing more money at the problem, but that that quickly becomes unsustainable in the longer term. And this is really the context where SilaDB fits in. SillaDB is the database for predictable, uh, for applications that require predictable performance at scale. Our architecture is specifically designed to eliminate performance bottlenecks that conventional database systems struggle with at scale. Rather than compensating with more hardware, CLDB is close to the metal architecture is built to maximize the performance of underlying infrastructure. That's why organizations trustyADB to power their most critical applications. With CyADB, a small 3 node cluster running on top of AWS EC2 is capable of sustaining millions of operations per second with predictable single digit millisecond latencies, even as workloads grow and fluctuate. This efficiency. Does not just improve performance, but considerably reduce infrastructure costs, giving you headroom to scale your most demanding data intensive applications. So, how do I know whether SyloDB is a database that's a good fit for me first. If your current response times are unpredictable, especially when your tail latencies are starting to violate your SLAs and your chains keep getting paged at 3 a.m. just for you to babysit the database. Second, if your data Base cannot keep up with throughput demands, particularly during peak or spiky traffic or when you are forced to overprovision just to stay ahead of a predictable load because your system lacks true elasticity. And third, your database spend is spiraling, uh, and adding more nodes, replicas, or caches has become the only way to mask the performance issues, which end up driving your costs out of control. If any of those three points, or a combination of these resonates to you, then you may want to check out on CRDB. So this chart on this slide demonstrates our user base spectrum, where the Y axis represents throughput, and the X axis represents latencies. As you can see, the largest cluster of users really live in the top left-hand side, with workloads ranging close to a million operations per second, uh, requiring between single digits to less than 20 milliseconds P99 response times. Now, why is silly to be? Our highly available architecture allows teams to confidently run mission critical applications at scale. We are built first and foremost with high performance transactional workloads in mind, while we still support real-time analytical use cases that need fast access to operational data. And more recently this year, with our recent native vector search, Silabi becomes a unified AI platform built for performance. CAB can store and serve massive volumes of data while using fewer nodes, reducing infrastructure costs and operational overhead. It's also designed to avoid vendor locking, giving you the flexibility to run your workloads on-prem, in any cloud, or in a hybrid environment, and effectively allowing you to serve data closer to your users, where it matters. Elasticity is also built deep, very deep in our architecture. We allow clusters to scale with demand, eliminating the waste of permanently overprovisioned infrastructure. So you get predictable performance during traffic traffic spikes and cost efficiency the rest of the time. So, how are organizations using Silla Beach today? Beyond SAS and FreshWorks, uh, at last year's reinvent, I had the opportunity to speak along with TripAdvisor here at the very same reinvent, who shared how SylaDB powers their real-time machine learning system for recommendations. We also have companies like Discord, who use SylaDB at massive scale to connect millions of users in real-time around the world. Disney Plus and Hulu are relying on CADB to power their media streaming services. CADB having supported the Super Bowl for actually many years in a row now, we've got that far, yeah, and uh, of course, most of the organizations listed here, uh, actually also run CLB within AWS, which makes it perfect for me to talk a little bit about our existing partnership with AWS. SilaDB has been for many years an AWS ISV partner, where we've partnered together in demonstrating the superior price performance of their EC2 I4I and I7I instances ahead of their launch, as well as the superior Graviton performance also in AWSEC2. We are a Graviton ready partner, and we have 2 listings, 2 listings in the AWS marketplace. We have CADB Enterprise for users who want to run and self-manage their own database deployments, and we have CAB Cloud, our fully managed database as a service offering. So with that, let me switch over to Brian Jones, senior manager of software development at CES, who is going to tell us a little bit about CAS's real-time personalization story. Brian, thank you so much for sharing your story with us today and, uh, with that, uh, the floor is yours. Alright. Thank you Felipe and thank you everyone for coming to uh to this presentation today. Um, so the first thing I wanna do is talk about. The big game, uh, so you're sitting down watching the big game and suddenly there's a break in the action. And now you get an advertisement and it happens to be the same commercial you've already seen in the previous break and that's very annoying for you. Why does that happen? Well, they are probably not a customer of sass. At SASS we've been delivering digital advertising during live sporting events, uh, like football, soccer, tennis, rugby. Since 2014. We have a robust AgTech platform. Seamlessly integrating with your own data, workflows, and partner ecosystem. Good ad decisions require us to activate data quickly. Which improves the outcomes for both viewers, advertisers, and broadcasters. In plain terms, the core of video advertising is composed of a few parts. The first of these is ad decisions. When we receive an ad request, we need to make a decision quickly about what to serve. And that response in video advertising uses the IB vast. Standard which will include some playback signals or events so as the advertisement is played out, the signals will come back to SAS, other partners. And those become then part of the reporting. That broadcasters and advertisers will utilize. And finally, um, especially for video on demand, uh, optimization can happen where you can choose how many ads to show or how long a break should be for individual users. So This picture is, uh, from inside the Ocracoke Lighthouse you can take a look inside of there during summer months and like the steps to summit this lighthouse there are a few challenges in digital video advertising. I want to discuss. So first you need to scale to support large traffic spikes. Integrate with the rest of the ecosystem. And make advertising relevant to each viewer. So at SASS we help organizations. Do amazing things with the data we and we've been doing it for nearly 5 decades. SAS is a leader in data and AI with headquarters in Cary, North Carolina. Our focus is on helping people to use data and AI responsibly and effectively. Size 360 match delivers. Decisions at scale for our customers around the world. So these are some of the numbers, billions of impressions, video ads, programmatic bid requests. And all delivered with low latency and high availability. So this image captures part of the new user interface for SAS 360 Match that I and my team have been working on. At SA, it is a first party ad server, uh, delivered its software as a service application to publishers, broadcasters, retailers, financial institutions. And we have clients like desktop browsers, mobile applications, connected TV, and many other platforms. So I work on Essentially all aspects of this product R&D, operations, security, governments, legal, audit, etc. etc. So if you have questions, uh, come talk to me afterwards. So what kind of data do we usually have when we're making a decision? So we have personal data like where are you, your interest, maybe some data that you use to sign up for a service, registration data there could be analytic scoring data that's also attached to your identity. Additionally, there's contextual data. Uh, Amazon has spoken about this for a couple of years now, uh, using AI to grab contextual data from video streams. In advance or in life, so you have episode data, break data. Um, also recent ad serving history data. Um, then we also can incorporate other data, so there could be product data or potentially offer data. So from all of this data, audiences and segments are created that appeal to advertisers. So there Are many methods available to provide data for decisions. An ad request may provide this data. The data could come from an external web service, for example, if you need to get the current weather for a location based on geo. Um, that data is fetched, used cash for a brief period of time, and used for that request. You can also reference data that we hold in memory or is stored in a persistent data store. And any of this data can refer to other identifiers which will load more data for you um and recently, as of earlier this year, we introduced the concept of data sets which is basically a hierarchy of identifiers where we evaluate each set of data independently to determine what should be served. So let's dig a little bit into. An ad break. So when a video stream is approaching an ad break. There will be a signal within the stream. Uh, Scotty's type of signaling, uh, indicating that a break is coming soon and we need to make an ad request. That ad request is usually templated. It doesn't usually have very much data, a couple of things at most, usually. Um, and we'll receive this. And then we'll need to make a decision about what to serve. And so we'll return a vast formatted XML template response. And inside of this we'll have a number of these events which I mentioned earlier like impression view various um standard quartiles those sorts of signals. And this is an example of what the break actually looks like to our software. It's a series of requests. In this example we have a duration server request. It's asking for a duration of 180 seconds. It says it needs uh that this is the particular break identifier. This is like a pseudo random identifier. It'll recycle after a couple months probably um then we have an identifier of some name for a visitor, so in this case SVID. And then Uh, after this ad request returns the vast response, the, uh, service side ad insertion or the player device, whatever will begin making these count requests. These are all the, uh, events that were in the vast response to begin with for all the quartiles that we mentioned before. So then how do we make a decision? Let's dive a little bit through that. So if we've never seen a request from a user before from visitor, um, we're gonna need to start a session so we will, we will start the session usually with an identifier that's given to us, but sometimes we'll need to generate one. Um, we'll then load data if we have it for the for the individual if we've uh got data to load for them, um, including all of their previous history that we need. Uh, to enforce other business rules. And then we can begin processing the actual request. So we'll activate more data, so super tags, these are in memory data, sometimes persisted data from identifiers, uh. Usually coming from expansions of things that are inside of the request itself. Then other identifiers may be loaded. Remember the products and offers that we talked about earlier. Connectors are the things where you can talk to any HD2E web service, load more data still. You can also talk to SASS Intelligent Decisioning. Uh, then we need to make a decision. So the customer can introduce prioritization based on their needs for their inventory. SASS can perform optimizations as well as Uh, targeting via Boolean logic on all of this key value data that we have in hand. And if the customer is using it, we can talk to Open RTB, uh, SSB, uh, uh, to for open web, uh, creatives, or they can, uh, make use of direct bookings in the system, uh, as well. So once we have selected the creative, we'll render response again from the vast template for video advertising uh we'll serve that and record this decisions one or more as well as metrics that need to be generated. So providing the best decision in the moment also means that we support the viewers, for example, by preventing the exposure to the same advertiser or ad too often. Ensuring that industry and government regulations are followed and respecting the privacy rules and regulations our customers must adhere to. So at SASS we approach things differently than many other players in the digital video advertising industry. SASS does not collect and aggregate data across customers as others in the advertising industry often do. So the broadcasters have control over their data and with whom that data is shared. We provide our customers with the tools that are open to integration. An extension to fit their needs. Both performance and availability matters not just for users but also advertisers and our customers. How do we scale our access to data? We require this is a requirement that the database have low latency reads. It must have high availability. Data redundancy and predictable cost. But there's not enough time. During this cycle of ad request to fetch the data, make use of it, and make the changes and write them back out. So, we introduced some other concepts. We activate the data on session start. We write changes to the data. When, when necessary. And this reduces the interactions necessary for um sessions. They are routed uh at based on. To find the right, uh, to find the right session within the cluster. How do we scale to handle large traffic spikes? So We make use of load balancing. In Amazon we've been using provision capacity from load balancers for many years. It went to GA earlier this year, I think in January, maybe December last year. Um, we also make use of auto scaling. Previously EC2, now Kuber's horizontal pot outlet scaling. Uh, schedule scaling is also available, so broadcasters often know or have an idea of the size of their audience based on time of day, program, etc. And uh we also introduced CPU utilization buffers, so. Rather than run it let's say 80% CPU utilization, maybe you run it 50%, you give yourself some room to handle spikes that auto scaling simply. Can't do. And finally load shedding. So there are several features in this area for one, in one case they can a customer can choose to prioritize certain types of requests over others so because some are more important to them for revenue than other requests are. So I've only touched on part of what SAS 360 Match can do and how it does it. We support many kinds of client applications. We use many Amazon services. And yes, we use psility B. So why do we use silly DB? FacilityB provides the low latency that we need for reeds. It linearly scales both vertically and horizontally with compute. Individual nodes in the cluster can be replaced without data loss or downtime. The data can be replicated to multiple data centers, AZs, or regions if needed. And for us we can also use workload prioritization to ensure that ad delivery is not impacted by other data processing that we must do. Here is an example of what a request spike looks like. So when we get to the spike seen on the bottom graph, there's essentially no change in read latency. Which is exactly what we want to see. What kind of data do we store? So we store viewer data registration data. This includes events for event counting so we know how many times you've done something that's important to the broadcaster or advertiser. We also can't uh store exposure so this is how many times you've seen a particular ad, and there are rules that will say things like, uh, let's not show this ad more than once a day. Those sorts of things. Um, Additionally, there is data that can help map from one identifier to another if necessary. You can see this for like, uh, cases where you only have a device identifier, but, uh, someone is signing in as a registered user, and so the device becomes associated with the registered user in the service. Uh, we also make use of additional data like the program and episode data, uh, break identifier data, product and offer data. So it's one thing for me to talk about what SAS 360 Match can do. It's another to hear it from one of our customers. So Alex Mason, head of digital ad platforms at ITV, says the following. The flexibility of SAS 360 Match allows us to develop things at our own pace. We define our own development. We're not waiting for a third party. What this means is that Alex can often use the existing capabilities within the toolbox that SAS provides to solve new problems without waiting for new development. All right, if there's just 3 things to remember from my presentation. Here's what they are. With SAS you can leverage a multitude of data sources. To make the best decision at the moment. And help advertisers reach their intended audience. And provide broadcasters with the tools to improve the advertising experience for viewers. And SASS does all of this at a scale that can handle some of the largest events in broadcast sports. Right. I'm gonna turn this over to Felipe now. Thank you so much Brian for such an awesome talk. Uh, before we let you go, allow me to ask, uh, we spoke quite extensively about the 360 suite. Uh, is there any particular, uh, examples of problems, uh, CLB helps you solve? Sure, so in Sility B I mentioned event accounts. So in one case a customer had a satisfaction problem where viewers were unhappy that when they, uh, went to go watch a video on demand stream they were shown advertisements and let's say they didn't actually want to watch that one they wanted to switch and watch another one and then they get ads again. Well, why there, there's a way to fix this within the ad software itself so we can count when you have last seen. Ads a break for instance and let's say you want to implement a rule that says if another ad request comes within 4 minutes of the of the last time you saw ads. We're going to skip it. We're just not going to do it. And this greatly improved user satisfaction for this broadcaster. It was a very small change to make that had a huge impact. And uh from that perspective, would you say a little bit somewhat sits in the hot path critical path on your application? The way I'd like to frame it is CLDB were ever to come down, what would be the impact to your size business? Well, still it doesn't come down. I mean, it's not, it's not a thing, too much redundancy, um, so, uh, still is critical for us for dated redundancy, um, and and ensuring we have the data we need quickly when we're trying to make an ad decision, especially in like the worst case scenarios where, um, it's a new user we've not established a session yet and so we plan for those contingencies and. Ensure that we have the resources necessary with Silla to answer those requests. Well, thank you again, Brian. Uh, Brian will be available later after our presentation to answer any questions you may have. Uh, with that, Brian, I will let you go. So during his presentation, Brian spoke about SAS 360 match API support, both real-time and batch up updates to data. He also mentioned about one particularLRDB feature called workload prioritization. That's a feature that allows. Users to assign different priorities for different workloads. In that sense, that's a feature geared towards infrastructure consolidation which allows you to prioritize real-time workloads in contrast with workloads that do not have strict latency requirements. Um, so, next up, allow me to callzridar, VP of Engineering, and Prank Kumar, senior manager at FreshWorks, who are going to share their story on how FreshWorks redesigned their AI software architecture for 10 times growth while we're improving latency by 95%, all of which we tender significant cost savings. Sriar and Prem, thank you again for your time and with that, yeah, the floor is yours. Awesome, thank you, Felipe. Hello everyone, good morning. Uh, this is Shriar Gre, um, head of AI and data for, uh, FreshWorks, and, uh, it's Prem, my colleague. Hi, myself, Prem Kumar. Uh, I handle the database at FreshWorks data platform engineering. My passion is data, and that's what I live for. Fantastic. So, uh, uh, over, you know, an extra 20-25 minutes, I'm going to, uh, you know, cover this presentation in 3 parts. I'll call Prim back into the stage, uh, you know, uh, do the initial part. Thanks, Prem. So the first part would be about the fresh works. Uh, essentially we talk about the company and its scale. Uh, that will be a good segue into section two, which is about, uh, the, the entire journey of, uh, um, our, uh, NoSQL, uh, you know, the, the, the quest to actually find CRDB as one of the great partners and then how did we actually go about building multiple. Um, you know, use cases, and then our migration story. Then I'll come back again and, uh, you know, wrap up with the uh AI use cases and then the takeaways. So that's us And so FreshWorks is a company started about 15 years ago in 2010 in a city called Chennai in India with the dream of building an uncomplicated software to enable companies to solve their business workflows easily. And we started as a Fridge desk, which is a customer support software, and over time we went on to build just the dream that we had about building a complicated software for all sizes of companies. And 10 years later we were the first company in the SAS field to have gone public in the US and we are very proud of that. And now we have close to 70. 5000 customers and we sell our products in over 40 different languages and touching distance to 1 billion revenues and we are very proud of that. And how do we build that we always looked at the customer side, how do we create uncomplicated software, but at the same time we also look at uncomplicated technology stack. So you'll see a foundation at the bottom where You have a cloud infrastructure scaling across and then you basically will have uh the platform which is, you know, all the foundational capabilities like say um single sign on and then analytics and everything else and on top of it you have again foundational AI and then you got the products on top of it. So this helped us scale leaps and bounds and uh uh and we're just getting started. And the journey has been fabulous for us, you know, like I said, in 2010, we started with a product called Fresh Desk. We were a single product company and uh with a humble, you know, 100 to 200 customers, but then soon we started adding fresher. Fresh Service is our IDSM. Um, you know, offerings, and now we have, uh, today we have, uh, asset management and the service management and even the employee, um, uh, workflow management, also in this, and this is one of the fastest growing, um, you know, IDSM, uh, systems, you know, on the planet, you know, on the SAS SAS world, and As you can see, we, uh, we also have, uh, you know, Freddie, which is, um, our, um, brand name for the AI offerings across the company, and then you'll hear a lot about Freddie as well, you know, in, in the upcoming slides and, uh, outside. So that's about FreshWorks. So FreshWorks, you know, in, in, in, in a summary, uh, it's a company that was built to revolutionize the experience for SAS customers, and we did just that, but then that also basically means that we keep reinventing. The data technologies and AI technologies to meet and exceed customer expectations. So, um, let, let me talk about double click into the data, uh, sort of reworks like, so one size does not fit all, right? So we moved from having entire business logic into MySQL, uh, or RDBMS to multiple data technologies across the board and like whatever actually, you know, fits that particular use case and at the same time we wanted to make sure that whatever we have in our dataverse portfolio is the best and best. In that in that space and that's where our whole partnership with the CLRDB actually comes in and that's what we're going to talk about. So we, we care about data engineering so it's not about operations or it's not about keeping things up and running. It's not about keeping the lights on. It's about engineering for data. Second thing is we care for reliability and scale. We spoke about scale already. And reliability matters. We at FreshWorks at a database layer, we give 4 9s consistently and sometimes 4.59s as well for all the products being supported by that. We also care about disaster recovery and BCP and also data residency. The customers are increasingly more sensitive about where their data is and also how up and running in terms of the disaster recovery in case of outages that happen on the cloud or elsewhere. And security is paramount for us. We care for the privacy and security of our customers' data, and that's where data, since the entire dataverse holds the source of truth as well as the persistence of data, we want to make sure that the security is ground up, the data at rest, data in transit, everything is kind of encrypted and secure. Last but not least, we also have close to 1,000+ developer community in the company for us to support who build our applications and products and AI on top of database, and we give them AI enabled. Uh, self-service portals for them to be able to, um, spin off, uh, data layers and then be able to scale, support, manage, deploy, um, everything else. So it's, it's, it's very seamless and it actually helps them also scale. So that's a, a glimpse of data works at FreshWorks right now going deeper into, you know, how we came about, uh, in our CRDB journey. Um, so we have been talking about, uh, Casenda workloads, and there are multiple and NoSQL use cases within our company. You'll hear more about, uh, those use cases. We kind of open up our, uh, playbook for you in a way, uh, selflessly to talk more deeper into exactly what are those use cases and how it kind of helped along with scale, um, but, uh, in a nutshell, we CEA kind of, uh, you know, allowed us to actually bring our own AWs accounts because we are a super complex AWs environment. Already, uh, in a mesh architecture with the platforms and AI and applications, so we did not want to change that. So we were actually in our own VPCs, but actually we could bring in Sila uh into, into our tech stack but still have all the goodness of hosted solution. And you'll also see performance and scale metrics. Uh, you'll see, like I said, uh, use cases. I think they'll, you, you'll be able to resonate with some of those as we speak. And uh but what's more important is how did we migrate with a third party uh Cassandra or open source Cassandra to uh where the Silla is right uh and how the uh like how was the journey you'll also see that and uh last but not least, there are also some future upcoming use cases that we are talking about, you know, this world is all about AI, so everything, uh, to do with AI and how we are empowering using self-service or agentic workflows how CLI is going to actually help us get there, right. Like I said, our customers care for data residency, and that's one of the reasons where, uh, while we use managed service, the entire data plane is within our own VPC. We have full control over it, and then we have full access to it and then it is actually within the boundary of our security guard tails and the control plane is with Sela and it's a perfect combination, you know, and it really helped us quickly deploy, quickly scale, and take it from there. And this is, I think, a defining slide because if you look at the numbers there when we actually did the benchmark when, uh, the executives from AIA came and met us in Chennai and said, hey, you know, we are this company and uh we actually solve Casenda problem, said like, hey, this problem has already been solved. Apparently not. So when we actually looked at it and then did the benchmarks and uh like we were just blown away with the, uh, especially, uh, you know, tail, uh, you know, the, uh, performance is much, much higher at like 99.99. Uh, you know, percentile, we generally look at P90, P95. Here we are looking at that depth, which means that no single request is actually left behind in terms of the performance. So that was our experience. And so, so with this, uh, we, um, I basically, I can hand over to Prem to actually walk us through, um, some of the use cases. You can actually go through it and then I'll come back and wrap it up. Prem, over to you. Thank you. I'm going to talk about some of the use cases at Trish Works. We use SkilaDB. So I'm going to cover three different types of database migrations as well, along with this. So the first one will be covering Cassandra to SkilaDB, which is a natural migration for us to consider. And the second one will be like how can MySQL data be moved towards SylaDB. And the third one will be Dynamo DB data with limitations kind of improved while we move towards. So conversation store is a platform service at Fresh Works. This kind of covers all the conversations, kind of stores all the product conversations at one shell, and then this kind of also as translation service. This can also be useful for us to store the AI conversations between customer and AI agents, and this is powered by Cassandra. And we also have uh UCR, which we call us unified customer portal. Like we kind of actually store all the contacts, whatever that is part of all these products like Fresh desk andre Service in one place, and we also retrieve it on a very low latency level, so that's a requirement. So we were using Cassandra for this. There were some bottlenecks in that and also some availability issues, so we did consider SkilaDB for that as well. And as I said, the scale is kind of 4 billion plus contacts saved in this portal, and the expectation is very minimal, right? The latency should be below 5 ms consistently. Like it should be even P99.99 should be also below 5 ms. And the current scale is 15 ks per second in this portal. So I'm going to walk through the migration story and as I said, we had some difficulties in Cassandra. Like every weekend whenever repairs runs, deletion runs, we had fallen into some latencies and also timeouts in Cassandra and also we did try some third party Cassandras like Data tax. There are some availability issues, so we did consider Skela for this particular action. And I'm going to walk through the migration story which is kind of a little interesting for us to and also challenging for us to push our limits because we want to do this in a live migration. Because we wanted to give the customer the ease of continuing the use of database and applications even on the weekends, so we cannot afford any downtime, and that's our benchmark. We want to, you know, make sure the customer is always up and able to serve because we serve across all parts of the country, and also all parts of the world. So customers, uh, we kind of push our standards that we ensure that there is a zero to migration always. And uh we kind of taken the first application and introduced the ZM proxy in the middle of Cassandra, which can actually do dual rates for us, for both Cassandra and Escilla. So while it does dual rates, the reed goes through Cassandra, that's an existing system. So when I say dual rates, all the new data, like right from the beginning of the ZDM proxy gets captured in both the places, Cassandra and Skiller. And now that CDM migrator comes to migrate the historical data like from where it left. So we started dual rates. Now we'll have to migrate the historical data into the new system ScallaDB. So we used a CDM migrator. We migrated all the historical data into SkierDB, and now that it has both historical data and the latest data through dual rates from the DM. And this is very well put out here as well, like how the CDM migrator reads data and then pushes to Scaler DV. And after we complete the migration, we also wanted to validate and then fix the missing gaps in terms of repairs and things, so we kind of actually use the CDM migrator to validate all the data is copied from Cassandra or Scala. So while we do that, we find some of the gaps because the data we talk here is close to terabytes, like 1.2 terabytes to be exact, and we find some bottlenecks and the job is keep on running for a while and then we took the open source CDM migrator, modified it with batch processing options. This helps us to kind of add the batch processing and improve the time to 10X. So one advantage. We have in this portal and disuse cases this only powers the majority of inserts. Not much is updated, so we can make sure that only if you check the record exists or not, that should be enough for us to cross through the validation phase. So we were able to get 10x improvement and now the CD migratory is upstream as well, and S is using for other customers while they migrates to Skiller. And as I said, validation is completed now. We also go to, we want to switch the traffic rate, and we kind of switch the traffic from, uh, I think I moved this slide, yeah, we can switch the traffic from Cassandra to Skilla at this point. Now while we do that, we want to ensure the reeds are switched first and then validated through logs for any errors or any timeouts and things like that. While we are comfortable, then we kind of move the right traffic also to Skiller. And we also ensured that after the rate is switched, some kind of JDM procedural rate happens to Cassandra as well because that gives us the convenience and comfort for us to come back to Casentra if there are any issues faced. But fortunately there's just one path. Once you move to Skiller, there is no coming back for us because the system was scaling what we expected and we were able to always solve the traffic within the SLA and the tail agencies were really helpful like Steve mentioned initially. And I move on to the next use case. This is workflow automated use case. So when I say workflow automator, this is going to cover all the workflows that run across our systems, all the products and all the platform services, all the use cases. So this is a bond use case with Skiller. There's no migration involved here. S. Supporting this and the uniqueness is this multiple concurrency and we have close to 250K plus workload workflows stored here and parallel executions has to happen and this is operating on the top of Netflix connector open source software that demands a Cassandra backend so we kind of selected Skiller because the throughput is very high and now AI coming into the picture, there's a lot of even more further workflows that has to be. Run seamlessly and optimized. So we selected ScalarDB for this use case, and it is being served very, very much in ease and also within the latencies, as he said. Yeah, so moving on to the next one. So I've covered the Cassandra to Skiladibi story. Now I'm coming towards, uh, what we wanted to migrate from bicycle to Scalladibi. So FreshBooks historically it's a bicycle cum ruby store. So we say the source of truth lies in bicycle. So while we say that, uh, I mean. We are able to handle the scale today because we are able to, we are always thinking about handling the next 5 years, what can happen and what will be the scale, and we want to see to that this scale has to be supported, as I said, without any downtime and without any customer latencies and without any with all the new features coming up with the AI and things like that, we want to deep archive and things like that. So we thought to move the text and blog data that is saved in MyScycle today. That takes close to 2 petabytes of data for us because all the conversations, historical conversations in store, but again, the tickets and ticket bodies' data lies in the bicycle at this point in time. So we felt this is only a lookup content. There is no filtering or anything on the top of this, so this can move to a different store. We evaluated multiple stores for this particular operations and this particular use case, and we finally felt ScalarDB is again offering this low latency. And also due to the reliability and availability and scalability options which they have, we felt this can also fit into. And while I talk, the problem which we face today in MySQL is we wanted to upgrade the systems pretty often, like 6 months from today because of the kind of situation, and to migrate this petabytes of data without downtime is becoming more and more challenging. And whenever there are increased workloads from customers, like they wanted to process some historical jobs and they want to process some historical contents and due to some cyber Monday deals, the traffic. Increases we'll have to scale it on the ease and all this becomes a little complicated in the relational database world, but this is what s going to help us in benefiting. So there are two advantages. So we kind of bring the bicycle storage lesser, which means we can handle the bicycle store with more further ease and also we can scale up, scale down, replication and all that will happen within the. On the other end, where the majority of data is going to shift towards killer, we can actually scale seamlessly, vertically, horizontally in however fashion we want, and also do version upgrades patching without any downtime. So this is the advantage we see. And while we see that, we also want to migrate the petabytes of data, and it's not going to be a direct option. And we have the same challenges like incremental data and also the historical data migration that has to be done here. And uh while I say that, we kind of uh selected Kafka approach for this, for the CDC changes, the incremental changes, and then historical data comes through a Sparky format. So, uh, and when I talk about this, I'll also need to Yes, so we need to do two better bits of data migration here as I already said, and we kind of use Kafka connect WBCM backed CDC options from MySQL. This will copy the Vin logs into Kafka through events and then save all these incremental changes, and that can be put into Skier DB through Skier DB sin store. So this will actually again part of the Kafka built by Scaler's team. This sync sync. We read all the data from Kafka and push us to Skiller seamlessly, so the incremental data comes in and the bicycle from iCycle we'll use the S3 export option to parquet, and then Parque files can be loaded into Skiller directly and then we'll validate using our custom lambda solution so that the data is present seamlessly and we'll switch the read traffic initially to make sure that the read things are happening and processed fine with the Skiller systems, and then we'll switch the right traffic seamlessly. So that's the plan we have. And I've already covered the cycle use case and the Cassan use case. Moving on to the Dynamo use case today our activities, microservices, it sits on the top of Dynamo. So when I say activities, it's another use case, use case number 3, which we actually see some limitations because of the size. The item size in Dynamo is limited to 400 KB, which is actually pushing us to. Split it and then write it into dynamo. Dynamo is fine. That's completely cool. It's great to use as well, but again, this limitation pushes into multiple options. We'll have to split it into multiple records and then it and then fetch it again. All these logics are handling some throttling in our computer layer, so we wanted to see if we can migrate this to Scala because the limitations are not there as such and also. We want to scale this further and also be a federal compliant and things in the future, so we selected this particular option and for this again we thought to migrate both historical data as well as the incremental data, pretty much the same approach like MySQL, but different connectors are used here, so we kind of enabled the dynamo streams here. So dynamo stream. will actually have the same data captured in Dynamo and from there we can actually use our custom lambda to read these events from Dynamo DB and push to Kafka events. So this will actually take care of the incremental changes as I already said, and these incremental changes will be taken to ScalarB like using the same sync connector that is provided by Scalar team itself. Data receipts in Skiller at this point and the historical data will be taken from the Spark job this time. We'll run a Spark MapReduce and copy the data from historical data from Dynamo to Skiller DB and then move the traffic to Skiller like the same switch like read and write. So this is well called out here in 4 phases. The first phase covers migrating the schema from Dynamo to Skill DB. And then we'll also enable the dynamo streams, as I said, that's what it's called a JDBCDC dual rates. So this will take the data to Kafka and from Kafka we'll have to take it to Uskila that is covered in the phase 2 and phase 3, and then we'll also cover the check sync validations right that will happen through our custom lambda and then we'll have to do the read and right switch periodically. So with this, I handed over to uh Sre to cover the next future use cases at Scalla DB for Fresh works. Thank you. Thank you, sir. All right, thanks, Prem. So, uh, just to wrap it up, it's not just the traditional MySQL or uh uh you know, uh these use cases, but we are going forward, we are going to. Um, you know, have more like doub doubling down on AI. So what are the different use cases you're talking about? So there are two, specific use cases that we are looking at going forward. One is, uh, something called, you know, um, uh, we are talking about like a feature store where, uh, essentially like most of your features, most of your, um, you know, the use cases are, uh, trying to reach LLMs in some cases, uh, trying to reach agents. So all these conversations are kind of cached in so that way your traffic between your app and the LLMs are actually reduced. Essentially the overall round trips depend upon the performance of LLM as well, and when you make multiple calls, that will be a very noticeable latency. So this is one caching use case that we are looking at. So basically ultra low predictable latency. And the second one is the, uh, we are talking about like a feature caching as well. Uh, so, so yeah, this is the one I was just talking about, um, where, you know, how do you, um, so this is basically in between uh the uh you and the LLM essentially. So, uh, this is something we are here to experiment, um, and we are hoping that I think we'll see some good, um, uh, improvement here. Next one is the data store for modeling. So this is more like an MLOps use case. So FreshWorks has two sets of Models that we use in AI use case. One is a generative AI where we do not do any kind of training. We just go this is our AI and a few other ones. These are the large language models. We don't do any training. We just do inference with them, but we also have self-hosted models, these are smaller models where they are kind of optimized for some kind of very specific use cases in industry articles that we have. It's like something like IDSM. Our first. So these are the ones that we kind of train them, and these training cycles are becoming more and more frequent. And when we are working with this, we are also looking at to see how can we actually have, uh, you know, uh, CRDB, like, you know, in the training pipeline so that way, uh, we are talk Prim spoke about 2 petabytes of data, and, uh, all the data needs to actually go into this, uh, training pipeline. So how do we actually kind of do some kind of caching to speed this up, right. So, um, and in the future we are talking about like notification services, we're talking about URL shortening. This is again, um, the 75,000 customers that we have, we let customers to have customer domains and, uh, you know, create their own custom URLs. So in that space, you know, how do we actually use CRDB to create URL shortening service as well and then attachment services. So, yeah, essentially all the products that we have. Um, you know, fundamentally they are ticketing solutions and they will also have ticket bodies and attachments. So, till now, uh, we are looking at, um, you know, the MySQL alone, but then, you know, can we actually use this as an attachment service as well. So, ultimate takeaways, so as you have seen, FreshWorks always reinvents the technology, um, you know, every single year, full stack, right? Whether it's a cloud layer, storage, compute network data. Uh, it's the platform layer we are talking about multiple capabilities like email analytics and, uh, in a single sign-on or the AI or the products and uh Sila has been our partner in actually building some of the cool technologies and there have been actually co-build partners and uh co-market partners as well. So, uh, we achieved, I think a 10x of what we thought initially that we would, uh, otherwise do. So it's always like, you know, awesome working with them, uh, so with that. Um, I'm calling, uh, Felipe and the stage. Thank you so much, Tre. Uh, you guys basically didn't give you all the value CODB gives to Fresh works. 11 particular question I have for you is, since you guys are running Sila Cloud and a friend spoke briefly about CADB Cloud and your BYA set up, what's the main value BYOA gives to you? As a customer and for your customers as well, because in the BYY setup all the data is issued under your own management as well. Could you speak a bit more about that? Yes, absolutely. I think bringing your own account matters a lot, especially we are talking about, so Fresh Works serves our products in 5 different regions the US, Europe, India, Australia, and the Middle East. And when we speak about it. Customers also care for data residency, so they look at where their data resides and how it gets transferred. So we want to make sure all of our subprocessors also live in the same region and it doesn't the data doesn't go out the storage or in transit. So keeping all this in mind when we are talking about bringing your own account, essentially our entire data plane decides within. Uh, our account, so that actually makes it very easy in terms of access, uh, scaling for our own engineers, right? And, uh, that could be a deal breaker for us, uh, and, and, uh, since you enable us, uh, with that feature, uh, it made it so seamless for us to, uh, you know, that, uh, you know, do the entire migration over to CAB. Brian also spoke quite extensively about migrations and the different strategies you guys took and I don't want to choose sugarcoat it. I mean, database migrations are hard. We often classify, we like to describe migration database like an open heart surgery. How did CLB helped and simplify the process of migrating across multiple databases? Choose a little bit. How did your support team, work with you guys and helped you guys throughout all the steps involved to move your data over? Yeah, yeah, no, I think the analogy of operating on a patient is actually perfect, uh, but we are operating on a patient where the patient is still in all the senses and actually not, uh, you know, uh, like, you know, on anesthesia or anything. Else, so our customers expect us not to go down at any point. They, they don't care whether you are doing maintenance or upgrades. They want the service to be up and running all the time, and that's one of the best features of CRDB, right? So we could do a hot, hot, um, scenario where, uh, while the upgrades are happening or migration is happening, um, we could do parallel rides like Prem just spoke about it. And then reads, uh, progressively we kind of shifted based on the accuracy as we get more confidence so that way, um, but if it is RDBMS at least you'll have like a few seconds of downtime and uh that is noticeable, uh, you know, for our customers and then every. time there is a downtime, we've got to notify 75,000 customers two weeks in advance and then if somebody says no, then in some cases premium customers, you might have to even move the window. So in this case it's like you're trying to maintain or replace the engines of aircraft while it is still flying. Yeah, thank you so much Frida and uh thank you so much Pram. I would like to uh tell you uh thank you for your continued partnership and I hope we will help you out with this upcoming use case. So with that folks, I would like to thank you all for attending and listening to, uh, both of our speakers today. If you liked what we had to share, please remember. To rate uh our talk in the AWS uh events app and uh with that if in case you have any questions to Brian or to Shri or Prem, uh, we would love to catch you up here and uh you are also welcome to visit our booth at the Venetian booth 1747. Thank you folks and have a great day.