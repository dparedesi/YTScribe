---
video_id: DZ6MMauVrZg
video_url: https://www.youtube.com/watch?v=DZ6MMauVrZg
is_generated: False
is_translatable: True
---

Welcome all to today's session on GAI powered contextual security analysis and remediation. I hope you're all hydrated and energized after lunch. In this session, together, we are going to explore and find out how to secure your multi-tenant workload. I'm Ashwin Vas Devan, a senior solutions architect at AWS, and with me here today is Sahil Tapa. Thank you, Ashwin. Once again, welcome everyone. Uh, my name is Sahil Thapar. I'm a principal solutions architect. Uh, we're really excited to have you all here right in the middle of what we hope has been a really exciting week of learning and innovation for you all. In the session today, we're going to take you through a journey, a journey through 4 key parts. First, we're gonna talk about some of the key requirements that one needs to consider to design an effective security operations monitoring solution for a multi-tenant environment. Then we'll, uh, look at the solution architecture where we'll walk you through the, uh, various pieces of the text stack that we have used focusing on the GEI approach that we have used to solve this problem. Uh, then we'll run through a quick solution demo and following that we'll open up the code to do a detailed code walkthrough again focusing on the GEI text stack components like strands and, uh, agent core to show, uh, how you can implement something similar in your environment. And then finally, we'll close the session with Q&A where we look forward to answering your specific questions, uh, and in your specific scenarios. So let's jump in. So we, let's get started with looking at some of the key requirements that you need to consider when you're designing a security solution for a multi-tenant environment. The first thing that you need to look at is designing for robust data ingestion at scale. What this means is you need to design a solution that can ingest and process, uh, security telemetry from multiple tenants, and this security telemetry data can range to millions of events per day. So you have to make sure that the ingestion solution meets this kind of a scale. Second, in a multi-tenant environment, the tenant context is very important because every security event needs to be analyzed in a specific tenant's context, its security posture, as well as its compliance requirements. So that is one important thing in a multi-tenant environment that needs to be considered. Uh, and then, uh, In a multi-tenant environment you have multiple resources, so you need to clearly understand what is the relationship of the resources both within the single tenant as well as across the tenant. How are the resources interconnected? This is important because you need to analyze if a security issue impacts a single tenant, does it propagate to the multi-tenant environment or not. And finally you need to have a differentiated approach to find out if uh environment if a tenant is impacted in the multi-tenant environment is the security issue capable of propagating across the environment. So that is another thing that you need to consider. So we have considered these three main requirements to design the prototype that we have built out here. Uh, Ashwin. Thank you. So this is the high-level architecture of the prototype. So, uh, what we have here is the design of the SAS control plane and the data plane, and I'm going to explain how this all goes together. So the highlighted one is the data plan. So we have 3 tenants for the prototype, and the way we have set this up is in order to capture the alerts, the telemetry data, we're leveraging Native AWS security services, which includes security hub, guard duty, and inspector. Uh, and we have also leveraged the OCSF for open search, uh, open source project to actually build a pipeline. And, uh, ship the telemetry data from Security hub to, um, Security Lake and OpenSearch. And once this data is all available in OpenSearch, it's all in OCSF format, which makes it really easy to query and understand the security risks in the platform across all the tenants and we have other path as well where we want to actually understand the relationship of the infrastructure components within each of these tenants and that's the reason we're actually using the Neptune database. So, would you be able to, uh, tell our audience like how we are leveraging Neptune database and what's the significance of Neptune database in this entire design? Sure, so, uh, Neptune database is a fully managed graph database solution. Like I mentioned, in the multi-tenant environment, we need to understand how resources are interconnected both within the tenant boundary as well as across the tenant boundaries in the multi-tenant landscape. What we have done here is we have implemented uh AWS config aggregator across the multiple tenant accounts, and this aggregator collects the resource information, resource identifiers, and the resource relationships, and we've implemented a back end data pipeline that reads this data from the AWS config aggregator and it processes it and transforms it into a graph structure. And in the graph structure what happens is the resource information, the AWS resources get transformed into the graph node or vertices, and the relationships that the resources have with each other become the graph edges. So for example, in case of an EC2 instance. The EC2 instance itself becomes the graph node or the vertex and its relationship to the VPC like security groups and the other things become the graph edges and that's how we designed the complete relationship map of the resources within the multi-tenant environment. Uh, what we do is once this data is this data is transformed, uh, we, uh, use Gremlin queries in batches to insert this data into the Neptune database. As a result, we're able to create a complete multi-tenant relationship map for the multi-tenant environment that we have. Thank you. So now that we have all the telemetry data, we understand the relation, uh, between the different infrastructure components within each of these tenants. The thing is, like, now let's see how a soc analyst or an admin of this, uh, infrastructure can actually analyze the threat and understand how to actually, um, analyze and also plan to remediate some of these security risks. So if you look at this, the so analyst first logs into the, uh, chatbot which we call it as a security assist. I'm sure Sahil is going to give a demo of what it looks like once, um, they put in their, uh, they put in the chat message. So for, for example, they would want to see like, hey, tell me the list of all the severity one. Uh, risks, security risks on tenant one or across my platform and also like where, what is important, what is significant, which one should I prioritize and remediate first? So there are a number of questions which they can ask once they do that what we have done is like it goes to the. Agent core runtime. So, Agent Core runtime, uh, before I get here, I just want to understand like how many of you are experimenting with Agent Core or are actually actively using Agent Core at the moment? Nice, I can see a few hands raised. So agent code basically does undifferentiated heavy lifting and it's very easy to uh deploy to production and um it's got a number of, uh, features packed into it including observability, memory management, so on and so forth, and we have used some of these features, uh, for this, uh, for the solution of the prototype. So this agent core runtime now has uh we have created a simple one for this, uh, for the purpose of this demo, uh, and we have some tools associated to this as you can see here, the back end tool which actually queries the open search because as I said, all this OCSF data that got ingested from Security Lake all sits in open search and we want to give our agent access to these tools and we want to give uh tenant, uh, the agent agent. Access to the tenant information because it needs to understand what are the different tenants so that it can uh find the risks or security uh events associated to those tenants and then we have a remediation tool just to give you a recommendation on hey these are the ways to remediate and again there's a lot more that you can add in terms of the tool and also in terms of how uh you can design this so for example if you want to go in the path of graph or if you want to go with the path of swamp. There's a lot of ways to expand on this particular solution and can be used as a reference architecture. And given that one of the most important thing is like when a SOC analyst or an admin interacts with the security assist they would like to ask questions which is quite contextual wherein like you want to uh we we need the agent to understand what was discussed in the past and respond accordingly. And that's the reason we have deployed agent memory as well. We will take you through this in detail in, uh, in the, during the demo and also while I show you the code. Oh you Sahil, thank you. OK, so, uh, this will be a quick demo of the prototype application that we have built. Again, this is a prototype by no means this is like a production implementation reference that we're talking about here. Uh, we have kept our approach very simple. What we have designed is a, a single page lightweight react application, and on the back end it is getting powered by API gateway and lambda function. Uh, it's very simple, straightforward. What we have done is we've implemented 5 tiles out here. So typically like. We mentioned a security analyst would log in into this application and to get a sense of what is happening in the multi-tenant environment, uh, the tenant, uh, the analyst will look at, say, all tenants. If you look at it, what we've configured here in the prototype application is just 3 tenants, and with the 3 tenants, uh, you can see the number of alert security findings that have been flagged are upwards of 4000. So you, you, the very scale of these findings itself is something that we refer as, uh, alert fatigue. Uh, the teams are usually not having so much time to sift through all the 4000 findings to find out which is the alert that needs to be focused on. So always in such applications there needs to be a mechanism to filter the most important alerts that the security team should look at first. So what we have tried to do here is. We've given a filtering mechanism at level one where the analysts can look at what specific tenant they would want to focus on. So for example, in this specific case, if the data plane one tenant is the production talent which is most critical, the analyst would go and look into that specific filtering tab. And then from 4000 alerts it's come down to 9900 upwards of 900 alerts for this specific tenant. Again at the second level what we have tried to do is we try to filter it again. We try to filter under critical high and medium buckets. So now supposing the security analyst needs to look at say high alerts. So for high alerts, uh, in the second tile what we have done is we have tried to aggregate the resources which have been flagged with the findings and again we've given a number of findings we have not tried to overload the analyst with all the finding information in the second screen itself. Now supposing the analyst wants to look at why this resource, this easy to instance, has been flagged. OK, so there are 5 findings of this EC2 instance. The 3rd tile here is what I was talking about the importance of relationship map of resources within a tenant, and this information is coming from Neptune database, the graph information that we had captured in the Neptune database. What this highlights is this specific instance, EC2 instance, is connected to various other resources within the tenant. We have not implemented the crossing mapping, but within the tenant itself, there are other findings also that have been highlighted which are getting impacted by this specific EC2 instance. For example, there's a security group here. There is a road mapping here which has got, uh, elevated privileges. So those are the kind of things that give a clear picture of what the security analyst needs to focus on. So moving on from here, the security analysts want to find out what are the five findings. So the 4th tile is what we've tried to give a high level, uh, summary of what the issues or the findings have identified, and this data is coming from OpenSearch. So this typically has been the approach of security analysts using this kind of a dashboard. What we have tried to do is we have tried to simplify this by giving a GI approach to this where we have created a chatbot which is powered by strands and Agent Gore on the back end. And here what we're given is uh interface for the security analyst to ask questions in natural language, OK? So supposing here if. Similar functionality if you want to say list all tenants. So this can be the primary interface for the security analyst to look at and the same functionality that we were doing here manually as to how many tenants are there, uh, this analyst can ask in this chatbot as to list all the tenants and what we have done is here we have used HTTP protocol. So this is going to time out under 30 seconds. We didn't want to do, uh, webSocket because we didn't want to wait for this demo, but typically you should be looking at webSocket for this kind of an application. So they should come back with a response quickly. You might OK, so this you might want to swap to the, OK, I will. It's timed out, but I will go through some of the questions that we have used before. So the question here is how is the security posture of the multi-tenant system? Anything that needs immediate attention. So the similar stuff that the security analyst was doing by looking at the dashboard and trying to sift through each tenant, here's a simple question. Tell me, is there anything that needs immediate attention in my multi-tenant environment? Let's see how quickly this can get back. So here if you look at it, it, it tries to summarize the information here saying that there are overall critical findings 61. It gives what are the overall critical findings that we saw in the dashboard and then it's trying to break down, uh, per tenant basis. So if you look at it here, data plan one for our example, we assume that that was a production tenant. It says that there are critical findings at 29, high at 233. It is time to summarize the information in an easily digestible manner and provides quick information. This, this is the value add here. It clearly highlights what are the resources that need immediate attention. It says that there is an Internet Internet expo security group, which is a critical finding. Multiple security groups have unrestricted Internet access. So you see here the problem statement has been simplified to a larger extent where the security analysts can clearly focus on resources that need immediate attention. Another question that typically we see is. Is my tenant vulnerable to any kind of DDoS attacks? So let's see how, how this responds. So it clearly says that yes, Dalane one, tenant has got a few security groups which are open to the Internet. There's an EC2 instance that is, uh, publicly exposed without AWS shield protection that is enabled. So you see the value out here, right? It quickly gives you the actionable items that you need to look at to. Fix the issues, uh, on the back end, it is trying to get this information from Neptune to build the resource map as well as from OpenSearch where it has got all the findings and it is trying to read that data and through agent core it's trying to summarize it and give you very actionable information without wasting a lot of time. I should note to you. Yeah. So I Thanks. So I'll take you through the code. So you, while Sahil was giving the demo, you would have noticed that sometimes it takes time, sometimes it's really fast. So let me take you through how it's deployed and, uh, some of the nuances within agent core like what you can do to make it actually fast to actually make it more accurate in terms of the response that you received. So, uh, you know, like when we were actually, uh, tuning this, even for the prototype demo purposes, we iteratively had to go through a number of things when it came to prompt. So, uh, Yes, so the main approach that we, what we were running into is, like I said, we wanted to give a response time. We want to get a response time under 30 seconds because this is a chatbot. Nobody wants to wait for more than 30 seconds. So the initial approach was to look at the system prompt and along with the system prompt, if we can make the system prompt very specific, giving exactly what is asked from the, uh, agent. That was one thing. And then we try to implement a few short, uh, prompting as well as role-based prompting where we gave an example of what exactly needs to be looked at and what are typical kind of issues that the agent can find out and highlight quickly like we gave a highlight saying that look at the security groups, look at the S3 buckets which are publicly exposed. So these are some of the steps that we used with prompting to optimize the performance. Thank you so. Um, as you can see here, this is the main deployment file, and this is how Agent Core actually, uh, deploys the agent in the AWS managed account. Sorry. So, uh, what we have here is the handler that's the entry point of where the call first goes once the chat request or the prompt comes in. And then you have to define the role for the agent because now there's a lot of interaction that happens. For example, it interacts with the open search in order to retrieve the security information and uh it also, uh, it, it also talks to our other data source which has information about the tenant because we did talk about the three tenants which I showed you in the beginning. So all that information, it needs some rules and this is where we define permissions and roles and again autore ECR. So what this means is like basically. An image is created when you run this, and this image is created in the ECR sitting in your AWS account. And once you build and you deploy this, what happens is this gets pushed onto the AWS managed account and runs in a, in a Fargate, uh, in, in, in a Fargate, uh, environment using containers. So this is one part of it. There's another configuration that I have actually introduced because in my case I did not want the call the, uh, the chattiness between the agent and my infrastructure to traverse via the Internet. So, uh, here what I wanted to do was leverage the VPC to which this open search is actually deployed. And this is basically the configuration. So what basically happens here is it configures such that. Sorry. It it configures such that the ENI, it creates ENIs. In these VPCs and makes use of these VPC makes use of these ENIs for the communication with OpenSearch and uh that that's uh pretty much the network configuration and this is optional as well. So, as I said, here is the entry point, and this is where the action actually happens. And what it does here is like it creates this is when the agent actually creates uh the object and returns and it populates it with all the attributes as required. So, for example, when the agent actually, sorry, when, when the agent actually gets created. We have a few things that we need to pass. One is the model of choice. Again, we need to keep this a bit open because like not all models suit all use cases. So this is open to any model that you want to use. And here, as you can see, I have leveraged the trans agent framework, but again you could use anything that you want. There is no uh such limitations with agent codes, so feel free to use what you're familiar with. And in terms of tools, As you can see, there are a list of tools that I have used, and this is an example that I created for this session just to show like what else you could do. Um, in this example I've got one agent with all tools, but uh that's that's not the only pattern. There are a number of uh different patterns and you have to optimize based on your requirement so. In this case, uh, let's say that you want to understand, um, what the cost and usage report, for example, as a SAS administrator. You might want to optimize cost on your uh tenant and, uh, in order to do that you need to first get access to the parent payer account and underneath you need to see like how you can actually reduce cost. So this is where we leveraged the MCP server, uh, for billing and, um, billing, uh, cost and for billing cost and usage report from AWS Labs. So all we need to do is just add. The billing and MCP tools to the tool, uh, to the tool parameter of the agent. Here I have done, um, dependency injection. So as you can see above the billing MCP tool has got this self.billservice.get MCP tool where it's got all the definition and once it comes up with, uh, the list of tools, it basically unpacks all the functionalities that are available within the MCP tool and now all of this is available for the agent to query. So, um, before the session, I actually did a deployment of, uh, this entire, um, with this new, by adding this new additional, uh, capability, I added, uh, that and, um, I compiled it. So what basically happens is this deploys, as I said, the agent. This deploys the agent to the runtime and not just that, it also comes up with all the information. One of the key advantage here is like. When it comes to performance, when it comes to tuning, as you saw, like sometimes it takes, uh, there's a delay. Agent Core has this capability or observability built within it that actually shows you clearly like what are the different, uh, paths to which the, uh, uh interactions or communication happened, and you can see the latency at each of these path and optimize accordingly and here it comes with the information necessary for you to tail those logs. Now, let me uh quickly show you an example of this. So the first place to actually um test or see like once this agent is deployed, whether it's functioning properly, whether it is as per your expectation, whether you need to tune this further would be uh to go to the agent sandbox and here I have, um, since I have, I just deployed that you can see this uh demo and it comes up with the. Endpoint as well. So, comes up with the endpoint. So, I have some questions that I've prepared. For instance, if you want to understand, will savings plan help optimize the cost for my tenant to workload? And, uh, Let's run that and see what happens. So while Ashwin is doing that, what he has done is he's extended the agent's capability by adding a tool which is calling the MCP server. So, uh, Ashwin, uh, what was our thought process if you can just let everybody know, if you want to extend the capability, is adding a tool the approach, or are there other approaches as well? There, there are several approaches, uh, as I said, like, uh, with agent code is packed with features we have, uh, for now, just for the sake of simplicity, I have one agent having a lot of tools which may or may not be the right, uh, thing to do. Uh, for example, when you're actually using MCP server, you can also convert your existing lambda functions to be an MCP server. Uh, so there's a gateway available as part of agent core. You could leverage gateway, and Gateway has got a lot more things because it keeps track of all the tools that's available for the agent, and it provides, it gives it, it lets LLM, uh, uh, pick the right tool for the right prompt. Now, um. For example, here in this case it it used the MCP server and what it had to do was it had to talk all the all the way to the payer account to understand which tenant we're talking about and then it came up with this detail but again like for now it's used two tools as, uh, for example, it used the MCP server. It also used the tool on tenant information. So, uh, you can now have, if you have to optimize this, or if you have to, if the solution has to be evolved to fit your need, you need to take into consideration what would be the right pattern with should I have graph? Should I have specialized agent? Should I have a supervisor, and then I have a specialized agent to. Different tasks because all of this would finally help optimize the performance and uh the most important thing at least what uh I uh observed during my uh when I built this prototype was prompt engineering. The system prompt has to be accurate. The system has to uh has to be um as close as possible to how you want this agent to respond. So in this case, what it basically says is, um, It's asking you like when you mentioned tenant 2, are you referring to one of the specific AWS accounts? So it's asking us back the question and uh maybe um we, we could uh rephrase this question. While this is running, I'll also show you like how I have actually configured this. So I wanted everything to be configured, everyone, everything to be easily configured and changeable as well, because I wanted the flexibility when it comes to my model, when it comes to, uh, adding more details, uh, to my, um, MCP server or if I want to, uh, for, for example, uh, for memory as well. Now I'm using, I wanted to use just one single memory, and I wanted to use that memory across all the sessions because. Uh, for, because this is just a demo, and that would serve the purpose, and I wanted it to have the context of all the previous discussion. So I initially created the memory here and I have all the configuration associated to the memory. The memory strategy, as you can see, it has a flat structure. It has, uh, the name space pattern and, uh, so it tracks all sessions for the actor. Here the actor is basically the user ID with which Sahil logged into the demo portal and, uh, these are the environment variables required for the billing MCP server to work. And these are the open search settings, which is required, uh, for the agent to talk to the back end. And model, as I said, like, model should be something that uh you should have the flexibility to switch between model and see like which one works for your specific use case. So, again, everything is configured using the config here. I think it's still running. Yeah, just to put in context like the other, the, the evolution of this solution that we were working towards was now we're doing with this single agent solution, uh, basically threat detection. The evolution that we are working towards was threat remediation as well. So we were thinking of building another agent which can take the input saying that, uh, the issue that was identified, for example. Well, a security group providing open Internet access, a second agent in a multi-agent architecture that we were thinking as an evolution would take input as, uh, specifically with bounded privileges, seeing that we can instruct the agent to go fix the security group according to the best practices in a specific tenant account, and that is like a multi-agent architecture that we were thinking in terms of evolution of the solution. Yeah, so it did take a lot of time as Sahil said that we were while we were doing this demo we were contemplating for whether we should be using WebSocket or HTTP because sometimes it makes sense for the delay in the response. In some cases you want it really fast depending on what the use case is. Now it has come up with more detail because what it wanted to do was it wanted to reason out and it wanted to clarify, hey, you're talking about some tenant which tenant are you talking about? Then I said, yeah, this is the tenant I'm talking about. Now it's come back. And it said like, Is, is, does it make sense for me to have a savings plan or not? So, this is one way of actually using this. And another feature that I actively used when I was actually developing the prototype was the observability. So as you can see, like, the observability basically, you know, that, um, I recently launched the VPC demo agent, so you can see that, uh, there are a number of traces associated to this, and you have the information regarding, uh, the traces and the session details. So this actually gives me a way to look at like, hey, how do I actually optimize this? And how we use this was when it was responding for taking more than 30 seconds is when we looked at the traces and we saw that it is being very chatty, and that's where we try to prompt it accordingly so that it, it knows what exactly is being asked for and response was able to get under 30 seconds. So this basically tells me like how long it was. It took, it is quite a bit of time and we can start diving deep into like what does this actually mean. So it gives a breakdown of all these calls and where this chattiness happened and then based on this, I have a lot of information here and based off this basically I can start to um troubleshoot and optimize the performance. So I found this uh pretty useful while developing the uh overall model. So we designed that as a control table which is mapping of all the tenants and whether in typical in SAS environments you would see in multi-tenant environments you'll have a tenant ID, the tenant account number, tenant priority, whether it's production, non-production, stuff like that. That is the control table and that's what this tool is referring to. So here what I did was I, uh, gave one of the tools here is list_ tenants. So what it has is it has a tenant information that when I say tenant information, it has the account ID. The name and whatever additional metadata you want to add to that if you provide this so now the LLM knows where to go for this information. So that's how it actually in the example that I showed you, that's how it was actually able to relate like when I said tenant 2 I actually meant. OK. Do you use the reasoning acting sorry mode of the like so reasoning and acting like, OK, cool, thank you. Sorry. Um, thanks. How, how is the Neptune graph database updated? Is the agent doing that updating too, or how is that being? Oh, no, that's a backend pipeline that we've set up. The AWS config, uh, aggregator reads the information continuously, and there's a backend pipeline that runs every 15 minutes, reads the information from the config aggregator, and uses gremlin queries to insert that into Neptune. It's not, uh, exactly real time, but we do it every 15 minutes. And the back end pipeline information is there in the git report so you can have a look at it, yeah, uh, you showed one, error message that multi-tenant, uh, when we were running that right, there was some observability that some of the ports were open or. In in the demo that information is coming from cloudwatch or uh where it is because like uh because I've enabled the observability uh you will see that error message in uh CloudWatch and you can also view that from within the uh sorry. Are you talking about a specific error message or the message where the ports were open for the security group? Well, that's a finding that is a finding. That's a finding that has been highlighted from the initial architecture that we showed. We're configured security hub, guard duty, and inspector. Those are the services that are monitoring the specific AWS accounts. The findings are flagged from there. Those are the findings that get into open search after the processing of OCSF format. Sorry, I thought you're talking about that, agent. Thank you all. Thank you. Thank you for your time. Thank you for your time. Thank you. Thank you.