---
video_id: ZZNfJ5HyNCU
video_url: https://www.youtube.com/watch?v=ZZNfJ5HyNCU
is_generated: False
is_translatable: True
summary: "This session, titled \"Move fast & don't break things: Maintaining software excellence as you adopt AI,\" features Ganesh, co-founder and CTO of Cortex, who addresses the critical trade-offs engineering teams face when integrating AI coding assistants into their workflows. Ganesh begins by challenging the prevailing industry narrative that focuses almost exclusively on speed and deployment frequency, highlighting a sobering statistic from a recent survey: 70% of engineering leaders now rank security vulnerabilities and quality regressions as their top concerns regarding AI adoption. He posits that AI essentially acts as an \"amplifier\" rather than just a productivity tool; it accelerates code production but simultaneously acts as a mirror that magnifies existing weaknesses in an organization's software development life cycle (SDLC). Specifically, Ganesh describes a growing phenomenon where, as AI writes more code, developers inevitably understand less of the underlying logic, leading to a state of \"managing incidents in a haze.\" In this scenario, service ownership becomes increasingly ambiguous, and the pile-up of pull requests leads to cursory reviews, making troubleshooting during outages significantly more dangerous and time-consuming. Ganesh meticulously breaks down the second-order effects of these risks: quality regressions manifest as increased incident rates, higher costs of keeping the lights on, and breached SLAs, all of which distinctively erode customer trust. Similarly, security risks are surfacing as data breaches and secret leaks, which are particularly catastrophic for consumer-facing businesses where trust is a primary differentiator. To mitigate these multifaceted risks, he advocates for a pragmatic \"back to basics\" approach, emphasizing that sustainable AI adoption must be built on a stable foundation of engineering excellence—this includes rigorous testing frameworks, clear accountable ownership hierarchies, and robust production readiness standards that are enforced, not just suggested. He warns leaders against \"boiling the ocean\" by trying to fix everything at once and instead suggests focusing relentlessly on the specific bottlenecks AI is likely to choke, such as code review capacity and incident response outcomes. Central to his proposed solution is the strategic implementation of an Internal Developer Portal (IDP) like Cortex to systematically drive \"AI Maturity\" and \"AI Readiness.\" He explains how IDPs can operationalize these abstract standards through \"Scorecards\" that enforce non-negotiable requirements—such as mandatory secret scanning, updated runbooks, and defined ownership—before code is ever allowed to reach production. Furthermore, he challenges engineering leaders to fundamentally shift their measurement tactics away from vanity metrics, like the percentage of lines of code generated by AI, which he argues are irrelevant to business success. Instead, he urges a focus on customer-centric second-order effects, specifically measuring how AI adoption correlates with key performance indicators like Mean Time to Resolution (MTTR), cycle time, and incident frequency. The session concludes with a demonstration of how Cortex’s \"Magellan\" AI engine automates the tedious discovery of service ownership and pager duty rotations, helping organizations maintain the rigorous accountability needed to safely scale AI usage, ultimately ensuring that the modern ethos of \"move fast\" does not come at the unacceptable expense of \"breaking things\" that matter to customers."
keywords: AI Adoption, Software Excellence, Internal Developer Portal, Cortex, Engineering Management

Hello, hello. All right, uh, really excited for this talk. Um, I'm gonna introduce myself in just a second, but, uh, just a little bit of context as to why I'm excited for this talk. Uh, I think a lot of the conversations that I'm in these days are really about we gotta move faster, we gotta ship more, we gotta deploy more, but are we talking about the downside of doing that? What are we giving up? What is the trade-off? As engineers we always know every decision is a trade-off decision. Moving faster is a trade-off decision. You are giving something up along the way. Uh, but maybe you don't have to. And so how do you mitigate some of those trade-offs that you have to make along the way? That's what we're gonna be talking about today. Um, a little bit of by myself. I'm Ganesh, one of the co-founders and CTO at Cortex, which is an internal developer portal. So everything from cataloging all of your services and defining ownership to setting best practices and standards with scorecards to defining golden paths and letting you drive those things across the organization, that's our bread and butter. That's what we do. And so excited to talk today about maintaining excellence in the age of AI. So instead of like coming up here and talking a lot about what exactly that is, I thought we would take a different approach and start with first principles like what does that even mean? What does software excellence mean? What should we be focused on and how do we work backwards from there so that way we can kind of walk away from this with an understanding of how to break down the things that matter to your organizations as well. So let's talk a little bit what are leaders worried about today. So if you're thinking about software excellence in an organization, generally you're thinking about it from the perspective of the entire organization in aggregate. So let's break it down from what do people care about and if we know what people care about, then we can work backwards to what are the things we should be investing in. So this year we ran a survey across a bunch of injuring leaders and it was about AI adoption. It was about their concerns, things that they're investing in because we wanted to get a state on the industry and one of our takeaways was that 70% of injuring leaders are ranking security and quality regressions as their top concerns. Obviously there's other concerns in there as well, but these are the number two, this is the top two things. And if you think about it, this makes sense, right? We're introducing AI that's writing code that we don't necessarily understand. Uh, it could be leaking secrets, uh, into our code. There's all kinds of new risk factors that we're introducing into our software development life cycle. And so the things that are kind of the unknown here are what is it doing to our quality and what is it doing to security. But that's kind of like a high-level thing, right? Like, oh, quality and security, you kind of wave your hands and like, obviously, everyone cares about this, but what does that actually mean in the day to day? So let's go to the second-order effects of that and break that down one level further. So quality regressions first. What does quality regressions actually mean from a thing that you can measure, the thing that can actually drive impact on? Quality regressions shows up as more incidents, right? More incidents because you're shipping things you don't necessarily understand, you're trying to move too fast and you don't have the right guardrails. You might be breaching your SLAs or your SLOs because maybe you're introducing new performance concerns or latencies, you're introducing again new incidents. The cost of maintaining your business goes up, right? The keeping the lights on time. Maybe you're spending more time on incidents and resolution and escalations and bugs and things of that nature. But at the end of the day, the thing that this causes is customer impact, right? Your customers, whether that's another business, whether that's an end user, your customers are impacted and you lose their trust over time. And so that's kind of the business impact of quality regressions. And you'll notice that these are things that you can measure, right? in quality regressions, you know whether or not you're having more incidents. You know if your SLA is getting breached or your SLOs are getting breached. You know what your MTTR looks like. These are the things that kind of show up as second order effects of quality starting to regress. The second thing, security risk. This one is a bit more open-ended, but I wanted to focus on two things that, you know, we know coding assistants are, uh, are causing. In a recent meeting with a with a few engineering leaders, the things that came up were secret leaks and vulnerabilities and data breaches as being the two things that engineering leaders were most concerned about. And interestingly, the pattern that we found was that businesses that were focused on consumer products were the most concerned about data breaches because at the end of the day, consumer trust, especially if you're, if you're building a premium product. The, the data of your customers is extremely important. I'm not saying it's not true if you're an enterprise business as well, but it happens to be one of your differentiating factors of your consumer business. Things like secrets are another area of risk where if the human is no longer in the entire loop of writing code, then things that we know as humans we can do of not copy pasting a secret into our, into our environment files or things like that, they're more likely to happen now with the LLMs and obviously you can put guardrails around these kinds of things, but these are the types of risks that people are concerned about. So now we've gone from, OK, what are the concerns, security and quality, and what is the impact that we're seeing as a result of those two things being a potential risk. And so talking a little bit about, OK, why, why now? We've always cared about these things like, obviously we, we talk a lot about MTTR and incidents and things like that. Why is there a renewed focus now? Why are injuring leaders more concerned than they were before about these particular things? Well, let's go through the kind of the breakdown of what is happening to our software development life cycle as a result of this. Uh, if you looked at the recent, uh, report from the Dora organization, they also kind of came to a similar conclusion where AI is basically an amplifier, right? Things that you do well, it helps you do more of that. It helps you do, do, do those things well. Things that you're not so great at, it's gonna make those things worse as well. And so AI really is an amplifier or a mirror or whatever you wanna call that. And so, kind of breaking this down, we know AI is writing more of our code. My hot take, what percentage of code is being written by AI and things like that, not particularly relevant here. We know that AI is writing more code and that's getting shipped to production. Developers are understanding less of it, right? This was already a problem when us as humans were writing all the code, right? I understand for the most part, the code that I write, but my peers, my teams, my, uh, cross-functional peers and other teams may not necessarily understand that code either. But now, if I'm not the one writing my own code. Not only do I not understand it, my team understands it even less, and you're creating a pile up of pull requests that people have to review and so people are kind of going through those reviews even more faster than they used to before, and they're giving it less attention and so you're kind of slowly building up, you know, the, the, the, the tribal knowledge and the lack thereof within the organization. And then ownership becomes starts to become very unclear, right? It's, we already see this today at a service level, right? That was one of the reasons we, we founded a Cortex in the first place was clear accountable ownership of services drives behaviors that you want across the organization. The same thing holds true for your code, right? But if your AI is writing more of your code, you end up in a state where like. I, I don't know, like this, I, I didn't write this. I don't know much about this, but it doesn't matter if it's your PR or somebody else's, we still need to have accountability. And as a result of all of this, you end up managing incidents in a haze, right? It's a lot of organizations don't have clear ownership of their services to start with. Now all of a sudden you don't have clear ownership of your code. And you don't really understand what's being written or what's being shipped, and incidents are already, uh, painful. We're already kind of scrambling to find the right information in the right context in those moments. And now when we understand that code even less, obviously that's leading to increased risk during incidents. And so we know that AI is amplifying existing risk factors. These are things that happen today. These are things that are happening pre-coding assistance, and AI is just making those things worse. So now that we've kind of defined the concerns that we care about as injury leaders, what the impact of those things are and why coding assistants are causing impact in particular, let's break that down. What can we do to actually mitigate that risk? Well, let's go step by step. Well, we know here we're talking about AIs writing more code. Developers understand less of it. Ownership is unclear, and we're managing incidents in a haze. So let's think about the practices that can help us code quality and testing, right? Those of you who have adopted more genic strategies for, uh, your coding assistants have realized actually writing tests is a great thing. If anything, humans should be involved in writing more of those tests because we can define the guardrails of what the code should do, and then your agents can operate within those boundaries. We should, even if we don't understand the code that we're pushing out in its entirety, being able to know and detect when things go wrong is at least a good first step. So that way your customers are not realizing things are going wrong and you can catch those things first. So set up monitors and SLOs and all the things that we know about. And so SLOs can capture customer impact. We were talking earlier about how the thing that quality, uh, regressions leads to is customer impact. Well, if you can't necessarily measure the input into that, at least measure the output. Measure the things that are causing customer impact so you know when things are getting too bad. Security practices, don't let repos kind of linger out in the wild, right? This is one of the anti patterns that we see is like, oh, we're only gonna focus on the repos that we know are active or critical. Those are not the only repos with code. All of your repos contain code that's a potential risk factor. And so make sure that you're unders understanding the, uh, the impact across all your repositories and make sure that you're enforcing security practices across all repos, not just the ones that you know about. And then finally, bring the human, keep the human in the loop. Create clear accountability, right? Create up to-date ownership. It creates the right culture on the other side because if I know that the buck stops with me and my team for the SLOs and the quality of my services and the impact of uh for customers of the services that I own, then that creates the right behaviors where I'm gonna go back. I'm going to invest in in those tests so that I'm, I'm shipping better code. I know that I'm being held accountable for the things that I'm delivering, right? So making sure that you have clear accountable ownership across all of your systems is, is more important than ever before. And it turns out injuring leaders that we surveyed also agree with these practices. Every single injury leader we surveyed believes that security, testing, discipline and ownership clarity are the key things that they're focused on for safe scaling of AI coding assistance across their organization. But I want to just pause for a second. It feels like I'm probably regurgitating things that we've already talked about. We know that most organizations still don't have strong production readiness or security processes, right? Most organizations have. A vibe for lack of a better term, of a production readiness process, right? We, we kind of generally know, OK, these are the things we should be doing. These are, we probably should have monitors, we probably should have SLOs, we probably should have good code quality, but those are all kind of part and parcel of a good production readiness program, right? When we say before something goes to production, we wanted to meet all these requirements, not just as a checklist, but because these are the things that can help mitigate risk when something goes to production in the first place. And so the thing that I wanted to quickly highlight here is. This is not new. These are things we've been talking about for a very, very long time, right? We've been talking about production readiness processes and the importance of that for software excellence for a long time. We've been talking about ownership and accountability as a key thing for a long time. Testing is not, none of these things are new. These are things we've been talking for a long time before, but going back to the earlier point. AI is an amplifier, and we should focus on the things that AI is amplifying and focus on improving those things, right? We could boil the ocean and, and think about all the things that we can improve, but most danger organizations have a lot of things they can be improving at any given time. So let's focus now as we're adopting AI coding assistance on the things that are becoming bottlenecks or choke. Points in our STLC and we know that because of its quality things like testing and accountability and monitors and SLOs are the things that matter. Well, it just happens to be that these things are actually basic engineering foundations. These are practices that we know we need to be following, but because AI is amplifying those things, let's go focus on those things and make those things better. And so I wanna kind of zoom out a little bit and just talk about like AI excellence in, in the aggregate. So how do we roll out AI coding tools with confidence? So kind of summarizing the stuff that we talked about, you want to build on a stable foundation, right? You're not gonna go and build a skyscraper on a, on a wonky surface, right? You're gonna build on a stable foundation. So things like the right testing, testing practices and the right test frameworks, investments in CI tooling, investments in production readiness, security processes, those foundations will help you adopt AI. They'll help you regardless, by the way. So you should probably do those things anyway, but especially as you're adopting AI. Coding assistance, these things are more important than ever before. So build on a stable foundation. Second, you want to measure the impact of coding assistance on those metrics. When I say impact, I don't mean what a lot of people in the industry are talking about today, which is, oh, our AI coding assistants helping us move faster. That's only one part of the equation, right? The thing we've been talking about today is what is the customer impact. So do teams that are adopting coding assistance, not only are they moving faster, are they improving or decreasing quality? Are is their MTTR better or worse? Is are the number of incidents better or worse? So think about the actual downstream impact. So don't focus on like the AI metrics on their own. Focus on the second order impact, right? One of the things I see a lot of teams get hung up on is, oh, I wanna measure what percentage of lines of code are written by AI, and we can use that to correlate with, with incidents. My challenge to you. is why does that matter, right? We're not asking the question of like, the developers who use Vim or Intelligent, how do they, how do they compare? We just say you use your tools, what we care about is the, is the end result is the end impact. And so that holds true even now, but we can build that correlation of, OK, as we're rolling out these coding assistants, do we know the impact it's having on the organization? So the measure the impact from the customer lens. And then finally, make the most of them. They're spending a lot of money on coding assistants, right? And it is a new technology. It's a new set of practices that we're adopting across the organization. Uh, you know, a lot of folks here have been through their cloud journey. That was one kind of iteration of this where it was new muscles and you're spending money on AWS and you wanna make sure you're, you're getting the most of it. Coding assistants are no different. You're spending good money on coding assistants. Make sure that you're, you're taking advantage of all of them. And so when we think about the IDP or internal developer portal, this is the shameless plug for Cortex. We kind of see the IDP as a, a very powerful way of driving these three, parts of AI excellence. So, in phase one, driving readiness, this is one of the core use cases for something like Cortex as an IDP is defining best practices and scaling that across the organization. So customers for the for the past 5 years have been using Cortex to drive things like production readiness today. So, you know, customers in e-commerce and tax preparation would use Cortex for things like, hey, we're gonna get ready for Black Friday every year. We're gonna make sure all of our services are up to speed for Black Friday or for tax season. We know our security teams are using Cortex to drive security standards and shift. Now you can use Cortex scorecards to drive AI readiness, so making sure that every repository has an owner, making sure that every repository has secret scanning and vulnerability scanning enabled, that you're reporting on code coverage and that code coverage is not regressing. So being able to set those guardrails and then letting your teams have the autonomy to operate within those guard rails is something that scorecards are very good at. The second thing that Cortex provides is measuring impact with a very strong focus on the customer impact. So it's not just adoption rates or usage of AI tools. It's about the correlation between AI usage and things like MTTR and incidents and cycle time, the things that we know are bottlenecks and friction points. And then finally, AI maturity. So making sure that you're adopting the best practices. So this can be everything from, are we setting up agent instruction files in a repository so that whether we're using cloud or something else, we're able to make the most of them. Are we following best practices in ourger tickets if you're following Spe first development now. Do are your specs actually good? Are you, are you following spec for development? And how do you scale this across your organization? So how do you make sure that every repo, every team is adopting this? Because I think one of the challenges you'll, you're, you'll run into as you're trying to roll out AI coding assistance is that there's very different levels of maturity across different teams and individuals, and this is a new technology. And so we wanna make sure that we're bringing everyone with us. And so the same way that a production readiness scorecards helped bring the entire organization with us in into a lens of this is what good looks like when we're going into production. AI maturity scorecards can do the same thing for AI adoption, where we can say, hey, this is what good looks like. Here's how you can make the most of the coding assistance that you've been given access to, and then bring everyone with you along the way. And so a couple of things that Cortex does to make this journey a little bit easier, uh, Cortex has Magellan, which is our AI data engine, where we can automatically map your entire engineering ecosystem and catalog them. So when I was talking about ownership and accountability, that's a very hard problem to solve as humans in the loop, right? You might have thousands of thousands of repositories. How are you meaningfully going to go out and define ownership across the entire ecosystem? And so the approach we're taking there is, well, if we're trying to Help you adopt AI. Why not use AI to solve some of those problems in the first place? And so Magellan can help you automatically figure out which team is accountable for which repository and define that for you up front. Same thing for automatically mapping things like pager duty rotations. We also provide out of the box scorecards for things like AI readiness and AI maturity. We have out of the box AI impact, uh, dashboards as well, and you can query all this data through our MCP. So if you're an engineering leader, you wanna, don't wanna learn a new tool. Give them access to the MCP is very easy to query that, and then injuring intelligence allows you to measure the impact of these things directly within the IDP. Uh, wrapped up a little bit early. Want to make sure I was on time here because I do have a tendency to go over. We're, uh, 2 minutes left. Um, if you have questions, uh, you can find us at booth booth 450. Feel free to add me on LinkedIn, uh, for additional thoughts and, uh, happy to connect and answer questions there as well. Thank you all so much. Hope this was helpful and, uh, have a great rest of Green even. Thanks everyone.