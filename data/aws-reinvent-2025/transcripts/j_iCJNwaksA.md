---
video_id: j_iCJNwaksA
video_url: https://www.youtube.com/watch?v=j_iCJNwaksA
is_generated: False
is_translatable: True
---

There is nothing more important to a teenage boy. At this time of year. Than winning his fantasy football league. My son is obsessed with it. And there are tons of websites out there that will help prepare you for the draft, and they will go through all the players, look at every quarterback, rushing yards, passing yards, touchdowns. Trying to help you figure out how to who to pick. But everybody used those websites. So if I could pull all the available NFL data and perform some predictive analytics and help him. Pick some people, you know, down a few rounds down who will catapult him to the top. How great of a father would I be? Good morning. My name is Brian Ross and I run engineering for Sageic Unified Studio. We launched the next generation of Amazon Sage Maker in Matt's Keynote last year, and my team and I have been working super hard to evolve this platform to meet the needs of all of our varied customers who are trying to put their data to work to power analytics and AI. And I'm here this morning with Sarah Biani who works directly with customers like you to make them successful in their AI journeys. And our special guest is our awesome customer Justin McDowell, who runs the data platform at Carrier. Now this morning we're gonna talk a little bit about the next generation of Amazon sage maker, the types of problems that customers have faced. Which caused us to build this platform. We're gonna dive deeper on how to architect with Sage Maker to make you successful. Then Sarah will come on and show you a demo, including a lot of the new features that we just launched this week. And then finally, Justin will come and talk about Carrier's journey with Stagemaker. The data landscape. is experiencing a fundamental shift. And AI is taking center stage in everyone's enterprise strategy. When McKinsey ran a survey earlier this year. And they found that 78% of companies are investing heavily. In AI across all their lines of business. But over 3/4 of those have seen no meaningful impact either on revenue or on cost savings. And everybody wants to make this work. But they can't figure out how to do it at scale. And we're hearing the same thing from our customers over and over again. They're all going back to basics in order to build a strong data foundation first, in order to expose good data to power AI. And as they do that, they face a bunch of challenges. So first of all, we see the lines blurring between what used to be formerly distinct roles data engineers, data analysts, data scientists, AI engineers. It's no longer one group does something and throws it over the wall to another group anymore. And then secondly, customers want to embrace cutting edge technologies, but they don't want to throw away everything they have. They've invested millions of dollars sometimes into building platforms for data. How do you Get the benefits of AI, the benefits of all the new technologies that are coming out. Without having to rip it out, optimize what you have for higher value. And then finally AI hinges on data you trust. How do you have an end to end AI governance strategy? That can lead to trusted outcomes. So, from talking to our customers, we see that the the journey from data ingestion all the way to analytics and AI has roughly 3 distinct roles, although they might be performed by the same people. So data producers, they spend their time ingesting data from operational data stores and from external systems. They're cleaning, transforming, curating data. Providing clean metadata and then making it available for others in the organization to consume. So these are often data engineers using engines like Apache Spark, or they're ETL engineers looking for a drag and drop experience. Data consumers meanwhile are consuming that data and they're using it to build solutions to real business problems. They might be data scientists training a model or analysts be building a reusable dashboard to help drive decision making, or they might be building knowledge bases to power agents. And then finally data governors, they're responsible for enforcing the company's rules and compliance to make sure that the outputs conform to expectations like data classifications and data quality and guardrails, things like that. And today, each of these personas. Are facing some significant challenges. So first of all, data ingestion is hard, and it's always been hard. You have to find, create network paths to access all the data wherever it wherever it lives. Manage all the credentials, take the data that's in various formats and normalize it so that you can, you can compare it or join it. Then when you do this, scaling ETL to large data sets is really hard, right? There's a lot of infrastructure to deal with and even if you can figure out how to spin up and manage infrastructure, you wanna do that at at a reasonable cost and not pay for stuff that you don't need. And then how do you generate business metadata out of this, not just the technical data, but how do I really have columns and rows and, and, and, and tables that actually are understandable about people who are gonna use them. And once I've figured out all that, how do I productionize that so that I can run that over and over again while I'm not there and then monitor it so I see how things change day over day as the data coming in uh evolves. At the same time, your data consumers have their own set of problems. Everybody knows that the data that they need is somewhere in the organization, but where is it? How do I find it? And even once I locate it, how do I get access to it? How do I collaborate together with people, maybe some engineers and scientists are working together on a project. And once I've done some analysis, how do I share that with others who may not have access to the the same uh systems as I have or may not have the right skill set to understand how to use those systems? If I'm trying to do AI how do I find all of the pre-trained foundational models and customize them to my needs? Or how do I take the data that I've got access to and build knowledge bases to expose them to agents? And while they're dealing with those problems, the data governors have their problems. Everyone is trying to democratize access to data, especially in large organizations. But how do you do that while maintaining the right controls? How do you enforce rules? How do I make sure that data is provisioned only to those who are allowed to see it, and then I can take it away if I, if they no longer need it? And how do I ensure that the data being used in my organization. Is of a quality. That meets my expectations. And to solve problems like these is the reason that we built the next generation of Amazon Sagemaker, the Center for Data Analytics and AI. So the next generation of Sage Maker includes virtually all the tools that you need for fast SQL analytics, for big data processing. Data prep, data integration, model development, training, G AI. All in one place with a single view of all of your enterprise data. So you get a single development environment with Sagemaker Unified Studio. A lake house architecture. That unifies access to all of your data, whether it's S3, Redshift, external systems, SAS applications, other clouds, or on-prem. And with the SageMaker catalog built right in, you get end to end governance for data and AI workflows. And to support different personas and different skill sets. The unified studio is a single place to access all the tools that you need. To get your job done, so that's query editors, notebooks, visual editors all together, and the, and the tools are now consistent across all the services. Think one query editor for all the SQL databases that you access wherever they are. You can build your workloads together. You can test them together. You can collaborate together with others on them and then deploy them together for production usage. I was meeting with the European Bank a couple of months ago when they saw a demo of this, they said, we've been waiting years for AWS to finally solve this problem for us. Let's dive a little bit deeper into the workflow for the producer. So the next generation of Amazon Sage Maker empowers data engineers and analysts. To complete the end to end workflow producing high quality usable data for their consumers. Now that includes data ingestion from a wide variety of data sources, Amazon services, and also those of other providers. Once you pull that in, you use our ETL capabilities. You can do it through zero ETL or regular ETL. And regardless of what you choose, the compute will scale effortlessly to even your largest and most complex workflows. You can use simple drag and drop, or you can write Python, or you can write SQL. And it's all backed by a very flexible. Engines like EMA and Athena and Glue. You can be fully erless and ephemeral, or you can manage your own customized persistent clusters, but the tools all work the same way, so you've done all that and now you've produced your silver data. So next you want automatic generation of human readable descriptions of your data and and you you can use AI to do that generated understand your data and generate it automatically and also of course give you the power of manual curation as needed and please don't underestimate the importance of this step. When we started on this journey, and you probably all feel the same way, when you started on this journey. Business metadata was documentation for humans to better understand what's going on. But in the age of AI, You need both technical and business metadata in order to form a semantic understanding of your data, because the AI is the consumer of both of those sets. So getting this right is now critical. Now you wanna do automatic data quality assessments and anomaly detection, so that you and allowing you to customize the rules so that you can track quality over time and even prevent updates to the data if it falls below your thresholds. Now you've got your gold copy. So now you can publish the data. Make it make it make metadata available to others in the organization and have control over the visibility of that metadata. Within the enterprise and also easily grant access to consumers. Along the way you need repeatable serverless workflows to automate the pipelines all the way to production and DevOps pipelines. For deployment across your stages. And then no matter which of these stages you're at. Full lineage is automatically captured and made available to you to view so you can have visibility into the entire data life cycle. OK, great. So now you've got clean, high quality data ready to be used. And your data analysts and scientists are now ready to use it to build some real-time dashboards or train models or, you know, publish knowledge bases agents and all that sort of thing, right? Great. OK, so now SageMaker provides you the tools for this too. So first you're gonna search and discover all the data you need. You're gonna view the metadata, the business metadata and the technical metadata like I spoke about. And you're gonna say, oh this is the data that I need to solve my problem, and you're gonna request access to that. Automatically now the data owner. Can look at that request and approve access either to the entire data set or to a subset of it. And once approved, you can use our slick new notebook to perform data prep or ad hoc queries and generate visualizations. Now, but the discovery doesn't stop with just data, you can also discover pre-trained models and customize them to your needs. Using the data that you already just consume to train them to work for your business. Now you should pick a bunch of these models. Run some experiments in ML flow. And then pick the winner and register the winner. And deploy it through use and inference. And now you can use all this data to build and deploy agents to things like Agent Core for real world usage. So now you've got a producer flow and a consumer flow that really works for your business. So we've been talking all this time about how easy it is to use the unified studio to ingest and process your data and make it available for analytics and AI. And Unified Studio gives you a built-in set of connectors that allow you to bring all that operational data in from no matter where it is. But where does all this data go? Where do you store it? How do you do that in an efficient, performant way that's also cost effective? Now for many years already, the answer to this question of course has been Amazon S3. And customers love Amazon S3 for the unlimited scalability, durability, and low cost. And to date, customers have built over 10 million data lakes on S3. There are literally exabytes. Of parquet files alone stored in SB today. Parquet has become the de facto standard of course for storing this data, and it's one of the fastest growing types of data in S3. And with the emergence of open table formats, it's become really easy to create petabyte scale tables with trillions of rows. Taking advantage of the scale and economics of S3 and among these of course Apache Iceberg is now the default choice for everybody. Hopefully everyone is building the data lakes now on Iceberg. The iceberg metadata provides a robust framework. For a lot of really cool capabilities like transaction support. And time travel and allow you to evolve your schema incrementally. So last year at Reinvent we introduced S3 tables. And estuary tables are purpose built to store tabular data using Iceberg, a fully managed iceberg offering, and estuary table solves a few problems that you probably run into if you're managing Iceberg on S3 by yourself today. So first of all, optimize performance with estuary tables, you're getting about 10x the TPS of regular, uh, iceberg tables on S3. You also get simpler security controls at the table level controls. Including using lake formation grants. And probably most importantly you're getting automatic storage optimization. You're getting compaction built in, you're getting garbage collection built in without you having to worry about it, worrying about the the process or the infrastructure behind it. And all the tools in StageMaker Unified studio are fully compatible with S3 tables. Including the slick new notebook, which you'll see about, see a little in a little bit, our visual ETL editor and our SQL editor. But while we believe that S3 tables is the best place to store your iceberg data, we have thousands of customers who've already built and continued to evolve their data lakes on S3 using Iceberg on their own, and the unified studio has full support for any iceberg-based data. That you want, you can use all the tools in Unified Studio with your existing S3 data and the Glue data catalog. With or without lake formation security. In fact, If you have an externally managed Iceberg Rest catalog, we can now connect directly to that as well. So you can read tables in a remote catalog with all the analytics engines that AWS provides and apply permissions to them using lake formation. Right within the studio So now the choice is really yours. You can either have AWS manage your data lake for you with Eu tables, you can manage it yourself with Iceberg on S3, or it can be externally managed. And the unified studio is great no matter which of these things you choose. And speaking of catalogs, it's time to talk about data governance. So The next generation of Sagemaker has a built-in catalog for data and AI governance. It's fully integrated with the studio. You can securely discover. And access all the approved data, compute AI assets through pub easy to use publish and subscribe workflow flows without any IT intervention, so you don't need an administrator to be providing access to every single person who needs access to every single data center model. You can have the data consumers request access and the data owners grant it and then even revoke it later and monitor how it's used. You're getting semantic search powered by the Gen AI metadata. Comprehensive monitoring from data quality. And sensitive data detection. And even full lineage tracking. And it's not just for data. You can do the same thing with models and the number of assets that we're managing and governing within the. Catalog is actually continuing to grow. So a great governance system has to provide this, but it also needs to address the unique needs of AI. Like toxicity detection and bias detection, hallucination reduction. And for this, You have guard rails built right into Stage American Unified Studio and the catalog, so you can use in natural language describe the types of things that you wanna make sure are catered for in your organization. Say for example, When people ask questions about customers, you can't get out their personal information. You can talk about the how the how the customer is using your product and the sales and things like that, but don't give their name and address or something like that, right? And then that will be honored in the resulting AI. So let's dig a little deeper though on how customers are using this. To architect their enterprise governance. So all of our customers are looking to democratize access to data. They want to accelerate time to insight, but they need to maintain controls and share in a secure way. Now companies more and more are trying to do this using data mesh architectures. But data mesh architectures are like really hard, right? I mean let's be honest, they're super complicated and even those who build it don't understand how they work. And I've been, I've been showing customers for a few years now how to use data measure architectures, and I don't think I understand it either. The vision takes some thought and planning, so we try to make it really easy for you. And please my advice here is don't overcomplicate this. Your goal is to ensure compliance while giving data workers the freedom to innovate. So the SageMaker model allows teams the autonomy allows your teams the autonomy and the flexibility to work right within their own accounts. So here you use Sage Maic Unified Studio with your team. And you can freely share the data that you're ingesting, preparing, and creating with the other members of your team. And then when it's ready You would then publish that data to the enterprise catalog, and now other people can see the metadata. And then decide whether or not to request a subscription. So all this permits a more centralized control for those who want it, while allowing the governors to define business glossaries, enforce metadata assignment, and control metadata visibility. So the decision to how to architect this is really up to you. You might choose a more complicated architecture than the one on this slide if you really must. But if you keep it simple, You'll find that. Both your your data engineers and your data scientists will feel like they can get their work done and they won't hate you. And at the same time you can make sure that any data that hits the enterprise, let's say, has the right data classification applied to it. So whether it's, you know, confidential data or has, you know, PII or things like that, you can ensure that while people are, while many teams are working independently, it doesn't become the wild west. The goal is to ensure that everyone can get their work done without thinking about infrastructure or governance, but those things are applied to them in an unobtrusive way. So my team and I have been really hard at work the past year delivering new features on a constant basis into Unified Studio. Including a few that just landed, which I'll take you take you through. So one of the things we've heard from our customers is when we launched Unified Studio um we took a very nice beautiful clean approach and customers really wanted to. Provision access to their data using human end user identities and groups. And they wanted to think through their data architectures and build really nice clean solutions. But as it turned out, those journeys are really long for most people because you've got 10+ years invested in provisioning data to existing IM roles and managing federation into those IM roles. And so customers, while they wanted to go on this journey with us, they wanted to get started and get productive right away. So we launched IM based domains which allow you to take the existing federated IM roles that you already have and get started in Unified Studio right away. So within 2 minutes you're already querying your data using the permissions you already have without having to go through an onboarding process. Now next, one of our core goals has always been with Unified Studio to give the give builders a really easy to use and immersive and rewarding experience working with their data and their models, and we've raised the bar here with a brand new native notebook experience. The notebook starts up immediately. It's fully polyglot, so you can write in Python, in SQL, in PySpark. And not only write in all of these, but exchange the data seamlessly between them without you thinking about how to send data around between various engines. It automatically just happens for you. Spark Connect is built right in. You don't have to wait for a cluster to spin up or anything like that. Just start coding and and Spark is sitting there for you and automatically scales to the needs of your data. We've got great visualizations in there and a brand new data agent for G AI assistance which cannot just do the. Code generation that you'd expect from from any model or code debugging, but also can debug your infrastructure configurations of your clusters and can even do full advanced planning so you can describe your entire use case like I did with my uh with my fantasy football draft picker. And of course it's tightly integrated with of course the rest of the Sagemaker platform. If you want a deeper dive. On these new notebooks as well as also building CICD flows within Sagemaker, I'd encourage you to go a half an hour after this session ends up one floor on ANT 21214, and there's a chalk talk we're gonna take a deeper dive, uh, on this, so please come to that if, uh, if you're interested in learning more. And then the last one I'll go through um when we launched the platform, uh, we had great support for repeatable workflows and we chose Apache airflow as the basis for that because it's the best, most widely adopted and most flexible way to build repeatable data workflows. And we relied on on MWAA, AWS's managed offering for Apache airflow, and on top of that we built a visual editor and monitoring experience right within Sage Brick Unified Studio. But customers asked us to do a little bit better. They didn't want to have to provision an environment, wait a half an hour for it to come up, and then pay for it for the rest of their careers. And so we just launched the world's first and only fully serverless airflow service. And it's built right into StageMaker Unified Studio. So now you just create uh a workflow either visually or in code, whichever you prefer. And then schedule it and it just starts up right away, runs right away, you can see all the results and monitor it and you only pay for the time that the workflow is executing. So this is really going to not only simplify but give you much better cost control over the over workflows and I'm really excited that customers are are gonna get this. OK, so. Who is the most undervalued NFL quarterback in fantasy this year? I'll give you a hint, it's not Mahomes. And it's not Lamar and it's not Josh Allen. OK, maybe I will keep you guessing just a little bit longer. So, we have seen the problems that organizations are facing and how the next generation of Amazon Sagemaker can help you solve them across all of your key functions and roles. But I have just been telling you about it, showing you some architecture slides and all that, seeing is believing. So I'd like to invite my colleague Sarah Biani, who is a principal solutions architect in the analytics space. To actually show it to you. Thanks. Thanks, Brian, for sharing all the latest updates on what we have been doing with Sagemaker. I'm really excited about all the new features that we have launched recently, and I would like to show a demo to, to see that in action. But before we get into the demo, let's understand what is the use case that we are going to build as part of the demo today. So the use case that I'm going to do today as part of the demo is a is a fixational customer use case for a financial services firm. It's a global investment portfolio, analytics and AIML. The main use case as part of this overall company is to manage customer portfolios across multiple regions. They also look at risk metrics and regulatory compliance, and they want to build an ML model for detecting the risk and also for portfolio optimization. So let's look at the various personas that we will see in our demo today. Now these are primarily the data worker personas who will work with Sagemaker and try to accomplish their jobs. So first we have business analysts. Business analysts typically looks at data exploration and validation. They would like to go and define business metrics and KPIs. So we will see how business analyst interacts with Sagemaker and does his work. Second, we have a data engineer. She wants to build ETL pipelines to do transformation. She has multiple tables which you want to join and then create a flattened table for another application. Next, we will see an ML scientist in play, where the ML scientist will go and explore the different models available as part of SageMaker AutoML and then use them to build and deploy a machine learning model and an inference endpoint. Finally, we will see a GAI builder who is going to look at what other different foundation models are available as part of Sagemaker, and can they deploy a model as part of their organization. But before we go into all of these data personnels, as Brian has been talking about, you know, like we have simplified the experience with the IEM-based domains. I wanted to give you a glimpse of how easy it is for a platform admin to go and set up this entire process, right? So, we'll, we'll look at the demo, but let's look at the architecture. So if you look at the left-hand side, right, we have all the four different personas out there, uh, who interacts with SageMaker Unified Studio as part of this, uh, process, right? And as part of SageMaker Unified Studio, again, we have all of our known services, including Amazon EMR Glue, Sagemaker AI and MWAA which we will use as part of the demo today. From there, the personas interact with S3, uh, uh, S3 buckets. And tables stored on S3 tables in the icebox format as part of the Sagemaker Lake House going through the glue catalog, and then the permissions are controlled via lake formation to enable that. Also, we will see that we have data stored in Snowflake as part of this use case and how easy is it for a data worker to go and connect to Snowflake and consume that data. All right. So let's go into the platform admin part that I was talking about, where the platform admin wants to go and set up Sagemaker IEM domain and create the projects. So let's see that in action. So I'm on the AWS console, and from here, I'll go and click on Amazon Sage Maker. Now, as I come here, I see there is a get started option. I click on that. From here, I see there are options to set up SageMaker Unified Studio, and I see I can either have SageMaker create a role for me or I can specify an existing IEM role which has access to the S3 tables, glue catalog, and other resources that I need for this particular use case. So I'll go and pick up one of the roles that already has access to all of these resources. And apart from that, you will see that I have S3 table integration enabled by default, so I'll keep it as is, and I'll go and say set up. And I see that the setup has been completed. Now that the setup is completed, I see that I'm greeted with an admin project. On the left-hand side, I see all the various tooling that is available to me. I can click on the user profile and see the IEM role and the federation role that I came in. I can go to the domain management as an admin and go and create different projects for different use cases. I see there is a portfolio optimization project already created uh for, for today's demo. All right. So we go into the portfolio optimization and the first persona that we will look at is the business analyst. As a business analyst, I'm responsible for analyzing uh the uh top performing securities by unrealized profit and loss as part of this use case. All right. So we go in as a business analyst into Sagemaker Unified studio. Now, the first thing as a business analyst I want to do is I want to find out what are the different tables available to me as part of the use case that I'm trying. So I go and hit search and I enter financial as the term. I see there are various glue tables available to me. There are some models available as well. I can go and click on one of the tables. When I click on the tables, it brings me into the data catalog view where I can see the schema of the table. I can also go and click on preview data to look at the data in action. And then I can also click on details to look at various details about the tables, including advanced properties, like what are the formats and other things, right? Perfect. Now, next thing I want to see is like I have an S3 table with Iceberg, uh, which is an audit table as part of the business analysis. And this table stores all the metrics related to what actions users are performing as part of this use case. So I go and open that and I click on query. As I do query, I see that I'm opened in a query editor, and that the query opens there, and I can see the results here. Now, as a business analyst, if I wanted to download these results, I can do that and send it via email to my management if I need to. I can also do a quick visualization by clicking on the chart view, and I can go and change the different parameters like X-axis, Y axis, and do a couple of other things. Plus, this is a SQL notebook. Which offers you option to add markdown and SQL. I can select the different catalogs which I want to work with, and I can also do a one-click scheduling for the SQL notebook. Finally, I can save this and run the final query that I want to do as part of my business analysis, which is the top performing portfolio questions. So here I see the results, and that's pretty much as a business analyst, my job is done. If I wanted to go and schedule this notebook further, I could do that. Next, let's look at the data engineer persona. Now, as a data engineer, my task is that I am building a web application called Portfolio Advisor for this use case, and I want to build an internal data product or a flattened table that will be used as part of this web application. All right. So let's see the data engineer in action. Now, as a data engineer, uh, on the left-hand side, I see that there are various tools available, but I want to see if there is a low code, no code option for me where I can go and visually build my ETL pipeline. So let's see that in action. So I'll go and click on visual ETL which is our uh ETL tooling. Uh, I can click on create job here. And as I do that, I can see that there's an option where I can even describe my entire ETL pipeline as a prompt. But for today's demo, we will just do a drag and drop and build that pipeline. So first, I, uh, take the Glue catalog and I pick up the Financial Services DB and the table customers. And what I want to do is I want to make sure that this table does not have like any null records are dropped out from this table, so it's a clean table when I'm using it. So this is my clean data customers uh ETL pipeline. And as part of drop nulls, I say that I don't want the customer ID to be anytime null, right? If it is null, then drop those records. I save that and I do the other thing for the transactions table as well. And then I click on run. And apart from that, I have the option to go and create schedule as well, right? So I can go and run this ETL pipeline on a schedule. I can also click on view script, which shows me the code that is generated. This is an open source spark code that is generated behind the scenes as part of the visual ETL. I can also clone it as a notebook and download it. Let's go and click on view runs. I see the run has succeeded. Pretty good. Now, let's go and see what we do next. We'll go back to the visual ETL and I see that I have both the ETL jobs. Now, I built the final ETL job, which is a flattened, which is joining transactions, customers, and holdings table, and I run it. Now, as it is running, uh, what I can see is that I have output logs generating here like a tail, so I can also see what is happening with the job. But hold on, what happened here? I ran into an error. What do I do? I have an option to do troubleshoot with AI. And what troubleshoot with AI does is it not only tells me where the problem is, but it also tells me the code where the problem is. I see the problem is that I'm joining two tables which have the same column names, so I need to go and rename one of the columns, and the holding table's called quantity to holdings_ quantity. That's what I do, and I save the job, and then I'll go and run it again. So the job run started. I go back and click on view runs, and I see the job has succeeded. So AI came to my rescue for helping me solve that problem. Now, since I have built all these 3 jobs, I want to create an orchestration so that I can schedule this. So, as Brian was talking about, we have the server SMWA integrated into unified studio. So I take advantage of that. I go to workflows, and I say create new workflow. So I'll go and uh click on create new workflows. Uh, I think it should take a second. Uh, it's doing this thing. There you go. Yup. So create new workflow. And when I do that, I see, I have options for various operators in MWA and I have even the SageMaker Unified studio operators. I take the data processing operator, which gives me the Glue task as an option. And here, I can go and give a name to the task, like I have the clean. Uh, data for, you know, like customers and other things that we did as part of the glue ETL job. So I do that and I finally build my workflow here. And under actions, I can even go and look at the code and see that it has created the entire airflow deck for me without even writing a single line of code. So very beautiful. I can also download this if I need to and run it in another place if I want to. And then here I can go and schedule this workflow, right? So if I wanted to get this new flattened table and the data product created on a nightly basis and refreshed it, I can go and set up the settings here. So awesome, right? We could see that how the data engineer can do their work. Now, let's move on to the next persona, which is our ML scientist. Now, as an ML scientist, my job is first to understand what are the risks across the customer portfolios that's, that's happening as part of this use case. And once I understand the risk, I want to build an ML model which can do portfolio optimization for these customer portfolios, right? And obviously, I want to at the end, deploy this model for doing real-time predictions as changes are happening on customer portfolios. All right. So we again go back here. Now, as an ML scientist, what I also see is that um I have some of my data sitting in Snowflake. It's not in glue catalog or S3 tables, right? It is sitting in Snowflake Horizon catalog. So let's see that in action, how ML scientists can go and connect to Snowflake and consume that data. So I'm in Snowflake right now here, right? And I go and search for the database, financial portfolio analysis in the Horizon catalog, and I see the database. I click on that and I can see various tables under the public domain, uh, skiba, right? Analyst ratings, currency exchange rates, and economic indicators. So I want to connect to that. So what I do is in Sagemaker Unified Studio, I go to connections and click on create connection. I see there are various connection options available here, including Snowflake. I can click on that and I can fill in the parameters, which I skipped here. Uh, and then I see that the connection is created. Now, I go back to my data catalog and under connections, I can see all the 3 tables showing up from Snowflake for me within Unified Studio. I can click on sample data and see the data from Snowflake coming here. Also, as a data scientist, uh, typically, I would like to work with S3 bucket and upload files or download files. So there is also a good option in the data catalog where I can look at the 3 buckets and do that. All right. So now, the data scientists typically works with notebook experience, right? They love that. So we have this new notebook experience that Brian was talking about, and we have the polyglot notebook experience that you just saw with the various options where I could select one of them. I do a query, I can filter the results here. I can also sort the results quickly. And then I can even do a visualization on top of this data using Python itself. I can change the chart type from here easily to a pie chart, and it makes it so easy to do that. Now, next, I'm doing a query on the snowflake data that we connected to. So you can see here on the right-hand side that we have selected next to SQL, the snowflake. Connection type. So I see the data. Now what I will do is I will take the output from both of these cells. Like I queried the glue table and I queried Snowflake, and I'm joining these two using Python, and I can see the data in the notebook coming from both the data sets. Now if I wanted to use the output of this data and create a new cell which is of type chart, I could also do that by Selecting the data frame, which is our Panda's data frame from Python that came out of the joint, and I can do a quick visualization. So this is like amazing, you know, like I can do all the visualizations, all the queries across multiple data sets, no matter where the data is sitting. And then I continue my risk analysis by querying some of the other tables like customer portfolio summary by region. Uh, hold on, I went into an error again. And I need some help. So I click on fix with AI and I see the data agent that Brian was talking about comes in action, and it starts printing out. Yeah. So the, the data agent comes in play, and it analyzes what's happening across this particular cell, why the error came. And it actually understands that this error is coming because I'm trying to run a query with Python, but the, but the table is actually in glue, so it needs to go against Athena SQL for for running this query. So it understands that and it goes and rectifies the error in the cell. And it tells me whether I can accept or accept and run this particular cell or reject it. So I can see all the changes with the version differences, and I go and accept those options, and I see that it has updated my type to Athena SQL, and I can go and run this particular query. So let's run this query, and hopefully it should get succeeded if the AI is good, and I see the results. So, so that worked for me. Now, I continue doing my rest of the analysis, which I think I will do like in a fast forward. We're doing some plotly graphs and other things as an ML scientist in the notebook. So all of those features and libraries are already available out of the box. And finally, I calculate the risk ratings for these customer portfolios and look at the top 10 highest risk customers uh by, by value. Right. So I can see all of those results. And since I was running this as a Spark code, I would like to look at the Spark UI and analyze what's going on. So we have an option where you can do a one-click uh to the Spark UI and you come into the Spark UI. Plus, as a uh typical Spark developer, I would also like to look at the driver logs in case something goes wrong, so I can do that as well. So, pretty, pretty easy to, to do all of that. Now, next thing I'm doing as an ML scientist is I want to start building my model. And for that, I want to make sure that the experiments are tracked with ML flow. So that's where I'm in MLflow and adding all the details for the MLflow server. So all good. Now, we'll go back to the notebooks and start building the model using Sage Maker AI to, to do the portfolio optimization and recommendation. So first thing I do is I have the notebook and I run the boilerplate code to get the project details and the S3 bucket and all those things. Next, I go in and I start selecting some of the data from another table, portfolio optimization. And again, I run into an error where I'm connecting to the ML flow. I see the AI detected that I have an ML flow server in my project, but the name is different and it corrected it. So very good. Next, I continue with my SageMaker AutoML job, right? So I, I use the AML features to, to build the job. And I can see the updates in the ML flow. So I go to ML flow, I see there is an AML portfolio, uh, job created here. I can look at that. Plus, I can see that on the left-hand side, I have other jobs as well, like fraud detection and other things that I've been doing. I can click on any of these options and look at the model metrics as well. So it makes it so simple that I can jump to ML Flow and look at all the metrics. Since we used AutoML with Sagemaker, there should be training jobs that were launched, so I can go to training jobs UI and look at all of the jobs. And obviously, I want to deploy an inference endpoint after doing the trading. So I go to the inference endpoint and I see the endpoint is deployed. Now, finally, since the endpoint is deployed, I want to run a real-time inference to test it. So I will go into the notebook again, and I have some sample data here to go and test the inference endpoint and the model before I put it into production mode. So I go and run that, and I can see the Scores coming out with the portfolio recommendations as part of that. So this is beautiful, you know, like as an ML scientist, I was able to go in and build my model, do the risk analysis, do visualizations, use Spark, use Python, connect to Snowflake, all of that as part of Sagemaker Unified Studio in a seamless fashion. Now, next, we will go into the uh GEI builder persona. So let's go into that. I don't know. Oh sorry for that. Yeah. So as a GAI builder, since I'm working with the financial services uh domain and the project, I have a restriction that I cannot use the publicly available uh G AI models out there, right? So my organization has tasked me to deploy a model internally that will be used for powering the chatbot agent as part of the web application that the data engineer is building, right? So I want to see what are the options available to me as a GEI builder in Sagemaker Unified Studio. So I go in here and uh as a GEI builder, I see under AIML there is an option for models. I click on that. I see there are various models listed from various providers like meta hugging face, open air, and so on. I'm interested in a Mistral light model for my application. So I go and search for it, and I click on it. I can see the overview with the licensing details and the way to use the payload, and I can open this in a notebook as well. So that's what I do. I open the code in the notebook and I go and run that. So I see that the model is getting created and deployed as an inference end point. And within a few seconds, we should see that. And while it is getting deployed, I also can go and look at the details, like what instance type and other things that the model is getting deployed with. Right, so let's close that and I come back here and refresh, and it should be in service. So it looks like the Mistra light model is available for me, and I can now do inference against it. So let's go back to our notebook and I can see that I have some predictor and I have the option to invoke the endpoint. There is an Example, payload as part of this notebook that Sagemaker Unified Studio provided me, and I can see that it works fine. It gave me an input and the output that I was expecting as part of this model. Finally, if I wanted to go and delete this inference endpoint, if I did not like the output from this model and I wanted to use something else, I could go and do it. Right? So as you saw in this demo, multiple personas can use the new SageMaker IEM-based domains and the amazing capabilities we provide as part of the new notebook experience with the built-in data agent. This can help you build the future state architecture and data strategy for your organization. And for today's demo purpose, I did not write a single line of code for this demo. Everything was generated using the data agent as part of the notebook to create the demo. With that, I would like to invite Justin from Carrier to talk about their success story. Thank you, Sara. Hello everyone. My name is Justin McDowell, and I lead data platform and data engineering for Carrier. Carrier is a global leader in intelligent climate and energy solutions. And our heritage goes back to 2002 when Willis Carrier invented the modern air conditioning. Today we operate in nearly 160 countries with 48,000 employees and a portfolio of 35 brands. That scale gives us massive complex data landscape, and it sets the stage for our modernization journey. Our data strategy. Had to support 4 business priorities. Customer centricity. Deeper insights and more tailored service. Energy ecosystems where our products and services are increasingly integrated. Digital solutions with connected devices driving real-time data needs. And margin expansion. Requiring efficiency and avoiding lock-in. These priorities shaped every architectural decision. That context exposed 3 core challenges. The first was data sprawl with fragmented systems and rising legacy warehouse costs. Governance with inconsistent lineage. Access and data quality. And lastly, scaling operations. Every project rebuilding pipelines from scratch. These pain points led us to rethink our approach to a data platform which internally we call Nexus. We address these challenges with an iceberg-based data lake. On AWS, excuse me, iceberg-based data lake house on AWS, there is a difference. Data flows from SAP, JDE, and Salesforce. And other systems through click and kinesis and also glue zero ETL. The ingestion account separates our producers. From the governed lake. And in the data lake account, glue, the sagemer catalog. And Iceberg managed schema, governance, and acid table creation across raw, bronze, silver, and gold layers. Sagemaker Unified studio in addition to the notebook experience. Also provides us with lineage and access control all in one place. And for the workloads that still require a warehouse, we integrate Snowflake as a reduced capacity consumer. So how do we scale this? The first way is to ensure that each line of business gets its own AWS account. Those producers all follow the Nexus Lake House standards. For raw data, published data with glue, data quality, and lake formation all built in. A centralized governance account. Becomes the single portal for access, lineage, and metadata. Teams publish and consume data products consistently without custom engineering. And this gives us federation with control. So how long does something like this take? Well, let me take you through that process. So we started December of last year, shortly after reinvent, with an Amazon-style PRFAQ and architecture workshops. By March, non-prod was live with our first model data products. In May we onboarded our first production workload. And we've spent the rest of this year focusing on onboarding products, um, projects, and the developer experience, making it even easier for teams to build and publish data products. Nexus is already delivering measurable impact. 66% lower implementation costs versus our legacy pattern. We have standardized pipelines that accelerate delivery and reuse. And a 38% improvement in accuracy for our natural language to SQL agents. So if you didn't know, now you do, that clean governed data pays off immediately. What's next for us is doubling down on our intelligence layer on this platform. We're focusing integrating more MCP servers to extend coverage across more domains and have tighter integration with AWS services. We are also deploying Agente workflows that automate lineage, quality checks, and publishing. This accelerates delivery while strengthening governance. I'd like to share with you 3 key lessons that emerged from this process. The first lesson is choose open formats. They give you flexibility as the technology evolves. The second is governance is not a retrofit. Build it in from day one. And lastly, use AI agents responsibly. Automation is the only way to scale governance and quality without adding headcount. These principles have guided every decision we've made. I'd like to give a special shout out before I hand the mic back to Brian to my AWS account team. Uh, Bobby, Keith, Yanni, and Sachin for helping us on this journey and helping us achieve these results. Thank you. I'd like to thank everyone for coming and joining us this morning. We hope that you learned something that you can take home about Amazon Sage.