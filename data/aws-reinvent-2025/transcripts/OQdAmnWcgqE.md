---
video_id: OQdAmnWcgqE
video_url: https://www.youtube.com/watch?v=OQdAmnWcgqE
is_generated: False
is_translatable: True
---

Good afternoon. Please put your headphones on. And can you put your hands up if you can hear me, please? OK, if you can't hear me. There's people to help you, so make sure you flag them down. OK, good afternoon. OK. Again, put your hands up if you have ever found yourself frantically searching through a sea of logs and disconnected telemetry. To pinpoint the root cause of an outage of your distributed application. Lots of us, right? Most of us. Some very lucky people who haven't gone through that. This reminds me of an incident that I have back when I was an AWS customer and I was an SRE. I looked after a financial application and we had an outage where no new logons would take place. All the existing sessions were fine. The authentication services looked fine, everything looked fine, but we, we couldn't have new people log on and we had unhappy customers. My manager was hassling me, asking me when is this gonna be fixed? Can you jump on the bridge? Can you tell me more about the problem? All whilst we all were trying to just find out what is going on. Long story short, it was an expired client certificate. But as you know that could have been anything else. Sometimes you just don't know what you don't know. But the good news is that with the right tools and techniques you can make your life easier in these situations. As observability focused solutions architects, my colleague Helen and I help customers drive operational excellence for modern applications, and that's what we're gonna cover today. An agenda for the session today we're gonna first very quickly cover challenges with modern applications and then we're gonna move on to must-haves for operational excellence. We're then going to dive deeper into distributed tracing and open telemetry then we're gonna do a demo. Then we're gonna show you how to fill in the gaps in your telemetry. And we're gonna summarize the session for you. So let's get to it. Once upon a time. There was a monolithic application. It was hard to scale, it had lots of tightly coupled dependencies. Even one bug would mean redeploying the whole thing. And it made it very, very hard for teams to work independently. Hence the apps we build these days are modern application meaning they run on microservices, they run onurless technology, short lived resources, and they give us that agility and flexibility that we were missing. So this might seem like a really weird question. Do we miss anything about monoliths, Helen? No, not really. Certainly don't miss how long we had to wait to get a single bug fix to our customer. I certainly don't miss, miss how much downtime we had to have to deploy those changes. Or how often we had to jump through hoops if we had to extend that window. And I really don't miss the stress I saw my team go through every time they had to go through one of those days, so, nope, I don't miss it at all. Yeah and I get it and ultimately I don't miss monoliths and I, I don't miss everything that Helen's just covered, but there's one thing that I do miss. And I don't know if you're, uh, you know, know what this is, but this is a KVM and I don't miss working in a data center, but what I do miss is the fact that if I, if there was a problem with one of the applications, I would walk into the data center, pull out the KVM. And all the data that I needed would be in one place. And if I really wanted to know what's happening on the database server or the reverse proxy, I'd change the KVM input and it would be there. Now if you think back on the example I used at the beginning of the presentation. I had disconnected telemetry everywhere. I didn't have a clue which services had direct dependencies on each other. So again, I do miss that part and only that part about monoliths that were simpler to observe. But like we said, You can make modern applications be more operationally excellent with tools and techniques. So we're gonna start with must-haves. Thanks, Anya. So let's take a little bit of a look at some of the things we consider to be really key for your operational excellence. So it might be obvious, but it's really worth restating that metrics are the best place to start. Always. If you've ever listened to an AWS principal engineer talk about how they observe Amazon at scale, you'll know that they are metric obsessed. Now business metrics are the best place to start. The specific metrics are gonna be different for every business. What is important is that they tell you whether your workload, your application is performing for your business and more importantly, when it's not and your business is impaired. Now tech metrics are important too, but we need to spend more time asking ourselves why. Let's consider low bouncer latency. What is the impact of a change in that latency on your business? Is it a backend load bouncer serving a batch job overnight? What difference does a change in latency make to your business there? Or is it a customer facing load bouncer, where the smallest change in latency can lead to customer frustration, and your business is gonna want that resolved really quickly. So the important thing is to be really deliberate with your technical metrics and keep asking why, until you get back to the business KPI that that technical metric maps to. Now the next thing is to standardize the way you name your metrics and what you collect. So that no matter where you are in that distributed application, you have the same view. Here, metrics like availability, requests, errors, duration, we call them the golden metrics or the red metrics, these are really valuable for you. Why Well, they help you understand how to measure your application health through the existence or absence of errors. And they also help you measure the quality of responses through availability and latency. And they help you with those unknown unknowns that Anya talked about. Because they're not measuring a very specific piece of functionality, they're measuring the impact of the experience. Finally Sharing is caring. Share your dashboards and information about your service with your colleagues. And when you design these dashboards, think about what your colleagues may need to know to understand if your service is operating, and so that they can also understand if there's any issues impacting their service or their KPIs. Let them be independent. So I've talked a little bit about business metrics and golden metrics. In Cloudwatch, these are called custom metrics. They are metrics where you define those metrics and the KPIs yourself, and you have full control over the metadata that shapes your metrics. In Cloudwatch that metadata is called dimensions. In Prometheus it's called labels. What's important is this is information to help you make sense of your metrics. Things like operation or service as you see here, but it might be your application name, your project, anything that helps you get business, organization, or implementation context. So that's great for custom metrics, I can add metadata with my dimensions. But what about AWS vended metrics, the out of the box ones that come with EC2 or lambda for example. I can't add dimensions to those. I don't have that control. Well if you've ever read a paper on operational excellence, you'll have embraced the advice to tag everything. Tags should be an integral part of your infrastructure. And the good news is that Cloudwatch allows you to query your metric data for your vendor metrics using those resource tags, using metric insights. For example, here, we're looking at low bouncer latency, and we're building a metric insights query. I can access the application tag, name and value in my query. I can use them in both the filter and the group by so that I can get at the right data. I'll show you a bit more on this in a minute. So we've talked about starting with metrics and tags and adding context. But what next? Observability is a journey, it's not a destination. It is a journey of continuous improvement. At Amazon, we have a process called correction of errors. Now there are public blog posts about that, so I'm not going to go into all the details. But I want to highlight a few things to you. First of all, it's a mechanism to power continuous improvement. It is a post-event analysis of your issues that focuses on key questions to help you improve. It is not a documentation of failures. It's designed to push improvement. And there are 2 really important questions that we ask when we do correction of errors. The first one is how can I reduce my detection time? How long did it take you to know that there was an issue? Now have a think, would some extra data help you have got there sooner? Do you need to tune your alarms or add some extra ones so that you can reduce that time? And the second area you should be focusing on Is resolution time, that time that it takes you to find and correct the issue. What else would have helped? Would some additional context, the dimensions or the tags have helped you understand and resolve quicker? Is this a time where you actually, actually need some additional data? And it's always a good time to revisit your dashboards and your run books and make her 3 o'clock in the morning life a little bit better. OK, I said I'd come back to metric insights and alarms. Once we have the important metrics and tags, we need to know when there's an issue. And to do this, we need alarms on the right metrics. The right notifications and the right information so that we can act. And to do this effectively, ideally I want one alarm for many metrics. Here, Metric Insights is your friend. So I've talked about dimensions and tags. And here we can see an example of using these in a metric insights query. You can see that I'm controlling which data to look at with the wear tag using the environment tag that is on my um cloudwatch synthetics in this case. I only want the synthetics that are tagged with the environment prod. And in my group by, I'm looking at seeing things by canary name and by my service tag. So I can have control over the data that I return. The more control I have, the more I can make my alarms the right scope, and I don't need individual alarms for everything. We also have notification control. I can choose to aggregate the results of that query into one single series. And then I get a single notification any time that aggregated series breaches my threshold. Or I can choose to keep my series independent so that every canary name series there. If it breaches the threshold, will send me an individual notification. The group by information here, Is what I get in my notification. So if we have a look at a subset here of an alarm event, you will see I have the values for the canary name and the service tag, which is what I had in my group by. So I can control what I query and what information I get, so that I can act either manually or send this automatically to a lambda function or to Systems Manager. And then I have that information available to those tools to take the right actions. Remember here. Don't ignore your alarm description, we like context. Please use it to put in information about your alarm and links to your dashboards and your documentation. Again, we're back to making Anya happier at 3 o'clock in the morning. OK, so let's move on from metrics and alarms and let Anya take us into the world of tracing. Thank you, Helen. So I'm often really surprised when I talk to customers who tell me they are having observability challenges with the distributed application and then they tell me that they're not tracing. The thing is tracing is inevitable when it comes to distributed applications. It also speeds up your time to resolution and improves your observability. When you are navigating a distributed application. You're gonna need a map so if you want to know if the problem is upstream or downstream or where the errors or latency are coming from, you're gonna need shortcuts. Which again means that tracing is inevitable and that alone is important for operational excellence. Now do bear in mind that there are still some components that are hard to trace, like networking. Later on in this session we do cover how to cover the gaps in telemetry, but over the next few slides we're gonna focus on driving operational excellence with distributed tracing. So one of the reasons my customers tell me that they're not tracing is because they think it's gonna be really complicated and it's gonna involve a lot of code changes and that doesn't necessarily have to be the case. Apologies. It is recommended when it comes to tracing that you start with open telemetry. And open telemetry represents vendor neutral open standard for implementing observability. And it gives you that unified approach for collecting telemetry data. And whilst open telemetry has reached stability for traces and metrics with logs still maturing, the underlying concepts of distributed tracing are well established and tested. And again it's more important for distributed applications to have that interoperability that you get with open telemetry. Now open telemetry also supports auto instrumentation. Auto instrumentation wraps itself around the applications and listens for calls to common system components incoming and outgoing HTTP calls or database requests, database queries. It then generates a telemetry and sends it to your trace back end. And again, that means note code changes as well. So this is a really simple example on how to instrument a Python app with Open telemetry. So all I am doing there is I am installing the and configuring the Python Hotel library. But as you can see I'm showing you no code because there's absolutely no code change required. Now, in many cases, auto instrumentation will be enough. So in the example on the screen, the flask route, the HTTP calls, and the database calls will be auto instrumented, no code change. But Helen already mentioned how important that business data is data specific to you, and that also applies to your span and trace data. So there are times where you might need to do a little bit of manual instrumentation. So in my example, I added things like the user tier and the order number. Uh, they are hard coded just for the ease of the demo, but obviously you'd get them dynamically. So this is the part of manual instrumentation that you might have to do. It's a couple of lines of code and if you're not sure how to get started, you could use energetic ID like Kiro, for example. Another important aspect of tracing is correlation and context. So we already just covered that business context that is important to record again it's really relevant in situations where you're troubleshooting. But there's other lenses here as well. We've got the telemetry correlation, for example, metric to trace correlation or trace to log correlation. So again, I'm using Python. The example here I'm using the um. Log instrumental in Python and I'm setting the logging format to true. And all that means that the login formatter will now automatically add the trace ID and the span ID to my log data. So that means that if I'm frantically searching through a sea of disconnected logs. I now have the trace ID and the log entry and I can switch over so we talked about needing shortcuts that's one way to get a shortcut. The other part is the technical context again that's really important when you're troubleshooting it's really important to know where the telemetry is coming from. What instance ID which host what server address? With open telemetry you have the concept of semantic conventions which not only give you a set naming structure they also mean that you can understand the meaning of the data so you have general attributes such as host ID, server address, client address, etc. and you also have cloud provider attributes like availability zone, AWS account, etc. And you can record these yourself and often also or instrumentation will populate some of them for you as well. Now sometimes you might need to propagate the context, the business and the technical context. So one way you can do that by using baggage. So with baggage, it means that you can set the attribute once and it's available everywhere. But it is really important to remember that this isn't for business logic. It doesn't replace caching. This is for telemetry data. But as I said, you said this once and it's available everywhere. So in the example, the checkout service will add order ID again I've hard coded it, but will add the order ID to baggage. But if later down the line, The payment service wants to record the order ID. It can retrieve that from baggage as well, and I'm using Python, but they could be using different code and that would still apply so that's again really useful. But you might be thinking, OK, but what about cost and what about the volume of the telemetry? How do I do that, Anya? Samping is how you control the volume of telemetry. And probably the easiest way to sample would be with head sampling at source. So basically here I'm just setting. That 10% of all traces are to be sampled. But the caveat is that you need to be careful because if you sample too low, then you might miss some data, so you need to fine tune that. Another option for sampling is tail sampling where you get a bit more control. So with tail sampling in this example, I am actually telling the code to send all the traces over to my collector. And then Here is my auto collector config. I've added a tail sampling processor and you can see that in the configuration. I have asked the processor to sample 100% of all traces that have errors, 100% of all traces that last longer than a second, but only 5% of all other traces. So this is how I can get more granularity. But there is one caveat. Your tails something processor sits between your code. And your traces back in, so that does add some operational. Considerations for you, it needs to be available. needs to have enough memory, but that's just something to bear in mind certainly an option. Another way to uh to control sampling would be to um set rules on the AWS X-ray side of things so you can create sampling rules and then your collector will make decisions based on them. Now X-ray now supports adaptive sampling, and what that means is that it will, it will, um, sample extra traces under certain conditions that you have defined. So in this example, there will be extra traces sampling using the sampling boost in in case of 500s and also we are using anomaly trace capture, but all the spans with 501s will be captured. But again, I can also limit that as well if I want to, which is the setting at the bottom. Now this next one is not actually sampling, but just another way that you can choose to analyze 100% of your spam data. So what you can do is you can configure transaction search. But then you can also choose to maybe index some of those traces in X-ray 1%, 10%, whatever you need. So with transaction search you can run span analysis on all your span data and then you can also decide how many you index and X-ray. The other thing about transaction search is that the span data gets saved to a log group called AWS SANs, and that means that you can run further analysis. Uh, on that data, which is really handy and you can choose to keep the data for longer as well so that's one way to um to control how you analyze your spam. Now Helen mentioned red metrics, availability errors, etc. we've just covered traces, so one way to implement what we've covered would be to use application signals and cloud watch. Application signals uses auto instrumentation to instrument your applications. It then auto discovers dependencies between your services. You then get the correlated metrics and traces, and I'll show you that in a minute, but if there's a spike in errors, you now get the correlated trace that you can take a look at. You also get SLO management, so we talked about KPIs. So this is how you can track your SLAs, for example, and you also get burn rate metrics so you can actually be alerted when you're on your way to fail an SL SLA rather than after it's happened. You'll also get that map that I talked about your application map you get transaction search and you get all the cloud watch integrations. So I think it's a good time to switch over to the console and to cover some of the concepts that we've talked about so far and to demo some of the application signals. OK. So, I am now in my cloudwatch console. I clicked on application signals on the left hand side and I selected application map. And here I have a list of the applications that uh that I'm looking after and straight away on the left hand side, I don't know if you can see that very well, but the one of the um boxes is slightly red and it has an SLI breach so that signals to me that there's a problem in that service. Let's park that for just a minute, we'll go back to it in a minute, but you can see that I've got other applications here and there's also uh 3 here that have this dotted line around them. And what this means is that these are uninstrumented, so actually application signals is signaling to me that I might have gaps in telemetry, so that's just really useful to know as well. But let me just double click on one of my more complex services which is the pet clinic front pet clinic front end. So I'm just gonna double click it. Uh, I don't expect you to see the um the entire map this is a lot of detail, but as you can see there are a lot of dependencies. It's talking to a lot of services, we're talking to Bedrock there. We're talking to all sorts of different services. Now I'm just gonna scroll around a bit so you can see it, oops. I'm just going to zoom back out again. So we talked about shortcuts, right? So straight away I can see a shortcut. There's a circle that's got a It's half red. There's clearly something not right with one of the services that the front end talks to. If I hover my mouse over, there's a 45% fault rate. So again, we talked about shortcuts. That's one way to get them. So I want to find out more, so I'm going to select view more. Just gonna expand that here. OK, and here I have the details for paymentservice.net. If there were any changes in the last 3 hours that would show up here, really handy to know when you're troubleshooting at 3 in the morning. There's some details about availability decline. And there are those red metrics that Helen talked about requesting availability latency. We also have percentiles because it's important to remove any outliers. So that information is there for me out of the box. I want to see more, so let's select View dashboard. So in this overview I will see the metrics that you've just seen in the little side panel. But I want to go over to service operations. And what I'm gonna do is I'm just gonna take a look at fault rate. There's a. A service operation that has a 100% fault rate. That's not very good. Let's have a look what it is. Oh, apologies. Let's do it again. OK. There it is. So it looks like there's a problem with the payments, which is obviously not great. So if I head over here to the metrics and I, if I pinpoint the faults here. What's gonna happen there's my metric to trace correlation. The associated traces will be displayed here, so I will be able to take a look what was happening. So I'm just gonna open a couple of those. But before I go over troubleshooting those, and I'll show you. Just another shortcut that I wanted to add to, well, another aspect that I wanted to point out, we talked about the importance of business metrics. So because I've manually instrumented. The owner ID for this payment, so this is a pet clinic. There's owners who are trying to pay for treatment. I've manually instrumented the owner ID. If I wanted to know business impact to see if is it a specific customer that's having problems, is it all the customers that are having problems, I have that information there, and that alone is again really powerful, but you can see the owner ID is 6310-1228, more or less having the, the, you know, the same amount of spans captured. So I think unfortunately everybody is affected, but again this is so powerful to know. And just um just to show you how this is done, so this is um this part of the application is written in .net uh we share this QR code at the end with you, but basically this is the code for the pet clinic and in .net this is how you would, uh, do this manual instrumentation. You would do do this current activity set tag and there's the owner ID. So this is how it's actually appearing. In my sam data. But let's just have a look at um at the streets that we opened earlier. I'm just going to close this a second. OK, so these are all my spans, and I can see that there's exceptions. So I'm gonna click on this one. There's some 500 error. I'm just gonna Go to this one. And you can see that it's the specified queue doesn't exist, so it's pretty clear that there's a problem with the queue. It's either been deleted or it's the wrong name. I can go and tell the application team to get it sorted. But let's just have a look at some of the other data that we have here. So if I just have a look at the span, for example, If I scroll to the bottom, there's correlated logs. If I was troubleshooting and just went in through the log, not through the trace. The trace, uh, the trace ID and span ID are recorded here. So again, that's, that's one of my shortcuts. If I just maybe have a look at the resources tab you can see this was auto instrumented with open telemetry for Java. And if I just head over to the metadata, for example, These are those semantic conventions general attributes I was talking about. But that's host image we've got host ID and there's that manual owner ID. Now we already know that there's a problem with the queue we can speak to the application team to get it fixed. It was pretty straightforward to narrow down the problem. But imagine like a different way. Imagine that your customer service has received a call to say customer number 2 is having a problem. You wouldn't probably go through this route, there's an easier way to do that. What you could do is you could go over to transaction search. And what I'm going to do I'm going to search for the owner ID. And then we also know that It was a payment problem, so I can even narrow it down to which service this was happening on. And I can run this query and then all the bands that relate to this service and this customer. Will appear in the list. There's the trace IDs. I can click on that it'll take me back over to the trace console that way. So that's yet another shortcut. uh, that would be really handy when troubleshooting. OK, so. I'm just gonna. Close this for now. So if you recall at the beginning, uh, we noticed that there was a problem with one of the services and there was an SLI breach. So I'm just gonna head over to my SLO console. And here you can see that I've got uh an SLI defined or SLO defined for appointment service availability and my goal is 95% availability and I'm not doing really well. I'm at 74.9%, which is not great and to make things worse, I'm actually I have been through my error budget, so I've really failed that SLA. But this SLA or SLO depends on an operation, so I'm just gonna click straight through to that. And if I take a look at this operation here, I can see that I'm at 25% fault rate. So like before, Let's select a point in the errors. And open one of the traces. But another shortcut. If I head over to top contributors. And if I sell it by visions. I can actually see that version 2 has 29% fault and 32.6% availability. So to me that indicates there's been a deployment and version two's not doing very well. So again, another way to shortcut but let's have a look at the trace. There we go. There is a no pointer exception in there. But actually, if I just look over to the metadata. And this FAAS version is a semantic convention for severalless function version 2. So we can see that there's a problem with version 2. We need to tell. The application team that there's a no point exception and to have it sorted. So again, a really quick way to for me to pinpoint an issue. Now the other thing that I wanted to talk about is. We're we're about to, to move on to the next bit which is filling in gaps with telemetry, but just wanted to show you some of the stuff in uh in the console so I have this service called the billing service Python. Now if I take a look at the service here. Pardon me, I'm just gonna close this. Straight away when I, when I'm actually taking a look in the console I can see that it's hosted in EKS and the name space and the workload so I'm not gonna do it, but if I wanted to I could click on the hyperlink. It would take me to container insights if I wanted to know the infrastructure metrics that come along with the container, the name space, but. Without repeating the whole flow, I'm just gonna give you a spoiler. This service has a dependency and the dependency is on an Aurora Postgress database and it does these inserts into the database, uh, and selects and updates so I can see all the metrics here straight away, which would be handy. There is no problem, but imagine there was a problem. If I wanted to take a deeper look and have a a a more specialized view because I've enabled database insights on my database, I can select straight view and database insights. And here I can see the health of my database fleet. I can have a look at the queries adding to the load on my database. I can see the calling services which is really useful. When I was an SRE I always struggled to to know what services are affected by by which database so that's really handy to know. And then what I can do is I can dive deeper into the database instance. I'm, I'm only showing you this high level, uh, at towards the end of the session we do share a QR code with you and there is a section there for interactive demos if you wanna dive deeper into database insights, by the way, please check that out, but. For now I'm just gonna show you some high level stuff so. I can take a look at my database load and let's just say I want to have a look by blocking SQL. And I have a look at the top SQL on my database that's affecting the load. I can actually have a look at the The SQL code that's running. If there was any usage plans, I could take a look at it and again. I can see those culling services which is really handy. I can see exactly which operations are running and the statistics for those operations so that's one way to have a a much more specialized view into my application. So before we wrap up the demo, there was just one thing that I wanted to really, really quickly show you. Uh, if you want to impress your, your boss and justify your trip to reinvent, you could show them how quickly you could implement red metrics and traces on your serverless applications, for example. So let's just say I've got a new lambda function. It's a pet photo processing service that just grabs photos from a bucket and puts them on a queue to process. And this lambda function is not instrumented. If I wanted to instrument it, all I would do is head over to configuration. Head over to monitoring operation tools. Click Edit here And tick these two boxes. Application signals and lambda service tracers. So what's going to happen when I click save? It's two things. First of all, an environment variable is added to say that I'm using auto instrumentation. And second of all, a lambda layer added for the code. For the auto instrumentation. And I'll take a few minutes for the telemetry to start appearing in the application signals, but because we don't have a few minutes, I'm just gonna show you how this looks, uh, for a function that I made earlier. So if I just head back over to my application map. And if I just double click the photo processing service automatically my dependencies are discovered. The red metrics are there. I can dive deeper into that function. So it took me 3 clicks I think. So there we go. So this is uh this is a demo just to show you the red metrics, uh, how to instrument the auto instrumentation, all those deep links, uh, etc. but uh let's move back onto to Helen to cover how to fill in gaps. With your telemetry. Like on the mansion. Chasing to just audio. Put my audio on. Can you hear me now? No, sir, we good now? Awesome. Thank you. Like Anya mentioned, sometimes tracing alone or even with metrics and logs doesn't always give the full picture. Sometimes we need to look at specialized tools to fill in the gaps. Let's have a look at a few of those that are available in Cloudwatch. Now Anya has already showed you a couple of these. If you haven't explored them, I highly recommend it. My personal favorite is synthetic canaries because again with no code changes, it's a really easy way to get started with visibility from your customer facing side. I want to look at two others that we haven't explored today. Container Insights gives you out of the box dashboards and you can explore the telemetry from workloads on ECS EKS, and Fargate. It collects metrics for you like CPU, um, like memory, disk, network, and you can explore these at various levels. Cluster node pod container. You also get event information such as container restart failures, and all of this data helps you identify and resolve issues more quickly. I also want to take a minute on Network Flow Monitor. It's been around for about a year, but we find a lot of our customers aren't aware of this. And something Anya mentioned earlier was networking is hard to get visibility into. What Network flow Monitor gives you is near real-time observability into network performance between your compute resources, your EC-2, your EKS and AWS services like S3 and Dynamo DB. Why do we care? Well, when your application gets high latency, we kind of all look at network first, right? So it's really handy to know, is that actually an issue? So network flow monitor gives you visibility into the network performance things like data transfer, packet loss, round trip time, but also it gives you overall network health indicators. You can drill into the flows, you can look at the topology and the metrics further. So basically you're getting all the extra information to decide where do I need to go and focus my attention. If this or any other of specialized tools that we showed are interesting, please go and check them out. See if this is valuable to complement your observability and move you forward on that journey to operational excellence. So earlier I said cutting incident response and detection time was a key aspect of your operational excellence. We didn't forget about AI. AI can accelerate your investigations and reduce your time to detection and resolution. Let's face it, machines are really good at analyzing huge amounts of data and patterns. So let's use it. Cloud watchers had machine learning features and AI features around for quite a long time. There are features like machine learning pattern analysis for logs that pulls out those constant patterns and the individual tokens that you're seeing inside your logs and helps you see when that pattern changes. So we have anomaly detection for log patterns. Or you could have a look at machine learning anomaly detection for metrics. What happens here when you choose to enable that on a metric is it builds a machine learning model based on the history of your metric data. Why? Well, because now it can tell me when things change. So in my alarms, instead of having a fixed value threshold, I can set an anomaly detection band and have it tell me when things change. Now we also have Gen AI back tools. So we have Cloudwatch Investigations. Imagine when an alarm is breached, if the AI agent can go and start gathering data and coming up with hypothesis before you even logged on. I know I would love that. And the point about Cloud Watch investigations is the human and the AI agent work together. You can disregard things that it suggested, you can add your own. And not only will it help you identify the issue, come up with hypothesis, and sometimes suggest how to fix it. You can also get reports, incident reports on your investigation that help you move forward with your correction of errors, your improvement process. If you've not checked that out, please do go and have a look. Alternatively, you could use AI through MCP servers. AWS has made a number of MCP servers, and there's more appearing all the time. And there are specific MCP servers for cloudwatch, for apps application signals, and for manage Prometheus. So I've done things where I've gone and just asked it, what alarms do I have, what's happening? Tell me about the data behind this. Please try it, it's quite impressive. OK. We have covered a lot. So let's just review what we've covered. We have looked at observability must-haves for operational excellence, about KPIs and metrics, about context through tags and dimensions. And about processes like correction of errors that help you keep taking steps towards a better world. Anya has taken you through a great journey on tracing and shown you how it doesn't need to be complicated, and some of the tools you can use to not only help you get that data, but to explore and navigate that data. And we've had a really quick look at some of the specialized tools. And the integrations, and don't forget about AI. So This is over to you. Where do you want to go next? Behind this QR code, we have a number of resources. Whether you are interested in reading, whether you like watching or whether you like hands on. There's some great resources here. Our one observability workshop gives hands on activities around many of the features, not just in Cloudwatch, but in our open source availability through AWS as well. The observability best practice guide gives you a great way to think about where am I and where am I going to go next and how are other people doing it. We've built that based on the experiences that we have with the many customers that our specialist community supports. If you want to learn more, first of all, we will be outside the room after this talk. Give us a minute to pack up and we're happy to answer your questions. But in the expo, in the village in the center, there are a number of cloud operations kiosks, from observability itself to AI ops, multi-cloud. Many of them there. All of the AWS booths like that for the different areas are all in the same space and I bet they've got some swag hidden away, just ask them nicely. Thank you so much for joining us today. Please do fill out the session survey. That data is really important so we keep providing the best experience for all of you. And the week is only starting, so keep your energy up, keep hydrated, and please enjoy the rest of your reinvent.