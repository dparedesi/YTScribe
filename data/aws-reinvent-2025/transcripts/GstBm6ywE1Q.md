---
video_id: GstBm6ywE1Q
video_url: https://www.youtube.com/watch?v=GstBm6ywE1Q
title: AWS re:Invent 2025 - Long-Horizon Coding Agents: Complex Software Projects with Claude (AIM3316)
author: AWS Events
published_date: 2025-12-04
length_minutes: 42.55
views: 115
description: "Vibe coding with AI to write functions & fix bugs? Table stakes. Building an agent that architects, implements, tests, and deploys a full application to AWS? That's fundamentally different. This session explores design patterns that enable Claude on AWS Bedrock to achieve \\"escape velocity\\"â€”the ability to make sustained, autonomous progress toward production deployment. You'll learn Anthropic's core principles for building agents that can handle full-stack development tasks independently. We'll c..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

Good morning everybody and uh if you don't have your headphones on yet, you might want to put them on if you can't hear me from where you are sitting up on the front row but welcome, welcome. I'm Alex. I'm a member of technical staff at Anthropic and I'm joined here by Nadine. Hi, I'm Nadine. Nice to meet you all, both members of technical staff here. And uh we're here to tell you about some of the really cool research findings that we've recently published at Anthropic on long horizon coding agents. We'd really love for you to kind of consider a couple of the lessons that we ourselves learned that we're here to share. One of those being that we're still the thinkers, right? And that's why I'm wearing this thinking cap. Uh, we're still the thinkers, even though the agents can do a lot of the work for us. Uh, that's one of the lessons, that's kind of the societal impacts lesson, if you will. And then the second one is the technical, technical learnings that we're here to share. Yeah, so if you're here at Reinvent Live, then you might have actually seen our demo live in the anthropics booth, specifically in uh Claude keeps Building booth, uh, in which Claude is building a project management tool that we're calling Canopy, and it's taking in feature requests from attendees like yourself, uh, to continuing to iterate on the minimum viable. Product, um, and so this is all being hosted on AWS infrastructure specifically Asian core runtime, and we'll be going in detail about the architecture a little bit later on but to get started we just wanna give you a little bit of context on why this is important and the lessons that we learned along the way to reach this point. Yeah, so let's jam with the silent disco and get into it. So, really, what are long horizon agents? What are we really talking about here? So, we've been thinking about this in terms of what are the ways that we can teach AI to handle the quote unquote boring parts. And so when we talk about boring parts, it's very much a tongue in cheek phrase. Uh, we, you know, how many of you here are developers in the room? Quite a few of you. Awesome, right? So, so if you think about, you know, the development workflow, we really want to encourage us to continue to be the thinkers in the room, while allowing the AI to do some of the things that maybe aren't as exciting. We'll talk about that here soon. But really what we want to do here are the lessons from building agents that work across these many sessions, and that's going to be the technical aspect of this presentation. So let me ground ground this in developer reality, um, so as you might all know your Monday plan might start off you have a few features or maybe like a pretty optimistic one where you wanna work on updating some API documentation, maybe writing some unit tests for edge cases, maybe fixing a bug, and then finally if you have the time, work on some sort of innovative feature that you've been wanting to get to for a long time. Um, but then reality strikes or hits, uh, and what ends up happening is the quick sync that was meant to take 15 minutes might have become a 45 minute heated debate about the road map or a SE1 comes and you have to fix some sort of production bug or there's a 47 message Slack thread, um, that requires your input to continue. Um, and so by the time you actually get to opening your IDE and working, uh, it's already 4:30, um, so basically what we were looking to solve and what we were looking to, what we were questioning was whether AI or LM specifically could handle some of this more mechanical or repetitive implementation and work and humans can. Focus on the judgment calls and I wanna be very clear we're not saying this is like a fix for everything this is not gonna solve all your problems won't like free up all your time what we're saying is that we've seen a specific um we've seen specific trends and specific um things that work for specific types of tasks and we wanna share those learnings with you today. Um, so yeah, that's sort of what our focus is, um, and to get that started there's a specific challenge that we wanna describe and that's the fact that LMs have a fundamental limitation, um, so. Yeah, thank you, uh, as you all might know. LLMs are forgetful, right? So they're unable to keep a persistent memory from session to session. So while in theory this new pair programmer might be great because they love to write tests, which is awesome, they never complain about updating the documentation, which is also great, and they follow your coding standards to the T, um, they might forget, uh, where they left off session to session, um, and that might. Seem like a problem, like a problem that we wanna solve, but what we've seen, um, is while this is great for example for quick tasks where you have a function, you ask Claude to write a function and it's, it finishes it and you're done for longer projects where it might be 200 features that run over hours and maybe days, this feels like it would be a problem. Uh, what we've discovered, and this might be a little counterintuitive, is that forgetting isn't necessarily a bad thing because forgetting means that the model isn't picking up bad habits, um, it's not sort of, um, reinforcing these incorrect assumptions. Um, and it's able to generally be more focused, um, so if we solve for this memory externally rather than through the model then forgetting becomes a feature, not a bug, and that's what we're talking about today. Yeah, and so, let's ground this in the reality, right? So if we think of us humans, it takes quite a while for us to get back into flow if we get distracted. You know, some research has shown that it can take 20 minutes for somebody to get back into flow as a developer. I've certainly been there. Uh, if you think about us here in Vegas, and you know, we go out and party and we come back the next day, it might take actually quite a bit longer. Might take, you know, an hour before you're back in flow the next day. With the, uh, with the agents, they really can do this very, very quickly, right? They can come back into flow very quickly, but there is a fundamental problem, and that's the context, right? So while we as humans take longer than, than agents to get into, get into kind of the next session, uh, the, the, the issue for the agents is, is, as Nadine was saying, how do we, when the agent wakes up, how does it know what to do next, right? And so each new session begins with no real memory of what came before. And that's both a superpower and a bit of a flaw too, right? So, really, we're talking about how do we imbue the agent from session to session with the correct context, so that it can continue building where it left off. Yeah and so this changes sort of how we think of the human AI division of labor and we wanted to show you what we found to be effective um so what we've looked at is that the human would be in charge of uh creating the vision or designing the vision, um, and the requirements, uh, and setting the standards and the. Constraints, um, and actually making the architectural decisions, um, and then reviewing the outputs of the models and then the models, the AI builders or more specifically the LLM builders will be implementing those features, um, following that vision or the guidance, uh, and then the test will be done. Within the like against the criteria that you've set um and the documentation will be constantly updated as it works which is really great um and then it will basically iterate on the feedback that you're providing it um and a key word here is that I wanna emphasize is constraints so you will have a sort of. Of standards file um and this should be read only to the agent so the agent shouldn't be allowed to go in and edit your style or edit your architectural decisions um the way I like to think of it is you're the architect and the AI or the LLM is sort of the builder that's following the blueprint that you designed. Um, so yeah, so let's talk about what environment as memory really looks like, and this is something that we're going to show you in a live demonstration here in a few minutes, so we're not just gonna talk off the slides, I promise, um, but really, you know, there are things that we have to consider when we think about the environment. One of those. Things is the feature list, right? So interestingly we've discovered that if we have the agent build up the feature list from a specification, this is kind of a form of specification driven development you've probably heard this before, both as developers and in the AI space. It's almost like, you know, all the old things are new again. Uh, we have the agent build up a feature list, and that feature list is a Jason file. And again, the reason why it's a Jason file and not a markdown file is that we've discovered that if we have it create a JSON file, it's actually less likely to write to that file if we give it instructions to do that, than if we had a readme file, a markdown file. If we have a markdown file, it's more likely to edit it, and we don't want it to do that once it has come up with the initial specification. So that's very important. Uh, as Nadine was saying, we also want the agent on every subsequent session to have the same standards as it had in the previous session. So we have a standards MD file that we give the agent with the standards of whatever development team that you're, that, that you are on, right? Um, there's also a a cloud progress MD file, and the clouded progress MD file is another sort of piece of information that we give the agent at the beginning of the session, so that the new session knows where the previous session left off. That's actually joined up with Git commits. So we as developers know what that's like, right? If we write good commits, next developers that come and look at the code base can look at the commit history that previous developers have actually added to the code. Base, right? And we are learning that if we give the agents the same sort of primitives, if you will, the agents do a better job. They kind of think like us, they work like us. Probably not a surprise because we train them, right? Even though it's on on the knowledge of the Internet. Um, and then finally, there is an init SH file. And it turns out that this is actually quite important as well, because when the agent starts a new session, we want the agent to start the dev server, right? We want the agent to look at the environment. There are sort of some things that are quite deterministic, right? And those deterministic things go into that in it SH file. So there's the non-deterministic nature of what the agent will actually do. But there is the deterministic part of actually getting that environment started on each session, and start it in a very repeatable and deterministic way. So that's what the NISH file is about. And so, with that, sorry, I'm getting a little bit excited as you can probably tell, cause this is a lot of fun. Um, the loop that never breaks, right? So, I was talking about kind of the fundamentals of each environment. But what actually happens when the session starts. When the session starts, there's essentially that this agentic loop. If you're, if you're familiar with, you know, any of the work that we've done and, and any of the talks we've done about what agents are, agents are nothing more than an LLM. It's environment, it's tools, and running in a loop, running continuously, right? So the loop that never breaks, pretty much, you know, from each iteration to each iteration, the agent is going to look at its environment first. It's always going to look at what happened before, and what environment is it working within. It's then going to run the test suite to make sure that the things that happened in the previous session are still repeatable in the next session. Probably quite familiar to the work that we do as developers. If you're a developer, you know that you've got to run the tests to make sure that they still pass from iteration to iteration, that no new regressions have occurred. So this is why we have the agent actually create a test suite and run it every time that the session starts. There's so there's some basic verification that it does. After it's completed the verification, we actually go in and have the agent pick up the next available feature to have the agent start working on that feature. As it's working on that feature, it's also continuing to run the tests. And there's some interesting insights here that we'll share about, you know, what we found so far, whether it should be a single agent, you know, two agents, or multiple agents. We'll talk about that here pretty soon. So the agent then continues to build, it continues to implement the feature, and this is something we'll show you in real time here. Um. And then it's gonna again test like a human and commit its work, because at the end of the day, we really want the agent to continue to write to the history. We want it to write the sort of the progress of the work that it has done. Excuse me, I need some water. Uh, see, this is why I'm human and not an agent. Anyway, so it will write the commits, and it will write the progress, and then eventually the next session will pick up and look at that history and be able to kind of get started again. Yeah. Uh, and this loop sort of emerged from the learnings that we had, so we wanted to like briefly explain to you the breakthroughs that we had moving from one shotting to incremental progress. Initially the approach we worked with, uh, and by yesterday I mean like around a month ago, uh, what we are working with is we had an agent that tries to implement everything at once, and we noticed that there was a bit of an issue with the context running out mid-feature, um, which was not ideal, and we also like. Realized that the model sometimes declared victory a bit too early. It was quite smart, so sometimes it gained its own success criteria and reward hacked. So we noticed that we also, so like as Alex mentioned, we noticed that we had to explicitly tell it that it was unacceptable to remove or edit tests to make them pass. And then what we decided to do and what we found most effective and what we'll show, we'll be showing you today and we've been showing at the demos, um, if you can. Yeah, is, is that, um, basically we ideally work on one feature at a time, uh, and this helps with what we mentioned with the context window running out, um, and then another thing is that we leave the environment in a clean state, um, that so we're using the commits, uh, as memory but we're leaving the actual environment clean. Every time it starts off with a new feature, um, and the most important thing is that it tests end to end like a human would so we actually give it the ability to use browser automation so it can check the UI and work through the fixes and the features to see that they're actually working and not just working in theory. Um, and this might not sound sort of revolutionary, um, but, and it seems sort of like what a good engineer would do from a day to day, um, but we, we were working on was trying to explicitly encode this and allow the agent to work in this manner, um, so yeah, um, yeah, and that's actually by design, right? We in our research have found that the closer we actually mimic the developer workflow, the better the agent does so precisely what Nadine was talking about. I will go further and talk a little bit more about this 200 feature manifesto in a moment, but I want to go back to some of what Nadine was talking about when she was talking about the specification and the tests. So we have the agent write the test as it continues to build. And what we've discovered as well, even just building the demo for the booth, that booth 810, please come and check it out, and we'll show you some of that today as well, is that if the agent is asked to write individual tests, what happens is, as the time approaches for the agent to complete its work, uh, it will actually run into these anxiety attacks, like, quite literally, you'll see in the logs, it's saying, hey, we are running out of time, so we better finish this quickly, and starts to reward hack as Nadine was talking about. And so some of the things that you ought to consider when you are building this kind of stuff is how do you prevent those reward hacks and what are some of the ways that that can be resolved. And as Nadine was saying, If we think about the agent as sort of a junior to mid-level developer, then we really arrive at this idea of a 200 feature manifesto, if you will, and it's a little bit of work up front, you know, building a specification, which we'll actually show you an example of that has enough details for the agent to do the work well, for the agent to write tests is. Not an easy task up front, but it pays dividends in the end, right? If you start with a specification that the agent can follow quite closely, but still have enough capability to act nondeterministically and explore different paths, that's kind of the holy grail. So what winds up happening is that we go from a specification, which is a markdown file that we as humans write. Into this JSON sort of format that the agent can then, or is less likely to edit, which we don't want the agent to modify this. We want the agent to use the JSON file specification and mark tests as passing, as it continues to do its work from session to session. And so that's kind of the interesting thing here. Uh, this changes how developers spend their time. Yeah, so we wanted to give you a bit of an idea of what this model would look like in practice and how this, uh, would change for developers like yourself. So we're going from originally you'd be writing the code yourself, you're debugging manually, uh, you're documenting afterwards if at all, and, um, you're probably context switching between tasks as you have a lot of different fixes at different times, um, and the model that we're envisioning for after is that you're spending. More of your time defining these requirements for the LLM to follow like I mentioned, um, earlier you're also reviewing the AI's like test results rather than fully debugging manually, uh, and making sure that they're correct, uh, and following what you're expecting, um, and then the really nice thing is the documentation is generated, uh, as the LLM or AI is working, uh, because this is, um, encoded into the workflow and the way that it's working. Um, and the last thing is that the agent is what's maintaining its progress, uh, notes, um, and how of how it's working. Um, and what we found is like you might be wondering if this is faster overall or if it's not for us from what we found is that often times it is faster, um, but it's faster for a very well specified projects so it really depends on for you, uh, how your, your specification, the domain you're in, and your tolerance for review cycles, um, and yeah, so we wanted to be a bit clear about what becomes possible and like. Where the limitations are, um, so yeah, so to start, what it's improving, so I've mentioned this a lot of times because, uh, this is really important to me, but the tests are written for every feature by default which is awesome, um, because then you have a lot of test coverage which we found when we were doing the demos, um, uh, so that's done by default and then the other thing is the documentation is updated for every session so it's constantly up to date, um, and it's constantly accurate. And then the last thing is that it's consistent with your coding style which is really great because like I mentioned earlier it's following your standards um and your style almost religiously. Um, but there are some limitations that we wanted to highlight. So one of the main ones and one that you're probably aware of is the vision gaps. So the models might not be the best at seeing every single visual bug. So often time we, it needs your judgment and that's why we're saying this is like a collaborative sort of model to check, uh, and find these bugs and these features and make those requests sort of like how we're doing live in the demo at the booth. Um, and then the other thing is that automation, um, like browser automations like puppeteer or playwrights, uh, oftentimes can't see native alerts, which is something you should be aware of, um, and the other thing is that like I mentioned and like we iterate a lot, this is not replacing human review, it's not sort of a fire forget and forget sort of model, it's one that you would need to like verify before it goes fully in production. Um, so yeah, that's sort of what we wanted to emphasize today, what's possible and the limitations there, yeah, and let's ground this in reality, and this is gonna be the preamble to the actual demo. So this is the architecture that we implemented for the demo booth and what we're gonna show here today and so you can see that hopefully this works. I'm trying to do the laser pointer thing maybe no, it's not gonna be that great on a white background, but hopefully it'll work, um. Yeah, so there are 3 major components to this architecture, really 4, if you will. Uh, the first is the list of, uh, GitHub issues, and we think of the GitHub issues as kind of the backlog for the agent to work on the MVP first and all the subsequent issues that are added. And because we're gonna do this in a public repository, we're actually going to very happily ask you to submit features and have the agent build them. Uh, this is going to be running over the next couple of days. So really our goal is to by the end of reinvent, have an application that's quite functional. It already is just over the last two days, you'll see what we've built. Um, and so GitHub issues is one of the components. I mentioned that there are 4. there are GitHub actions as well. So the GitHub actions that we've written is essentially, you can think of that as just the CICD that we're all very happily familiar with, right? Um, and so what happens is there's an issue puller that basically sits there and on a cron job, essentially on a scheduled basis, looks at the GitHub issues that are in that GitHub repository and picks up new issues to work on. And the way that we do that is we, we've kind of come up with a way to allow folks to vote on the issues and also for some of us, myself, Nadine, and Philly. Uh, at the booth to actually approve issues that could be worked on by the agent using reacts. So there's a thumbs up emoji for voting. So whenever any of you add a feature, you can also come in and vote for that feature. Um, we'll show you that in a moment, and then, uh, you know, we'll approve those features if we, if we think they're good to go. And then those features are gonna be built by the agent. And that brings us to the agent core runtime. So we think of the Asian Corps runtime in this architecture as kind of the, uh, the infrastructure provision and infrastructure orchestration piece. Uh, we initially started by thinking about building this with ECS and managing all of our own infrastructure, and that could have been something we could do, but we only had about 3 weeks. And we thought, what can Kind of accelerate our work. And Agent Core runtime actually turned out to be a great choice. There are some interesting caveats there. So for example, this architecture diagram, uh, this EFS storage piece, so initially we were considering using EFS, an EFS mounted volume and an ECS container to maintain state. Uh, it turns out that with Asian core runtime, we can't do that, but that's OK. That's not a bad thing, right? So we figured out that Asian core runtime is quite ephemeral, and we can't use it with an EFS volume mounted in it. Maybe that's coming, I don't know. I don't speak for AWS. Um, but what we did instead is we used GitHub, so a Git repository as, as the state, as the source of truth instead of an EFS volume. And that then brings us to the fourth component, which is, uh, here. So the anthropic API on running on cloud Opus 4.5. Uh, what we did there is we've, we've taken the work of our amazing applied AI team and our researchers who built a custom harness, and we built on top of that harness. And that harness leverages the, uh, claw code, uh, cloud agent SDK recently renamed. Um. And we use the Cloud Opus 4.5 model. And that's actually quite nice because if you know anything about Cloud Opus 4.5, it has much better vision capabilities than some of our older models. And so with Cloud Opus 4.5, Running inside, uh, essentially running, uh, uh, excuse me, with the agent SDK using, uh, Cloud Opus 4.5 and, and the runtime, and, and these GitHub actions, we're then able to have the agents that the agent runtime build, uh, build new versions of the application. Those versions then get deployed to an S3 bucket fronted by a cloud front distribution for all of us to see. So that's essentially the high-level architecture that we're about to show in the demo. All right, let's do it. So this is where things might get a little exciting. Let's see what happens. Right, Live demos. Um, so, Maybe Nadine if you wanna talk to this feature, uh, yeah, so, um, I can quickly also show you just a lot of the issues that we got requested if the Wi Fi is in our favor if it's not, then never mind. OK, perfect. So we got throughout the reinvent we got a lot of feature requests, uh, and we've basically been building on them and once we have proved them, you can see that the label changes and becomes, um, when it's finished agent complete when it's building it's actually. Um, an Asian building label. Wi Fi is a little slow, so that's not ideal, but you can check it out in the repo, um, and this is an example of a feature that we actually got at the booth. So we got a feature where someone requested to translate individual pages into Japanese, um, and what you see here is, see, once the agent started building it changed its tag to Asian Building, and this is all done by the Asian, by the way, this is not us doing it. Um, and then it starts the session and then as it's working it's actually taking screenshots and this is because we've connected it to like the playwright MCP and it's able to take those screenshots and show us what it's seeing, um, as it's working to debug, um, and we'd be looking a bit, we'll dive, we're gonna dive deep into sort of what you're seeing in the logs as well because it helps you see how the model is reasoning and what it's thinking, um, and how it's looking at these pictures that it's taking, um, so yeah, uh, Alex, you wanna go ahead, of course. Yeah, so, uh, we'll jump into the logs here momentarily, but I actually wanna kinda come back to one thing that I forgot to uh come back to earlier. So, uh, this is the initial project specification that we were describing. Uh, and you can see here that this project specification has a certain, uh, certain shape to it, if you will, right? And you can see that it's XML, uh, and you might be wondering, well, why isn't this, you know, just a standard markdown file. So, you know, it turns out that as many of you probably know, that if we, if we add sort of, uh, XML tags, uh, we add structure, and the model is much better at reading this kind of structure than just a markdown file. You can see here that this defines kind of the overall idea behind the application as we were describing earlier, this is, you know, essentially a Jira clone that we call canopy because, uh, you know, it's a canopy of trees and, and features. Anyway, uh, and then, uh, there is, you know, a definition for the technology stacks. So like what is the front end of this application look like? What does the data layer look like? So this is like fairly prescriptive and this is the MVP version of the app that the agent had built and we'll show that to you here momentarily core data entities and so on and I wanna add like we mentioned a lot about testing so also within this we've sort of given it if I can get to it sort of testing scenarios um OK, it's good it's hard to find it OK um. Yeah, here, like test scenarios, these aren't for specific features, but it's sort of just to show it how it can do the testing, um, and like we're mentioning actually using the browser automation to do this. So this is within our build plan, um, yeah, and so then, uh, what we'd like to show you is what was initially built. So if we go here and I think it was issue 144, yeah, so we mentioned that this kind of builds several iterations and so let's go back to the MVP for a second. And maybe I shouldn't have done this because the internet is not cooperating. It'll load eventually. Um, maybe load the other one. Yeah, good call. 36, was it? Yeah. All right, let's see if this works. There we go. All right, good. Uh, we didn't fail in the demo, right? So, uh, so, so this was the initial version of the application that was built. This is the MVP based on the specification that you saw. And then, uh, Nadine was showing you earlier that there was a feature that we had the agent build, uh, which is this, uh, you know, translate, uh, translate the application into Japanese. And so what you can see here is that first of all, this is like issue 36. We started with issue 14, so you can do the math in terms of the number of features that were. Built between then and now, uh, and then you can see here again, as Nadine was mentioning that there is a very, very nice audit log of all the things that the agent has done, right? So you can see that there was a workflow that got started, that the session started, there's a session ID from, uh, from the agent corps run time. You can see the screenshots that it takes. Uh, one of the folks in the audience, uh, submitted that one, the admin feature, uh, he's sitting somewhere in there. Um, he's, he's on the, on my team, so actually he's my boss, uh, Ellie. Um, thanks for that one. and then, you know, sort of, um, continuing, continuing through and, uh, and looking at everything that the state, uh, was at the previous session, so it's taking screenshots, uh, of, of kind of the previous state, uh, of, of the application, uh, and then eventually. Uh, it gets to a point where it like actually does new things. So you can see so many screenshots in here, and you might be wondering to yourself, well, how the heck is it able to deal with so much context, right? If it's taking these screenshots and it's reading them in real time as it's working. With the agent SDK there's built-in, uh, built-in compaction and built, built-in context management, and so it's able to actually sort of work with its own context and, and minimize it down to only what it needs. So that's important to note there. Um, and then as we continue, more and more screenshots and then eventually there's gonna be a commit. This is like probably 150 screenshots that it took, something like that, something wild, um. And then eventually, uh, it finds, it builds the, builds the feature, right? And then it takes screenshots of the translations. Um, and then here, let's see if we can scroll down a little bit more. Uh, there's the commit, and the work has been done. Uh, the, the application has been translated into Japanese. And so let's take a look at what that looks like here. And of course I forgot to put a slash at the end and Uh, that caused me to not load this page. So let's do that. Uh, and so here if we go into settings now, I believe is where it implemented it. There you go, Japanese translation, right? So if I click into that, we have the app translated into Japanese. It did this in, in one session. So, um, I think that is a pretty good idea for you in terms of like what, uh, what the workflow looks like, but Nadine also mentioned logs. Do you want to cover some of that? Yeah, so, uh, if we look into, um, CloudWatch, so the awesome thing is because we're using Asian core runtime we can actually look at the cloudWatch logs and sort of get an idea of what's going on in the process, um, so we can see it. Starting off, uh, here, so let me just get to the first message. Uh, it started in the background, then let me get to it it's initializing some things. It's first starting off by let me understand the test structure. Let me read the end of the test.json to see the latest tests. So it's actually looking at the uh previous commits and understanding what. Uh, the previous session was doing, um, and then it's like let me, uh, understand the current app structure and the file so you can basically get insights into what it's, uh, doing with its reasoning, and also when it does take screenshots we also get insights into what it's seeing. So, uh, if I go somewhere here. OK, it's a little hard to get, but, um, basically it will be able to see and it will make judgments based on that like it's saying let me update this and this and this because of the screenshots that it took, um, so it's really great to get basically an insight about the development process and what it's going through. Do you wanna add anything, Alex? Um, no, I think this is probably, probably good, and we can take questions in the end. So let's continue, yeah, um. Yeah, so, uh, we covered this, but just to summarize everything that you've seen, sorry, um, so as we've mentioned in kind of the introduction, uh, every time that a session starts, you know, we, we have the agent set up the environment, so we have the agent look at the previous commits, we have the agent look at the previous screenshots that it took, take new screenshots as it runs the tests, uh, show the coding agent. Picking up the, so, so we, we've already done that but it kind of just going over what we, what we, what we demonstrated, um, it picks up a new feature to work on in each session and then we, we demonstrated that it does automated testing and one of the things, one of the key takeaways that we, we wanted you to walk away with. Is that when we, when we ask the agent to write its own tests, it's much better to uh sort of uh to, to suggest to it that it should write the tests for full workflows end to end. And this is something that Nadine was covering earlier, because we found that if we, again, just to reiterate, if we ask the agent to write tests for just individual screens, then that winds up ending up in reward hacking, right? Reward hacking behavior, so ask it to write full tests, full end to end tests for each feature. Um, and then of course, uh, the, the get progress and, and sort of the notes that it takes from session to session, and that's something we can show you as well, uh, where it writes to, to a file. And so, the compounding effect of this is that uh we, you know, over a period of time, this agent builds a full end to end application, right? Uh, and, you know, in the first few minutes, you'll kind of see everything we've just talked about, where, you know, the agent sets up the environment, starts building some of the core functionality by 30 minutes, you know, if you look at the MVP feature on that repo, you'll see that it completed the work actually in a little bit longer period than that, but it was about 1 hour. Uh, toward a full MVP, but every subsequent session winds up taking a lot less time because it's already built the MVP. So now it's just building the next, the next iteration, and that, that actually takes fewer than 30 minutes. Um. And so this kind of shows the, the workflow for building that initial MVP, but every subsequent feature actually takes a lot less time. Uh, and so, you know, by our, in this case, by hour one, because we've actually constrained, we initially started with, uh, a full 8 hour time horizon. Uh, we've constrained it down to an hour, uh, 8 hours is the limitation that Asian Corre has, um. But we decided to narrow it to kind of demonstrate how the sessions can be restarted. Uh, so that's, that's one reason that we, we took it down to an hour. Uh, but at any rate, uh, you know, if we, by the end of, by the end of, uh, just a couple of days, uh, we've, we've had the agent build a full, fully featured application, and that's really the compounding effect that we wanted you to take away. Um, and we just wanna emphasize that we're still learning here and we're open to your suggestions and your opinions, um, and there are a few questions that we wanna highlight, uh, and that we'd love for you to come and let us know like your opinion on. So the first one is something we're still figuring out. So is a single general purpose agent best or should we have you or you specialized agents in, in a more multi-agent architecture? Um, so this is still something we're deliberating at the moment. Uh, the way it works is that we have one agent, um, that's sort of the initializer, and then another agent that's doing, uh, the main coding and then most of the work, um, but yeah, we, we're not sure at the moment if that's the best approach, uh, and that's still something we're working on because this, this is research in progress, right? Um, and then another thing is we're questioning whether uh. Um, these patterns generalize beyond web apps, so we've sort of optimized this for, um, like JSON TypeScript, React, no JS, um, but we haven't yet had the chance to experiment sort of with more data science workflows or like different domains, um, and things like that or even mobile. So, um, that's something, um, where we'd love to explore and then another thing is that. We don't really know the full failure modes, um, and there are probably failure modes that we haven't discovered yet, right? So we've been running this for days or actually maybe like closer to weeks, uh, but we haven't been running this for months, right? And as you run things for longer. There are more things that you discover like design problems things like that, um, and this is something we're still looking into and we'd love for you to feedback to us so that we can find these uh issues and actually work towards fixing them and optimizing this, um, yeah. Yeah, so let's, uh, let's summarize, uh, you know, the developer experience challenge and, uh, what every team faces, right? Uh, when we, when we as developers spend a lot of time on repetitive tasks, uh, you know, there, there, there's certain things that the agent can do for us, uh, you know, context switching as we talked about, right? If we, if we come back and wanna get into flow, it can take 20 minutes or longer. Uh, that's, that's an issue that disrupts our flow as developers, agents don't experience that, so it's a really. Sorry if I tapped the mic and it was loud, I apologize. Um, uh, the, the context switching can disrupt flow, but it doesn't do that for, for the agents, right? In fact, it's, it's sort of a feature, not a bug. We actually do want it to context switch while maintaining previous information. Uh, you know, documentation can be out of date. These agents will run the write the documentation and the tests for you, so that's really fantastic. Uh, you know, the tests are written as the agent continues to build, and you can actually verify that in the repository. There, there are specifications, end to end specs that the agent has written and committed to the repo. Uh, the technical debt can still accumulate, so this isn't perfect, right? We, we did say in the beginning that this isn't a solved problem. Uh, because this is, you know, an engentic architecture, like there's, there's a lot of nondeterminism and so the human is still very much in the loop. Remember the thinking cap, so, you know, we're, we're still, we're still the ones that, that kind of control the flow, and, and I do want you to walk away with that, that like this isn't about, you know, full automation. This is, this is a lot more about augmenting us as developers with this fantastic, fantastic tool. Um, this will help with most of these things, but it's not gonna, you know, it's not gonna solve everything, and it's not gonna solve everything in every case. Uh, so that's very, very important. Yeah, so here's how you can basically go try this yourself. So the Cloud Asia SDK, which is what we've been working with the framework that we're working with, um, is out. You can go use that. You can go play around with it, test it, uh, and try this out yourself. Uh, all the patterns that we have here are documented in the blog post, um, that Alex mentioned and we'll be. Sharing a QR code so you can scan that uh and get access to it and read through and understand our reasoning in more depth um and then there's also an example documentation in our Quick Start that you can also fork uh and work on so you can really get started today if you wanted to, um, and that's what, um, we'd love for you to do, um, and one thing we wanna. Emphasize and it's really important even though it's at the end of the slide, uh, but it's your uh it's your standards, um, so you're in control, um, the agent follows your standard. MD, um, so if there's something you don't like about the output you can update the standards. It's not a black box, uh, fully, so yeah, we want you to really play around with this and take control. Um, and if you're at Reinvent, um, which you guys are, uh, we'd love to see you at the booth and see it. You can come see it live or even ask us more questions if you would like, um, yeah, and with that, uh, just a couple of the core takeaways on the technical side, uh, you know, we. When we, we, when we were working on this research, we, we thought to ourselves, how can we mimic real developer workflows, uh, and that's what you're seeing here on the slide, right? What are the parts of the developer workflow that we can imbue the agent with so that it does work similarly to how real developers do this work, uh, so you know. Starting out by reading the docs, understanding the current state of a feature before it continues to build additional incremental changes, uh, you know, leaving, leaving things cleaner than than it found it, and really what that means at the end of the day in practical terms is writing tests, writing tests that it can run on every subsequent iteration, uh, and that's, that's extremely critical, right? So leave things cleaner than you found them and that, and that also has to do with looking for errors, right? So what we wrote in our system prompt, uh, is to. And I don't mean system prompt for, for Claude, I mean, for this, uh, for this harness that we created, uh, to suggest that, you know, not only do you write tests, but you also write, um, you also verify that there are no errors in the browser logs. Uh, and so leaving things cleaner than you found them means, hey, leaving things in a clean state where there are no errors for the next session. So that when the next session starts it doesn't start with a whole bunch of problems. Uh, we do have an interesting thing here, uh, that I do want to highlight when you, if you look at the code base, uh, is that, uh, one of the things that we do is I mentioned that we've constricted the session time frame to, to one hour, uh, so you know, sometimes what will happen is the agent will not have completed its work by that one hour time horizon within that session and so we actually have the agent, uh, make a commit right before the session ends, even if it has not completed its work so that no. Matter what, if the agent runs out of time and gets anxious or whatever, it still at least writes the history and then the next agent can pick up and clean up whatever the previous agent left in maybe not such a great state. So that does happen, that it's a failure mode if you constrict that time frame and it can't complete its work in time, um, and then of course the final one which we've already said so many times before, test, you know, test before declaring victory because uh if you just have it write code and not test its code. You're not gonna get the best results, and this is what Nadine and I were talking about earlier. If we ask, you know, Claude to just one shot, it can produce a good outcome, but it might have surprising failure modes, surprising bugs. So just like we as developers, we want the agent to write those tests as it continues the work. Yeah, so that's all thank you so much guys for attending this talk and we hope it was valuable um if you're here, um, we'd love for you to come to our booth, take a look we'll also be sharing the QR code now like that I mentioned, um, so there it is, so feel free to scan it this will give you access to the blog post, uh, the architecture, um, and also, yeah, so a few things so please do scan it, uh, and come and see us and. Yeah, thanks so much, and we do have some time if, uh, I know we might be breaking out a little bit of, uh, the AV, uh, advice, but if anybody wants to give us a question, we can take one, but if we're just gonna have to repeat it and you're gonna have to speak up because it's, we don't have mics, right? So I mean we can go into a lot of different paths with your question, but those are some like, you know, top of mind thoughts that I wanted to offer. Yeah, yeah, of course. All right, well, thank you all so much and uh enjoy the rest of Reinvent. Stop by the booth. We'll be there, uh, basically for the next few days. Thank you so much.