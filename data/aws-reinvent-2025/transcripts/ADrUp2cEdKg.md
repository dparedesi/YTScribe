---
video_id: ADrUp2cEdKg
video_url: https://www.youtube.com/watch?v=ADrUp2cEdKg
is_generated: False
is_translatable: True
---

Hi, everyone. Good morning. Welcome to Vegas. Uh, welcome to Advanced Reg Architectures. It's a Yeah, 4, it's a 400 level talk. Um, I'm Vivek Mittal. I'm AWS solution architect. Be here with AWS for around 3.5 years now and based out of Tampa. I'm part of AIML specialist group, and, um, my focus area is, uh, building applications using RAG or retrieval augmented generations. So that's where we are. Uh, I'll hand it over to Palavi, my co-speaker, so that she can start the session. Thank you. Fantastic. Thank you, Vic. Uh, thank you. Thank you, Vivek. I should have done my speech exercises this morning. Good morning everyone. How is everyone doing this morning? Great? Yeah, do you feel energized to be at Reinvent? Yeah, it's always a fun thing. I mean, whenever I walk through that, uh, hallway, it's such a fun thing to do when the music is going on. You feel so precious and it's just a fun time. So my name is Pallavi Narg. I am a principal solutions architect at AWS. I have been in the IT industry for about 25+ years and and uh out of that around 7.5 years I have been with AWS like Vivek, I am part of the AIML specialist group, so I have been working with Sagemaker and Amazon, Sagemaker for 5+ years and Bedrock for 2+ years and one of the things that I'm most passionate about is the rag architecture, which is retrieval augmented generation, and that's what we are going to talk about today. But before I begin, I just wanted to do a quick um check from the audience. How many of you are familiar with RAG? Excellent. How many of you have uh put rag architectures in production? Fantastic. How many of you are struggling with that ag accuracy? Yes. So that's what we are, um, here to talk about today. So the agenda is, um, first of all, this is a 400 level court talk session. We're not going to be presenting a lot on PowerPoint and since you are familiar, the audience is fairly familiar with rag architectures, we're not going to spend too much time on the slides. We will be going directly to the code. Um, we are going to have a quick overview of Amazon bedrock knowledge bases since that's what we are using to showcase our, um, rag, uh, or how to improve the rag accuracy. How many of you are familiar with Amazon Bedrock knowledge bases? OK, a few, a few, which is good, um, so we will, uh, have a brief overview, but throughout the code we will be explaining the different APIs and the different features that are with the, uh, Bedrock knowledge bases. Then we are going to discuss advanced rack technique and we are going to do the code walkthrough. Now, um, I know this is a rack architecture and improving the accuracy of the responses. I feel it's a science and art by itself. I really love solving complex problems that you will have questions we may not get to all the questions today because we. We do want to talk a lot, a lot about rack techniques, but we are going to be outside in the hallway. So if we don't get to your questions during the session, feel free to reach out to us. And if you are in between sessions, um, my, uh, we are available on LinkedIn. Send a direct message and we will, uh, we'll meet you and answer your questions, but this is something Vivek and I are passionate about. We talk to our customers a lot about solving different problems, so we will be, uh, we are, uh, happy to do that. So, uh, the goal, as I mentioned, is, is to improve the accuracy of your rag architectures. Now why is it important? Because, first of all, you are putting your enterprise data set in front of in your generative AI applications and you're making certain decisions on it. And now the concept of user is also changed. It's not only just a human who's going to be maybe querying the rack. But it is also autonomous systems. Your agent TKI systems are also querying the rag architectures. In those cases it's important to make sure your retrieval quality is is improved as well as your response is improved, and that's the idea that we are going to talk about today. So everyone knows RAG, so we're not going to, uh, dive. What is RAG in action. Just a reminder, 3, I mean, RAG is again retrieval and generation 2 main components, but let's not forget the ingestion of the documents. And now for the purpose of this session, we are going to be focused on unstructured data because that's where we see a lot of issues that occur with respect to improving your RAG accuracy and unstructured. It could be documents, could be images, could be your videos, or it could be a mixture of those things. So document the data ingestion workflow is extremely important. There are certain techniques that you can apply during the data ingestion workflow so that the information that is getting into the vector store is, um, stored well. So chunking strategy, maybe doing a more transformation on the chunks, etc. We, we won't get time to focus on that today. We'll focus on the retrieval and the generation part of it. So the next is again when the retrieval part when the users are asking questions, you are retrieving specific chunks and then you are applying a model, uh, sending to LLM to generate the response for the user. So having said that. For those who are not familiar with Amazon Bedrocknowledge bases, it is a managed, uh, it is a managed feature. It's a managed service for your end to end rag workflow. So as we saw earlier, when you put together a rag architecture, you need to have select an embedding model, you need to select chunking strategy, you need to select what kind of vector store you're going to, um, use. You need to select a LLM. That is going to generate the information. So there are a few components that come into the picture, and they need to integrate with each other. So Bedrock knowledge bases gives you that end to end rag workflow with which with few configurations you can put that together very quickly. And from the data source perspective, S3 is a main data source, but again there are multiple, uh, data sources you can connect to such as SharePoint, Confluence drives, etc. And we're going to see a lot about knowledge bases in the code. So now. We have, we defined, OK, we have a rag rag architecture now majority of the times initially when you get started, maybe in the POC phase, etc. you are going to have your unstructured data, put that in the vector store and start asking questions, standard rag. So some of you may know it as naive rag as well. So basically at that point in time we are not doing additional pre-processing, um, or any post-processing after the, uh, context is retrieved. However, and this works fine if you have in certain cases, especially if you have, in my experience, if you have, let's say not a huge volume of documents or the documents are fairly straightforward, they are not interconnected with each other, or maybe there's not too much of, uh, proprietary data or abbreviations that are around it, Standard rack works just fine. However, as you move towards complex applications, then it all depends on your, well, how complex your documents are. Do you have a large number of documents? Sometimes you may have more than 10,000 documents and suddenly things start getting interesting with respect to rag architectures. You may have your complex. Your users their usage pattern is also hugely dependent because they may be asking some complex queries, etc. so how those rack systems are being used, that's also important. So in all those cases then things start getting interesting and that's where you need to start focusing on the uh quality of uh retrieval and responses. So this is where advanced rag architecture pattern comes into the picture. So advanced rag is again, it's a technique that's going to enhance your standard rack, and the idea is to enrich your retrieval and generation responses. Now there are two main, uh, prominent categories that are out there. Um, again, these are common. There are others also, and this is a it's a constantly evolving field. So there are new papers that are published all the time. But these are the things that are the most common patterns is going to be a conditional branching and parallel branching. Let's take a quick look at what that means before we proceed. So conditional branching now, so in the as you start putting more generic or more complex generative AI applications, you're not restricted to data within just one vector store. You are going to, you will have applications where you will have multiple. Vector stores that you need to query in those cases you need to basically your generative AI applications or it could be an agent that needs to make a decision about which vector store do I need to query from and that is where the conditional branching will come into the picture. Let's take an example. I hope everyone was shopping uh during the Thanksgiving. Let's say you have a query about a product and there is a, uh, there is a customer support. Generative AI agent that's answering your questions if there is a specific query about the product, the agent should be intelligent enough to go to that particular product related vector store. But if you have, let's say a question about, well, what is the return policy for that particular item that you're buying, in those cases it will go around the policy related vector store. So that's the conditional branching. However, things are not so binary. It's not so black and white anymore. We, you want to have data coming in from multiple vector stores and then making decisions from it. For example, if you are, let's say, let's take a manufacturing, um, example on the factory floor there is a maybe an instrument is showing an error code or somebody notices a condition that the certain equipment is not working well. And you have a generative AI application that's going to give you a root cause for that particular error code, but would you stop there? Why would you stop there? Once you know what is the root cause for that particular error code, you are going to get what, well, how to fix it? Is there any step by step instruction? Is there an instruction manual around it? So you would want to get that information as well. And present it to the user. You may also take an action upon it saying that, OK, this might be a potential, uh, root cause, and this may be a part that is required. Do I, can I order it? Do I have that in stock? So you can see that there are multiple, you, you are getting the data from multiple, uh, your enterprise data sources, then you're ranking and pooling that data and giving, giving the right information to your user. So that's where the parallel branching will come into the picture. Now. Well, we said yes, there are different factory stores we can connect to and now fantastic. I, I'm getting my chunks, but does that still improve your retrieval and um generation quality? No. So there are certain going to be again what is the pattern of your usage, how your users are. that pattern. And that is where additional techniques that need to be applied. Now, we are going to walk through a couple of techniques in the code, but query reformulation is one of the most common techniques that we have seen. And again, there are additional techniques, you're going to be re-ranking results, etc. We'll cover a few of them at the end. But the query reformulation is one of the most common practices that we have seen, uh, our customers employ and what is the query reformulation? Basically from your usage patterns you will see that if the query and majority of the time that query will have sub queries or it's a multi-part query or a complex query that is associated in those cases that query will be broken down into multiple pieces, then it would fire. All the queries would be fired up against the vector store. It will collect the relevant context or chunks from the vector store, and then it is going to rank and pool the right response. So this is where the query reformulation comes in, comes into picture and Amazon Bedrock Knowledge Bas does have a feature where you can configure this particular orchestration feature and get started with it. So having said that, we are going to go to the core. OK, so for the purpose of this code now, um, OK, thank you. Thank you, Vivek. So we are going to execute the code in Jupiter notebook, uh, so that some of the, so, A, first of all, for this particular code, we have taken a, um, example of a financial analyst company. So Octank is a fictitious company. It has produced a 10K report. And the 10K report is also generated by LLMs. It's all, it's completely synthetic data. So the idea is we have put together, um, generative AI application that is going to help some analysts ask questions around that 10K document. Now, in order to do that, we definitely, we need a few things. We need to first upload that 10K document to an S3 bucket, so which we have already pre-provisioned. We also have pre-provision in knowledge base because it does take 2 to 5 minutes for the knowledge base to come, to come up with all the different components and the data to be ingested. And again, we are at the mercy of internet gods, so I didn't want to take a chance of something going wrong. So the knowledge base is already pre-populated. Having said that, we'll step through the code. So this is the, um, Asiantic rag notebook that we have seen. Now, one thing is there as part, and we will clarify why there is a Dynamo DB table also that we have provision. The Dynamo DB table is going to be storing, uh, glossary of text. So we have some proprietary, um, abbreviations that our company uses. So those that the glossary of terms has been stored in Dynamo DB. We have chosen Dynamo DB for the demonstration purpose. You could choose any, um, uh, any different storage like you could have it in S3 or anywhere or in the memory itself. So, um, we are going to install the necessary libraries and just a quick look at the requirement. Um, TXT we have B 3 open searches, the underlying vector store that we are using and the different, um, frameworks that are there. And we're going to talk about some of these frameworks during our, uh, conversation. So this part, since the requirement of TXT, um, and the notebook, we have already started, so let's focus on the next part. So now, the, there are going to be some configurations set up. We are using Haiku 4.5 as our model to generate the response on the context. Uh, naturally, first thing we need is the right AWS credentials. So we are just making sure there is nothing coming from the memory, and then we are setting up the right credentials. Now, here we are going to be basically looking at the standard. Um, first of all, we need a board of 3 library and then we are, um, getting additional libraries that are required. So we are getting the handle on our AWS session and we are going to quickly make sure everything looks good from our, uh, connectivity purposes. And yep, I have the account, I have the right region, and, um, things are, things look good. Now, so setting up the creating a knowledge base, and this is already executed, so I'm not going to execute this particular code, but I'm going to explain what this does. So this is where we are going to specify the knowledge base name and the description so that you have. Now, the bucket name that I'm specifying here is going to house my 10K documents. So that's where I'm creating a bucket first and going to house my 10K document. Once that is done, then I'm going to use the Bedrock create knowledge base API. Now, here you will see that there are a few parameters that we are providing. Again, we set the name and description for the knowledge base. The data source is there, which is S3. Now, the chunking strategy, I'm not disturbing it. I have a 10K document which I know is going to flow through. I'm going to use the fixed size chunking strategy. So I'm going to specify a few parameters for me to create the Bedrock knowledge base. However, let's take a look at what it initializes. So this is my, um, it's a utility that knowledge base uh.pi, and then as you can see, there are multiple. There are a few things that I've already initialized as part of my class initializer. So if you look at this, I'm already, I've already selected what kind of embedding model I want to choose. So I'm using Titan embedding. Uh, you can use cohere also. Coher is also available. Uh, then from the generation model, um, here we have by default, we had selected, uh, 3 if nobody specifies, but we are overwriting that with, uh, uh, Haiku 4.5. And chunking strategy is also fixed size, but I wanted to show these, um, arguments to you so that when you are applying knowledge base within your own systems that are going to be different decision points that you can make. You can choose your vector store as well, so knowledge. Bass support uh open search as a vector store and, uh, which is the default one, but you can also have as three vectors you, you can have, uh, postgraora we have, um, that are, and we will come up we'll keep coming up with the new connection so you can definitely define the vector store that you need. So this is where, um, and then again, that is going to be the chunking strategy. We do support fixed chunking. That is a default one that you can select, but you can also select a, uh, semantic chunking or hierarchical chunking based on your, um, use case. And once we have done, so this is where all the initializers that are happening. Now, there is a lambda function, lambda client also that you see. So if you're looking to get, basically while chunking, if you need to preprocess the chunks additionally to extract additional insights from that, you can do that by using, uh, lambda function. So that is an option that is available. So you set up your knowledge base and there are a bunch of other features, um, but I think, uh, just I wanted to highlight the main ones. So using that, now we have created the knowledge base, and this is where we are uploading the Octank Financial, um, 10K document. So this is basically the 10, uh, 10K document that our LLM has generated, and that's the document that we will be ingesting. Now, the next step once you create the knowledge base is to start the injection job. We are not purposefully executing these cells because the, these do take a minute or so and we are, um, we have limited time. So this is something basically we have already executed. But this is where from the uh data store, it is, if it is the first time, it's going to ingest all the documents, but subsequent runs, any incremental files that are dropped within the data source, or if there is an update, that will be ingested when you start the sync job. So once you ingest those documents, now our knowledge base is ready and we can start with the basic rack flow. So I'm going to execute this particular cell and as you can see, I have my knowledge base ID and Dynamo DB which is already created. Um, there is, uh, first, this is the Dynamo DB table that creates, um, and it's just dumps basically inserts these particular glossary records into the Dynamo DB table that we will be using. So now let's execute the basic rack flow. OK. So now, once you have your knowledge base that is generated, that is created, you have, you have access to two different APIs. One is going to be retrieve API and one is going, the second one is going to be retrieve and generate. So Bedrock knowledge base, retrieve API is going to give you just a handle to the context, to the chunks, and then you are free to basically, if you want to post process if there is additional steps that you want to do, or if you may have a small model that you have already defined or it's you already have an endpoint and then you want to generate the response. Responsor using that, you can use the retrieve, uh, API. For our purposes, we are focusing on retrieve and generate, where it is going to not only retrieve the data from the vector store, but it's also going to use the LLM that we have identified, which is the Haiku, and generate those specific responses. So let's look at the knowledge-based configuration for retrieval purposes. So here, we have set that for retrieval configuration, the vector search configuration, the number of results you can return is 5.5 is a default. We are keeping it as is. You can retrieve up to Um, 99, uh, different chunks from the vectors store. But again, right, and then this is a completely configurable, but by default, it is going to retrieve 5. I'm not specifying anything else within this particular retrieval configuration to demonstrate what the basic rack flow looks like. So now, once we have 3 different questions that we are going to ask to our, uh, this 10K document. The first one is, what is the fair value of HTM portfolio? Then what are the main business segments of Octank Financials and how many employees, um, does Octank Financial have? Fairly basic question. My financial analyst doesn't trust the system. He said, let me just, um, yeah, they say just, OK, let me just give it a test run and let me, um, see what that, what it does. So fantastic. We are going to execute this and let's see what our rack. Comes out to be. OK. It is able to find the HTM portfolio value, and it is also able to get, generate the Uh, so here it did, let me highlight that. So this is where it did find the answer for the HTM portfolio. For the second one also, what are the main segments of Octaang Financial? It, it did come up with fairly, uh, comprehensive answer. And for how many employees also, it does have that information right. So let's stay in. investigate what happened here. So in our document, I'm going to just type in to say whether HTM is existing. And as you can see within the text itself, HTM is clearly defined. So the standard rag or the basic rag had no trouble finding the right chunks, and we were able to retrieve those documents. Now, our financial analysts are feeling confident, and they say, OK, let us start querying the things that we basically do it most of the times that we are going to operationally use. So some of the test questions that they have are a little bit more. Oops, yeah, first one, yeah. So the second question that we have, the first question that we have is, what is Octaang Tower and how does the whistleblower scandal hurt the company and its image? Second one is provide a list of DCMs and provide a list of DCMs and how many regional offices. So I am going to execute this part and let's see. OK. So the first query, it did say it could not find any information about Octank Tower, but it did find information about, specifically about the, uh, whistleblower. Now, let's inspect our document. Let's see what we have in the whistleblower. So there is a clearly section that identifies the whistleblower document. So this is why the partially the rack system is able to answer, but it, and does it have information for the octank tower? Well, it does, but it is not being pulled. Into your. So there is a, it does identify there's a headquarter at Octank Tower, but that information is not present in your context that you have retrieved from Vector Store. That's why that information is not present in your, the final output that you have given. So this is where we need to then figure out, well, what are the techniques that how we can improve and make sure all the user questions are relatively answered. And the third question also, the DC about the DCM. Well, let's see if we have anything on the DCM. So, I was encouraged by HTM thinking DCM will also be very well defined, but I do not find anything on DCM because that's the company abbreviation or the proprietary data that is being used. So now we want to improve that RAG accuracy. So the first one, that we are going to employ the technique is going to be decompose and generate. And as, uh, we talked about it earlier, is we are going to reformulate the query, which is we are going to break down those query into multiple parts and then, uh, query our rag application. So if you look at this, at this point in time, my configuration that I'm going to supply to the. Knowledge base I have, I'm still keeping my vector search configuration same, however, I'm adding new orchestration configuration. Orchestration configuration is going to let the knowledge base know that you need to, before you send this response, you need to basically, um, do have additional enrichment steps onto that particular, um, query. And this is where I'm saying query transformation use the type as query decomposition. So that's all I'm going to specify. I'm not going to have any additional steps or any additional piece of code that I need to write. At this point in time, I'm going to have Bedrock knowledge bases used as managed service to decompose the query for me, and we are going to then similar retrieve and generate call, the same API call that we executed. So that's what we have. Now, we are going to just test one question for now to see how it does. So the first question I asked, what is the octank tower and how does the whistleblower scandal hurt its image? Now it is able to, it has already said that I'm going to use the call, the decompose and generate, and it is able to now associate what is my octank tower and how does it, um, how does it basically, the whistleblowers, uh, scandal affects. And we are able to sort of synthesize the response. What do you think will happen if I query DCM? Do you think it will find it? No, because at that point in time, even if I'm having DCM, DCM, the query, it still doesn't know. So even the query decomposition feature at this point in time is not going to help because there is really, it's not a multi-part question. So how do we solve that? And that is where we are going to use the query expansion. Now query expansion, I'm sure you're hearing all the Asian tech tools, etc. So we are going to basically define a tool that is going to now do a lookup on our proprietary abbreviations, which is going to be in the Dynamo DB table and get the relevant data. So, uh, we are using strands agent for this particular purposes. It's again, um, it's our, um, Bedrock SDK to create agents. It's fairly straightforward, but you can choose any, um, other framework to create that particular agent. So we are defining it at the tools. So that's one of the tools that agent has access to. So look up term, you're going to be basically, uh, querying against the Dynamo DB table. And now I have given the project uh, expression as I need to expand upon those terms. So go get me the term definitions. So when I look at that, if I'm going to expand the query, so the function query expansion is going to expand the query using term definition via intelligent agent-based lookup. So it's giving me that, OK, I need to go query the tool right now. So here, I have created my agent. I've provided the name, the model ID. I'm still using Haiku 4.5, and the system prompt. Now I have, I'm, uh, letting the agent know that you have access to this particular tool, and I've also given in tools, I have also defined that as a lookup term. I've given, uh, the information that you have access to this particular tool. So now go and get, returned the information from the, uh, uh, to get the definition from the term glossary and then expand upon it. And then there are specific instructions for this particular agent to call the lookup term for each relevant and it can call multiple times because your query may have multiple questions. So once that prompt gets executed, we are going to then transform this question using relevant term definition. So let's execute and see what happens. So at that point in time, the question still remains, uh, provide the list of DCMs. Now, it knows that the DCM is the term that it needs to expand. Now it has, uh, the result is disclosure committee members. Oh, DCM stands for disclosure committee members. That means I need to now go look at the document and fetch, fetch those results. So that's where I have, um, I got these particular results. There are 3 particular, uh, individuals who are part of the disclosure committee members. Now, we walked through two techniques. Fantastic. Can we do, however, can based on each user preference, can we define which technique to use or instruct any user? No, we need a better way of an Asiantic approach to identify which particular technique needs to be applied based on user query, and that's where Vivek is going to step through the self-corrective rack. Thanks, by the way. So you saw the different techniques, and like Pallavi said, we need a way. That based on the user query, it is automatically able to decide which particular technique do I need to use, whether I need to use the basic rack or I need to use query expansion, or I need to use a query decomposition, or maybe some other technique. That's where self-corrective gentic rack come into the picture. So you have an agent loop. And then you have the different components. These components are nothing, but these are the different techniques that you can utilize. And using that, we developed the self-corrective agentic rack. So, with self-corrective agentic rack, um, you have a user question which is supplied to a central agent. Now the central agent will first retrieve the context out of the data source. This context will then be analyzed. It will be checked whether this context is relevant to the user question, yeah. And based on uh whether the context is relevant on the user uh to the user question or not, it will select one of the strategies. It may be a query expansion, it can be a query decomposition, it can be, it can be a combination of the two, yeah. And once we have selected the appropriate technique, then the response needs to be generated after uh the context is augmented to the user query and uh ask the LLM. Now, once we have the response, we again want to check whether this response is relevant to the user question. This is the 2nd check. First, we did the context relevance, and now we are doing the response relevance check. Now, within the response relevance check itself, um, we want to check the relevance of the user query. To the response, then we also want to check the completeness of the response, and we also want to check the factual accuracy of the response. And based on that, we will either again loop back to select the appropriate strategy again if the answer is not good enough, or if the answer is good, then we have the answer directly. And we will loop till we get the right answer or we reach a maximum threshold so that it doesn't go into an infinite loop. That's the self-corrective gentic reg architecture, a simple one, that we are going to walk through the code. Yeah. So let's look at some real code now. Thanks, Pahlavi. So Pahlavi already covered a few of the techniques. The basic rag Then the quarry expansion. The query decomposition. We will be adding two more techniques over here. One is retrieving the document. but we touched based upon it earlier where Using Knowledge Base retrieve API, we can just retrieve the context out of the vector store. So we will use this tool, which is an additional tool, and we are adding another function over here which will evaluate the quality of the response which is generated, um, uh, from the LLM. So this is another tool and, uh, as we discussed earlier, it will be based on the relevance, the factual accuracy, and, um, um, the relevance factual accuracy and the uh uh accuracy of the response. So let's look at it. So we will first create all these 5 tools that we just see. The initial tool, retrieve documents tool. Here we are using Retrieve API. This is a knowledge-based API. We just provide the KBID and the question. It will be able to uh get the response out of the vector store and provide the uh chunks to us. So that's the only thing it is doing. We are not using retrieve and generated just the retrieve API. Yeah. This is also useful in some cases where, um there are certain uh LLMs which are provided by Knowledge bases, but say you want to use your custom model. So in that case, you can retrieve the document using retrieve API and then, uh, generate the response using your own custom LLM, yeah. So this tool is defined now, retrieve and generate. This is exactly the same tool that Pallavi already went over. So I'll Skip over it. Retrieve and generate API. Decompose and generate. This is again the same tool Pahlavi already went over it. We are using the same API retrieve and generate. The only change here is query decomposition as the query transformation configuration. Yeah. Now, query expansion agent. So this query expansion agent, uh, it's written using strengths agent, which is, uh, open-source SDK provided by AWS, uh, which is used for writing. Agents. You can write agent code using the uh strengths agent. You have QAI you have Li chain, um which you can use. Over here, we have used strengthsagent. So, with the strengthsagent, I have defined the agent, which is query expansion agent tool. Model ID lookup term, this is the same tool that Pallavi already defined, and lookup term is the tool for this query expansion agent, yeah. So this we already went over the court, so I will. Go to the next tool, which is the last tool, evaluate the response quality with agent. Now, here, again, I'm using agent. This is, this is an agent which is a tool, right? So, I will come uh over how to use uh this within the central agent. So, here again, I defined the agent, I provided the name of the agent. The model, which model I want to use. So here, if you see the model that I'm using, it's a different agent. It's a quality check model ID. I'm using cloudsonnet 4.5 here. So, earlier you see for the basic rag, we are using cloud Haiku 4.5, which is a lighter and a faster model. Here, I wanted a reasoning model, and I wanted a model which has more parameters, so I'm using clotsonnet 4.5 over here. So, depending on the use case, you choose the right model. And then I'm providing a system prompt. Now, here in the system prompt, like we discussed earlier, I've, I have provided an evaluation criteria where I wanted to check based on the relevance, completeness, and the factual accuracy of the response and uh provide that, OK, this is the quality score according to that. And that's it. So this agent is defined. Now, I have all my tools defined. And you would have noticed some of the tools were mere functions, and some of the tools were actually subagents. Now, I, I need to create a central agent which can orchestrate the flow between all these different tools. That's where I will create the central agent. And again, I'm using strength agent for writing the central agent. So here, first, I have defined all the tools. The tools have already been defined here. I have just suggested that these are the tools which are there. And then, same way I'm creating the agent. I'm defining the name of the agent. I'm defining that this is the model ID. Here also, I wanted to use quality uh check model ID which is the strengths, uh, which is uh clotsonne 4.5. And then I'm providing these are the, all the tools that you have. And then I'm providing a system prompt, suggesting the same flow which we discussed as part of the flow diagram in the presentation where you have to first retrieve the chunks, then just uh do the uh chunk relevancy. Uh, check. And this, um, uh, this particular agent will do that. And here, uh, if you would have noticed it, I'm using LLM as a judge concept for doing the context relevancy check and as well as later on for quality, uh, uh relevance check. So, Both are LLM as a just concept. There are different tools which are available like Rack Checker or Ragas, which you can also use. Those are open-source tools for doing the context relevancy check and the response relevancy check. Here, I'm using plain LLM as a judge concept. And once I have defined this central agent, So let's execute these cells, right? So first, I'll execute the cell where I'm creating all the tools. And once I have all the tools created, are defined Let me execute the cell where. It creates the central agent. And now I need to kick off the central agent. This piece of code, it's just calling the central agent that we just defined. The rest of the code is just for the display purpose, so that the output is displayed so that you can understand that. Here I will be asking the same questions that Pahlavi just went over. The first two questions are plain and simple question, and then the rest of the three questions are a little bit, um, uh, complex question, yeah. So we'll see how the cells, uh, corrective central uh uh agent behaves over here. Let me execute this particular cell. OK. So the first question. What is Fair Value of STM Portfolio? It first used Retrieve Documents tool to retrieve the chunks out of the vector database. Once it has the chunks out, it will do the chunk relevancy check. So, here, it, it checks, excellent. The retrieved documents contain highly relevant information that directly answers the question. So it is satisfied with that. So, it checked which strategy should I use. I already have the relevant chunk. Let me use the basic rag strategy where it used to retrieve and generate tool. And it found the answer, the fair value of held to maturity portfolio was $1 1.2 billion dollars as of December 31st, 2021. Now, I have the response. I want to do the quality of the response check as well. So, I will check the response quality. So this is the quality inspection agent which is triggered, and it found that it is relevant. The completeness is also good and the factual accuracy is also good. So overall quality is good, excellent. So it identified it as the final answer. It didn't go to the loop again. It found the answer. This is the answer. So the simple answer, it used basic correct technique and it got the answer. Now, let's go to the second question, which is, what are the main business segments of Octang Financial? So here, again, retrieve documents as the first tool. Got the chunks? Let's check the relevancy of these chunks corresponding to the question. The retrieved document contained highly relevant information about Octa Financial. The context is clear and directly addresses the question. So it determined that I need to use retrieve and generate, which is a basic rack, and it found the answer response saying that, OK, these are the different uh investment banking divisions which are there. Once it has the response, it again did the response quality check. The response is relevant, highly relevant, 10 out of 10, complete, and factual accuracy is also good. So the quality is good. Excellent. It used basic rack and it found the answer within one go itself for the first two questions. We're good. Let's move to the next question. Which is provide the list of DCMs. And as Pahlavi showed earlier, DCM is not even defined in the PDF. So let's see how it behaves over here. It did the retrieve documents, it found the uh chunks. It did the chunk relevancy check over here. When it did the chunk relevancy, it sees that I can see that the retrieved documents are about Octa financial data centers, committee, and corporate governance. So this is how it internally expanded. But they don't specifically address DCMs um as a list. So, it is not relevant to the, uh, the chunks are not relevant to the question. So, it used query expansion agent. Let me see, uh, let me try to expand DCM. So it used lookup term, query expansion, uh, uh, subagent, and within the query expansion subagent, it used the lookup term. It found the definition of DCM disclosure committee member, and it provided a new question saying that provide the list of disclosure committee members. Now, let's ask this question. It used retrieve and generate. And it found the answers to the question. And then once I have the answer, I have to do the response relevance check. So let's do the response relevance check. Relevance is good. Completeness is also good. Factual accuracy is OK. The status is good, the oral quality is good. So it selected this as the final answer and suggested that, OK, I used query expansion, then I use the basic rack, the quality is good, specifically, it's checking the relevance, uh, response relevance, which is good, and it found the uh answer to the question with 11 attempt. Now let's go to the 4th question. What is Octane Tower and how does the whistleblower scandal hurt the company and its image? Now, again, it will do the same thing. It will retrieve the documents. It will check the relevancy of the chunks which are retrieved. It sees that It has two parts. It is able to identify that, what is Octane Tower and how does the whistleblower scandal hurt the company. It found that the whistleblower scandal information is there, but it didn't, doesn't notice any information related to Octaang Tower. Now, since it identified it's a multi-part question, it straightaway used decompose and generate tool. It It found out the answer, Octaang Tower is this, and then the whistleblower scandal also, it found out the answer related to that. The whistleblower scandal involving person X, right? Now, once I have this response from decompose and generate, I have to do the response relevance check. So, The response relevance check is done, the relevance is high, which is most important. Completeness is good, factual accuracy is good. So it selected this as the final answer. It used decompose and generate and found the answer. Now let's go to the last one. Provide list of DCMs and how many regional offices. Again, it did the retrieve document, check the relevance of the retrieved documents. It doesn't find information related to DCMs, so it used query expansion tool to find the information about DCM. Disclose a committee member, it again. Uh, now, here, you see, it didn't use retrieve and generate because it identified that it is a two-part question, multi-part question. So here, after query expansion, it used decompose and generate, and it found out the answer, which is, um, suggesting disclosure committee members are these, and then, um, These are the regional offices, uh, which are within North America, Europe, Asia, and O and Oceania. And last, I have to now do the response quality check. So relevance, the answer is in fact relevant to the question. It is complete. Factual accuracy is also good. So it identified it as a good candidate and um uh oh, it actually identified some of the recommendations for improvement. So it again went for retrieve and generate this time. And OK. It found, OK, query expansion, decomposition, basic rack. Again, the quality is similar to what it was earlier, and it identified that, OK, I have the final answer. I tried decompose and generate and I got the answer, this is also similar quality. So it used that as the final answer. Which is, uh, disclosure committee members are this and these are the regional officers. So, yeah, this is how you see um how the self-corrective agentic rag work where Based on the central agent, it is able to utilize the appropriate strategy. And again, we have quality inspection and the chunk inspection as one of a few of the strategies that it is using for doing the looping. Now, these are a few of the techniques that we have, but there are several different techniques which are there, but we have limited time. So, um, That's where we have to actually, um, I, I just want to touch up on a few of the other uh techniques which are there as well. So, Pallavi touched upon ingestion flow as well as the retrieval flow. And within the ingestion flow, you can enhance the quality of the chunks which, and the embeddings which will be ultimately stored in the vector database, which helps in ultimately improving the retrieval accuracy as well. So some of the strategies are chunking strategies. We use fixed chunking over here, but there are other chunking strategies like semantic chunking where it, it keeps the semantically similar. Uh, uh, embeddings near to each other so that within the same chunk, you can find it. Other, uh, strategies, foundation model parsing. So say you have a document which has a lot of images, graphs, uh, these kind of things, then you might first want to use foundation, uh, foundation model to actually ingest the data out of that document and then ingest that data within the vector store. So this is another technique which can be used. Multimodal parsing is another thing where, say, you have images within your PDF and when you are retrieving the documents or we are, and we are generating the answer, you want to show the images also as um along with the answers. So that's where the multimodal parsing come into the picture. Metadata labeling is another technique where you can define that, OK, you have multiple documents within your data source and you want to label it so that when you are asking a question, depending on your question, it will identify, I have to use this particular label document and it, it can directly get the answer from there. These are a few of the techniques for the ingestion flow. Then we have certain techniques within the retrieval flow. We touched upon several techniques here, but the metadata filtering is another technique where uh we can use the metadata, we can specify that, say, say, for example, um, You have a scenario where um um you have say English um say there it's related to educational domain where you want to get an answer for English or um science so it can use based on your metadata, it can use the particular document from your knowledge source because it is labeled reranking is another technique when you get the chunks out of the vector store. You actually want to re-rank them so that most relevant chunks are on the top of the search, and then it can find out the answer out of that. Hybrid search is another technique where it is, you can use the combination of semantic chunking, semantic search, and the keyword search. So, with semantic search, you get the semantically similar answer, but sometimes you want to use the keyword search for exact answers. So it can identify which is the best technique, and it, it can use that particular technique for answering the question. So these are a few of the techniques which you can use for further enhancing your rag accuracy. And um They are, these are some of the uh blogs and papers that you would like to um just have with you and um uh look at it for some further learning. Yeah. We are open for questions. Uh, we have, uh, 7 more minutes, and, uh, we, we will, we'll try to answer as many questions as we can, uh, but we have to leave at the end of the hour, so we will be available outside for any further questions. And just, I think, uh, just to conclude, I mean, we did see the whole goal for us was to mainly divide this into first context relevancy check and then the retrieval. Relevancy to improve your rack accuracy and that's where the sort of the decisioning with the self corrective rack came into picture is to identify so just when you are thinking about how do you, how do I improve the accuracy of the rack, think the document ingestion, think about the context relevancy that's the first thing you can check and the retrieval relevancy.