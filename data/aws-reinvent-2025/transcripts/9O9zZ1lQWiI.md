---
video_id: 9O9zZ1lQWiI
video_url: https://www.youtube.com/watch?v=9O9zZ1lQWiI
is_generated: False
is_translatable: True
---

Hi everyone and welcome to Dev 415 where we are gonna look at how to build scalable self-orchestrating AI workflows with A2A and MCP. My name is Gunard Giros. I am a developer advocate at AWS and my name is Alan Helton. I'm an AWS serverless hero and an ecosystem engineer at Momento. So this is a 400 level session. So we're going to assume that you know some things about AWS and certain things about the services we're going to use, but we are specifically going to focus on a few things. For instance, the agent to agent protocol, A2A MCP model context protocol, and some of the specifics around the services we're using to build these agents. And we're gonna do it by showing a lot of the concepts we're gonna jump in and show some demos as well all throughout. So this is basically what we're gonna cover. How you can design these self orchestrating systems using choreography instead of central control, we will look at, like I said, the A2A protocol and MCP, how you can use that so they can discover each other, these agents, and how they can do that and. Invoke each other in a dynamical way and then how MCP gives agents the consistent way to access tools, context, and data and then finally how agent core and lambda can turn these agents into autonomous scalable workloads. OK, so why are we here? Why is there even a session on this in the first place? Well, if anyone here has used AI agents before, generative AI, we should already know that generative AI doesn't really follow linear playbooks. It's kind of the wild west. You never know what you're gonna get. Reasoning jumps based on context, definitely not on your code, so we need to prepare for that. And every single task that you give an AI agent can potentially take a different path even if you call it multiple times with the same context. So because of that, that whole paradigm in general, static wiring with traditional orchestration services don't really work the same way. So what we need is to break out agents and we need to have agents that talk to each other, agents that collaborate with each other at run time. Ones that can adapt work flows safely. And ones that have stable outcomes, even if their reasoning isn't always right on track. And this really can be boiled down to the debate of choreography versus orchestration, and it is, it is kind of, kind of. Choreography is powerful, but if you've ever used it, it can be a little bit tricky when you start introducing, uh, non-determinism in your structure. And as much as we'd like to think that coordinating agents is just this ideal pie in the sky, it's a lot like a game of telephone if you've ever played that where each agent along the chain adds its own interpretation instead of clarity on the commands that came in. And so these small little deviations compound into major workflow errors or very, very different outcomes, uh, if you don't set it up right. Maybe you have the wrong collaborator inside of a workflow or you have the wrong action either way you're gonna get the wrong outcome, which is really what we're concerned about. Collaborating on a shared plan separating that work out, uh, is really what we're looking for. So if you don't have that, which sometimes can be seen in choreography, uh, you're not gonna be able to reproduce results. You're not gonna get consistent outcomes inside of your complex workflows. So debugging and when you're actually in production, practical considerations, uh means that you have to reconstruct uh he said she said kind of deal and that's very difficult. So what we're here today really to talk about is that autonomy without rules is chaos, absolute sheer and utter uh use case for no success. And that's why. We're gonna be talking about deterministic coordination and really hounding on determinism in a non-deterministic paradigm. So we're gonna jump straight into our first demo before we dive into actually using multi-agent coordination. We're gonna start with one single autonomous agent. So that gives us all a clean baseline basically to see how agent in the loop in action without anything else involved. So think of this as kind of a warm up to it all one task, one agent, and one clean workflow. So once you understand how this works, I think a lot of the things around multi-agent will become much easier to understand as well. So this is the first demo we're looking at a think very common situation in the last mile delivery. A a driver arrives at a customer's address to drop off the package, but the customer isn't home. So instead of turning this into. A manual exception that someone has to follow up on later, we want the system to take the next step automatically. So the moment the driver reports that the delivery attempt was unsuccessful, the agent will then evaluate that situation. And this scenario is intentionally very simple. It shows how one single agent can then react to these real world events, interpret what it means, and take an appropriate next action. So like I said, it's there to give us a baseline for this. So Alan, take it away. OK. So here we have our delivery driver web portal. This is what they look at whenever they are unable to deliver a package, and this is for my logistics company, Swift Ship, #1 competitor to Amazon.com. And as you can see here, We're not, we don't have chat interfaces really we're just filling out a web form we're gonna say that the customer is not available for delivery. We left a notice on their mailbox and we'll attempt redelivery tomorrow. This is a scenario I imagine many of us have run across before. Very frustrating, but it is the fact of life. And so what we're gonna see here is what we're gonna uh is a peek behind the curtain for what is actually happening behind the scenes. And you can see here that. Uh, and this is updating in real time, by the way, so you can see here that we're gonna schedule that order for red delivery with our order management agent and then we're gonna notify the customer of that rescheduled delivery. Now, like you said, this is an intentionally simple demo, probably the most boring one you'll see this week. Underwhelming, I think is the Underwhelming, yes, that's the word I was definitely looking for, uh, the word I use to describe all my demos. Uh, but the point there is that that's an agentic workflow. That is about as simple as it gets, and it has run through everything that we are about to talk about. Yeah, so before we start wiring these agents together, we need to align on some of the technical primitives that make these autonomous systems viable. So these aren't high level concepts, they're mechanisms that then keep these multi-agent workflows deterministic like Alan talked about and reliable. So if these foundations aren't solid, well, choreo choreography is going to break down quickly. So let's look at some of these foundations to begin with, and I've mentioned agent to agent, A2A protocol a couple of times, and that is what gives our agents a consistent way to. Discover each other and then also to collaborate. Each agent, they publish an agent card that defines its capabilities and the version of each capability that it has and the input schemas that it's gonna expect in our architecture we are using these capabilities and exposing them through lambda function URLs you'll see that later. It means that each agent is reachable through just standard HTTP endpoints. And when one agent needs another to perform some sort of work, it is then just gonna create a typed A2A invocation, and this invocation includes the capability name, the version, the task identifier, and some sort of structured JSON payload that then conforms to that schema that was advertised in the agent card. Then we had model context protocol MCP and that's, I think most people now know what it means. It feels like it's been around for ages. It turned one year a month ago, yeah, if even something like that so but it feels like it's something we all know by now but MCP provides us with this standardized way for agents to. Load context and interact with tools instead of passing raw text or unstructured data, agents can call these MCP tools that expose typed, validated interfaces. So that could be shipment data, order information, it could be inventory levels, other types of workflow inputs. That are all retrieved through these tools. So MCP is what ensures that every interaction follows a schema that remo removes ambiguity and improves the reliability of model reasoning. So MCP tools they. Define the strict input and output shapes, agents will then always work with predictable structures, and that's super important because it means that the behavior of the system remains stable even when multiple agents are relying on that same data. And then we have our core services, of course, and The system is built on 3 core systems or services for us together that then support these autonomous agents. The agents themselves in our architecture, they're going to run as standard lambda functions, so nothing strange about that. Each lambda. is isolated, which means that every agent loop starts with a clean state and it loads exactly the state that it needs to have once it's invoked. So lambda, as we all know, it gives us elasticity, it's predictable concurrency behavior. So the things we need to be able to build these scalable systems. We have Bedrock Agent Core as well, and we're using two capabilities within Bedrock Agent Core. We're using memory and we use observability for our agents. So since lambda doesn't have any built-in state between invocations, we're using agent core memory as a way to store and retrieve task information, and that gives us a consistent view of what. What has already happened basically within that workflow even when multiple agents are contributing and collaborating on the same tasks at different times and then we're using Memento that's where Alan works. Momento is our, I think we can call it our fast coordination layer. It's using this. Um, Memento cache for lightweight state cached MCP tool responses and quick lookups to do, to do things that really don't belong in any durable or long term storage. So this gives us low latency access to shared information without slowing down that entire agent loop that we look at. So those are our three main components. And before we go any further, we've talked about what we're building, but now we're gonna talk about what specifically we're gonna be building. In order to do that, I kinda wanna take you 3 years back in time when Chat GBT was first introduced, it was a marvel at the time and. Looking at it now, it's still great. But it was a chatbot. I ask it something, it responds to me. I ask it something else, it responds to me again. And that's really the extent of the exchange I can have with a chatbot. Fast forward a little bit and the model context protocol releases uh last year and agents started booming. Everybody started making agents. Now what's the difference between an agent and what we were doing with Chat GBT? Well, an agent is basically a chatbot, but it's been empowered to do something. It's been given code via tools. That it can actually go and do things on your behalf so if you say go book me a flight or what um. You know, where can I go to buy a pineapple for my pizza. It'll look it up, say, oh, that's on Maui probably let me book you a flight there and it'll do that. It'll actually do that, but it's part of that exchange. But that's really not it. That's not what we're building today because that's still a chat interface. I'm talking to it, it's talking back to me, but it's also doing things too. What we're talking about today is autonomous agents. These are agents that you can't see. These sit behind and are placed inside of your architecture, ones that hopefully you're already pretty familiar with that run a sink in the background that never see humans. These are responding to events in your system. These are actually responding to to things that happen. So in a, in a very basic uh situation really what uh the Swift ship delivery system is doing for us is inputs come in via API gateway. There's a compute layer sometimes with lambda goes to Dynamo DB. And from there we can execute our async workflows with something like EventBridge that then passes all the data necessary to an agent maybe running in lambda that responds to that specific event so very different from a chatbot that has been given a tool. This is something that's interacting uh as a result of something happening in your system. So the autonomous agents that you'll see here today, they are built on what we call the agent loop. It's a very simple but yet very powerful way to look at agents, and I think once you understand the agent loop. A lot of the entire architecture makes a lot of sense straight away. So the agent loop is the execution model that every agent should follow basically no matter where it runs if it's in lambda as a lambda function, if it's running in agent core or in a container or in the EC2 instance anywhere, this loop isn't tied to a specific compute service. It's more of a discipline of how to build these autonomous agents so that they behave predictably at scale. So each iteration of the loop, it looks the same. We start with the compose phase. It loads its typed context through MCP tools and retrieves task state from agent core memory, for instance. Then we move on to query where we call the model, and in our case that's using Bedrock where we call the model through and that gives us a structured tool result and a schema aligned output. Then we have the execute phase where we actually do something and that maps that structured output from the LLM to real actions. Those could be A2A invocations, it could be an API call or memory updates, for instance. And now we have a question Should we continue the loop or are we done? So we decide whether the workflow is complete or if we should persist state and then run another iteration of it. So by following this pattern consistently, each one of our agents, they become re-entrant, they become even potent, and they are safe to run in a distributed system. So let's break down these phases to look more exactly what that actually means. So the first one compose, that's the phase where we transform an incoming event into a deterministic input package for each or for the model. So even though our agents run as lambda functions like we've said, this loop structure is platform agnostic. And in Compose, the first thing we do is we hydrate context through, for instance, tools, and these tools enforce strict schemas and validation rules, and the agent always begins with typed, structured data instead of. Having loosely formatted prompt text like in a chat for instance, so after that, the agent reconstructs any prior task state that it might have from agent core memory in our case and because lambda functions are stateless, this is what makes the loop reentrant. Every iteration will load its own history and the context instead of relying on warm function state. And once the data is assembled, we start building that system prompt that we use and we follow something called the reason framework. This creates a stable contrast for the model and its behavior. The role section defines exactly what the agent is responsible for, so it stays focused on its role. Instructions specify how the model must behave. And that includes constraints and error handling rules. Then steps outline the deterministic procedure that each agent follows on every iteration, and then expectations that describes the precise output shape that the model must return and that's anchored in the same schemas that our code will validate and then finally narrowing. It makes the agent's boundaries explicit by stating what it must not do. So to make it even more clear, this is a simplified version of the system prompt used by one of our agents, uh, and it's structured as you can see according to R. So when you read through it through that lens, it becomes clear that it's not just a block of descriptive text for the agent, it's a behavioral contract that it must follow. So we have the role for the specific warehouse management agent. We have the instructions, we have the steps, ordered steps for it, what it should do. Then we have the expectations, what it's supposed to do, and then narrowing. Making sure that it stays within its boundaries. And at this stage of the compose phase, the agent is not only building its system prompts using RSen, it's also reconstructing its understanding of the task by loading state from agent core memory. And since every every landing vocation starts in that clean environment like I said, the agent depends on this memory to be able to restore the context of the workflow, and it includes previous decisions, intermediate results, correlation identifiers, and any other types of events that are generated during earlier steps in the workflow. So agent core memory, it is structured and it's queriable, so the agents loads that specific name spaces instead of pulling out big blobs of opaque text. So this allows the agent to then build its prompt in a deterministic way. So the memory is also optimized for re-entry, which means that the agent can safely then resume work even if the previous invocation perhaps timed out or had some sort of retry as well. And agent discovery is in this system. It is driven entirely by A2A, and it's using something called agent cards, so tiny Jason text. You don't need to read it really, but this is a document that is the authoritative description of what an agent can do. It is machine readable capability surface, so instead of us hard coding these RPC codes or maintaining some sort of registry for our agent, instead every agent will publish this capability card at a well known path and then when another agent needs to delegate work, it fetches these cards, interprets the capabilities, and decides how to interact based on the contract the card exposes. So it defines the name of the agent, its purpose, the actions it supports, and the schemas for input and outputs of each action. We have a more visualized way of showing it as well, like a proper card is something Alan hacked together. I think you have it available so people can see it at the end as well in the resources, uh, but it's kind of a, a, a friendlier way of representing these agents and these cards in the same information, turning that JSON into a capability graph basically that shows the actions that the the agent supports and how those actions fit into the larger workflow. OK. So you've assembled all of this context, you've built a system prompt, you have fetched additional data to build the user prompts, you have loaded agent cards into memory. Done everything that you need to do. Now it's time to feed that all into an LLM. But it's not that simple, unfortunately, never is. Because models vary wildly in their output and their behaviors. I asked two different LLMs the same question, all, you know, related of course to Swift Ship, and we were doing our, uh, interviews for LLMs. And just gave it a prompt, what would you do in this situation, and I asked a. LLM that's known to be very clear, very blunt, coherent, uh, command R, and it gave me a blunt answer. And then I asked a model, it's a little bit more capable, does some more reasoning, it's not, not quite as blunt, and it gave me a longer answer and probably a better answer for the situation that I am using it for. So the, the model differences here, they're not just stylistic, really they have to fit the scenario that you're building for. And uh that will ultimately end up shaping how the loop behaves because it's going to be responsible for planning out a multi-step workflow. And speaking of that, this is about time where we branch into something a little bit deeper with A2A where we introduce two different types of agents. Used primarily with A2A and multi-agent collaboration, we have supervisors and we have workers, very much like an engineering team, exactly like a tech lead or a manager. And individual engineers that are actually doing the work. So the supervisors and the workers, they run the same loop, but they have very different needs and when it comes down to which model you're selecting, very different, uh, model capabilities that they require. Supervisors are kind of that orchestrator. They need to know everything. They need to be responsible for global reasoning, so a planning out multiple steps in a long workflow. They need to be capable of dependency resolution. So what do I need in order to satisfy this or given the agent cards that I've been presented, who do I need to call to do this step or that step? And also they need the ability to be able to judge an output from a worker. Is this good enough? Does this satisfy the reason why I called you in the first place? Workers, on the other hand, are, to put it simply, the grunts. They need to run fast, they need to run cheap, they need to run deterministically. And they need to be really schema perfect, meaning if I tell you you need to have these parameters to operate this tool, you need to use those parameters to operate that tool. So, when we're thinking about this phase in agent loop, querying. Really there's model selection, but there's also agent selection, so you have multi-layer reasoning for what you're looking for with the query. So that's why you have for supervisors more reasoning capable models like Nova Pro or there is a Nova Premier. In our one of our rehearsals I said that and Gunnar laughed at me because he didn't think that was real, uh, or you know, Sonnet or Opus 45, ones that are really capable of strong reasoning, uh, versus ones that are workers that do very specific things so you could do like Nova Canvas if you're drawing an image, uh, or one of the sonnet or the haiku smaller lighter models that you need something just real quick and real fast. OK. I don't like you calling me out. So, alright, we've composed and we've now queried the LLM and now it's time for phase three where we actually do something in the end and like in our previous demo. Now we can do some sort of simple action where we turn the model output into real effects in the system and The first step of course is schema validation. We need to make sure that the expected output shape from that recent prompt and everything that doesn't conform to that is gonna be rejected basically. This protects the system from malformed model output and any type of partial structures or drift across retries. Once the output is validated, the agent is then gonna perform the action described, and for instance, that could be updating a database. It could be calling an API. It could be writing to Dynamo DB, updating state in agent core memory, publishing event, or anything else. Every action should use item potency keys that we derive from the task ID, so retries shouldn't be able to create any duplicate side effects. Most of the time execute produce some sort of concrete system update, but in some cases the action the model requests isn't a local update. Instead it's gonna be a handoff when the model indicates that another agent should take responsibility for the next step, execute isn't gonna perform the work itself. Instead. We're gonna look at A2A agent invocation. So execute has now determined that the next action isn't any local update. It's gonna be a handoff to another one. So we've composed the prompt, we've queried the LLM. We're trying to execute something and this agent can't do that thing themselves. So what happens then instead is that we construct an A2A payload that encodes the capability of version, the required typed input, and the task identifier, and this is gonna conform to that A2A protocol. So the specification of the protocol tells what this message is gonna look like. The receiving agent is then gonna know exactly what to do when it receives it. So we then publish this message and that's gonna launch an entirely new agent loop where we start off with composing, we're querying, we're performing some sort of action, and that could be a local one or it could once again spin off to a new agent loop to do something. Then it decides if it should continue and if that one is done, we're closing that loop. Reporting that it's done and can do the same decision on that initial loop as well. So this is how we get that distributed, scalable and deterministic behavior without having any central orchestrator. The agents define if they need to launch a new agent loop. Alright, phase 4, let's close the loop on closing the loop. We're gonna again talk about supervisors and workers here because this is actually a really important piece and. Before I go into details on continuing the loop, I wanna talk about where we are in code. Not gonna show code, but we're just gonna talk about it. So if I've written uh an agent, phase one is composing the prompts. This is actually code that you write in your applications that's loading data, that's assembling string. We've queried the LLM that could be the bedrock, converse API invoke agent, whatever it is. We've done execute action. As far as the code goes, we're still in that converse call. And now we're on phase 4, which is continuing the loop. We're still in that call, so we haven't received a response yet. This is still in the capabilities of that converse or invoking the actual agent. And what it's doing here, it's done work and it's asking itself, do I have enough information to finish the task that I'm working on. And based on whether or not it's a supervisor or a worker, it's gonna evaluate different things. The supervisor, of course, has its uh global perspective. What was the original higher arching task at the top? What was the command that came in? What's the event that I'm responding to that I need to uh satisfy? It looks at the results from the other agents, basically the output from the tool calls it made when it invoked the sub agent loops. And then it's also considering the unmet capabilities that track back to that global parent task. It's doing things like validating the scheme as returned by the workers is this actually what I asked for? Does it have everything that I need? And what it's doing here is it's figuring out, do I need to call more agents, do I need to wait for some to respond, maybe it's gone async and I need to wait, uh, or am I actually done and I can really continue on with the code. That loop's gonna continue so if that answer is I need to do one of the, the first things, it will go back and it will build that system prompt again it will execute actions it will. To do everything that it needs to do, uh, the worker, it's roughly the same thing, but it's scope of what it's concerned about is tighter, a little bit smaller, because it's not doing as much work, it's not doing the planning, it's figuring out, you know, did I satisfy the individual work item that came in in A2A terminology that's a message. There's a task that goes across all the agents and then there's individual messages that go to the sub agents. Did I do what the message said for me to do? Yes, cool. Maybe I need to run some more tools and do some more work. Maybe we need to validate the results again. This is before you get a response back in your code, so it's gonna go and it's gonna decide do I need to refine my results? Do I need to do it again, uh, for that one very specific requirement. OK I think it's time it is time. We're gonna talk, we're gonna make things a little bit harder for the uh swift ship delivery. Let's set it up first. Build it up, build up the excitement. Now we've seen firsthand how a single agent can satisfy a simple prompt. Customer wasn't home. I left a note on the door. I'm gonna reschedule it. Cool. That's not exciting. Whereasus reinvent, we have exciting things that we're gonna talk about. And I know I'm not supposed to spoil what the what the scenario is I'm, I'm not gonna talk about that, uh, but what I am gonna talk about is why we need multiple agents specifically. And What you don't want, actually, first, show of hands. Does everybody know what a monolith is? Oh, there's a lot more of you in here. I can't see you with the lights. OK. Everyone knows what a monolith is. Are they good? No. OK, alright, uh, so there's a new term on the streets they call it a mono agent, and it's an agent that does everything, has access and the capabilities to access your payment systems, to access your ordering, uh, services, has access to everything in the database. Think that's a good thing? Nah, it's not. It's not. What we need to do is we need to scope agents down to specific domains. It's And in the case of Swift Ship in our example, what does that look like? You know, we, we say that we need to scope things down to domains. Well, we're going to have a supervisor agent that I call triage agent, figure out what to do. Here are your workers, and those workers are are designed to be for a very specific part of my application domain. I have an agent for payments, one that is tasked only with handling money. That's its only job. It's not responsible for anything else besides money. I have another worker Is responsible for the warehouse inventory. Do I have what I need? Do I need to order more things in the warehouse? Do I need to allocate stuff for an order? And then I also have an order agent, worker. And this is the one that's actually responsible for moving that data along. I can update statuses. I can duplicate orders uh if I need. And so what we're doing here is we're exposing capability surfaces based on domain. And not one single agent is allowed to cross those domain boundaries. Focus it in, tighten it up, only have the permissions for one area of your application. Said a lot of words. This probably sounds familiar to a lot of you. Sound familiar to you? It does sound familiar to me. And has anyone heard about microservices? It's kind of familiar to how these agents are working. So even though the agents, they use LLMs for reasoning, they behave much closer to distributed microservices than I think most people expect the same architectural patterns that appear. We see here, but they surface through A to A interactions instead and the agent loop instead of having rest calls perhaps or service meshes. So agent discovery works in a similar way to service discovery. So instead of registering in some sort of mesh or um DNS layer, each agent is gonna publish that agent card that I showed before. Exposing capabilities, versions, ski masks, and so on, and then other agents can then resolve these capabilities during runtime. So this gives us late binding without having hard coded knowledge. And health behavior, well, it also maps quite clearly to how distributed system works. Instead of polling health checks, agents will infer health from behavior. They will, let's say an A2A request fails, um, some sort of validation or it times out or it returns some sort of inconsistent data. Well, the system will naturally retry and it chooses perhaps a different execution path. So there is no central health manager in place. Instead, the system converges on healthy behavior because agents respond to real interactions, not through external probes, and task distribution, well, it works in a similar way to, to load balancing basically. We rely on agent core memory and distributed state to coordinate these task IDs that we have. But also item potency keys, progress markers, and so on. So a lot of this is very familiar to things we've been doing for for. May I say decades now even. But when we move from that single agent to a multi-agent workflow, um, the system only stays predictable if every interaction is deterministic. Alan talked about that before as well. Choreography can remain elegant at scale, but only if each of these agents follow the rules that remove randomness from the workflow. So the idea here is simple if the inputs are the same. Well, the outcome should be the same even when several agents are involved and running in parallel maybe. So we begin with clarity and prompts. Every agent should use the reason pattern. The model should start with a stable contract for the behavior of the agent. The role section should limit the scope of the agent. The instruction defines the rules it must follow. The steps gives it predictable procedures that are repeatable. And then expectations define the output structure and narrowing section prevents it from drifting into responsibilities that it shouldn't have like Alan talked about, so this removes interpretation. Type errors that can then ripple through any type of distributed system. And item potency is super important here in any type of distributed system. retries are not an exception. They happen all the time. It's regular to have that because of network variations or perhaps cold starts with lambda functions or timeouts in different parts of the system. So every agent must be able to receive the same instruction multiple times without us ending up transferring money multiple times to Alan's account, for instance, and this is why we use task IDs. This is why we have memory checks, consistent verification before executing any action, and we also make sure that the IP APIs that we create, that they are atomic each. Call should represent a complete and safe unit of work. There shouldn't be any partial updates within these systems. If an agent updates state or performs an action, the work, uh, the workflow should be able to perform it incompletely, completely, or it should be able to roll back if needed. We've pushed this ahead quite far in this presentation, right, Alan? I think it's time. Yes, we did kick this can down about as far as we can. Can't really talk about production software without talking about observability, at least a little bit. And we're gonna talk about this before we even do the demo because A2A really when you're talking about multi-agent collaboration, uh, but specifically with A2A has a couple of really cool things that I wanna make sure that we know before we see it. So when you think of observability, typically think of three things metrics, logs, traces, it's the same, it's not really any different with agents it's definitely plenty of talks this week going, uh, very deep into this subject, but. We'll talk about it at least at a high level. Uh, bedrock Has a very brand new observability dashboard. I think 2 weeks ago came out with this that just works if you use Bedrock, if you use really any of the APIs inside of Bedrock, you get to see a cloud was dashboard that rolls up all the important metrics for operational cost performance, uh, things like what's your total invocation count, what's your per model token spend, what's the invocation latency. What are the input tokens versus the output tokens things that are going to. Act more or less like non-functional requirements, but definitely requirements on your wallet. And Bedrock does actually or Cloudwatch does a really good job at servicing these things front and center for you without really taking anything into consideration. You don't have to build anything to get this. What I want you to take away here metrics are important, but you can't really optimize multi-agent systems without metrics because you're gonna be using a lot of different models. You're gonna have different models for workers you're going to have different models for your supervisors, and they're gonna be taking in different amounts of tokens. They're gonna be spitting uh output tokens out at different rates, so you need to have that insight so you don't spend all your money on. LLMs. OK, but here's the cool thing. Here's A2A that does a really cool job uh for observability and specifically with traces. And a task, an overarching task in A2A, something comes in, a supervisor is going to start up a new task. It's a multi-step plan. Go all the way across the life of this invocation. Supervisor lives with that task and then it's going to cast out these messages to its sub agents to actually do the work. And A2A provides a fantastic opportunity to hook in and watch in real time as these orchestrated workflows pan out. And this is where, uh, Memento comes into play. Uh, we have a service called Topics that's a centralized third party, uh, event bus. You can just hook straight into these messages inside the A2A protocol and. Subscribe to them in the front end, which is why we're talking about this now before we see the demo. So at the beginning, at the end, even at tool calls in the middle for these agents, you get opportunity to see what on earth is happening in this formerly black box that is agent execution. OK, we talked about observability. Yeah, it is finally time for the, the, the real star of the show, which is our full multi-agent workflow demo. This is where it gets exciting. So new scenario. Alan or one of his drivers is out there trying to deliver a package, but when now we have a bit of a bigger issue. Um, driver arrives at the stop and discovers that the package has been damaged. We can't just schedule a redelivery in this case. Needs to trigger a bunch of different actions inside the warehouse in the order department, and so on to make all of this happen, so. Now let's see how that same system is able to handle this type of scenario. Let's jump over to. OK, so the reason that we use. Agents in a scenario like this is because of this comment box right here that says notes, we can code for anything that happens in exception type, but notes, this is free text, anything can happen in the notes. So we're gonna test it, we're gonna see what happens. I have in here package severely damaged during transit. Contents appear compromised. Let's make that worse. Contents caught on fire, then we're run over by my car and the car. Behind me. Then the ashes caught on fire. This was my fault. Typical delivery event, yeah, we, we see these a lot at, uh, at Swift Ship actually. It's kind of a problem. And so I had a photo prepared, but I did the hackathon yesterday, and now I just have 100 pictures of eggs, uh, on my machine, so I can't, uh, I can't find. There is the picture of a package it's a package that was on fire that was run over by a car, uh. Instead, I, I'm not gonna show a bunch of pictures of eggs, uh, so we're gonna submit this. So now that we've done this, the triage agent sees this package that got run over, it caught on fire, the ashes caught on fire, they're gonna run over again. And there's definitely things that have to be done more than rescheduling that package for, uh, delivery. Gotta replace it. We're a good delivery company. We'll take care of your packages for you. And again this is happening in real time. So I'll zoom in, we'll take a look uh at this in a minute, but we're gonna let it, let it run its course here. We have multiple agents. That are in the mix. Now I see why you wanted me to record a demo before instead of doing it live, uh, because this is not what I expected but. I feel like that just drives the point home that agents are nondeterministic, so what we did see. Is it an incomplete workflow uh of the the. Triage agents saying hey we need to refund that package it was destroyed we're going to give them their money back. And then we're gonna toss the responsibility over to the warehouse agent and we're going to see. Is there, uh, is there inventory left in the warehouse to actually do a full replacement and send it back out? My guess what happened is I didn't reset our demo data and the, uh, there was no inventory. So what it's supposed to do, this actually might have been a good, a good demo. We can restart now, it's fine. Uh, what it's supposed to do is allocate the inventory, say yes, I have it, and the triage agent comes in and says, oh great, OK, we're gonna duplicate the order, we're gonna allocate that inventory again. And then we're gonna finalize that payment to payment processing. But that didn't happen because I forgot to reset the demo data, uh, but we did see. Multiple agents actually collaborating based on that wild west that is the notes on the delivery system. Let's quickly jump over just to show the architecture of it. We've been talking a lot about this. I think it's cool to actually see what this means and. It's nothing strange really. It's similar to what Alan showed earlier as well, a very simple one. It's Amazon API gateway. And he submits something in that form that calls a lambda function that just stores something, the delivery status message in Dynamo DB. It's connected, has a Dynamo DB stream that sends an event to event bridge. We have a rule there that picks up that event and it's gonna trigger our triage agent. That's the first agent. That then decides it runs the loop. It decides what it's going to do next invoke the payment agent, invoke the warehouse agent, the order agent, and so on. And all of these are using. Bedrock for querying the model but also for agent core memory. So it's a super simple and very common serveless architecture. There's nothing strange about it. So let's then move over and see some code as well. Yes, we can talk about it and see demos all day, but for all the engineers in here, if you're anything like me, you're not really gonna get it until you see it, and we're gonna take a look. So A2A, we talked about the agent card. Which is how supervisors know what you can do. This is basically their, their, um, ID card has the name, has the description. What can I do as far as capabilities go? Do I stream? Do I push, uh, but this is the important part here. This is the part that can change on the fly and in the future when supervisor agents need to load information about the order agent, they see, oh this guy, this order agent. He's responsible for changing order statuses or duplicating orders and we have in here no schemas. This is only natural language. It's the beauty of A2A we're not really uh doing anything that's tightly coupling us. This is just natural language. What can you do and what can I ask you? It has in here examples change the order from this to that. Of that order and it knows what to do, that's the agent card. We have down here, inside of our agent handler, our system prompt. We also talked about building that system prompt with a risen framework. What's your role? What do I want you to do? What are the steps to do what I want you to do? What do I expect? And each one of these things in here is used specifically to build up that determinism layer. What do you do in this situation or that situation? And it will go through and uh use this to figure out what. What to do when the time comes. We also have tools Cool, that's what takes us from chatbot to agent. What's a tool? A tool has a schema, this specific one is changing the order status and it accepts order ID and the new status in any notes. And you know what it's doing? Exactly what an API controller would do if you were trying to change an order status. It's called Dynamo DB. That's it, there's no LLMs in here. This is just running deterministic code to set a status of an order to a specific value. And the agent knows based on the real, uh, the description up here. Exactly what to do and what to expect when it calls that tool. So we looked at a worker agent And we looked at a tool. Let's look at the supervisor. Surprise, surprise, it's the same. Because really we're doing the same thing, it's not different just because we have the supervisor versus the uh the worker we might give it a little bit more information in this case we do we tell it what to do in different scenarios so if there's just a basic non delivery. Just reschedule it. What do you do in a total loss? And that's where we were supposed to see the payment agent and the order agent and the warehouse agent, uh, but the data, uh, wasn't there in the database to do it. But this is where you again are defining deterministic behaviors. It's really important to build these out so you don't have to build them out in code. It's telling the agent what to do in given scenarios, and we've tailored these, uh, these steps based on the types of events that it's gonna be responding to in our system, not chat. And that's about it. It's, it's the same here. This is probably the last thing that we'll, that we'll look at in the code here is registering agents. So we just have the URL of the agent card. Of all of these agents and what this is gonna do is in the code that I wrote specifically this is a layer on top of A2A it's gonna go and it's gonna fetch the agent card from that URL and inject that into the system prompt so the triage agent knows what the capabilities are of these three agents. All it is is a URL. The last thing, actual last thing. For real, uh, is the, uh, how does a supervisor agent actually talk to a worker? This is a big aha moment for me, uh, when I was building this several months ago. It's just a tool call Supervisors have a tool that says invoke agent. Give me the agent URL and give me the command, give me the task ID so I can track it. And all it's doing is it's using the A2A client published by Google and sending a message to it and then we kick off our agent loops on the sub-agents and it returns that data back in our execute action step. And that's really all there is to it. That's multi-agent collaboration and code, easy, right? All right, so let's bring this all together. Few key takeaways that I think you should try to remember that agents are what drives this system. It's not work flows. You don't design these flows, you design behaviors, so each agent is responsible for a specific job and the system emerges from how they collaborate together. And A2A is really the key to what enables this real collaboration. Instead of having some sort of central orchestrator, the agents discover each other at run time and they delegate work and they launch new autonomous loops and MCP is what makes context reliable, being able to use tools and schemas that returns this deterministic typed, uh, data. An agent core together with service infrastructure, it allows us for massive scale when we build these systems and with very, very little. differentiating heavy lifting. I love that phrase. So, deterministic and determinism is what then difference from chaos into a system that is reliable. And make sure that just like with any distributed system, you design this system for item potency and with compensating actions. I'm sure you all wanna try out and start competing with Swift Ship, and from the looks of it we can easily compete with Swift Ship, uh. Since packages aren't delivered, but scan the QR code takes you to a GitHub repo, uh, where all the code is available, and feel free to submit pull requests, I guess, to create a new agent that can order from the supplier. So then the, the demo would work, yeah, so you can deploy this yourself and experiment with it, and you can use it with or without memento as well, yep. So with that we wanna thank you all for joining us in this session about building autonomous multi-agent systems. I hope you found it enjoyable. Remember to fill out this session survey as always and we'll stick around on the outside if anyone has any questions. Thank you all very much. Thank you.