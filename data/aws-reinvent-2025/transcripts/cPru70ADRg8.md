---
video_id: cPru70ADRg8
video_url: https://www.youtube.com/watch?v=cPru70ADRg8
is_generated: False
is_translatable: True
---

Hello, everyone, and uh good afternoon and welcome to 8 of us reinvent day 2 and welcome to our session by the end of the day. So before we officially kick it off, and I want to start with a kind of quick, easy warm-up question. So in the year of 2025, any of you have not heard of or used the web coding, coding agent or coding assistant, please raise your hand. OK, great. The same thing in the year of 2025. Any of you have not heard of or used any of the agent, either for your personal life or at your work. Maybe just booking your entire trip or generate your PowerPoint slide for your presentation tomorrow morning. Please just quickly show your hand. OK, great. Not that many. So everyone is in the right session. Building autonomous AI at scale with Amazon Bedrock. So my name is Stephanie. I work at AOS, uh, worldwide Bedrock go to market team, and joining me today are our Bedrock service PM Shang and also our customer Kay, the co-founder and the CTO from Johannesburg. In the rest, roughly about 60 minutes, uh, we're going to cover seven major topics. First, understanding the autonomous AI landscape from 2024 to 2025, and why it matters. And then understanding and identifying the 4 key reasons why prototype to the production fail, and then diving into performance, cost, optimization, reliability and scalability, security and guard rails, and by the end, we'll hear from the real world example from JSpark. So first, let's look at where we are today in this GI journey, as 2025 is fundamentally different from 2024. So the year of 2024 is basically what I call the foundation year where AI as a tool or as a system. So Bedrock has served over 100,000 customers since the launch in 2023. And here are some of the top use cases that define the trend of 2024. The first one is the content generation from a piece of the internal, uh, the document to the external facing the email or the blog post. But please notice that it's always AI suggested, brainstormed and drafted, but the human edited, decided, and approved. The second one is the customer support. We have seen a huge, huge adoption of the customer support from the first line query handling the FEQ to the ticket routing. But it always requires a clear escalation path and handoffs to the uh human agent. The third one is the data analysis, where basically we leverage the GN AI for the automated insights, an anomaly detection and pattern recognition. But again, AI identified the patterns, but human decides what they mean and what actions to take. Number 4, the search enhancement. The search enhancement improved the user experience for the uh enterprise applications from the semantic search and the smarter ranking algorithm. But fundamentally, it's the improvement of the existing system. So let's look at what are some of the common patterns of the top use cases. Number 1, it's human in the loop. For every AI decision, it's always human verifies it and approve it. Second, request and response mode. Each interaction is basically discrete and stateless. And your system does not have the background or the knowledge across the prior sessions. The third, the single task focus, where your AI uh where basically your AI tools hyper specialized. You have this tool for your customer sentiment analysis. Another tool for the image generation. And they operated in the silos. Last but not least, the supervised execution. That's where the fundamental limitation is. They limited autonomous initiative. So overall, it's reactive, not proactive. Now let's look at what's happening in 2025. The 2025 basically AI stopped being a tool and become a coworker. And here are three distinct trends that define this transformation in 2025. The trend number one, we evolved from a single format content gen to a unified multimodal content creation workflow. So in 2024, you probably still need like three different models, the tools and the platforms uh for your text, image and video generation. And there's still no guarantee of the consistent branding across all the formats. Well, in 2025, a single agent can execut the entire creation workflow with the consistent branding across all the formats. So let's take it as uh the global marketing campaign as an example, where you have your agent, not the, not the human creative director, but the AI agent, creates the video spots and it generates the social media imagery which matches your video perfectly and writes the marketing copy with the consistent brand voice across all the content formats. And by the end, it adapted everything into 12 additional regional markets with the different languages and a different context, um, cultural context. So what is different and critical here? Your AI agents understand the context across all the formats. And trend number two, we evolved from the reactive rack, the retrieval augmented generation to the uh to the agentic knowledge system in 2025. So basically, the rack in 2024 uh is fundamentally passive where you ask a question your system, uh retrieve the relevant documents from the knowledge base and the synthesize the insights and they get back to you. It's useful, yes, but it's reactive. Well, in 2025, the Agentic rack is able to monitor the information sources proactively and synthesize the insights across the domains and predicts the patterns and proactively send the relevant information to the key stakeholders. So let's take this legal research assistant example. Your legal research agent basically proactively monitors all the law cases changes, and synthesize all the insights across the different law domains, and also flag any of the potential precedented conflicts even before you're aware, and also generates the legal arguments with all the citations. So what is different and critical here is it continuously building your knowledge graph and predict what's the additional information needed and the prep for the research proactively. Let's look at the trend number 3, where we evolved from this, we call conversational interface to the fully autonomous workflow execution. So basically, the chatbot in 2024 uh manages the scripted conversation uh within the certain boundaries. Uh, it's great for the customer support and it's great for the FAQ, but it's inadequate for the uh complex and multi-step problems. In 2025, the autonomous agent is able to execute the entire workflow, coordinates across multiple backend systems and make decisions based on your organizational policy and on the result end to end. So let's take the most popular, the customer service agent as an example. Now, when your customer calls in for a complex problem, your agent is able to coordinate with your inventory system to locate the right product and interfaces with your shipping, um, shipping system to basically manage the, uh, and order the delivery and communicates with your billing system for any of the charge and refund as needed. And by the end, proactively communicates to your customer for each step. So everything is proactive, not reactive. And here comes to the next question. So what is driving this transformation? We see the evolution uh from the AIS tool to AIS co-worker. This is not a philosophical, it's definitely technical. And the foundation model's capability improved across the multiple key aspects enable everything we just discussed. And here are some of the examples. The number one are the context we know exploded from the early days, uh from only 32K and then To 1000 and now it's up to 2001 million and and even more. So what is the difference here is basically uh from analyzing one chapter of your legal document to process the entire 500 pages of your contract without losing any of the details. The second one is the planning capability. Now the models gain the autonomous planning capability, where they can automatically decompose any of the complex goals into the actionable subtask independently. The 3rd 1 is the tool integration. Where the tool integration evolved from the prior, the fixed and hardcoded API to more dynamic context where tool search and selection. So in the past, we pretty much have to kind of configure, OK, use this API for your wider data and use a different API for the stock price. But now the models can dynamically discover which tools are available. And the reason, what are the best combinations to solve these problems most effectively. And, and chestrate this tool chain without preprogramming of any possible combinations. The last but not least is the reasoning depth, where the model evolved from the prior more black box statistical like pattern matching, until now, it turned to be more transparent and explainable with the chain of thoughts. As you can literally tell each logical step from the model. And we see all the model improvements serve as the technical foundation for uh for uh the technical advancement. But the organizations don't just deploy the foundation models. They deploy the specialized agents for the different businesses. Let's look at what are some top adopted specialized agents in 2025. The first one is the coding agent and uh which offers key capabilities as the uh the pool request review and white coding. Basically we are you can just describe your desired functionality in a natural language and then your agent generates the uh the working code with the error handling and the testing. And the document agents for the multi-format extraction, where it automatically extracts the structured data regardless of the input format. And also for the automated data entry, which we know is a table stake for a lot of the industries such as healthcare, financial services, and insurance as you name it. The customer service agent where it offers the the real-time customer sentiment analysis and a 24/7 multilingual support, um, and the automatic uh the issues resolution. The last but not least, the sales and marketing agent where it conducts the AB testing at a scale and also offers hyper personalization and the predictive predictive analytics. So all the specialized agents are great. As they're optimized for the specific domain, but they cannot work across the different domains. That's where the general purpose agent become essential. Or if you haven't heard about different name as super agent. So what is super agent? So, fundamentally super agent is the orchestration layer sitting on top of the specialized agents. And here are a couple of the core capabilities where the super agent brought in. So the first one is the multi-agent orchestration. Where the super agents coordinates across multiple specialized agents to solve the complex and cross domain problems. And each specialized agent does their own part of the work, while the super agents ensures they all work in harmony with the proper sequencing and not in a silos. The second one is the unified context management. Where the super agents share your knowledge estates across all the coordinated specialized agents to make sure there's no information loss during the handoffs between the specialized agents. And the third, task planning and dedication, where the super agents that the intelligental routing of the sub tasks across the appropriate specialized agents. Last but not least, the conflict resolution and decision arbitrary. Well, you know, sometimes your specialized agents might disagree with each other and generate the conflicting recommendations. That's where the super agents arbitrates based on your organizational policy, the priority, hierarchy and risk parameters. So if we want to kind of make this uh architecture more concrete, you can think of the super agent as your enterprise AI chief of staff, where, um, who manages like a multiple different specialists and one does the meeting scheduling, and there's another one works on the document prep, another one for the follow up and task monitoring, and then another one for the executive briefing. And they all work together. So, and we have pretty comprehensive review now for the autonomous AI landscape. We know autonomous AI is great and it's here, but the enterprises are still struggling. Let's hear from Sean. Why this happened and how to avoid it. Thanks, Stephanie. Yeah, so, Yeah, Enterprises continue to, to struggle to bring, uh, agents to the market. Uh, there's a few kind of core struggles that, uh, we see emerge. One is, uh, performance failures. Uh, which often stem from, uh, either wrong model choice, uh, or, uh, developing for a really narrow set of, um, inputs that just don't capture the breadth of what you'd see in a production environment. We also see lots of cost overruns again, uh, model selection plays a role here, uh, as well as, um, some of the prompting, uh, optimizations that you can, um, put in place whether they're there or not, um, can have a big impact. Uh, reliability and scalability issues we often see when there's not enough, uh, done to optimize for, uh, failure modes and, um, you know, there's no robust and error handling and, and no robust uh mechanisms for, uh, failover to, uh, other models should there be availability concerns, um. And, and lastly we see privacy challenges, you know, enterprises need to be able to control the inputs and outputs. Um, they need to be able to ensure that, uh, PII isn't leaking. They need to ensure that, uh, the model is behaving, uh, in a way that's compliant with the regulations, um, that they face. So we start by tackling um a few of the the ways that you can address some of these issues. So first is, um, we're gonna talk a little bit about performance optimization um and when we're talking about performance in this specific context I'm really referring to speed, uh, which other name is latency. Uh, specifically time to first token, time to last token, and our end to end latency and which latency, uh, is important to you just depends on your use case. Uh, other aspect of this is intelligence. So, um, is is, is the model capable enough of handling, uh, the nuance, uh, that your use case has? Is it, um. I is it calling the right functions, um, and. Oftentimes what we see is that customers start their journey with uh. Sort of choosing a model before even assessing the use case um and the match for the model and so one of the big uh important uh things you can do to uh optimize your performance is select a model based on the use case, not based on, uh, the model itself and and any sort of acclaim or brand recognition that's around it. So The uh The the guidance is you want to start small and then sort of get uh work your way up as needed. Most customers, uh, will actually find that a smaller model than what they're using is, um, is perfectly sufficient for their use case. So if you think of something like a, like a summarization use case which might be used within, um, an agentic workflow, you can go with a very, very small model, uh, which is going to save you cost, it's gonna save you, um. Uh, latency. So, uh, that's, that's certainly something to consider. The other, you know, as, as reasoning capabilities become standard, um, many customers tend to, um, to flock to it and, and utilize it even when it isn't necessarily appropriate. And so you're gonna end up paying, uh, a latency tax and also an increased number of tokens, uh, just to, uh, to use reasoning even if it doesn't necessarily enhance your accuracy so. Uh, with all that said, you want to develop a set of evaluations that really measure how impactful, um, reasoning is, how impactful the size of the, the model is, um, and, and test it against a wide variety of inputs. You wanna make sure that the inputs are reflective of what you're going to see in production and not just a narrow subset of what you think, you know, might be best case scenario. Uh, and you know this will pay dividends down the line as long as you can establish up front, uh, a framework for evaluating models that are speci that's specific to your organization, uh, you can repeat, um, you know, rinse and repeat over and over again, uh, as you add new use cases and new agents. Um, if you're not able to find something off the shelf, um, you know, you obviously have the option to, um, fine tune a model on your own data, and, and bring that, uh, in. So Bedrock offers a custom model import, uh, as one way in which to, to bring that to bear, uh, you've got, um. To think about the, the trade-offs here though, right, there's upfront, um, through time and uh cost that goes into, uh, fine tuning, um, but on the other hand you are gonna get an efficient, um, uh, well aligned model for, uh, for your use case. So let's say that you've now got your uh model and you're now deep into the weeds of creating the uh the agents that you're um that you're going to be shipping. Um, it's really important, uh, to optimize your function calling. Your function calling is, is really the core of, uh, of all of your, your agentic, uh, workflows, and there are really five kind of categories, um, that I sort of recommend, um, using as, uh, to, to sort of orient yourself around how to optimize. So one is you want to define clear contracts. So specifically, uh, you want to be explicit about the, the schema, the intent you want to, uh, be very descriptive with the, uh, parameters that you're, uh, giving your functions. So, as an example, um, if I have, uh, just a, a get weather function, you know, the, the common example, uh, that takes two parameters. One is a. Um, a, a temperature unit which could be Celsius or Fahrenheit, um, and the other is a city, um, a customer, uh, will define. Well, I don't want the parameters to just be named something like parameter 1, parameter 2. I don't want them to be some abbreviation. The model needs to, uh, have completely unambiguous, uh, meaning, so they want the, uh, if you're better off having the wording. Be very explicit like uh user city and uh you know, a temperature unit to make sure that the model is going to properly call uh the function. Uh, so, uh, in addition, where possible, you're going to want to constrain your arguments. So if in the example that we just talked about, you've only got two options for temperature unit, you've got, uh, Fahrenheit and Celsius, make it into an enum, make it more explicit for the model so that it has a better chance of calling, um, with the correct, uh, arguments. Uh, you also want to, to design for, for natural language. Um, you want to align the names of the functions with the customer intent, so, um. Let's say that uh I have a customer that wants to, you know, summarize uh emails that's uh an agent uh use case that I'm I'm working on. Well, I want to have the function name be something like summarize emails. I don't want it to be reflective of maybe an existing, um, you know, back end system that orchestrates across multiple functions like, uh, something like stream messages, um. And uh you know sort of uh summarize uh summarize content. I don't want the to have functions that are not semantically similar to what the query is that the customer is, uh, is sending in. So, um, The other, uh, the other factor here is, you know, you, you wanna have, um. A single wherever possible, a single uh. Function orchestrating on behalf of the model rather than have the model do all the orchestration of many sort of individual micro functions, um, again reducing the, the ambiguity reducing um the chance for error. Uh, and, you know, to that, uh. You wanna sort of guide the the invocation, um, logic and you you want to sort of um give a very high signal prompt uh, the high signal prompt should be guiding when exactly what are the very narrow parameters under which you want to call, uh, a function that's gonna improve your odds of calling the function, um, and, uh, you know, making sure that it it's called with the right parameters as well. Um, additionally, you wanna optimize, uh, context, and what we mean by that is you want to keep just the bare minimum context that you need in order to, um, to actually execute on the task at hand. Lastly, um, you, you wanna instrument everything. You wanna make sure that you are, uh. Logging your uh your function calls that you're logging the parameters, the uh the failures, um, the corrective actions that the model might have taken, um. You want to use all of that to inform how you might tweak uh your prompts in the future to deliver a better experience uh in the long run. So, uh Beyond sort of just the the function calling, there's more generally um some optimizations that you can make uh around context management that are going to uh help you with latency they're gonna help you with costs um. So One of the, one of the key strategies that you can use is compression. So oftentimes you don't need all of um or at least you don't need the entirety um of the prompts that came before in previous terms. You can compress them, you can summarize them, um, you can um. Strip out maybe just unnecessary portions of uh of the context like you know verbose uh language that's already that's in the prompt. Uh, these kinds of optimizations are gonna save you, uh, on tokens, which means that you're gonna have fewer tokens that need to be processed before your output is gonna start to generate. You're gonna, uh, which is faster time to first token, you're gonna have, uh, lower costs because fewer tokens, lower costs, um, you're also going to reduce, uh, the chances that. Uh, stale context is going to sort of misdirect the model, uh, to call the wrong function. Uh, sort of another, uh. Another strategy you can use is sort of prioritizing context so you know, think of it as, uh, you can look at it uh over time some context is gonna get stale you can just remove that context, um, and, uh, and call with without the, the context from, you know, and many turns ago, uh. You can, uh, there are, you know, multiple libraries that, uh, help with, um, with the sort of prioritization and summarization, um, you know, an open source, you know, lang chain and, and Lam index, um. The, um, you can also focus a little bit on, uh, dynamically, uh, uh, budgeting your prompts and so, uh, using the token counting APIs you can call in, um, you know, as you're structuring your prompts to check sort of where you are, um, in the, the overall context window, um, and, and start adjusting based on that. And lastly, I'll just touch on this for a second. You, you, you can, um, for any repeated context that you're gonna use, um, across model calls, uh, let's say a system prompt, uh, that you have, um, you know, defining all of the, uh, all the behaviors that you want your, uh, agent to exhibit, um, if that's gonna be a fixed portion of your prompt, every single turn you can benefit from caching. Uh, which will save you on latency and it will save you on, on cost. Uh, and then, you know, as an aside, if you have a, um, a use case that's more customer facing, um, there's a perceived latency benefit that you can get with, uh, with streaming, so, you know, um, you can also leverage some of that. Uh, so I'm gonna talk a little bit about cost optimization and that brings us right back to, um, model selection, right? As I said earlier, a lot of customers will select a model that's either too large or just sort of, um, you know, unnecessary, unnecessarily costly for their use case. Uh, so start with the use case and not the model. Um, it'll pay dividends over time. Uh, scale the model only as as needed and, and again like make sure that you're evaluating at every stage, even in production. Uh You know, the, uh, the other sort of side of this is you can. Trade off Your latency, uh, with, uh, to get some cost savings. So if, if you have a, a less latency sensitive use case, um, there are different tiers, uh, and I'll let Stephanie talk a little bit more about this, uh, uh, in a few minutes, but, uh, there are, there are different tiers like flex tier that would give you some, some savings as long as you're willing to trade off on, um, on the cost, uh, or on the latency, excuse me. Uh, and as I had, uh, mentioned, uh, prompt caching, so, uh, if you have an exact match to an existing, um, you know, prefix in your prompt, something like a system prompt, um, as long as it matches it, um, you know, syntactically, uh, and, uh, semantically, then you will, um, you will get a cache hit and. You can save, um, save on cost, and the reason you can save on cost is because there's less compute associated, uh, um, with actual processing of those tokens because you're pulling, uh, the sort of intermediate attention states in from, um. From memory rather than having to recomput them every time, uh, you're also getting a latency benefit because you're, um, you don't have to spend as much time on prefill, uh, aspect of, um. Of, uh, LOM calls you can sort of skip forward, uh, straight to, uh, the generation port. Um, I'll hand it over for. So now you have optimized your performance and your cost. And let's make sure your AI system stay up and scale peacefully. Good job. So we always consider the multi-region deployment is a fundamental and critical for your enterprise scale AI systems. There are very important. There are 3 key reasons here. Number one is the high availability. So, today, uh, Bedrock service is available within 36 A global regions and including the gov cloud. And with the implementation of the cross-region inferencing, it largely improved the availability of the models across the world. The second one is the latency optimization. The intelligent routing behind the scenes basically prioritize not only prioritize the availability of the of each model, but also try to uh fulfill your inference request from source regions, and then seamlessly reroute to a closer region to minimize the latency impact. The third one is the regulatory requirement. So, beyond this kind of uh the global cross inferencing endpoint, we also introduced the US, EU, Japan, Australia, and the gov cloud in the US in order to meet your data residency requirement. And now let's talk about our, uh, the inferencing options on Bedrock today. So we recently introduced the infferencing service tiers in order to meet your different requirements uh for your different workloads. Now, there are 4 different service tiers available. The first one is what we call the reserve tier. Basically, the customer is able to reserve they prioritize the compute for any. Your mission critical and production workloads as all your reserve tier requests will be prioritized among the others. It offers the target 99.5% uptime, uh, gives you the flexibility to provision the input and output tokens uh independently to meet your request to ship and also automatically spilled over to the on-demand during the peak traffic. The second one is the priority tier, where if you don't want to reserve the capacity ahead of the time, but you still want to make sure your critical request is served as soon as possible at a request level. The priority tier gives you the prioritized compute compared with the um standard on-demand. The third one is we call the standard tier, which is basically the default on-demand inferencing for the real-time gene applications. The last one is what we call the flux tier, uh, which is the cost efficient way for your less urgent workload, where you're able to, uh, take a few minutes, a few seconds and up to a few minutes latency, but at much lower cost. So, think of the use cases as the uh the model eval, content generation summarization, and the non real time, uh, multi-step gentic workflow. And here, the last topic, the security and the guardrails. This is what we always can emphasize for any of the in-production workload. And that's the reason the bedrock is built in day one with the data privacy and the security. So, uh, as the, as the other core uh Abu Foundation services, we have been emphasizing that there's no customer data is used to train the foundation models and shared with model providers. And your fine-tuned models are encrypted and saved in the security container which is only accessible to you. In addition, Bedrock also achieved over 20 complex standards for you to meet your regulatory compliance requirement. And the guardrails. The Amazon Bedrock uh guardrails help the customer to apply the safeguards for your GEA applications. Now, there are 6 different policies or safeguards you can apply, including the content filter, uh, the word filter, deny topics, and others. And then we also expanded the capability where you're able to apply the safeguards, not only at your database account level, but at your database organization level across the account for all the model indications. In addition, now the guardrails also applies for any of your coding use cases. So you can apply the safeguards to detect any of the undesirable contact with the coding elements and we support 12 major programming languages. And now you have heard a lot about autonomous AI and about bedrock. And that's now hear from Jan Spark. OK, thank you. Thank you, Stephanie. Hi, everyone. I'm Kay, uh, co-founder and CTO from Genspark. Let me do a very quick quiz here. So anyone here have heard of or have used Gpark before, please raise your hand. Oh, not a lot. So, I will do a very quick introduction of ourselves. James Market was founded in 2023 November, and we launched our super agent suite this uh, this April. And after launching our super agent, we actually received a tremendous tremendous user feedback. We reached the $150 million AR and in just in the end of September, and, you know, we are actually recognized by the Forbes uh Forbes BDP and also entered the OpenAI trillion trillion token club. Last month, we we announced our second round round of funding, which is $275 million led by emergency emergency capital. The key insight here of the of this journey is that the world is actually ready for a truly autonomous AI agent, and Gens Spark is a real-life proof. And we are very proud of reaching this milestone for us, the 50 million ARI in 5 months, and also 1.2 billion evaluation in 20 months. And we thanks a lot for all the users love and use GSpark. And for James Spark, our mission is very simple. We want to enable the 3-day work, work, uh, 33 day work week for more than 1 billion knowledge workers, and we are working very hard on that. This is our launch, uh, this is our launch calendar. During the past 6 months, we actually. Launched more than 19 products, almost 1, almost 1 new product every week. We work very hard and everything is done, almost done by AI, AI agents, and also the AI coding tools. So more than 80% of our tool is written by AI tools. So as a software engineer, I'm super excited to bring the cursor of cloud code-like experience for software engineer to to regular white-collar workers. That's what James Spark is doing. Last month, Gen Spark is launching our AI workspace. For GenSpark AI workspace, it is composed of three components. One is the collecting information, the center is the processing information, the end is the generated outputs. Let's talk about it one by one. For collecting information, Genspark can collect, collect information from both digital world and the physical world. From physical world, Genspark have the feature like AI meeting notes and CareFM can gather information from the physical world. And also for the digital world, it will collect data from the public data source and also your private data through your AI inbox, AI inbox, or AI teams. For the output, GSpark implement a set of, uh, a set of subagents, which is specified in different kind of output. For example, AI slides, AI sheets, and AI documents, the standard office suite, and also the AI developer, AI designer, the standard AI employee set that you can use in Gpark. In the center of it is the GSpark super agent. It will coordinate and coordinate and uh you know, organize all the super uh all the sub-agents and all the, all the GSpark features together to help user finish their work end to end. Let's see it in action. Imagine I'm in the apparel business. So this is our, my order database in Superbase. It, it has more than 50,000, 50,000 orders in this database. For every, for every order, it has the time, all the detail information, uh, category, and so on. Now, we can use Jan Spark to actually analyze it. Let's first start with a very simple analysis. This is James Spark super agent. I will ask, what was the sales share by the subcategory in October 2025? Yeah, when you input, when you input that, let's see it's in action. G Spark will use MCP core to connect to the superbase and issuing a SQL query to it. And this is the AI sheets output. You can see it's it's totally compatible to Microsoft Excel, and it will generate the it will generate the table and the chat compatible with Microsoft, uh, with, with all the right Microsoft format and also right right formula. And let's try a more complicated example. This is another one. I will ask a more complicated one. For each sales channel, identify the categories and the SKUs, which are the top performers, uh definite defined by the growth rate comparing 2024 to 2025. For this more complicated one, G Spark will issue more MCP calls, doing all, all kinds of computation, writing all the formulas, writing the Excel format uh writing in the Excel convex, and this is the result. For every, for every subchannel or categories, we have the top performance, but data itself is not enough for action. Right? So we can ask Jens Spark to give us the action plan, action plan of the analysis. We can ask Jens Spark to analyze the subcategory and product selection strategy by channel. After another round of analysis, you can see Gpark will create the subchannel analysis, uh, create a new tab of the spreadsheet, and uh we can have everything together. You can see for the mobile app, for the eBay channels, all the action, action recommendations, and also even the cross-channel strategic insight. The Excel is very good to, you know, communicate inside your team. You can directly send this uh this Excel to your operating teams for the further action uh discussion, but you may not want to show it to your boss or to your clients directly. Now, GNSpark all in one workspace actually come to rescue. So in for for Jaspark, you can just ask it to create a 5-page channel strategic deck answering what, what to what to play and how to win. And Japark will collect all the information together and using the AI slide subagent to generate a boardroom ready deck for you. This is the DEC generated. All the chat and all the graph is computed in Excel, and the character and the number will be, will be absolutely right. And then you can even have the implementation roadmap with it. Let's push it a little bit further. What if you don't, you don't want to present it by yourself? What if you cannot be here? Let's ask Jens Spark to generate the audio with the slide. Using the voice your slides feature, Gpark we are actually using the AI podcast capability to generate the audio for each slide. Now let's hear it, hear the result. Welcome everyone. So this is our executive briefing on channel strategy. We've focused on two fundamental questions, right? Where to play and how to win. So, let's start with where to play. Our performance analysis, it reveals a really compelling story. We have two channels, eBay and our mobile app, that are just significantly outperforming all the other channels. This is where our opportunity is. So we're gonna double down on what works. And this naturally, it leads to how to win. We're going to implement a proven winners. Yeah, obviously Jan Spark can generate a presentation better than I do. So maybe in the future I will just ask Jan Spark to do the presentation for me. Let's come to the techno technology part and the design principle behind the G Spark AI agent archi architecture. We have the principle to to be less control and more tools. Let's talk about this one by one. For less control, we are, we are actually fully embrace the agentic engine. We we actually convert from the traditional rigid workflow to the adaptive agents. That's very important for us because um the traditional rag-based search actually use a kind of fixed, fixed workflow and then it's often break on edge cases. And by shifting to the agentic engine that plans execute and observe and backtrack in real time, we observe much better results. And this by using the agentic way, it is more robust to error, because in the workflow, the all the errors actually accumulate. And in the agents, um, all the errors are actually re recoverable. And for the more, for another philosophy is more tools. We believe that we need to give the agent more tools. All the tools have natural effect. And uh we have built more than 150 tools that can be composed like Linux pipe. One tool's output can be another tool's input. That's what we show, that's what what we have just seen in the in the demo. And all these tools have network effect because each new tool will create exponential, you know, uh, combinations with the existing ones. Some of my friends are kind of fundament fundamentalism. They believe that you only need to give the agent the large language model one computer, one mouse, and one keyboard, and that's it, nothing more. But in Gpark, we don't really we don't really believe in this way. Um, because imagine there are two types of software engineer. One software engineer with only one brand new Mac, nothing else. Although they have, uh, he has the, he or she has the, uh, internet connection, can people install everything. However, comparing to another software engineer who has the laptop pre-installed with all the work critical apps, and also with all the menus and all the documents, documents with it, who will perform better? Obviously, the later one will have better efficiency. So that's what we believe in more tools. Let's explain very quickly about G Spark Agentic engine. The Agentic engine has two components. One is the orchestration layer. You need to put everything together and make it work. Another one is the self-improvement layer. You need to make sure everything works better and better with the user using it. So, for GSpark, the top layer is the orchestration layer. We combined more than 30 plus AI AI models in a mixture of agent way, and uh more than 150, 152 sets and and more than 20 data sets uh curated in-house. Also, the the bottom layer is the self-improving layer. The evaluation is the key. We're actually using the large language model as judge to evaluate all the agenttic execution and uh attribute all the session reward to the single step. And this this way which we can accumulate a lot of uh a lot of data feedback and a lot of learning experience. All these learning experience is internalized through large uh through reinforcement learning and prompt playbook. In that way, when the users are using GenSpark more and more, the Genspark's performance is getting better and better. Let's, uh, let's quickly discuss about a little bit about the mixture of agent architecture. So in nowadays in the agent working, some of the stuff is very demanding, is very hard. None of the frontier model can answer it very perfectly. However, you can actually ask this kind of demanding prompt or demanding questions in parallel to a bunch of frontier models. And you can record all the model's output and also the syncing traits of them. Then in the aggregation layer, you can have a aggregate aggregator model to combine everything together, read through all the output and the thinking process to synthetic a better result. By this way, we discovered that we can reduce the hallucination by a lot and achieving much better final result. And also talk about a little bit about AWS infrastructure. In GSpark, our image studio use a lot of GPUs and by using AWS elastic uh infrastructure, we can save our cost by 70% and also uh can scale scale very easily, you know, when, when we are when we are vibe growth in the Japanese market. Also, Amazon Bedrock is also very key to our uh agent infrastructure. Just as Tiffany, Tiffany and uh Sean mentioned uh before just before, uh, Bedrock actually have a very good performance optimized infrastructure, multi-region deployment, and also enterprise grade gua guarantees. Also, Bedrock have very good uh prompt caching optimization for GSpark. I will talk a little bit about promption uh for James Spark. It's, it's the prompt caching is actually a little bit, you know, understated problem and that mentioned but not really mentioned by a lot of people. GSpark actually achieved more than 80% of the prime caching hit rate on uh on Amazon Bedrock. And it actually helped us reduce cost dramatically, you know, 10 times uh reduction in the cost and also maintain very low latency. The way to achieve that is that we are applying the apparent only context design. So just as showed in the right graph. Each time, um, no matter it is a two calling or a user, uh, a user input, we we always append the context to the bottom of the, to the context window. So this will guarantee our we have a kind of stable context prefix. But what if the task is too complicated and it actually use up all the context window? You can use 22 ways to fix that. One is the contact compacting and another one is the context isolation. So for context compacting, it's actually uh it actually triggered in a more like uh garbage collection type of way. When you are used up, for example, 70% or 80% of your context window, you can trigger one round of context compacting. It will, you know, summarize all the, all the important information, compress a large, large comp uh large output and offload them to the file system and also maintain the critical decision and the session goals. By this way, you can free up a lot of your context window, you know, by using the context compacting technique. Another way is using the context isolation, just as you see in the previous demo, Gpark super agent actually using the AI spreadsheet and AI slides as sub-agent. So all these subagents is taking care of all the You know, context detail about how to operate on a slide, how to write a formula in the spreadsheet. All these context contexts will not pollute the lead agent, and the lead agent will just, you know, orchestrate what to do, you know, of the sub agent. Finally, uh, we also thank uh AWS a lot for the all the startup program and it helped a lot for James Spark. So this is what um the short summary of my part. We talked about the less control more tool. It's the design principle of Genpark super agent, the GSpark agentic agent, the orchestration layer and the self-improving layer, and also the mixture of agent architecture and the the prom caching and the context engineering techniques. Yeah, that's, that's all for my part and uh please uh feel free to try Japark in Jaspark. AI. Thank you. Thanks. Thank you. So yep, I just a, a couple, um, couple points to wrap up here and we've talked about the evolution of, uh, of agents. We talked about, uh, the common struggles that customers have, um, with performance, cost, reliability and scalability and, and safeguards. We've also talked a little bit about how you might uh optimize for each of these, um, and then you saw, uh, the, the proof uh in GSpark and and how um how these optimizations can um lead to, uh, impressive results, uh, so I just wanna say thank you, uh, very much for taking the time to, to chat with us, uh, this afternoon. OK, thank you. OK.