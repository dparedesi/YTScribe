---
video_id: 4LR7wkddw1o
video_url: https://www.youtube.com/watch?v=4LR7wkddw1o
is_generated: False
is_translatable: True
summary: "Reka Sheadrinathan and Paulo from AWS introduce SageMaker HyperPod and EC2 UltraServers, framing them as the \"steel wire\" innovation needed to bridge the gap between current infrastructure constraints and the massive computational demands of modern Generative AI. Reka draws a compelling parallel to the Brooklyn Bridge, where John Roebling's invention of the steel wire was the necessary architectural breakthrough to span the East River, arguing that just as 1800s transportation needed new materials, the AI revolution—driven by transformers and foundation models—demands a fundamentally new compute architecture. She outlines four critical challenges facing AI adoption today: limited access to high-performance compute due to scarcity and rigid reservation models; inefficient resource allocation utilizing static quotas that leave high-priority jobs queuing behind lower-priority ones; the immense technical complexity of distributed training where models like GPT-3 exceed single GPU memory, requiring intricate model parallelism; and the high cost of hardware failures where a single GPU fault can halt an entire cluster, drastically reducing \"goodput\" (the time spent making forward progress). SageMaker HyperPod is presented as the comprehensive solution to these hurdles, purpose-built for Generative AI. It introduces Flexible Training Plans, allowing organizations to reserve capacity for as little as one day up to six months, offering up to 68% cost savings and flexibility compared to multi-year commitments. To solve allocation inefficiencies, the Task Governance feature enables administrators to define priority policies and preemption rules, ensuring that high-priority inference or training jobs instantly reclaim resources from lower-priority tasks, maximizing cluster utilization through a fair-share model. For the technical challenges, HyperPod offers pre-benchmarked Training Recipes for popular models, and critically, a suite of Resiliency Features. These include health monitoring agents that proactively detect hardware or network issues, automatically pausing training, replacing faulty nodes, and resuming from the last checkpoint without manual intervention, thereby saving hours of data scientist time. Paulo then dives into the hardware ensuring this performance: the EC2 UltraServers. He details two variants: the Trainium-based UltraServers, which interconnect 64 AWS-designed Trainium chips for cost-effective training, and the NVIDIA-based UltraServers, specifically highlighting the Grace Blackwell superchip architecture that unifies CPU and GPU memory to allow efficient activation offloading. A key innovation discussed is the networking layer powered by the Elastic Fabric Adapter (EFA) and the Scalable Reliable Datagram (SRD) protocol. Unlike standard TCP which halts on link failure, SRD sprays data packets across multiple network paths simultaneously. This \"multi-pathing\" resilience means that if a network device fails, throughput only dips slightly before recovering, rather than causing a catastrophic stop—a feature essential for the massive, chatty communication patterns of Mixture of Experts (MoE) models where \"expert\" GPUs must constantly exchange tokens. The session concludes by demonstrating how these hardware and software layers combine into UltraClusters, capable of scaling to over 25,000 GPUs working as a single system, enabling enterprises to train state-of-the-art models with the speed and reliability previously reserved for tech giants."
keywords: SageMaker HyperPod, EC2 UltraServers, Generative AI, Distributed Training, Resilient Computing
---

Today we're gonna dive into the world of Amazon Sage Maker Hyperpod and EC2 Ultra servers. I'm Reka Sheadrinathan. I'm a senior manager of product on Amazon Sage Maker, and I'm joined today on stage by my colleague Paulo. Hi, I'm Paulo. I'm a principal specialist solutions architect working on AWS helping customers build their foundation models. Thank you everyone for being here today. So before we get started. I wanted to start with some history. How many of you here. Recognize this bridge. OK. Uh, yes, it is the iconic Brooklyn Bridge that connects the boroughs of Manhattan and Brooklyn. Now back in the mid 1800s. The very idea of constructing a bridge across the East River. was considered absurd. The river was deep, the currents fierce. And the span Longer than one that had ever been attempted seemed impossible. Yet One man, John Roebling, had a vision. He had a vision for a suspension bridge unlike one the world had ever seen. One that could transport thousands of people and carry tons of cargo every day. While the newspapers called it a fantasy. The idea persisted. And it led to the invention. Of the steel wire. The steel wire that John Roebling designed was something called the 619 design. This is basically 6 strands, each with 19 wires surrounding a core. And this unique hexagonal design created a very strong but yet flexible wire. And he used this to construct the bridge. Now the construction of the bridge itself took over 14 years. And when it finally opened, it was the first steel wire suspension bridge and the longest suspension bridge at the time. Now we stand at a similar moment in history. For decades. We believed that many forms of intelligence, reasoning, creativity, language, were uniquely human domains. We believed that scaling models beyond billions of parameters was beyond our computational reach. Yet breakthroughs arrived. Transformers, foundation models, rag. And reinforcement learning. And now the impossible seems inevitable. In fact, over 70% of enterprises are already using AI. In at least one function. And it's not just chatbots anymore. Reasoning systems and agentic workflows are creating new efficiencies and growth. And organizations like you are investing millions in AI. But just like the Brooklyn Bridge needed a new architecture of steel. The challenges of AI and these AI breakthroughs demand a new compute architecture and new software systems that are designed to solve the unique challenges of generative AI. Now here at AWS it is our mission. To build the best place for generative AI training and inference. And that means building the best compute infrastructure and software systems to solve these unique challenges. And that's why we're excited to talk to you about Sagemaker Hyperpod and EC2 Ultra Servers today. So in today's session, we'll first talk about some of the challenges with generative AI development, training, and inference. We'll then introduce Sagemaker Hyperpod and tell you how it uniquely solves some of these challenges. After that, we'll talk about EC2 Ultra servers and Paulo will do a deep dive on the architecture. We'll also go through a comprehensive demo on how you can use all of these tools yourself. And then finally we'll go through some advanced use cases and give you some resources that you can use. So let's get started. So why don't we walk through what a typical AI journey looks like for customers who are training and hosting models. So the very first step that customers need to take is to get access to the compute that they need. And because these accelerated instances are in high demand. They're not always available on demand. Which means that when you're trying to get these instances for training, they may not be available at the time that you need. The alternative for you is to potentially make a long-term reservation, which means making an upfront commitment for 1 to 3 years, and that's not always feasible for every organization. It may also lead to underutilization because you may not be able to use these resources for the entire duration of your reservation because by nature these AI workflows tend to be spiky with lots of experimentation. So let's say you've procured this compute. The next step that you're gonna take as an organization is decide how to allocate this compute across multiple teams in your organization. There's gonna be different teams who are fine tuning models, training, hosting them, building applications, and you need these resources for all of these use cases. Now typically, an administrator will go around asking different teams what their compute needs are. And then statically allocate this compute across the organization. But that can lead to inefficiencies. What do I mean by that? So let's go through this example where you have 2 teams in an organization. So here you see the first team. They have 2 instances allocated to them. And they have multiple jobs that are queued up. The 1st 2 jobs are of lower priority, and they're occupying these 2 instances and then a 3rd higher priority job comes in. However, it's standing behind queued up waiting for these jobs to complete. Now ideally you want your higher priority job to get access to these compute resources and then pick up the lower priority jobs later. Now let's look at team 2. Here, they've got 3 instances. And they have two jobs of varying priority that are using those using two of those instances. But you've got a 3rd instance there that's sitting idle, and ideally that instance could be used for one of the backlogged jobs for Team 1. But because you're statically allocating this compute. It ends up underutilizing your resources and you're always not prioritizing the highest priority workloads in your organization. So you've procured the compute and you've allocated the resources across different teams. The next step, if you're training models or fine tuning them. Is to see how you can use this hardware effectively. With the latest models. Now the foundation models have been growing faster and faster every year and even if you look at GPT 3, which is now quite old, that was a 175 billion parameter model. Now if you're trying to load up the model, you need memory to store these parameters. If you're using FP 16 precision, that requires about 2 bytes per per parameter, and so you need 350 gigabytes just to store the model weights. And now add in activations gradients, the memory needed is far north of that. But a single H100 GPU only has 80 gigabytes of memory. So what do you do? Teams will typically use model parallelism or split that model across multiple GPUs. But that requires expertise in distributed training and requires weeks or months of iteration to get the right configuration on this particular type of hardware that you're using and the particular model family that you're trying to train. So if you've done all of this and gotten this far, and you've kicked off your model training. The next challenge you run into is hardware failures. So although these accelerated instances give you a lot of compute and memory power, they also tend to fail. And because distributed training is a highly synchronous process where there's a lot of internal communication. A single GPU failure can halt your entire fine tuning or training workload. And when that happens, a data scientist or I'm an engineer in your organization. Needs to go and isolate the issue, debug the failure, solve the problem, and restart the training from a last safe checkpoint. And all of this can result in hours of frustration and downtime. And these failures can happen at any time. So let's say they happen in the middle of the night, your entire cluster might be sitting idle until that problem is resolved. And that can lead to a lot of downtime. And as the size of your cluster or models get larger and larger, the probability of these hardware failures only goes up. In fact, the industry has coined a term called goodput for this. If you come across this, this basically means the percentage of time that you spend actually making forward progress, so the percentage of the time that you're actually productive instead of doing all of these activities, debugging, etc. And many of the customers that we spoke with as they started training these models were suffering from low utilization and poor goodput on their clusters. So as you can see, there are a host of challenges with Gen AI development, and they're not all technical. Of course you have memory bottlenecks and communication bottlenecks, but you also have operational challenges and business challenges as well. And all of these problems are interconnected. So solving them in isolation doesn't usually result in good results for your organization. You need a more comprehensive solution that attacks all of these issues. And that's why we built Sagemaker Hyperpod and launched it right here at Reinvent. 2 years ago So Sagemaker Hyperpod is purpose built for generative AI development. And it provides you the tools that you need to address the unique challenges that we discussed. It comes with unique resiliency features that help you debug and isolate failures quickly, helps you maximize cluster utilization, and gives you observability tools that you need to make sure you're using these resources effectively. So let's revisit that AI journey with Sagemaker Hyperpot. The first challenge we talked about was access to compute. So SageMaker Hyperpod supports flexible training plans. So instead of reserving capacity for 1 to 3 years, you can now request capacity for from 1 day up to 6 months. Up to 8 weeks in advance. You can go to the Sagemaker console, specify how many instances you need. And for for how long and you're able to then see a series of plans that meet your needs. Once you find a plan that you like, you can purchase that plan. And then reference that training plan when you're creating your hyperpod cluster. Later, Paulo's gonna show you a demo of how all of this works. But Hyperpod will simply scale up the cluster when that training plan becomes active and scale down the cluster when the plan expires. Once you've set up your cluster, you're then able to start submitting your fine tuning and training workloads and and begin using the cluster. The next challenge we had talked about was properly using these resources across multiple teams and how that can be challenging. StageMaker Hyperpod comes with a capability called task governance. Task governance helps you efficiently allocate these compute resources across teams. The way this works is an administrator can go in and specify compute allocations for different teams, but along with that compute allocation, they can also specify priorities for different tasks. And set up preemption rules. And once this is set up, Hyperpod will automatically prioritize the highest priority tasks that come in. And while every team gets their guaranteed compute allocation, if there are idle resources, then another team is able to opportunistically run tasks on that idle compute. And that helps you maximize utilization. But still make sure that every team is getting their fair share of resources. The next challenge we talked about was distributing these models across large clusters. Hyperpod also comes with training recipes. Think of these as a playbook for distributed training, where we pre-benchmark recipes for different model architectures and instance families. Ahead of time and you're able to submit your jobs using one of these recipes and get started on your fine tuning or training workload quickly. The last challenge we had discussed was hardware faults and how once you get started on your training. You might run into hardware faults or network issues. Hyperpod has a bunch of resiliency features that help you with this. First, it comes with a health monitoring agent that is proactively monitoring your cluster for hardware failures or network issues. When an issue is detected, Hyperpod will pause the training. Either reboot or replace the node, depending on the kind of error. And then restart your training from a previously saved checkpoint. All without any manual intervention, so this is happening behind the scenes without needing to use your precious data scientist or ML engineering resources. In addition, there are also deep health checks that you can optionally enable on the cluster. So when you're creating your cluster, you can set up these deep health checks. And they're comprehensive checks that will run and reduce the probability of failures once your workload actually starts running. We also have some exciting new features on Hyperpod that were released just today where we're able to resume your training by loading the model state from peer nodes without needing to go to durable storage or a previously saved checkpoint, which saves you even more time. We've also launched a new feature today called elastic training which allows you to reconfigure your training on the fly depending on the resources that are available as well so I would encourage you to also check those out. Now it is because of all of these features that top AI companies are using Hyperpod to both train and deploy their models, and if you happen to watch Matt's keynote earlier this week, the CEO of Ryder talked about how they've trained their latest models on Hyperpod and been able to speed up their training by over 3X. So now that I've given you an introduction to Hyperpod, I'm gonna hand it over to Paulo to talk more about EC2 Ultra servers and give you a demo of how all of this works. Thank you so much, Raka. So, we talked about how to create those clusters and how to locate that compute. Let's talk about that advanced Ultra server compute. First of all, Ultra servers, they are a definition of how you combine hardware together to make your model training, your workload, perform faster and works better. And initially we announced the last reinvent Ultra servers based on AWS training chips. Those chips, they were designed and developed by AWS by Annapurna Labs, which is one of the Amazon company's group. That company developed that ship focused initially on training, and that's why it was named Tranium. But nowadays, the way that the ship is designed, it does training and also in France. In order for you to develop your model and use that model on training devices, it's very simple. You have the AWS neuron SDK. That SDK integrates with most of the machine learning frameworks existing on the market, such as Spytorch, for example. And when you're doing your model training, all you have to do is define the device instead of Eco coda, where you would be leveraging the Nvidia GPUs, you define the device Eco Neuron. By doing so, Pytorch and the frameworks that are supported by Neuron will compile the graphs and run that model training on training devices. To help you speed up the training, we developed the ultrasover. But a server based on ranium. It combines 4 instances together. Those instances are interconnected by a very fast network, which is not a traditional network. It's similar to what Nvidia has in terms of, they call the NV switch, which is a dedicated proprietary network. We develop a proprietary network for those neuron devices so they can communicate together and bundle up 4 of those servers up to 64 devices. So each server has 16 devices. The same goes with the GB 200s and the recently announced GB 300 service, where we are bundling up now any video GPUs. Putting them together, different servers, each server with a different amount of GPUs. Put them together with up to 72 GPUs together. And we can understand the difference of training your model on a single server with a few GPUs and why. An Ultra server is very important for you with 72 GPUs. But before we discuss this, let's understand how those servers are interconnected and how that innovation was um developed. So, it is not a single server. When we talk about Ultra servers, the name implies one server, but actually you have different instances. For the Ultra server based on training, I mentioned we have 4 instances, each instance with 16 devices. For the Ultra servers with any video GPUs, you have up to 18 instances where each instance has 4 devices. Those instances they are interconnected by NV switch, which is uh NV link, the proprietary interconnect on an external switch. If you heard Jensen Huang, the CEO of Nvidia, talking about this last week, it is a piece of innovation in terms of that proprietary connection which is outside of those servers interconnecting up to 18 of them. And that's not the Ethernet. That's not the infinite band network. You also get that secondary network in case you need to expand your workload on multiple servers. OK, so is, is the Ultra server the only innovation? So putting that together on anV switch, is that the only innovation? And the answer is no. The other thing that Nvidia did was that they developed a ship, a CPU. So a company that was heavily based on GPU decided on creating a CPU. And why is that? Because they understood that to help customers, that CPU, which is AR-based, is not X86, required to be connected to that GPU in a very fast network as well. So they developed the super ship, which is a GPU and a CPU together. And that super ship allows you, for example, on interesting use cases. If you're training your model and your GPU, which has 108 gigabytes of memory, requires to access the memory mapping on the CPU. Now it can on a single coherent memory mapping. So that was a very interesting innovation, especially when you're talking about offloading activations, meaning your model is training, is learning the weights, but at some point, those weights which are not being used, they are offloaded from the CPU to the, I'm sorry, from the GPU to the CPU memory. And now you have available memory on the GPU to load more data. Well, of course, when you're training your models and you're using a lot of those devices, you want data as close to the compute as possible. In order for you to do so, Amazon Sage Maker Hyperpod, we have a feature called Topology Aware scheduling, where when you're deploying those models with many different servers together, we understand how to deploy those models so that data and that compute, they are close together. On the Ultra server example specifically. We have that ultra server ID annotation, which you have to put on your container definition. And we're gonna see during the demonstration how um even though you're not a cabinet expert, you can use that because we have some um tools to help you remove that efiniate heavy lifting. So let's go to the demo and understand how we can connect all of those features together. During the demo, I'm gonna be walking through a video that I pre-recorded, so we didn't have any problems with the um Internet connection, but we might have some problems with the video? No? OK. Well done. Perfect. So let's start with getting compute capacity. I have here the SageMaker AI console, and we have already a training plan, but let's go through the creation of a training plan. Training plans, they can be, um, defined for SageMaker hyperpod clusters or for training jobs or even for inference and points. You choose an instance, you choose the number of instances you want for the training plan. And you choose the start date. So the start date could be in the future. You can plan getting capacity ahead of time if you want to. Then you click on find training plan, and you're gonna have some options. Those options could be immediately available as we are seeing those two first options, or it could be some options available in the future, even though you need the start date today. After you have done that, you can choose which training plan you want. And it's important to understand which availability zone that compute capacity is allocated, because once again, we want that compute capacity to gather as close as possible data and compute. So you look at the availability zone and then you decide that that's the right training plan. You have to give that training plan a name. It could be any kind of name. You can add tags to that training plan. And then you have this summary, this review. So you review the availability zone again. You review how much you're gonna pay for the training plan, and remember, when you're paying for the training plan, that's an upfront cost. And you also have the price per instance and all of the details, and of course you have to confirm that you, you want to buy that training plan. When you click on that button, it will show up on the console now, and it will show up as pending and the number of instances that you require because it takes a while, a few minutes, so that training plan is actually active. So after that training, training plan is active, you can go to your SageMaker hyperpod cluster, and you can allocate that training plan on a specific instance group. And an instance group is a logical definition of a specific instance, the name of that group. The training plan, and then when you're submitting your jobs, you can submit the jobs and based on the instance group that you have, that job will be running. So let's say that you need a job that's going to be running with GPUs, you're gonna have an instance group with GPUs or if you need a job with CPU that instance group can be with CPU as well. Some of the definitions on that instance group. I define the instance type, the training plan, but I also have to give an IIM role which allows the permission for that instance group to be used by the um orchestrator either SLA or ECAS and also this S3 bucket where you're gonna have the life cycle scripts. After you have your instance groups, and you can see I have a few different ones there. Now we can go ahead and we can use other features of the SageMaker um hyperpod, right? So remember, those instances they might take a while to come up, because we are installing a lot of software for you, we are configuring the network, we are running some benchmarks and make sure that everything is running correctly. After they are allocated, you can go back to the training plan console and you can see how, how many of those instances that you have bought are allocated or idle. Perfect. Let's go back to the cluster and let's um actually activate the task governance feature. So to activate that feature is as simple as a single click. You click on a button and behind the scenes we are installing the software for you. We are configuring task governance, and after it's ready to be used, all you have to do is to define the policies. Policies are going to be the different priorities on your queue. So for example here, I'm defining 4 different priorities where inference has the highest priority, fine tuning has the lowest priority, and I also have the experimentation training priority. After I have defined those policies, now I have to actually allocate compute to different teams. So I can create different teams, get that capacity that I just bought through flexible training plan, and I can allocate that to my teams. So, right now, I'm creating Team A. I'm gonna call it my awesome team A, and that team, um, sorry about those bugs on the cable. And that team will actually get some capacity allocated, right? One of the things that I can define when I'm allocating that capacity is, am I going to be generous with other teams? Yes or no, right? So I can lend capacity or I can borrow capacity from those teams. That's a decision I have to make. And after I decide what kind of capacity lending and borrowing I'm gonna be doing. I can also define how many instances of capacity that I have that I'm gonna be lending from some other team. And the last feature on that specific panel is to enable preemption. So if you remember Reca talking about you have different prior jobs running on a queue, and those are like lower priority jobs, and then higher priority jobs come in, you want that higher priority job preempting the lower priority. So you enable that for um teammate when defining the allocation of those compute nodes. You can either choose different instance groups, so you don't have to choose from all the instance groups. You can choose to allocate one complete, one whole instance to that specific team, or you can define that you're gonna allocate just a few GPUs, a few VCPUs, and some memory to that team. So the allocation of computer resources. Is the baseline of compute that team has. It doesn't mean that compute the team cannot have more compute, it can be borrowing. So in order for you to experiment that, I'm defining a second team, Team B. So both teams are going to have one single resource, and the example that I'm gonna show you submitting the jobs now, we're gonna see how we are bundling together everything that we just configured. So first of all, this is my cluster. So my cluster has two instance groups. One is the controller, the other one is the training instance group where the controller doesn't have GPUs and the training instance group has some GPUs. OK, so after I reviewed that, what I'm going to do, I'm gonna submit a basic training job, and I'm not prioritizing, I'm not defining any tag for prioritization here. I'm just submitting that, and that Sagema HyperPod cluster is orchestrated by Cobonetis, but I'm using the HyperPod CLI which allows you to submit those jobs without knowledge of Cinetis. So with the hip create hip Python job and the definition of job name, container image, um, the pool policy, the number of nodes you require, so you can see I'm defining three nodes here, you're submitting and behind the scenes what the hyperpod LI is doing is creating the container definition for you and submitting that container definition to the Amazon EKS cluster. So that job was submitted. We have 3 replicas, meaning we are using 3 pods, 3 servers. The pods are running and the workload is running. So let's see those pods running as a simple, simple cabernets administrator. I could do cube keto, get pods. I would see those pods like that, and let's see if the pods are training. Yeah, so we can see that the pods have started training. We have the training split, so we are splitting the data set into training and validation. And we are downloading, fetching the files directly from hugging faces. So, OK, perfect, simple to submit a a training job, right? But what happens when one of those nodes fails? And usually during large distributed trainings, those nodes will fail. Right now what I'm doing here on this demo, I'm simulating a failure by annotating one of the nodes as unscalable. And why? What's the reason pending reboot? So let's see what's happening when I do that. I submit this command to the SageMaker Hyperpod. I can go to the management console, and I'm gonna see the node spanning. So if that was a harder failure, actually, what SageMaker Hyperpod would do, it would. Annotate that node has failed. Remove that node from your cluster. Bring a new node, which now is running perfectly. And add that node back to your cluster. So initially you would see your nodes in an unknown ready state. But after a few moments when Sagemaker hyperpod had replaced that note, You would see that node back online, right? So because we don't have a node, one of the pods stopped running, it's on pending. Now that the node is back online, that specific pod is back running, right? And you can see all of that on the StageMaker Hyper product management console. OK, so far easy. Now let's talk about Um, Let's talk about borrowing compute compute capacity, right? So, I'm doing the same thing. I'm starting a training job, but what's the difference here? The difference here is now that I'm gonna, I'm, am annotating my training job with a priority, which is training priority, and I'm giving that training job also a name space, and that name space was defined when you created the team. So when you create the team, you get a name name space definition, so you have isolation of different teams when running on the same EKS cluster. OK. So, submit the job. Now the job was submitted successfully, the pods are running successfully. And I can see that the hyperpod CLI automatically annotated that job for me. When I checked the queue for those two different teams, Team A has won a limited job and Team B has no jobs at all, right. Perfect, but how am I running that if Team A only had a single node and they submitted a job for more than one node. So if you, if you see over here, node count equals 2. So let's see what happens. What happened is that Team A actually borrowed capacity from Team B, and we can see that borrowed capacity through data location. Instead of having only 48 VCPUs, we borrowed additional 40, and now we have 88 VCPUs and instead of having only 4 GPUs, we borrowed another 4, and we have a total of 8 GPUs to run our use case. So perfect. So that's borrowing capacity. And then When we go back. And we say, what happens if Team B submits a job? I'm borrowing capacity from Team B. Will that job run? So let's see what's going to happen. I'm, I'm submitting the job the same way as I've been submitting the job, hyperpod CLI, just a different um, different team with the same priority. When I do that, the job is running, we can see that. We can see that the job was annotated. So let's check the queue, let's see what's happening right now. We can see that the job that was running for Team A, now it's suspended. It's pending, and the job for Team B is running. And why Team A is suspended, it's pending, because it doesn't have that borrowing capacity anymore. Team B. Got it back, reclaimed. When we look at the jobs from um a hyperpo CLI perspective, we can see that the name space Team A has a suspended job and the name name space Team B has a running job. So perfect. Now let's go ahead and let's preempt a lower priority task. So right now training is a lower priority than experimentation. So we're gonna submit a new job on Team BQ with the experimentation priority, and we can verify that experimentation has a weight of 80 while training has a weight of 70. So when we submit the job, the job is submitted successfully, the job is running, and now we can see. On Team BQ we have one running job, which is the experimentation one, and that training job had been suspended because it has a lower priority. And if we check the allocation, we can see that Team A is not borrowing capacity. That's why the Team A job is still pending, and Team B is actually borrowing capacity from Team A. So that experimentation job requires some additional capacity. It borrowed from Team A, and that's where we are putting all of the features together. So that's it. For, for the demo, that's what I wanted to show you, and I want to go back to the slides and show you some advanced use cases for Hyperpod with those um Ultra servers. So before we go and we dive deep into mixture of experts and how to leverage Utra servers with that, let's recap with what we have been so far. We have viewed that the rapid evolution of GAI brings challenges, and then we have SageMaker Hyperpod to help you overcome those challenges and increase your goodput. One example is perplexity. So perplexity is using SageMaker Hyperpod and speeding up their development by 40%. The other thing is you can easily access GPUs by using flexible training plan, and one of the advantage of using training plan is that you get up to 68% discount versus on-demand pricing. So you reserve capacity through flexible training plan, you plan that capacity, and you also get some discounts. And task governance together with topology aware scheduling will make sure that all of those jobs that you are submitting, they are landing on the right compute close to the data and using the fastest network possible. OK, so combining all of those and thinking about real world use cases and advanced use cases that need and require a lot of compute power, let's dive deep into how you can leverage more than one server. So all of these jobs that I submitted during the training, during that demonstration, they were using more than one instance, and how they are connected together. You don't want one instance on a data center and the second one on a second data center. You don't even want instances which are not interconnected into the same network spine. So that's why AWS developed the ultra clusters. It is a combination of many different servers. Interconnected on what we call a single spine, meaning they are interconnected with the lowest latent possible, highest throughput possible, the least amount of network devices in a single route, OK. Not only that, but ultra clusters also bundle up together storage because you want computes close to the um data. You want to bring those data sets close to the GPU and you want computes close to the GPU because you need to save the checkpoints as fast as possible. Ultra clusters connect Amazon FSX for luster on that single spine as well, so you get all of that performance. So, I've been talking a lot about the network, but is it really important? Am I just talking about a specific AWS products? So we profiled Niemontron 15 billion parameters, and while profiling that training run of Niemontron 15 billion parameters, we identified that 60% of that training time was spent on the network. And it was spent on what we call, or the market call communication collectives, meaning when a GPU is training in the model, it, it is learning weights, but at some point it needs to synchronize the gradients, those weights, and do some math, apply some lost function, and when it starts doing that, now it's using the network a lot. And those collective communications, they are based on a library called Message Passage Interface API. And they heavily use the network. So how come AWS developed a network that can help those model training go very fast? Traditional networks, they are using eco-cost smooth path, meaning when you start transferring data, you open a single channel and you transfer all that data that you need on that. Channel you might open a single channel and start transferring a a second pair of data, but those two channels might use a specific network device in the same route and that network device could be either a bottleneck or a single point of failure. Right, so what AWS did to help customers is that we developed alask fabric adapter which uses the scalable reliable Datagram SRD. It's a protocol developed by AWS and we're using on our network. SRD, when you open that channel for communication, it actually shards your data and spray that data into multiple different channels using all of them together. So you transfer data from instance A to instance B using all of the available channels, all the available network devices at the same time. It means that if a single network device fails, you're not heavily impacted. There are some benchmarks that we have run and that are published on the market that showcase that your connection between instance A and B without SRD. It stops for a while if a network device fails, and with SRD and EFA, it doesn't stop. It has a small hiccup in terms of performance. It might lower from 100% to 80% and then back into 100% very quickly because we are spraying those packets into multiple devices. So, putting it all together, the super chip from our, from Nvidia, the Grace Blackwell GPUs and those EFA devices, this is a simple diagram of one of those instances where you have the network devices connected directly to the GPUs. So the GPUs have access to that network, to the fast network, and you have the CPU connected to that GPU. How that ties together with mixture of experts, which is an advanced use case. So, so far we have heard about transformer models, and Transformer models, they were, you know, doing encoding, embedding, things like that, and transforming the data, generating new data, correlating that data. Then advanced use cases, like for example, if you want to really specialize that model on maybe doing MO model of translation. Image creation and other things. Your training run was taking too long because all of the GPUs wanted to do everything and that model wasn't getting a good specialization on different domains. So a mixture of experts helps you to actually have different GPUs which are going to be the experts. And you are going to have a router, which is just a function, a logic, that understands how to manage data throughout those experts, so the experts get the right data. A good example could be if we have in a classroom different teachers, we could have, could have a math teacher and let's say an English teacher, right? So the English teacher doesn't want to get those very complex algebraic equations, right? That English teacher wants to get maybe the Shakespeare um books and all the literature, and then the math teacher won't get all the numbers, right? So you need someone to distribute that load, and you have that router. Then again, I'm talking a lot about network. Mixture of experts requires a lot of network communications. And how those networks might happen on that specific use case. So first of all, you have a leader, a ranked 0, which will start that process and start the communication and understand how to define those different experts. After that, those tensors, you know, part of your neural network, they are mapped to those experts. So the part of your neural network that will will actually train on those experts are moved there. When they are there, when you have the definition of those experts, now data is exchanged and The routers might be sitting on a specific instance sending data to another one, and on the bottom of that slide you can see an image, an image where you have node zero, which is an instance with 2 GPUs, and node one, a different instance with 2 GPUs as well, but with 1 GPU, I'm sorry, and you can see that you have one of the routers which send data to node 0 and node 1, which is the router in the middle. In that case, if you're using ultra clusters, 72 GPUs are interconnected together, right? So you can have different experts running on those 72 GPUs without leveraging a network focused on using the NEV link switch. But if you're using more than 772 GPUs on Ultra server, or if you're using other instances that have less GPU, then you need the network for that router communication. And of course, at the end of your training, you have to collect all of those gradients, save that model state for one last time, so now you can use that later for inference. OK, um, how that maps to the Ultra servers and how that maps to those Ultra clusters, which are the interconnect of everything together. So on the Ultra servers, if you're creating your model and you're enabling the mixture of experts, you can define expert parallelism equal 18, meaning you're defining every single server on Ultra server as a single expert, and your expert will have access to 4 GPUs. By doing so, you are isolating the communication of that specific expert on a single server, and you don't need to, you know, do any computation outside of that instance. And then you define tensor parallelism equal 4, meaning you're sharing that neural network into the 4 different GPUs and you have 4 GPUs with 14th of your neural network spinning up your model deployment. But if you're moving further away from 72 GPUs growing to 144 or even more than that, now it comes in the, the ultra clusters. So, you have the Ultra servers connected with NUV link switch up to 72 servers, um sorry, up to 72 GPUs. And then you interconnect those Ultra servers with the Alaska PE adapter on top of the scalable reliable Datagram protocol, and then you can get more than 25,000 GPUs interconnected together working as a single system. So some call to actions that I really would like you to understand before you leave this session. So the first thing is there are a lot of best practices on running those Ultra servers. It's not only just acquiring capacity using the SageMaker Hyperpod features to Do task governance and priority on those skills, but you have to set up specific libraries with specific versions, and we have all of that well documented on StageMaker Hyperpod documentation and the Amazon Institute or Server documentation as well. And you might have heard about multiple instance group, which is a feature that we recently announced, meaning you can get a GPU and you can Charge that GPU for example, into two diff different virtual GPUs, right? So that is not supported on Ural servers today. The other thing that I wanted you to go out before uh leaving this session, you can get your iPhones if you want, your mobile phones, and scan those QR codes. The first one, AI on SageMaker hyperpod, is a repository that we created with a lot of instructions on how to put all of those features together easily and quickly, so you can speed up the deployment of those clusters on your AWS account. And most of what we have seen today on this demo, it's explained there how to replicate on your own account. The second repository, which is the awesome distributor training, there we have examples of starting, um, for example, a fine tuning on Deep seek on when. A training on LA 4 and Lemma 3, or even doing some micro benchmarks if you want to test and troubleshoot your cluster. So all of those assets, they have come from customer engagements and we understood that they are um things that you can use to speed up and leverage so you don't have to do um some documentation study or maybe don't understand how to put all of those things together. With that, um, let's close the session, and thank you so much for being here, not only on Reinvent, but coming to the session, listening to me and Reca. Reca, thank you so much for sharing the stage with uh, with me, and please don't forget to fill out the survey. As a data-driven organization as we are on AWS and Amazon, we really need, um, that survey because we read your comments. Whatever you put there, we're gonna use as lessons learned for our next, next session. And hopefully you're gonna have much more fun than you had today. Right, thank you so much.