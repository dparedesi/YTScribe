---
video_id: 0Ss7PcSLS_8
video_url: https://www.youtube.com/watch?v=0Ss7PcSLS_8
is_generated: False
is_translatable: True
summary: "This session, led by Dr. Ryan Ries from Mission Cloud, delves into the practical implementation of Generative AI, focusing on bridging the gap between technological hype and measurable Return on Investment (ROI). Ries acknowledges the prevalent industry skepticism, citing statistics that claim 95% of companies fail to see measurable ROI from their AI initiatives. However, he counters this by highlighting that success often depends on expertise and strategic partnership; specifically, 67% of companies working with experienced partners like Mission Cloud have successfully deployed valuable solutions. He argues that the failure to demonstrate ROI usually stems not from the technology itself, but from poor use case selection—such as focusing on low-value tasks like image generation for marketing rather than high-impact automation of routine workflows—and a lack of understanding regarding the necessary cultural shifts within engineering teams.

Ries provides a historical context for Generative AI, tracing its roots back to probabilistic text generation in 1906, through the neural networks of the 1980s, to the cloud-enabled GANs of the 2010s. He emphasizes that the \"revolution\" of 2023 was primarily about accessibility; interfaces like ChatGPT democratized access to models that had existed in research labs for years. This shift has cemented the \"chatbot\" as the default interface for modern applications, even when users are interacting with complex business intelligence or coding tasks. He discusses the concept of \"spec-driven coding,\" noting that as AI tools take over syntax generation, developers must evolve into product managers who meticulously define requirements and architects who understand the broader system, rather than just writing lines of code.

A significant portion of the talk warns against the pitfalls of blindly adopting \"agentic\" workflows. Ries observes that many engineers are over-complicating architectures by deploying agents for tasks that could be handled by simple, faster, and cheaper prompts. He advises a pragmatic approach: analyzing latency and cost requirements before deciding between a single-turn prompt and a multi-step agent. He also addresses the persistent challenge of hallucinations, noting that while models like GPT-5 have high intrinsic hallucination rates, these can be drastically reduced (from ~45% to ~6%) by grounding them with internet access or Retrieval-Augmented Generation (RAG) frameworks. He suggests teams stick to one model family to build deep expertise rather than constantly switching to the \"flavor of the week.\"

The presentation concludes with concrete, high-ROI use cases. Ries details an Intelligent Document Processing (IDP) solution for insurance underwriting that reduced manual processing time from five hours down to two—a 50% efficiency gain. He also describes a low-latency Interactive Voice Response (IVR) system built using Amazon Nova Micro, Amazon Connect, Lex, and Polly, demonstrating how selecting the right lightweight models is crucial for real-time applications. Ultimately, the session advocates for a disciplined, use-case-first approach to AI, rigorous validation through unit testing, and a focus on solving real business problems rather than chasing novel architectures for their own sake."
keywords:
  - Generative AI ROI
  - Mission Cloud
  - Intelligent Document Processing
  - AI Agents
  - Amazon Nova
  - Hallucinations
  - Chatbots
  - Spec-driven coding
---

Good morning. Wow, clapping. I love it. Can you guys all hear me out there? It sounds like it. I almost forgot I had a clicker put in my pocket on accident. So yeah, excited to be here today and talk about G A I, uh, just a little bit about me. I've been in this industry for over 20 years, I guess I aged myself a little bit back when we used to have other words, right? We used to talk about NLP more and now it's all Gen AI and everyone might remember image registration versus C. Computer vision and all those good things. So I've been working on this a long time and helping companies develop that and we work with um AWS. We've won a lot of awards and been runners up in Gen AI consulting for the last 2 years in, in AI and other years, submission. is really here to help you guys. We've built out over 175 projects over the last two years. We're the first partner to get access to, uh, the first SI partner to get access to Bedrock. So we've been building on Bedrock for 2.5 years. So if you guys have questions, feel free to come and find us. Our booth is right behind this in the Dev Center, right behind it. So excited to talk to you guys and see, you know, whether we can continue the conversation we have today. How many of you guys saw this article? How many of you guys actually read any of these articles? Oh, actually way better than, than usual. It's, you know, I love headlines nowadays. It's all a lot of clickbait, right? Trying to get people to come in and read it. But what did the article actually say? It did say yes, 95% did not see measurable ROI, but what does measurable ROI mean, right? It doesn't mean they didn't see ROI. They just didn't see something that they felt really had a big return. Right, 2, 3X or things like that. It might have only had like a 1.1x return, but they still 40% of them did put this into deployment. And so it's still, even though that article said only 5%, you still saw 40% put into deployment. And the number I really like is that 67% of those were done by companies like Mission that were able to come in and help companies build because we have that expertise to help you. The other thing it said one of the issues people always had was they chose bad use cases, right? That is the one thing you should always be looking at is what use case am I doing, right? AI isn't the problem most of the time. AI does what you tell it, but you are choosing a bad problem to solve. You were looking at maybe something that was, I'm going to build images in my marketing team or something like that, and then you're having a hard time. Time quantifying it because hey I'm only saving a little bit of money by doing that and so you're not seeing that massive ROI now. People that were looking at better use cases that were automating, you know, run of the mill tasks that weren't that interesting we're able to steal a lot more ROI. Also, you know, a lot of internal experts are, you know, important to have. You need it, but it's not insufficient, right? You need to also be working with people that understand the tech. And then I'm sure you guys have seen it over and over, especially as you look at coding tools that this is not just a technology change, it's a cultural change, right? How many of you guys have started to look at like Quiro as a, a developing platform? A couple of people and so Quiro is really pushing on spec-driven coding, right? Most of the time when you're looking at coding, people just kind of start and they start making their application and they don't really think out all the little steps. But now as you start using these coding agents, you have to start. Thinking about what are each of the steps I need to start looking at it really early and so now your developers need to start thinking more like product managers, right? So you're gonna see this real cultural change that has to happen inside these systems so it's really important that people understand that um so. Little bit of a summary. We're gonna talk about who we are, kind of mentioned it already. We do a lot of AWS stuff, uh, tons of competencies in all the different spaces, and so we are a one-stop shop where we do costs and resell 24/7 support and then professional services. So if you're looking for any of those, feel free to come and talk to us. And now I'd like to just talk a little bit about G AI. Who thinks Gen AI is new? And obviously you can see my slide and know that I don't think it's new, right? It really dates back to 1906. Who thought it went back over 100 years? Anybody? Nobody thought it went back. So that was probabilistic text generation in 1906. Think about that. These are like handwritten algorithms to figure out what text should come next in a sentence. Then you kind of had a lot of early concepts in the 1950s. There were a lot of rule-based systems. You started to see early neural nets, but then we had the AI winner, right? And then the 1980s came, and now everyone had personal computers and you started to see clusters in universities, so neural nets were pushed further and further again, right? And then in the early 2010s, this is when the research on GAN started and a lot of it was really enabled because now I have the cloud, right? That's kind of when the cloud started. Now I have infinite compute and storage essentially, right? And so that's really where you started to see all of these things and then. You know, in the 2020s, did anyone use GPT1 or GPT2 past myself, a couple of guys maybe, yeah, so like in the early 2020s we were doing GPTE one stuff and then 202 GPT2, there are things being done and now everything is, is agents. Um, who looked at my pretty chart, right? Uh, I love using image generation. It's great. Does the timeline really work? Did, did anyone notice, notice that great timeline? So I then prompt it better. I'm like I, I can prompt better. And I made it worse. How, how many of us love prompting? Any anybody love prompting? But I, I really like a good timeline that has the 1,500s mixed in with the 1900s. It's, it's a really powerful, uh, powerful tool here. So I'd like to ask this question. What innovation happened in 2023 that made everyone excited about Jeni I? Anyone shout it out. Chat GPT. What was chat GPT? Anybody? OK, chat GPT was someone made a website. That's it, right? These models existed. They just were more accessed by people like myself that's a data scientist that would spin it up in Sage Maker, you know, maybe you're using hugging face at the time, but now you have a website that fronts a model that you can now. Talked to they made it intuitive, right? It's cool, it works. And so, but that's the innovation when you really think about it is someone just made it easier to access a model. That's it. That's the whole craze that started is it's now easy for anyone to think about it, look at the ideas and understand where you can go from there. So for us, we have our own chatbot. Everyone probably has a chatbot now, right? You can make them super easy. Bedrock even has open search and you can create a chatbot and you're gonna ask all kinds of, of fun questions to your, your chatbot. Like I got a, a little joke going on with my own chatbot. But why is it a chatbot? It's really because that's the easiest way to do back and forth conversations, right? And you're really looking at how do I simplify the interactions, because everyone is looking at, I just want to be able to talk to it and get information out. Now what we've noticed is that people now just confuse everything with a chatbot, right? Because they want to talk to things. They may want to have traditional ML models. You still may want to do clustering or you want to do some kind of prescriptive analytics or things like that. But all that people want to say is like, I just want to talk to my data or I want to do this. And so everything gets buried behind a chatbot. So even though someone's coming to you. And they may be asking about Gen AI. Often it's just an interface, right? So to think about that, your chatbots often just your interface, and now chatbots are going to kickstart, you know, agentic workflows. And then the other thing we see a lot of is just Gen BI. How many of you guys have used like Quicksight Q or QuickSuite or or any of those tools? A couple of people. So, you know, they're really nice, but like you said, they, they do have limitations and so people are using generative systems with cool React libraries to, to come and populate all of your graphs and then you really get into pushing an LLM to write code. So there's a lot of really fun things there, but at the end of the day, everyone is looking at how can I just talk to the system. So that's why chatbots have become our main interface. So. For a minute. Um, I was playing, I like images as you guys can see, right? The good. I asked for a monkey astronaut riding the bike on the moon. How, how did we do? We do good? Well, it did mostly good. There is a moon in the background if, if anyone didn't catch that bit. And early on, it was really bad at letters, right? So it actually works pretty well with, with letters and, and all that. And then we get into the bad. Anybody have a guess on what I wanted the system to do? Any guesses? Tower Bridge, good guess, good guess, but I asked for an AWS architecture diagram of glue connected to redshift connected to quick site. How did we do? Anybody? Did we do good? Did we do bad? This woman is very smart. She says we did good, and we did. I asked for things for an architecture diagram. It's buildings that you made architecture. I said glue, glued them together. How many people know what redshift is? Sunset, right? Sunset. So it made a sunset. It did exactly what I asked it to do. It just didn't really understand that I needed to have a diagram that had boxes of Sagemaker and things like that attached to it. But it did exactly what I wanted. And so that's one of the bad parts about Gen AI is it will always give you an answer. It's not necessarily the answer you want or you need, but it will give you an answer. And so you as the individual have to be looking at the answers and you have to come and understand what am I trying to do. Now the ugly. We were doing an event in July and we wanted Rudolph the Red-Nosed Reindeer. Now these are some older Jenii images, but I use them for an example because the new ones will create Rudolph the Red-Nosed Reindeer, but before they wouldn't. And we went through a lot of prompt iterations. There's like 30 pictures in the series, so I'll only show a few. And, you know, I was like, give me a reindeer with red, with a red nose and nothing. Then I was like trying to go closer and I, it was like, give me a, a reindeer with a clown nose, and it gave me a balloon with a clown balloon. So it just, it didn't really get it, right? Now, I was giving this talk a while back and someone goes, do you know what the biggest problem is with your images? And I said, no. I, I'm, I'm sitting there thinking, hey, I'm, I'm gonna win because it's not Rudolph the Red-nosed reindeer, right? And so this guy's like, do you know the real issue? And I don't know how many of you guys know, but this is a white-tailed buck deer. It's not even a reindeer. So if you're asking questions to Gen AI and you don't really know the answer or what you're expecting, you can't even. validate that it's giving you the right answer because to me I was like, oh yeah, that looks like a reindeer, it just doesn't look like Rudolph. So it's really important as you're building these systems that you can understand how do I validate it? How do I prove out that it is giving me the answers I want? How am I going to write unit tests? How am I going to make cases that just validate that yes, the answer is coming out, especially when you're going to deliver a system to people that may not know what the answer should be because it will hallucinate. How many of you guys get inundated with all the news, right? Even last week was crazy, right? There are like 10 new announcements. Everything's giving you whiplash. The advice we often give to people is choose a model that works for your particular use case, and each use case may have a slightly different model that you're using, but choose one family, and then you just keep using that family because you're going to gain expertise. They're all like coding languages, right? You don't want your coders to be switching from Python to .NET. To everything because they're just not going to be as adept at getting the answers as if they just always are working in the same family and every model's always like leapfrogging the next, right? So wait two weeks and whatever your favorite model is, is going to be the top model on the leaderboards. So you don't have to always do that model hopping. The other thing that's always kind of interesting to me is, did you guys read any of the reports when OpenAI launched GPD 5? Anybody read the reports? So OpenAI said that GPT-5 hallucinates 40 to 45% of the time. Anybody know that? Huge, right? Hallucinations are astronomical in these systems. Now, if you connect GPT-5 to the internet, which is why OpenAI has GPT-5 connected to the internet, the hallucination rate drops to 6%. And so that's why. If you're building a model inside of AWS, you need to be either attaching it to the internet or putting it with a rag database so that you can push your hallucinations down, right? If you're not doing that, you will be looking at a high hall, the probability of a high hallucination rate depending on the questions you're asking. So it's really important to think about that when you're building out these systems. G AI agents, is everyone tired of Gen AI agents? No? Wow, I'm tired of Gen AI agents. Um, so agents are, are fun, right? It's gonna go do everything. I've got an agent there. He, he's my group leader. He's got all my other sub agents. I'm gonna send them out. I'm gonna do all kinds of cool things. We're gonna have some autonomous goal achievement, and we're gonna decide how to, how to answer all of our questions, right? Now when you look at agents, it's really important to understand your use case because I feel, especially, you know, most engineers I talked to have just gotten lazy. They're like drawing a diagram that's just like agent to do this, an agent to do that, an agent to do this, right? They're not even really putting in the thought like what does each of those agents do? Is it actually an agent or is it a prompt? My team, there's a lot of people and we have conversations all the time. Is that a prompt or is it an agent? And there's a lot of things between the two of those, right? Like if I go to an agent, I have to build the framework, although there's agent core right now to do the framework to make it easier. But you still have, when you're doing agents, delays in your time. So if you have time critical matters, often you're going to want to just run with. Prompts versus agents because you're going to get the answer back faster through just a prompt and most of the time you can prompt most of the things you're trying to do that an agent can do. And so that's things to look at is what's my use case? Do I really need an agent? Agents are also potentially going to be more expensive and they take longer. So you really have to think about what's my system. Am I really doing something that's an agent or not an agent? Um, so we've done a lot of use cases, as I mentioned, tons and tons of cool stuff. I won't get to talk about too many of these, but we've done everything. I haven't seen anything new in at least 6 months. So if you guys have use cases, want to ask about, hey, would this be possible? Can we do that or not? We're, we're here, like I said, back over that way, right on this side of the crowd strike booth. The biggest use cases we see in the market right now are IDP, right? About 60% of the market's doing IDP stuff, really cool space. Then you've got chatbots, as I mentioned, doing chatbots. But then there's everything else, right? Code generation, recruiting, drug discovery. There's cool things you can do with chatbots where you can do training, educational training or other training of your employees. And if they're going the wrong route, you can iteratively. Change how the chatbot responds. So we had that built in where we were working with a company that does medical training for new doctors and if the doctor was going down the wrong path of questioning, the the agent would then get agitated while the patient would then get agitated and start being like, hey, you've asked me that already and stuff like that, right? So there's really cool things you can do. Diving in translation has been big, all kinds of cool stuff. Um, and then like I mentioned, chatbots is that interface. If you really want to, to learn about chatbots or have us build you a chatbot, we have a lot of really cool fast track packages out there that we can help you guys build these. These are things we've done day in and day out. Uh, they're really cool, um, quick suite, quick site, you know, that, that's something you guys will see a lot of this week, something to, to look at and talk about. So I'll just skip through IDP for a second. So this was a really fun use case for IDP. We're working with an insurance company and they're underwriters, so they get applications all the time, and the underwriters were spending 4 to 5 hours on the system. And so this company, it gets $10,000 to 15,000 applications and they were growing. They just got bought by a bigger insurance company and so they can't hire fast enough, right? So that's one of those use cases where you look at. How much just manual work is there? How are you going to solve that problem? And if it's most of the time I hire and train somebody, then you can probably automate it, right? So that's a system to look at. I'm going to automate this. And then you can build out IDP really cool, and then you can put everything into a database and you can ask questions to it, right? So now the underwriter will get results and then if he asks questions where he wants to. Further answer, he can then ask about the application to do his work and so it was really neat. We were able to take their time from 4 to 5 hours down to 2 hours, saving them 50%. And so there's massive things. This was just the first phase, right? We're looking at how can we help continue to improve this to get it down even further. So there are tons of savings out there when you look at these chatbots. So another chap retrieval augmented generation. I'm sure everyone knows retrieval augmented generation. Here's a pretty standard architecture, right? Some kind of interface connected to an LLM connected to, you know, your agents at the end of the day and all that. Um, and so we were working with a company and this is where we get into all of that like timing bits, right? So this is the IVR system. So now you have to think about. Somebody's on the phone. They're calling and they're talking to your bot, and so it has to be fast. So I don't know if you guys have used Nova, but Nova Micro is actually pretty awesome where you can sort through really quickly with Nova Micro on the inference side, get to an answer, and. What's going on there and this was kind of the architecture for kind of an IVR system. How's Connect, which we all love Knect Lex Pauly to do the text to voice, so a lot of really cool things going on now. I have way too many slides, but we do lots of cool stuff. We could have talked about agents and cool things. So on agents, I'm giving a talk with AWS in like 2 weeks, about agents. So, you know, anyone that was here, just look for a talk about agents. It's, it's a pretty cool use case. Lots of, lots of fun architecture at the end. Um, Agent Core, and I, I have way too many slides. Sorry. But if you want to work together, you can find us in the, in the booth in the back. I put out a newsletter every week. I think we talk about fun, cool stuff. So if anyone wants to hear my random thoughts, this is how you, you can hear it. Um, appreciate you guys all attending today and they're gonna give me the hook in 19 seconds. So if anyone has a question. You know, like I said, I'll be over at our booth for the next thirty-ish minutes and happy to, to talk through anything you guys want. Uh, but thank you, everyone.