---
video_id: HC8jSzNmNpU
video_url: https://www.youtube.com/watch?v=HC8jSzNmNpU
title: AWS re:Invent 2025 - Moodyâ€™s: Architecting a multi-agent system on AWS (IND3303)
author: AWS Events
published_date: 2025-12-04
length_minutes: 58.07
views: 517
description: "Financial institutions need to orchestrate specialized analysis across diverse domains while maintaining regulatory compliance and performance at scale. Learn how Moody's built their Research Assistant using domain agents for ratings, climate, and economics; task agents for KYC screening and company credit assessment; and workflow agents for sales and marketing and portfolio monitoring with early warning signals on Amazon Bedrock. Explore architectural patterns for multi-agent coordination using..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

People that have put a headset, can you hear me fine? Can you raise your hand if you can hear me fine? Awesome. Perfect. First of all, thank you so much for being here on a Wednesday morning in Vegas at 8:30 and choosing us versus Swami. I appreciate it. Um, so this is IND 3303, Moody's architecting and multi-agent systems on AWS. Um, my name is Samuel Baruffi, and I'm principal solutions architect, uh, at AWS, and I have the pleasure to have with me Dennis Clement. Dennis is a managing director of engineering and architecture for Moody's Digital Content and Innovation. So in the next 60 minutes or so. Me and Dennis are going to go through the journey that we've been working together on how Moody's have revolutionized their way of thinking and serving their customers through an Agentech system. Before I get started, maybe I can ask a question for the audience, and you can just raise your hand. How many of you are currently building agents? OK, a good majority. How many of you are in the financial service industry? OK, awesome, the majority of you as well. So hopefully this will be very useful for you. A quick look at the agenda on what we're going to cover today. We're gonna start with Dennis taking us through their journey from almost 3 years ago on how they saw an opportunity of revolutionizing their business through the lens of generative AI. What was the vision? What was the challenges they faced. After that, we are going to go through some of the technical foundational services that they are currently using on AWS to implement and deliver those services through Agente AI. And of course this is a technical session. Then it is going to take us through the architecture they've decided on taking, some of the challenges they face, how they solve those challenges, and how they actually evolved from a simple chatbot through a multi-agent system on AWS. And a very important, I think, lesson learned that Danny is gonna share with us is financial services have a lot of instructured data. How do you actually retrieve insights for those financial documents into a structured, um, ability to use that into an agents, right? And then Dennis and I are also gonna share a little bit more looking for what is the vision and what the future entitles. So I'm gonna pass it over to Dennis to kick us off. Thanks, Sam. So Good morning. Um, so I'm Dennis Clement. I'm a managing director, uh, at Moody's Analytics, and I lead one of the largest engineering teams focused on generative AI. Now this is a team that has been producing, uh, actual production grade generative AI products since about 2023. So I'm gonna show you how a 100 year old financial institution is redefining what is possible with a multi-genic system on AWS. Um So I need to start with who we are because I think it's really important to understand who we are so that you know why we built what we built. So yes, Moody's is a 100 year old credit rating agency, right, but that's what's actually really interesting. Right, that 100 years of domain knowledge is actually what makes us really, really dangerous in the Gen AI space, right? We are not a tech company trying to understand financial, uh, institutions. We are the like preemptive, uh, service that financial institutions need to solve their problems. Right, we serve over 1500 customers across 165 countries, 97% of the Fortune 100, and this is all not just, uh, ratings anymore. We are doing risk intelligence across a bunch of different domains, right? We are doing credit, climate, economics, compliance, right, you name it. And folks aren't coming to Moody's right just to browse. They are making billion dollar decisions right across a ton of their own internal systems, right? And we need a strategic AI strategy, right, for customers to actually make a difference on a day to day basis, right, so. One of the big challenges, right, that we have is the fact that we have a lot of different types of customers that are coming into our systems. We have commercial banks running loan origination, and they need a multitude of different data sets to solve some of their problems. We have asset managers, right, that need climate risk model models to do portfolio analysis. We have insurance companies that need regulatory compliance validation, so it's the same platform. But very, very different problems to to solve. So this is our like one pager of our data universe because understanding this complexity is key right to understanding what we build. So we have about 4 core pillars in our data universe ratings, research and insights, data and information, and decision solutions. So we and we create our own authoritative data sets through our ratings agencies. So this is decades of research documents, credit opinions, sector outlooks, right, you name it. Now we also operate Orbis, which is one of the largest, uh, databases of company or entity data in the world. This is 600 million entities, and we are, uh, creating climate risk, uh, analysis. We are doing economic forecasts. We do know your customer screening, uh, we do regulatory filing. So as you can see, the scale is pretty incredible. So with all of this, we have to truly focus on being accessible. Accurate and fast. So the question is how do you build an AI AI system, right, that takes all of this into account. So our customer diversity, right? Is what really drives, you know, why we cannot compromise on accuracy, right? You gotta look at who relies on us 2600 commercial banks processing loan originations, uh, 1900 asset managers making portfolio allocation decisions, and 800+ insurance companies, right, running regulatory stress tests. These aren't like low stakes, right, use cases. And that's why we are introducing generative AI into our system was challenging. So this is just a list of industry awards. Right, this right here really is our accountability markers, right? This is what why we deliver precision, right? Our SAS products, right, carry the Moody's name, and when our company makes like certain credit uh decisions, right, and certain cre uh risk profiles, the market moves, right? So this is why, right, we really needed to think through why our AI systems need to work for high stake financial, uh, environments. So This is where our journey though really begins, not truly technical, right, but what forced us to kind of rethink everything. And this is the big fundamental tension right that we faced right when a commercial bank is making $500 million loan decisions or an asset manager is rebalancing a $2 billion portfolio, right? 99% accuracy just isn't good enough. That missing 1%, right, could be catastrophic, right? So. Let me Paint a little picture, right, of why this is so hard, right? Look at the complexity we're dealing with a single customer, right, comes to us needing uh expertise across all of our different domains, credit ratings, sector analysis, economic comparisons, and regulatory compliance, all of this simultaneously. And they just aren't adjacent domains. These are different knowledge universes, right? And they require separate analyst teams, right? Now, not only that, we are also in a regulated space, right? So we have to think about compliance, we think we have to think about uh data separation when users actually input their data into our systems, we have to think about true data isolation, right? And so all of this, right, means that we needed to make really, really specific decisions about how we were producing our products, right? And what makes it even harder is our our customers aren't just querying Moody's data, they want their own data as well, right? And so they are uploading financial documents. They're uploading PDFs into our system, and we have to seamlessly, um. Kind of merge not only our expertise right but also with their data sets right to solve real world problems, right? And let me tell you, the PDF reality and that unstructured data that is difficult and we are still tackling it and we're still uh fighting that fight, uh, but hopefully a, a, a winning fight. So, now we'll give you a little bit of how we evolved, right? Now, this timeline tells our story. We started like most people did, right, with a rag application, right? So we deployed our research assistant uh about December of 2023. And users loved it. They got answers grounded in real research, right? and it was able to truly solve a lot of their problems. But the moment somebody asks something truly complex, right, where you're comparing risk right for multiple companies, their financial metrics right across your portfolio analyzing news, all of that is when it started to kind of crumble these scenarios were very difficult for a for a basic rag application to solve. Then in August 2024 we got the understanding that people wanted to introduce their data into our world, right? So we released our first PDF upload uh capability, and that is where a customer adds their data into our systems, right, to solve real world problems, right? And that is where I would say that's where our, our unstructured, uh, data story kind of even begins because that's where we really started to realize where we needed to upskill ourselves and our systems. And now in the end of 2025 it's the culmination of all of that, right? We have learned from our customers. We have created custom orchestrators with specialized workflows with specific task agents all sitting there trying to solve a designated task, right, for our users and all of this powered by AWS's infrastructure operating genta loops and all the sorts of evaluations that we need, uh, to for our systems. So Here is how this kind of works in practice, right? This is Moody's research assistant, and this is our first Gen AI chatbot released again in in 2023. And you'll see it's a simple interface, right, uh, but behind the scenes there's so much complexity. This is a specialized intent engine right routing to the right expertise routing to the right data that is that is necessary it quickly provides context, right? and it delivers really fast and reliable answers, right? And it. All grounded in real data and let me tell you we painstakingly cite everything that we can do right because these kind when we do that right this is what gives our customers right the continued trust in what we are doing and how we are serving our clients. But This is where kind of our vision really went and this is what changed everything for us, right? We spent much of our time in research assistant tweaking prompts tweaking the intent engine and all of these for really small gains and we realized early that the future, right, isn't better prompts, it's better context, right? Research assistant right was proven though it was saving people 60% it was giving people 60% faster insights, right? It was 30% reduction in task completions and we were processing decades of Moody's proprietary research, right, all to serve our clients, but we. Realize that we could do even more we realized that there were certain things that this uh chat spot was just not doing for us, so we moved from let's ask a question and get an answer to let's orchestrate specialist intelligence, right? And this is how we were going to move forward. So now I'm gonna let Sam kind of uh show you what the foundation kind of helped build all of these systems. Thank you, Dennis. So before Dennis takes us to the journey on how they've implemented AWS services and AWS infrastructure into their gente system, let's just quickly go through some of the services and high level idea on how they work. The journey that Dennis just took us through is what we've seen in financial service industries and other industries as well, and I really like this slide because it kind of demonstrates exactly the journey that they've taken. They've started with research assistant as a generative AI assistant back in 2023. They started creating specialized agents as the next step. Dennis mentioned the PDF upload. And of course, they are now working and have released agentic AI systems as part of a more autonomous multi-agent system collaboration that is gonna spend a little bit more time dive deeper in a moment. But how do you achieve that with AWS services? Well, There are multiple services. We start, of course, with Amazon Bedrock. If you watch the keynote from Matt yesterday, Amazon Bedrock is not just a service, it's a full ecosystem of services that allow companies across different industries to build production grade workloads through agents and foundational models. The most important aspect of this is the ability to choose models from different providers in a true serviceless fashion, which Moody's is currently doing. But if you have different use cases such as you require a rag database and you need a managed ingestion pipeline, Bedrock has also capabilities to allow you to do that. Now, one of the challenges of moving gente systems from a simple chatbot to a multi-agent orchestration that requires a lot of tokens from foundational models is how do you achieve that capacity, so. This year, Bedrock was very heavy at work on helping customers with more capacity and the capability of expand. There are 22 features within the Bedrock uh inference of foundational models that I want to touch base first. The first one is what we call the global cross region inference. The way it works is previous when Bedrock was released 2.5 years ago, you could call a model within a single region. So on this slide here and showing if you have a specific application running on a container or a lambda, you, you could call Bedrock within the region, and that region have a specific limit and capacity for a specific model. What Bedrock now allows you to do is if you enable the global cross-region inference end point, and if you see at the bottom of the screen, this is the inference profile ID that you can pick when building applications, what that allows you to do is if the region that you are located doesn't have capacity or you have achieved the limits of the region for your specific account, it can automatically have a routing. Using the internal backbone of AWS network and ask the question, where is there available capacity that is closer to my user across commercial regions that Bedrock and that model is available. This model, of course, is clot on 4.5. There are multiple models that support a global cross-region inference endpoint. So in that case, Bedrock behind the scenes will route you to a specific region that has capacity. In this slide we've chosen AP Southeast one. Keep in mind that everything behind the scenes is using the AWS network backbone, so nothing is going through the internet. And if you also go and hit the limits of that specific region, Bedrock and then redirect the traffic across any commercial available region that that model is available. So what that allows you to do, of course, increases your resiliency, increases the limits that you can actually call that specific model for your specific application. No. A very common challenge that financial services companies face are you need to be bound at a specific geographic location. Maybe you have some data residency requirements. Maybe you have some regulatory compliance, um, that you need to obey that it can only serve a specific geographic region. Now you can still call just a specific region. And that is fine, but you have the limits of the capacity of bedrock and that model within that specific region. What we introduced as well we call geographic cross region inference. It works very similar to a global cross-reference endpoint. The difference is rather than picking a global endpoint, you choose a specific geographic location, for example, Europe, the US, Asia, and what happens behind the scenes is if the capacity in this case for EU West one. Have been breached from your account and you need more capacity behind the scenes working the same way that I explained on global endpoint we'll find another region within Europe specifically and we redirect the traffic to that specific endpoint for that specific model and of course there are more regions that support Bedrock within. The European regions and that allows you to do that. Now why is that important? If you are within a regulated industry, you can still keep the data within a geographic region while increasing the capacity and the ability for you to serve multiple agents that requires a higher throughput of tokens. So that is definitely something Moody's is using to help them orchestrate it across multiple regions their system. Now, Dennis mentioned multiple times about the challenge of embedding unstructured data into a rag vector database. And part of the Bedrock service ecosystem, we have something called Knowledge Base for Amazon Bedrock. What this allows you to do is to have a fully managed end to end reg workflow. So it just not only gives you the ability to embed documents into a vector database, it focuses on the whole ingestion pipeline for a raw document, a PDF sitting on S3, for parsing the document to creating chunks for the document. Embedding the document and saving a vector database and give you the ability to choose multiple ways you wanna retrieve the document. So it's a completely fully managed solution that allows you to have a peace of mind rather than trying to manage every single component of the stack. You can just rely on AWS with Bedrock knowledge base. So how does that work? If you look at the data ingestion workflow, let's assume you have Different file formats you have text documents, you have PDF documents, you might even have some multimodala documents, maybe some audio files, maybe some video files, and you want that that insight from that data to be by the end of the day in a vectorory store that you can retrieve. So, the way Knowledgebase works is you choose a S3 bucket where you save those files within that S3 bucket. That 3 bucket will, you know, evolve with time. Maybe we're gonna have updated documents, we're gonna have new documents. Knowledge base can keep an eye and update the ingestion pipeline for you. Once it if it realizes there is a new document, there is an updated document, it can do the parsing. There are multiple ways you can choose to do the parsing. You can use the default way for Bedrock knowledge base. You can use a large language model to parse the document, and you can also use what we call Bedrock data automation, which is the last service I'm gonna mention today. Once it does that, you decide what chunking strategy you wanna use. You might wanna use a fixed chunking strategy. You might wanna do your chunking strategy with the lambda function which allows you to do that, and Moody's is actually using the capability. And after that you can choose an embedding model. So the embedding models are available within Bedrock. We have Amazon Titan, Amazon Nova multi-modal, multi-modal, uh, embedding model that we just released about 3 weeks or so ago, and also the coherent embedding models. Now, if you don't want to use some of these models and you wanna use maybe some open source, open weight models, you can also host those embedding models on Sage Maker and point knowledge base into Sage Maker. And finally, because Bedrock wants to provide you with flexibility. You can choose the vector database you want to use behind the scenes. You can use open source servers where you don't need to manage any infrastructure. You can use some partners as well, such as Pinecone, Reddis. If you are into more relation databases and you're very familiar with, uh, Postgrass, you can do PG vector and use either Aurora PG vector or RDS PG vector, and the list is bigger as well. We've just announced yes ready. The general availability of S3 vectors, which is the ability for you to store vectors into S3 with 90% price performance compared to traditional vector databases. Now, That is going to talk to us about how they've taken the very, very fundamental challenge of unstructured data. So you have these PDFs. These PDFs are full of complex financial tables, graphs. How do you extract not only the text from those PDFs, but the insights, the tables in the proper format? We've, we actually, there was a study that was done by Amazon that 80% of the data in financial service is unstructured, right? It could be in maybe 10 you documents, maybe it can be an audio or a video from the financial reporting that the company has, uh, the early reporting that the compa the company published, but only 20% of organizations are actually currently able to take advantage of this instructured data. So We have introduced a year or so ago another capability on Bedrock. It's called Bedrock Data automation. So for those specific challenging documents that you have a lot of different data, unstructured data, how can you actually choose, just put all the unstructured documents into 3, and how can you actually extract the insights from that? Well, you can use Bedrock data automation, and the way Bedrock data automation works are two ways. The first way you can call Bedrock data automation as a traditional API. And you can pass the document. It can be so it supports multimodality, so it supports audio, video, image, and documents. Dennis is gonna go through how they use Bedrock data automation for financial documents like very complex long financial PDFs. So you put the document, you choose the capability of extraction, so you can do summary, you can do text, you can do fields. And we've actually seen a way increase of performance and accuracy when using Bedrock data automation instead of traditional large language model. The hallucination distraction of a foundational model versus Bedrock data automation. You can actually gain an advantage both from price and also accuracy performance with Bedrock data automation. So with that said, I'll pass it back to Dennis when he's gonna go through the architecture deep dive on how they've implemented their gentic system of database. I you that. Alright, now the fun part, uh, let's get technical, uh, so I'm gonna walk you through how we actually built this, um, and the architectural decisions that we've made, uh, and the theory right around how we put the how we put our theory into production reality. Uh, so again, the evolution, right, so, uh, everybody raise your hand here if you've built a rag application. Right, so all of, all of us have started with that. It's the initial deployment that we all do. It's a rag system, one model, one context window, right, trying to solve everything, right? And so users started to ask about credit risk, sector analysis, all of that fun stuff, and this one system had to be an expert in all of it, right? and the cracks appeared immediately, right? So context window limitations, performance degradations, right, and just shallow expertise, right? And so we were really asking the impossible. So Um We kind of made a few series of realizations, right? The first one is financial intelligence, right? That should mirror how we as humans, right, and expert teams actually view some of these problems, right? And it requires specialized expertise. Right? Second, we noticed that our users were asking, now weren't asking just random questions, they were orchestrating repeatable workflows, right? And then 3, it's, it's context switching, right? Just like a human context switching, switching kills performance, right? Even similar expertise benefits from true isolation, and a gigantic systems need that focus context. Right, so the answer we came up with was deep specialists, intelligent coordination, right, and repeatable patterns. So this was the fundamental shift, right? No more prompt engineering, right, but true context engineering. Alright, so before we dive into this, right, uh, raise your hand if you spent hours tweaking a prompt. Yeah, alright, and I bet you a bunch of you also hit a really brick wall and at some point no tweaking was ever going to solve that problem, right? And that's the reality of the systems that we, that we've built, right? We have to move away from pumped engineering, right, to optimize that optimizes little instructions and little and little, uh, parameters here and there, and we pray to the LLM gods and voila, we probably fall short. Right, so here's what we learned, right? The breakthrough wasn't about better prompts, right? It was about better context boundaries, right? These multi genic architectures, these multi-gentic work flows, right, they own precise context boundaries, right, about their specific domain, no cross domain interference, there's no diluted expertise. Alright, so here I'm gonna, uh, give you a little, uh, context into, uh, our, our workflow system. Do I have to just. Nice, uh, so in late 2023, around 2024, right, we're running research assistant and customers loved it, but we started to see, uh, a real specific pattern, right? Users were asking questions like pull credit ratings for these five companies, right? Now do an analysis of their, uh, financial metrics, right? Compare it to news and cross reference it with some sector research. They were forcing chain of thought, right, through our chat. So that's when we realized they don't want smarter chatbots, right? They want an orchestrate they want the orchestration power, right? They want to visually stitch together Moody's expertise, right into repeatable workflows and so to create like really specialized outputs, right, which this means charts, graphs, tables, you name it. So we built this, this is our workflow designer this is before OpenAI's agent builder this is before most orchestration platforms existed, right? We built it because our customers needed it. This was true customer empowerment. And so our system here, right? Has gone from You know, a 20 step workflow to 400 step workflows, right? It will take anything from a couple minutes, right, to 15 plus, right? But what you get is a structured output, right, of our expertise, right, that ourselves and our customers were able to stitch together, right, to solve real world problems, um, and I will say. This is probably one of those systems that if we we realized that we needed to create this isn't about something that our users like came to us and like theorized right? This was us understanding what users wanted why they needed it right? and how we were gonna solve some of their problems. But we're very proud of this. Um, So now the fun stuff, right? So I wanna dive deep into like the architecture, right, and go through this, but first I wanna align on terminology, right? Um, so I'd, I'd be really interested to ask this, right? So how many times have any of you been asked by some executive inside of your company, uh, to say, hey, I want you to ingest all of our documents. I wanted to be able to ask any question you want and I want it to be 100% accurate. 100% and then they will also ask you that they need that probably tomorrow or next month, right? That's just not realistic and they also say, hey, can't you just throw some agents at this now. Uh, I think it's really, really, really important for us to understand, right, the terminology. If we align on the terminology, we can have a lot better of a conversation, right? So here I'm gonna talk through what we in our team have aligned on what our tools, what our agents, and what our work flows, right? So a tool, right, is a system that performs a very specific task. It is a process that returns context back to an LLM, right? Think of it as a discrete, uh uh uh process. So you are fetching data, right, you are doing calculations, right, uh, and it is isolated, right? So. An agent for us right is again an LLM, right? It's autonomously choosing tools it's operating in some form of loop, right, and it determines when it's task is complete. Now, yes, I know that sounds smart, alright, uh, but it's not Skynet, right? It is, uh, it, we all, we all know what it truly is, right, which is a, honestly, it's a it's a 4 loop with kind of better PR. Um, now a work flow, right, that is a deterministic orchestration, right? This is a predefined, right, sequence of steps, right, that we are coordinating tools, right, agents, some something along those lines to get a very consistent output. Right, so, but fundamentally, right, whether, uh, tools can also be agents, agents can also be tools, right, uh, but the, the real reality here is these are just the building blocks, right, that we have built the system on. So Uh, with this, right, I wanna talk through kind of like the five pillars that changed, right, uh, or the five pillars that we've thought through, right, in orchestrating the system, and so the first is we strive to be serverless. Right, uh, the financial world that we live in is incredibly spiky, right? So at any moment, uh, credit risk, uh, uh, a credit change happens and we go from baseline, uh, users on our site to 50X, right? And that is just the way we work. So we built our agentic systems in that kind of same way, right? Truly understanding that we were gonna use the foundations of our serverless architecture, right, uh, to perform the agentic tasks that we also need, right? And so second is we learned that tools, those, those tools that perform like operations, those are the essential building blocks. So we wanted to focus on how do we surface those tools to agentic Systems and to the company as a whole, right? And that we chose to do in lambdas. Right, so the reason for this is because they are single purpose, they are fast, right? Uh, and they are stateless, right? And so this allows us to again have a ton of different, uh, uh, sets of workflows or agents utilizing the same tools and it will be able to scale with us. Um So 3, right, is our agents, right, and we build kind of like two types of agents. There are simple agents, right, and these are, uh, some form of system prompt, right, a set of curated tools, some validation steps, right? We serve those all in some form of a JSON object, right, so that they, we can just run them at will, right, and then they get orchestrated by our orchestrated systems. Right now then we have complex agents, right, and these are honestly it's just custom built software, right? It is again lambda's, uh, I'm sorry, it is uh tools, right? It is, uh, our data sets and it is code, right? and all of that kind of stitched together right in our world. As a as an ECS, why? Because a lot of times those require state, right? They are long running in nature, right? Uh, one of the, one of the caveats to using lambdas always is you have to remember they're not great for a giant amount of compute. They're also not, they have a limit to, I think, 15 minutes or so before they will, they will, they will crap out on us. Uh, so ECS is on the other hand, give us that state full information, right? They allow us to have long running tasks, right, and that is kind of how we push those things forward. And then 4th here is going to be our orchestrator, right, and that is the brains to this whole operation, right? So our, uh, custom built orchestration system, right, will take again in, in JSON format your list of tools, your list of agents, your list of prompts, right, all were orchestrated together, right, in a, in an ECS, but. The complexity there that we've built into our orchestrator is behind the scenes, right? So inside of this we are juggling the act of it being performant, right? It being cost effective, right? So we are parallellyzing as many of these, uh, steps as humanly possible, right, but we're also understanding that, um, you know, you need to have X amount of these steps finished right before the next step is is allowed to go, and we kind of, uh, we, we do all the magic there. Now the concerted effort here has been to. Uh, save costs, right, to handle our errors, um, any time we get throttled by, by our LLM providers, right, that's the real power in our orchestrator and so our, our, our lead engineer when he created this, uh, he set a, a max limit to about 20 or so steps, right, because none of us thought in any way, shape or form that you would need more than 20 tools or 20 steps to ever solve a problem. Um, now some of our uh customer work flows are over 400 steps. Right, and so that really blows your mind about understanding like what people are trying to do inside of our systems. And last is, is, uh, last kind of like architectural decision that we needed to make was, you know what LLMs to choose, and we made a concerted effort not to be pigeonholed into a specific model right for our, our systems so every agent, every. Step every tool has the ability to choose a very specific LLM for its needs, right? and so these are things that we can then test, validate, right, and, uh, and, and push forward a small isolated systems. So one of our uh agents could use uh a reasoning model and another one could use even a small language model, right, for small computational pieces. Right, so. Um, let's take a step back, right, and we can talk through the numbers, right? So we have about 80 or so tools, uh, we have 100+ workflows, right? We have many specialized task agents themselves, and we're processing over a million tokens a day, right? And I just want you to know that this isn't a demo, this is in production today, right, satisfying our customers' needs. All right. So I wanna take a moment here though and talk about uh something that is truly near and dear to all of our hearts, right? So we spent some time talking about the sophistication around our orchestration systems, right, our work flows, our agents, right? But none of that works right if you do not have the context right to give our agents, right? So. This is our fundamental PDF processing and retrieval challenge. Now in our industry like Sam said, everything is a PDF, right? 10 queues have hundreds of pages, annual reports again are hundreds of pages, uh, earnings reports as well. Uh, there are regulatory filings, right? And all of this is a challenge, right? And LLMs are brilliant at reasoning, right, but without the right context, what is it actually going to do? So we actually call this like an archaeological uh dig problem, right? You are trying to excavate layers of buried information across hundreds of different pages, right? A table header can be on one page. Page right, but the actual table can span 2-3 pages afterwards, right? There are charts, there are images, there are infographics, there's text all over the place, there are footnotes that aren't always attached to what you think they should be attached to, right? And you know that was. Kind of a fundamental issue. All of these PDFs are always the layouts are different, right? And so they needed a very unique way of trying to solve all of our problems and the reality is in the financial world, right, one decimal, right, that's off could represent a catastrophic right change to a customer and what they need. Right, so I think one of the biggest bottlenecks in our financial AI right, continues to be, right, this, the complexity around trying to deal with this unstructured data. So How do we tackle this, right? And I wanna say like a like a good engineering team right? we tried everything that seemed reasonable, some things that seemed unreasonable, uh, we over engineered a bunch of things and we also under engineered. A bunch of things. So I'm gonna go through all the ways that we failed, right? And this is how we can be an honest engineering team together because I think in noticing what, where we have failed, hopefully that can help you as you are, are reaching for, for your solutions, right? So first, like a lot of us, right, we started with basic Python libraries, right? It goes, it extracts a bunch of the text, right, uh, and it does it pretty decently well and fast and cheap. Uh, but the problem there is you lose all context, right? I think about it as throwing a 200 page, uh, document into a blender, right? You're gonna get all the parts back out, but it might not have all the context that you truly need, right? So, honestly, the verdict here was it failed. It did not provide us the type of context needed to solve financial problems, right? So verdict number 2, right, custom parsing algorithm, and this is where we realized that we wanted to understand the true hierarchy, right, of a PDF document, right? So we would go through, we would try to do some bounding boxes around certain sections so that we could group some of these things together. The screenshot that you're seeing, uh, on the right is what that looks like, right? And. And that's complex, right? And it is a little bit of a of a of a crapshoot, right? It's not always gonna be able to work it's not always going to be able to scale with the different types of documents that get thrown at it, so the reality there, it also failed, right? It wasn't able to scale for us. Right? And then approach number 3, right, was like our multi-modal foundational models, right? And that makes sense, right? We now have vision models. We should be able to look at these PDFs, right? Just like as a human would and LLM should be able to do the same thing. And honestly it did pretty well, right? Um, but we definitely found that certain complex tables, right, definitely certain complex layouts, right, it would just struggle, right? and it wasn't able to be as accurate as we wanted and the reality. That was very, very costly, right? So thinking about how to do that at scale was gonna be very, very difficult, right? So the verdict here as well failed, it's gonna be too pricey, right, and it just got just enough things wrong to make it not usable in production. And last, right, so this is our 1 million context windows. Once those babies came out we were like, ah look, this is gonna solve all of our problems, right? We can just throw this entire giant, uh, documents in there and it will be able to understand them all. And also did really, really, really well, right? Uh, of course though, the problem is here larger the documents, the more you stuff into a context window, the more, right, you are going to start to degrade what comes out and the reality here too is this is expensive, right? This is incredibly hard to scale. Right? So here we have it, 4 approaches, all of them failed, right, to reach the kind of requirements that we need in the financial, uh, industry, but the reality is we learned so much from this, right? And we weren't going to just stop. Our, our clients need this, need this solved, right? So we needed to learn from our, our, our failures. So That led us here, right? This is the concept of not trying to find one solution to solve our problem, right, but we would start building pipelines that could route different content types to specialized processes, processors, right? So the insights were this. Right, not all pages are created equal, right? A text heavy narrative page needs far different processing than a page dominated by complex tables, right, or charts or graphs, right? So we built an intelligent, uh, page classification system, right? It's an upfront analysis step and it could categorize a page, uh, that is dominant, uh that is text heavy dom dominant right and that would go to a bedrock LLM that could do OCR for us right and that would then convert that into markdown, right, and we have something that is a lot easier to query. Now for table uh dominated pages this is where AWS's uh bedrock data automation came in and BDA became absolutely essential, right? BDA is purpose built for these kind of complex table extractions and honestly it was a game changer. From us traditional table extractors just couldn't handle the complexity around these, uh, these, uh, financial documents. The reality is even myself, I look at some of these tables and I'm even confused, right? So cheers to anything that can kind of look at those, uh, uh, and, and, and handle them, right? So. Last, there is gonna be our charts and our images, right? And so those then we would go into a vision model, right? A vision model is able to look at those, right, create some kind of metadata for it and create something that allows us to uh query them in some form of vector database. So this multi-modal approach is what finally kind of unlocked the fact that we have a scalable PDF processing system, right, so different modalities, different tools, intelligent routing, right, that's kind of the architecture that solves some of this. But here's the thing, this is like maybe half the battle, maybe even less than half the battle, right? The real battle comes in how do you get this information then back out of our systems, right? So. This is where we went into a kind of different approach to get our stuff back out, right? Um, this is about moving away from a traditional keyword search from semantic search right? because that all fails to understand the complexity around these documents, right? And here's where like traditional single shot vector search, right, which then just returns the top K uh chunks, right, or combining that with some form of uh of keyword search you run a re-ranker, right? You get that information into an LLM and you're like ha, I'm done this works, uh, and then you realize it doesn't. Uh, so what we needed to do, right, is we found that there were very specific types of questions, right, that people were asking us where the information is just scattered right across the, the, the, the, these documents, right? Think of a use case where somebody's trying to find in an annual report, um, the business units of the, of the, of the entity, right, uh, cross reference that with the revenue that it may have, right, and then also analyze maybe its sector. Information, right, so this kind of search query right on a document is incredibly complex business units, right, that exists in one table probably stretched around a couple of different pages, right? The revenue and other kind of financial metrics are in another table. Maybe some of that information is in a footnote for some table, right? And so how do you, how are you able to pull all of that stuff together in a single shot search? It's just not gonna happen, right? So this is where we went into agentic retrieval, right? And so we built agentic retrieval, right, to be, to look at a document like a human would look at it. Right, so in agentic retrieval, right, we get the user's query, right, we create a plan off of that, right? So this means decompose, uh, the, the, the queries, right? Create a search strategy, we then execute that, uh, strategy that could be multiple different searches across it. Right, then, uh, it will reflect, right? Did it actually answer that question? Did it, was it able to pull down that information if not continue the loop, right, and then finalize this by providing the individual chunks, right, with its proper citations, right? And so this is truly intelligent document navigation, it's not just search, right. And so to take a step back right into the kind of conversation that we're having today about this multi-genic system is that this now becomes one of the tools right inside of our tool belt, right? This is a tool that now any workflow, any agent is is able to utilize, right, so that it can pull the right information at the right time, right, to solve its needs. So this is a whole kind of like brings together our complete solution, right? We have a bunch of different tools we have specific task agents right that work inside of their known domains right orchestrated across a really complex orchestration system right all there right to solve a specific uh customer need. Now We need to talk about what's next, right? And so we're evolving, right, uh, at all times, right? And our customers realize that they need our intelligence, right, but they want to not only just consume with it, they wanna build with it, right? And this led us into MCPs, right, and our smart APIs and a lot of the tools that we are built and we utilize inside of our orchestration systems, right, we are now able to uh, expose to our customers directly now. I think it, it's work we were incredibly fast to the market, right, and we have been custom building a lot of our solutions because there just weren't solutions available to us, uh, already. So I think one of the next steps that we have to take internally is really figure out the like buy versus build. Right, and honestly, after being on in in reinvent for the last few days we have realized that a lot of the things that they that have come out right in Bedrock in aging core are all going to be things that we are going to be able to replace a lot of our custom code with, right? And it's gonna be able to solve our problems and all of these utilizing of our managed ser of these managed services is gonna reduce a little bit of our own tech debt, so. Speaking of the of the of the plat of the of your primitives, right, Sam's gonna go through agent core right and teach you how AWS is trying to build this, right, so that you guys can also have production ready things at scale. Thank you, Dennis. So, I'd expect the majority of you have heard a true reinventor before about Agent Core. This is not gonna be a deep dive. This is gonna be a very high level uh introduction of Agent Core. So Agent Core is an addition of our bedrock ecosystem that gives you primitives to build production grade agents at scale on AWS. There are 8 primitives. We just announced one, a new one yesterday that is not here because it was an embargo and I couldn't add it to this slide, um, but some of these primitives that I strongly believe, uh, Moody's will be able to actually use and benefit from that. Are the runtime. So we start with the runtime, which is the ability for you to have a surless compute environment where you can run agents with session isolation at the VM level with a pay as you go, uh, surless, uh, scenario, right? It supports any framework. The Moody's have built their own framework so they can bring their own framework. They can call whatever model they are using for specialized agents at any point. The whole idea of Agent Core is to give flexibility and options for users. You can pick and choose any of these primitives at any point and combine with your own solution. So runtime is definitely a big candidate for helping with the compute power. But then, then as mentioned, they currently have 80 tools just for that specific multi-agenttic system, and I could imagine that tool list will keep increasing. We have released Agent Core Gateway which allows you to create a virtual remote server that has a complete managed service solution for you to manage your tools that maybe you can call directly APIs. You can have lambda behind the scenes to add your own logic. You can even actually call behind the scenes your own existing servers in a centralized platform. Now, everything that you do, especially on a such a regulated industry as financial services, you wanna make sure you have proper identity, authentication and authorization, right? And you separate, when you think about an agent, you can think about two types of authorization, right? You can think about inbound authorization, meaning who is the user accessing the agent and what are the permissions that that user have in order to call that agent. But then you know agent assistance brings a new challenge into authentication authorization. What are the tools and what tools and what type of calls those agents can do on behalf of the user, and how can you do the metadata, the federation propagation from the user identity into what we call the outbound authentication? So we have introduced agent core identity that can help you with all this complexity of managing the authentication and authorization. Great news about agent core identity, you can integrate with your existing IDPs. So if you're using Cognito, you're using Octa, or any custom provider that supports a lot, you can just connect your agent core identity, and agent core identity will actually call on your behalf those providers to validate the token and validate the credentials of your user. So we're not trying to replace your identity providers. We're just trying to give you flexibility to support both inbound and outbound authentication. And of course everything you do here you want to make sure you have observability. How do you make sure you know what the agent is doing, if you need to troubleshoot, or you want to make sure you have regulatory, you know, compliance, you're saving every single step that you know Dennis showed us a very complex workflow that had. Dozens and dozens of steps. How do you actually collect every single step that every single agent took behind the scenes, every single large language model call, every single tool call, every single reasoning chain of thought prompt with agent core observability, you can either use runtime on Agent Core gateway or you can even use outside Agent core. You can extract all that information and ship to cloudWatch or a third party observability provider of your preference using open telemetry. And you have all the capability there. And then finally, which I think is a very important one, is agent core memory. Let's say another bank using Moody's Agenec AI systems. And I, I would like the system to know that you know I am from this bank and I am a maybe part of the investment bank and I have these stocks on my portfolio. Wouldn't it be awesome if the system could actually collect that data as I'm interacting with the agent and save for long term purposes with agent core memory, you have this fully managed solution that you can collect both short term memory, which is just a user assistant interaction. And automatically extract insights for you, preferences, summarizations, or even your own custom semantic information. If you're interested in Agent Core, there are a lot of sessions available in Reinvent. Highly recommend you checking it out. But that will, that is pretty much it for our session. Just wanna say thank you so much for joining us. And the last thing, please rate us on the AWS events app. Our session ID is IND 3303, and thank you so much, everyone.