---
video_id: GxZ2c4XsXOw
video_url: https://www.youtube.com/watch?v=GxZ2c4XsXOw
is_generated: False
is_translatable: True
summary: "Alex Livingstone (AWS) and Jared Nance (AWS CloudWatch Principal Engineer) present a blueprint for implementing observability at scale, addressing the challenges of monitoring modern distributed systems with thousands of microservices and accounts. Alex begins by highlighting the high cost of downtime (averaging $300,000 to $1-5 million per hour for enterprises) to justify investment in observability. He encourages a shift in mindset from tracking low-level infrastructure metrics (CPU, memory) to **business outcome metrics** (e.g., \"Orders Per Minute\"), which directly reflect customer experience.\n\nJared Nance explains how Amazon uses CloudWatch internally to monitor its own massive scale (processing 20 quadrillion metric observations/month). He details Amazon's \"two-pizza team\" structure and how they use standardized blueprints. These blueprints come pre-wired with infrastructure-as-code templates that automatically configure execution logging, active tracing, and standard metrics. He emphasizes the importance of co-locating metrics, labels, and event data to handle high cardinality without exploding costs or hitting limits. Tools like **Contributor Insights** allow Amazon to identify \"top N\" contributors (e.g., a specific customer causing a spike in latency) in real-time without needing to index every single dimension as a metric.\n\nAlex then showcases a suite of CloudWatch features designed to solve enterprise observability problems:\n1.  **Centralized Logging:** A new feature allowing logs from multiple accounts and regions to be centrally collected and queried in one place.\n2.  **Metric Insights & Multi-Resource Alarms:** Instead of creating an alarm for every single container, you can now create one alarm that watches all resources (e.g., \"alert if any container > 80% CPU\") using SQL-like queries.\n3.  **Transaction Search:** Enables 100% tracing without sampling gaps, allowing users to find specific \"needle in the haystack\" traces by business attributes like customer ID.\n4.  **Application Signals:** Provides auto-instrumentation (agent-based) for Python, Java, .NET, and Node.js applications to generate \"Golden Signals\" (latency, errors, traffic) and service maps with zero code changes.\n\nThe talk concludes with a discussion on adding intelligence to observability. This includes **Anomaly Detection** (for metrics, logs, and traces) that automatically identifies outliers and pattern changes. They demonstrate **CloudWatch Investigations**, an AI-powered tool that correlates alarms with metrics, logs, and traces to provide natural language root cause analysis (e.g., identifying that a function duration spike was caused by throttling from Bedrock). Alex also mentions the integration of **Model Context Protocol (MCP)** servers, allowing developers to query their observability data using natural language directly from their IDEs."\nkeywords: Observability, Amazon CloudWatch, Distributed Systems, Application Signals, Anomaly Detection, Centralized Logging, Contributor Insights, Business Outcome Metrics, AI Ops, High Cardinality\n---

OK, hi everyone. Um, my name's Alex Livingstone. I'm a principal specialist solutions architect specializing in cloud operations and particularly observability. I've been doing kind of operation stuff for about 25 years. I feel old now. Uh, and I've been doing this, this stuff in AWS for about 9 years. So I want to start with like the reality of um enterprise observability today. It's a lot different from traditional monitoring. You might have to be worrying about having hundreds or even maybe thousands of accounts across multiple regions, thousands, maybe tens of thousands, in some cases, maybe even hundreds of thousands of microservices or services. And for some of you, you might have petabytes of of telemetry daily. I mean, not everyone's gonna have that. And what we're gonna talk about today is actually it's best practices generally, so it doesn't, doesn't necessarily have to be at huge scale. And some of the challenges that this leads to generally when I speak to customers is they have lots of lots of different tooling. You might have logs in one tool, tracers in another tool, metrics in another tool. And this becomes challenging. Also, it can become very costly. And it can lead to alert fatigue. How many of you have alert fatigue as a problem? Yeah, quite a few of you. And obviously that can lead to missed alerts and and not spotting critical issues. So these are the daily realities probably for a lot of you and for enterprise teams in general. And when, when your observability doesn't scale, you see these kind of problems. Increased mean time to resolution. As I said, alert noise and fatigue. Data silos, and this might not just be actual data silos, but you might also have team silos as well and that that can make things more difficult and if your data is siloed, that kind of reinforces the um your teams being siloed as well. And then if you've got multiple tools, then you have to do manual correlation across those tools. Um, and I see this with customers a lot, you have if your metrics are in one tool and your logs are in another, then you have to be looking at one tab for your metrics, another for your logs, you have to look at the time and across these different tabs and switch back and forth, and that can be a bit of a nightmare. And then you also have inconsistent coverage if you're using different tools, different teams are using different tools, different standards, uh, and that makes it even more difficult. Now I'm gonna come onto the cost. Before I click next on my uh on my clicker, how many of you here know exactly the dollar amount per hour for downtime for your application or for your organization? Anyone? OK So for 90% of enterprises, it's $300,000 an hour. Now I'm not saying this to scare you. Um, and it's like it's 1 to $5 million per hour for 41% of enterprises. So yeah, this is not to scare you, this is, this is to help you, um, to justify why observability is important to your business, your company, so you can justify why you should be investing in observability. Because if it comes at that cost, when you have downtime, the, the relatively, it's not tiny, but the relatively small amount of money you can invest into observability to stop this downtime happening, the return on investment, I mean, it should be, it should be really clear. Obviously, as well as, as well as just the cost, you have the uh the lost time from engineers. Customers experience problems that can lead to reputational loss as well, beyond just the actual cost of downtime. And this can also lead to delayed features. There are other repercussions as well, but delayed feature delivery that might. That might cause problems with revenue that you're not yet getting. OK, So before we go into the technical solutions, um, I'm gonna start off by, I'm trying to get a, try and get you to, for some of you anyway, this might not be all of you, to fundamentally think differently about the way you do monitoring. So this, this section is just about the way you think about metrics. And traditionally we kind of, when we look at monitoring, we look at infrastructure metrics. So here we can see things like compute, network, containers, storage, and then obviously we vend metrics for nearly all of the AWS services. And then on top of that you might have your application metrics, so performance, API calls, um IOPs, uh you might even have metrics on individual features. And then golden signals, so latency, traffic, errors, and saturation. I'm sure most of you have probably heard about these. So Let me think how to phrase this, how many of you kind of stop at that top layer? Do do any of you think about metrics that may be above that layer? Yeah, a couple of, yeah, a few hands. And this is what I want you to think about. Business outcomes. So business outcome metrics are really, really important, and I want you to think about this, um. And there's, there's two fundamental reasons why, which I'll go into in a minute. But think about. No Think about what your customers care about, they don't care about the CPU on your EC2 instance. They don't care about the utilization of your containers. They don't care about the um. Uh How much, uh, how much storage or CPU or memory or lambda functions are using? They care about things that matter to them, and you should be caring about the things that matter to your customers. So if you go onto Amazon.com for example, our most important metric, as you can guess, it's not gonna be CPU utilization. It's not even gonna be some performance related metric. It's not even latency. These things are all important. But the most important metric is orders per minute. Because the idea of you coming onto Amazon.com is you want to buy something. If you can't buy something, there's a problem. And the reason you should measure these business outcomes, there's 2 major reasons. One is, if you measure for business outcomes, if there's not a technical metric telling you you've got an issue, the business outcome metric will definitely tell you you've got an issue. The other reason to do it is if there is a technical metric that tells you you've got an issue. How do you know what the business impact is? A lot of the time, maybe you've experienced this issue before, you can take a rough estimate of what it might be, you might have a good idea of what it might be. But the only way to be sure is if you're measuring business outcomes. So if you take, I hope you take more than just this away, but if you take one thing away from this, I'd really like you to go away and think about. The business outcome metrics for your applications, what matters to your customers, measure those. The other advantage it gives you as well, is your managers will love it if they can see these business outcome metrics in near real time, rather than waiting for BI tools and getting weekly and monthly reports. So We're gonna go through like a blueprint for for how you can implement observability at at this kind of enterprise scale. We'll talk about centralized logging. How to collect metrics. We'll talk about tracing. Then we'll talk about adding intelligence to that. So we'll talk about anomaly detection. Jared will talk about um how to do how to look at high cardinality metrics. And then also correlation and how to do that automatically and not have to, you know, mess around between tabs. And then to add onto that, we'll talk about cloudwatch investigations, um, application signals for uh APM. And then some specialized insights integration which I will get into later. So now I'm gonna uh hand over to Jared and he's gonna talk about how we do this at Amazon. Hey folks, uh, my name is Jared Nance, and I'm a principal engineer at Cloud Watch. Um, I'm gonna talk a bit about how we use CloudWatch internally to monitor our own services, but to set some context, I want to take a moment to, uh, help you understand the scale of CloudWatch. So every month we're processing, uh, 20 quadrillion metric observations, 13 exabytes of log data, 455 billion traces, 800, and we're running 861 million canaries every month. Why is this relevant? Well, it's, it gives you some insights into the scale of our own services, and we actually use CloudWatch to monitor these services. At Amazon we've we've largely standardized on CloudWatch for new workloads. Uh, there are of course some older systems that, uh, don't use CloudWatch, but, uh, from the early days, but, uh, most of the new ones all depend on, on CloudWatch and, and we use this across Amazon, uh, and the way that we approach monitoring and observability really starts with how we organize our teams and our services. So as I'm sure a lot of you know, Amazon follows uh a DevOps model where we partition system complexity. And into services, and those services are operated by what we call two pizza teams. So let's uh let's start by looking at what happens when we create a new service. We have a set of blueprints that we use for common architectures and a team can just create an instance of a blueprint with everything already set up. So consider an API workload running on Amazon API gateway with a lambda function uh integration that might look like this. How do we ensure that this system is set up for success with CloudWatch? Well, for all of those vended services, the infrastructure is code templates that we use to provision them already configure things like execution logging from our Amazon API gateways and active tracing from the gateway and the lambda functions, and of course you get all of those vended metrics for free from the services. And if you're running on AWS Lambda, it's automatically integrated with uh CloudWatch logs. But what about your custom telemetry? And when I, when I talk about custom telemetry, what I'm, what I'm really referring to is the workload or business specific instrumentation that you need from your services. All of the data that we need to know what this workload is doing, what it's interacting with, how many records it processed in a batch, these are all, this is all custom data that we can add into our, our telemetry. So whenever we create these blueprints, they come prewired with instrumentation frameworks, so a basic lambda handler like this would also come with an instrumentation framework that's preconfigured the way we need it. Whenever we create an instrument within this function, uh, it interoperates with all the other frameworks we're using like our caching frameworks or our web frameworks and so here when we when we create the instrument, it'll automatically add things like the identity that's calling the function it'll link trace IDs into our logs, and these are all things that as a developer I don't have to think about. Then we can go in and we can add our custom metrics and the labels that we want to include in our telemetry. We then pass the instrument factory to child operations so they can continue further instrumentation, carrying all of the context as it goes. And interoperability here is super important. In Amazon we've standardized on our instrumentation frameworks a long time ago, but you can use tools like open telemetry to get this kind of interoperability so that the frameworks that you use, whether they're caching frameworks or web frameworks, are all emitting telemetry in a consistent way and carrying context as you're adding information into those into your telemetry. When we emit telemetry, we co-locate metric metrics and labels and event data that might look like this. This allows us to store high cardinality context in the spans while using metrics for other things like alarms and dashboards. It also enables us to quickly answer questions like which requests failed or which customers are impacted by running logs insights, queries and contributor insights. One of the most challenging things in observability today is dealing with high carnality data. Uh Today some of those challenges with traditional monitoring systems include that you may be emitting millions of unique metrics which may hit limits on some of the existing systems. The insights are also buried in that Cardinality explosion. So if you consider a service that just wants to emit a metric latency and it has dimensions like customers, API end points, and regions, if I have 1000 customers and 50 API endpoints in 10 regions, that one metric now explodes to 500,000 unique metric dimensions. So Contributor Insights takes a different approach. It uses an automatic top end analysis where we take those structured logs and we identify top contributors for the metrics in your data. It allows us to do real-time contributor ranking, and it's cost effective as it doesn't result in metric dimensionality and cardinality explosion. We offer a structured format for the data called embedded metric format which allows this to happen automatically so when you emit those that span data, you have the structured data in your cloud watch logs and you have the metrics that you want extracted in your metrics all through a single event. And we also have alarm integrations so you can actually alarm when you have a single contributor that is breaching some threshold for some metric. So this allows you to identify when just one of your customers is having a bad day. So this is a kind of graph that you might get if you're using contributor insights internally we use this for all of our critical metrics. We've had cases where we see a small drop in availability and we realize that it's all caused by a single customer and we've been able to work with those customers to identify issues that they may have introduced through deployments that may be impacting them. This is we actually use contributor insights to power the personal health dashboards so that in a short amount of time we can identify during a large scale event which customers are impacted and quickly notify you and this is the tool that we use to do that. So contributor Insights allows us to go from an aggregated metric and identify the top contributors for that metric. But what if I want to go from an aggregate metric to a very, very small slice? Maybe I have one error, one request that's failing. Maybe I want to see the full trace for that one aggregated metric. So suppose I have a graph for service availability that looks like this. This is an aggregated metric, and each sample in this data may contain many different actual requests, and I just want to get one of those requests to understand what's happening. Because we're co-locating our metrics and our structured log data in the same events, we're able to run a logs insights query that looks like this, where I am first isolating the data by the API and filtering it to just the errord requests, and now I can get things like the trace ID and the exception message. But what about cross account and cross region at AWS we partition our workloads across, uh, accounts for region isolation. Uh, so whenever I deploy a service into a new, uh, into a new region it's entirely isolated into a different account. But how do we, how do we get visibility across all of these, right? When, whenever an operator gets paged in, they know exactly which region and therefore they know which account to go into. If an operator group owns multiple service accounts, they can create a central monitoring account that aggregates the cloud watch data across accounts and regions. Uh, so now let's talk a little bit about incident detection and response and how do we discover that there are actually issues that may be impacting our customers. Whenever we create those blueprints, they also come with predefined dashboards that we deploy through infrastructures code into CloudWatch. This gives us and every week the service team will go through and they'll review all of our customer experience metrics and ask questions about what's driving this increase in latency, and this gives us a regular cadence to deep dive into issues and ask questions about whether we're monitoring the right things, what kinds of regressions are we seeing in our services, and identify things that may need automated detection that we actually haven't configured yet. But since we don't sit around staring at dashboards all day, we use alarms to notify us when something goes wrong. We don't want every single alarm triggering tickets and paging operators because large events may actually trigger multiple issues, so we use composite alarms to actually interface with our teams. So whenever a child alarm goes off, it will trigger a composite alarm, and that composite alarm will create a ticket in our incident management system. When that ticket gets cut, we automatically trigger a cloud watch investigation. So the cloud watch investigation will, will initiate an investigation that's AI driven. It will look to identify the root cause and it will hopefully do that before the operator has even logged on to investigate the issue. We embed a link back to the investigation into the ticket so that when the operator logs on, they go directly into the ticket and then they're able to federate directly into the account that's having the issue, review the AI summary, and if needed, do further manual investigation. So with that, I'm going to hand it back to Alex to talk about these features in more detail as well as a few others. Thanks, Jared. OK, the first thing I wanna talk about is, uh, and these are all things that are gonna help you. uh I'm gonna talk about a bunch of things here that are gonna help you do this thing, um, at scale. Uh, the first thing I'm gonna talk about is centralized logging. Now we introduced a, uh, a new feature. A couple of months ago and it's filled a big gap because we've had the ability to centralize all of your monitoring, so your metrics and your traces cross account and cross region in either a single monitoring account or multiple monitoring accounts for quite a while now, and the thing that was missing was the ability to do this with logs, and it's a thing that customers have asked for for a long time, and you can now do this. So if you weren't aware, we released this a couple of months ago. And um you now have uh the ability to have a free copy of your logs in one central location, so one account, one region. And that means that you can then query all of your logs from one place. So I think this is a really big deal, this is really exciting cos this now this now allows you to have Cloudwatch as your central destination for metrics, logs and traces. And before this it it was a bit problematic with logs. So this is multi-account and multi-region. And as I said, this now allows you to do these unified log log insights queries across accounts and across regions. And it also means you can have centralized retention policies um and to manage your costs as well. Another thing that's difficult to do at scale is uh creating, creating alarms. And this is uh something else that was released just a few months ago. And the, well, let me talk about the challenge first. So before this, um, you'd have to create an alarm per resource. So imagine you've got, um, like, hundreds of thousands of containers, and you want to set up an alarm for each container. And even if you do this as infrastructure as code, it's still a bit of a pain, and you'd have to, you'd have to create. An alarm per resource. And that would also lead to having inconsistent thresholds, so different teams would set different thresholds. And obviously it creates alarm alarm sprawl. You can end up with 10s of thousands, 100s of thousands, maybe even millions of alarms. And there's no centralized management. And then when you're looking at threshold tuning, it's reactive and different teams are doing different things with their, with their thresholds. So this kind of creates a nightmare scenario where you have thousands of alarms, but. Like no confidence that they're actually measuring what matters. So with Metric Insights, we've always had this uh SQL-like queries for your cloudwatch metrics. But the two things we've added recently, one is tag-based filtering for vended metrics. So this means you can use tags in the query. And the biggest thing I think is uh multi resource alarms. So what this allows you to do is say you've got 100,000 containers and you want to say uh alert me if the CPU on any of those containers goes above 80%. You can now, you don't have to create 100,000 alarms, you can create one alarm to do that. And this obviously gives you this central control. And it also allows you to have a, a better view across your entire state to look at uh trend analysis. So this transforms how we can do alarm management. and manage all of these individual resources. And this is what it looks like. Um, so, uh, here we just have, uh, in fact it's the example I said. So I'm looking at the maximum memory utilization across, potentially across every single container that I have. um. In my account I don't have that many, so there's what, about 20 or so there. um, but I, I'm able to immediately just do one query and I can set an alarm on all of those containers. How many of you here are doing tracing? OK, quite a few, maybe about 1/3, 13 to 1/2 maybe. Uh, and how many of you are having to sample your traces for cost? Yeah, quite a few. Um, so with transaction search. Um, It solves the problem of sampling, so you can capture 100% of your traces rather than sampling 1 to 5% maybe. This means you can do, it also allows you to do. Like real-time searching of those traces, so we can go through millions of um uh millions of uh traces in seconds. You can add custom attributes to your traces and they'll be indexed, optionally indexed. Um, and this allows you to have this, this correlation without gaps. Now, if you. If you do sampling, there's a problem. Now, even if you do tail sampling, which is, let's say I want to, uh, I want to set up tail sampling, which I can do, and say I'll only uh I'll have 100% of my errors and I'll sample 5% maybe of my successful requests. Well, even that doesn't work because you might have a successful request that you want to go and have a look at, because maybe actually technically it was successful, but actually there was a problem and you need to go and look at it. And the problem is with sampling. Is You either get an aggregation of what everything looks like, or you can go and look at individual traces and see what some people had issues with maybe. Um, but if you've got a particular issue and it's not in your sample, then I was gonna swear then, then you're um. Yeah, you're, uh, I can't think of a, I can't think of a good word for it. Uh, you don't have the chance to go and do that. So what transaction search allows you to do is the ability to have every, uh, every single trace, query across millions of these traces in seconds. You can filter by the, uh, business context. So these are things maybe you'll have a customer ID, you may have a, a session ID, things like that, even feature flags. So you can add those into your uh into your tracing. And they correlate with logs and metrics automatically. You can also export this trace data for things like machine learning analysis. And you can do this search on up to 10,000 accounts. So that's gonna work for most scenarios I think. Uh, and this is what it looks like. So here I've just done a search on all of my traces and just grouped them by status code. So I've got accounts and uh by status code. And then you'll see there's a button there called summarize results, and that just uses AI to summarize the results of my query. You can do this in login sights as well as uh in transaction search. Um, and then it's given me a summary telling me. Uh, a bit about how, which status codes I've got and how many I've got. So with anomaly detection, we have anomaly detection on all three pillars, so metrics, logs and traces. With metrics, we get baselines that take into account seasonality. It uses the last 14 days' worth of data. We continuously adjust the model, so if we create a new model that's better, we'll replace the model. If it's not better, we'll keep the old model. It supports custom metrics and vendor metrics. And what it does is it just identifies outliers. And this is really useful if you've got um if you've got metrics that are repeatable, so you know you have a busy pattern during the day and maybe it's quiet at night. Or you have a metric that keeps on rising or one that keeps on going down. Or maybe you're going into production for the first time and you have no idea what the baseline for that metric is gonna be. So there are kind of 4 use cases for using anomaly detection for metrics. And then we've also got anomaly detection for logs. This is built on um something we introduced, I think it was last year, which was uh pattern detection in logs. So we've built on that pattern detection in logs. And, um, and you can automatically surface anomalies in logs, just turning it on for a log group. And there's 5 different, uh, anomaly types. Um, one is the frequency, is this, well, I suppose two of the frequency is, is this happening more often, or is this happening less often? Or you might have a new pattern that's emerged, or a pattern that's just disappeared entirely. And you can run this continuously with log anomaly detection for your log group, or you can run it as a log insights query. Uh, another thing you can take away, if you just take away 11 log insights query, uh, unfortunately I don't have it on here, um, but if you do pattern, app message, pipe, anomaly. You will see It will, it will basically uh give you a summary of all of your anomalies in your chosen log groups, and you can choose all of the log groups. So if you've centralized all of your logs into one account in one region, you could go into login sites, write that pattern app message, pipe anomaly, and it would tell you all all your anomalies in all of your logs in that time period. Really, really useful. And then we've got traces. So For traces, it integrates with uh X-ray analytics and it looks at things like latency and error rates and dependencies on other services as well. And I'll show you what these look like. So this is A typical metric that's kind of quite steady and um uh and repeatable. And you can see here everything is kind of fitting within, within that gray band. We can adjust the size of that gray band. And then you can alert on anything that goes either above or below, or just above or just below that band. And this is um. This is log anomaly detection. Um, and here it's detecting, uh, an error that I've not previously had, or, or an increase. No, it's, uh, one I've not previously had, I think. Um, And you'll see here that when we detect patterns in these logs, what we're basically doing is looking at. And the patterns and taking out the variables, and we call these tokens, we've got token values here, and I've chosen to look at these token values because they're showing me the trace ID. So what I can do with this is I can look at this log anomaly and go, oh OK, I wanna investigate this a bit more, and I can actually just look at those trace IDs and then go and have a look at those traces and see what, see some more about what's happened. And because traces and log events are correlated when you look at the trace view. If you looked at the trace view of that of uh of any of those traces, you'd see every single log event correlated with that trace as well. And this is what anomaly look anomaly detection looks like in traces, so this is in the X-ray console. Um, and this gives a bit of, uh, added value as well. It, uh, it tries to give you the, um, uh, a description and the, the root cause of the issue as well. Well, the root cause service. And you can drill down a bit further into the, into this, and it gives you more information. OK, so. Application signals, so I've said more insights, less work, so you have to do a bit of work here but not very much. So before application signals. Um, and before open telemetry, I guess, you'd have to do, uh, manual code instrumentation. Um, You'd have to, uh, so your developers would have to add, add code to, or, or you if your developers have to add code to your applications. You'd have to do manual updates every release. You have inconsistencies in data collection, which Jarrod talked about earlier. And, and there was selective coverage as well in, in what libraries were covered. But now with application signals. For at least for Python, Java.NET, no JS. You can have automatic instrumentation. And this uses open telemetry, right, so this is using open standards, we're using open telemetry to do the auto instrumentation, uh and then we're sending the tracers to X-ray and the metrics to Cloudwatch. And it's an agent-based deployment, there's no code, it's just drop in, uh, and plug and play, so it's just configuration. This is particularly easy to do in EKS because you just add an observability add-on to EKS. And then you just turn on application signals and that's it. There's a little bit more work to do when you do it in ECS or EC2, but it's basically just uh deploying an agent and adding some configuration. Uh, and it's also built into lambda as well as an option. And obviously this gives you much faster implementation. There's like hardly any developer effort at all. It's easy easy to maintain, we, we maintain the uh agent for you. Um, in the EKS we we can update the agent for you as well. You get standardized telemetry across all of your applications, and this is what gives you that that full stack visibility without, without any effort. Now I would say there is one, there is one exception where you might not want to use this. If you've got very, very um latency sensitive applications like high frequency trading or something like that. Uh, you wouldn't want to use auto instrumentation at all from any, any vendor. Doesn't matter if it's Cloudwatch or anything, don't use automatic instrumentation because there is a tiny little overhead to, because of the way auto instrumentation works, it looks at the code and it, it's analyzing it on the fly, so it adds, uh, it adds a tiny bit of latency. But unless you're doing that, it's kind of a no-brainer to use. And It's really hard to Get a screenshot that shows you everything with application signals, but this is kind of the high level overview you get. So it does service Discovery. It allows you to, it, it creates um the golden signals for you. It allows you to create SLOs based on those golden signals. So you can see, I can see the health of my services there. So uh this is a a weather application, you can see. I've probably been a bit aggressive with my SLOs. It's actually not that bad, but it's showing that they're, they're all unhealthy. And you get services by fault rate, and when you go into these individual services, you'll see things like um what dependencies they have, what services they're interacting with, and the SLOs for that individual service. OK, so this is better, more insights, no work at all. You have to turn it on, but that that's about it. So container insights, uh we can use for EKS and ECS and it gives you um resource utilization right down for right from the cluster level, right down to the pod level. And it can give you um metrics on container performance, cluster performance, um, uh, information about deployments, and it integrates with application signals as well. We have database insights. Again, something you can just turn on. Well, actually it's turned on by default, but there are two, there are two tiers to it. Uh, and if you're running something serious in production, you probably want to turn on the, uh, the next tier. By default you'll just get the basic tier. So it gives you performance monitoring for RDS gives you. Uh, analysis of your queries, things like connection pool tracking. And it can also give you performance recommendations, so it can proactively give you recommendations on what you should do with your database. And we've got lambda insights. Again, you can just turn this on and this gives you function level metrics. Things like cold start analysis, memory and CPU utilization, network, um, and uh this integrates with tracing as well. I'll just show you what this looks like. So this is what container insights looks like, I've got a view of uh one particular service there. I'm just running two pods and I can see all my metrics for that individual service. For database insights, there's lots of information and database insights. This particular database is not very busy, um, but we can see the uh the top sequel there. And for Lambda Insights, you can have a multi-function view or a single function view. Here I've got a single function view. This is one of my services which goes and gets the current weather. And we can see things like invocations, duration. Memory utilization, CPU utilization. We can also get a view of the last 1000 invocations, we can link directly to application logs. Uh, and there's also um a link directly to application insights there as well. OK, and now, so we've gone from doing a little bit of work to doing no work, and now we're gonna let AI do the work for you. Um, So we've got anomaly detection, as I've talked about, across metrics, logs and traces. And we can do a correlation of um of uh of all of these resources using Cloudwatch investigations. So I'm gonna kind of show you quickly what this looks like. This gives you a, also gives you a visual mapping of like the causal relationships between um between your services, and it gives you root cause analysis with natural language. It can even recommend run books as well. So this has a scope that can cross multiple accounts, so you don't have to worry about just running it in one account. Well you run it in one account but it can, it can take in the scope of multiple accounts. It can correlate with change of events, so it looks at things like It obviously it looks at metrics, logs and traces, but it looks at things like cloud trail as well. As I said, it gives you that visual overview of uh of the scope of your investigation. You can share the findings of the investigation with your teams. Obviously there's programmatic access as well. So like Jared mentioned, we use it internally, we have an alarm, we create an investigation, you can, you can uh trigger an investigation based on a cloudwatch alarm, and then obviously you can use the APIs to then send that information wherever you want. So yes, you can integrate it with your instant workflows. And I suggest that if you do use this, do it in a similar way to we do, like integrate this with your ITSM processes. Like if you're using ServiceNow for example, put this into your uh into your ServiceNow tickets. So this is uh an example of what an investigation looks like. It's telling me what happened, it's giving me the evidence and the likely causes, it's given me that um visual overview. Um, there's some reasoning below this. And don't worry, I'm gonna kind of go through um what I did, so. I was basically looking at this metric and I noticed on the left hand side, you can see where it says impact start, I noticed there was a bit of an increase in the duration of my lambda function. That was all, so my lambda function was running for nearly 6 seconds instead of roughly just under 4 seconds. So I asked it to investigate. And this is the root cause summary. So essentially I've Bedrock was throttling me because uh I'd reached my rate limits um and my retry logic was failing because of these repeated um uh failed calls to Bedrock. And it was telling me my, my function duration went from, you know, 4 seconds to 6 seconds. So it's, it's told me, it's told me the root cause, uh, and it would tell me then to obviously go and increase my bedrock limits. So we've also got um MCP servers for Cloudwatch. We've got two of them in fact. So we've got the Cloudwatch MCP server. So uh this allows you to uh obviously use natural language to go and do things like look at uh metrics, logs and traces for you. And we've also got application signals, so that can look at all the data in application signals. Um, so both of those working together are probably, probably the best way of doing it. And these are the kind of at the top here, the kind of natural language things that you might ask. And this is one of the things I did ask, so here I'm using Quiro. Um, And in Cairo, I've I've got these MTP servers set up. And I've just said my application is deployed in EU West one based on the code base, so it's got Kero will have context of the code base once I put hash code base in. Shown me CPU spikes in production in the last hour. Now because it's got my code base. And I've got the MCP servers, it knows what resources to look for, it's not gonna go and find um uh stuff that I don't care about that's not related to this application. And you can see here there's there's logging information and stuff, but uh but that's not really, that's not really important for now. What I wanna show you is the output. So the output here is just showing I had some spikes in CPU utilization for my EC2 instances. It said the critical spikes detected. I would argue they're probably not critical. My uh EC2 instances spiked to like 30% maximum. And then I've got some EKS pods, some uh ECS services, and it's given me a kind of summary of that, and at the end it said, the good news, your, your application and other critical services remain stable throughout. Um, but all of that, it's gone and done for me just with that, that one little prompt. So this is really nice if you're uh if you're able to use MCP servers, um, have a look at the Cloudwatch MCP server and the Application Signals MCP server. They can be really useful just to go and uh dig into things as well as using Cloudwatch investigations. So I wanna We've talked about lots of different things, logs, metrics, traces, these are the kind of fundamentals, um, before you can build up to using those uh AI tools in a, in a meaningful way, so. You have to set the foundation. Configure a monitoring account, right, so you've got everything in one place. Set up log centralization again so you've got everything in one place. Make sure you've got retention policies. Um, that reflect, they've got to reflect your organisation's requirements, um, things like, um, uh, compliance requirements as well. Deploying an alerting framework, so by that I mean the kind of thing that Jared talked about, about when you do an alert, or when you have a cloud watch alarm, you know, follow a standard pattern, and maybe it would involve triggering a cloud watch investigation, um, opening a ticket on your system, in the way that Jared described. And make sure you have tagging on your resources, um, especially with the ability for metric insights, at least for vendor metrics right now to uh be able to run queries based on tags, that's, that's super useful. Uh, and another thing is I've not mentioned on here, I should have mentioned it probably on here, is create standards, create standards for logs, create standards for metrics. You tell all of your teams that they should be logging in JSON format. These are the kind of fields that we want in, in every single uh application or or service. Things like who owns it, what application is it part of, um, what environment is it in. All of these things are super important. And then we come onto insights. So the, the, on the left we've got the kind of fundamentals. Insights, these are really simple to deploy. Um, there's no reason, uh, obviously they, they come at, come at a cost. But in terms of ease of uh ease of deployment, there's no reason not to do these. Container insights, just a tick. And Database insights, same thing. Lambda insights, same thing. Obviously you can do this with infrastructure as code as well as using the console. Um, have cloudwatch agents on EC2. Um, how many of you have not got an agent on ET2? Any kind of agent Good, well that's good news. Sometimes I speak to customers and they've got, they've got nothing running on EC2. Now the advantage of having the Cloudwatch agent on EC2 is obviously that gives you um everything in one place. Um, it also means that you can use compute compute optimizer to uh right size your EC2 instances as well. If you use compute optimizer and you're not using the Cloudwatch agents. Uh, then you will only get CPU and networking metrics, and who wants to right size their EC2 instances without taking into account memory. Um, enable network monitoring, we've got some really nice tools in. In Cloudwatch to monitor networking. I think very recently there was um something announced with uh monitoring EKS as well that's built into Cloudwatch. Um, we haven't really had time to talk about this in this like short space of time. Uh, have a look at Internet Monitor, that's a really nice thing to go and have a look at to see the kind of thing you can do. And then start to look at application performance. If you've got. Applications that are written in Python, Java, Node, or .NET, then, um, then have a look at the, have a look at applications and signals. Try it on one of your workloads, um, in, in development and, and just see what it gives you. Because it's very little effort and it gives you lots of information. If you don't want to do sampling. Um And you're worried about the cost of tracing everything, enable transaction search. Uh again, that's super useful, it means that you're not gonna lose any data. Uh, you make use of application signals for your SLOs, but even if you're not using application signals. And it's not immediately obvious this, but if you go into the Cloudwatch console and you look under application performance monitoring. Uh, and you see SLOs, you can actually create SLOs on any cloudwatch metric. It doesn't have to be an applica, it doesn't have to be related to application signals. You can enable real user monitoring for your web applications. And you can uh create synthetic canaries, synthetic canaries are are really really useful. You can run them up to every uh minute. And you can either just do like a heartbeat check or you can run through a complex workflow. Um, and you can run them, uh, to a schedule or you can run them on demand. So they might also be useful for things like smoke testing. So you create, create a bunch of smoke tests using synthetic canaries and you run them on demand at the end of your pipeline. And then add intelligence. So use contributor insights. Who uses contributor insights here? Or no one, this is a. This is really sad. Um, I, I think Contributory Insights is one of, one of the most powerful tools we have. Um, and I mean, I'm not going to go into it again, Jared talked about it, but have a look at contributor Insights because it's, it's really, really powerful. Use metric insights for, for your alarms. Have a look at metric anomaly detection, look at log anomaly detection, also look at just how you can use uh pattern analysis in uh just when you're doing queries and login sites. Once you've got all of this, then you can start to really make use of Cloudwatch investigations properly. Now you can use Cloud Watch investigations when you haven't done all of this, but to really get the best out of Cloudwatch investigations, obviously like, like anything AI powered, the more information it has, the better it's it's gonna be at getting to the the right answer. You can create run books in Systems Manager. How many of you create run books in Systems Manager? Yeah, a few of you So when you uh you can, when you have an alarm. Triggered, you can choose to run a systems Manager automation document, um if you've got something that's repeatable um and you can fix with an automation. One really, I think one really simple explanation of this is if you've got an EC2 instance. You don't, you don't want to get woken up at 2 o'clock in the morning just because the disk is full, right? Um, so there's, there's a Systems Manager automation document already built into Systems Manager where you can. Uh, if the, if there's an alarm for a disk space full, you can just trigger this document. It will expand the EBS volume, then expand the OS volume into the EBS volume, and then you can investigate in the morning when you get into work, rather than having to be woken up at 2 o'clock in the morning. We've got AWS chatbot as well, so you can use that to uh to send information like to and from uh Slack and Teams. Um it's really important to set up these incident workflows. Um please don't be in a situation where you just send your alarms straight to Slack or Teams, and you don't have any proper, um, incident management workflow. And then after you've done all of this, that's when you can begin to optimize the cost and performance. Maybe then consider using composite alarms, the composite alarms allow you to create like a a tree of alarms using boolean logic. You can use things like embedded metric format. And you can start to optimize uh things like your log retention, maybe, um, uh, also you can optimize metrics and um. Uh, and also for tracing as well, you can optimize your sampling, uh, especially if you're using transaction search. But that's it, thank you very much for coming. I hope you've enjoyed your time here. Um please uh fill in the survey on the app. Thank you very much.