---
video_id: 4GKXx9vIqsk
video_url: https://www.youtube.com/watch?v=4GKXx9vIqsk
is_generated: False
is_translatable: True
---

So this talk is about Dynamo DB um. I'm going to start with this one, which is probably a better talk. It's like. You guys didn't wanna give me a round of applause? Come on, start, OK, um. I did not expect this slide here. Do you have any idea what the slide is about? Oh, I, I put this up. OK, so. This is probably an indication for you how the rest of the stock is gonna go. Craig and I worked together for several years. Um, my name is Amrit. I'm the guy on the right here. Um, We worked together for several years, but we go back and forth a heck of a lot. That's what you're going to see during this talk. Both of us work on Dynamo DB. He's Craig. I'm Amer. What I've found in the last 6 or so years that I've worked in Dynamo DB. In AWS is that customers who build applications on Dynamo DB. Need to understand not only how the service works, But also how we made some of the decisions about how the service works. So, this talk is more about how we made those decisions. Um, this is who we are. The exact decisions we made may not be relevant to you, but what I hope you will take away from this is how we come about those decisions, because those may be more applicable to you as well. So, the way in which as a large organization making a service like Dynamo, uh, I think somebody here is wondering which talk they're in. This is the Dynamo DB talk. OK. Um, The objective is if you have a large team. And you need to make everybody in the team able to be productive at their maximum level. You cannot have centralized decision making. We build a distributed database. The idea is you want to distribute things across many nodes and have the power of many nodes. What's the point in having a team which has one decision maker? No point. So we have tenants in Dynamo DB and these give the team an understanding, a shared understanding, so that we can push down decision making into the organization. This is something which I think you should understand. This is how we work at Amazon. This is something which is common to all AWS services. It helps teams make decisions, and these are our tenants. First, Security Second, durability. Third, availability. These are non-negotiable. I don't care what the feature or the implementation choice is, I will never trade security. In order to give you something else. And what's the 4th 1 down there? Predictable low latency. This is a Dynamo DB feature. We want to guarantee predictable low latency at any scale. But we will never trade one of those. So, in the rest of this talk, what you're gonna hear. is a reiteration of these tenets security. Durability Availability and predictable, low latency. These need not be the tenets in your organization. But as a decision maker in your organization, the thing I would urge you to do is think about what these tenets are. Make sure that everybody in the team has a shared understanding of these tenets, so that when they're making a decision. They will understand what these tenets are. No. In the rest of this talk, we're going to talk about this over and over again, but many of these choices are going to come down to predictable low latency, because like I said, security. Durability and availability are non-negotiable items. So, if we have to give you predictable low latency, just show of hands. How many of you here use Dynamo DB and how many of you have never used Dynamo DB? Never use Dynamo DB show of hands. All right. Very small number. The rest of you understand we, we try to give you predictable single digit millisecond latency at any scale. So with that, let's talk about, uh, all right, all right. You, you've talked enough, right. This is how it usually works with Craig. You, you wanna take it over, man. Go ahead, yeah. So we're talking about scaling, right? And I wanna give you a sense of the scale of Dynamo DB, um. You know, 5000 requests per second, right? That's a lot, right? We have hundreds of tables right now, just right now, that are all pushing 500,000 requests per second. This is just normal for us every day, this is what we see. And it's not just, you know, just barely half a million. We've got, uh, for instance, the Amazon stores business, uh, on Prime Day, they peaked at 151 million requests per second. That's one customer of ours. So a lot of the scaling causes us to think about things a little bit differently. Um And when you combine the scale with state in a distributed system, this is where a lot of the challenges come from. You know, I'm not saying that stateless distributed systems are easy by any means, but state full is a whole another level of complexity. Before we get into a lot of the details of uh Dynamo and some of the specific choices that we made, I wanna talk about some kind of some of this problem in the abstract just kind of so we're all on the same page. So if you're building an application that has a state. The simplest thing to do is put the application in the state on the same instance, right? Sorry, are you saying that it's gonna be easy to do this, or like, it's like, is this easy or what? Uh, I mean, concurrent programming is not necessarily easy, but relatively speaking, yes, I'm saying this is easier than distributed state management. This, this pattern works great for a lot of applications, but pretty soon you wanna scale, you wanna scale things independently, and so you move your state onto a separate instance. Um, And now you've got to deal with failure. Right? Because you've got this, these two things now that have to be available. Your application server has to be available, and now your stateful server has to be available. They both have to be available, so this is strictly less available than the, the previous approach. But you get some benefits because you can now scale the state and the application independently. But it also means that the state can disappear out from underneath you at any time, and so you have to, you know, think about these failure cases throughout your code everywhere. So how does your application actually know what the state of the underlying database is? Well, In some cases it doesn't, right, because there's, especially as we start scaling, we get to the world where we say, hey, 11 instance isn't sufficient. You know, we're gonna have multiple instances and it's now we're introducing coordination and coordination is where a lot of this complexity comes from, right? And so you can end up, you know, this one of the simplest things you can do then is move from a, a single instance model to a primary and secondary model, right? And this is great, you do all your reads and writes to the primary. And You know, you, you think through this and you're like, OK, well, I gotta deal with when the primary fails, right? That's, that's easy. Everybody thinks about when the, what happens when the primary fails. What happens the other side? Secondary fails. The secondary fails. Yes, this one's actually more interesting, and, and this is one that's, it's a little more subtle is because what do you do in this case, right? Do you continue to accept rights on the primary? But those rights won't end up in the secondary because it's, it's down. And so what happens to those rights, you know, when the secondary gets healthy again, like how are you gonna repair all this, and you start getting into a lot of these corner cases and the complexity that comes from the distributed state coordination, right? And so, so 2, right? Well, maybe 2 isn't the 2 isn't the answer. So 32 doesn't work, let's try 3. We'll just keep counting up here. So 3 is the real minimum then, right? Right, well, this is great because now if you want to survive a single box failing you can because you've still got two healthy and you've got another one. The right comes into one of the nodes and you can get it to one of the other nodes. So now your right's in two places and that right is no longer susceptible to losing a single box. If 3 is not enough, what are you gonna do? Well, so 3 is not enough. Let's try 4, right? If 3's good, 4 must be better. OK. Well, so with, with 4, you know, now in theory you can survive 2 failures, right? Because I can still get the right to the other node, and that's fine. But um. Uh, well, no, I, I, how do you know these two are the correct two? OK, how do I know that? Yeah, so there's this, this failure case, right? Like maybe all the nodes are actually healthy and they just can't talk to each other. We have a network partition, right? So both sides think that, well, I'm on the healthy side, the other side's dead. I'm gonna keep accepting rights. And so you end up with this, this what we call split brain, where you've got uh rights, the data uh the data set diverges over time. And now when this partition heals, you've got these two totally different data sets you have to merge. So, having an even number is a horrible idea. 4 is a really bad option. So basically the idea is you need to have an odd number of nodes. Is that, is that the story? Yeah. So what about 5? You know, let's just, let's keep counting up, right? So 4 is out, we could do 5. So with 5, you can survive 2 failures. Maybe that's good. Like maybe that's maybe that's what you want, but. Well, what we've found through a lot of our experience is that as soon as one fails, you're working as hard as you can to get that uh replica back to par. You're bringing a new node, you're gonna repair the situation. And so what you're actually doing here for the, the second failure is you're racing, how fast can I get a node healthy again to get this cluster back to par, back to normal, before we lose another box. And so a lot of the effort that goes into having more nodes might actually be better spent trying to repair a node faster, because you're going to need to do the repair in this case anyways. At this point you're basically saying you need to have an odd number of nodes. Is that it's like 3 is OK, 5 is OK, 4 is not OK, 2 is clearly not OK. Is that, yeah, right. And, and you know, as you're growing up you, you, you have to do more rights like even in this world, like you have to have the majority healthy so that you know that you're on the healthy side. And so you're, you're adding more boxes and so you gotta think about the cost. So, let's take this to the extreme. Like, let's say for scale reasons you need to have 1000, 1,0001, because it's gotta be an odd number, right? So you got 1,0001 in order to accept the right. I've got to have 502 boxes. It's gotta be the majority except the right. No, statistically you can't do that, no, because any one of those 500 and whatever boxes can fail. So if you do this, you're gonna trade off availability, yes. It's also really expensive, because you just like you've got all these, like, you really need to write that thing 502 times just so you know you can survive some failures. So what do you do? So an alternative that that we think is, uh, is a much better alternative is you have lots of groups of 3. You still can have, you know, 1000 nodes or however many tens of thousands of nodes, wherever you want, but you do, you, you group the data sets into groups of 3. Because in the um in the, the groups of three, it has all the properties we like, right? We can replace the boxes quickly, um, we know it's correct, it's a nice odd number. How do you know where to send a request to? You and your complexity, yeah, if I send a request, I wanna, I wanna put all these boxes behind a load balance, right? So the client doesn't necessarily know, you know, in this case, CF and J are the, the three boxes that are in the cluster for the data set that this client is interested in, but if I send a request through a load balance or whatever, it's probably gonna land somewhere else, almost certainly as the number of boxes goes up. And so what K has to do then is like, OK, well, Ky can either return or redirect, or it can proxy onto the correct note for you. But, but if you think about that, right, K then just becomes the client, because K's gotta know the, the data set, and so we have the exact same problem to go solve of for this particular data set, where is it? I got a lot of boxes. So basically you're gonna add one level of interaction here and say that every problem in computer science can be solved with one level of indirection and Kay needs to know where to go. Look this up. OK, fine. All right, OK, that, that, OK. Have you seen these slides before? No, I have not. You made the slides. I haven't seen them too. No, yes, another level of interaction, right? And so, we add, uh, a data set which is the data about where the data lives, so our metadata. Right, and so the, the protocol then is, the client, in order to go figure out, I wanna go talk, you know, get this particular data, I do a lookup in metadata, and then I go get the box that I need to go connect to. Right? 2 lookups is like expensive. Yeah. So now we're doing double the lookups, and so we have to do X requests per second to the data set. I also have to do X requests per second to the metadata set. So I know what you're gonna suggest next. Yeah, there, there has to be an easier solution because if you were to do this, then you need to scale your metadata to the same capacity as your actual data, which I'm not willing to do. So you need to give me a different answer. So we've got 2 of our classic computer science solutions now, an extra layer of interaction and caching. Right, so if you had a cache here between the metadata, which is fine because these data sets don't change that often, you can cache this, you have less than 1 requests per second, and metadata doesn't have to scale to the same request rate. Any snarky comments about this one? No. OK. And what are you gonna do if your caches are cold and what are you gonna do in situations where your metadata is not able to keep up with the uncached data, all of those things? You're foreshadowing your portion of the presentation's coming up later. Thank you for telling me what I'm gonna talk about. It's gonna be nice to know. So caches can be a risk, and we'll talk about that more later. We'll also talk about the how we do, uh, this routing a little more detail. The other thing is boxes can still fail. Right, so we have these large boxes, we have to deal with this case, and so when a box fails, and, and so in this case J has failed and we want to replace it with L. Elle then has to then go and update metadata somehow, and indicate, hey, I'm part of this set. Right. And so we have this problem, right? This problem is Who, who is the authority of which servers participate in which groups? Right, you know, you, you think it, it should be the metadata because I'm doing all the lookups. But we've got this race between when a box becomes part of one of these clusters of 3 and when it can serve traffic and when it tells metadata and when the client caches can can recognize it. We're talking a little bit more about this as well. So basically the idea you're talking about is. A system which is a system of record authority. And something which points to it which is potentially eventually consistent. So, are you suggesting that for scale you need to understand eventual consistency and deal with it? Yeah. Yes, OK It's, it's hard, but all right, it's hard, but that's really for me, right. But of course we've got this magic metadata thing we've been talking about. What the heck is this? Like, how's that gonna work? There's, there's, there's two options here, like. We could just make metadata another one of these tables, right? And this, this works great, like, but you still have that routing problem of which subset of metadata do I need to talk to and so I gotta go solve all of that again. It makes the rights relatively simple though because there's only one place I need to write to and. You know, the system's gonna have a lot, a lot lower request rates because of the caches, but it's also going to be significantly smaller because all you need is a single item to say, hey, like tens of gigabytes of data that you're looking for, they're over there. And so this system's gonna be multiple orders of magnitude smaller in terms of data size as well. Which opens up an alternative of instead of having subsets of this, you can have entire replicas. Right. If you have entire replicas, the lookup problem gets really easy because as a client, you have to talk to one of the metadata nodes. It doesn't matter which one. But the trade-off is it makes the right problem a lot harder when we change the membership set. It somehow we now have to go update all of the metadata nodes that exist because you can't predict which one the uh the client's gonna go talk to you later. So this makes it more eventually consistent. Oh, that was the that was the back. I apologize. There we go, uh. So for the rest of the talk we're gonna, we're gonna dig into three aspects of this and choices we've made in Dynamo and look at it in kind of a more tangible of what, how did Dynamo decide to solve some of these problems. We're gonna look at, uh, request routing, how does the client can get to the storage that we've just described. We'll talk about, uh, how we structure our metadata. But also in a system this size, we'll talk about some of the, the constraints and the limits that we have to put into place um and why those exist. Customs uh will, will run into these and when you, if you understand why, um, we think this will help you build your applications. So routing. Very simple description. This is what Dynamo looks like. You know, storage node fleet on the right is what we were just talking about with that large, you know, thousands of nodes with clusters of 3, and the request router fleet is the fleet that sends the requests to the appropriate box. And so when a client comes in, they get sent to a random request router node. The request writer has 2 jobs. First one is to do authorization and authentication. Are you who you say you are? Do you have access to this data? This is where we do the SyncV4 processing. Then we also do the metadata lookup. And from there we say, OK, now I know which particular storage node to talk to. We forward the request on. OK. This picture's too simple, there's more to it, right? I said load balancers. So we've got load balancers in front of the request routers. And uh They're just normal no no bouncers. We use a network load bouncer. But because we have so many low balancers, we have DNS in front of the low balancers because it becomes like the load balancer of the load balancers because it's the thing that sends traffic to all the low bouncers. And of course we're running within a region. Regions have availability zones. Availability zones are independent failure domains, uh, you know, availability zones composed of multiple buildings where the, the actual EC2 instances eventually run. At the end of the day we're all talking about like there's real hardware somewhere. So this, this separation is really important for how we provide our availability, durability, guarantees, um, but provide some challenges for latency, so that's what we're gonna talk about. So everything we have is just an EC2 instance at the end of the day, which means it runs in an availability zone. We've been very careful about how we've put our servers across the availability zone, right? So we've, we've striped them across multiple availability zones. In every availability zone, you know, we we have a choice here. We've constrained our load balancers to be within uh an availability zone because we find it easier to think about these units of failure. You know, this is the choice that we that we've made, and this picture's overly simplified if you zoom in on one availability zone. What, uh, I think I double clicked on that one. I did double click on that one, If you zoom in on one availability zone, we've got a whole bunch of load balancers, and behind every load balancer's a bunch of instances, um, but I'm simplifying the picture. So if we go back um to the, the simple picture, like low bounces there. Every dynamo table it's divided up into partitions, which we'll talk a little bit more about later. Every partition has 3 replicas. And so we've spread the replicas across the 3 availability zones, right? And this is how we provide our availability and durability guarantees, right, because as we talked about at the beginning, we can lose 1 box, 1 replica. And this cluster can stay healthy. Right. Our risk is if we lose 2. And so as soon as we lose 1, we're gonna try very hard to replace this, and so we're trying to replace the instance as fast as possible, um. But our risk is correlated failure. And the most likely correlated failure we'll see is across an availability zone because of, you know, um, some larger scale events, because we do all of our, we do all of our software deployments, we scope them all to to an individual availability zone. We want to make sure if two boxes fail, they don't impact two of the replicas in a partition. We do this by spreading them across availability zones. So this is critical to what we do, but there's a consequence to this, right? These availability zones, like I say, there's, they're physical buildings, they're connected, and, and you can and you should do this. I look at the distance between some of these, the network distance I'm talking about. Launch two instances in availability zone. Ping them, see what times you get. Do this in different availability zones, you'll see it's different in different availability zones. Do the same thing across availability zones. You'll see it's different across availability zones. What's always true is that the the network distance between availability zones is significantly higher than the network distance within an availability zone. And so our clients are running on. EC2 instances, lambda functions, you know, containers within ECS EKS, whatever it is, they're in availability zones too. And so the the simplest thing we can do for load balancing is to randomly route across all of our available instances, which means we're highly likely. To route a customer across an availability zone and back across availability zone. So we'll be paying the extra latency penalty multiple times. So ideally what we'd like to do is this. And use the shorter network path. And this is meaningful to a service like Dynamo, because if you look at the components of our latency, our server side processing time is actually relatively small and of a similar order of magnitude to the network distance itself. It's one of the biggest things we can do as we work on improving our predictable low latency over time. Is shrink the distance. We don't have to, we don't have to do any like software optimizations within the server itself, we just shrink the distance, and by shrinking the distance, that ends up with a meaningful um improvement in uh customer latency. So I said DNS is our load balancer across load balancers. Um, so if you dig, um, you know, do a DNS lookup for, uh, our domain name, this is US West 2, you keep doing this over and over again, you'll get different IPs for different load balancers within the regions. So one of the things that we can do then is we can do a split horizon DNS is where you get a different answer depending on where the query came from. And so if you're in one of the availability zones, you'll get a set of low bouncers in that same availability zone. If you're in a different availability zone, you get low bouncers from there. And so that means that DNS is our availability zone selector as well. Yeah. And that's it, piece of cake, right? We're all done. Woohoo. Well. Failure is where this all gets hard, right? This is what we expect to see, this is what we like to see is you end up with, you know, 1/3 of the traffic in every availability zone, meaning every availability zone processes 1/3 of the requests. It's relatively easy to scale and, and deal with from a capacity planning perspective. We gotta deal with a case where an entire availability zone uh fails, or we want, you know, we wanna take it out for whatever reason. In, in that case, what we then have to do is all the traffic from that availability zone now has to go somewhere because we prioritize the availability over the latency, right? As you'll notice here that availability zone 1 and 3 are now seeing a 50% increase in the amount of traffic. They would have had otherwise. So as a regional service, this is a thing that we have to plan for ahead of time. And so we just have capacity sitting there ready all the time. So the price of doing business as a regional service and you know, ADBS services, you know, price follows cost, and so, you know, this is baked into the price of Dynamo DB. To be able to handle things like this so customers don't have to know under the covers that, OK, we're dealing with an impaired AZ at the moment. So it's fine. But if you think about the case like, well, what if the traffic in one availability zone is significantly larger, we have a traffic skew. Well, one option is we could just scale up the number of servers in that availability zone. Oh, that's relatively easy, right? But again, think about what happens when that one fails. Now what you see is availability 1 and availability zones 1 and 3, they have to handle a doubling in traffic. So that idle capacity that we have to have sitting there ready in case of this event is significantly larger. So this is a way more expensive way for us to run this service. So that would mean that we'd have to pass on the cost to customers, and so we don't, we don't like that option. The alternative is what we do is we say, well, we'll never send more than 1/3 of traffic. This makes our DNS management a lot more complex, and it does mean that some requests will always be going across availability zone. In reality, you know, law of large numbers helps us out, and we don't have a very large SKU, and so we don't really see this scenario. We also have some traffic coming straight from the internet, and so the internet is close to every availability zone at the same time, and so we can send that traffic wherever we want with more DNS complexity so we can fill in the gaps. But these are the types of things you must think through as you're building this style of architecture. That's how we shrink the first hop. This is a big latency win, um, lose more, right? Because you look about that 2nd hop between the request router and the storage node. Again, ideally we send a, you know, 13 of the traffic everywhere. Um, from capacity planning perspective, this is easiest. Um, Dynamo did this since launch, and the, the thing is we've kind of baked in this model, um, into our, our capacity planning, our pricing and our, our limits, uh. Because when you're, you're doing a strongly consistent read, you must talk to a node that we've elected leader. So there's only one box you can talk to, you have the request processing capacity of one box, but eventually consistent reads, you can talk to any of them. And so, so we said, OK, well, well, one of the boxes might be unhealthy, so we can't count on that. But this is, but we've got the processing capacity of two boxes, that box has to be there anyways, so let's just use it. So this is why an even assistant reads, the limits are higher and the price is lower. It's because we have this capacity that would have been idle otherwise that we can use all the time. Actually making this change to route locally is pretty easy because we control the, in this case we control the client and the server. We have to know about all three replicas anyways, uh, and so we just pick the one in the same availability zone that we are. We send all the traffic there. Unless of course you're relying on this fact that you can send 2x the request for an eventually consistent read, because you could now be sending 2 boxes worth of traffic to a single box. That's no good. And so what we have to do is we actually have to monitor statistics on the server side and detect when this happens. This happens sometimes, it doesn't happen super, super frequently, but when it does, we can send a message back to the client and say, hey, for this table, let's go back to the random routing mode because we need to prioritize availability over the lower latency in this case. And again, in reality, uh, most of the tables that are, you know, much higher traffic, their clients are distributed across availability zones all the time, and so they're being regretted to each individual availability zone, which means that when we do the local AZ, we see the, the spread that we'd expect to see and so we don't end up in this case as often as you might expect. This is, so this is the path on uh that we've taken and how we can shrink our latency and a few times we've bumped up against this, you know, the availability versus predictable low latency trade-off and when forced to choose, we're gonna make the service available, but in a lot of cases. There is no conflict, and we can do both. It leads to a bunch of complexity on our side. But these are the tenets and the most important things for our service and for our customers, and so we think this complexity is worth it in order to provide a better product to uh to you folks. I know the next slide is, do you want me to take this one? Yeah, I hope so. OK. All right. Get a drink of water or whatever you want. So a couple of things which Craig mentioned, um. He talked about how we do client routing, um, and 1/3 of the traffic goes to each. HAZ. Another thing which we do is we try and keep 1/3 of the leaders on. E AZ Therefore, even on the strongly consistent reads, you're evenly distributing traffic across the ACs. Um, sorry, I didn't want to interrupt you when you were talking about it. Uh, that's a first. All right. First time I'm seeing these slides, so it's gonna be interesting. All right. So you have a request, it comes in, you talked about how it goes all the way through to the request router. The important decision which a request voter needs to make is Should we serve this request? And should we serve this request is a three-part question. Are you who you claim you are? Is your SIGB4 signature matching your request? Do you have the permission to do the thing which you want to do on the table which you're claiming you want to do it on? And the third one is, are you within your limits? So, those are the three questions which we have to answer each and every request we get. Now, We've got hundreds of customers who do over 50 million requests per second. We do this stuff. Billions of times a day. And we have to do this really, really fast because what are we after? Predictable low latency at any scale. That's what we're supposed to get. OK. All right. I don't know about this crew, but, OK. All right, thank you. Somebody heard it. Um, so we're after predictable low latency, but once the request router knows that your request is legitimate. And we do want to serve it. The next question which we have to ask is, which storage node do we send it to? Now, Craig talked about distributing a table into multiple partitions and partitions across multiple storage nodes. We have 3 storage nodes here. I'm sorry, I don't have a clicker here. Uh, but there's literally hundreds of thousands of storage nodes across which we distribute data. When I asked earlier, how many of you use Dynamo DB? Or how many of you don't use Dynamo DB at this point, only one person raised their hands. So the rest of you, I have a simple ask of you. Look to your right Other right. OK, look to your left. All of you who use Dynamo DB, your data is co-located on storage nodes with people who are next to you. It's completely shared infrastructure. We do not spin up a cluster for you. So your data is co-located on hundreds of thousands of storage nodes with other people's data. And hundreds of millions of times a second, we need to decide where to send each of these requests. So, that's the problem which we have to solve. Once the request makes it to a storage node, first, all data is always encrypted at rest. Anybody know why? Shout it out if you know why. What's the first tenant I talked about? Come on, shout it out. I can't hear. I don't have, thank you. Security. All data is always encrypted at rest. You can provide us a key. But if you don't provide a key, one will be provided for you. All data is always encrypted at rest. So we need to get that security. That key. Then we need to decide, are you overrunning a partition? Shared infrastructure, rate limits, provision throughput, all of those kinds of things. We need to make sure that we want to admit the request. So there's two places where we decide whether to admit your request or to throttle you. Once we're done with that, we have to serve your request and your response is gonna go from the storage node back to the request router and back to you. Everybody good with this flow so far? Quick show of hands, yes or no? All right. More than 50%, OK. For those of you who have used Dynamo DB before, simple table. Unique attribute login name. Non-unique attribute human being's name. So somewhere in the middle there, there's two people with the same name, but they have different login IDs. You've all faced this problem. The primary key on this table. Is logging. So, when you create a table, we ask you for 3 things, only 3 things. What's the name of the table, what's the primary key, what's your credit card number? Only 3 things. How are you gonna pay for it? That's your primary key. Once you tell us what the primary key is, we compute a hash of your primary key. We order your data based on the hash. Notice that the order of the names is different from the order of the names on the left because we ordered it by hashes here. Contiguous ranges of hashes become partitions. Craig talked about partitioning a table for horizontal scale. This is partitioning for you. Once you've partitioned the table. If you want to fetch an item. I need to find the partition where it is. So, I wanna find the item for Jorge or for Richard or whoever, I will compute the hash on that. I will find which range of hashes it falls within, and I will send my request to that location. Hundreds of millions of claims a second. With predictable latency. That's the thing we're after. So, in, in matter of fact, we built a system which will do this in tens of microseconds. Tens of microseconds, literally. We measure this thing and when it starts to get out of this bound, we're really antsy about it because we do it so often. And what we have to do is locate the partition where your data is. You all right? All right. Um, look at, look at where your traffic data is. And today, the traffic we have is 100s of millions of requests per second. But at any point in time, the things which we have to deal with are How many storage nodes do we have? How many request routers do we have? All these numbers are measured in like 6 digits or more. And at any point in time, you're creating tables. You're dropping tables. You're scaling up traffic on some table. We're splitting a table because of that. Partitions are moving around and we need to figure out where your data is in tens of milliseconds, 10, sorry, tens of microseconds. So, I want to talk to you about how we go about doing this. And I want to reiterate, effectively, what I want to get back to is the point which Craig talked about, which is State is hard. Shared state is really hard. This is a distributed system we're trying to build, where these things on the left here, I really should get a laser pointer, but these request routers on the left here. Need to be able to figure out in tens of microseconds which storage node your data is on because those storage nodes at the back there. They're doing 3 cards, you know, shuffling all the time. They're moving partitions around, they're splitting partitions, changing key ranges. You're dropping tables, creating new tables. These things have to stay in sync, right? This is the problem we have to solve. So, how do we go about solving it? These are the piece parts which we have to deal with. There's request routers, there's storage nodes. There's a control plane, create table, update table, drop table, delete table. And there's this thing which you introduced called metadata, partition metadata, which we're supposed to go look up. These are the piece parts. So, let's talk about this situation. You create a table, you update a table. Do a synchronous update to partition metadata. Yeah, this is scalable. How often does this happen? Relatively low volume, 10s of thousands of times a second, not a problem, we can deal with this. Partitions moving around, partitions splitting. Leadership moving around on storage nodes because we're doing software deployment. Hundreds of thousands of times a second. If you're gonna start updating your partition metadata, you're building a pretty serious database in the middle there. That in itself will become a scalability problem for you. The worst situation you have. Is you, our customers, sending us billions of requests. Each of which need a uh look up look up of metadata. So effectively, this metadata system is going to have to handle the same front-door traffic. Which Dynamo DB handles? This is not a scalable system. So, let's look maybe at the next obvious choice. Any snarky comments for me? OK, all right. Um, First option Put a cashier. How many of you have been in a situation where you're building a system and somebody in your team says, I need to scale, I'm gonna put a cache in there? How often has it worked out well for you? Caches are a great solution. They are a dangerous solution. And we'll talk about why. Great. So I have a cache. Every request router now has a cache. Magically, we've implemented one. The size of the arrows from the request router to the partition metadata went down. Let's assume this cache has a 99% hit rate, OK? Partition metadata is serving only 1% of the front door traffic. Cash misses. Any idea what could go wrong in this system? Shout it out if you can. I can hear you, even if you have headphones on. Stale, all right. What happens if all your cashs go stale? If one cash goes stale, yeah, who cares? What happens if you have a situation where for whatever reason, hundreds of thousands of caches go stale? Now your partition metadata is going to get hundreds of thousands of cache misses, and it's going to fall over. We thought about this and said, yeah, the system, yeah, maybe not. So we went one step further, and we also said, This is a situation of a large fleet and a small fleet, a large fleet of request routers, and a small fleet in partition metadata. Could cause the small fleet to fall over. One of the things I said when I started was the specific choices we made may not be relevant to you, but there are some concepts which you should take away. If you're ever building a cache. And a large fleet drives a small fleet. Be very careful, because it's not going to end well for somebody. All right, so this didn't work. So we came up with a next-generation system where we built a two-tier cache. Where we said MEMDS is going to be one tier of cash, and the request orders are gonna have another tier of cash. One tier of cash is eventual consistency. This is now eventual consistency on top of eventual consistency. So keep that in mind when I say this. You create a table, we're gonna push it down to MMDS. Great. There's gonna be a polar there which is gonna go read all the storage and say, what partitions do you have for what tables? Great, I'll push it to M MMDS. Great. This is now, MEMDS is now an eventually consistent cache. You make a request on the request router, if it gets a cache miss, it goes to MEMDS which is itself an eventually consistent cache. This is the system we built, and let me tell you why I think this works. Control plane push new table creation, great. The one thing which we added here is we versioned the data across the board. Whenever there's new data in the place of it, you talked about place of authority, right? Yes. The authority is the storage nodes. The storage nodes know what data they are. Here's a storage node. Do you know what partitions you have? Absolutely you do. So if the partition publisher comes to you and asks you what partitions do you have, you will give an absolutely authoritative answer. That's now pushed to MDS, but that is the state at that point in time. But you also say, my version is now version 20 or something like that. At some point, if you want to move a partition from one place to another or there's a split, it comes to the same thing. Bump the version number. So here's what we did. A storage node had version 2 of metadata. It split a partition. It moved a partition. The new storage node says I'm now at version 21. Now if a request were to come in to the front door. The storage note says I have a cash value which is 20. Maybe it had a cache miss and it went to MMDS and MMDS said value of 20 doesn't matter. It goes to the place where version 20 pointed it to. Version 20 is the authoritative source and it says uh uh not gonna work. You actually need to go to this other place because now we're at 21. The immediate thing which we do is The new, the request is going to go straight to the new place. And it is going to get served immediately. So, even if you have eventually consistent data in the system, like the caches are 2 tiers deep and eventually consistent. If you do have a cache miss, which happens infrequently because there's a partition split or partition move, we are still able to serve that in reasonable time. If you're interested in how all of this stuff is supposed to work, We published a paper about this, that's a QR code, scan it. Um, actually, this is not being recorded, so you probably wanna scan it. Um, or if you want to contact me afterwards. The basic idea is this, you've got a system with data which has entropy. You have a need to do stuff with cassius. And these caches are going to be eventually consistent, and they need to deal with the situation where all the caches can probably go cold. So far, I've talked about the first two, I haven't talked about the last one. So let's talk about that one. System of record, authoritative there, 2 levels of eventually consistent caches were good so far. Now, let's talk about the situation with Large fleet, small fleet, and hedging. If you're ever building a system and you want predictable latency. Use this concept called hedging. We do it internally. When a request arrives, and by the way, hedging is not our idea, it's an idea from Google, that's a copy of the paper up there. When a request shows up, Add a request router In the eventuality that we have a miss on the cache, we don't make one request, we make two requests to 2 MDS servers. Why is this a good idea? Assume that your system is like. Most systems and your latency is along some kind of normal distribution. If you make 2 requests, statistically, one's on the left of the median, one's on the right of the median, take the first response, it's always good. We hedge our requests. What this means is that in the eventuality of a cache miss so far. MDS is serving. Twice. The traffic which we're getting for Kashmir. We go one step further. What happens if all the caches happen to be cold? We introduce this concept of constant work. You make a request to Dynamo DB. That request goes through load balancers and all that stuff, shows up at a request router. The request writer says, where's this item? The cache is correct. It makes a request to the storage node. The storage node serves a response. We've already given you the answer. What do we do in the background? We send two requests. To 2 MDS servers, even on a cache hit. Everybody with me? On a cash hit. We sent 2 requests to 2 MDS servers. Why? Assume someday all the cashus went cold. Memdius would not know the difference. Large fleet, small fleet. This is the cost to us of doing business and giving you predictable low latency at any scale. These are the things which we keep in mind when we build our systems. And we try to build systems with a guarantee of availability. We can't just say your cash is cold, therefore, we had lots of availability. Can't do that. The latency needs to be not just low, but predictably low. These are some of the considerations which we had to go through because these were our tenants. Figure out what your tenets are. And have your decisions mirror your tenets. You're not going to probably have to make the same choices, but in the event that you do, My suggestion to you is be very, very careful of the person who says, we add a cash in front of it, it's going to solve all the problems. Cashing is damn hard. I've been doing this for about 35 years. The number of times when somebody has said, I'll put a cash in front of it and life is going to be good. The number of times they are wrong is close to 100%. Caching is hard. But if you do understand that you want to have caching, Understand eventual consistency. Build systems with version data so that eventually consistent data is your friend. Don't run away, run away from it and say I need synchronous upgrades across large numbers of nodes because that is not a scalable solution. Eventual consistency is your friend if you want to go to a scale. Why are these things important for us? Because our tenets were security, durability, availability, great, predictable, low latency at any scale. If we want any scale, we need to understand eventual consistency ourselves. Everybody good so far? I've talked a lot about caching. I've talked a lot about the things we do on a request order. What are the things we cash? You want to connect to Dynamo DB and make a request. We cast your identity credentials. You want to make a request on some particular table, we cache your table metadata. Suppose you were to make a new connection to Dynamo DB on every request, you're probably going to a new request router. Are you going to get the benefit of caching? No. Make sure you have a long-lived connection. What does that mean? Right size your connection pool. If you have low traffic, have a small connection pool. If you have high traffic, it's OK to have a larger connection pool. If you have a very small amount of traffic, don't have a huge number of hosts who are serving you that traffic. Think about these kinds of things to make sure that you get predictable low latency. Also, if you want predictable low latency from your application, hedge your requests. Send 2 requests. If they're rights, make sure they're item potent. If they're reads, eventually consistent reads. Choose the first one, don't change the timeout. And if you're building caches, please, by all means, build constant work into your uh into your plan. The other thing which we did in order to give you a predictable low latency is this whole concept of limits in Dynamo DB, and there's numerous limits we have here. I'm going to talk quickly about a couple of these. There's read and write limits. The motivation for all of the limits we have is predictable latency because at the end of the day, Dynamo DB is running on physical hardware. Physical hardware has limitations. And this hardware is shared infrastructure. We have to make sure that we operate a service which is cost-effective to us, so we can translate those benefits over to you. So, at the end of the day, we've got physical hardware with some limitations on it. If you created a table, And that table had some number of provisioned RCUs. In the old days, this is historical. We would divide that table into some number of partitions. When we split that table, unfortunately, we divided the provisioning onto those two partitions. This was called IOPS dilution. The reason we have this up here is there's a bunch of people who've been using Dynamo DB for a long time who think this is still a problem. This is no longer a problem. The way things work today, if you've got a number of partitions and we split a table. When we split the partitions, each partition gets the same partition level limit. Currently, the limits are 1000 RC 1000 WCUs and 3000 RCUs. The numbers may change over time. Don't take a commitment on those, but we have those because we want to guarantee predictable latency. I will make one small detour here. A lot of customers ask me, what is the partition count on my table? This is a meaningless metric for you to ask us. And the reason is simply this. If you were to have a table. There's no guarantee that each table has the same fraction of the key range. Each partition has the same fraction of the key range. The real question you're asking is, I have a big cyber sale, Cyber Monday sale in a couple of days. Is my table gonna be able to serve my traffic? The question you need to ask us is this. If I have a different size for each partition, I just made up these numbers here, which partition is gonna throttle me first? The real question is, that partition there is the one which is gonna throttle you. So, just telling you that you happen to have 8 partitions is gonna do nothing for you because 88 times 3000, 8000, 1000 is useless to you. So, we introduced this feature, which was partition table warm throughput to answer this exact question. I have a major event coming up. Is my table gonna be able to serve my traffic or not? So, warm throughput is a feature which we launched. We launched it last year at Reinvent. Describe a table, it will tell you what the traffic is that you can serve without throttling. Similar limit which we have, transaction size limit. When we launched Dynamo DB, a transaction could have no more than 25 items. Today we have 100. We offer standard asset transactions. If you build a rela build stuff with a relational database, we have the same transactions for you. It allows you to build Banking applications, all of the guarantees which you want. But our transactions are different. We have two kinds of transactions. We have read-only transactions and writeable transactions. All of the items in the transaction are specified at one time. All of these things are things which we did because we want predictable low latency. Have you ever been in a situation where your application is hung because somebody started a transaction and went to coffee? Cannot happen in Dynamo DB because the transaction contains all of the changes which are gonna happen in one place. We offer standard seriallizable isolation. If you build applications with relational databases, you understand this. We use the standard two-phase commit approach, standard two-phase commit, prepare, commit. Everything is standard the way in which you would expect with a two-phase commit transaction. We do those things. We did do one optimization for reads. We do not do two-phase commit for reads because it's cheaper for us to just do the read two times. If the items didn't change, your transaction is good. The question is then, why do we have transaction limits? Why 25? Why 100? And the simple answer is this, we want predictable low latency. We did a bunch of testing and we said, if we increase the number of items in a transaction, what happens to latency and what happens to availability? If you have contention on your items which are being modified in the transaction, availability goes down, latency goes up. When we launched, we were able to safely do 25. Today, we're able to do 100. We want to increase the number even further. These are some of the reasons why we have limits on transactions. Similarly, item size limits. Why do you have an item size limit of 400 kilobytes? Well, It's shared infrastructure. We wanna make sure that you do not impact your own usage on some other item, which is in the same partition. As the num size of the item goes up, the number of times you can read or write that goes down. The the number of times, the, the amount of time it takes is gonna go up. We have item size limits of 400K for a limit. If you do need a larger number, I'd love to talk to you about it. We have limits on GSIs, Global Secondary indexes. When we launched, it was 5. Today the number is 20. The reason is we want predictable low latency. Am I going backwards or forwards? I have no idea. OK, today is 20. We were able to make changes to the software so that we gave you consistent, predictable replication lag to the GSIs. Today, the number is 2. Why do you have GSIs? Because you have alternate access patterns. Now, of course, if you have a need for more than 20 GSIs, that's my email address. Please, do contact me because I think there's a different conversation we need to have about your schema. Many of these decisions we took, every one of the limits were to give you predictable low latency because I think that's the one thing which differentiates us from any other database. Predictable low latency at any scale. So, I'm gonna kind of Oh, I came to the end of this, pretty good. Um. So conclusion is, we talked about the tenets. Every one of the decisions we make as a team. is based on our tenants. These are our high-level tenants. Whenever we have a project, whenever we have a new feature, whenever we have a new Thing which we want to develop. One of the things which we ask the team is, what are your tenants? Because those are the things which help distribute decision making across the team, and they make sure that the teams are able to iterate faster. But as a service, these are our tenants. If you ever have a question why we did something, the answer is probably one of these 4. Why did you not store your data unencrypted on disk because it will be faster? First one up there. Why don't we give you some other right guarantee other than we will write the two availability zones, and it'll be faster, won't it? Durability. We are a regional service. We guarantee that if a complete availability zone were to go away, we will not be in the least bit impacted. Your data is written to 2 AZs before we commit it to you. Availability is our 3rd guarantee. There's no point having availability with wrong data or giving your data to the wrong person. Security, durability, then availability, and to us as a service, predictable low latency. So, my ask to you would be this. In the projects you're building with Dynamo DB without Dynamo DB it doesn't matter. Think about these things. What are your tenants? How do you drive decision-making in your organization? This is how we drive it in ours. So, I hope this was useful to you. We're gonna hang around here and answer any questions. But apparently, if you want a hoodie, you have to come and talk to somebody in the database booth. Um, There's no QR code here. There's a whole bunch of trainings which we do offer. The one other ask I have for you is this. Uh, one to ask I have with you is this. We do these presentations because we believe it's really important to share with you the things which we have learned operating a service at scale. And making the choices which we made and learning from you. A part of it is standing here and talking to you about these things, but a part of it is listening to you after these conversations. But if you want more content like this at Reinvent, please do fill the feedback sessions. Thank you very much. I think that's the last one.