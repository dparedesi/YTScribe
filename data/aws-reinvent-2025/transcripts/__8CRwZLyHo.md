---
video_id: __8CRwZLyHo
video_url: https://www.youtube.com/watch?v=__8CRwZLyHo
is_generated: False
is_translatable: True
summary: |
  Mindy Ferguson explains why streaming and messaging are foundational for agentic AI, then Francisco Murillo dives into design patterns, and Chris Jackson shows how Olympics.com uses them for anomaly detection. Data teams struggle with fragmented data, disconnected tools, governance gaps, and scaling infra; streaming addresses these by capturing/processing immediately, filtering before storage to cut cost, powering AI with fresh, high-quality data, and keeping systems in sync. Agentic AI (iterative reasoning and action) builds on rules → ML → GenAI evolution, and real-time “closed-loop” processing is crucial. Messaging matters too: SQS decouples microservices and also enables asynchronous inter-agent communication at elastic scale.
  
  AWS streaming/messaging stack: Kinesis Data Streams and Amazon MSK for durable streaming storage; Firehose/MSK Connect to deliver to lakes/warehouses; managed Apache Flink for real-time processing; SNS/SQS for event fanout and async agent messaging. Pattern for anomaly detection: ingest events (transactions, IOT, play-by-play) into streams; Flink aggregates and detects anomalies via rules or ML (e.g., Random Cut Forest), enriches with state/context, then invokes agents. Bedrock Agent Core provides managed agent runtime (session isolation), short- and long-term memory with extraction prompts, tool/KB integration, and rich observability (sessions, traces, latencies) to move from POC to production. Single “do-everything” agents become slow/expensive; better to compose specialized agents (e.g., analyzer, summarizer, router) and coordinate via SQS/SNS.
  
  Olympics.com case: huge data volumes—3.2 PB of user actions (325B users), 4.6M play-by-play events, 130k stories—flow into Kinesis (now using Kinesis On-Demand Advantage for elastic scaling). Legacy play-by-play pushed to S3 triggers SQS/Lambda. Flink detects traffic spikes (anomalies), deduplicates, and emits “moments.” Previously, Lambda handled everything; for Milano Cortina and LA 2028 they’re shifting heavy lifting to Bedrock Agent Core: agents include an “interest scorer” to validate anomalies, enrichment/summary agent pulling context from multiple sources, and channel-specific deciders to tailor outputs per market/channel; SQS links agents, outputs to Slack/visualizations/other systems. Agent Core observability shows traces, error rates, per-step latencies, and memory usage (built-in RAG-like context) to tune models (e.g., avoid unnecessary frontier models). Demo showed real-time anomaly detection on replayed Beijing traffic (USA vs Finland hockey), with AI-generated explanations and goal alerts; traces revealed an Anthropic call as a latency hotspot (~6s). Key changes vs Paris: elastic Kinesis, delegated agent logic to Agent Core for better dev experience and reuse.
  
  Takeaways: streaming is a must for agentic AI; your data differentiates AI outcomes; familiar messaging patterns (SQS/SNS) suit agent coordination; Bedrock Agent Core helps move from POC to production with memory, tool/KB integration, and observability. Resources include courses and sessions; feedback encouraged.
keywords: streaming, Kinesis, Bedrock Agent Core, anomaly detection, SQS
---

Good morning everyone and welcome to AN 310. This is powering your agentic AI experience with AWS streaming and messaging. My name is Mindy Ferguson, and I'm a vice president of technology at AWS. I lead our data streaming, messaging, and collaboration teams, and if you wanna know what that means, it means I get to lead great services like SQS, SNS, Kinesis data streams, Amazon MSK, AWS Clean Rooms, and so many more. I'm excited today to be joined by Francisco Murillo, a senior streaming solutions architect at AWS, and we have an incredible customer speaker with us today. This is Chris Jackson, the head of digital data and analytics for the Olympics. Dot com. Now someone told me earlier this week, what do you mean the Olympics? Like the Olympics Olympics, and that's right, the Olympics Olympics. We're 8 weeks away from Milano Cortina, and so we're really excited to have Chris with us today. Thank you very much. We're gonna hear from both of them as we go through the day. All right, so here's how today is gonna go down. I'm gonna start with some of the fundamental concepts of streaming, messaging, and agentic AI. And then this is a 300 level session, so we're going to move very quickly to where Francisco Murillo is going to dive deep into some of our key design patterns and as I mentioned, we're going to see a real life use case that Chris Jackson will bring to life using data from the Olympics. And make sure this is not the session you decide to leave early today because there is a demonstration at the end of this presentation. And I can guarantee you you will never look at the Olympics the same again. It's quite fascinating. These are really exciting times in the world of data. It should be no surprise that the data landscape is experiencing a fundamental shift as AI takes center stage. According to the Harvard Business Review, 89% of chief data officers say that their organizations are moving forward with AI initiatives. But more than half of those same chief data officers say that their organization's data foundation isn't quite ready for AI now at AWS we work with a lot of customers and we see that customers who are the most successful in their AI initiatives are ones who have gone back to the basics and have really worked to strengthen their data foundation, and as they do that, they normally find 4 key challenges. The first is that team collaboration is often disconnected. You can have data scientists, analysts, and engineers working with different tools in different locations, and often they have to switch context and that can hamper collaboration. The second is that data fragmentation is still a very real thing. Just like people, we have data in very different locations in different formats, and even within an organization we can have data that's siloed, making it really challenging to create a cohesive data foundation that supports both AI and analytics. The 3rd challenge is that end to end data governance supporting AI and analytics requires using new tools and new approaches, and the 4th is that it's really challenging still to build scalable and performant infrastructure that can flex with the demanding AI workloads while still maintaining efficiency. We see customers choose streaming data technologies not only as a way to address those 4 key challenges. But they're also realizing significant additional benefits from using streaming. The first is they can now capture and analyze data the moment that it occurs, and that means they can deliver instant insights and make reliable, predictable decisions. The second is they're able to use real-time data to help filter out unneeded or unnecessary data, and that's helping to remove costs. So now I can filter out unneeded data before storage and processing and lower my infrastructure costs. The third is that they're using real-time data to power AI with fresh, high quality data, and that's driving automation. And the 4th is that real-time data is allowing them to keep their systems completely in sync, and this is powering the most complex workflows. See, real-time data is the foundation for modern AI, and I wanna walk you through this evolution. How many of us here have actually built a rules-based decision system. Yeah, I think I've done this a few times in my career actually, um, and not that long ago, and they were great for predictable scenarios. But they are also also rigid, and they require manual rule creation. So then we advanced over to machine learning where we were now able to not just classify or Read rigid rules, but we could learn patterns from our data. And this changed how we were able to move forward. The next leap came with generative AI, and with generative AI, now we're not classifying and predicting data, but we're actually able to create new content like text or images, and now even creating code, and this is fueling automation. But now we're in this world of agentic AI, and it's the most powerful evolution yet. See, agentic AI really brings it all together. Now we're able to drive real-time decisions. We're able to power machine learning for pattern recognition and create dynamic content with generative AI, but we're also able to power some of the most complex workflows with event-driven. Architectures At AWS. We define AI agents as autonomous software systems that can think, reason, and take action on behalf of humans or other software systems, and what makes an AI agent so incredibly powerful is its ability to think iteratively. It can evaluate results, adjust the plan. But it's always going to keep tracking towards the goal. And if you're thinking about an AI agent as something that just answers questions like a chatbot, that's just not where agentic AI is today. It actually solves problems through the process of exploration and refinement. But why is now the right time? We've been talking about generative AI for a while. Why is now the right time for agentic AI? Well, there's really three reasons. The first is that almost every single week, and if you've been here at Reinvent, it's already happened this week, we've had so many new foundation models that come to fruition. We see it almost happening on a weekly basis, and it provides advanced reasoning capabilities, but it's also changing the economics of how we're able to use foundation models. The second is that so many of you have been working over the past few years to get your data connected into AI systems and to make that data valuable, and this is a really key point because AI doesn't work without your data, and that's super, super key to keep in mind. The third is that we're finally seeing so many tools coming into the market that we're now able to democratize access to AI. And we're going to show you that a little bit later in this presentation with Amazon Bedrock Agent Core. So far we've discussed the importance of data and how it empowers AI. I'm going to shift gears a little bit and talk about how streaming and messaging differentiates agentic AI, and I'm going to start on the streaming side because streaming has something that's really cool. We know that streaming is providing real-time intelligence, but the important part about intelligence is that you need to be able to get your insights. The faster the better, and time really does matter. But with agents we can form a closed loop process and this is really fueling the power behind real time data. See, we can gather data continuously, we can process continuously, we can deliver insights. And we can act autonomously, and that closed loop process means that we're always able to learn from our past results, and that's what makes real-time data with streaming and agentic AI a powerful combination. Now in the title of this talk today we talk about messaging services and so many customers say to me I didn't really understand how messaging services can work. How many of you use SQS today in your architectures? It's been around for 19 years and it's one of what we would call the OG services. SQS is an amazing service and it powers your ability to decouple your microservice architectures. But here's the cool thing. That exact same pattern allows you to communicate asynchronously between agents. And we see it in almost every single agentic AI architecture because here's the great thing that all of you know SQS scales without you having to think about it and so it can scale elastically to handle all of your demanding AI workloads and it's a really incredible tool as you start to think about agentic AI. All right, we're getting really close to Francisco coming out here, but I want to talk a little bit about some use cases that we see. Most of us already know that Agentic AI is being used to power customer experiences. In fact, I would hazard a guess that 90% of us today have had an AI-powered customer experience already. And if you haven't, I know you won't make it through the day without one. But Agentic AI is also being used in manufacturing and operations for anomaly detection, predictive analytics, preventive maintenance, alerting, and in fact, Chris is going to tell you a little bit later about how the Olympics is using anomaly detection. It's quite interesting. We also know that AI has been providing us some human benefit for quite some time, being able to speed up some of our productivity. But we see customers also using Agentic AI to reduce human bias. We have a customer who is in the insurance claims business, and they're using Agenic AI to receive text, images, videos, and they're using agents to help process claims. They have a human in the loop still, but the claims agent is able to help them understand and provide an auditible tracking back to why they made the decisions that they made. All right, we're ready to get dive deeper, so here we go with Francisco Murillo, and we're gonna dive deep into some key design patterns. Thanks, Mindy. So now we're gonna see some design patterns for building real-time data architectures with AI agents, being able then to empower your AI agents in order to respond and react to when events are actually happening. As Mindy mentioned, my name is Francisco Murillo. I'm a senior streaming solutions architect at AWS, and what that means is that I help customers build, design, and scale the real-time architectures using AWS services. So for the today we're gonna be eva evaluating a real use case which would be anomaly detection. This use case is not really just bounded to any specific industry as it can relate to a financial companies, gaming, manufacturing, and even e-commerce. So when we think and as mentioned, as Mindy mentioned, organizational organizations are processing large amount of data. However, that leads to specific challenges that they may have during that process. First is data visibility. Sometimes we're not even able to detect whenever there's an anomaly if there's a spike or search or drop of our transactions, user events, or a specific information coming from our data sources, and by the time that an anomaly has happened and we are able to realize it, it has already impaired our systems. Second, this can also then lead to operational inefficiencies as we're not able to detect and react, we have then to prepare a response plan after everything that has already happened and has affected our company or our applications. And then lastly what this can also then lead is for limited context awareness. Sometimes even if we're measuring the right stuff, we're missing key information about what systems were impaired or how this actually relates to our business. Without context, being able then to react to an anomaly, it just becomes guesswork. So at the end what we see is that this leads to increased operational costs. Reduce customer satisfaction and lastly potential revenue loss either through downtime or even then just customer satisfaction in terms of how we're able then to respond and provide a service at scale. So let's take a look at a high level architecture. How does this actually then looks behind the scenes? First, we'll have our data sources, which can be databases, it can be IOT devices, it can be transaction logs coming from a website. And we will ingest them in real time to a streaming storage. The reason for this is that we need to be able to process and store this data at scale as we then have many consumers being able to process, react, and in this case detect our anomalies. Once we have detected an anomaly, what we will do is use this with an agentic AI application in order then to provide a necessary context in order to make the right decision. The simplest use case could be once an anomaly has been detected, do some enrichment, do some research, and send an alert either to the specific teams that need to make a decision, but this could be blocking a specific user or even then making high lever or more important decisions. But how can we then build this with AWS? Here is where AWS streaming messaging service come into play. With those, we provide simplicity, durability, and scalability in order for you to be able to react to your data. With AWS streaming and messaging services, customers can move at the speed of their data. Our services are, are, are organized around infrastructure, movement, and processing. First we have for streaming storage our native cloud service for Amazon Kne's data streams and our open source manage service for Apache Kafka. With those we're able then to process and store all of our information as they're constantly being fed in real time. With Amazon Data Firehose and Amazon MSK Connect, we can easily deploy and write data into our data lakes and into our data warehouses. And then lastly for stream processing we, we, we have our managed service for Apache of Flink that provides the easiest way to react and process data in real time. But now let's just dive a little bit deeper into each of those on how they can relate to our gentic AI applications. So for streaming storage, as we mentioned, we have Amazon Kinesis data streams and Amazon MSK. Well, the reason for this service is that they provide durable, scalable storage for our streaming data, allowing us to write in parallel from many producers as well as reacting and consuming with producers. By doing so, we're able then to provide continuous information to our agent to be able to have all the necessary data that they need when they need to make a decision. We can use it to store our raw data before we do some processing, such as doing some embeddings or enrichment from multiple data sources and which later on then provides our agents deterministic reasoning. So if we go back to our anomaly detection use case, we have our data source and for once we have received all of those events are they happening, we can then store them into Amazon Kinesis data streams or Amazon MSK. So once we already have that data available, what do we do with it? Well, in this case we process it and we now we need to be able then to detect and react to an anomaly detection. Which for which we can then use Amazon managed service for a patch of link. In order to provide real-time decisions and real-time enrichment of all of our data sources, one of the key benefits of using a patch of Link for stream processing is its broad set of connectors, so you're able then to not only process data from our streaming storage, but you're able to read from your data lakes, databases, and so many more. By doing so and enabling its built-in state, we're able to build our own contacts and our own prompts before having to activate or invoke our agents. So going back to our architecture, we can then leverage Amazon managed Service for a patch of link to detect and react to our an anomalies. We can do this either as Mindy mentioned, as a rule-based system. So whenever we set a specific threshold, we do some aggregations such as number of views, number of users, and provide an anomaly event. Another way to do this and as we see companies do so is using machine learning. So using models such as random cut forests or even then having some models be able to be invoked synchronously, we're able then to detect those anomalies in real time with the lowest latency possible. So now once we have detected our anomaly, what's next? Well, in this case, now it's time for us to build and deploy our AI agents. How can we do so? I'm gonna ask a question just to see a raise of hands. Who here has heard of Amazon Bedrock? I'm not surprised. So with Amazon Bedrock is the easiest way to build custom agents. We provide open source as well as our own internal models that you're able then to evoke through a single API. And as Mindy mentioned, as we're going through reinvent, even more foundational models are being added now. Bedrock includes a new service or a new set of features called Amazon Bedrock Aging Core. Which is the easiest and simplest way to deploy agents at scale. So you might be wondering what's the key difference or why do I actually need to care about deploying agents at scale? Well, because from moving from POCs and demos into actual production ready agents, it's quite a hassle. We need to maintain repositories, dependencies, provide endpoints, as well as having to wonder how our agent is doing, and it can become quite challenging in order to understand everything happening behind the scenes. And all of these gaps are provided with Amazon Bedrock aging core. It provides its own dedicated runtime for session isolation. It provides short and term and short term and long term memory for context awareness so we do not need to worry about our agent forgetting what we were just discussing but also we, uh, this is quite interesting, we can provide extraction prompts so that we can tell our agent what key information from our session or from our evocations is actually important and doing so without having to manage any kind of databases. And I would say that the most important feature that I believe that we see from customers, uh, stories with Amazon Bedrock Aging core it's its observability. Because with Bedrock Agent Corp we're then able to actually monitor and track all of our agents, our sessions, our traces, and actually see how our agent is behaving behind the scenes and how long it actually then takes through reason and iterate as it's making a decision. So with that we can then have our end to end architecture be able to detect our events and do some agent invocation that can be doing some research actually verifying that this is an anomaly and sending an alert. But what we see that happens is that agents start focused and small. We have all done that POC that looks amazing. It's able to respond precisely what we want and in this case we have just a set of tools, so it's quite specialized. We're able then to analyze anomalies, validate, and take some specific actions. That's amazing, but we then see some success and we start adding additional tooling. We may add some knowledge bases in order to provide more specific responses to specific anomalies. And then as well we go from version 2 to version 3 with more knowledge bases, more tooling, and if we take a single agent too far. Well, we won't be surprised that this will lead to challenges actually then maintaining that single agent can become a nightmare. We have to write specific a prompt in order to avoid hallucinations in order to detect or avoid having tools being misused. And as doing so, our agents will now be taking more time to respond. They will become more expensive, or we need to use frontier models in order to provide the actual response that we're hoping for. So How can we fix this? Well, using multiple agents. We can then have specialized agents being able to have a set of tools or a set of boundaries in order then to just focus at their task at hand. But as we then move forward, we might be wondering how I can have all of these agents working together. There are specific protocols that allow you to do this such as A2A. But what will happen is, what, what we would wonder is what happens if an agent fails? What happens if we are talking about scale and we need to have multiple, multiple agents working in parallel. So how can we fix this? Well, Mindy actually gave us the answer shortly, which would be with AWS messaging service for multi-agent communication. And for this we can leverage Amazon SQS or Amazon MQ. Which would then facilitate agent to agent communication as synchronously and allows us to decouple our agents as they can then work in parallel. So effectively what we're looking for is, and we're all familiar is going back to moving from a monolith agent or an application to seeing agents working as microservices, being able to react as synchronously when an event or when the agent has finalized to to making a decision or sending a response. So now if we then just let's say we modernize our application. We can have our stream processing application right into a queue. Where we can then have lambda functions invoke our endpoints or our specific agents. Be able then to make specific decisions and work collaboratively. But now enough about AWS services. Now it's time for us to see how this actually looks for a real use case. So I want to introduce to the stage Chris Jackson. There you go, Chris. Thank you. Thanks. So I'm gonna talk to you a little bit about how we use this in the Olympics. Uh, to introduce myself briefly, I work on the digital services for the Olympics. Within that, I lead teams responsible for AI, artificial intelligence, uh, analytics, and a bunch of data management topics. I've got a mixed background in tech and business topics, particularly around sport. And accidentally I was the first person to talk about big data on Twitter, if anyone remembers what that used to be. Um, Paris in some numbers, some big numbers. Perhaps the one on the top left is a little less impressive to attendees of Reinvent. I think the conference this year is almost 4 times that size, but still 24,000 media personnel to bring an event to the public is a pretty large grouping of specialists from one industry. They're using over 1200 cameras. If we talk about venues, there's, I think, 6 venues in Reinvent. We had 39. And the 32 sports that are across those is definitely bigger than any other sporting event. Two is unusual, one is the normal. So we've got a huge complexity in terms of the scale of our operations. The other thing that is really striking about the Olympics is it's truly global nature. A lot of at-scale businesses can talk about a couple 100 countries and territories where there is serious activity. For me, the exciting stat is 91 countries that won a medal. So really engaged to the level where they can be in the top 3 in the world. Digitally, we're very proud of our numbers. In Paris, we hit 325 million users on Olympics.com, and over 16 billion engagements on social media. Behind the scenes, it's complicated, we're not one organization, and this definitely has an architectural impact in how we create things. Every host city is its own organization, every sport is its own organization, every national team is its own organization, and of course we have a range of other partners, our sponsors and our media rights holders like NBC who bring it to you at home. A lot of complexity there that we have to think very carefully about how to manage. Let's look at numbers around data in particular. We have in terms of the number of users at home who are coming to us around 40x what we would see in a typical month during the Olympics, and as you would imagine, they're a lot more engaged, almost 4 times as many actions. So that gives us a times 150 peak in terms of rights into our system. Interesting to manage. Um, from a read perspective, this is also significant. Although it's only a roughly one order of magnitude increase, doing that alongside the rights has an interaction which we need to be really careful with. And that's because we have around 3 times as many data users coming and querying stuff, and as you can imagine, they're an awful lot more engaged as well. And every time they query, they're querying for a lot more data because a lot more has happened in the last hour, day, week, month, whatever it is that they care about. So we have to work extremely hard to be able to manage these peaks. We have a saying in our team, batches cause calls. And the reason for this is, you know, traditionally you would architect this stuff with, I don't know, a batch that runs perhaps every day, maybe every hour, maybe you get it down to 15 minutes or 5 minutes or whatever it is that your system can run. Because of the size of the load and because of the variability of it, even if you have a superfast batch. That batch can be a super variable size and it ripple effects, creates provisioning issues all the way down through the various parts of the system. So honestly, the only way we know how to manage this, and the way we've managed it ever since we had a data team working at scale, is using streaming systems, making sure that as the at scale data sources come in, they're going through a series of streaming systems, and we're then managing in terms of, is there a bottleneck, is there a latency issue that we can handle, rather than starting to worry about availability or fundamental issues that get you. As an engineer onto a call rather than typing in Slack and dealing with stuff as usual. So we feel like the. A synchronous way that we handle data allows us as data people, particularly engineers, to also work in an asynchronous manner, and we're religious about this. We'll only have a batch for small data, and if we really can avoid it. Um, and what that means essentially is that we scale up, of course, but we don't overprovision because we know we'll be in real trouble if we don't get there. We scale to a reasonable level, and if there's a problem, we scale further in order to make sure the data keeps flowing through, and that keeps our costs quite reasonable as well. So I want to talk to you about a project that we ran in the Paris Olympics, and we're rolling forward from here to the future Olympics called Interesting Moments. Uh, the idea of this was to solve a very specific problem with AI. The problem In a few words, is that we have an extremely long tail in the Olympics. This is counterintuitive if you watch the Olympics in any one country, because you will see a relatively small number of stories, really well told for the audience in your country. But in reality, only about 20% of engagements is with the top 5 athletes, and that's on a sports level rather than as well as overall. So you've got almost a flip of that typical 80/20 Burrito rule, where normally the 80% matters most. So this means that we handle a lot of topics with a large team that creates content, that creates marketing messages, that iterates through, but we may well be able to address 4 times as many countries, and within that increased number of countries in each country, around 5 times as many sports. If we used AI to understand what those messages were and to go a long way towards helping. If you like, content workers with things like clip selection, social posts, writing articles, notifications, live blogs, and crucially with how we assign people to go and cover all of these different sports. It's a tantalizing opportunity, um, and we made some good progress in Paris. Let me show you a little bit about that. So, just to visualize this, we made in Paris, uh, an AI video-driven map where you could punch into a venue from the overall map and see what was going on. This video has jumped quite a long way forward. Excuse me a minute, and I'm just gonna manually jump back on this video. Hopefully that will play from the start. OK Hands off. So, as I was saying, map of Paris, we have some nice AI videos where we can punch into a venue and see what's going on. This was a nice way for us to visualize it, and it clearly is insisting on jumping. So I'll tell you what the video says, essentially, which is that you see as you go into individual areas, a really clear indication that any one sport is being watched by a large number of countries. And any one country is watching a large number of sports, and we're able to start generating already fascinating AI messages around all the things that were going on in that around Paris. And the last bit you did see there in the video was about contextualizing it. So it's one thing to be able to take a moment, put that together and say this is really interesting. Lots of people are looking at it and yeah, there was a goal or a record or whatever happened in that sport. It's another thing to then be able to say this was their childhood dream, or this was as a result of a really challenging lead up to those games. So essentially we're using AI to understand the key moments. Understand why there was a goal, and then understand why that was important, what the human story was behind it. So Mindy mentioned that bringing your own data is a differentiator here. And that's something that we've done at scale in the Olympics, and I would 100% agree with that message. Uh, we have on the small side, over 130,000 stories, which we've got in an Amazon Open Search database. play by play. It's a message every time something significant happens in any one of those 39 sports going on, going on across a couple of weeks, we have 4.6 million of those events. This is interesting. It's in a legacy application, not super easy to deal with, but we were able to dump each of those messages into S3. Have a trigger that fired off a lambda and sent it through the rest of the process. So even with an application that is a little bit old, we were able to transform pretty easily into the streaming world and then handle it in the way that we handle pretty much anything else that comes from an application that perhaps we've been able to build relatively recently. Last but by no means least, uh, 3.2 petabytes of user actions across those 325 billion users during Paris. That goes straight into Kinesis and feeds into our streaming infrastructure. So let's get a little bit more into the solution. And essentially what we're doing with that huge amount of data in kinesis. Is feeding it into a process to anomaly detect. When it gets over a threshold, we're then starting to look at whether there's an interesting peak, and every single one of those peaks is an anomaly. It's a message which feeds on down the pipeline so that we can turn it into a moment that we can use further. So here's a real demonstration of that. Here's our actual traffic, um, moment by moment, um, and this is in one country, and we're able to jump onto one of those points and immediately get an explanation. The AI has written that text. The AI has worked out where the in and out points were. This particular one was about Simone Biles, um, helping the team to get to a point, and you see that moment there. Semi-finals do well for us, and here's the USA men's basketball team. In a key moment where they were struggling to come from behind, um, and each of those points we're able to explain entirely with AI. That's great for our analysts, what the hell happened there that caused that peak. It's even more interesting for content teams to understand how we work with these things. OK, that was a lot of talking in a level 300 session without an architecture diagram, so I'll, I'll show you that now, um. What you see here is essentially what I've just described, put into components. So that 3.2 petabytes of user actions across the top, feeding through into the central branch. Similarly, the play by play, into S3, into SQS into a lambda, and again feeding across. So we're triggering a pipeline either because something interesting happened or because there's a peak that we perhaps don't understand yet that suggests that our audience thinks something happened. Probably. Um, that feeds through into a lambda, and these are architecture diagrams that I'm seeing around a lot these days. Um, hopefully a lot of streaming services and a lot of those orange lambda dots where people are implementing AI functionality. And that's where we were at for Paris. And that lambda was doing a lot of heavy lifting in the middle of the diagram. So let me give you a really honest evaluation of what that was like. First, we were super proud to have something which we could call agentic just about in July 2024. As far as I can see, the term was only coined in May 224. So we did well in terms of having something which was really doing more than a chatbot and creating real intelligence over a couple of months. It worked at near real time, it worked at scale. The evaluations, both quantitative and qualitative, were extremely positive. The challenges Francisco, um, hinted to, well, he talked about it quite a lot, this idea that we've really got to create a good developer experience. And put simply, my developers don't like me very much already because this was very challenging to build and maintain. And the reality is we need to do more. We need to add complexity in terms of extra data and in terms of supporting a lot more applications to really get the business value out of this. So it's very clear that keeping on building this through lambdas, maybe we split it up, is gonna get us to some of these microservices problems that I'm sure a lot of you have experienced before. Um, and we really, really need to avoid going to that place. So We've got an Olympics coming up. In Milana Cortina. It's a couple of days plus a couple of months away. The team back in Europe is working extremely hard on a rehearsal this week to get everything configured, to get everything working, to iron out the final problems. And what I want to do is talk to you through a bit what our approach is for Milano Cortina. This will be an approach we will then carry forward towards LA in 2028, not far from here in a big event in the summer. So this is a really key jumping off point for us, and we've worked very hard with Francisco's help and others' help to get to the point where we've got an updated architecture for this event. So Backing off a little bit from the architecture and talking about the conceptual, these are some of the main applications that we're gonna have driven by AI in Milanna Cortina. Let me zoom into two of those. We have a bunch of work to support people creating content with agentic tools that help them to do their job faster and better. And we also have this interesting moments application, which needs to understand the Olympics, essentially. Bunch of different functions, you'll notice that a number of them are overlapping. So immediate advantage that now we have a single agent pool that across applications is able to create a bunch of different agents that can be knitted together in different ways to do very different jobs across the enterprise that we run. That in and of itself is a massive advantage. So here's our architecture. It's pretty much the same. In the first bits. One important change is actually I showed that numbers chart to Mindy a year ago, and my comment at that point was we've got a big peak to manage, streaming certainly helps us, but fundamentally we're having to provision all of these different services. Largely upfront through traditional provisioning, and that's particularly the case in kinesis. Yeah, we don't provision quite to the maximum peak we imagine we can handle that, but we do need to provision upfront. And we didn't feel that the on-demand solution was really designed. Especially in terms of pricing for the scale that we've got. So one change we'll make is to use on-demand advantage at that first bit, which gives us the ability to scale despite the massive amount of volume that we're putting through. Other than that, most of this is looking more or less the same until you get to that lambda. It's still there, but it's now delegating most of its work up the top to the purple bit. To Agent Core. So let's zoom in and now we're just looking at what's going on within that purple bit, um. We've got here those interesting events going in at the left hand side. We've got SQSQs knitting together a range of different agents and finally going out to the endpoint. So there's an interest scorer agent which is looking very carefully to understand whether this anomaly is truly something that we should take seriously. Sometimes it might just be a coincidence that this that's happened and we can't find a strong enough explanation for that level of anomaly to be able to carry it on through the process. Enrichment and summary then takes a bunch of different sources and makes sense of that moment, adding in context and describing it in a way that makes sense to humans and adding a few other parameters that help the workflows further down. The place for us where it gets really interesting is around our challenge of getting out to many more different parts of our organization, so that they're all getting benefits from this. And what's quite beautiful is that we're able to have what we call channel decisives, which for the folks working in that channel, implement the logic they care about. For some of our channels will say, if it's not a market of this scale, we unfortunately can't deal with it. So take out all of the markets that are smaller than that. Others will want to reprioritize in particular orders. Others may have particular times of the day when they can handle these things and we're able to organize it like that. Others may want the information presented in a particular way so that they can copy paste that into an application or even feed it directly in. So that used to just be Slack, honestly, a lot of automatic Slack messages, which is fine, but. Tends to involve then further work downstream. Increasingly then we're feeding in directly to visualizations, directly to other systems so that we can create all of those things increasingly automatically further down. So, without any further ado, let's look at a demo. Uh, one word about this demo. Every, every job I've worked in previously, if you're testing something out, you're probably testing it on something approximating production traffic before you put it into production, so that you can see how that works. Our challenge is that our production traffic this month is nothing like the production traffic in February, uh, both in terms of scale, but also in terms of shape. So, what we do with some modifications is replay the previous equivalent games, which in this case was Beijing, just over 4 years ago. So you're going to see some information about Beijing, but essentially with the Milanna Cortina implementation that I've just described. So here we go, and I thought it would be interesting to look at a USA example. This is uh in the women's hockey versus Finland. Um, and you see the traffic down at the bottom there. Um, let's dive in and look at the Flin application that is doing that anomaly detection, first and foremost. If you use Flink, you'll be familiar with this interface already. Essentially what's happening is we're generating moments up at the top of that flow, in the bottom, we're doing a bunch of deduplication. To make sure that even if it's a local peak, it's genuinely interesting. Flint gives you out of the box a bunch of instrumentation, which helps you to understand how those various bits are working and make sure that you debug that pipeline. Um, so, look, there's been a penalty. Fancy that. And if you look at the traffic down at the bottom, I think we probably will see a peak up in it because certainly something interesting's happened, and as if by magic, yeah, it, it's going up there. So here's the anomaly. We're about 40%. Up in terms of traffic on that particular moment versus what our expectation was. And you see the traffic going back down there again, but a pretty clear signal that the Flin application certainly agreed with that something interesting is probably going on there. You do see another anomaly that's been created meanwhile, but we've created one moment. So this is the uh agent core flow now, taking that, deciding it's important, unlike the other anomaly, and summarizing it with a bunch of different fields that we can use. OK, now there's been a goal, as you'd expect, that's also an anomaly, um. So the system will process that and while that's happening, let's look into the agent core part of it. So you see the various agents there that we discussed in the architecture diagram. Um, and what you get is a bunch of really interesting information so that you can understand how well that's all running. So pleasing to see a 0% error rate at this point. That's not always the case. Um, and some pretty good other statistics. As and when it tips up to see something different, we will notice that. Memory's key. The way I look at this is it gives you essentially in agent out of the box, rag. And it's particularly good when the agent is collecting a bunch of information with different tools, but then that comes into a local store that we can use immediately in our prompts so that it's contextual aware without us having to go implement a database connection and a bunch of rag there. So very important in terms of how that works. Um, for us, oh, yeah, just to say that it's doing it between 100 and 200 milliseconds, so it's working pretty fast, and latency is super important for us. We want to get those messages out. You're pretty frustrated to see the anomaly without the description, and that's the same thing that our editors feel. Let's get that done as soon as possible. So let's dive in a little bit deeper, um, and we see in some components that there is an error rate, something that we may well want to investigate, um. If we look at how fast this is running, um, we're gonna probably start to see some issues in that as well. Um, so let's go into one particular one, and yeah, that's running at just under 10 seconds. We'd like that to be a lot faster. Um, and you're able to see this across each different session that's been run. Um, so you've got the, uh, various different hockey games. So we can select the one that we want to look at. Maybe we had some feedback, but one was running a little bit slower. Um, and now we're diving into one particular trace. So this is something you definitely don't get with a lot, without a lot of coding in that custom-made lambda, is you can go in, you can look at the kind of graph of how it's going, um, and I love this graph because I can go in even as a manager and understand why something might be running a little bit slowly. Over on the right, you see an anthropic call that's taking almost 6 seconds. So that may well be a particular part of the reason here. I suspect we're ending up using a frontier model, as Francesco mentioned, when we don't really need something that fancy. So another anomaly came through there. A goal. In fact, there was a goal just before we went away, a second one has come, and as you would imagine, we've got a really interesting description there too, of exactly how it was generated. So, in short, This thing that we did in Paris, we've been able to upgrade significantly because we've got Agent Core in place as a framework that allows us to trace it, understand it, and make sure that we're creating an experience for our developers which encourages code reuse, gives a bunch of tools out of the box that just speeds everything up, no end. So that's Milan, but let's cast our mind forward to LA. Um, I'm sure it will be of interest to a lot of people here, and while Milan and Cortina is super important to us as the next event, the real challenge is gonna be a few years after that, um. I think our needs are first and foremost streaming that gives us great peace of mind. We're able to manage these applications very smoothly. We do need to onboard and scale across a really complex range of organizations and the regulatory part, especially with AI but also across rights, is a real complexity for us that we also need to find ways of managing. Alongside those three, if you like, core challenges that we think about, I think Agentic is a really interesting, um, foil. It's giving us an approach that scales AI across many more applications without taking us to a place where it's slow and continuing to work in real time. Thank you very much. Thank you so much to Chris, not only for your partnership, but, uh, Chris was great in telling us last year at here at Reinvent what his needs were and so many times we talk with all of you as customers and we really do wanna hear your needs. Kinesis On Demand Advantage is available to all of you, by the way, but it really does help with your scaling. And I'm super excited to see it powering the next Olympics in just a few short weeks. Here's some key takeaways that we want you to think about today as you, as you walk away from here. The first is that streaming data is not a nice to have in the world of agentic AI. It's actually a must. The second is that your data is what really makes the difference when it comes to any form of AI. The 3rd is that the same messaging services that have powered your architecture for years allow you to asynchronously communicate between agents, and it's a wonderfully extensible pattern in your architecture. And the 4th is that you can build and deploy agents moving quickly from proofs of concept into production with Amazon Bedrock Agent core. Now, if you want to dive a little bit deeper into agentic AI. Here's a QR code to help you get started. Uh, we offer more than 1000 free digital courses, labs, and immersive training experiences. I'm gonna pause here. I still see some cameras up. Um, if you'd like to learn more about agentic AI streaming and messaging, we still have some sessions here at Reinvent. Um, now, before you leave today, I have a question and I have an ask for you. As part of your reinvent app, you will get surveyed. I will tell you your survey feedback really does matter to us because we want to create content for next year's reinvent that applies. So like your favorite ride share driver, if we've knocked it out of the park today, please give us 5 stars, but give us some feedback as you. we really do appreciate it Chris Francisco and I will be outside of this auditorium. We're happy to take any questions. I really do appreciate all of you for coming today and for your tremendous engagement. Enjoy the rest of your reinvent and thank you.
