---
video_id: Jzhp3ppiaIk
video_url: https://www.youtube.com/watch?v=Jzhp3ppiaIk
is_generated: False
is_translatable: True
---

Awesome. Uh, I think, I think we're, we're starting a little early, so I just, I just wanted to just to see if you can get a little energy in the room, um, and, and, you know, myself and Janek, we have an awesome set of, uh, slides and awesome set of demos so I could show you. We're actually gonna build some stuff together here. We usually don't do that in a breakout session, but I think it's gonna be really cool. Um, just a quick show of hands, how many of you are developers? Wow, that's a lot of developers. Awesome. So that's good. That's a good part because you're gonna enjoy the building part here. Um, how many of you have heard about the like the two really big transformational launches in Lambda, this reinvent? OK, a few hands, so I think that's gonna be useful to a lot of developers. Uh, how many of you are engineering leaders? OK, lots of you are engineering leaders. Awesome, awesome. So I think there's, there's gonna be a bit of a mix of both things here today. Um, I'll introduce myself in a second. Um, there's, there's gonna be a little bit of a mix of, um, lambda strategy or serverless strategy all up. What are we trying to do? Um, I'm gonna actually share, I, I, I mostly speak to a lot of customers this week. I'll share some anecdotes about that too is, uh, you know, every reinvent has a bit of a surprise. I think this is my 9th reinvent, um, so why don't we get going and I'll, I'll share my stories with you guys and we'll, we're gonna build some cool stuff together. So, OK, we're talking about the build, building the future with AWS Serverless, and we've actually played around with the, with the title a little bit. We're saying we were gonna talk about the future of Serverless, but I think one of the things we wanted to share with you is what is our strategy, how we think people are gonna build in the future, and why Serverless is so key here. Um, I'm gonna introduce, let my partner introduce himself, when, when he comes up to do the, the first of the demos, but I'm Osman Khalid, folks. I'm the head of serverless compute. Uh, I've been with AWS for almost 12.5 years, and I started my role, uh, with a little known service called Autoscaling and auto scaling groups, which was a precursor and it's still the thing that powers, uh, lambda, ECS, EKS. Um, and I've been with the ser in the server space with Eventbridge step functions. Uh, these services are under me as well. Uh, at AWS since 2018, 2019. So it's been quite a few years with the, with the team here, um, and still going strong, and my passion always has been developers. I've been a developer myself. If you were to Google my name, you'll see I made a break dancing puzzle game that not a lot of people bought, but, OK, it was a great learning experience. Uh, so I know I'm not a break dancer in case you're wondering, um, so. That's me, uh, and why, why you should listen to me is basically because this is, I'm the guy who sets the strategy for Servius, and, and, and Jennek and I work really well together to work through this, and we, that's why we want to share with you. We wanted to share with you in this for the next hour the things we've obviously done and how they connect with that overall strategy and what we're, where we're taking Servius because Servius, you know, we celebrated 10 years of lambda last year. Um, but it really is, it's at an inflection point, and I think all of us, we're all in technology, lots of developers, lots of engineering leaders here, we're all seeing this inflection point in technology, kind of like what, what the Internet did in the early 2000s. I was a little too young for that, although my gray hair speak otherwise, um, and, but I think with AI, and look, I'm not gonna talk tons about AI and I'm, I'm, and I'm still learning how to use it myself and. And no, I don't believe it's gonna change, you know, all our jobs, but I think it, it already is accelerating us, so I'm gonna share some of the anecdote anecdotes for that. So I'm gonna set the stage for what the strategy is and the agenda. We have this build a challenge, so we're gonna build a few things. Janek is gonna kick that off, and then we'll, we're gonna talk about some of our other innovations which were launched a week before we invent. So you might have missed those as well. And then we kind of recap and kind of share with you where we're going next. That's what we're gonna cover today. Sounds good. Yes, nodding heads? Awesome. Alright, so, uh, I love this quote. Uh, you know, I, you know, we all the engineering leaders all here and the engineering and the engineers themselves, you know that changes is constant, right? But my favorite quote at all change is about change, and I actually found that the, who actually said the original one is PJ O'Rourke. He's a journalist, a US journalist, um. And look, I already kinda signaled this to you folks. We are going through change, right? And, and kind of like the big takeaway for me talking to customers at Reinvent, especially when they, when they're talking to me about Serverless and the future of Serverless, is really the maybe their their old platform strategy, the old way of doing things is just not working time and time again when I hear from developers, developer, development leaders, platform leaders. Is that there's an even bigger struggle with the engineering teams between the engineering and platform teams now because the engineers just wanna try things when the cost of trying something has just plummeted, you can, and I'm not talking about vibe coding, I'm talking about real production engineering here, and that's what we do at Lambda and then Serverless as well. We have, and this is some of the stuff we're gonna show you how we were able to accelerate that because we, we use AI as a, as a tool, as an accelerant. When things have become so much faster, if your deployments, if your operations, if your patching activities are getting in the way, if you've just kind of inverted the, the entire SDLC, frankly, and that's not just at AWS, multiple customers are seeing the same thing and that's why I had more so more, more customer conversations this year than uh in the last few years simply because. You know, people are coming around and saying, yeah, we had a community based based platform, but like what can we do more with Servius because our developers just wanna go faster and faster and they don't wanna have all the liabilities of, of managing the code. So really what what I'm really trying to say is that we are at an evolutionary time. We all feel it. I don't have to, I don't have to convince it whether what the final stage of this is we don't know. I certainly don't know and I'm not claiming that I know, but we are going through an evolutionary period. And so one of the things that, uh, as which is a key part of our strategy at Servius, we, we always focused on developers. Maybe we didn't focus enough on the developer experience, but we have, so, and we, that's a key pillar of our strategy so far, but. It was all about speed and evolution and creating evolutionary architectures and I'll show you in the architecture and what I mean by that, but if, if, if we're living in a in a in a time and we are where things are changing rapidly. If your systems, your processes, your people are able to evolve, those are the ones that are gonna be the most successful, the companies who actually are able to evolve and transition are gonna be the most successful, so why not your architecture as well? So for I didn't check how many of you were system architects, but I'm guessing a lot of you, a lot of the engineers are systems designers as well. You all know that there's no such thing as the right architecture, right? It's only the right trade-offs, um, witherless. Yeah, you have a, you build highly evolutionary architectures, but just to, so I kick off and get into the early part of the conversation, I'm not here to sell you anything either, so I'll be, I'll be as balanced as possible, obviously I'm a, I, I, I'm very passionate about the space, but there are trade-offs if you have a high, highly, highly evolutionary system, you're, it's very hard to manage change. It's very hard to observe what's broken because things are more decoupled because, uh, are, are, are changing very rapidly. Um, for some of the platform team owners, one of the things that's weighing on their mind is control. How do I, how do I govern? How do I make everything compliant? How do I, while, while my developers wanna move faster and faster and faster, so yeah, there are trade-offs that we have to manage. But the final thing that I always tell when we have this conversation is like look evolution is is not a new thing in in tech like just because Gen AI has captured the hearts and minds of so much and it has been so disruptive for us, the engineering community here. Speed always has mattered in business, and the, the fastest you are, the the more likely you are to win. So let's go to about the server list. I don't know if this is the biggest secret, and Jane believes it's the biggest secret in server list, but look, server, serverless was always running, you know, I, I operate a fleet of literally hundreds of thousands of servers, bare metal servers too, so those are extra fun to patch and scale. But what we've done is we've created a facade for developers where the developers just don't have to worry about the, the management of the servers, management of runtime, scaling, load balancing, you know, request routing a lot of those things are just taken care of us directly. But so while there's hundreds of thousands of servers in our fleet and from USC1 all the way in Asia and I don't think we have an we have an African region now too, right? So there you go all over the planet. But to you or the developer or to the engineering leaders who are who've built a surveless architecture, this is what it kind of looks like in the in this pic this picture here is simply me trying to play around with an idea of like, hey, I wanna build an agent or build an application that can guide users during national emergencies. So yeah I'm talking a lot of MCPs here I'm talking to a bunch of different uh services that are available I think I have FEMA here etc. What I, what I mean by evolutionary here is that each feature, each facet of my idea is actually one of those, those, these, these horizontal lines you see that see that go through lambda. I'm obviously using an, uh, an AI, uh, I'm using Bedrock to host my models, but if I imagine if I in this particular architecture I wanted to add a new feature, I, I deliberately didn't put it on a slide. I wanted to add a notification feature where customers can sign up to a notification and maybe over SMS or email I can email them notifications or SMS them. Um, if my, my advice from my app, for example, if the sign up, that would just be a branch. I'm not updating a bunch of microservices, that is just a branch and one of the key things, and again I know the tools are heavily evolving even in the last 6 months that they're heavily evolving, one of the things that I, I've seen my engineering teams do is that smaller changes, smaller contexts, smaller files just are so much better, so much faster to iterate on, and the AI is far more right. And, uh, building those things and it is just like one shotting, I mean white coding I think really is a marketing term at this point, frankly, uh, and guess what, all of these things are a single responsibility of lambda functions and super easy to manage and update and super fast and, and Janek is gonna show you some of these things when he comes and talks about it. And look, at the end of the day when you build that architecture, yeah, there were lots of parts to it. There's IAC that you had to, to probably use to actually create and manage the, the, the, the system. But once you have that set up, I like to talk about the ilities. Right, and maybe, maybe you folks have a platform team where there's other humans who go and manage and scale servers for you. They fixed focus on reliability, durability, but that's not actually true, right? Even in the platform team case they don't know your software, so many things that are that are that are around like security and, and scalability all the way go into the software that the developers are writing too. And if you're choosing an underlying infrastructure technology. Then you are responsible for those things, and if you ever start thinking about your ideas going to global scale, you can see how hard it gets. And look at the heart of it, what, what customers love with and why they build, sorry I went too fast, why, why customers love building with Servius is that those ilities that I had in the previous slide are all managed by us. That is kind of like the heart of Servius at the end of the day. It's not about having those servers. It's about having no servers to manage and no infrastructure to manage and then expressing your logic the fastest way possible. OK, so speed is the key thing. I mean, I already set that up like speed always wins and that is our number one goal. Our number one goal has been always how can we be the fastest to market? How can we take an idea that customers have and be the fastest way to do this? And I, I mean the funny. Anecdote I would share with you, uh, many years ago, I think it was 2013, 2014, it was about a year of working on auto scaling. My boss asked me like, Hey, what's the charter for auto scaling? This is before lambda. This is before ECS and Kubernetes, and I was like, Hey, it's the fastest way to go take an idea and take it to scale, and the best idea, let the best ideas win. Obviously we've come a long way from just using auto scaling VMs now, but, uh, this, this statement is at least has been true for me, and as I've been in this space of helping people move faster. And faster because this is what matters. I think this is the thing that that matters all the way from what why customers adopt the cloud all the way to how they build cloud natively to move faster. OK, and I wanted to share, uh, a really quick anecdote. This is a very recent anecdote as well the last couple of months where, um, Cyber Arc, which is a platform engineering, and a team in Cyber Arc, basically built their entire platform on Serverless, and they were able to do this, uh, the automation work that they need to do, uh, and, and, and basically save something like 4 months out of the 12 months it took them to build new services. Um, I mean this code itself is probably 6 months old. I, I would say this number is probably much, much lower now given the way the state of tools is. And again, once something is written. I mean, I think the final, final stage, I, I have a Tesla as well, but then I unfortunately played full, full self-driving for it, which it doesn't do after many years, uh, because at the end of the day it's liabilities, uh, the last LT is liability, and when you serve us, more of the responsibility is on our line of the fence. At the end of the day, myself and engineering teams are the ones who are responsible for the scale and patching and security of your applications versus you doing so. I mean there's still a shared responsibility, but a lot of it is on our side of the fence. And look, it's not a new technology anymore, although I, as we get into the thing, I think we've, what we've done over the last 12 months and launch, launch and reinvent really transforms this 10 year old technology as well, but several this is already everywhere. I'm obviously not gonna go through all of these things. I just wanted to kind of highlight to you how many big names have major applications at scale applications running on lambda today. OK, so let's go. Let's enough talking about like the context of why we're here. I think a lot of people aren't familiar with the feature, so I wanna get into it and have, uh, introduce my, my partner Jana Agarwal, who will come introduce himself, and he's walk us through some actual of some, some actual building things. Thank you Audible. No, Am I audible now? All right, I'll thank Usman again. So, hello everyone, I'm Janna Kagarwal. I used to be a developer for around 8 years, so my perspective on Serves has really been informed by both sides of the equation of being a developer, uh, you know, whose services need to run in production, you know, flawlessly. And now you're trying to build tools and services that developers can trust to run their critical workloads on. So, I like to still think of myself as a good developer, but I'm probably not, uh, but I am a product manager and I lead uh PM for Lambda. So, Lambda has been around for a decade now. And as with any product or technology that manages to stick around for around a decade and more, there's some notions that build up, there's some preconceived notions, biases even about what the technology can do, what it cannot do, what is it good for, what is it not good for, and so on. But then there's always some inflection points that come in, and I believe, I really believe that Serveus is at such an inflection point now. So, what we're going to do next is we're gonna have some fun, you know, over the next 30 minutes or so, we're going to build an application, and along the way, I'll show you some of the capabilities that we launched that allow you to now bring workloads to serverless that you could not before. OK. So here is what we're going to build. It's a note taking application, you know, it's going to have create, read, update, delete, uh, as functionalities. Uh, we're gonna scale the application. We're going to then build new features that our customers will want encryption and decryption of notes, analyzing the sentiment. And finally, for those who really like to write lengthy notes, the generation is not, uh, at least research says the attention spans are going down, so we're going to use AI to summarize our notes. OK. So let's move on to phase one. We're going to build our foundation now, the CRD APIs. And I'm not going to show you my typing speed. What we are going to leverage here is uh vibe coding, as Usman was talking about. And uh a key to uh VIP coding or successful VIP coding is uh is a technology that we released earlier this year. It's the serverless MCP server. What it enables your favorite AI coding assistant to do is to convert your national language prompts better into well-architected codes, which are uh compliant to, you know, you can run it in production very fast. So let us get to it. We're gonna do it in 3 phases, OK? So, the first is we're gonna install our MCP servers, and my personal, personal preference is to also install the DOC MCP server. I found it to be really useful in using new technologies which have been recently announced, and uh I love to hear, and I'm pretty sure all of us love to hear that you are absolutely right. Uh, the DOC MCP server reduces it. If you know, you know what I'm talking about, but, uh, I, I like to use it. In the second phase, uh, we're going to actually write code for our CRUD APIs. Uh, it's going to be a fully serverless architecture, so API gateway for ingress, lambda functions for CRD operations. Uh, we're gonna use Dynamo DB tables for serverless, uh, databases, uh, cloud watch, structured logging, you know, observability is really critical in server-less architectures. And I like to think that I'm still a developer, so I like my types. I'm going to be using TypeScript for this. And then finally comes the build and deployment phases, and what we will see is that the MCP server defaults to using this tool for build and deployment called SAM. SAM stands for serverless application model. We've uh purpose built it for serverless builds and deployments. Uh, it really simplifies your uh builds, your deployments, and uh also enables you, enables you to simplify your local testing phases. So what I've done is I installed the serverless MCP server, and here's a picture that shows you the tools. At the time when I captured the picture, around about a week before reinvent, you know, we had 25 tools. These tools are now available for the AI coding assistant. Some of the critical tools are, uh, you know, it gets guidance in what workloads are good for lambda, what are not good for lambda, how to build and deploy web apps, how to build and deploy event-driven architectures, including your Kinesis and Kafka, ESMs and so on. It also knows how to, when I see tools there to get metric, you know, to uh get metrics for cloudwatch, you know, you have to know which metric, where to get it from, and so on. It helps to uh fine tune that for you. And also assist SSE. Next, we're going to write the code. So, I threw in the prompt that we saw on the previous slide, and in around about 5 to 10 seconds, uh, Quiro, you know, my AI assistant, uh, it's gonna tell me that the code files are already done, and the next step is to build and deploy. So I've magnified some images here for us to examine what it did under the hood. So you see the project structure that uh Quiro or my AI coding assistant made with the help of the MCP server. Uh, you see the template. YAML file, you, that is the IAC assistance that you get out of the box. The package.JSN enables you to simplify dependency management for your application. The TS file is obviously the business logic, and the TS config uh helps you to simplify the build or know knows the build steps to run as you transile the TypeScript to JavaScript. And then there's some error handling which is now automatically built in. So without the MCP server, this was not built in. I don't have a before picture, but the after picture is here. You see there's sufficient input validation. We're doing 3 retries when it comes to writing to Dynamo TV, you know, good practices. Uh, here are some additional best practices built in. Uh, you see the global error handler here. We have consistent HTTP status codes, uh, structured logging with CloudWatch, all, you know, unavailable from the get-go in the V0 of the code that was written automatically. So the code generation process, you know, it's, it's much more compliant to a well architected framework with this. The next step for us is to build and deploy. So I've thrown in the command to build and deploy to my assistant. What it does is, you know, SAM will take over. It will use the YAML files, the TS config files, and the package files to run the deployment for us. It will enforce best practices along the way, you know, we'll magnify this image in a bit. It will show us the cloud formation change stack or the change set uh that shows the delta of all of the work that we're going to deploy to AWS. It will upload the code to S3. In a second, hopefully, that is complete. Here's the change set, and then as we go to the console, we will see that our functions then begin to light up. The council should load any moment now. There you go. So, here are all of the 5 functions that we wrote, and they're right there available for us to sort of serve traffic to. So, here's some uh magnified images. So while building, you know, Sam detected that our APIs had no authentication. It was asking me to confirm if this is really what I want to do and uh since this was a demo, you know, I chose to go yes. Here's a snapshot of the change set that it is walking us through. It is adding uh all of these files, these rules and permissions for databases and functions and so on. So your build and deployment steps are also much simpler with SAM. So if you think about it, uh, I don't know if I spoke more than 57 minutes, uh, but in around about 5 to 7 minutes, we have a fully working back end and the cloud operations, the cloud APIs are deployed fully using natural language with well architected framework, you know, sort of baked in. And when we talk to customers, you know, they do tell us that generative AI has actually sped up the code generation process, but it's not resulting in shipping the actual software faster. That's because the ship cycle has a much more, you know, process baked into it, you know, there's IAC, there's code reviews, and so on, and all of that is not really becoming faster with Gen AI, and that is where Survellus has tried to innovate right across the stack. So we enforce best practices while you generate the code, we just saw that during build steps and also during deployment steps. The key takeaways I want us to sort of focus on here are that uh MCP server helps you to uh generate, you know, best practices sort of code baked in, the best practices baked into the code. And I work with a lot of developers. And their productivity is high, you know, but they are now actually able to ship software also faster. Because the IAC is baked in and uh what they love the most is the code that is produced is of a consistent quality because a lot of people, you know, a lot of customers also sort of complain that, hey, uh, some people just wipe code and then send out a code review without, you know, making sense of what it is. It sort of helps you to minimize those problems by producing a code of uh consistent quality. Alright. So moving on, let's say our application was picked up by some news media outlet. Now, what that has resulted in is a bunch of concurrent traffic, you know, right off the bat. So the way customers tell us they handle this scenario is uh they overprovision capacity. When I say overprovision, meaning, you know, they provision for peak with the, with the understanding that hey, at some point in time, you know, I'll go back in as a developer of that application and try to optimize my, you know, costs by applying the right scaling policies. But this provisioning for peak leads to higher costs and also a bunch of human-led maintenance. And, you know, tomorrow never comes. Um, the, the scaling policies, you know, you're constantly having to optimize, but you find ways to just build features instead of focusing on also on optimizations. So how do we handle that with serverless? So with Serverless, we give you hands-free scaling options. Our scaling rate is pretty much uh fastest amongst all compute choices that you have. So we give you 1000 execution environments every 10 seconds. So if you think about it, if your function's execution time is 100 milliseconds, what you're really getting is 10,000 RPS every 10 seconds, right off the bat without lifting a finger. Let's uh run a load test. So, I've designed a very basic custom load test. Uh, I'm around about a minute in here, and you see that I've literally 700, 800xed the traffic in like 25, 30 seconds. And there is not a single error, there's not one throttle. You know, lambda is able to just absorb it right off the bat, and you didn't have to lift a single finger to do so. So this is the power that Serveus gives you. This is the kind of workload that Serveus really shines in, uh, leads to a lot of happy new users, end users, and all of this without any idle costs. So the key takeaways are like, our scaling rate is really fast, you know, fastest amongst all compute. And let us call this type of a traffic, you know, needlepoint traffic. So, imagine that, you know, you're building an application where, uh, you know, hundreds of thousands of spectators in the stadium have to scan a QR code whenever a goal is scored. You can just, uh, you know, seamlessly handle that. Or, you know, there's a flash sale, and I see a bunch of people wearing shoes of that company, and, you know, you can, uh, a new shoe drops, a new flash sale starts, you know, you can just handle that seamlessly. And uh testing is quite simple because you're literally only testing your application logic. You're not testing scaling at all, it just works. Lambda provides it to you out of the box. And multiple pieces of the functionality in our application, so you, you saw we had 4 or 5 APIs that we built, all of them scale at the same rate independently of each other without affecting scaling rates of anything else. So the, the noisy neighbor problem that you have is sort of eliminated. And this is all without managing any infrastructure. Right, so far, so good. Let's uh move on. So, you know, we're good people, you know, we listen to our customers, and based on the customer feedback, we are now building this new feature called encrypting, decrypting notes and analyzing, analyzing its sentiment. So if you think about it carefully as a developer, the, the profile of your workload here is shifting. It's no longer just CRD APIs, uh, it is a more CPU intensive workload. You know, because we listen to our customers, you know, the feature also achieves popularity. Uh, what I mean by that is the scale to zero aspect is no longer super important for you. There's always some traffic to serve, always some users to service, so there's always, in other words, you know, some steady-state traffic. And I'm loosely defining steady-state here as, you know, with a peak to main traffic ratio of around 2. How do we handle that with serverless? Well, it was hard. So when we talk to customers, this is what they tell us. In this phase of the application, they really want to drive optimizations. They want to optimize costs, they want to optimize performance, pro probably by leveraging the latest in compute, memory, network intensive instances and so on. And they want to do all of that with a familiar developer experience that they currently, you know, use, they like, they love with all of the integrations and with fully surveillance operations, you know, so in different words, they don't want to remove the focus away from core business logic, they want to continue to leverage the practices that they use today but get more choices. So what did they do before this week was, you know, we, we saw that they would just focus on rearchitecting the solution away from serverless, away from lambda. And that was an incredibly inefficient use of engineering time. It dilutes the focus away from business logic, results in an increase in ongoing maintenance costs in perpetuity, and again, incredibly inefficient. So we wanted to design a better way to support such scenarios. In serverless on lambda. Uh, and we're delighted to introduce to you Lambda's managed instances. So the mental model behind this feature is that we want to give you all of the Lambda's sim simplicity in developer experience and integrations and tooling. And marry it with EC2's specificity and flexibility of choices and compute and network and memory that you get. So, with the LMI you get access to latest, you know, for instance, families like Graviton 4, you know, or probably Graviton 5 as soon as it comes out. You're going to get uh the latest generation in instances in memory compute, network optimized. All of this continues to be fully managed as Usman was saying earlier, you know, surveillance is not the absence of servers, you know, we just manage the servers for you. Here we're giving you those options, but we continue to manage the servers for you, so we're going to continue to scale them, you know, patch them, route requests to them, you know, the whole thing. You continue to get the same extensive event source integrations that you used to get with lambda and you still get with lambda rather. And with this uh LMI we're also adding a new feature called multi-concurrency, or the ability to serve multiple requests from the same execution environment, which has been a long-standing customer ask. And uh when you combine this feature with EC2's pricing incentives of savings plan, reserved instances, and so on, your cost really, really optimizes. And we'll see in a second how. So using lambda managed instances is as easy as just creating a capacity provider. When you create a capacity provider, you have the option to specify any choice you want, and again, I, I focus on the word option because this is really an option, optional setting that you specify. Any instance types, any scaling policies, you know, maximin, all of that is configurable, but optionally. You can just be hands-free and let lambda and AWS take care of it. Once you do that, you will create your function the way you, uh, you do today. You'll just configure it to a capacity provider. And then lambda will scale it, patch it, you know, run it, provision the instances, uh, it will pick the suitable instances for your workload and, and drive this continuous optimization loop for the utilization. Now, adding servers to server list, you know, it's a tricky thing to sort of get right. So we did uh try to do deep research here to make sure that the experience is as simple as, as it possibly could. Let us see how. So, I'm on the Lambda console now, and I head to capacity providers, the new feature that lights up. I'll create the capacity provider, give it a name, give it a VPC, subnet, security groups, and an operator role that has access to manipulate my EC-2, and then I head over to the advanced settings where the action really is. So here you can choose your architectures, you know, from Graviton or X86. You can uh include or exclude certain instance types or, you know, just let Lambda pick for you. You can apply scaling policies, you know, a max to cap your costs, or a min to always have some pre-warmed instances available to serve your traffic. You can tag them for tracking, tracking purposes, and so on. And when you head back here in a second, you'll see that the capacity provider is now active. Now, at this point, you know, there is no EC2 instance that has started because it's just the capacity provider construct that you've created. There's no charge for creating this capacity provider. The next thing we have, we, we thought of getting right or we just wanted to get right was the same developer experience that you have with Lambda today. Let's see how we did. So, here's the updated create function flow. You can see that it is the exact same, but there's just one new additional parameter, which is the capacity provider config. And along with here, I wanted to highlight two additional features. One is the multi-concurrency support that we've discussed in a second, a second back. And then there's also the ability to customize the memory to CPU ratio. Just like EC2 instances, you can now choose your memory to CPU ratio on lambda to conform to the compute-intensive or memory-intensive or, you know, general-purpose instances. So you can imagine the classes of new workloads that you can now run on lambda, which you could not before. So what we'll do next is I've, I'll throw in that create config command to my assistant. And I'll then create my function, I'll go back in there, and when I see the configuration tab, there it is, it's configured to the capacity provider. I'll go in, change the memory settings to 4 GB for the function for demo purposes, and I'll go back into that other setting in a second, and I will shift the multi-concurrent setting to 64 concurrent requests from the same execution environment, change the memory to VCPU to 4S2 1. And when I hit save, and then head back to the capacity provider, I'll see that the function is now active. And it is at this point that my EC2 instances will be created. Here's the EC2 console. And uh if you notice uh carefully, you know, the, when I provided subnets initially while creating the capacity provider, I provided it in 3 AZs. So my EC2 instances are also across 3 AZs now. So my instances and by extension, my application is also now finally AZ balanced. And at this point, once the function is active, the EC2 instances are spun up, your execution environments are pre-warmed with multi-concurrency support enabled, ready to serve your traffic. So let's start a load test with our synthetic workload. And uh I'm around about 13 minutes into the load test. I enhanced the load test tool for this. Um, you see, the traffic is much more steady-state. It's still increasing, but still steady-state in, in, in our definition here. And we see that the scale up from 3 is now to 7. We've achieved a utilization of around 25%. The utilization of memory and VCPU and the instances, the underlying EC2 instances, utilization is still pretty healthy, you know, between 15 to 35%. And, and remember, the higher the utilization, the more the cost optimization for you. So really, the, the key aspect here is the utilization is baked in. And now around an hour into the load test, you see the traffic is still steady state, increasing but steady state. There's throttles, 932 throttles to be exact, but again, throttles are good, you know, we can handle it in our code by way of retries or queuing or so on, but there's not a single error. The error rate is still 0. We've scaled up to 21 instances now, and if you observe carefully, the scale up and scale down has both been triggered. Uh, it sort of tracks the CPU utilization of the capacity provider, and we're still at around 25% utilization, 22.5% in this case to be exact. So I let the load tests run for around 90 minutes. And here's a key result I wanted to highlight. So when we talk to customers, a bunch of customers tell us that, hey, they take this technical debt when they provision for peak, when the workload profile shifts. And when they take this technical debt, they don't apply the scaling policies properly or they don't spend time in optimizing it, you know, they achieve low utilization rates. So at around about 6% utilization rate, the, the, my synthetic workload, you know, the load test would have cost me $8.5 but if I improve it to 9%, you know, manually, it would be $5.67. But with lambda managed instances or LMI, the auto-scaling is actually built in. You, you actively have to go in and choose to turn the auto scaling off, so from the get-go, you're optimized, and at the lower end of the scale we're able to achieve around about 25% optimization off the bat. This is literally a 60 to 90 minute test, and in 130 minutes itself, you, you could see earlier that the utilization had reached 28.5%. So, with that utilization, you know, my costs just for the EC-2 would have been $2.05. Now, on top of this, Lambda managed instances applies a management fee of around 15% for EC2 instances. Now, in that 15%, think about what are you're getting. You know, it's uh automatic scaling, you know, provisioning, patching, continuous optimizations based on your workload profile shifting. There is literally no need for you to rearchitect the solution away from lambda. So you're saving all of that time. You have to continue to use the same CICD pipelines, the same observatory tool set, and the same integrations are all available. So, some key takeaways here, uh, with lambda managed instances, you know, the mental model again is to give you the simplicity of lambda and serverless with the specificity and the flexibility that ECT gives to you. And it is simple to maintain, you know, out of, let's say, the 5 functions that make up your application, you know, 4 require scale to 0, you can leave them where they are. The one that has achieved steady state or has a different workload profile, CPU intensive, you can move to lambda's managed instances. We continuously optimize the utilization for you to give you the benefit of costs. There is zero infrastructure management here, and with the new functionality of configuring CPU to memory ratio, you know, you're able to bring in much more workloads that you were not able to run on serverless very easily. And the managed instances, it just works. Usman was talking about the ilities, as he calls it earlier, you know, the reliability, patching, you know, scalability, availability, all of that, you know, AWS and Lambda continues to be, uh, responsible for and all of this with the same developer experience that you get with Lambda today without having to rearchitect anything. Next, we're gonna build a workflow-based architecture. I'm gonna request Usman to come in, uh, show how to build that, and also talk a little bit about our strategy. So, thank you very much. All right thank you Janet. Ah. No, I, I think LMI does this or deserve a clap. It I actually it was really, really cool. So a couple of cool, cool, cool stories. I didn't do this in my rehearsal, so because I would be mad at me. The engineers were actually really mad at his demo. I don't know if you guys saw what he was doing in this demo. He was basically creating these spiky traffic, 1000 TPS for 1 2nd only. And for those of you who operate the distributed systems and uh scale distributed systems, you know this is pretty much the worst workload. So the engineers are like this is, we talk about synthetic workload. This is as synthetic as it gets. But again you can see the results of how the system, uh, scales and works. Uh, it was a fun, uh, fun last few weeks getting it out the, out the door, OK. And the second thing I wanted to connect with you just to kind of connect to what I was speaking to you folks at the beginning. I I talked about trade-offs. I talked about observability. I talked about control. I talked about, um, evolutionary architectures and how you have to use ISC. Let's talk about where we are with the trade-offs now with the, with the, with, with what Judang just talked about. And then I'm gonna show you a couple more things here that we just recently launched. This was launched on Tuesday. What I'm about to show you, um, so one of the, one in the picture, if you, for those of you who remember the picture I showed about that FEMA or res emergency response application. What was the, what was the, what was the challenging thing about it? Your, your application now is highly evolutionary single responsibility. You don't have monoliths. You're using infrastructure as code. And then what Jane showed you with, with the power of AI, uh, actually generating SAM with the best serverless practices pulled in is taking that trade-off away. One of the hard, I mean for me personally as a developer, I, I've shared with you folks, I, I used to be a video game programmer, a hardcore C++ developer. Writing YAL was always hard. It's still always hard. I don't know why I can't do it, but no, no, I don't have to do it. I actually get really, really, really awesome results from our MCP server with the best practices built in. So that's one problem taken away from developers and from infrastructure developers. Second, uh, problem, and we, and I, I'll be again very direct and forthright with you, people used to say, Hey, lambda is too expensive at scale. If my idea ever got big, I will hate it or my boss will hate it if I'm an engineer like because the costs go high. And I was Jenakaki showed you with LMI how we're able to deliver incredible usage and look folks, I, I shared with you that I, I used to run auto scaling. I've created hundreds of auto scaling groups in, in my time at AWS. I don't think I've ever managed to run code on those auto scaling groups simply because I was just testing the service. This is where I can go from infrastructure to code to highly utilized code literally in, in. In a couple of minutes and that is, that is the most incredible thing about LMI. So almost two fundamental things about Serverless which was like, hey, it's expensive at scale, or my, if I have to, I, uh, my idea gets big, I have to rearchitect or I have to deal with ISC and that's complex. We've actually tackled those things, uh, really, really well this year. Now let's talk about a third thing which is, hey, lambda doesn't run long run running workflows or rather long running jobs, and if I ever, uh, have this long running issue, I'll hit in a 15 minute timeout and again I really hate my life. So let's talk about workflows now and look folks, I've been with the workflow services for a long time. Auto scaling was one of the, is still one of the largest users of simple workflow, so a little bit behind the scenes. Um, I, I've, I own Simple Workflow as well. I, I'm the engineering leader for Simple Workflow too. One of the things we talked about, we've been talking to customers, at least I've been talking to customers about workflow for workflows for 12 years. Developers just didn't get it unless you worked at Amazon, obviously, because then we really get it, uh, internally we, we know when you wanna do reliable distributor systems you need a workflow. The thing that has really brought workflows back front and center, not everyone wants to talk about workflows, obviously is AI agents or AI-based workflows. Um, so you obviously there's a ton of interest in this, but look, this is, this is the system we're building, uh, and, and why we wanna do this is because orchestration is important in the type of applications, the new type of applications customers are building, right? So in, in this particular, particular case I'm talking about an enhancement to the, the thing that, that Jenna kicked off where I wanna be able to summarize the note from our no note taking application. So these are some of the steps, right? You'll have to retrieve the note from the storage space. I think in my example is Dynamo DB. You need to have, I, I mean, here's the thing with LLMs they are asynchronous. You are waiting for, for a response from them, and if you wanna scale your LLM, you have to make it more, more asynchronous versus everything is synchronous around it. Um, you, you generate the summary and you need to store the result. Those are your steps that quite literally map into a workflow, right? And look, if you were to write code today, you can run this code on anything, EC2, any, any compute, we can run this code on lambda as, as it is right now. You are now responsible for the reliability of all these steps. If, if you're not using a workflow system, you are responsible for figuring out, OK, when to handle retries, when to roll back things if things have gone, gone, gone incorrect, and you can see, you know, I have manual checkpointing here somewhere. I don't have the line numbers unfortunately, and I'm doing a bunch of sleeps and weights and while you're waiting, especially if you're waiting on lambda. You're paying for the compute Right, OK, so what we heard from developers basically was, look, I, I, I don't wanna write code this way, even if I, if I, it's simple, AI can write this code for me really simply. I don't wanna write this code myself and make it figure out how to do it reliably. Obviously I wanna still just still write code and I wanna use my tools and ID. Um, pausing and resuming is a really powerful step simply because if you, if you look at the most powerful use of AI today. It's basically AI code generation and guess who who where the human is in the loop? It's, it's uh, it's you guys, it's the developers and so it's super powerful to be able to pause a workflow and then resume it, um, and finally I wanna use my favorite programming languages and so look, we, we heard you and now we're introducing lambda durable functions or our Matt already introduced it and so our lambda durable functions that you do is you just simply write code. You write simple sequential code. Well, all code is sequential. Uh, and reliability is baked in reliability retries your workflow semantics are just baked in. Um, right now we support Node and Python. You know, more languages are coming, as I said, the team pushed super hard to get this out for reinvent more language languages support obviously is coming, uh, but Python and, and Node are super popular with Lambda, and we, we covered both the most common languages there. Yes, you can suspend and resume long running operations, and while you're suspending the operation, you're not paying for lambda at all, um, and finally. It's all the, the, the beauty of managing the elities. It's still lambda. And so, yes, you know, you, some of you might have heard of durable executions or, or, or workflows before. What, what this is super unique is that this is a compute service lambda with that reliable workflow stuff built right in. There's nothing else. You're simply going to write a lambda function and I'm gonna show you what that looks like. OK, so in a durable, what durable functions really how what behind the scenes durable functions do is they come up with, come with a very simple SDK. If you choose to, and I'll show you how to do it, if you choose a durable function in the lambda console, the SDK is loaded as part of the runtime for you. Um, they have the ability to checkpoint. Uh, you decide when to checkpoint. By the way, this all the system is built on top of the same underlying system that powers step functions as well. For those of you who are familiar with step functions, a lot of these things will make, make a lot of sense to you immediately, um. The you can checkpoint and step functions. Every state is checkpointed then in this particular case you're writing the code and you decide when to checkpoint and then replay. The idea behind replay is if you're waiting or you're resuming your function or um you know there's a failure and then you wanna retry and replay from failure because that's where reliability comes in. You do not have to replay all the stuff that was already checkpointed. You simply get those results back so you're not ever wasting compute. And so tying it back to what I was saying about long running workflows, we really thought long and hard about LMI as well, and we said, hey, should we allow customers, I mean they're already paying for the full EC2 instance, should we just allow them to run lambda functions for hours. But the whole point of doing that is that you're building inherently unreliable software if you're gonna do that. So we said, hey, can we do something better? And that's how we got, got, got together with the step functions teams and the teams collaborated together to build a, a joint capability inside lambda. So look, these are the durable, these are the things that make something durable execution. So getting started is simple, you know, you choose a function name, but you know you're providing an optional durable configuration. That's how you turn it on. Uh, you mean execution timeout and the retention periods are have defaults, so all you're doing is basically saying I wanna do, I want this function to be durably config configured. And look, I'm gonna actually show you what a running function will look like. So I've, I've already created one of these functions. So what, what they come up with is, is they have a new tab on the console called durable executions. And again, for those of you familiar with step functions, you'll be, you're used to the observability of the workflow. We've taken a lot of things. So I'm gonna kick off my durable, durable function. Don't worry, I'll walk you through the code in a second. And you'll see that there's a new execution that has started, and we'll click into that in a second. Uh, and you'll, you'll see that the steps are described where, where the, you know, the, the system and the workflow has already fetched the, the, the note, it's starting to process it. It's starting to then send it to the LLM for a summarization, and you can actually kind of see the progress. I mean, in, in my demo here, obviously there's no failures, but if anything was being retried, you'll be able to quickly pinpoint where your asynchronous task. Is not behaving the right way and look there you go, everything is done and so these workflows can run run up to 1 year obviously the most canonical use cases short lived transactions which are probably under under sometimes under 1 2nd, sometimes a few minutes, but these you, you absolutely can build human in the loop systems here as well which cover which can which can last a long time. So let's take a look at what the actual code was. Let's zoom in. So the first thing I wanted to highlight again for many of the developers who are familiar with lambda. You'll see that you know lambda has a context object. When you create a durable function, you get a durable context object, and the durable context object has those things I talk to you about which is enable you to do waiting for for or checkpointing, which is a step, and it has structured logging as well for observability to see where you are while the workflow is running. Um, the durability part, uh, is, is again baked in, so I'm gonna actually show you this is the, the, the step for actually retrieving the note from, from Dynamo DB in this particular case. So the code you're writing is, is, is basically the, the step which comes from the context object. Um, the Dynamo DB cos we go to get item command right there, and then you have that structured logging to actually, you know, in the console or any, any of of your observability tools where you're sending your logs to from your lambda functions, you can actually see, hey, what was actually done right now one of the coolest things about step functions or workflows in general is is item potency. Item potency is super important when you do transactions. You item potency enables you to like basically have one workflow, one unique workflow of a type, and not allow a second version of instance of the workflow to get started. So that's through the name of the durable context when you start the durable, durable function, and you give it a name, uh, or, or, or an ID when it gets started, you're able to kind of maintain that consistency. So no two objects are working on the same thing. Um, so super powerful use case for, for a ton of, uh, uh, a ton of applications. And finally I wanted to highlight weight. Weighting is as simple as just calling context of weight. And while you're waiting we shut down the execution environment so you're no longer paying for it and once whatever you're waiting for either resumes I'm I'm, I use a very simple code which which is basically a wait you can actually do call back you can do a callback with condition where where you know you are able to wait for some condition to be true and then the execution environment wakes up all of these things basically allow you to save time while while you're waiting and you're not paying for any computer and you're not paying for any resources at that time. Um, and then the workflow starts from exactly that, that same step after the weight is complete and look, in the previous example there was a bunch of steps I, I gave you. I, I kind of quickly walked through the code what would it take to actually build such a workflow, but for the developers here and the engineering managers who review the architecture for the developers, you can imagine with what, what would it take to in a traditional architecture to build something like this. You're using cues. You're using, you know, compute all over the place at different steps between the queues because now you're not bundling things properly. Your deployments are more complex, complex. Debugging is more complex. If anything goes wrong, replayability, rehydration, all the some of the, some of the basic challenges of a distributed event driven architecture come in. But with a simple orchestration, a simple deployment, a simple STLC, you're able to basically build really, really reliable workflows. And again, as I said, there is no real comparison to this, this technology. Uh, out there there's lots of workflow technologies out there. This is actually a compute technology with work workflows built into it. And so look, we've got long running work of, uh, lambda functions now that can run up to a year. Um, so anyway, I covered these points already, um, and so we can get going, all right, so we, we talked about our MCP server and what we're doing with Gen AI development. There was a whole bunch of things we've done like, uh, how many of you, by the way, have installed our, um, our actual dev tools for VS code. Few people Awesome, um, look, I would highly recommend for the developers here to go try to check out our developer tools or from the AWS tool kit. I'm assuming most of you use VS code. Um, one of the things you might have missed, for example, is you're not able to remotely debug, uh, lambda function. You can actually put a breakpoint into our running lambda function through our tools as well. So we really, really are focusing on developer experience. But, um, look, Jenna already talked about MCP and how that makes developer experience super easy, especially on ISC and our best practicetresses. We talked about LMI, which lets you run long lived workflow, uh, sorry, steady state, large workloads at incredible, uh, discounts and, uh, and choice of EC2 instances, and I talked about, uh, durable functions for you folks as well where you can now run really reliable because reliability and durability is super key for long run running workloads because it's not a question of. If you'll have an infrastructure issue, it's when you'll have an infrastructure issue. So, um, but we're not done. I do. I wanna talk to you about one more thing we launched a week or so before we invent this is really around, uh, especially security sensitive SAS applications. So here's a, a scenario. You have 3 customers or let's say 3 tenants of your SAS, um, and you're using lambda. Lambda has, uh, you know, maybe you're using a global variable in the lambda function. Maybe, maybe you have some, some, something in the temp file you're using our temp storage. Um, maybe you wanna restrict and you wanna say, hey, uh, an execution or invocation to this lambda function should only talk to the Dynamo DB table and only fetch data from the Dynamo D table DB DB table row for this customer who's calling or invoking this function. Um, so in this case I have a blue tenant and, you know, they created a lambda function, we created an execution environment for them and maybe there's something left over, maybe there's some global variables left over, and then the yellow tenant calls. But the problem with the yellow tenant is they, they might have some side effects that they leave in the lambda function as well, um, or, uh, you, you, uh, you're not, you're not not able to isolate those iso uh, those side effects, uh, if you've written code this way. And finally, of course, you have a green tenant, and again the request is being, being sent to maybe the same execution environment, and there's some side effects. And look, this is nothing lambda specific here, right? This is true for any computer you you run. You can do you see two instances containers. Um, one of the hardest challenges is how do I isolate my customers from each other without actually creating individual infrastructure for each customer. So something pretty expensive to do, you can do it. Uh, the way you do it on lambda traditionally before this feature is you would create a lambda function per customer. I know it sounds ridiculous, but there's customers and use cases with that where you have to do that. Well, now you don't, uh, so a coup, a week or so ago we launched tenant isolation, and what that does is that in the invocation, so it's still the same lambda function you're writing, you pass a tenant ID. And then what we do is we create individual execution environments which are not shared by any of the tenants for you. And the idea behind this is that if you have sensitive software, if you have AI generated software, you have, you're, you're in an environment where you really wanna even isolate your own customers from your infrastructure now you can do that and it's super, super simple, no extra infrastructure to manage, no, not even extra lambda functions to manage. OK, so we're almost through, and I know I, uh, we have a little bit of time, so I'd love to, uh, invite Jonah up as well and take some of your questions as well, but I wanted to kind of share with you kind of what is our strategy because we're not done yet. I, you know, I, I'll, I always like leaking a few things, and you know we've got some big things done by Reinvent. I suspect in the next 6 months we have some other big things we're working on that didn't quite make it, so keep your ears and eyes peeled for more information, I think, but all those things will be in the same in this team. Lambda has always been about developers. Lambda has always been about, about speed, and we really embrace that idea. What we're trying to do is we're trying to get the objections out of the way. Like objections from platform teams that it can get too expensive, objections from our ISC is really hard to get right. Um, are the fundamentals like, hey, can I really do cost control with lambda really, really well? All of those things are gonna be built in and we're really, really laser focusing on, on developers and how developers can move fast with services. So like with this we were, we over the last 18 months I talked about remote debugging and, and some of the stuff we didn't even show you. We are heavily involved in the dev tool space to make sure that serverless development, you don't have to be a serverless developer, you just have to be a developer to get the the benefits out of it. Alright, so just to recap the whole thing again, we, we have a mirror road map that you can see that's focusing on developer experience and then the fundamentals are there. What's coming up next is observability hotel support. We've heard from customers. We wanted to have like a native hotel support. We've already done a bunch of launches over the last few years around structured logging and getting lambda to be ready for a hotel, but we wanna have full hotel support for customers. Um, that's a key part. It's a key trade-off that I mentioned in an evolutionary architecture, so we wanna continue to make our observability better, um, more runtime, uh, again you might have missed this, but we just launched, uh, Rust support in lambda as well now, uh, that was, uh, I think a week or so ago as well, but more languages, more frameworks, more runtime are coming. Um, look, with LMI, with durable functions, we are able to now, uh, open up a whole new class of applications that you just couldn't use on, on lambda, but we're not again done. There's so much more we wanna do because we, we believe customers shouldn't have to choose to, to manage a lot, a lot of those ilities that I was talking about earlier simply because they have a business need that doesn't fit, um. And look, integrations are bread and butter. The way lambda delivers that speed is that so many things from EventBridge to SNS to API gateway to ALBs to SQS to Kafka, they're all just built in. You're not trying to figure out how do I make this technology work with my code. They're all just built in and it's our responsibility to make in. We're gonna continue to do more both on the dev tool site and an integration site so that your favorite, like, you know, uh, CICD tools, your favorite observability tools just work with lambda. And look, you might also not know this, but we just recently, again just right before we invent, so Janek and the product team were quite busy. We, we launched our roadmap as well publicly. So if you were to just to take a look at the QR code, that would take you to the, to the road map as well. Give us feedback. We love to hear from you. Uh, I'm gonna invite Jenna over. Janek, we have a a few more minutes. Um, thank you so much, folks for sticking around and coming out on Thursday. If there's some questions, we'd love to take them. Thank you. Thank you.