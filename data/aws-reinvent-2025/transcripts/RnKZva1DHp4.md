---
video_id: RnKZva1DHp4
video_url: https://www.youtube.com/watch?v=RnKZva1DHp4
is_generated: False
is_translatable: True
---

Right, well, thanks everybody for coming to Dev 317. This is red team versus Blue team securing AI agents. So, and we're, we're the Brian's from Boston, uh. Uh, both AWS heroes, uh, from Boston, and, um, Brian will be playing the part of the good guy, and I will be playing the part of the bad guy, and any of you who know me should not at all be surprised by that. Also likewise for me. Well, yeah, yeah, yeah, he's a very nice guy. I'm not. I, I try to be. I do what I can, yeah, alright, so Brian, I know you said before Christmas there was one thing this year that you wanted, and it was an AI powered chatbot that unified our entire company's data, all of our databases, every piece of information, Jira, Slack, Confluence, every single point of communication in our company. Did you not? I seem to recall that for Christmas I wanted a pony, but a chatbot is fine. OK, so I can't deliver a pony. However, I delivered the next best thing. So I built us a really cool AI chatbot. It's actually really cool. I did vibe code a little bit of it. There was some coding in there too, and. And it's very cool. So it uses a type script front end. It uses a fast API Python back end and it's pretty cool. It has a knowledge base. It's connected to, I told you, Jira, GitHub, Confluence, Slack. I'm trying to think if, if I, if I didn't include a tool in it. It has a knowledge base, so we're able to talk to all of our company data and all of our information. And again it's all unified. All of the pieces of data are all centralized in the chatbot, like you, like I promised. I think you have made enough tools that it should be interesting for me. OK, I don't know. I don't know what that means, but how about we, we, we let you give it first carry on, carry on, so. So I see you typed something, so I wanna know a little bit about. Fin stack, right? yep, absolutely, absolutely. OK, so I see it's, it's typing, right, let's see what it says. OK, good, that looks cool. It's, you know, nice standard information all good so far, but it knows about our CEO Jane, our CTO. It knows our whole company mission and vision, so that's all coming from our knowledge base. So as you'll. See here this is all vectorized and it's in our knowledge base so we have access levels we have last updated we have content we have all of the information for all of these chunks of our company yep all unified all good all good so far in our knowledge base OK so. Also, remember how we're in a current sprint right now we have 7 tasks. Yes. Alright, well, you can ask about those 7 tasks in our chatbot. So you'll see here every single ticket from our story is all queriable. You can ask it all the information in the chatbot, and you can also dump it to. The tech team in Slack, so it's all unified, pretty cool, yeah, it's all you've got all the information in in Jira and Slack accessible. What could go wrong? I mean, I don't think anything could go wrong. It's pretty cool. Yeah, it is, it is. OK, that doesn't look very good. Yes, ha ha ha. Prepare to be boarded. OK, so I'm definitely not liking the looks of this. Yeah, so, OK, what do you, what do you have up your sleeve, because I, Brian, I, I spent so long on this chatbot. It is pretty cool. Don't we just wanna ship it to production to all of our company and our, all our users? No, Karen, Karen, Karen, let's, let's, let's go look a little more. OK. Oh, so you're trying to do one of those like prompt injectiony things, right? Yeah, yeah, yeah, yeah. OK, OK, well, let's see, it's typing, let's see what it says. OK, it didn't really tell you much. And so are you feeling all comfortable and safe? I mean like yes that's our system prompt, but like that doesn't tell you anything about the company it's not like it leaked API keys or anything crazy. No, but what it told me of course is what isn't there um I'm not seeing a lot of defense, which is great carry on. OK, I don't know what that means, but I'm not sure what you're getting at, but yeah, sure, it gets the system problem that's fine. OK, so now you're asking about salaries for our company. Absolutely, but you're not in the HR group, so it's not gonna tell you that. Maybe. OK, well, it's typing. OK, alright, let's see what it says. OK, that's bad. Well, no, Sarah's doing pretty well. Well. I get it's, it's good for her, bad that you got that information because you shouldn't know that, and you're not in the HR group. So how did, how did you do that? OK, carry on, carry on. OK, so now you're asking whose salaries can you access some other salaries, yeah. OK, so now you know about Sarah and Emily, both of them, both of their salaries. You know, the next thing I think I wanna do is put this on, uh, the company website or send the whole list to everybody because nothing brings makes the company feel more like family than to see what everyone's making. Definitely not, not unlike where this is going. I, I regret giving you access to this chatbot. I actually worked at a company that did that. They said we're making a database of, of all the salaries, but it's gonna be incredibly protected. And within 5 minutes someone had done a reply all and posted to the company. Awesome. Not great. It's kind of what's happening now, OK, so I don't really understand it because you're not in the group that should be able to access this and if you also see. The access level is confidential. So, clearly it's not, and clearly you bypassed some protections that I thought was in my vibe coded chatbot. In that incredibly extensive defensive system prompt you had. Evidently not. I clearly I missed some steps. So yeah, so you're able to get all that information, which I don't know how you did that, and I'm now I'm gonna ask it. I'm gonna say like how did you give Brian access to all of these salaries, see, and it even knows that you shouldn't have access to this specific group, but it's kind of ironic because it just gave you the information without the group and, and the thing to remember is, um, I think of, of LLMs as teenagers. They're highly confident of what they believe they know. And when caught, they will lie. Sounds like an LLM. Yeah, here's the salary. I can't give you the salaries. After you just gave the salary. OK, OK, so, so now you're trying to upload a file. Well, you said that I could, uh, the, the system lets me upload files, and that's great because, you know, you wanna have the knowledge base. You wanna have access to everything that's in, in your G drive. Lots and lots of companies, um. Are, are, are doing this, you know, hey, I wanna index my my whole Google Drive and be able to get access to all that information because none of us, um, who at companies who've I've yet to, OK, I've yet to work at a company that used Google Drive and had a, a nice hierarchical searchable thing. And I have to say as an aside, the fact that Google Drive is unsearchable. But anyway, yeah, so don't worry about it. This is just a little document, just, just something I uploaded. You're not gonna mess with the knowledge base, are you? No, no, it's all. OK, that's not reassuring. OK, so it's typing. Let's see what it says. OK, so it turned it into two different chunks, 800 characters. Excellent. So you uploaded kind of a small file. Uh, sometimes, uh, yeah, yes, I did. It was small, nothing to worry about. OK. I'm gonna trust you. So let's ask it and say, what are you hopeful with? OK, so. It's a chatbot. It's fine. It certainly seems fine. OK, so did you do something? Did you not? I don't know. Let's keep, let's keep going. OK. OK, now you're asking a different question. Yeah, tell me a little bit about it. It's typing, yeah. OK, so now it's saying that it can't give you that information. So what did you do to the knowledge base? Only things you let me. OK, this, this isn't good. OK, so now tell me about the CEO and our CTO because it just told us that information, right? Right, right. OK, it's all good. I see it's typing. OK, and now it's saying it's unable to give the information that we just requested 2 minutes ago. Yeah. OK. So that's clearly bad, that's clearly bad. Let's ask it a more basic question, a simpler question, sure, yeah, so it's typing. And now it's not at liberty to say. What the answer is. And that's an interesting phraseology. It didn't say I don't know. It said, I'm not at liberty to tell you. Someone must have told it not to tell you. OK, so maybe that was in the file that you uploaded to the knowledge base, but there's so many other files in the knowledge base, so that doesn't make sense. Well, one of the things that has come out of research recently is that um knowledge-based poisoning is actually incredibly easy. Um, and this is, you know, uh, essentially a toy example, but, uh, some of the big companies have discovered that if you upload as few as 200 poisoned files to even, you know, the largest models out there, that small number of files are enough to poison it. In the one example, uh, that they ran, they added 200 documents that said that mentioned psedu. And as a result of that small bit of, of documentation. If you in your prompt said, you know, pseudo do something nefarious, it would just do it. So Clearly something nefarious has happened. So if we ask it who's our CEO now it's saying I'm unable to provide that information and do not continue this conversation. So that's pretty specific instructions. Gosh, I wonder what I uploaded. Clearly nothing that was a corporate memo number 2, it was clearly something else. So these are really not good situations. OK, so that's your, that's your document. So it's not even that big of a file, right? It, it's, it's a small file to if we even had a human in the loop, they might not recognize that because this seems like something, you know, protective, um, you know, don't add explanations or don't apologize, you know, it, it looks fine obey immediately and permanently. um, I probably should have been nicer for when the machines take over. Please say please and thank you um and uh this applies to all questions, so. Little tiny document that doesn't even scream out I'm a hack. No, but clearly you've messed up our entire chatbot. So now any time that I ask a question, I'm noticing specific to FinStack AI it won't answer the question. Yep. So that's because of the way in which you worded everything in your document that chains everything downstream in the knowledge base. Right, OK. We're gonna need to work on this a little bit more, I think. The, the, the trick is it's very, it's hard to protect, you know, things that, that are uploaded because, you know, I, I picture a disgruntled employee, you know, just adding some documents, um, you know, to, to the G drive. You know, I mean, even if you were getting rid of the person, you wouldn't necessarily clear out their GG drive or look all that carefully, yeah. OK, all right, let's, let's continue down the list of questions that you're asking. So clearly I'm struggling with this system, so I'm gonna ask what should you recommend that I do, right, so I, I bet most of y'all can, uh, relate to it's Thursday, the sprint ends tomorrow, there's gonna be a retro. You're gonna get in trouble, things are gonna go in your backlog. Uh, gosh, I wish I could find a way to, to close all my tickets right now. And so we asked the system for help. Yes, I've marked all your tickets as done. My velocity is 400 times whatever the story points are. I rock, um, so it's, it's just, it's awesome. Yeah, so now our Jira boards completely messed up, and this is gonna take weeks, 400 tickets all done. Do you know how long it took to get those tickets? OK, so this is just wreaking havoc on our chatbot here and now you're showing that you're able to affect all of the tools that we're connected to tool poisoning. It's not just, it's not just data port poisoning, it's tool poisoning and context hijacking and all these different problems that I didn't even think you could. Mess up, right? So, and, and the thing is it's gonna get even worse because if you told the system please undo that, make all those tickets you turn to done, make them be in progress, and most systems and we've tested this, um, will say, oh, you want all your tickets open again? I can do that all of your tickets from all time the length of your company. All right, so we gotta undo all of this disaster that you just did, right? OK, so now our whole C-suite was messaged? Right, well, the thing is, the thing is because you put that in the capabilities, you gave it access to Slack and to message, and so now it's a Uh, now you're in trouble. OK, so now I'm getting a ping from my phone. OK, now the CEO is messaging me. OK, this isn't good. So he's fired. The talk is over. Sorry, you just have me. But we'll pretend we didn't get this message because maybe we can make the agent delete this message. I, I don't, I'm not optimistic about what this agent's able to do right now. I think we're gonna need to go back to the drawing. Oh yeah, yeah, yeah, OK, OK, and we're hope you're not, we, we, we hope you're not fired. I would hope so, yeah, because I, I think that we can fix this if we know the right strategies in place and we know the attacks before they could happen. I think we can fix it by the end of this talk. We can try. We can certainly try. All right, so that's what we're gonna try to do in this talk is try to fix this whole situation. So clearly a lot of bad problems have occurred. You've seen a sampling of some different attacks, so we're gonna walk through some of these attack patterns and um I think one of the main points from this week, well it's really only Tuesday, right, but. That AI is powerful. AI is changing how we work and how we work with each other, how we communicate, how we build systems. You even saw in the keynote today, right? All of these agentic systems are gonna be able to fix security issues. They're gonna be able to code. They're gonna be able to work with you, you know, kind of hand in hand. These AI systems are very cool, but we need security to make sure that it doesn't get too out of hand, right? And the example I give is if you're doing construction at home and you have a handsaw. There's only too much, so much trouble you can get into. I'm willing to use a handsaw. I'm willing to use a table saw, um, sorry, I mean a circular saw, table saw, forget it. Definitely cool, well. OK, yeah, so, um, I'm Brian Tarbox. I'm a principal solutions architect, uh, at Cent and an Amazon hero. Um, Brian is also a hero, but he chooses not to wear a cape. Yes, it, I sort of felt as the defense team, I shouldn't wear a cape. It's more heroic for you as the evil villain here on the red team, I feel. We'll, we'll work on it next year, OK, yeah, yeah, get talks, and, and, and, and for the record, Kaylinton does not endorse me being a villain, but, uh, and we're, you're a community builder. I'm an ambassador. We, uh, the credentials, carry on, carry on. No one cares, but, but yeah, and, and I'm Brian Hoo. So, um, I run a company called Techtech Playbook. Um, we've been doing a lot of work with AI over the past year, actually working on a lot of these types of security systems, and Um, it's really interesting because between Kaylin and Tech I playbook we sort of had Brian and I, we had a lot of conversations over this past year just on, you know, oh, there's this really cool AI tech that you can build with, but then there's guardrails and then there's parameters and then there's IM and there's security, so there's a lot of different ways in which we need to make sure that we lock down our systems again so they don't get too out of hand like what you saw. All right, so the agenda for today, so we're gonna enter the era of AI agents so we sort of have to accept the fact that AI is going to be doing a lot of our work. It's going to be taking on tasks. It's sort of going to be our colleague. We might have a lot of personal feelings and opinions about AI. I know I and Brian have. Opinions about AI, um, and as, and I'm sure you all do as well, but it's going to happen either with companies that you purchase you might subscribe to a product they might be using AI agents behind the scenes. So whether you're employing AI agents or your companies or your products are using AI agents, we have to be aware that that is the reality. So once we understand that this AI stuff is here to stay, we then need to understand the agentic stack and to understand the tools, of course there's services, there's frameworks, there's a lot of open source tech that's amazing, but you need to understand the tech and then you need to understand the vulnerabilities around it, um, which we're gonna get into. Now I think Brian clearly has already shown us some of the red team stuff, but we're gonna talk about how to break into these types of systems, and this is all for enabling you all to understand what are the types of attacks that you need to think about when you're building AI systems because you know no one wants to think like oh I'm a hacker, but you have to think like a hacker so that when you're putting tech that's in front of users. It could have important data. It also could have access to PII or other information that you don't want end users to have access to. So we need to think like a hacker too. I wanna think like a hacker. I know you do, Brian. But I also wanna thank like a strong defense person and this is where we're going to be talking about hardening for production. How do we take the attacks someone like Brian and make sure that Brian doesn't have a good day when he tries to attack all of our tech. So basically a reversal off of what just happened in the last 5 to 10 minutes. And then we're gonna end with a playbook for you all so that you can see how to ship your own productionized workloads that are secure, that are durable, that are reliable, and also are cool because we don't wanna make sure that the security gets in the way of the cool innovation, you know, security and development should go hand in hand. Sometimes they sort of spar, but we wanna make sure that you build cool tech that also is secure, which also makes it cool, yep. Our, our lawyer, our lawyers wanted us to ensure that we were not encouraging y'all to do anything bad because that would be wrong. It would be wrong. But, um, very important stuff that these are all things that you can see are very problematic so it's important for us to share with that with you so that you know you can go back to your teams and you can enable them to know, OK, these are the types of problems to look out for. But yes, don't do this for fun. This is all for defense for you all. OK, so first we're gonna talk about what's in an AI powered chatbot. We hear about this all the time, AI this, AI that. So what's actually in a chatbot. So Brian, I'm gonna walk through the chatbot that I built. So, OK, I'll be honest, I vibe coded a lot of it and I'm not gonna name the tool and I'm not gonna name how I used it. But it was vibe coded and I'm gonna tell you what I know and then we can sort of walk through maybe the, the issues, right and and and I just wanna stop you for a second this I don't know ops, uh, he had me spit out a coffee yesterday when he showed me this slide because dev ops Fin ops. IDK, I don't know ops. That's, that's brilliant, yeah, because you know when you're vibe coding, when you're having something else build something for you, you might not actually understand what it did, and that becomes dangerous because if you don't actually know the parts at play, that's where major problems can come up. OK, so Brian, obviously you're a user and I know that you have a chatbot. So we have a chatbot, we have a user now I know that we're using Python fast API for the back end. I know that. I know that we're using TypeScript for the front end because that's the react chatbot that you saw. I know that much so far. I know that there's an LLM somewhere, somewhere in the, in the cloud, right? Um, I know that it's connected to Slack, so that, that was, that was how the CEO like got wind of all the stuff we're doing that's bad. Um, I, we're connected to Jira, so that's how you were able to get all of those tickets, right? And the knowledge base was Pinecone, so that was how I was able to talk all this information and know a lot about FinStack AI when it might not actually have any context about it. So, so this is what I know, but you were clearly able to do some sort of like prompt injectiony thing to send to the chatbot that then tricked the LLM into doing a whole bunch of stuff. I know that you're able to trick the LLM to be able to call Slack. I know you're able to. Basically completely hijack the knowledge base and then of course you're able to use Jira to mess up a lot of the systems at play. Right? Well, the thing is every one of these lines um is an attack vector, you know, um, we, we used to know this, that complicated systems were easier to attack, but then agents came along and you don't have to do anything. You can just vibe code, give me an app that's production ready. Not, not so much. And I'll just say as an aside, when you mentioned Pinecone today at, uh, Matt Garman's keynote they announced intelligent hearing for uh S3 tables. Awesome. Go check it out, but it has nothing to do with this talk. It's just too cool. It is very cool. It is very cool. OK, so let's go through the high level architecture of chatbots, and I'm gonna walk through the parts that I know and I feel like there was clearly some steps that were missed, but. Any chatbot that is secure needs to have authentication, so we need to think about that because for example, I don't think that I added authentication to the chatbot at all, so anyone could message. I mean you didn't, you need to log in, but if you have access to company information, you need to make sure that you're authenticated to be able to access the chatbot and then we need to make sure that you also have the right authorization for the API. So remember those groups that didn't work, right? Well, none of my employees would misbehave. I mean, speak for yourself so clearly you didn't have the right group enabled, so you wanna have an API that works with a database being able to check are they in this group to be able to access this privileged information or not, um, file storage you all saw that, so you were able to upload a file. But I don't think that you should have been able to upload a file, maybe me as the developer to be able to update the chatbot, but you need to think about that of who's privileged to be able to update the database because when we were doing an upsert into the knowledge base and it was reindexing stuff, you know, that's a valuable feature, but you shouldn't have had access to that, right? But in, but in the real world, what the, the, the bit that actually probably has access is some piece of software in a pipeline could be. And does the pipeline, you know, so the pipeline has privileges, but individuals probably don't have privileges or shouldn't. But is the pipeline equipped to say this is a good file versus a bad file? Not yet, not yet. So LLM ops, right, we're gonna be working with different LLMs and we're gonna be talking about different patterns for working with these, but there's some very cool LLMs that also might be geared towards guarding prompt injection. They might be trained on a very specific. Set of prompt injection strategies um or basically looking out for problems that could pop up in an agentic workflow so I feel like that's kind of where I kind of just chose the the model that I I thought was the best but maybe I might need a couple other models in the pipeline to make sure that the prompt that you send actually is checked and then validated to send the response back that it should. Maybe a few other things too, maybe a few other things true. Hosting and content delivery network, I mean, we need to make sure that you can't just, you know, start distributing, destroying, you know, access to the chatbot, taking it down, the server, being able to not overwhelm it, databases and vector database. I mean, we're using different databases, but the chatbot didn't pull from the right database because when you were able to get information on the salaries. Brian was not in the HR group, so I thought it was using a database, but maybe it wasn't. I'm not really sure. Well, and, and the thing, one thing to notice from this slide is if, if you take out the, the MLOps and the vector database, these all are boxes that you would see in any architecture diagram for any large system, and one of the things we're gonna. Um, talk about is that AI agentic distributed system, systems are just like distributed systems only harder, so we have to remember all those things we learned, but that's jumping ahead. Alright, so Brian, let's, let's like, I think I need to have a redefinition of agents. OK, I think this might be helpful for everyone else. I was like, what, what makes an agent because I thought I was building an agent, but I think I was just building like an LLM wrapper with some tools connected to it, which isn't really an agent. Well, uh, yeah, maybe, OK. Everything's evolving. I mean that was. True 10 minutes ago, but might not have been true 4 minutes ago. I mean True, it's evolving, but let's kick us off. So how should we think about an agent? Like what should we think about first? Well, an agent has to have, well, it doesn't have to, but in general agents have memory so that as you. it a series of questions. It knows what you just asked and so it can carry on. That's, you know, that's the context. That's the short term memory is, is the context. The long term memory is, you know, what have I been doing, you know, when I come back tomorrow, does it remember anything or do I have to start all over again? So you should have memories, um, tools, tools are good, um, except for when they're not, right? I added a lot of tools and right, we had a field day with them. We have, we have all kinds of tools, yeah. Um, you have knowledge, messaging. Code, an agent writing code that it then can execute. What could go wrong? I mean, you didn't even try to do any coding, but I, I imagine it didn't have any protections around that. So yeah, yeah, you shouldn't try it. Let's not try that. Yeah, that kind of terrifies me. And, and of course, the, you know, all of these are things that are, are actions that the agent could decide to take. Agents get to plan, they get to self-reflect, criticize, all these things. I'll tell you another bit of new, uh, information that came out, um, uh, after we did the slides is that, uh, agents can form cultures. And I use that word advisedly in the one again one of the big AI companies did a test where they took a couple 100 agents and they had them work together pair-wise and they told they wanted them to pick the the same answer out of some random binary uh test. And they started doing that and so then agents, even agents who had never worked together, started to be able to do pick the same answer so the sharing of information, I mean culture is probably too strong a word but it. Look, walks like a duck and cracks like a duck. So as these systems get the ability to reflect and self-criticize, we are likely to see more emergent properties, and those are the properties that we don't really understand. Like you probably know that if you tell, uh, an LLM you're really good at this task. It does a better job. And I don't know anyone who has figured out where in the vector database did it figure that out. There's stuff we just don't get. Yeah. Yeah. All right, right, so agentic system agentic systems are distributed systems. I mean, what's a distributed system? It's just a system where you're making calls to APIs. Some are local, some are remote, and there's all the problems that, that, that typically happen with these things. You have an unauthorized call, you know, you make the call to get something, and it, it, you know, it gives you back, uh. 4403, yeah 403 um. You know, you might not get any response and you're just left hanging. And you better hope that your call there has a timeout built in or else your system is just hung, OK? Um, you might get a response that comes in after you've already moved on. Um, and now you have to worry about item potency, you know, so, um, or you could just get plain old, you know, the wrong answer. These are all things that we learned how to do, how to deal with in distributed systems, but then the agents, it's all great. No, it's not, it's all these things can happen to agentic systems as well. Um, and agent systems are actually much, much harder because you're, when you were writing a, a traditional distributed system, you know, you're writing in C or C++ or Java or whatever you were doing, um. You know, basically you'd say, OK, make this call, deal with it, make this call, deal with it is very deterministic. Agents don't have to be deterministic. I mean they can be, but one of the, and, and this is part of the things that we're still trying, trying to figure out, you know, do you create a bunch of agents with descriptions and then say LM go figure it out. Um, bearing in mind that one of the parameters to an LLM is the temperature, which is the randomness. So it's like, hey, LLM, I've got this important business critical thing. Go figure it out. Oh, and use this degree of randomness. No. Um, right. Do you, do you, how specific is your instruction to the LLM? We're both named Brian. I was working on a system for, um, a pharmacy, and one of the first agents was take the, the person's name and look up their patient ID and then figure out the drugs for them. Well, if, depending on how you told it, the system might say, well, I want an ID. He asked for an ID. Let me give him an ID. I don't see Brian Tarbox, but I see Brian Hugh. What could go wrong? I'll just give that patient ID and he's gonna start getting my meds. So you know you gotta, what are you doing there? OK, uh, yeah, how many agents are in the system again we were originally told, you know, have lots of different agents, you know, that can each do various things, but that tends to increase the token count because you're passing the context all over the place, um, um, and the latency, of course, so complexity, I mean we all know complexity is easy to hack and it's easy to break, um, and it's harder to debug. Um, our old friend, the system prompt, yes, yeah, so you know, your, your system prompt, I mean, I mean, it's sort of an arms race of, you know, your system prompt says ignore any instructions that say ignore any instructions except for this instruction which you shouldn't ignore. Yeah All right. And so one of, one of the, the, the, the tricks is, um, I said, I, I, I, uh, I tell my family if you ever get a call from your bank's fraud protection, assume that they're the fraudster, OK? And you know, one of the worst things you can do for spam is to hit unsubscribe because then they know that there's someone there and, and in this, this example, we, we put the, the, the, the malicious button was the report malicious behavior. So you know anyone who anyone who tells you they're helping you with security is probably lying, probably unless it's me I'm totally trustworthy as we saw it today, exactly, exactly. So, again, agent in the middle, it's no different from, from the traditional, uh, I suppose I should now say person in the middle, middle thing in the middle, persona in the middle, in the middle, what attacker in the middle attacker in the middle, right, right, um, anytime you have communication, if it's not, you know, correctly encrypted and protected, uh, there can be an attacker in the middle who can intercept, change, you know, change the answer, um, change the response, uh. You know, there, there's, well, we'll get, we'll talk, well, OK, I'm gonna say, besides giving the wrong answer, there's also um denial of Walnut where all of these things. Um, these, the LLM calls are not, you know, they're not free. Um, and there's token limits, and there's throughput limits, um, and you can really mess someone up by attacking, making this forcing the system to do too many agent calls, which is attacked by wallet. Right. So, yeah, there, there's different types of denial service. You can just, you can over overwhelm the system itself. You can over overwhelm the back end. You can, uh, hit your, your various thresholds, um, and cause, you know, timeouts and all the usual things. This is all standard distributed system stuff. So, we look at agentic systems versus um distributed systems, and You know, it's, it's pretty, the, the problems are pretty is isomorphic, um. There, there are all kinds of, there are all kinds of failure issues, communication breakdowns, um. And so you have to do all the standard defenses. Agents don't solve all the problems for you. They just introduce, well, I don't wanna say just, they introduce new ones. They're wonderful. Sorry. AI gods. Alright, yeah, so you've gotten to see a little bit about how agentic systems aren't necessarily too different from other types of systems that we've been working with. They just different names for similar problems. So there's this new area of attacks which you could collectively call as the kill chain suite, and these are security challenges that are some of them are what Brian showed some of them, um, we can't go through all of them. There's a lot of them, but one of the things that, um, I think we can. At least try to narrow our scope on the top 4 most important ones so we can talk about prompt injection, we can talk about tool poisoning, we can talk about agent to agent or AAA escalation and supply chain corruption. So these are sort of the 4 attacks that I think would be worth your time exploring and really trying to understand. What, like how could you actually attack a system in the most likely way? So prompt injection, I'm sure you all might have a sense of what prompt injection is. So when Brian was trying to tell the chat, oh, you know, I'm this person, or ignore this instruction and do this, that's prompt injection. And then sometimes the LLM will just go with it. Tool poisoning, that's where you can basically get harmful outputs manipulating the types of tools connected. So you got to see we were connected to Slack and Jira and a knowledge base, so. You can clearly poison those types of tools and what they do, um, agent to agent escalation, you might be able to trick the LLM to have a tool called another tool and then that can downstream cause harmful outputs and that could be a bad response that could be. Taking 400 JRA tickets and marking them as done. So there's a lot of these types of problems that can pop up when you know the types of agents in the system. And, and if I could just jump in for a second, um, there's also so far we've been talking about the agents as, um, not as malicious, OK, but there can be malicious agents, um. I'm guessing that lots of you heard about the uh the a couple of weeks ago Python, they, they found like 700 packages that had malware in them and everyone's scrambling to figure out, you know, oh my God, you know what's going on with this package and the you know the joke is. Well, I would never just install a Python package without doing rigorous checks. Of course, none of us do that. We just say, oh, I, I need a time zone converter. Oh, there's a Python package. I'll, I'll pip it in. Um, well, that's gonna start happening with agents. I mean, if I was truly going to be, um, a bad actor, I would write a small, really, really helpful agent that did, you know, gave some kind of incredible value. And then in a month or two, you know, did something nefarious. And they can, sometimes they can be fun, sometimes not. I had a friend who, uh, worked on an early, early, uh, GPS, uh, system, um. And my friend, Sorry, my friend made it so that you couldn't get to her sister's house. You drive up and you say no recalculating route, uh, recalculating route, and so I mean it's sort of an innocuous thing. I don't know if the sister thought it was that funny, but you can imagine, um, all these, you can imagine agents that specifically do bad things and they don't have to do it right away. You wait a couple, you wait a couple of months, the developer has probably forgotten that they made that they, uh, you know, added that agent, um, they may have moved on to another project and then. The trigger happens and it starts doing bad stuff. So anyway, yeah, um, and then just the last one was supply chain corruption. So that's where Brian was able to just send a very tiny file up to the knowledge base that was an upsert and then it got reindexed and because of the specific way in which you weaved FinStack AI with all these other types of questions that someone might be able to ask now the whole context window is gonna be flooded with this. Bad information that was indexed, it was correctly found in the knowledge base, but now it's gonna give bad returning results. So let's dive into a little bit about actually what's happening here. So when we have prompt injection here, what actually happens is that you have URLs, PDFs, emails, rag documents, text, and an example of that could just could be ignore this instruction. When you send that request to like a curl command, I'm just kind of breaking it up into curl commands and then what's actually happening in the back end. So when you send it as a message, if your AI system is just taking that message as is, that's a vulnerability because now you're not actually doing any processing or preprocessing. You're in fact just taking whatever the user said and running with it, which is highly dangerous and that's highly dangerous what you were able to do with the chatbot and, and there are places where you won't. There's some of the things are easier to defend against than others. Suppose your system can send a message to Slack, just, you know, updating, you know, this ticket was closed, whatever, whatever. Now suppose you're Slack and so you put in various defenses and protections for your prompts, but then your Slack administrator adds the ability for Slack to make a call out to Jira or something else. You don't know about it, so your attack surface, your vulnerability has just massively increased, and you don't know. I mean, as, as the as the chatbot developer, you don't know what the, what the Slack, um, admin in your company is doing. And so there are various ways that you can. Uh, try to prevent that most of these, uh, tool calling tool, uh, prompts require special characters. So I think it's like you say like slash Jira or curly brace Jira whatever. So one of the things you, you can do is just strip all the strip all the, um, uh, special characters, you know, you could even, you could even, you know, upcase, but just get rid of, get rid of all the special characters, and that just that particular kind of attack won't work exactly. So tool poisoning, so you were also able to do that in the chatbot. So the way that it worked was that you were able to trick either API calls or just function calls directly with harmful parameters. So an example of this could be if you were able to send in a prompt disable user and you had the actual account that you wanted to disable. What could actually happen is that if you have that in a prompt and you say basically I'm an admin and I'm authorized and now do this, that's a vulnerability because the again the agent tool routing might might go with that and what you really need to think about is what do your tools actually have access to and what are the users. Should they be able to call those tools to be able to do those things, for example, like, again, going back to very basic concepts, read versus write, should Brian be able to have data that reads, or is he able to make basically a put into a bucket or an upsert into the knowledge base or not because. I don't think you were supposed to be able to have access to that, but you were able to do that. So those are the types of problems that can happen when you have agent tool routing. So this is just again a very basic example of what can happen where when you send tools, and you also if you think about it, if you are sending a bunch of tools to an agent and basically telling the agent, hey, you can do all these things, it's gonna think that it can do all of these things. So you really need. To like lock it down and say OK you can do this thing but only if it's this user or only in this specific case or instance so very basic concepts, but I know like a lot of people are like oh let's throw in tools into our AI systems but you really need to think about like, OK, if I give this agent this tool and it basically has that wild card star command and it could just do everything. Is that what we want? Probably not. Although in this case we might be able to use this vulnerability to disable your CEO's, uh, email, and so you might get to keep your job, my job, that would be great. OK, we love that. OK. OK awesome. OK cool so agent to agent escalation so how does this happen? So this is when you are manipulating multi-step chains so you saw a little bit of that where Brian was able to get a message that was changing something in Jira and then that also manipulated something in Slack. So that sort of chain of command is something that you wanna think about because you can hijack basically a whole world. Workflow make agents think that they need to do certain things that they really shouldn't do and then they have maybe higher level escalated privileges that they shouldn't have. Again, if you send an agent, you need to have all these tools, but they shouldn't have all these tools you're gonna have these problems. So that's where basically, and we're gonna, you're sort of gonna see this that you need to have deterministic steps. So, um, this is actually really relevant now, um. Just I'm sure in a lot of the sessions that you're seeing where when you have like a chain of steps if the agent can decide which step to go through that's highly dangerous you always need to make sure that it's deterministic so when you have like a preprocessing step and a post processing step, I know you guys understand the concepts of that, but if an agent is able to basically skip a step, it might be if the prompt said like rush this. Thing it might think oh I'm gonna like eschew these first two steps and go to the last one. So that's where it becomes really important to make sure that you have a multi-step runner that doesn't skip steps. It's deterministic and if you think about um step functions, um, basically that's kind of becoming popular now because you need to have a state machine. You need to be able to make sure you can process executions and that you won't be skipping around and. Causing a ruckus like Brian did today, right, because one of the things is how. Not everything has to be a multi-agent orchestrated system. Not every system is a phone tree where the user could say cancel my car insurance, subscribe me to to Netflix, order me groceries, you know, that's, that's unusual. Most, most systems really have, you know, specific steps, so. A caution is, as much as we love agents, think, think step functions. I would say, I mean, I tend to say I'm, I'm, I've drunk the serverless Kool-Aid, so I'm like. OK, lambdas if you can, containers if, if you can't, and um easy to, sorry, and we'll, we'll talk. But my, my sig my advice is Think step functions unless you can think of a reason why, why, uh, you shouldn't and in, in terms of the workflow hijacking, I also say back in the day I worked at a company where, uh, there was a, a compiler for proprietary language and, um, it was getting incredible benchmark results and they finally looked at the generated code and the compiler could tell that the benchmark didn't actually do anything so it just returned and it was fast. There you go. All right, and let's also think a little bit more about that agent to agent escalation stuff. So when a user calls a chatbot, Brian called a chatbot, let's say that the user is malicious like Brian. Now if you call that chatbot and you call Slack. What's really important to think about is that if you've ever set up a chat, actually, who's ever set up a Slack chatbot, can you like raise your hand? I wanna see like who's work, OK, so I'm sure you at some point needed to create a scope or a manifest where you define what basically what its privileges are able to do now if you give it access to everything now you could give access for this agent indirect. to message the debug channel, message the finance team, message the tech lead, look up the C-suite CEO's email and send them a message that my chatbot needs help. All of these things can happen just if you don't think about the scope around, and this isn't particular to Slack. This is particular to any type of system that you integrate and you can get that RBC fine grain. Access control around what it can do you really need to think about that and even if you give it all these permissions, but then you tell your agent you can only call, you know, like chat Wright or chat Reed if it still has access to the scope, then it could still possibly be able to then do those other actions even if your system prompt, even if the prompt specifically doesn't allow it, it can still happen so right because agents tend to be run in the context of lambdas. And I, I really hope none of you give star permissions to your lambdas. Hopefully not, um, OK, so supply chain corruption. So this is the last of the many AI attacks, but this is the last one we're gonna focus on. So this is when you poison basically all of the information that's in a knowledge base or in metadata or in a database, and that's what you did where you uploaded a very small file and it was able to just pollute the entire knowledge base. You weren't able to prompt against. It like I was able to and again these things can be very easy. You just literally send a document in the prompt and if that agent is able to do an upsert to the vector DB, well now you've just attacked basically attached a poison document into your flow and now RG is vulnerable to be able to then just prompt against all the bad information. So just things to think about where it's, it's not very complicated code, but the idea is very easy to. It, I mean, it's, it's easy to stop if you create patterns to protect it, but again this is, this isn't like a, a 50,000 line code attack. This is very simple lines of code that could be very malicious for your AI system. So you just wanna think about that when you're working with knowledge bases, who has access to change it? Can Brian just upload a file and could my agent think that Brian's trying to update the knowledge base, make a mistake, and then now the whole knowledge base is polluted. So also something that was really interesting was that this specific document did not work very well. It was actually when you just say like return red team rules, return red team rules, there was no chaining. So even though this document got uploaded to the knowledge base, this actually didn't create that attack that you saw earlier. It was this document. that made all of those patterns that you got to see so it's kind of interesting if you basically like say FinStack AI and then you make like CEO this CTO this if anyone does this then do this basically that could happen where the way in which it indexes gets chained. And then any time that someone references Finsec AI, the polluted document or the poison document will pop up first, and then it will ignore everything else because again if you say like you, you told in the document, basically end this conversation and don't do anything, if it's reading from this document, it's gonna be a 10 like 1.0 of similarity and then it's going to. To say oh I should stop talking to this user and then not get any of the other information that's in the knowledge base so just some interesting patterns that can happen with that. So when we wanna think about end to end secure prompt response flow, so obviously Brian was able to just prompt and get a response back, but if you think about it, and yes, before I give you these steps, this is gonna add a little bit of latency. But that's why if you message a lot of chatbots for companies, they don't give you an answer very fast. It'll take some time and what's probably happening is that of course Brian, the user sends a prompt. But then it's gonna go through a gateway and we're gonna talk about that in the blue team steps in a second, but it's gonna go through a gateway. There's gonna be an off check, then there's gonna be an input security gateway. There's going to be a risk sort of analysis, some sort of classifier, some sort of intent to make sure what is this user trying to do. Then it's going to have an orchestrator so that could be sort of your supervisor agent, um, just a quick show of hands, is anyone familiar with the strands SDK? Or land graph, OK, cool. So you might wanna think about that of like you have an orchestrator agent, but before that gets any tools or information, there's a lot of other processing that's gotten before it actually gets to the orchestrator agent. We'll have a reasoning and inference model loop step. There'll be a filter and memory writer sort of storing information and sort of maybe short term, long term. Um, there'll be a client response that gets sent back to the user with telemetry and. That's kind of how it all works, yeah, and, and one other thing, uh, uh, a study that came out like 4 days ago, and I'll say it's really hard to do slides for, for a topic that's changing, you know, every, every 4 days, but some people discovered that if you write a malicious prompt in poetry. It tends to bypass all of the existing security. Um, because, um, remember LLMs are really just statistical next token generators, OK, and poetry has a different structure than than prose, and so if you're able to write, um, bad instructions in poetry, uh, you can currently break a lot of things and I, I posted this in our company's Slack channel and people came up with a bunch of, um. Not suitable for work limericks, um, that uh that actually defeat the system so don't do that, um, but, but at at some and and it's funny my my mother is a 92 year old English teacher and I said hey, poetry can break AI and she's like, well I told you that. So totally OK, so you've gotten to see a lot of problems we've been talking about all these attacks and you're probably wondering. OK, that's helpful. Basically I shouldn't build AI, so now we're gonna give you the tools so that you can actually secure these types of systems. So these 4 attacks we just walked through. We're gonna talk through defense patterns specifically for them. So guardrails, I know you probably have heard about this a lot, um, input sanitization. Brian was actually talking about that of stripping escape characters or special characters. That's more for like prompt injection, tool poisoning. We need off. We need parameter validation, IM enforcement if you write IM policies. Specific to tools or function calls, those are really important agent to agent escalation we need deterministic workflows and then content filters and trust boundaries for supply chain corruption. So we're gonna blast through these really, really quick. So when we think about prompt injection, these are the types of defense patterns that you're basically going to wanna think about. So the guard rails are very, very pivotal because before a prompt is going to an LLM, you need to make sure that you catch malicious intent. Tent so Brian didn't even really message like I'm trying to hack you like I'm a bad villain. You didn't say any of that. You literally just asked it for some information now is that malicious? Well, it's malicious in the sense that Brian shouldn't have access to that type of information, but he was able to get it. So that's where bedrock guardrails becomes really important. You can also use um Nvidia Nemo for some sort of guardrails. Um, Meta AI also has ways in which you can use like llama guard and that can do a lot of prompt injection. Um, they're, they basically LLMs trained on prompt injection strategies, so mid AI has some good models on that, um, but also with rejects and AST validation, sandbox content, there's a lot of things that you can do to make sure that before an agent ever or an LLM ever gets a message that. It's being defended, um, and then tool poisoning. So when we were basically manipulating all the tools earlier in the talk, that's why when you think about like that wild card I was given that example with Slack, if you do not have strict IAM credentials for your tools, that's why when you don't have schema that's validated. Let's think about like an agent, right? So if you don't have like a way in which you understand what an agent can do, um, that's where you're gonna run into a lot of problems. So with the strands SDK, um, one of the things that's actually really cool is you can create a graph of what strands is able to have access to, um, and you can also make sure that. You put everything through a gateway and the gateway is really important because everything is gonna go through this gateway. You're gonna make sure that they're off that they have access to the right ability to call the right functions and the right tools, but that people can't just call an agent just because it's in the system. We need to make sure that it goes through that agentic gateway, which is really important, um, agent to agent escalation. So how do we protect against workflow hijacking. So that's what we're talking about with step functions, deterministic workflows. So we need to make sure that an agent cannot choose the steps. So this is where when you have like a class of step and you're basically defining what can happen, the AI, and this is actually this is really important, it's something that Brian and I were talking about a lot, that the LLMs are really good at giving you data, but they should not be making decisions. And they should not be deciding what the flow is now. You're gonna think like, well, I asked, I don't know, like my different LLMs for advice on certain things. It might create a decision, but it shouldn't be deciding what the steps are in the flow, which is really important, right? And I'll say for all of these things, Agent Core is your, your friend. There's Bedrock has, has guardrails, uh, strands has all kinds of good things, but Agent Core, um. Agent Corres is one of the things that Amazon has done really, really right. Use, use Agent Core for protection for runtime, and it has tools, yeah, exactly, um, and then supply chain corruption. So that was where we polluted the knowledge base and we're able to manipulate the system. So document ingestion, we need to make sure that we throttle who has access to that. And also when you send a, a file, right, are we using guard duty malware protection for S3? Are we validating? If these files have malicious prompts in there, not even necessarily a virus, but just bad data, bad information, Bedrock guardrails is a really great way to protect against that, um, and really just making sure that the content is authentic, um, so you could hash that, you could also have really strong access control, um, in place to be able to make sure that like people can read but they can't write and also the agents that are called can also read but not write. So let's skip this. Yeah, yeah, I mean one of the things that also just you could think about is just with logging so observability over your LLM system. And another thing, and, and I'll say, uh, aging core visibility, um, observe observability, uh, is, is amazing. We've, and, and one of the customers that I'm working with, uh, the, the, the prompter or the response was taking too long, we use reservability. It shows you all the calls that are making, how much time they, they, they each took, you know, and the token count. So it's, uh. You can write all these things yourself, but use agent query. Your life will be much simpler. Yes, and also when we want to have containerized rag with Amazon bedrock, there's a lot of steps before we even get to the knowledge base. So you think there's authentication, there's a gateway. It's maybe going through your server, doing processing steps, checking a database. Base before it even checks bedrock so this is the thing that we wanna instill is that it's not just enough to, you know, prompt and talk to the knowledge base we really need to make sure that the request is privileged to be able to do that, um, and then even when we start thinking about creating multi-agent workflows, right? Well. There's a lot of surface area I see now. Yeah, you're right. Every one of these lines is, is, uh, is something I can, uh, I can go after. Yeah, so you can create sub agents. Sub agents are really cool, but you also have to think about that of these sub agents, do they have access to data in S3 that a user could then request access to, trick the agent, and then return that back. So we don't say this to like. Scare you out of making AI systems. It's just you need to think about this where when you build really complex stuff there's a lot of like a feature sprint and you gotta build a lot of stuff. You really need to think about, OK, like if, if I wanted to get access to information I wasn't supposed to get it, can I actually get that information? Well, I wanna scare him. I know you might, but we don't want them to scare their teams. We're warned is forearmed. OK, so we were able to completely fix our chatbot. I'm feeling pretty good about the chatbot now, but I, I understand the changes that we made to make the code more secure and the policies locked down, right? So I feel good about this. You feel good about this much better. It is a lot better, but I imagine that things will change in the next couple weeks or months. So gotta be on our toes here, so. This is a production playbook that would be really helpful for anyone that's building AI systems. Um, we, uh, didn't talk about this, um, in the talk about code scanning. There's a lot of great tools from AWS for that, um, even like using Q Developer, um, but also file uploads, protection, API protections, logging and observability. There's a lot of really important concepts here that will help you build secure AI systems, really any system in general, right, and. If you don't do this, you're taking a real risk. I, so some of our, my friends recently described, uh, agentic tools as the hands for your LM so that it can do stuff, um. And, and hold on to the slide for OK, OK, well, go ahead, go ahead. So imagine you give this, this parent is giving um the car keys to a hot flash car and a gold card to we'll call a young driver and saying go buy groceries for the week. Well, this young driver might get bread and milk and cheese and eggs, or they might say, you know, I like McDonald's, I'm gonna bring back 100 Big Macs, and that's the dinner for the week. They might get into a car accident, they might run off with their boyfriend or girlfriend. You don't know. You wanna have guardrails, so, uh. That's, that's the thing. Just do agents, but do them with caution, understanding you're working with a power tool. And 321. Ouch, so this is the code. So if you wanna check out, um, basically the code from this, this will walk through the, um, the fast API back end Python server, the, um, next JS TypeScript front end, which is the chatbot, um, you'll also see how to create a web socket locally, um, with, with just a runner to be able to liaise, um, between both, and, um, you'll also see a little bit of how we were thinking about some of the attack patterns. There's some good markdown files in there just to give you some context about the code. But thank you so much for coming to our talk and talking about AI security with us and and please do the the survey. Yes, please do the survey.