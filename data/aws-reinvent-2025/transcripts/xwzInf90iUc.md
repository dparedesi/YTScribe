---
video_id: xwzInf90iUc
video_url: https://www.youtube.com/watch?v=xwzInf90iUc
is_generated: False
is_translatable: True
---

Welcome everybody. Uh, I will say, getting up and chatting in front of all you folks usually would be one of the more stressful things that I'm spending my time doing, but it's been a rough month. So I am very happy, even more than usual, to be up here to chat to you today. Uh, and, Thank you so much for taking the time to come down and see us. I know it's 9:00 a.m. fresh and early on a Monday. I assume for most of you this is probably your first session, so welcome to Reinvent. Um, we're so, so happy to have you here. Um, when I let my co-speaker know that we had the Monday 9:00 a.m. session, I could see in his eyes. He thought we would be talking to an empty room, but we have got quite the opposite here, so, so thank you for showing up. Uh, my name is Neil Thompson. I'm a container specialist, solutions architect AWS. That tongue twister means that I spend most of my time, uh, talking to customers about why their EKS clusters have run out of IP addresses. If you know, you know, um. But for those of you who don't, I'm, I'm very happy for you. Uh, I'm joined today by my co-speaker, who will introduce himself. Hi, uh, my name is Hasid Kalpay, and I lead the platform engineering and security at uh Cisco's uh incubation unit. OK, cool. So I'll be kicking us off here and then Haith will take over in a little bit. So, um, we have a packed agenda today. Uh, this is a 300 level talk. We have a brief introduction, but we do not have time. To talk about what is platform engineering, we don't have time to talk about what are LLMs, we kind of have to just jump straight in, hopefully to stuff that is more useful to you, that you can take home and and use in your organizations when you get back from Vegas. So we're gonna jump straight into uh how we're seeing organizations uh provide AI capabilities to platform teams and to application teams on top of their platform capabilities that already exist. We're gonna look at how they are using open source tools to do that. Hasith will show us some great stuff from what Cisco has been doing in their platform at Cisco as part of Outshift, and then he will talk about how they're open sourcing that work, so that hopefully we can jumpstart you to try some of this stuff out yourself, and then we'll be wrapping up. So Platform engineering, this is all we can all we can really base on a platform engineering, continues to be a very popular way for customers to. Operationalize on top of AWS, um, the, the Dora report from this year, 76% of organizations say that they have at least one dedicated platform team, at least one, some more than, more than one, but in terms of like just what is platform engineering, I like to think about it in terms of building centralized capabilities. Um, some of these capabilities might be golden pass to production, right, stamping out a standard CICD process that gets you from zero to production really fast, maybe self-service capabilities, right? Backstage, for example, being a very popular developer portal to do that, uh, standard observability that comes out of the box that just works once you've stamped out your application. Um, and usually some sort of abstraction around the compute. A lot of the time we see this being Kubernetes, but we have AWS customers who are building platforms on top of, uh, Amazon ETS and AWS, even AWS Lambda customers are starting to embrace some platform engineering. So it doesn't matter what you're using to run your workloads, we're seeing everyone deal with platform engineering in some way. Now when it comes to platform engineering, there is a certain amount of technology bingo that you inevitably have to play. Um, sometimes I like to call this the, the CNCF hipster stack. These obviously are not by any means technologies I am recommending. These are just some examples of, of some of the things that we most commonly maybe see folks doing, especially in the Kubernetes world. There are like 5 alternatives to. Each one of these that you could potentially choose, do not take these as recommendations from me, but from an open source perspective, this is what we see a lot of, and these are tools that are open source, battle tested, and we've seen a lot of community adoption around, and they'll also help us break into some of the AI stuff a little bit later. Um, but for those of you who here would say they're already doing some form of platform engineering in their, in their organizations. Yeah, quite, quite a bit of you. So you of all people will know that getting platform engineering up and running can be pretty challenging, uh, even at the best of times. It's more you have to combine all those tools that we just saw with your infrastructure, your organization, how it works, it can get very complex very quickly, and if you don't build a good developer experience around all those tools, your adoption is probably going to struggle. The developers that do adopt your platform are probably going to struggle using the platform. And the cognitive loads that they experience can start to spiral, right? There could be abstractions that just weren't thought through all the way, or it could be documentation, a wiki that hasn't been updated since 2022, right? It, there's lots of reasons why this can start to stumble and what ends up happening is the platform. Just ends up handholding all of the time, right, instead of building out more capabilities and more features. And what we're not looking for here are teams, platform teams are just vending out Kubernete's clusters, right? We're looking to build stuff that makes developers more productive, and you're not going to be able to do that if you're just simply answering Slack messages or Jira tickets all day, just trying to get them through the day. Now, one of the open source initiatives at AWS that we helped form. To try to help with these, these, these platform engineering conundrums is the cloud native operational excellence, which is quite, quite a mouthful, but we're shortened to canoe, and this is essentially a group of large, Organizations that have embraced platform engineering that are collaborating out in the open and sharing. Their approaches, their strategies, their tool sets, um, and reference architectures for how they're going about doing this. So, if you haven't checked out Canoe, it's, it's potentially very useful, uh, to take a look at for your, for your platform engineering efforts and maybe you'll learn something from, from taking a look there. Um, but another tool that we're seeing organizations reach for is, is AI, which is probably unsurprising this, this year of 2025, um, and it's been pretty cool. To see AI evolve since, since it first started, right, from LLMs where we just threw a prompt, we got a paragraph of text or some code or an image, moving on to slightly more sophisticated agents, right, that would start to uh be able to break down tasks into smaller steps and potentially reach out to. External information, right, through APIs or even take actions. And I think one of the things that we're starting to see even more recently is, Giving those agents on the far right here even more autonomy, where they're maybe not even triggered manually, they are reacting to events in your existing architecture and taking at least some form of action, usually still with some level of review and approval, um, so that you're spending less time having to ask the eye to do something and it's it's kind of just starting on, on your behalf. Now, when it comes to developers and AI, I think coding seems to have been a pretty popular use of of LLMs, right? Uh, the Dora report again this year, which has been renamed, this caught me off guard a little bit, it's been renamed to the state of AI assisted software development. So the state of DevOps is gone. Uh, but this, uh, respondents here said 90% report using AI in their daily work. 80% said it's made them more productive. Now. Whether you believe the 80% number or not, I'll leave that up to you. There's lots of different studies and reports getting done, but 90% of developers are reporting to be using it, which means the ship has sailed. People are using it daily, so as platform engineers, we need to start to figure out how we dovetail AI with our platform abstractions, so that the AI is working with our platforms and developers are not pumping out code and configuration that just doesn't work with our platforms that we've built in our organizations to make them more productive in the first place. Another useful stat, uh, the median developer spends less than one hour a day coding. So if all we think about when we think about AI is coding, hands on developing code, we're we're neglecting the rest of their day, right? Their CICD pipelines are going to be breaking, they have to fix them. They're going to be patching vulnerabilities. They're gonna be working on issues in PRs, uh, commenting and reviewing. They're going to be, uh, dealing with incidents at 2 o'clock in the morning, they're going to be cost optimizing. So if we want to really make them more productive, we can't just concentrate on generating code for them. There's lots of other areas where we could benefit that naturally fit as part of platform engineering that I think we, we want to think about. And if we want to start to spread this to more areas of platform engineering and developers, we need to start to spread out where we can start to, you know, inject AI in the places where developers are, right, meet them where they're at. And you know, developers are Coding in their IDE and increasingly in in the CLI now, right, tools like Kiro, CLI, Cloud code and all the other ones that you have, have them coding in the CLI more now. They're working on those issues and those PRs and GitHub. They're navigating their org in backstage, they are working on incidents and incident management. All of these are places where we can start to inject AI to help make them more productive and to make sure our platform. And the abstractions we've built, and all the productivity stuff that we have is further helping them along in all these different areas. So let's take a look at what this could actually look like. As a real example, now. I more than anyone love to give live demos of Jenny I, um, it takes the usual risk factor of a demo and pumps it up just a little bit more, makes you a little bit more nervous, but, This time last year in the Mandalay Bay, a chunk of their Wi Fi went out, and so I'm gonna use that as an excuse to dodge that bullet today, so I have something that has been pre-recorded, um, but I, I would love to be showing this live, so. In our case here, our developer John is innovating and transforming away, coding, using that one hour of time he's got, and his pipeline fails. The contents of the pipeline don't really matter that much, but we can see over on the far right, our deployment to our staging environment has failed. Now, the platform that John's team uses is built on Kubernetes. Uses Argo CD for GitOps, and, well, for some reason code pipeline, but that was just the easiest thing for me to get up and running. Um, but we want John to figure out what's what's broken with his, with his pipeline, so John is gonna hop into Hero CLI. Now if you're not familiar with Kiiro CLI, it was up until very recently Q Developer CLI, uh, it was recently rebranded to, to the Kiiro brand. And it's a tool that we can use in our command line to use AI directly to write applications, to deploy applications, to write tests, all sorts of great stuff. What we're showing here does not have to be done with Qiro, right? Anything like, like your clawed codes, for example, are perfectly capable of doing this, right? This is the open source track. I'm not explicitly endorsing any AWS stuff to do this. There's lots of things that you can do, most of the stuff that I'm showing here. Now, here, one of the things that these tools often have is what we call tools, right? It's a way for the AI, the model, to reach out and do something, usually or get something, right? It could be to read a file, it could be to write a file, it could be to access an API, you can do all sorts of great stuff with tools. Now, our platform team, if you see down the bottom there has added a single tool called query. I know it's a bit small, but I couldn't make it much, much bigger, um, so they've built us a custom MCP. Now, in our case, our awesome platform team has built us a centralized AI that developers can use that's reusable from all the different places that our developers are working, um, so what, John is gonna do is use our MCP hook into our centralized AI to ask troubleshoot why the last CICD pipeline execution failed for my payment API workload. Now he's not using any system identifiers, he's not saying anything about code pipeline or Argo or Kubernetes or anything like that, it's a pretty vague statement. Um, but because after we approve the execution, because we've hooked it into our, our central AI agent, it will go off and start to do a ton of work for us, right? It's going to. Crunch through a bunch of our systems, look at, you know, what, what went wrong and come back with a recommendation. I'll give you a more detailed idea of what happened there once we've covered off some of the tech, but it comes back and says, Uh, well, it looks like either John or one of his teammates, I'm not pointing fingers, managed to fat finger their last update into, uh, their helm values file, and they set the resource request for the Kubernetes memory higher than the limits, and the Kubernetes API kicked it back. That caused the Argo CD push to fail, that caused the pipeline to fail, and they got their notification. So Hiro has come back and offered some options to remediate that. We'll take option one as a suggestion, and at this point, Hiro takes all the information we got from our upstream central platform agent and does the local work on our, on our, on our desktop, sorry, on our laptop. To actually put the fix into action, I think this part is really important. We see a lot of Jenny I demos where people just spit out a bunch of information, and leave you to imagine what can be done. I really like to show Jenny I filling the full loop, so we actually get a remediation to the problem that we had, rather than just leaving them with, I found a problem, what are you gonna do about it? So here we're actually solving the problem, Hero is updating the YAML file, and then all we have to do is review it. Push it And our pipeline will, will kick through and we've solved our issue, and that's taken us from issue to remediation pretty quickly with John doing very little work other than reviewing. Now earlier I talked about Different channels the developers are working in, so, since we've built this AI agent centrally as part of our platform capabilities, right, not directly in like our peer or our cloud code, we can call it from Slack and get the same thing back. Now obviously Slack is not gonna go and update our code for us, but just to show that we get the same result from the same AI in different places. We can do this across multiple different channels and get, get the same result. But what if, and this goes towards that more autonomous mode, the pipeline fails, it fires an event. The AI triggers automatically and just raises a pull request, to solve our problem. This is where we start to get to that more autonomous part of the equation where, We don't have to trigger the AI explicitly, it's triggered through an event, but we still get to review the proposed change, we're not just letting it run riot and update our environment on the fly, we still get to go in and check and validate the change that it made, approve it and promote it, and get all the assistance that we get from AI while still having a human in the loop. And I think that There's tons of other, to be honest, I had a bunch of other use cases, I don't have time to put them in. I think there's just, this is just scratching the surface. I think your imaginations are probably also covering a bunch of stuff we can do here, whether it's just simply, Helping our developers when they code building with our platforms, right, our helm templates or our our helm charts that we've built internally, our terraform modules that we've defined centrally, all of this stuff becomes accessible across all of our different channels to help them build and ask questions of. We can troubleshoot production issues or CIACD issues or anything trying to reduce mean time to recovery or just the amount of work the developers have to do to fix them. We can. Use them for security use cases, I think, you know, we, we have already a ton of great tools for finding vulnerabilities and, and even, you know, tools like Dependabot and renovate that will raise PRs, but there's still a lot of work involved in many of these, these, these cases to like actually fix the problem, and we can start to factor that in too. And finally, cost optimization, not necessarily trying to replace all the great tools that we have, but how do we take a cost optimization recommendation. And actually do it as part of the abstractions we have in our platform, whether that's things like t-shirt sized. Instance types or whatever else you've built to make it easy for developers to consume. So that is, I think. Some art of the possible around what you can do, right, a real example I think that I hope is pretty realistic and you could see happening in your organization, so we can switch gears now from that to how, Was that put together, right? And this will dovetail into what Hasita's gonna talk about in a little bit. So the first thing we need is, uh, and this will build up to our overall diagram of what that demo was built with. The first thing we need is a is an agent now. Agents themselves are, I guess compared to the LLM's relatively straightforward, right, you've got that agent loop, which is just a continuous loop of input, decision, tools, and user response. Um, but you, you, there's other stuff that you want there, right? You want model flexibility. The models are evolving at such a fast rate, you have to be able to keep up, right? You need to be able to switch that out. You need session management, you need memory. You want observability to make sure you understand what's going on on the covers. Um, so these frameworks are designed so that you, like any framework, you don't have to start from scratch. And if there's one thing that we've learned from the JavaScript community. There's always room for one more framework, right? There's, there's just, you can always have one more. Now this is, again, not a recommendation by any means for what to use. There are probably 3 or 4 times as many frameworks available as I've listed on this slide. These are just some of the popular ones and strands on the left is, is one from AWS which is open source as well. Um, and each has their own strengths and opinions on how you attack this problem, and it's really up to you guys to figure out what makes more sense, but, The point is, you don't start from scratch, right? You've got something to work with. Now, once we have our agent and our model. We've got something pretty powerful. But It's gonna be very generic. It's gonna give us public documentation answers or medium posts or stack overflow Q&A. What we really need to make it productive, like we saw in that example just there is it needs to know about our platform and hopefully be able to take some actions related to our platform, right? So, look at all, I mean, look at all this great context we have, right, that you can, that you can get into those agents to help them actually do. Things that are specific to your organization, right? Even just simple things like all your, your documentation, which you can search through. There's hopefully the software catalog that you're building out through Backstage, that maps your workloads, the owners, their relationships, the infrastructure that applies to them, all of a sudden that sort of information becomes a gold mine, um, all the other stuff that we have our CICD system that we just showed, kind of getting used, cloud cost, incident management, all this becomes, The difference between asking the agent, how do I deploy my app and getting a Kubernetes YouTube tutorial, to getting. How to use, how to actually deploy using your platform, and this is what makes the difference, and if we want to, and if we want to allow it, potentially taking actions to make that even quicker. Now this is where uh most of you are probably familiar with this, but we just wanna make sure we fill out the picture, the model context protocol comes in, or MCP. So this is essentially our standard protocol for connecting AI to other stuff, right, so, letting it query things, letting it potentially take actions on our behalf, um, and there's a few things to call out here. Firstly, because it's standard, it works across AI sorry, against across agent frameworks. And across different models, we can build an MCP server once and use it hopefully everywhere. Um, real-time data also becomes the thing here, so we're, we're actually hitting the APIs and getting the data back, we're not hitting a knowledge base that you've built up that could potentially be stale. There are pros and cons there, but this gets us straight to the source of the data and we can also take actions potentially if we want to. And lastly, the specification is being developed out in the open by a whole bunch of companies originally developed by anthropic, but now a lot of organizations including AWS are contributing to that specification. Now, it's not just the specification that's being developed in the open, a lot of organizations are building their MCP servers in the open. Um, now, on the AWS side, we have, uh, this MCP repository under AWS Labs that has, well, this, I made this slide a little bit ago, 55 plus, it's probably more by now, um, and this gives you all sorts of great stuff, pre-built MCP servers for, for great things, uh. The AWS API MCP server basically gives your AI agent access to the AWS CLI. But without actually having like shell access, for example, so it's a really interesting approach to a token efficient way of navigating your entire AWS account, which you obviously lock down using permissions. Um, actually, and this is one of the reasons why I like to do these things so early on a Monday. Yesterday we just announced preview of the hosted AWS API MCP so you don't even have to run this yourself now, we just offer you an API endpoint. That's protected through SIGV4 authentication that you can just hit, so you don't have to run this locally, or remotely, we just give it to you now and you can just start using it, so take a look at that, there's blogs and stuff that I didn't have time to add to these slides, but that's a, that's a great addition, um, but a lot of these other ones, uh, we can, Uh, use the Knowledge MCP servers for the agent to hit AWS docks. We can use the Dynamo one to grab data straight from Dynamo. Uh, we have the cost explorer for, for grabbing cost information. This gives the agents access to so much information. Pretty generic to AWS but still a great start. And if you look more broadly open source, there's an Argo CD MCP server under the Argo project. Backstage now has an MCP server built in. You don't even have a separate MCP server. There's MCP servers starting to get built out as first class citizens for so many open source projects now that you can take off the shelf, plus a lot of organizations that are building hosted ones, like we are, that MCP servers are becoming something you can just grab and use and you don't have to think about building. Um, so if we take our agent and add MCP, it starts to look like this, right? We can access code or issues or PRs through the GitHub MCP server. Maybe we can access pods or events through the Kubernes MCP server, and maybe we access our backstage catalog, our tech docs through the Backstage MCP server. All this becomes pretty straightforward and it makes our agent a lot more specific to our platform almost immediately. But as we start to add more and more and more of all these MCP servers we have now. We start to run into some practical implications. The As you add more tools, the agents tend to start to struggle, picking the right tool and figuring out what to use. The more of a task we give it, maybe the, the context window of our, our models starts to, starts to, we have to start to manage that a bit more carefully. And we can't optimize for specific tasks, right? And uh Lang Chain did a great article on this where they tried to measure the impact of single agent versus multi-agent architectures. So, Running multiple agents is becoming a pattern that we're seeing a lot more commonly, right? It's instead of creating one generalist agent. Each of these individual agents has its own tools, its own prompts, and they become specialists in their own domains, so that we can start to specialize them and make them more efficient at making the right choices and pulling the right information, and a lot of the agent frameworks that we saw earlier have their own opinions of how to do this, and there's lots of ways to do it, right, even in terms of the design, but also, do you run these as a monolith in one container, or do you run them distributed like microservices, you've got even options in that regard. And if you are going to run distributed agents while we're talking open source. The agent to agent protocol is another thing to be aware of. Now this is uh a protocol that came out of Google but has been donated to the Linux Foundation, and where MCP was a standard protocol for connecting agents to information. Agent to agent, as the name suggests, is a standard protocol for connecting agents to each other and letting them collaborate and work together, usually when they're running in a distributed way. So, a few things to note about this one, firstly, it starts to make things interesting from an autonomous discovery, right? Agents can find each other and do almost a form of negotiation of what each other can do so they can figure out where to delegate tasks to. Um, collaboration, the agents can start to work together in more of a collaborative way. And obviously it's also open source, right? The specification is being built out in the open and as far as I'm aware, they're working towards a one of the specification right now, um, that has lots of improvements to it. So if we take a look at, Uh, horrifically simple look at A2A in practice, this means that we can have multiple agents using different agent frameworks, using different models with different MCP servers who can start to work together, and they communicate over A2A, which is HTTP calls, right? Um, things like Jason RPC for example, uh, and this then allows say the agent on the left. To find the agent on the right, figure out what it can do and say, oh, I can use you to, to solve this more specialized task for me instead of doing it myself. One of the ways that they do that, if you see on the right, is what we call an agent card. So this is, if you think about like OIDC, right? Open ID Connect has that well-known endpoint that tells stuff where all the different endpoints are like your token endpoint. This is the same thing for agents, right? It's a well-known endpoint where the agent can advertise its name, a description, a version, the URL, but it can also give examples of things like these, this is the stuff I can do. And here's some examples of prompts you can send me, and this means that agents can actually start to dynamically build prompts themselves based on the examples, you're basically giving prompt engineering tips to another agent through the agent card, so they can work together, which is how these agents can work without super specific instructions, you still give it some hints, but it makes them a lot more decoupled and a lot easier to kind of fit together. So if we take a look at the demo that I showed earlier. This is basically what I pulled together using that stuff that I just showed you. The demo that I showed you earlier was actually more agents than this, but I didn't have space on the slides to put them all on. Um, these are all built using strands as the SDK for my agent framework. I was running an EKS. They're just normal deployments, right? They're just APIs really, um. And in terms of like what it looks like from a workload perspective, it's relatively straightforward. Now Let's take a look at the scenario we did to see how the information flowed through it, so, originally, We were in Quiro, or CLI and we, we asked the original question, and Hiro had that MCP server that we'd built. The MCP server was actually built into my agent. So I actually just built an MCP endpoint into that agent remotely and Kiiro was configured to use it. Uh, it sends an MCP call, so MCP interestingly can be our client protocol here as well as something on the back end, and it sent that query saying troubleshoot my CICD pipeline for me, remote agent, I'd like you to help me. The platform agent reached out to the agents that I'd made it aware of and, you know, said, how can you guys help me, got, got all those agent cards back and started to figure out its strategy using chain of thought, uh or or whatever pattern you you you wanna, you wanna consider and of how these agents do their form of reasoning, um, and then it formulated its plan. So the first thing it did was it reached out to my catalog agent. Which reached out itself over MCP to Backstage, it hit that MCP so right inside Backstage and got my catalog information about turning payment API into a workload identifier that was more specific about what I want to do, right, so I clarified exactly what I was working with. The platform agent then said, OK, I know exactly what workload you're talking about now. I'm gonna go and hit the CICD agent, which I know can help with my CICD pipelines, and it then hit. The AWS APIMCP server to get my code pipeline status, my code build logs. It hit my Argo CDMCP server and saw that the application in Argo CD failed to sync and actually got the Kubernete's events back, which said, The requests and the limits were mismatching. And then finally it reached out to GitHub, and if you were eagle-eyed earlier with the smaller text you might have noticed there was actually a commit ID it mentioned when it was troubleshooting earlier, it actually reached out to GitHub and pulled the commit that triggered my pipeline so it could check the code change that was done. And it takes all of that And it comes back over and the agent itself, the CICD agent formulates a response to the platform agent, which then came back to Quiro and hero was then able to do that little bit of work, you know, locally to update the file so that we could push the fix and resolve the resolve the issue. The Slack example of messaging worked pretty much the exact same way. All this stuff in the center that our platform team has built as a centralized capability as an API for us using all that open source technology. Is reusable across those channels, I just, from the Slack side called A2A directly instead of MCP er that was the only difference, um, but you, you, as you saw, you got the same result. So, to wrap up this section, when we're building uh all these capabilities as part of our platform team, open source is giving us so much to work with, right? We get those protocols like MCP and A2A that are being developed out in the open. We get those agent frameworks that we have. A whole swath of to to pick from based on exactly how you want to work and how you how your opinions are formed, and then we have the MCP servers themselves, which you're increasingly just able to take off the shelf and platform engineering especially has a ton of these that are just available for you to use that you can take and and use as part of building out these capabilities. So with that, I'm gonna hand over to Haith to talk a little bit more about exactly what they've been doing with this in practice over at Cisco. Thank you, Neil. Bye. Thank you. OK, uh, let me start a little bit with what OutShift is. So OutShift is Cisco's incubation unit. So we very much look at what's emerging in the future, and currently there have been two focus areas. One is agentic AI and the other one is quantum computing. Uh, and for agentic AI, uh, we have been doing various, uh, explorations and work in the last, uh, uh, 22 years or so. Uh, most, one, most recent one being, uh, uh, open sourcing of agent collective, uh, to power the future internet of agents. Uh, now, let's look at Oshift platform, uh, at a high level. So this is a simplified overview of, uh, what OutShift, uh, platform is. Um, we have a single cloud provider strategy for speed. There are 3 environments or dev environments, staging, and production, for engineers to incubate and take ideas all the way into production. There's edge computing when it comes to things like GPUs or quantum processing units, or where you have data concerns around influencing or training. Uh, and then, uh, you have, uh, command and control and CICD aspects in another AWS account, as well as uh some Cisco-specific security functionality around secrets, uh, active security, vulnerability scanning. Then we also have Splunk that we use as a central, centralized observation, observability platform. So let's think about the history a little bit. So, if you go back about 15 years, we had Devonops split, developers were throwing things over the fence to operations, and then we introduced DevOps and SRE also came in, came in about around the same time. And then you had microservices. containers, Cubanetti, Cloud Native, everything exploding with complexity and diversity. So today you have, you know, 10 different ways of doing something, and it's a, it's a problem. So in the recent years, platform engineering is deliberately introduced as a bottleneck. You guys might have read this famous book called The Goal. Bottlenecks are not a bad thing, they're good. However, The problem is because it's a bottleneck with developers and platform engineers on either sides, if we don't operate the bottleneck efficiently, that leads to an ineffective platform, and then that creates problems. So because of this, why platform engineering is challenging and most in most organizations, they're not as successful as they aim to be. Uh, now, In fact, with AI, it's even adding a lot more. Toil into the exists into this bottleneck. So you can leverage AI in order to sustain platform engineering as we know it and evolve it for the next years. Uh, now, let's, uh, let me share a story in the last two years. Um, so I started a new role, uh, back in January 2024, uh, responsible for all things platform and all things security at the incubation unit. Uh, what I found when I walked up to the job was that there was a somewhat burnt out SRE team being pulled in so many different directions, uh, being in an incubation unit, and With the other efforts going on, we started this grassroots effort around how can we apply agentic AI into platform engineering. Not a, not a top-down project, uh, it was a very much bottoms up project. Uh, we had some ideas, uh, we trialed a few things through internships, and there was some one exploration project that was going on. Uh, then we also had to change the workflow. Uh, and we were trying to think, hey, should we use something like Argo workflow or should we leverage Landgraf and think about it as a workflow engine, uh, which is somewhat contradictory, but that actually quite worked, and we ended up with a multi-agent system that was quite successful. Now, if you look at what's been happening in the industry since then. Uh, MCP was exploding March 2025. Uh, you had the A2A and agency collective that I mentioned, uh, and we also joined this, uh, canoe, um, uh, Cloud Native Operation Excellence Group, and then it made a lot of sense to open source this effort. To, uh, make a, a special interest group in the uh in, in CU and then uh create a uh Cape, which is uh uh cloud native AI platform engineering, uh, built by the community for the community. Uh, now This is a visual representation of How it looks at Outshift. So, in one end you have the OShift developers, uh, they can, uh, uh, talk to the agenttic system through the many of the existing interfaces. So we use Webex for as an instant instant messaging, uh, Backstage as an internal developer portal, and then you have Jira and you have uh your CLI and the, uh, you know, the IDE. And then in terms of the functionality. Uh, you have, uh, knowledge bases. This is a, uh, extremely useful place to start with, uh, because most of the time, uh, in platform engineering, you have, uh, documentation in wikis, playbooks, uh, and a lot of, uh, uh, tribal knowledge in these, uh, uh, type of, uh, locations, including chat history. Uh this is uh very useful to gather. And then you have live tool calling that you can do in order query systems, and then often you end up having fragmented data sources like your vulnerabilities are in one system and you may have to correlate certain things. So combining this, you can really get a lot of insights and data very quickly. So things like insights, even with access to systems, a platform engineer could take. 2 hours, whereas the agentic system could answer that within within a minute or 2. Now then, let's think about self-service and other tool calling. holy grail of current platform engineering is to get to a form. That you can click submit. However, the problem is understanding what's in that form and actually filling it out in the right way, getting to the right form, is not straightforward, and there's a lot of back and forth that happens. An agenttic system is perfect here because gentic system can close that gap and be a personal butler almost to any developer. To guide them through that process and then we also did some more advanced stuff. We have this EK sandbox where using natural language, a developer can iterate through the Cubanities an application on Cubanities without having much knowledge on Cubanities at all. Now, let's talk about the impact. So, before any of the agentic AI was being introduced, we had a dedicated 3 engineer support desk, and there was a lot of toil there, a lot of requests coming through. We have managed to almost completely remove that and leverage that time in order to do more creative and engineering work. And things like questions. There was a lot of questions being asked in multiple ask rooms. We had over 20 spaces where questions were being fired, not everything was being answered quickly, and now you have AI systems that people can use to get the answers, as well as it can intercept and give an answer if there's high confidence. Now, some simple tasks like, hey, I need an LLM key or I need a dev machine used to take, you know, half a day, a day or multiple days. Sometimes nobody had looked at it, but these types of things are now end to end automated with the agentic system helping out. Uh OK. So, here's our internal developer portal. So, this is uh what it looks like, um, yeah, yeah. Built on backstage, uh, you know, you may recognize a few things. Now, uh, on the bottom, um, Uh, right hand corner, you have this icon you can click in order to bring up the chat interface for the agentic system. Uh, and when you click that, you have this, uh, friendly interface, uh, hey, I'm, uh, Cape, uh, how can I help you with? Uh, and if you, if you go on to ask, uh, hey, what can you do, it'll list out, uh, many things around the CICD, uh, uh, life cycle, uh, with, with all the tooling and tooling it has access to that it can, uh, uh, help you with. So let's actually go for an example. Let's say I'm doing a new gente project and I need to get an LLM key. We have this request happening multiple times a day. So this was something that was taking at least half a day and many attempts at it, and there was a lot of contention here with a lot of incubation going on. So, whereas, when you ask something like this, uh, the system comes back, hey, I need to know where to get it from, the provider, can you tell me what model you want, uh, and which project are you part of, so it can be attributed and whatnot. So, you complete that, uh, it, uh, processes it, and the information gets sent, and here's the, uh, here's an example of the information coming. Now, something that used to take half a day. It's done under 2 minutes, end to end, uh, um, and it's also done with the AI LLM gateway behind, so we have all the good platform engineering practices applied. It's not a a key that is given uh without any auditing and tracing. OK, let's look at another uh example. Uh, so, here's, here's a Jira ticket. I'm trying to do something else. Uh, uh, so, same type of request, and I'm asking for a, a development machine. I haven't exactly specified what it is, uh, and I'm kind of saying, hey, I probably need an EC2 type of instance, maybe Ubuntu. Uh, can you recommend me something? And I create a Jira, uh, ticket on this. Now it hits the service desk, so the service desk engineer knows, oh, Jarvis can take care of this. The service desk engineer goes and assign it to Jarvis, and the system takes over. Now we currently have a human here, but in the future, potentially that assignment can automatically be done as well. Now, uh, then, It processes the information. Now, Jahavis is a bit like the knowledgeable new BSRE, right? So it's almost like it knows about what systems are available. It says, hey, I can create an EC-2. I can create an EKS as well. What do you want? And it's giving all the options around it. So in some ways, a more experienced SRE probably would not even mention EKS here. So, so in this type of situation, the systems could be potentially improved further if you don't want it to kind of offer EKS, uh, but here you have all the options, and, uh, now I'm gonna respond to Jarvis saying, hey, uh, I need a uh EC2 instance. Here are the details, uh, use this account. Uh, hey, give me Ubuntu, please, uh, and no EKS cluster is needed. And then it goes back to an actual human in the loop approval flow, so it hits the SRE uh service desk and somebody in SRE need to approve it in order for this request to be served, and it's, it's approved in the normal GitOps workflow just by giving approval on the PR. You get a link in the chat interface. Uh, about what we use at work for collaboration, so it's not outside their existing workflow. It's similar to how normal GitOs is done. OK, and then, uh, once that's all sorted out, uh, I get the information. Hey, here's the details. You can access your uh dev instance. Uh, here's a private key. Uh, it's sent on a secure channel, so we have, uh, you know, handle, handle responsibly. So, so something complex like that into an automated with the assistance of gente, and the, and the beauty of it is. The agentic systems can go back and forth for any incomplete or incorrect information. So what happens with the Jira ticket type of a situation is there's a lot of back and forth going on between two humans asynchronously, whereas now here you have an agentic system which can validate as well as instantly respond when your user is asking for specific things and information. So that way it can reduce a significant amount of toil that people face in normal platform engineering teams. OK, let's now think about a slightly different use case, very typical scenario, I think most of us have been here. So there was a big outage. Asari on call didn't sleep at all. So, uh, OK, we, we want to, it's morning, so let's, uh, help them out. Uh, so I'm asking, hey, can you get the JIAS and what's open, uh, so, maybe, you know, we can sort, sort those things out, uh, giving a better experience for the customers as well as not troubling the SRA guy and making sure he gets some sleep. Right. So, uh, the, uh, agent, so supervisor, uh, Uh, look, plans the tasks it needs to do. Uh, I need to, uh, query pager duty. I need to, uh, uh, look at Jira for what's open, then I need to present my finding. Um, this is actually called deep agents. Uh, it's a new concept that's, that's emerging. Uh, so, the supervisor is a deep agent who's able to, uh, populate it, populate a list of tasks and accomplish this. And you can see the list of tasks are done, including one additional step that was discovered during that process, and then you have the response. Hey, the SRE on call is this person, you can reach that person on this particular address. There's a link to pager duty schedule in case you know you want to go and check it manually. It explains how it looked at Jira. And then, uh, uh, most importantly, it has all the 4 issues found with the links, so I can now do something about it, uh, and, and take that burden off that, uh, SRA. So, uh, Also looking at how this was executed, it was done within 40 seconds. Obviously many iterations and steps are involved, so you can see the top half of it was the agent supervisor planning, planning how to accomplish the tasks. Then it decided that it needed two paralyzed agents to do it. And then it got the information processed and then the response to the user was presented. Now, this is a relatively simple example. You can do things that are significantly more complicated with involving tens of agents with the available tooling you have, so you can Imagine like if the system is uh have those capabilities, you can really do very complex tasks, uh, explaining and telling, hey, this is what I want to accomplish, and you can also iterate on those. Now, let's talk about the challenges. So, many technical challenges. So you have, uh, obviously the usual cost accuracy, uh, needing golden data sets and trajectories. Uh, you, you end up having to use LLM as a judge in order to evaluate the system. Then, uh, once you get acceptable performance, um, When you iterate on it, there's a cost because if you change a model or if you change, if you introduce a new agent, the system behavior could change significantly. So CI4, these new probabilistic systems need a lot of thought and very decent CI process if you don't want to get burned by that. And obviously there's a lot of safety, security, governance concerns that are being introduced by agentic systems for us in particular. One of the biggest issues was all the tooling that the sub agents use. They have privileged access more often, but then on the users on one hand have various RBC, including, you know, hey, if you're an AA, you have certain RBAC. If you're in a particular project, you have different RBA. So. There is, there's a significant risk of privilege escalation accidentally or intentionally which need to be safeguarded. Um, now, This is a fundamental transformation both for your users and the team itself because you usually have a lot of distrust in AI and as well as if you don't think from a growth mindset and a learning type of an attitude, it's going to be very difficult to actually make these type of transformations possible. I mean, I would say there's a, when you look at the technology, so many things are possible. But it's that human transformation and uh uh aspects are the most challenging in an organization. OK. Now, let's uh move on to uh what we are doing around the community AI platform engineer. Uh, so, Cape is uh very much, uh, an attempt to redefine platform engineering, leveraging AI built by the community, for the community. Uh, what it is, is, uh, uh, it's got, uh, It's a, it's a scalable system that you can apply in production. You have a built-in knowledge base. You have many open source agents. You can interact with developers at different workflows, and it's built on open source technology, so MCP, A2A, Landgraf. Uh, in terms of, uh, so if you, if you abstract anything to MCPOA2A, you can integrate with the system, in the multi-agency system. Uh, frameworks, currently you have Landgraf and strands agents there. You can use other frameworks as well. And then when it comes to agentic observability, everything is abstracted to hotel. And then you have agents and you can have specific multi-agent systems to serve specific needs. Uh, the backstage plug-in, that's open sourced, so this is A2A compatible chat plug-in. It integrates with Cape. You can potentially integrate with other things as well. Now, the important thing is, it's not just about the chat. So things like streaming, what's happening in the system, uh, forms generated by structured inputs, structured output. Are key to having a very good use experience, because if you, uh, you know, simple things like presenting a form with the right things populated, makes a massive difference to whether the workflow actually works for people or not. Now I'd say My big advice would be, if you're starting with something, start with the the knowledge aspect, because that's a low barrier to enter. Uh, it's, you know, help me understand my platform documentation before going and changing my production deployments. Um, and Better context you have in terms of playbooks, wikis, uh, tribal knowledge, if you can get into the agentic system, it will lead to a quality outcome because lots of the time, and especially if you're leveraging LLMs that we do today rather than training specific models, having that, uh, uh, you know, for, for high quality outcome, knowledge is the key. And then, obviously, if you have a good system with internal knowledge. In the future, you can build more capable AI systems as well. Now, uh, Uh, this is, uh, uh, uh. Uh, unified rag architecture. Uh, we are on the 3rd iteration now. Um, so it has, uh, both rag and graph rack that you can use. You can ingest data from different systems, as well as, uh, an ontology agent that would map out the relationships automatically. It's all open source, so you can try it out, uh, and also connect and evolve as you need for your um own enterprise needs. Now, uh, uh, get involved in the community. You can, uh, access it at, uh, cape.io. Uh, we have weekly, um, uh, meetups, uh, yeah, and do, if you like the project, do support it with a GitHubar to bookmark and, uh, support the project as well. I'm gonna hand over to Neil now. Uh, yeah, thank you. All right, thank you so much, Joseph. Uh, I'm really grateful that, uh, Haith came over to share what Cisco is doing in, in practice. It's one thing for me to, to stand here and, and, and show you guys some stuff, but having Jesi talked to what they're actually doing at Cisco, I, I hope is valuable to see what people are doing in the real world. So, um, with this, we just want to wrap up a bit now, uh. As I said at the beginning, this, this is a tricky topic, I think for us to figure out what to share with you guys. Obviously, there is so much going on in this area. We're building on top of platform engineering and AI and layering on top of this. So I really hope that what we've been able to share with you today has been something that will maybe inspire you to try, try stuff in your organization, give you an idea of how you can go about doing it in an actionable way. Um, obviously, the Cape project gives you a lot of useful stuff to get started, but just to quickly recap, I guess. It's not just the folks at at Cisco Outshift they're doing this, right? I'm talking to customers, uh, many weeks that are also looking at doing variations of this, and there are other talks that you will see at reinvent along these lines. I believe there's one from Salesforce we're going to talk about. Something that I think is a bit more on the incident management side, but is very much in this platform engineering case that I would recommend you look up in the catalog, if you want to see, see more of that, but the Cisco is by any means not an outlier here. There, there are lots of folks looking at this, and it's something which I think is useful to at least invest a little bit of time in. Um, open source, as with many things that we do these days, is giving us a pretty solid foundation, right? We don't have to start from scratch. We have our agent frameworks, our MCP servers and all those great protocols that we can use for interoperability that are all being developed out in the open that you can get used to get started quicker along with Cape, hopefully that you guys can take a look at and they can, if you are interested in taking a look at this at your organization, get you off the ground quicker. So, with that, we are going to. Thank you so much for taking the time to come down to Manly to hang out with us this morning. We really appreciate you taking the time. Please rate the session, leave us reviews. That's very important for us. Uh, it lets us know how we're doing, and we really appreciate if you take the time to fill that form. If you would like to chat with us, we're gonna go hang outside for a little bit. Um, we're not allowed to take questions right now, but other than that, I hope you guys have a great reinvent and thank you so much for, for coming along.