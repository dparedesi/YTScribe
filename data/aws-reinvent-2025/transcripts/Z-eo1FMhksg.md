---
video_id: Z-eo1FMhksg
video_url: https://www.youtube.com/watch?v=Z-eo1FMhksg
is_generated: False
is_translatable: True
---

Hey folks, thanks for being here. Hey Umaya, what's up? Hey Mustafa, I'm doing great, but I could use another coffee, not enough. Or you want another cup of coffee? Say you pull up your phone now, Umaya, and you find a coffee shop nearby. And you walk up to this store only to see that they are closed for maintenance. That would suck, right? Yeah, yeah, sad smiley right there, yeah. So are you a coffee geek like myself? Yes, yeah, they sell coffee in Atlanta too. There is, yeah, yeah. All right. I mean, in Seattle it's the law. You have to be a coffee geek. Do you follow your local coffee shop on social media? Yeah, of course I do. Yeah, I only drink like 5 coffees, uh, uh, before 10:00 a.m., before 10:00 a.m. Yeah, I didn't ask if you, if you have an addiction or something. I asked if you had a coffee. I might have one. I see. So say you get an update like this on social media from your local coffee shop. They say they have new features. They have new syrups to choose from, new baked goods. So you go up there and check it out, and it's open, which is a great sign. But then you see the coffee machine is broken. So how would you feel? Yeah, almost crying, maybe actually crying. I see. You have coffee addiction, that's for sure. So I think these coffee shops could use some operational excellence and reliability, and that's the topic of our conversation today. I'm Mustafa Tun. I'm a senior principal engineer in AWS. I've been with the company for 12.5 years now, and most of that time was with Cloudwatch or observability and the monitoring team. And I have Amaya joining me on stage. Yeah, folks, my name is Emma Majaganathan. I lead the worldwide specialist Solution architecture team at AWS and my team focuses on cloud operations. I've been at AWS for close to 8 years now, uh, working almost all the time on cloudwatch and observability services. So, uh, I want to start with the word itself, excellence, and, uh, it's not perfection, it's excellence. We, we use this word for a reason, right? So the, the, the difference between these two words is better two leadership principles of Amazon. We have more than 2, but these two apply here bias for action and insist on highest standards. So one of them asks for speed, one of them says take calculated risks. The other one says it might be uncomfortable sometimes, but you should push for the highest quality, right? And there were times I was pushing my team on this end where we were very careful, but we were not shipping as fast, right? So in the, in the center of this, in the balance stays excellence. So it's not perfection. Perfection would instill fear, perfection would slow us down and it wouldn't accept mistakes, right? So with Operational excellence, we understand that we will make mistakes at times because we have to move forward, but the key is we learn from our mistakes. Let's get back to our coffee shop and see what would happen if AWS was operating this coffee shop. So we would add redundancy at the infrastructure level. We would add much more coffee machines to the store, just like how we run our systems on multiple servers. We create multiple copies of the data you give us, and we wouldn't run just one coffee shop. We would run multiples of these just like how we put multiple data centers within a zone. And of course, we will put them in uh uh different parts of the region, different parts of the city, just like how we do available to zones. And, and of course we put these in different parts of the region, uh uh the world, and, and, um, uh, some services are zonal, some services are regional, and, and, uh, all these are uh creating fault isolation boundaries. And there's a great white paper in our website if you want to read more about our infrastructure and fault isolation. And this QR code is a companion website to this talk. So there are links to external resources we are going to refer in the remainder of the talk. So let's talk about some architectural choices we make for reliability and operational excellence. So say we have an AWS service that has 3 dependencies and say one of them is having a bad day. We design our API services such that only those APIs that depend on that dependency is impacted. The rest should be functioning. And this is done through dependency isolation. The simplest thing you can do is to create different thread pools for dependency that are of course more advanced methods. The second one, I'm gonna talk about is cellular architecture. When a ship hits something, when the hull of a big ship is breached, the ship doesn't sink because the hull is made of different cells. So similar idea, we create multiple copies of our stacks and we put them in our data plane next to each other and we route them with a thin layer. So, so this cellular architecture allows us to reduce our blast radius. If one of those cells is impacted, the rest of the services can continue to function. There is a great uh set of uh uh um. Articles up there in, in our website, in Amazon Builders library. Again, the link is in, in the companion website, and then you can read through this architectural choices we make and apply them to your own systems. And again, well, architected framework is also a great resource, and, and it discusses all these topics, the infrastructure, the architecture, and operational excellence. All right, Operational excellence. It's a feature for us. We add features, we, we add new services to, to uh AWS, but we invest equally to operational excellence. Operational excellence for AWS is not an afterthought. It's not a burden. We invest in it. We, we are intentional about it. We know you choose AWS for many reasons, and one big reason is that we pay attention to our operational excellence, and we are serious about it. I'm gonna share some learnings. I've been in the front seat of our operational excellence journey in this decade. I've seen we tried new things and some of them worked, some of them didn't. So I'm gonna learn, I'm gonna share these learnings and one common thing uh you will see is that we don't rely on good intentions. We rely on mechanisms. And I'm going to quote Jeff Bezos. He told this to us before I joined in 2008 in an all hands. He said, often when you find a recurring problem, something that happens over and over again. We pulled the team together, ask them to try harder, do better. Essentially we ask for good intentions. And he says this really works because people already have good intentions, right? And, and he said if good intentions don't work, what works? Mechanisms. Nobody wakes up in the uh in the morning and say I'm gonna make operations worse, or during an incident, nobody's trying to make things worse, right? So everybody's trying their best. So let's talk about, let's see what the mechanism is. It's a system, you have an input, an output, and it starts with a tool. It's not a manifesto you put up, it's not an email you send and expect people to read. You create a tool, and that tool, if it is good, it will drive adoption. People will start using it, and the more people use it, the more feedback they will give to you. And that, that, that, that will allow you to inspect and that retrospect will allow you to improve the tool. As you can see, this is a, a, a virtuous cycle. Things like this are called flywheels also, and if you are a gym person, and if you use the stationary bike, that, that big wheel is flywheel. It comes from mechanics. It keeps the momentum going. It stores the energy. I wouldn't know. I don't go to gyms. I prefer donating to gyms. In January, I have a gym in mind that I'm gonna donate to them. So the first virtuous cycle flywheel in Amazon was introduced by Bezos himself and his team at the time. So they were targeting growth. So customer experience is a driver they focused on. And customer experience will drive traffic, right, if you improve customer experience, and traffic will bring more sellers, more sellers will bring more selection, and more selection will make our customers happy. And as we grow with these drivers, we will have lower cost structure which will lower our prices. And that lower price, low prices will feed into customer experience itself. So I talked about what drivers exist in AWS that, that, that drives our operational excellence because it has gotten a lot better over a decade, and I drove this. This is my take. Uh, I think it starts with observability. We make our services observable. We make uh, we, so that we get more insights from it. We, we can respond to incidents better. So incident response benefits from it. And the more incidents we have, the more we review and learn. So all that data feeds into our review process. We of course don't wait for incidents to happen. We also continuously review our systems, and observability helps with that too. And the more we review, the more action items we take, and it feeds into our readiness process. We become more ready, and most action items we take are, hey, let's close this blind spot, and that feeds into observability. And as the operational excellence keeps growing with these drivers, we become better reviewers ourselves, so it feeds into our review process. We ask better questions as we get better at operational excellence. And in my opinion, operational excellence feeds into Bezos's wheel. Customer experience benefits from operational excellence. So in the remainder of this talk, we are going to stay with this wheel. You will get tired of it, and there, there will be 4 sections, and we will start with readiness, and we will dive deep into each one of these drivers. So, I have 5 topics to discuss in this section, and the first one is literally called operational readiness review. This is a mechanism. That we ask service owners, operators to, to fill a form, a, a checklist, and, and it asks certain questions like this one. These are verbatim questions from a new feature operational readiness review uh template. It says, does this feature have SLOs defined on customer experience metrics and have alarms? So this, this, this is a short question, but critical things are it says create service level objectives, SLOs, so have a goal-oriented operations and focus on customer experience metrics, right? You could be alarming on many things. So this question is a great one, and the second one is Testing, does this feature have testing to discover and address any unexpected performance issues. So it's not just asking for, for, uh, discovering it, it's asking for addressing it, right? So it's, it's asking for a mechanism from the team. These checklists are filled and bar raised by, by barras, and then it has to be approved by a director or above before the service can go live. If, if the bar raiser or the or the director is not happy. The, the team has to address those before they can even take their service online. Speaking of testing, we test our software extensively, of course. I'm not gonna go into every detail of how we test our software, but the thing that applies to our conversation today is we test for failure. We create failure scenarios continuously in our deployment pipelines, and we test for them. Of course, we do load tests. We, our pipelines, every day we check-in runs automated load tests, stresses the system in pre-production just to see if we created any regression or not. One thing that usually is overlooked is we also test for a load from a single user to see if, if the noisy neighbors will, will cause a regression or not. We also do some manual testing days, we call them game days. We literally break our system in these tests and and we we create network outages. We create dependency failures. We simulate these so that we test two things how our systems behave, but more importantly, how our operators behave. Our operators, uh, were they paced on time? Were they able to respond on time? Did they find the correct RAM books? Were they able to find the right dashboards? How long did it take for them to diagnose and mitigate the issue? And, and we learned from these simulations and we, we improve our systems accordingly and our processes and mechanisms. We have a fault injection service in AWS. You can also, we use this ourselves on these days. You can also use it to, to create uh game days in your own organization. We don't want humans to touch our systems. So if, if we believe in automation and, and, and continuous deployment, but if, if a human has to touch at times, we write every step in detail. We call this change management, CMs, and, and the CMs are also bar raised, and, and they have to come from a bar raised template. And what bar users uh pay attention to when they, when they are reviewing the CM. Uh, did the team put rollback steps for each step, right? If, if, if something goes wrong, we want the team to be able to roll back with ease, and we want them, we also check if they have tested this in pre-production, so we don't leave anything to surprise. And we have a team called Release Excellence, so this all this automation is great, but how do we know those pipelines are doing their job? We have, uh, we have systems in place where we analyze all our deployment pipelines and we check them against some rules. Uh, for example, uh, we have 7 rules and this pipeline is not adhering to 7, of those rules, and this pipeline was blocked. So it might be CICD but it's not, it's not, uh, able to deploy because the team has to address these expectations. What are those expectations? Let's, let's look at an example pipeline before we even hit prep road. So, so we have usually 3 stages in our AWS services before prep road, and, and we treat our last stage before prep prod gamma like it's a production environment, and we run all sorts of tests in that stage and, and including load tests. So if this is not in place, if the team missed baking time, for example, that would be a miss and the pipeline would be blocked. Clay Le Gri, uh, uh, uh, uh, another senior PE in our company, wrote a great article, automating safe, hands-off deployments, and, and it's also in, in the builders' library. Let's move on to the second driver of our wheel, observability. So what's observability? It's a measure. It comes from control theory. You can measure a system and say the system is less observable or more observable, and the more observable it is, the more insights you can get out of it. The more, the more observable it is, the more questions you can ask that you didn't even know you should be asking. And. To, to make, to make our systems observable, we basically do two things. We instrument our systems and we standardize them. And, and we of course, we also use a great tool to, to uh accomplish observability. So we run our services all right, but as I said, we pay a good investment into operations and we collect uh uh uh three pillars of observable, metrics, logs, and traces, spans for traces. And, and all, uh one thing I think Amazon did uh right in the sense is we standardize this, and I want to emphasize that. So say you are a, so AWS developer, you are building a new API and say you are using Java. We have a, a library for building API services, and, and you can import that library and import a class called activity, extended activity for your API. And you have to uh implement this function, and this function will be the executed when, when this API hits this particular server. And even if you don't do anything, you deploy literally this piece of code, you will get with every request a blow up like this. This is a simplified example, of course, for demonstration purposes, but we will be measuring many things infrastructure level and application level measurements like duration, CPU, memory used, whether the call was successful or not with request ID and such. And this is called embedded metric format. Uh, and this is available for you. This, this is an external format. If you write this to Cloudwatch logs, Cloudwatch logs will process this directive and will emit metrics out of it. So this directive is saying, take the, take the duration field and, and emit a metric from that field, right? So what team gets uh by just deploying that code which does nothing, all these measurements, uh, uh, and again, it's a simplified example, we collect a lot more data than this. So, of course, the developer can get a metrics object and add their own business measurements, right? They can add accounts, they can add time, they can have their own key value pairs, and all of those will be captured in the, the same adjacent blob as different fields. And since bean count and brew time here are measurements, they will be also added to the EMF directive so that we can get metrics out of EMF. And the team, after deploying this software, they will be able to add these new widgets for their metrics. So it's very super easy to imit your metrics. You don't have to think about the transport, the formatting. It's all uh implemented into the library that you are using to build your application. And again, so observable, right? So metrics are fine, but what if you want to dig deeper and you wanna, you wanna do some analytics? Since in this example, I'm emitting customer name. To, to cloud watch logs. I can run a cloud watch logs query and I can uh run a query like this, which all it does is uh find average brief time and group it by a customer. So, uh, since everything starts as a log, I can do cool things like this. Every request is a blow of log, and some of them are aggregated metrics and, and I can run any analytics I want. Uh, EMF again, uh, is externally available. You could read about it on our companion website. And when it comes to tooling, we use CloudWatch. Amazon and AWS, uh, prefers CloudWatch. Uh, one reason is we built it, right? So we dock food, we use the product ourselves so that we can improve it for our customers. The second thing is it can take our scale. It can, uh, Amazon and AWS generates lots of data, and this is probably an outdated slide by now. So cloud Watch logs is accepting 13 exabytes of logs per month. That's 5 terabytes of logs per second. And, and quadrillion, I think is billion square. So cloud Watch Metrics is accepting 20 quadrillion metric data points every month. So, since I brought this slide up, we probably accepted 100 terabytes of logs already. So, of course we have other priorities and you told us what priorities you have when you choose an observable tool and Imaya will walk us through those. Go ahead. All right, so, so, um. My team and I, we work with hundreds of customers every year, right, engaging in very deep conversations about what customers, uh, you know, want to do in terms of choosing that observability solution. Um, and you have made it very clear for us. We see these kind of patterns emerging as to like what is important for you in when you make decisions on what observability solution to use. And what I'm gonna do is I'm gonna focus on, uh, you know, demonstrating how CloudWatch as a solution meets these expectations from you based on the learnings that we have internally that we have, uh, that most of us discussed so far, right? Um, the first one to begin with is open standards, and, uh, Mustafa talked about the instrumentation part like how the, the standard instrumentation library that is available within Amazon like developers really don't have to worry, although all the essential signals are automatically captured. What you have told us that you wanna use an open, so you, you wanna be open standard. Compliant and there's no question about it. Like everybody would like to be open, um, uh, standards compliant and that is beneficial for obvious reasons and open telemetry is the way to go. So that's why we have AWS GISR for open telemetry SDK and also we have the our instrumentation agent which automatically captures important metrics, industry standard metrics which are red, you know otherwise. Called red metrics, request rate, errors, duration, etc. We also, in addition to it, capture faults as well and uh we also have, uh, Cloud Watch also has uh open telemetry endpoints or OTLP endpoints for logs and traces that you can send data from anywhere whether it's AWS, on-prem, hybrid environment, it doesn't really matter, uh, as long as you have CIGB for authentication, uh, the OTLP endpoint will just accept signals from you and you, you're, you're using open telemetry at that point, right? And uh the second one is, yes, I have all these features and stuff like but if it, if you make it hard for you to actually set it up and actually collect signals, it's gonna be kind of pointless. So for if you're using EKS uh you have this uh cloud watch observability add-on which automatically deploys the cloud watch agent which also open telemetry is, is which is also open telemetry compatible. And also it deploys the auto instrumentation agent or injects the containers into the pods that you want based on simple configuration. A similar thing is available for ECS as well. Uh, you just have to mount the auto instrumentation container into the application container in the task definition and you're good to go. And we have open telemetry-based lambda layer available for serverless workloads as well. Then the third one is, yes, we have all this data. We set it up really quickly, but we have a lot of data now. How do we. Wanna be in the business of creating dashboards. Uh, I don't wanna be in the business of doing manual correlation between signals between when I say between signals, yes, of course, logs, metrics and traces from applications, but also I, you don't wanna be manually correlating between infrastructure data, uh, from containers to application data like this microservice interactions and database calls and all of that. You want all of that to be out of the box and that that message was received as well and um. You wanted an opinion, highly opinionated sort of getting started, yeah, of course you want the ability to create your own dashboards, but you also wanted an opinionated way of doing things and that's uh what we have uh with uh uh the application signals or it's an APM solution within CloudWatch, right, so it has uh an application centric observability where it is you're not essentially focusing on like infrastructure or microservices, but you're looking at from application point of view which is. What your, uh, which is how everybody within the company, for example, looks at, uh, an infrastructure and, uh, with the AI driven root cause analysis we make it even easier for you to find issues from all the different signals that you're collecting from the different environments and, uh, you also want to kind of reduce the data that is being collected. You don't wanna, you don't wanna hack collect, you don't wanna get into the fear of missing out like FOMO mode and collect everything that I see a lot of customers get into and then. Uh, cost goes up. It takes a lot of time to you, uh, for you to actually find the root cause because there's a lot of noise and the signal to noise ratio is, is not, is not, uh, in the right balance, um, so. You wanted to be, uh, you wanna be more controlled, so the, uh, most of our mentioned about SLOs that we said the what we recommend is also, uh, uh, you know, for customers is also SLO based monitoring. The application signals in fact is, is centered around, uh, SLO. So how do you say SLOs? You, you start with the business SLA. So you start with the, you ask what the business wants in terms of what your. Uh, application should do, for example, this request should be responding within 200 milliseconds, and that's a request that you get. But then you build an SLO, which is a service level objective, uh, basically go and track whether the application is actually doing that or not, and you have an error budget, so there is wiggle room for you to play around with if there is, if there are any, uh, challenges there, and that will dictate to you what kind. Of metrics you actually need to collect when you do that you actually have a direct correlation between the business requirements and the technical things so you have, uh, observability at that point is not a technical concern anymore. It is also a business function, right? And with that we have seen customers really reduce, uh, you know, uh, in terms of like how much data they're collecting and also it really improves in meantime resolution. So let's take a look at how CloudWatch, uh, how you can use CloudWatch application signals for, um, using SLO-based monitoring to troubleshoot, right? So in this, uh, example, I am, I'm on the application signal screen on the services, if I click on that, you will see the service screen. So I have an, uh, I have a sample application deployed that has multiple microservices in there. The moment you go there, it shows a microservices view. It's all listed based. On the uh SLO status and then at the at the at the overview in in an overview I can actually find out like if there are any applications or services that are actually having issues and in this case there are uh some applications that are unhealthy based on uh SLOs, right? I can go click on the SLOs and uh look at the list of SLOs and um click on view all SLOs indust microservice and now, uh, once I. Click on that I'm able to see the uh SLO status whether the whether I'm meeting the SLO that I've set or not if I have exceeded the error budget and also if there's what kind of latency is is causing that right? and uh what I'm actually interested in this uh is one of this SLO's availability for uh availability uh SLO that is there and once I click and obviously the widget automatically changes accordingly. Um, what I wanna do is I wanna go and find out what this SLO even means. In this case, this SLO is actually measuring a specific operation called visits, right? And, uh, anything, let's go take a look at what this even looks like that's in that particular service. Let's go look at the SLO so we understand what we're talking about. So all this, uh, what I did was I actually set up application signals on this environment which has, uh, which is deployed on an EKS environment. This is a mul multiple microservices in there, and it automatically collects this, uh, uh, signal. Data like I mentioned before, all the industry standard metrics like request rate, errors, duration, etc. are automatically collected. In this case that is what I'm actually making use of, but it's not just confined to that. You can actually select any cloud watch metric. It, it can be a metric from containers. It could be easy to, it doesn't matter. You can actually select any metric and you can create an SLO based on whatever metric that is there. You can obviously publish your own metric as well from logs if you want to through um uh by extracting metrics out of log events. So here in this case, uh. It's just a description of kind of this the setting of like what this SLO even looks like is the availability and also um the SLO is uh measured every day. There's a goal and if, if the, if I 50%, if, if I'm a, if the other budget goes beyond 50%, then I wanna be alerted and so on. That's just the description of the SLO. So here what I can do is, yes, the SLO is, I mean right now it's, it's, it's not healthy. Now I can actually go and see what is going on in the operations. I actually can find out what is. Passing the issue. So when I click on that operation, it takes me to the screen where it, it shows all the actual metrics that are being, that are being collected, the request rate, errors, faults, etc. right? Uh, all of this that you're seeing here is out of the box. Like there's nothing that I had to create, right? And, uh, here's the, uh, runtime metrics as well, like for certain workloads like Java, .NET, etc. we collect runtime environment metrics too, so in case if there is a, you know, problem with garbage collector or something like that, that is also visible out of the box so you can go fix it if you want to. Um, further going up you can see that there is this false graph and there is a peak. Uh, I wanna go investigate that. I see 284 errors in the last, uh, 3 hours or so. Uh, if I go click on that, that's gonna show me all the different spans, uh, that were collected at that point. Uh, yes, I can look at the, look at that particular trace and go look at all the spans and investigate if I want to, but I also talked about you want the ability to sort of correlate the infrastructure information along with the application data, right? So in this case, I, because it is deployed in, uh, on an EKS environment, it is able to show the nodes as well. Obviously if it is other environment it will show that, uh, EC2 instances as well, but in this case I can also not only see what is going on in that particular infrastructure, but also I can go. Uh, into the pod level details and find out like what is happening there as well. So I can look into, go to container insights from here. This, uh, I don't have to, uh, really go hunt for this. I it's just a direct deep link. I go there and then look at container insights and look at all the, uh, container, uh, specific information. I can go dig deep and look at all the different deployments of that part and then look at all the, all that infrastructure information. There was a problem here I would obviously easily spot if there was, uh, a podcast crashing of the crash loop going on, right? Um, but what I really wanna do is I wanna go back and look at the trace itself, right? Uh, in this pants, if I go and expand and, uh, you know, investigate that trace that is gonna show me the specific, uh, trace map, how the request itself was processed in my environment. There is a client, there is multiple, a couple of services, and then there's a Dynamo DB that went in. Everything that you see on the right hand side is all open telemetry, uh, based, um, uh, data that got extracted and captured and extracted and shown here that's all enriched through Otel. And um if I go down I can actually see the uh the the span timeline that shows essentially how the request went through and what the different services that were impacted. And when I select the Dynamo DB itself uh that it it actually shows that there was an issue with Dynamo DB throttling, right? This is an error that was captured in the, in the trace itself as a as a trace event. Now when I go further down. I can see all the log events that were captured while this particular trace was captured. Obviously I can go read through it if I want to do the analysis myself, or I can go look at cloud Watch logs insights. I can go inside, uh, do my own analysis here automatically I see the log group and query and all that stuff. So I, when I click on run query, it gives me all the information now, um, what I can do is, um. Uh, what I can do is I can, um, um, I can do the analysis or I can actually do the pattern analysis myself. So if there are, if there are let's say 600 or 6000 log events, then it would be really useful for me to find out what kind of patterns are emerging in these log events instead of going through every single log event. So, if we go to patterns, it extracts the, it groups logs based on um uh based on the patterns here, uh, and you can see that it extracted tokens and it is showing me the token that is uh uh uh for the endpoint which is that visits uh uh API and um uh if I go further. Uh, this is a little bit slow, I think. So there it also shows my container as well. So, um, this, this makes it really easy for me to, you know, kind of troubleshoot the issue and, you know, get, you know, uh, find, find out the root cause very quickly. Um, I talked about the application centric piece. So what we looked at was from a microservice from, from the status of an SLO or from a microservice we went and started troubleshooting, but I talked about how you can look at it from an application point of view. That is where the application map feature comes in. So I. I'm on this application map screen, so all the different microservices that I showed you are part of this few applications. I have 5 applications here and what you can do is now you look at it, OK, there is 1 application that is having an issue, that's having a problem. I can go into that application. And uh once I go into that application it's supposed to go. I don't know why it isn't. OK, so I can see all the dependent microservices there and all these microservices uh uh are shown with with the dependency graph and so on but also it shows all the log events and even does a log audit to find out and show me if there are any obvious issues that are emerging and I can go do uh troubleshooting from this as well. I can go uh find out the application logs. From here I can troubleshoot from here too and uh you know that that should that should basically give me all the details, but how am I grouping the application? There are certain things that are inferred like based on the topology graph and all that that is an automatic understanding of what an application looks like, but that is also in your control so you can essentially go and create an application yourself like based on. Um, uh, based on AWS tags or open telemetry attributes to essentially group what an application should look like and, uh, uh, and basically, you know, uh, group based on like maybe environment or team name or anything like that so it is really easy for you to find out what application is breaking, right, um, so here, uh, this also allows you to filter based on SLI status, based on, uh, server faults, etc. so it's really makes it, you know, kind of declutters the interface for you to find out what the actual issue could be. Thank you. So, let's talk about the 3rd driver on our operational excellence wheel, flywheel, and I have 5 topics to cover, folks. So when we are reporting an incident, uh, we, we are very comfortable. DevOps model helps us a lot. So if you think about the DevOps is a, uh, a flywheel itself, it's a uh it's, we learn from our operational experience as developers and, and then we design and implement and deploy our services accordingly. Um, so, when, when I respond to an event, right, I don't want any surprises. So we are very disciplined in writing standard operating procedures and run books. The thing is, uh, this is slowly, slowly, this is increasingly slowing down because as Emaya showed you just now, Cloudwatch and other operational observability tools, they are getting so good that you start from a dashboard, you start from an error, and, and the, the product itself walks you through and, and helps you with root code. Right, but there are still scenarios where we have to write steps in detail, and ideally these, these SOPs, they, they go away because we are also able to automate, right? So the, the drive for SOPs, it might be a hot take, but I think it should be down to zero at some point. If you have to write an SOP, there are some things we pay attention to. For example, we are not vague when we write a step. We don't say go find this front-end service logs and, and look for errors, right? What, what would an operator do in the middle of the night. So we want to provide deep things. We want them to be able to immediately go reach whatever we want them to go reach and, and, and figure it out from there. First question in an SOP should be, can you roll back? Even if it wasn't deployment related, the operators should check if there was a deployment and, and not sit there and try to fix a bad deployment. And, and if you have SOPs, or if you create one, you could be thinking in the age of Asiantic AI, why, why would I do this? Well, you can feed them and create a knowledge base out of those, right? And, and like I said, ideally your SOPs leads to automated runbooks. We have escalation rules. Our ticketing system will escalate. If I don't respond to a page, the secondary will be paged, and if they don't respond, then the manager on call will be paged. That's the system to make sure that we respond to our tickets on time, but I want to speak about the culture. When I first went on call years, decades ago, my, my manager told me it might sound to you not intuitive, but please page me if you see a customer impact. Please page the manager on call. Don't be scared. We are going to come and help you. So when there is an outage in AWS, usually many leaders are on a call somewhere. When it is very large, most of us are on a call, we are trying to help the teams, right? So we have this culture of, hey, escalate, let me know. I'm going to come and help you. We have an incident response team. This team will be paged if nobody is responding. They have a dashboard that they, they know everybody's KPIs. Every service is KPIs, and, and if there is no engagement, they will create a ticket and they will engage you. They also help us during large scale events. They, they create aggregated tickets. They engage, they help us find the right team because they have the phone book of everyone. And they made their services external, so you can buy their services if you want to implement something like this in your own companies. Of course with AI in the last 2 or 3 years we have been incorporating AI to our operational operations and the way we approach AI is AI is helping us. We are still in the driver's seat. We are as humans solving the problems, but increasingly we are. Getting help from AI to drive our operations forward and we already externalized some of the things we do. We launched Cloudwatch investigations and we launched CloudWatch servers, and I will walk us through a demo for those features. All right, so, um, so it's really important for us to, uh, for you to empower your developers, right? So I had some issues with the animation earlier, so I hope it doesn't happen again. I'll try, uh, so, um, what we have is we have a couple of MCP servers, uh, CloudWatch MCP server and CloudWatch Application Signals MCP server, and because it's really, um, it's really, uh, a powerful thing for a developer to have all these tools so they can get insights into what is going on within that application without them. Having to go switch into the uh you know, applications, go into the browser, log into the AWS console, look at different things. But what if we, we gave them all the power that we, that we can into their IDs itself? So here I'm on uh Quiro and obviously you can use uh any uh. OK, I'm gonna try and see if the animation works. Um, any ID that you have? So here, uh, what I'm doing is I'm actually asking Quiro to kind of list all the MC all the SLOs, uh, in my AWS account, and here it is listing all the SLOs in my AWS account. It's the same thing that I did earlier. What, uh, the, the, the troubleshooting mechanism that I did in the workflow earlier manually that I'm gonna show exactly the same thing using Quiro, using MCP servers. Here it's listed. All the SLOs now I'm gonna ask you to list all the um um uh SLOs that have issues and it it really listed that there are issues there uh and uh I'm asking it to give me details about the specific SLO that is breaching which is availability for scheduling visits. It's giving me an understanding of, OK, these are the likely causes for this thing and uh that could be the problem and uh let me go next and um I'm gonna ask you to hopefully. Uh, that would work, but apparently he doesn't like it. OK, but anyway, so the thing is it, um, actually went and asked, uh, uh, asked me the permission to, uh, make a call to the audit services, uh, API call through the MCP server, and then it came up with an understanding of what exactly happened. So in this case it also came back and told me that there was a Dynamo DB, uh, throttling issue and it is even making me suggestions on like. What could, what I could potentially do to solve that problem, uh, with this without me having to log into the AWS console or going through that process, um, you can obviously, uh, you know, this, uh, you can use your own model. It really doesn't matter because end of the day you're making APA calls, uh, through MCP server, uh, and, uh, and so on. So next, what if you actually had access to AWS console? Like how could you do that? That's where the cloudwatch investigations comes into play. So in this. Um, what I'm going to potentially be demonstrate is that, uh, the ability to sort of go and investigate, right? So there is, there is a way that you can actually go and investigate an SLO if it is an NLD. So if you click on that, you can click on investigate. That's not the only way for you to start an investigation. You can start an investigation when you're looking at a metric that is that is not looking good or maybe you're you're creating some logs that are log events you can ask uh cloud watchch investigate to investigate if there is a problem or you can even automate the whole thing, uh, make a cloud watch alarm and as an action you can ask it to start an investigation as soon as the alarm goes off, right? So by the time the alarm goes off and then you go there and find out it probably potentially has already found the problem, um, so here I'm gonna click on investigate. Uh, if I'm able to, I gave it a name. You didn't get to see it. I apologize. And then when you click on start investigating, uh, start investigation, it basically goes and, uh, starts, uh, a series of tasks, right? So in this case it is, uh, looking at cloud trail logs, uh, you know, cloud watch logs, metrics, and you can even, uh, create an access entry, give it access to the EKS cluster, so. Can go into the resources and find out what's going on. It'll, uh, create a topology map based on the, uh, on the trace data that it finds, and you don't have to stay in this uh console at all. You can actually leave and come back later because it takes a few minutes. I've actually sped up this process. It takes a few minutes to actually complete this, um, and you can just go and what you can do is you can also feed more information to it so you can, let's say you start an investigation. And you can go run a query and then hey, I have this data also like include this data or maybe you can also add another metric, uh, some metric information to it so you are actually aiding the process of, um, uh, of troubleshooting as well. So once you do this, it will essentially go and, uh, do the, do all the analysis in any second it, it's gonna come up and tell me what the root cause is and it basically comes back with a hypothesis, right? And it comes back and tells me what the potential problem could be here. It says the analysis is complete and the investigation, investigation has has concluded, and here, uh, it came up with this hypothesis and, uh, and what I want you to do is I want you to pay attention to this hypothesis here. Um, mainly to the, um, the what happened, uh, uh, section here in the, in the third one. So obviously the root cause that it found is that, OK, there was no, uh, the cloud V2 model was not available and that was the problem. It found all that, but interestingly it even came up saying that the application signals fault metrics really did not find this problem, so I had to go dig deep and find this problem for you. So and it even gave me an analysis of like, OK, so you. At the bottom all the way at the bottom you can see that it says that you, you know, that is this represent that is a monitoring gap that you really wanna, uh, focus on. So I know that there is some work to do that I have to go, uh, perform, right? So this makes it really, uh, uh, uh, you know, when I, when I'm without this, I would probably have to do all those steps that I did earlier like going into the SLOs and it, it's not that it is hard. Yes, it is harder than this because this is doing the work for me instead of me going and finding out what the root cause is. And you can connect the CloudWatch investigations to a ticketing system. We connect it to ours, so every time an operator in AWS receives a ticket, by the time they engage, they already have Cloud Watch investigations running and providing updates on the ticket, and we collect feedback from these operators. The most recent number I read, we were about 90% satisfaction rate, so the teams in AWS are liking Cloudwatch investigations, and as I said, there are many services now in AWS. All their stacks are native AWS and they use CloudWatch end to end. Um, let's talk about the last 4th 1 in our driver, the last driver reviews. So we review our incidents, we review our operations, and I'm going to cover 3 topics here. First, the weekly dashboard reviews. We get together as small teams on Monday. And we go through our dashboards and we look for anomalies. We look for spikes, and this is an excuse to do a retrospective, to be honest. You start with dashboards, but then you have an honest conversation with your team members about your operational posture and whether you should take any action items for that week, right? And when we are. This dashboards, our widgets usually have two lines alarm threshold and investigation threshold. So let's say we are looking at a latent symmetric over time for an API service, and if we were to see a widget like this as a team, we would be thinking, you know what, why is our investigation line that high? Can we pull it down so that we can increase the operational bar? If we see a spike like this, of course we would investigate, but sometimes there are things like this you see where they are not breaching anything, but they are suspicious, so we also investigate those. And of course we use machine learning and AI to help us to come up with a report. So by the time we start that meeting, we already have a report telling us where to pay the most attention to. Another learning I have is don't just look at your last week if you are doing this weekly, because you would miss longer trends, right? So, so use the seasonal tendencies of those machine learning tools because you would miss things that are actually going bad, but that week it might look just fine. And we review these dashboards on Tuesdays and also on Wednesdays. Tuesdays as larger groups and on Wednesday as a whole company. So we have an operational excellence meeting in AWS where all the senior leaders join and, and we, we go through certain things and one part of it is weekly dashboard reviews, so we pick a team random. And it was a physical wheel when we first started this years ago, but of course it stopped scaling. We made it a software wheel, and it's on GitHub. You can download this and, and create your own weekly dashboard review process. And, and the idea is not to stress anyone. It's stressful when you come up there and present your dashboard. I've been there multiple times. You have your cold sweats, of course, but the idea is to learn from each other, learn and, and uh as leaders, when we ask questions, everybody who's dialing in, they're also hearing, so they take those actions as well, right? So it's a way to scale our operational excellence. Similarly, we review our tickets every week. So in the same meeting when we are reviewing our dashboards, we also have a discussion about the high severity tickets we received. And the point is we are trying to find the recurring problems, things that keep coming back and back. Remember the Bezos quote. He says when we find a recurring problem, right, if we don't look how we are going to find a recurring problem, and the idea is we don't want to be in this reactionary mode where we keep resolving tickets and move on. We are trying to identify the root causes, and Gen AI, as you can imagine, is great at summarizing the tickets. So we are increasingly using Gen AI to start from something, not from nothing. And the last thing I wanna talk about is correction of error. This, uh, we discussed correction of error extensively. There are lots of articles we wrote about it, but for the, for the completeness of our discussion, here we go. When we have a customer impacting event, when we have an event that is large scale, when, when we think we can learn from, when we can learn from an event and, and share this with the broader company, we write a very detailed report and it's called Correction of Error, COE for short. And When we write this, we are very careful that we don't make a blame tool. The person, the team who is writing the COE should never feel like they are being blamed. They are being punished, and, and that's very important as, as leaders of your companies, of your organizations to, to instill that culture that this is a learning tool because we don't. Assume bad intentions, we assume good intentions, and we look for the mechanism that failed, right? So, a, a correct output of a correct COE should be what mechanism failed and what can we put in place to fix that. Here are the sections of a correction error. It's a template. It's a tool because it's a mechanism, so you can create a correctional error. It will give you a template to fill, and then it will be bar raised and reviewed, including in the Wednesday operational meeting. If is broad enough or if it had great learnings, we pick them and review them. So we go write a summary, we put the relevant metrics and graphs, we write the customer impact. We also share what went well. Maybe the team did something great and we want to share that, right? And incident response analysis is about how we reacted to the incident, and one favorite question of mine is this one. How long did it take you to detect the issue? And how would you have cut that time in half? So if you keep asking this question and if you keep taking action items over time, you will naturally improve. Post-incident analysis covers things like uh the diagno issue diagnosis and the same question. OK, you've detected, but did you spend an hour trying to figure out what went wrong? And, and we are again asking team, hey, do the thinking exercise, how would you have cut that time in half? Or did you have a test for this scenario? Similarly, how we ask for tests in the pipeline rules or, or the operational readiness review. We write a very detailed timeline just so that we can see the the the gaps in the timeline and, and the things we can improve, and this 5Y section 5 is is not a hard rule, but it's a rule of thumb. The first why is the symptom, why so and so service impacted blah, and the last why, answer to the last why should be the root cause. So this is appeal, the onion exercise for the team. And of course, it has lessons learned and action items at the end. And every COE if you were working in AWS and Amazon, every COE you read would have this structure. And uh we, we, we use this COEs to refer to uh uh uh to, to each other and, and, and um we, we, uh, so, so the thing I wanna point out is, as you can see, it's a tedious process. It takes time, right? So with AI we we build some systems to again start from something, not from nothing. For example, timeline. Can can timeline be crafted for us? So, so can we use prior COEs, to use that knowledge base to give teams some ideas on action items and such? And we already externalized this. We call this Cloud Watch incident report, and we have a quick demo for that. So I'm gonna continue from where uh we were. So we're looking at that investigation. So one of the investigations that I already completed is uh right there at the bottom. It's called appointment scheduling and troubleshooting when I click on that if I'm able to. OK, so, uh, what I did was, uh, there's a button called incident report which I was supposed to be shown that uh the animation did not show that but once you click on that. Automatically creates this report. So what it does is on the right hand side, everything that you see, these are all the facts that it collected when the investigation actually happened that you can actually go and edit if you want to say there, it, you know, if you want to update or it actually impacted more customers or less customers, you can go impact that you can, uh, address that if you want. However, um, what I wanna focus on is what is on the, on the left hand side on the report itself that it creates. You can see that it creates, um, uh, you know, puts the title out. There's a background there's, uh, of the incident. And creates a nice summary and also adds the visualization uh with all the graphs and it talks about like customer impact exactly like Mustafa mentioned what went well, the and the the entire incident report analysis and so on. So what I'm really interested in showing is the five eyes like Mustafa talked about and it, it essentially goes to um. I hope, yeah, so it basically asks all the rights documents all the five ways and finds out like what the root cause itself. So every everything is entirely fully documented and then obviously it doesn't have to stay here. Uh, it's, it only is more useful if you actually share it with your team. So you can go export and, you know, copy to a clipboard, like share it to put it in your document repository if you want or download it and share it with other team members, right? Yeah all right. I mean, it wouldn't be a talk if there wasn't any technical difficulty, right? But we are not aiming for perfection. Um, we have come full circle, folks. Uh, we covered our flywheel, and, as I said earlier, the more we get better at operational excellence, the better reviewers we become. And of course, I forgot to mention something. So the last thing we mentioned was COE in the review section, and the first thing we covered was ORR. And this is an example where COE is actually feeding into OR. Every COE we we write. eventually ends up as a question, I mean, not entirely, but every interesting COE every COE that is broadly applicable becomes an oral question and, and it feeds into our our process. Um, uh, so. And again, the operational excellence makes us better reviewers. Um, you might be thinking like these processes, these mechanisms are great, but, uh, Amazon is a big company, they can afford these things. Maybe you're just a startup. Remember, Amazon was also a startup. Right? So, the ORR when it first started, it was just a Word document with a bunch of questions in it. Now it became a tool, it became, it they have, we have 2000 templates in there. Anybody can create their own OR template. So you have to start from somewhere. So I'm gonna suggest some steps. If you were inspired by anything you heard here, any mechanism, I suggest like start thinking like how can I implement a version of this in my own company. It will be your own version. It won't be the same. But, but you can perhaps start looking into that, and I would love to help you folks, and you, you don't have to use CloudWatch, you could be using anything. I just have operational excellence improvements. If you want to invest in them, I would love to help you, talk to you and give you some ideas and review your plan, for example, Animayah, the same. He helps our customers to raise their operational excellence every day. And there are lots of stuff online that we already referred and there is even more. You can read this uh uh observable best practices guide Imaya and team put together. Again, it's in the same company and website. We also have an observtory workshop. All those demos Imaya was showing, you can deploy them yourselves and you can play with them. And, and learn more. And I wanna finish on a, uh, a lighter tone. So when we first put this deck together, this slice together, um, with Maya, I was in Bay Area visiting uh our San Francisco office. I'm from Seattle and uh I was gonna do a dry run to a small group of people. And I wanted to grab a coffee, uh, in the morning before I go to the, the SFO office, and I found a coffee shop right across the street. If you can believe it, it was called Flywheel Cafe. And, and, uh, and that's the store, and inside there is a flywheel up there and if you can believe it, there was a first responder ordering right before me, uh, from San Francisco Fire Department. So I thought this was, I was destined to give this talk. Thank you for your time. Thanks for being here. I hope you found it useful and if you want to connect with me or Imaya, we are on social media. And if you have any questions, we will be right outside in the next 5 to 10 minutes. We would love to connect with you in person, connect with you in person too. Thank you. Thank you, folks.