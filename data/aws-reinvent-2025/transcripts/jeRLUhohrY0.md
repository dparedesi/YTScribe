---
video_id: jeRLUhohrY0
video_url: https://www.youtube.com/watch?v=jeRLUhohrY0
is_generated: False
is_translatable: True
summary: "Michael Webster, a principal engineer at CircleCI, discusses the impact of AI agents on the software development lifecycle (SDLC) and how to address the challenges they create. He begins by tracing the evolution of agentic workflows from simple copy-pasting to IDE-integrated agents and now to headless agents that can be triggered by cron jobs or webhooks. This trend is real and growing, with data from the public GitHub archive showing a rapid increase in bot activity, particularly in pushing code, not just commenting on PRs. CircleCI's internal data confirms this trend, indicating that agents are performing economically valuable work. However, this acceleration in code generation is creating a bottleneck in the SDLC. PRs are becoming massive, reviews are taking longer, and build stability isn't improving. This is a classic queuing theory problem: work is arriving faster than it can be processed, leading to delays. Even with AI, many organizations can't deliver software faster because their delivery processes can't keep up. This is reflected in the DORA metrics, which show that while some high-performing teams are seeing benefits, many are experiencing increased instability and minimal product effectiveness from AI. The return on investment for AI initiatives is not yet meeting expectations. To fix this, Webster argues that organizations need to ""go faster"" in their delivery processes. This involves not just writing code faster, but also validating and shipping it faster. He emphasizes that this is not about ""whiz-bang multi-agent orchestration,"" but about solid engineering fundamentals like reliable acceptance tests and robust delivery pipelines. AI can help with this, for example, by converting slow scripts to faster-compiled languages. The key to going faster, according to Webster, is **validation**. He proposes a simple loop: plan, do, judge, and iterate. This validation-centric approach is scalable, durable, and tractable for most organizations. It's more effective to invest in validating the output of agents than to chase the latest prompting techniques. The validation pipeline itself becomes a source of context for the agent, creating a powerful feedback loop. By starting with a clear check, organizations can improve how they task agents and extract reusable tools. Webster introduces **Chunk**, CircleCI's new agent, which is built on these principles. Chunk is ""validation-first,"" meaning it always executes CI pipelines to verify its work. It focuses on improving delivery by tackling issues like flaky tests, improving code coverage, and automating CI pipeline maintenance. By analyzing the results of builds, Chunk can automatically tune itself to be more effective, creating a virtuous cycle of improvement. In conclusion, Webster reiterates that AI agent adoption is a real and growing trend that is causing some negative impacts on the SDLC. To fully realize the benefits of AI, organizations must invest in their delivery processes, with a strong focus on validation. This approach will not only make agents faster and more reliable but also improve the overall software delivery process."
keywords: AI agents, software development lifecycle (SDLC), continuous integration (CI), validation, CircleCI, testing
---

Uh, yeah, hi everyone, thank you. Uh, my name's Michael Webster. I'm a principal engineer at Circle CI, um, and we're gonna talk a little bit today about, um, kind of where we are right now with agents and what they're doing to the SDLC, some of the problems that we're seeing already being created, uh, and, uh, some techniques we're finding effective to fix it, um, including some things we're adding into our product. Um, so yeah, let's go and get started. Um, as a quick recap, I wanna go over sort of like kind of the history of agentic workflows. Um, I won't go too in depth, but I think kind of where we are now is starting to see a little bit of a decoupling point that we didn't have, right? So back in the days of 2021, 2022, there was a lot of developer copy pasting between terminal IDE, a chat GBT window, what have you. Um, then we start seeing IDEs come along that are still driven by the developer but are capable of more, um, agentic tasks. They can do more long range planning and execution. This was really interesting, unlocked a lot of potential, uh, within, within the AI space. But now what we're seeing is these sort of headless agents starting to emerge, right? You take a CLI, you throw it inside of a dock or container, and now suddenly you can start running cron jobs, web hook triggered, agent runs, um, anything you really like in order to, to impact this. Now, there's a lot of products on the market they've launched in the last 6 months to do this. Um, and so I thought it was interesting to maybe take a look and see how real is this trend, right? So, I went through and I looked at all of the, um, bot activity in the public GitHub archive, um, going back to, uh, the end of last year. Um, and just sort of looked and said like for, for these known coding agents, uh, a small subset of them, 5 or 6, what, what are they actually doing on GitHub? And you can see a, a normal sort of like really rapid growth trend overall within these agents, um, across, you know, all the big common ones, co-pilot, cloud, Codex, um, all of those. But it gets more interesting. So, so there's obviously some growth, there's activity from these agents. But it gets more interesting when you break it down into the event types, like what are these agents actually doing, um, and you start to see a really interesting trend, um, from the first like 3 months on and then forward. Um, when these things first launched, they were just doing PR comments. This was just like code review bots. Um, every example starter workflow of how to use an agent in an action or, or in a circle CI config, um, it was doing things like issue triage and code review, and that's what they did. Um, but you can see around May of, of this year, you started to see these agents actually starting pushing code, right? This was something that people sort of expected, but it took a little while to happen. Um, and at this point, uh, they're almost doing as much, this data is as recent as October. Um, in some weeks you're seeing just as much push activity as you are comment and PR activity, right? So these agents are actually doing real work. They're pushing actual code to real repositories, uh, which is what you would expect again with, with the growth trends. Um, so that's public GitHub activity, but that could simply be experiments, people with hobbyist projects, um, nothing necessarily like, uh, in a, in a real-world setting. So, uh, at Circle CI we, we see a lot of build activity, um, from multiple projects that aren't on the public GitHub archive. Um, these are, these are, you know, enterprises, uh, large and small startups. Um, so we decided to look and see what is our activity pattern see. Do we see a similar growth trend as what we see in the public, uh, public archive? And, uh, this is a screenshot, kind of some aggregated and anonymized data, and you see again, we see this, we see this trend. Uh, it's, we see the same pattern repeat itself. So, there's activity happening, and in particular with Circle CI, this is, these are cases where we actually ran a pipeline. Um, this is not a case where someone simply updated to read me or maybe was building a static blog. This is someone took the time to configure a, a pipeline to run unit tests and do deploys in response to a push event. All right? So this is like, so you would think this is economically valuable work being done by these agents. It's not just, I had a hobby project, let me go turn on and see if Claude can keep my GitHub, you know, activity green or something like that. Um, and again, this is all looking at agents that we can distinctly identify. So this is our, this is our low-end estimate of what's happening. If you use cloud code or codex or Quiro and commit under your own name, uh, we're not gonna necessarily be able to tell that it's you. So this is sort of like a very low. Bar estimate and we've seen within really about 5 or 6 months, um, we've seen a very fast growth clip and this is reflecting in the revenue of these tools um and, and just the the general trends overall where people are more willing to try out um these headless agent votes. OK, so why is this a problem? Like, this is the thing that everybody wanted. Uh, we, we wanted these agents to do more than just like get us out of the IDE instead of multi-boxing cursor, you can now just have multiple agents running in parallel. Um, and the, the issue here really comes down to, um, the code isn't really valuable until it's in a customer's hands, and all of the stuff that happens after you write the code, um, is not necessarily keeping up, right? Uh, PRs are getting massive. Uh, open source projects are requiring AI disclosures because they suddenly get mysterious 2000 line PRs that all look alike. Um, reviews are taking longer. Humans, humans looking at those 2000 line PRs. Um, and in general, the, the build stability isn't really improving, um, and this is kind of what you would expect. Um, this is a basic kind of queuing theory. It's a branch of math about how queues operate, um, to simplify it a lot, really, if, if work is arriving into your system faster than you're able to process it, you get delays. Uh, you probably. Understand this intuitively. If you've ever been at a store that only had a single checkout line when they were busy, everybody has to wait. Eventually, some people might get tired and give up, uh, but when we have, you know, a revenue feature on the line, we can't really just stop. So the queue just builds up and builds up, and then people stop working to go drain the queue and do all the, the reviews. Um, so the reality is for a lot of organizations, even though you can write a lot of code now, you probably couldn't actually go much faster if you wanted to in terms of your delivery processes. Um, to give an example of what this looks like, um, I put together kind of a queuing simulation of. Under various scenarios of what happens with different speedups uh due to AI, right? So, on the bottom, we kind of have a status quo baseline. It, it assumes that you can process code twice as fast as you can write it, um, but if you hold some of these things constant, As the AI gets faster and faster, the delays get larger and they, they come up even, even even more quickly. Um, and this is a particularly a problem because with humans, the workday ends at some point, right? You don't have an infinite backlog because people don't work at, at an AI pace all the time, but the agents can. If you want to hook them up to your Jira backlog, your Linear queue, whatever whatever it is. The reality is that for most organizations, you're not actually gonna be able to get the benefit because you're gonna be spending all your time reviewing the PRs um and waiting for the deploys to go through. Um, this theory aligns with subjective feedback. You look at the door metrics, um, there's actually some, many teams are reporting an increase in instability from AI. There's minimal effects on, on the product effectiveness, um, and, and, and, in general, burnout. Uh, if you look at other, uh, indust. Benchmarks. When you dig in further, you see that a lot of the AI gains, they center around 10% improvement, um, but that's widely distributed. It's basically a bimodal distribution. People who are really good at delivering software are getting most of the benefit. People that are average to mediocre, they're actually seeing no benefit to negative effects from AI. OK. And this is a problem, obviously, because like, you know, as a technologist, we want these tools to work, but also, we're not putting 10% level improvements from your senior engineer investments into AI. We're spending hundreds of thousands to millions of dollars building data centers, all kinds of inferences. Um, we, we need more than just incremental improvements. So, how can we fix this? Well, the simple pithy answer is to go faster, right? Um, you know, in that simulation I showed before, we said like, OK, the code comes in faster, but the delivery goes the same. Well, what if we just made the delivery faster? Um, and you can see again, expected effects here. Uh, if you're able to ship the code faster. Or the curve kind of bends down, um, in some cases, even if you don't get any improvement from AI, if, if your agents are still producing at the same output as your developer, but you've made your delivery faster, you can actually cut your delays, um, so this has benefit even if you're, if you're using AI or not. OK, so we have a a hypothesis of how we can improve this. We have some levers that we think we can pull. Um, so what does it look like? How do we actually do that? Just go faster is not a particularly actionable, uh, task. Um, to start with, like, I love AI, I love agents, but I wanna be clear, a lot of this isn't like whiz bang multi-agent orchestration. Like, it's block and tackling engineering, like stuff we've been doing for 1525 years. Um, You know, having things like acceptance tests that will reliably tell you if things broke, um, being, uh, being confident that when you, uh, get an automated PR from a bot, whether it's a, a, a CDE bot or an AI bot, that it didn't cause a regression, right, where you're comfortable trusting the fact that if your tests say you're green, you're green, and you can go ahead and merge. Um, you have to make investments in your delivery pipelines, and, uh, so I, I don't wanna, I don't want to downplay all of this work. This is all super necessary and it's very tractable. Um, and it's definitely a thing that everyone should invest in. AI is really great at helping out with this too, right? If you need to convert scripts, if you've got a bunch of batch scripts that are slow and doing subprocessing, you can rewrite it in a language that's compiled and, and, and the AI will, will help you with that. So it's definitely a spot where AI benefits you, um, but you do have to do this work. OK, but let's just assume for the sake of argument that's still not fast enough, that only gets you, you know, a slight reduction, um, in your queuing delay. Um, well, how can you go even faster? Um, and the way we think about this at Circle CI is that it really comes down to validation. Um, I'm riffing on a really old saying about amateur stalking strategy and professional stalking logistics, but the idea here is like, instead of obsessing about what's the latest and greatest way to prompt an agent, what's the latest and greatest memory framework, um, what have you, think about how you can validate the code. Right? Um, I'll talk about, I'll try to make the case of what I mean by validation here. This is a really simple loop. Um, you make a plan, you do some work, and you have something that judges the quality of that work, and then if it passes, you go, go through. If it doesn't pass, you go back to the beginning and you start over. Um, this is the foundation of, if you want to do red-green refactor, classic CI, um, any type of like the local loops that agents go through. This is the pattern that's really effective at, at, uh, getting work done. You do something, judge the results, give feedback, and continue forward. So this is the basic recipe when we talk about validation. Um, to make a further argument for why I think validation is, is so important, um, it has a lot of nice properties here. Um, it's really scalable, right? So, the, the, the work you put into making sure that your code, that you can validate code written by agents, it works for individual tasks, right? You probably use a version of this um in your local loops where Claude will run tests, run linters formatting. It works if you want to tune the prompts, right? Having a set of, we know this was a good change, we know this was a bad change, um, if you even want to go full on like training your own RRL agent, uh, the same, that basic recipe I showed of. Task, output, evaluate, and loop is really the fundamental recipe. So you, it works all the way up from your developer on your machine all the way up until you get to the point where you're wanting to train your own agent. Um, it's also durable, right? If you've been in the AI space at all, you're familiar with all of the churn. It's chain of thought, then it's graph of thought, then it's, now it's program of thoughts. Like there's like there's so many techniques that people learn, and I'm glad that people are researching them, but for in developers, what ends up happening is they get trained into the underlying models. So trying to chase sort of like techniques and tactics, um, you need to do that to some extent, right? You want to use the tools that are available to you, but making that the, the centerpiece of your strategy, you can probably, uh, steer your investment in more productive ways. Finally, um, agents are more tractable for most organ or a validation is tractable for most organizations. Uh, it's possible, but not straightforward, um, to run and train your own models. But making your test faster, understanding why your deploys failed, understanding what good, uh, code review feedback looks like so that you can give it to the developers earlier, uh, this is all information that's readily accessible. It's, it's happening inside of your organizations. You're probably using a developer experience tool that is mining this, um, to tell you, uh, feed subjective feedback on your developers. So, when you look across all of the things that you could do, uh, when it comes to agents. Investing in how you validate their output, um, to me feels like the most uh effective high ROI option that we have. OK. Um, the other thing that you, that I would recommend investing in is context, right? So, uh, whenever, whenever we are dealing with agents, uh, one of the things that you have to manage is what does it know, what is it having to remember, and what is it having to parse through, um, in order to complete the task. Um, the nice thing about this, about context is your, your validation pipeline becomes an input for context for the Asian. Taking a, a, a, a log of having cloud code work on a backlog of tasks. Having another AI summarize the results of that task and then you feeding that back to improve the prompts, the tools, the sub-agents that you might be using is a really powerful technique. So yeah, at the end of the day, you might be changing a prompt, but you're driving this off of real feedback that's tuned to your organization and your codebases. Um, very quickly, when we talk about context, there's, there's a few levers that I think are really important that you can deal with. Um, you really kind of can control the tools, the task, or the prompting of the agent, as well as the check. Um, uh, sort of, you know, this is a very simple version of what an agent does where you give it a task, it keeps looping until it passes some standard. Um, I think this check, I, I would recommend a bottom-up approach here. Start with a check. Then use that as a way to improve how you give tasks to the agent and then extract those things into tools that the agent can use, right? So you use that validation as a way to, to ripple things back up until you end up with a library of things that you have really good confidence that the agent can use and execute on effectively. Um, so I'll, I'll give like kind of like a verbal version of this basic recipe that we found really effective on that circle CI when, when we're working in our development teams, which is we, we use the feedback from CI pipelines or local testing runs, um, to get the model to produce the results we want. Once, once we have a baseline, once we have a, we. Know that this is good, then we can start extracting that process into rules to get the model to do that all the time. So instead of everyone starting from scratch, you, one person goes through the process of how do I get a playwright-based flaky test fixing process working. And then we're able to use that across all of our projects um to perform a specific task. And then just like any library development kind of process that you might have done before, once you've got a few one-off encapsulated functions, um, you can then turn those into rules that are more abstract. So it's taking the, the same techniques that we approach with software, um, and just applying them to the prompts all driven off of a clear indicator of if what the agent did was good or bad. All right, uh, and so, yeah, I'll talk about Chunk now. So Chunk, this is our Circle CI, the agent that we're working on with Circle CI, um, that's built from the ground up with most of these principles in mind. Um, the idea behind Chunk is, uh, to, to help make sure that your software is always validated, um, and to make the process of validating that software faster, more reliable, more effective. Um, Chunk, we built this thing very much validation first. Anytime this agent touches your code base, it's going to execute your CI pipelines to verify that it works. This means you're getting feedback from real environments that you've already built. You already know that this, this environment and these commands are good enough to say that this code can go to production. Um, and so we're just gonna reuse that. Uh, there's, there's no real need to completely reinvent the wheel here, um, and have to think too much about hooks or some other side process of development for. You've already configured a way to test and see if these things are good. We also keep your software production ready. Um, the first thing we're doing is going after flaky tests. Uh, when it comes to delivery, flaky tests are kind of the bane of at least my existence. Um, when you have a change that you know is good, but you have a test that is unreliable and can't, can't always tell you if something is good or bad. Um, that slows everything down. It slows down humans, it slows down agents. Um, it really grinds, uh, it's, it's sand in the in the, in the gears to any kind of smooth delivery process. Um, we're also working on, you know, things like improving code coverage, which again is very useful context for agents to understand, um, how. Files relate to each other, uh, and, and how, how they're tested effectively and just generally handling a lot of the toil. Um, nobody really likes maintaining CI pipelines. I work at a CI company. I don't like writing YAL, um, but agents are really good at writing YAML, um, so things like build optimization, keeping your build fast, these are all really, um. These are all first class tasks that we'll, we'll be adding into Chunk. The other thing that Chunk does here because we have context on all of the changes that occurred and whether they were good or bad, right? We can follow a change all the way through from the commit that came into GitHub to was this change deployed, merged, deployed, or eventually rolled back. We're able to build a really good understanding of um Of how you can make an agent work more effectively. So instead of you having to, uh, build your own loops, your own feedback cycles, your own RL environments, we can take the results of your builds, and we can use them as feedback to further tune the agents in a more automatic fashion. All right. So, uh, quick recap and takeaways here. Um, the AI agent adoption trend is, is quite real and, and growing. Um, this is not, we're, we're kind of out of the realm of reviewing PRs or triaging issues. These are writing real code. Um, what we're seeing today is probably an underestimate of reality, um, but it is a, it is a real and growing trend. Um, we're seeing that this is causing some negative impact and definitely is not uniformly beneficial. Um, not all organizations are getting the same, uh, results from from AI initiatives that they would expect. Um, also, hopefully I've convinced you investing in the delivery process is a really good start, um, and a way to get going, and that the validation that you do in delivery is the foundations of keeping your agents fast and reliable, um. Yeah, and so that's all I have for you today. If you wanna talk to me more about Chunk or see a demo, um, I'll be hanging out in the back, and then we're in booth 1451 at Circle CI. Great. Thanks y'all.