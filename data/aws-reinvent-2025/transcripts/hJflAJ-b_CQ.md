---
video_id: hJflAJ-b_CQ
video_url: https://www.youtube.com/watch?v=hJflAJ-b_CQ
is_generated: False
is_translatable: True
summary: "This hands-on session, \"Build a GenAI Race Tracker w/ Confluent & AWS Bedrock in less than 60 Min (ANT327),\" features a Confluent speaker who guides the audience through building a real-time Generative AI application using data streaming. The use case is an F1 race tracker where race data is streamed via Confluent Cloud, processed with Flink SQL, and sent to Amazon Bedrock (Claude 3.5 Sonnet) to generate live race commentary. The session introduces \"Confluent Intelligence,\" a suite of tools including Streaming Agents (for model inference and tool calling), a Real-Time Context Engine (early access), and built-in ML functions for tasks like anomaly detection. The speaker demonstrates how to set up the entire pipeline—from Confluent Cloud API keys and Kafka clusters to Flink compute pools—using a setup script. They then walk through creating Flink SQL statements to enrich data, call LLMs for commentary, detect anomalies (e.g., engine issues), and even perform vector embeddings and retrieval with MongoDB for querying past race data. The session emphasizes how this event-driven architecture simplifies \"spaghetti code\" middleware into a few SQL queries and provides best practices for managing LLM costs and quotas."
keywords: Confluent Cloud, Amazon Bedrock, Flink SQL, Data Streaming, GenAI, Real-Time Analytics, Kafka, Event-Driven Architecture, Anomaly Detection, Vector Search
---

Oh, OK, wonderful, we're gonna go ahead and get started, everyone. Alright, well thank you for being here. I'm really excited to share with you the content that we've prepared for you. So we're gonna be talking about building a Gen AI race tracker using Confluent Cloud and Amazon Bedrock. Now due to the length of the title already we couldn't include the fact that we're also going to be using OpenAI and Mongo DB. So when we get to that part, I'm, I'm excited to show how all four of those play together. But for now, let's go ahead and jump into this agenda here. So this is our roadmap for today, uh, for the next hour at least, uh, we're gonna be talking about what the current approaches are to Gen AI and, uh, along the way, as we know, things have really become more about agentic AI. So we're gonna talk about agents, what it takes to be able to make those successful and, and get those into production. Uh, we're also gonna be talking about how data streaming plays a part in the whole narrative and story of generative AI and agents, and then we're gonna introduce you to confluent intelligence. So with that, and then the next part following after that, and I think this is what kind of makes this session special, we're gonna have, uh, it's not an official hands on workshop, but you're gonna have the opportunity to follow along with the demo that I show you. So we'll, we'll provide a couple of accounts so that you can. Actually go through the content alongside with me as I go through the demo if if you want to do that, right? and then after the demo we're gonna tie things up with some best practices assign you some homework, and we'll be well on our way, all right? So with that we're going to go ahead and start with our common introduction which is generative AI today. Now we're starting to see that many enterprises are adopting generative AI and of course more specifically uh agentic AI and we can see that and it's corroborated by this number here which is 33% of enterprises and their applications will include agentic AI in some form or fashion. Now this adoption and level of integration is going to drive 15% of automation in our day to day tasks. Now while that doesn't seem like a large number up front, let's put it into perspective in our own personal lives at in, in work. Imagine if you had 15% more bandwidth or 15% more time or 15% more headcounts with which to work. Or if we wanna take an out of an office example, think about runners or powerlifters. What if somebody could run 15% faster or lift 15% more? So with that kind of framing, hopefully we can be excited for that 15% that is coming our way. Alright, here's another number, and this one's a pretty bold 1. 80% of customer service issues resolved without human interaction, and this is all before the end of the decade. So just this past week I actually had to call in to report a lost credit card. I went onto the website. I found the line that said report a lost or stolen credit card. Here's the phone number, and right beside that phone number under it it said available 24/7. And look, listen, I, I appreciate the commitment to customer service, but this was 11 p.m. on a Wednesday. I, I actually felt kind of bad adding a data point, pointing to the fact, yeah, we probably should have somebody manning the phones at the most odd hours of, of the day. And so with that, this is gonna be the, the last, uh, statistic we'll we'll share which drives home this point Agenic AI is going to bring 30% savings in operational costs before the end of the decade again. And so that's not just in customer service, this is also in any other use case horizontal, vertical, diagonal, etc. OK, so it's, it's fun to daydream about the future, what it's gonna be like, all of the benefits, but today we are tasked with laying down the foundation so that we can even make that that future possible. Now, as many organizations are starting to do that, they're running into a very known problem or a problem that we're all gonna come to know very quickly, which is LLMs cannot deliver their full value without the proper context. I'm sure y'all have been to plenty of sessions, the keynote drilling home this point, and so we can just take a quick look at this example here we're interacting with a customer service agent, uh, that is, uh, part of a major airline. So in order to be useful, this agent has to be prompted properly because it doesn't know who I am. It doesn't know where, what my reservation is or whatever the reservations uh that are associated with me because it wasn't trained on that data. And it has to look at any number of reservation systems every single time I ask it a question regarding my flights and has to continue to look at any number of systems as I continue to ask it all the while saving the conversation for context because it's every single interaction is stateless so we realized pretty quickly prompt assembly and optimization get pretty complicated real quick. So this is what makes agentic AI different from previous generations of AI, right? In classic ML, predictive ML, uh, pre-built purpose-built models, however you wanna, uh, name them, you would take a domain specific set of data, you'd build a domain specific model, and you use that model to solve a very specific use case, and it's, it was amazing, and, and don't get me wrong, it still is an amazing way to solve a problem. But the problem is with that is that you can't just take all that effort, take the model, plop it somewhere else, and expect it to solve something, uh, some other use case. And so that's where Generative AI was, has shifted that paradigm. We now have models that are infinitely reusable, but now we have run into that limitation that we're all aware of, which is it's very smart about general information. But it, it cannot tell you anything about your business, your customers, your private information, and, and, you know, quite honestly at this point with the, the LLM, that's great because we didn't want its training on our personal information anyway. But to make that model respond more accurately and appropriately, you do have to provide it the con the contextualization at the right time using techniques like rag. So that's where some of those first challenges that enterprises run into when they're trying to implement that foundation for for agentic AI, uh, finding and liberating the data wherever it exists, those silos that we often talk about is, is quite a big job. Not only finding them and liberating it, but then also getting it to the right place at the right time adds on to that that complexity. Now the reality is it's not practical to go and and rebuild an LLM based on your data, and we've emphasized already that a single AI agent is relying on a quite a few different dependencies external to itself because of the data systems that it's interacting with. Now like humans, most problems are too big to be solved by a single person, so we build out teams. Those teams have specialized skills and they are able to scale out and the same is true for agents as well. But as you scale out systems as opposed to humans, you get more and more interdependencies and because of the agents talking to other tools, other agents, uh, data systems so if you aren't careful in your design, you end up with a very brittle interconnected hardwired communication path that contributes to that spaghetti code that we've often seen and probably quite honestly have experienced ourselves. So you've solved the problem of providing context. However, uh, you have now found yourself with a new problem. It's kind of like one of those nesting dolls where instead of popping out a smaller, cuter little doll, you solve one problem and you get to solve yet another problem. Alright, so with that, that is where confluence steps in, all right, and so actually I, I apologize is. It actually skipped the slide. OK, I apologize. So, uh, luckily we've seen this problem before in the past with microservices. So if you recall when we built a lot of our, our enterprise software, it was at a monolith where everything was on a single server, and in order to scale you had to go wide, you had to go tall. All right, to solve this, we started to break down the monolith into microservices, but once again, uh, uh, referring back to that nesting doll analogy. You solved one problem of breaking down, but then you ran into a different problem when it came to scaling because these micro surfaces often relied on the request response model. So to solve this, people introduced event driven architectures where it is when an event broker is introduced to be able to allow each microservice to simply produce or consume events as they occur and so there is thus a loose coupling between all of these different services. OK, so this is where confluence steps in. Now there is more to data streaming than just moving data from point A to point B very quickly. That's just a single, a single component. Uh, you also need to be able to have the ability to scalably, repeatedly connect to all of those different services and applications. You need to be able to govern that data as it's flying around your different organizations, and you need to be able to process that data so it's readily available by the time it gets to wherever you're sending it. And that is what Confluent provides is a full data streaming platform. So instead of what normally would be four different systems that I mentioned, uh, Confluent brought it all under one platform, and all four of these capabilities together allow you to be able to deliver that real-time context and data to your multi-agent architecture. Alright. So this is the story of how data streaming event driven architecture fits into agentic architectures, all right? And so confluence not only provides that data streaming platform but also has further enhanced the ability to be able to apply your, uh, capabilities within your gentic architectures by creating even more services within confluent cloud which I will introduce to you now. Right, so this is confluent intelligence. So again we've talked about the data stream platform. It allows you to move things very quickly while governing it, streaming it, and, and connecting to all your different, uh, data sources and, and applications. This is yet another step in the right direction as it relates to AI. So this is confluent intelligence, a wonderful offering of tools that allow you to seamlessly integrate large language models, machine learning, um, retrieval, augmentation generation, all those, all those fancy things that we've been talking about for the past 2-3 years into your streaming workflows. So this offering we can, we can logically separate into three categories here. So the first one you have is streaming agents, which is a collection of functions within Flinn that allow you to build, deploy, and orchestrate event-driven agents natively on Flink. Next you have real real time context engine. Now this is a managed, a fully managed context serving layer that allows you to continuously build, update, and deliver context to your agents and other, other applications. And then lastly you have built in ML functions that provide built-in uh flink functions that allow you to build AI ML workflow, so a little bit more of the traditional uh ML stuff that we've we've seen before. So we're gonna jump into each of these categories a little bit more deeply. So we'll start with streaming agents, that streaming agents section. So the first built-in function within FLN that we have that we're gonna be talking about is model inference. So this allows you to simplify the development and deployment of AIML applications provide by providing a unified platform for both data processing and AIML tasks. So with this you can allow agents to directly call out to an LLM, for example, Amazon Bedrock, um, OpenAI. You can also seamlessly coordinate data processing and AI workflows to improve efficiency and reduce the operational complexity that often exists when when putting those two things together. And because all of this is based on Kafka and Flink, AI decision making is, uh, allows us to be able to leverage that fresh contextual context for our to support the patterns that we've been talking about up to this point. Now one of the things that I, I really like pointing out on the slide, my favorite part about the slide is that the stuff that you don't see, right, it's all of the middleware that you normally would have had to build in order to get these systems communicating with one another. So think about this. Each time you would have a new use case, you'd have to think about, uh, writing up the, let's just say Python application, JavaScript application, decide what the underlying compute is. You'd have to deploy the, uh, a CICD pipeline onto that compute. And then you would have to manage both the code and the infrastructure. All for a single use case and then multiply that by however many use cases that an organization has today and then also any of the future applications or use cases that they would have so you can see all of that that I've just described is condensed into and this is a maybe a little bit of an oversimplification but really does still convey the idea that all of that can be reduced to a few sequel, uh, SQL queries within Flink. All right, we've mentioned it before. Most LLMs aren't trained on our data, but they can still incorporate private data by retrieving it from a vector database. Uh, and so with this way we're able to use confluence built in, uh, embedding feature that allows us to create those embeddings. So we're not quite retrieving it yet. I, I got a little ahead of myself if I'm being honest, but we are gonna be creating those embeddings that can then be inserted into a vector database like Mongo, um, OpenSearch or Elastic. And so you can call a variety of those embedding models whether it's the Amazon Bedrock Titan, uh, whether it's OpenAI's embedding model, um, you know, we can generate all of those and instantly sync it into Mongo DB and as soon as they're created they, they'll be persisted wherever you send them, right? So again, not shown here is all of the middleware you would typically have to build in order to even have that. Normally that's just a separate, uh, microservice right here we can see the microservices consist of a couple of queries. Right, tool calling in Flink allows agents to invoke tools in real time. So we again have simplified how we define a tool. We already tell it, hey, this is what you have available to you. Here's a bunch of the data that we're gonna feed to you. And by the way, I've already processed the data, so you don't have to spend that time computing it. I'm just gonna tell you what it's supposed to look like as it comes in, and you let me know whatever your answer is. So that contextual tool invocation natively in Flync is defined in an MCP server or as a UDF leveraged context from live and enriched events uh for more accurate agent responses. Right, this is where I was getting excited about, which is talking about what good would embeddings be if we didn't retrieve them. So we've talked about the fact that you can create those embeddings in Flink. Well, let's go ahead and take a look at what it would look like to retrieve them. Here you're able to create a reference to an external table. It doesn't necessarily have to be a vector database, but in this case we're, we're given a little bit more heavy emphasis just given the nature of the talk. Uh, it could be, you know, RDS. It could be any type of data set, and what that allows us to do is be able to join, uh, for example, a Kafka stream with an external table or in this case perform a vector search for a quick external table lookup. Uh, alongside and probably in 11 single motion we can also apply AI functionality with the ML functions that we're gonna be talking about in Flink. And then we have MCP tooling and access to other frameworks and libraries like Lang chain uh via UDFs. OK, so that was that first pillar that we were talking about that was streaming agents and the overall offering of confluent intelligence. We're gonna move on to the second pillar, an offering that, uh, confluent provides to be able to enhance your agentic architectures with real-time context engines. So I, I mentioned it, gave you a little bit of a, a, a brief summary earlier, which is this is the, uh, uh, the context layer that provides real-time context to AI agents. Thus enabling low latency and access for intelligent applications, so this is that that fully managed context serving layer where it continuously builds, it continuously updates and delivers that context via MCP. You know what, how that works is it materializes the streaming data. And puts it into memory, in memory, low latency views that can then be queried instantly while preserving that full governance, the lineage, auditability. So when upstream definitions of data is changed, that engine automatically reruns all the impacted data, and then it prevents drift to ensure that all the downstream systems aren't going to be impacted. So all of that's managed on your behalf. You don't actually have to think about that. It's, it's already, it's already doing that for you. So again, the key benefits are is that you have low latency live data that AI decisions can now make, uh, make choices on your behalf using the most current states. It doesn't have to be a 5 minute wait or 30 minute wait. You can, you know, exactly whatever is being served is the most recent data that's available for that, that moment, and then you're able to unify, process and serve so that that context can stay stay accurate. Alright, so for those who are interested, I will say real-time context engine is an early access. So if you're, if you're interested in a capability like that, please do, uh, you know, scan the QR code even if you may not be in this exact moment. Uh, this would still be a good slide to, to take a picture of so that you can refer back to it and, and request that access. Alright, so that was the 2nd pillar of confluent intelligence, we're moving on to the 3rd, which is the built-in ML functions. Now these are easy easy buttons for, you know, forecasting a lot of the, the more, uh, what we classically think of, of ML, right? So it provides time series forecasting, it provides anomaly detection, uh, and these are in the form of SQL functions and so for streaming data that means that it allows you to be able to derive those time, real-time insights. And these functions are built on algorithms like Arima optimized for real-time processing so that they can deliver the accurate forecast and reliable anomaly detections that y'all can, you know, come to rely on uh for your use cases. So common use cases include operational monitoring, so making sure that system failures or performance are detected, uh, as soon as possible, and we can minimize downtime. Predicting trends or irregular spending habits um for financial data, monitoring sensor data for predictive maintenance. I know that's a very, very common one, and then forecasting demand for, for retail, you know, we just had, uh, you know, a few holiday sales this past week. This would be a very, very common use case to use this with. OK, so with this we are going to go ahead and start on the workshop or the, the demo. I don't wanna call it a workshop because I don't want you to make you feel like you have to go through it, but that being said, you do have a couple of options that I think make this session very unique, um, so you have option one sit back, relax, watch the demo, uh, totally valid, right? Uh, we'll, we'll do the tour bus thing where we pause the, the, the demo every once in a while, talk along the way, have an explanation of what's going on, and then we'll keep on moving. Now for those who are interested in getting hands on, you, you can pull out your laptops. We have a few AWS workshop studio accounts that you can use, so you don't have to use your own AWS accounts. Uh, I will say you will have to sign up for Confluent cloud, so you have to bring that account or if you already have it. Uh, but otherwise if you do complete the workshop as the follow-on demo, um, and you show me after the session, you show me at the booth, I have a, a hat for you. Confluence actually has sponsored one of the F1 cars, uh, the VCRB cars, and so as a, as a homage to that, we have a couple of, a few hats that, uh, we'll be able to give to those who do end up following along this demo, right, so completely your choice. So with that, here is the GitHub repo that we're gonna be spending a majority of our time in today, right? So if you wanna take a, a quick scan of that, take a picture of it, it's also a great resource to, as, as a kind of getting started or quick starts repository, a great way to be able to have a reference architecture fully built out so that you can kind of, you can follow along data paths or or connections or anything like that. So I'll leave this up here for a couple of seconds until I see some phones lower. Now if we do have any questions we do have a a a a helper walking around uh that you can raise your hand and he'll be able to to help you out. Don't feel like if you can't get it done during this session that that it's game over. You can these accounts will be up for at least a couple more hours after the session. You can continue to tinker, experiment, explore, and then come visit us at the booth and say, hey, look, I did this, uh, you know, where's my hat? And so with that don't feel like you have to, to get it done right away. All right, so before we jump into the actual demo itself, I'm just gonna throw this architecture up here so we know what's going on. So we have two sides. You have your laptop, which is, you know, the, you know, it's gonna host the application for our race tracker, and then you have confluent cloud, which is, uh, allowing us to do the event-driven architecture that we mentioned, right? So what we're gonna do. Here's on on that left hand side we have a back end. It's gonna be producing race data so there's gonna be 8 racers. They're all gonna be changing positions. They're all, uh, they're gonna have different time stamps, and that's gonna be pushing into confluent cloud into those Kafka topics. Now as they arrive into those Kafka topics, we're going Flink is going to pick up the fact that there's a new event and it's going to process that event and that processing is going to be calling out to Amazon Bedrock, a little bit of concatenation, maybe dropping a couple columns because we don't, we don't think that it's relevant to, to for the LLM to be able to produce, um, and what the, what the LLM is doing actually, it's going to be. Producing a summary so you know, oh you know this, this driver passed, uh, passed up going 30 MPH faster on this corner. Let's make a, let's have the, the LLM make a comment on that, um, so it's basically gonna be narrating the race for us, right? And so as that comes back it's gonna, it's gonna drop into a different topic there, F1 commentary, and then it's gonna be consumed by our application so that we can see that on our front end. Right, so pretty simple architecture that we're gonna work with here. All right, so for those who do want to follow along again, not obligatory, but this is how you would do so. You're gonna go ahead and navigate to this, uh, workshop studio endpoints that they, they, uh, use to vend out AWS accounts. You're going to hit get started and then put in this access code. Right, if you're again, if you're interested in following along, take a picture of this so that you don't lose the code, uh, that is going to allow you to be able to retrieve that. And it says limited limited numbers. See how many we have here, we actually probably have plenty for those, so don't feel like, you know, you have to race to it. There's gonna be plenty of accounts for, for the number of people we have here today. You're right. Just waiting for a couple of funds to go down. All right. Here we go. So as those that are following along again, you'll have to sign up for Confluent Cloud. You can just go ahead and do that. It's, it's very easy. Just try Confluent Cloud. You can Google that and it'll, it'll come up with the, uh, the offer. The nice thing is, is as you sign in, there's already $400 being put into your accounts when you sign up, so you don't have to worry about using your own credits or or any of your own spend. All right, so for our race tracker, the first thing we're gonna do is actually hop into confluent cloud and create API keys. These are confluent cloud API keys. So we can do that by hitting the hamburger on the right hand side. We're gonna hit create API key. And you're gonna see a handful of API key types you're gonna hit cloud resource management. It's very important that you select the right one because this is the only type that's gonna allow the script that we later run to be able to create environments clusters, and so make sure that you click the proper API key type. Alright, now for those that are worried, don't worry, these keys are long gone, so that's, uh, this, this demo environment is not even up anymore, so you don't have to worry about that, right. Once you have the confluent cloud API keys created, we're gonna go ahead and hop over to the terminal, and you can see here we have done the get clone of the repo. And once that has been done. We're going to hop into the actual repo itself. And then we're going to CD into the admin directory. All right, now we're gonna copy the Yamel. Example and then we're going to edit it and this is where we're going to input our confluence cloud API keys. All right, it's vim, nano, whatever your flavor is, as long as we're getting that in there. Now you're gonna see these options on the bottom. I'd encourage you not to touch them because we already have everything set. This is for those who for whatever reason have to use USD 2 or something like that. Today we're gonna be hanging out in US East 1, so we're gonna leave those as all, uh, as commented out, OK. Now once we've done that, we're gonna go back to the main main directory here. And we're gonna have to update the executable to be executable or a set up script. And then we're gonna run that script and the nice thing is, is we spent quite a bit of time, not so that you didn't have to go PI install everything, MPM install all that, and so it's doing everything for both the front and the back end. And on top of that, what we'll see later on here pretty soon, it's also going to set up all of your confluent cloud resources so you don't have to go and click through. Uh, you know, probably about 5 minutes of setting things up and so once this is done, you're going to be able to see the Flint compute, you're gonna see the environments, you're gonna see the Kafka cluster all already set up for you, named properly, so you don't even have to worry about a configuration error. All right. You see here at the at the bottom. It's gonna prompt you, so good news is it's not just gonna deploy on you without you asking or without it asking, so we do have to hit yes. And again you can see it creating the environment. You see it creating that cluster. And then it's creating that flin compute pool which is essentially the, the application where we're gonna all those queries that you saw that during the, the previous slides that's where we're going to be inputting those queries, right. Setting up our roles, setting up our scheme of registry, updating any of these environment variables for us to be able to make sure that we're, we're pointing in the right direction. Right. And then with these Kafka API keys, you don't, you don't have to use them, but this script will to be able to create the topics, um, so you know, for example, the F1 commentary or the F1, uh, raw data. So it's, it's the, the tracking data, right now, quick notes, you're gonna see some of you may see this error up here where it says failed to register and you're gonna see the, the X emojis. That's a known issue. Don't worry about it. All we have to do is just run the script again. Now, if you're curious why that has happened, it's because the script is moving pretty quickly along and so when it creates the schema registry keys shortly thereafter, it's, it's tries to go and communicate with the schema registry saying, hey, I want to be able to register a schema for this topic. Here are the keys schema register says, Whoa, I, I'm not familiar with that because the keys haven't propagated. So you can see here all we're doing is running the script one more time. It's, you know, we already have everything installed. It says, oh yeah, you already have the environments. You already have, uh, the topics perfect. And then, then, you know, tier awesome, we got the schema registered, right. So we'll pause here for just a couple of split seconds here to allow anyone else to catch up if they're following along. All right. So you know this really has worked uh when you hop into confluent cloud and you can see a number of things we'll take a quick tour of what we've been able to create with that script. So the first thing is, is we see our environment F1 leaderboard environment or ENV. Within that environment we have a cluster that we'll click into and within that Kafka cluster. We see the topics that we're going to be using for for this demo. Right, there's nothing in it quite yet, but that's expected. We're gonna fill that up here pretty soon. It's got the API keys. All right, and then We're gonna show you the Flint compute pool just again a little bit of a tour. This is the application within Comfluent Cloud that's gonna allow you to be able to manipulate the data, query it, transform it, uh, make a call out to Amazon Bedrock. This is where all of that is going to happen now. Alright, so while we're here we're gonna go ahead and start playing around with Flink. We're gonna set up all of the different queries so that when data starts flowing through we're gonna be able to transform it, call it out to Bedrock, and, and store it in Kafka for. The time being now something I will note as you're doing if you're going through this, this top right hand side is pointing the flint compute pool to the right environment and to the right Kafka cluster. If you do not set this, you're gonna say, hey, please find this. Topic as F1 commentary topic, put some data in there and Flink is gonna say, I have no idea what you're talking about. And the reason is, is because we didn't point it to the rights, rights, uh, catalog and database. And so catalog is just an environment database is a Kafka cluster. Right? Now next thing we're going to do, we're going to start setting everything up. First step, we're going to say, Flink, I want to be able to communicate with Amazon Bedrock. Here's the endpoints. I know it's looking a little small. You can see already that we've included the cloud 3.5 sonnet endpoints. And we'll talk about cross region uh inference later, but you could see this is, this is what we're gonna be using to generate those again. These keys are long gone. Don't have to worry about anything here. So we're gonna run that, and what this does is it establishes a connection between confluent cloud and Amazon Bedrock so that when we need to make a call out to it, it already knows, cool, I know what endpoint we're talking to, and I have the credentials to do so. Right, next thing we're gonna create a model. Now this is a little bit misleading. We're not creating an actual LLM. We're actually creating more of a, a logical entity within confluent cloud so that when we call out to Amazon Bedrock, uh, we, we call out to this almost shadow model within Confluent Cloud which then is what calls out to, to Amazon Bedrock, right? And so. Um, I don't want you to get tripped on the fact that we're, we're calling it a model. Technically it is, but it's more of a logical one than it is an instantiation of a full-on model. But you can see here we're, we're telling, uh, we're, we're telling it, hey, this is what I, the expected input is, uh, this is the output, um, you know, this is what we're gonna be doing with the task generation. There's quite a few other parameters we can pass it as well, but for the time being we're, we're only gonna pass at the max tokens. Alright, now if you see here as uh in, in here I'm clicking to the left and I'm hitting this plus button, all it's doing is it's creating another space for the next query. Some of these queries are gonna be long running queries. Some of them they hit complete and that's it. You could just copy paste over, but I like to see this sequence kind of like in Jupiter notebooks. I like to see the sequence of, of commands that I run just so I can go through and when I'm debugging. Right, next we're gonna create a table. So this is another topic. This is a new topic. So under the hood, a table in Flink is a topic. So that's where the data is being stored. We're defining what that schema is. And so this is what the expected data looks like once it comes back from Amazon Bedrock. And then to cap off this particular part of the pipeline, we're going to run this insert statement. And you can see here I'm gonna allow it to get to a running state first before I pause this. 821 it might take a little bit. I'm sure it's going OK, well, OK, perfect. Right when I wanna keep going, um, alright, well, let's take a look at this, uh, statement real quick, right? So if we're looking, I don't know what screen to point at here, um, right here this inserts, so we're inserting into F1 commentary, and you can see here on this, this, uh, right under the select we're already doing some data processing and this is real time, so we're doing some concatenations, we're adding a time stamp, we're, we're doing a case statements where we're saying, hey, if the position. Is numerically a number 1. Let's go ahead and make that a text of highlights or if it's position, uh, you know, a different, uh, kind of position, if it's less than that, then it's gonna be, you know, warning else it's just gonna be info, right? And then we're sending that to, and it looks like a jumbled text, but I'll show you that this has meaning. We're sending that to the commentary generator. So if you look back up to the model that we created, we named it F1 commentary generator. And so this is, this is where we're calling out to the model. And we're saying, hey, here is all of the data that we're going to use to generate the summary. And then you can see the prompt here, generate exciting F1 commentary, please keep it under 80 characters. And then as uh when you give me the results, throw it in as a commentary and then you can just see up here we're renaming that commentary as message so we can more easily find it later down the road. One quick note, you'll notice down here that the statement of status or uh statement status is running and it's got that green check mark as opposed to completed. So when you're working with fling statements, some of them are going to be you run it, it's done. This is not one of those statements. This is one that you want to keep on running because what's happening is each time a new event comes in. It's going to experience this exact process. It's gonna experience the transformation all up here in the select. It's going to be sent out to Amazon Bedrock to that model, and then it's gonna come back with the, the commentary, right? So you don't want that to happen just one time. You don't want one event to come in and then the rest of the five don't get processed. Because it stops running, so you expect a statement like this, an insert to be running constantly. So you could think of this little box right here as essentially a microservices. This is a stream processing application all condensed into a SQL statement again not shown here. All of the work you would have had to do if you wanted to do this with Vanilla Python, uh, you know, with, uh, whatever infrastructure that you wanted, which don't get me wrong, there's a time and place to do that, but if you're looking to be, uh, you know, quite nimble, you wanna keep things a little bit more simplified within one single platform, this allows you to be able to do that, uh, given that there were no limitations, uh, with your use cases or specific needs of your use case, right? OK, so we'll keep on moving on that uh is that sequel statement. Now as we go and navigate what's gonna happen next, we're gonna go look at the actual topic within the the Kafka cluster. You're gonna notice there's nothing in there yet that's totally OK. Again, we haven't started up those applications, but we do like to be able to see because sometimes we run a command and it's like, yeah, I know that I, I technically created something, but it's nice to be able to visually see it with our own eyes. And so this is what we're doing now. We, we see the F1 commentary. Take a look inside. Nothing yet, but when it does come in, I know exactly what it's gonna look like. Right, so we are now ready to be able to set this up to get the data flow. We've set up our pipeline. This is if you ever played solitaire and you, uh, drop in that last card and it gives you that cool animation, um, this is, this is essentially that moment. So we're gonna go ahead and create a couple of tabs in the terminal because we need to spin up both the front end and the back end. So the first thing we're doing here. Uh, we open up a new terminal tab. We're still within our repo directory, but we're gonna navigate to that back end directory. And we're just going to start our virtual environment and then say, hey, let's start a Python uh main.pi. All right, now if you see any errors, no worries, we're gonna resolve those later, but for the time being, if you see this, it's working. Alright, so that was the, the back end. Let's start up the front end application. We're gonna do the same thing but just for the front end. Open up a new terminal tab, navigate to the front end, and we're gonna run that MPM, uh, dev run or run dev. And then once that's done we can see the local host uh endpoints where we can see that front end application so copy that. And this is what the application looks like. So the way this is gonna work when you get into this application is to start the race, all you do is select your racer. So for now I think with this one we're just gonna go with Lando for now. And it's gonna start the race. So what's happening in the back end, it's producing the race data, so we can see here, excuse me. Uh, all the race positions down to every second, and we already see our first comments that came back from the LM, so that it's pretty quick, right? Sometimes, in fact, I'm gonna tell you right now it's so quick that our front end isn't necessarily able to keep up. Um, and so this is just due to, you know, latency, local host kind of stuff, but we can still confirm by heading over to confluent cloud, and we can see that data streaming in pretty consistently with all that data, and this is not just the raw data. The topic we're looking at right now is the commentary data. So, you know, every 8 seconds or every 2nd 8 messages are being sent. Hey, this person is in place 1, this person is place 2, this person is place 3. Let's go ahead and make a comment on it. And you can see here that you can click into each message and it's it's saying essentially what's happening. Right, so again, our front end isn't necessarily gonna display that in real time, but you, uh, you, when you do build, uh, an actual application, you know, it'll be able to consume it's a lot better than my laptop can. Right, but the way around this is that we can simply wait for this race to end. We've got about 5 more seconds on this race. And then once it completes that's gonna allow the the application to consume catch up, uh, with the, the a little bit of the lag offset there and you can already see if it see part of it's running or populating in the front end. So when you get to this view, don't refresh you're action and click, go back to leaderboard and you're gonna see the commentary start to continue to trickle in as, as your laptop catches up and says, hey, there's still a lot more to consume here. Let me make sure I throw these all back into the front end. And again we're kind of poking around these messages taking a look at if anything you know uh comes out at you we can actually look at the position so this is the raw data that was coming in and again sometimes it's there's just so much data that it takes a little bit for the UI to catch up here so. We can see here every, every second we have this kind of data being passed into confluence and then subsequently to Amazon Bedrock. All right, yes, uh, logging into cloud is there. No, if you sign up, the question was, if, if you're following along, um, and you sign up for Confluent Cloud, is there a promo code? No, there's not. There should already be. So if you brought your own account, um, I would, I would recommend creating a new confluent cloud account. So just do an email alias Bob plus AWS at, you know, uh, google.com or, or whatever it is, and, uh, that will allow you to create a new account that has the $400 already pre-populated in there, yep. OK. All right, and then commentary here we can see all the comments already. All right, so one more sanity check to see that's that we're getting a response from the LLM. Alright. And then you can see as you go back to the front end of, uh, front end application, those comments have started to populate in. Now if you're a curious person and you click View anomalies, you notice there isn't anything there yet, and you could probably surmise it's because we haven't set that up quite yet. And so that's, that's that other pillar we were talking about the, the ML functions, uh, for the within the conflict intelligence. So we got to experiment with streaming agents. We're gonna experiment with this other pillar over here which is the ML functions. So we're gonna go ahead and set up this next part of the pipeline. And the way we're gonna do this is we're gonna create a new workspace. Now this is just a a nice way to be able to keep my queries clean. So you can see here this is all the, the, the queries related to commentary. And then we're gonna create a new one pointing to that right correct environment, and we're gonna create a new one for all of the queries related to anomaly detection. Right, and so I'm just gonna give it a nice pretty name so that we can keep track of it. OK. Now before we can do the anomaly detection, we need to set up quite a number of things here. We're gonna make sure that. We're gonna have to copy in this one here, so we're gonna create a table, and what this is is we want, we don't want just to detect an anomaly and it to just be lost to the ether. We wanna save it somewhere, so we're saving it to a Kafka topic which then can be consumed by our front end application. So you can see here the expected schema. Um, and then we're going to do this insert, so it looks kind of gnarly, I assure you it's, it's mostly a bunch of transformations again if you recall for this inserts, this is a, um, uh, a data streaming or, uh, data processing application, so we expect this to be running at all times. We could take a look at what's happening here. Uh, we're getting all the data, we're transforming it. Uh, we're, we're just getting the stuff that's relevant. Looks like we're packing in, uh, adjacent object and then we're also going to, uh, where is it right here, detect anomalies here. So we're just getting the information set in a way so that the anomaly detection function knows what to do with it and it can produce a result for us. Alright, so we're gonna go ahead and we're gonna need to restart our back end. You'll notice down, uh, earlier it was looking for a specific topic. That topic didn't exist until just now because we created it. So all we have to do is just control C and restart the back end, and you're, you're off to the races. That pipeline is now complete. That was, that was a very quick, uh, quick development of that particular part of the pipeline. All right, and so what we can do here is we can refresh. And we're gonna go with a different, uh, we're gonna go with Carlos this time to start a new race. So with this again back end is going to start generating race data. Every second there's gonna be 8 new positions being sent out to Amazon Bedrock. It's gonna start making commentary. Now the anomalies, if we click into this, and it'll do so here pretty soon, it's gonna, because there's so much for this laptop to to consume and already it's having a hard time trying to consume off of the commentary. What we're gonna have to do is just wait for the end of the race and we're gonna be able to see all the anomalies once it's caught up with that that lag offsets there. So we'll wait another 30 seconds here to make sure that that is allowed to, to, to catch up. But what we can do, and we did this with the previous thing. Which is we can navigate over to the confluent cloud UI, click into the anomaly topic that we had created earlier, and you can see those flowing in in real time. Take a look at what it says, what it looks like. Alright, so for now we're just gonna hang out here for a couple more seconds. And it's gonna populate or show you this new view of your race is complete, what do you wanna do, and you're just gonna click go back to the leaderboard. Right. So you can see it's already started to catch up. Your laptop is as, hey, cool, awesome, and thanks for allowing me to breathe and consume off of that topic. Now we could take a look at some of these, uh, anomalies. We could see where engines were too cold, too hot. We kept it pretty simple, but we can see that, uh, it, it was able to catch it with a 95% confidence. Right, so again, once again, I can't emphasize enough what's not pictured within all that stuff that we just did was all of the middleware you would have had to develop. Imagine all of the custom code that you would have had to build and then on top of that maintain and then documents and keep alive for however long that that application needed to to be used for within the organization now reduced to a couple of SQL queries. Right, so I mentioned earlier we're kind of poking around in the topic now looking at what the anomaly was, all right. So that is the main parts. I, I added actually a, uh, a bonus to this demo for those that are following along. You won't be able to do this because there was a little bit of extra set up, but I really wanted you to be able to experience the full capabilities of streaming agents. So what we've been able to experience is a call out to the LLM. We've been able to experience the anomaly detection with the ML functions. What we're now gonna do is create embeddings from Flink, and then we're also gonna be able to retrieve them from MongoDB. All right, so the first thing we need to do is, uh, create a connection which you saw this, uh, when we did this with Amazon Bedrock. We use Flink to create it, and now I'm just using the UI here, but I'm gonna be calling out to the embeddings, uh, the Open AI embeddings model. Give it the API key. And then I'm just gonna give it a nice name so that I can reference back to it later on. All right, so when Flink says, hey, I need to create an embedding, go to open AI, this is, this is what it's gonna be using. Now on the other side we're also setting up a Monga DB connection because we need to be able to read from Monga DB because not only do we need to create the embedding, we need to be able to use that embedding to do a semantic search within that vector database and so we're setting up that connection so that when we do that vector database search we say, hey, I'm using Mongo. Here's the credentials, uh, go, go pull it from there. Alright, so now we could see I like this, uh, this view is, uh, it, it, it seems like it doesn't say much, but this is, you know, 4 different organizations all working together on your behalf to make sure that your genic architectures or applications are able to get real-time data and, and make those decisions on behalf of your customers, your, your businesses, so. We're gonna head back to Flink now and we're gonna start building out this pipeline. There's a common theme here. Each time we need to add or build a pipeline, think Flink because you're gonna be doing the data processing and you're gonna be, you can see that data pro the the lines between data processing and calling out to these, uh, the LLMs, vector databases are blurring. It's, it almost seems like it's, it's all one thing and that's, that's really the goal of what we're trying to do with this, this kind of tool. So we're gonna create one more topic. You see the Mongol commentary because once again when we get the research, we get the results from our vector database search, we don't want that just to be lost in the ether. We need to store it somewhere. We're gonna bring it back into Kafka so that our application consumes from that topic and we can, we can read from it. Right? Uh, again, don't get caught up on the fact that we're creating a model. This is not, we're not creating an instantiation or an actual standing up a model. This is more of that, that logical entity within confluence so that when we do call out to it, it knows where to go. Right? Now we're creating one more topic called comment or actually I lied, there's gonna be one more, but we're gonna create a topic called commentary queries. This is where, where we're gonna present a question. So think of a chatbot. This is just an easy way to, to demonstrate that. So when we ask, hey, give me commentary on Hamilton, uh, for the past 3 races, it, this is where that query is gonna land. And then we're gonna do one more query of alright, well, now that I have my question, so I just gave you an example of give me commentary on Hamilton and his performance for the past 3 races, we're gonna create a, a vector or embedding of that and so we need to land that in a topic before we do a search in, in Mongo. And then this is where the magic starts to happen, so you can see here. We're calling out, we're doing the ML predict, which is basically saying call me, go out to this embedding model that I had just created. Or uh within flank and here's my query, right? And so this is my essentially the question what is my question? And again, one more time, we expect this to be a long running uh insert statements or application, right? Alright, next we're going to go ahead and do a vector search now that we have the embedding. We'll move this control panel out of the way for you. So once this goes into a running state I can review it with you. So what this is saying is that, hey, give me the vector each time a new event comes into this vector topic, so the topic that contains all the new embeddings, uh, let me go ahead and do a search, right, and I want the top 5 results. OK, so nothing's gonna populate yet, so you can see here because we haven't set a query yet, so the way we can do that, there's a couple of ways. The easiest way is just to do a quick insert into our query topic. So comments about Hamilton insert that. What's gonna happen is gonna this particular query it's gonna get sent to OpenAI embedding model. It's gonna have an embedding created and then it's gonna take that and take it over, uh, take that embedding and give it over to Mongo DB vector database and say, hey, I'm looking for comments. Related to this particular query, can, can you give me the top 5, and you can see here that it presented with us those search results. Um, if you click into it, it's gonna give you the top 5 results, but, um, you know, by comments as well as the actual embedding itself, right? Alright, so let me just share with you one last thing here. So I'm giving you the architecture diagram of what we've been able to. I'm gonna review kind of capstone everything that we've done. So the first thing we did, we took the car metrics and we transformed it. We sent it to Amazon Bedrock for commentary, and then we got comments, right? So that was that first part, port portion that we set up. The next portion was even simpler. We said, alright, let's go ahead and take the car metrics that we're getting anyway, and let's perform ML function on it, in this case, anomaly detection, and we were able to get the anomalies. Oops, there we go. That's a live one. And the last one that we just went through. We created a pipeline to say, you know what, I wanna be able to ask about the commentaries that have been created. Let me go ahead and create an embedding for every new query. Take that embedding And send it out to MongoDB so that we can get the top 5 matches and then when we get that we're gonna land it in a topic so that our front end can read it. OK, so again, if you didn't, uh, weren't able to keep up, that's totally OK. You, these accounts are gonna be live for another couple hours. Feel free to stop by the booth 7:21 in the expo hall. Show the, show the fact that you did the demo, and we'll be able to get you, um, you know, hats or another prize if the hats run out, OK? So we're coming up uh towards the end. Let me go ahead and wrap up with some streaming best practices while you guys finish up for those who are following along. So I got 3 top tips for you, right? Don't send everything to the LLM. I think that's a, a, a pretty obvious one, but it's not as obvious when it comes to data streaming. So I mentioned the fact that there are 8 messages every second coming in. We can very easily, uh, accidentally send 1000 messages per second and overwhelm the LLMs, you know, run up a bill. And so the way that we prevent. This is by you can see at the bottom here that where statement, you know, maybe we should only care about the high priority ones or we care about the the loyalty tier or even the region, whatever it takes to be able to cut down on those costs by making less calls. More importantly, the way we can frame that is what do we actually, what should we be caring about? We can't just be throwing everything into the compute just because. Right now, one way to be able to help us do this is just go look at the quota limits again, kind of one of those things that seems like an obvious, but you don't realize that the, these, how much these apply until you actually run into an error. So what I like to do is I go to the service quotas within the console and I'll say type in whatever model I'm gonna use in this case cloud 3.7, and I can see, hey, max tokens for the day, max invocations per hour or per second, uh, per model. And so I can work now backwards from these limits and make sure that I'm not going to accidentally run into these, these limits. All right. And then I mentioned this before, I would strongly encourage you to take a look at the, uh, cross region inference profiles. This is basically, uh, AWS is a way to make sure that they're not accidentally overloading any of their LLM endpoints. And so by, and you saw it earlier, typically it's prefixed. The endpoint is prefixed with a US dot, and what that does is it just sends to a logical. Br for those API calls and says hey I know you don't care where we're we're doing the compute you just want a response from the model we'll take care of of routing it intelligently to US West 2, US East 1, wherever it's not busy, um, and typically these cross-regional inference profiles allow you to get even more uh quota or service limits because it's able to distribute it across the globe. Right, so for the last minute I'm gonna flash up a few resources for you to follow up with. First one is confluence Marketplace. So this is a great way to be able to get started, discover, deploy, and even build streaming solutions. So all one place, uh, $400 in credits. Please take a picture and sign up there. There's a code at the bottom, so that's what you're gonna wanna use. Alright, if you enjoyed streaming agents, uh, and what you were, what you saw, all the, the fact that we didn't have to build a bunch of middleware, they were all just sequel statements and flink, this is how you can get started, uh, for those who didn't follow along, you can just go to confluent.io, get started, and then, uh, quick agent or quick start agents, right? And then if you wanna get hands on, I got two more slides here. One is the an even more in-depth workshop that you can join. Go ahead and take a picture. All right, one more, and this one is, uh, more ice or sorry, delta tables with AI agents. So this is our joint workshop with confluence and data bricks. All right, so we're out of time. Thank you so much for being here. If you have any questions, please come up after the session. Otherwise, thank you so much for, for joining us.