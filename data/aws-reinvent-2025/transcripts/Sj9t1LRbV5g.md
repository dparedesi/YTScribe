---
video_id: Sj9t1LRbV5g
video_url: https://www.youtube.com/watch?v=Sj9t1LRbV5g
is_generated: False
is_translatable: True
summary: "This session details the significant technical migration of Peacock's global streaming infrastructure from self-managed Kubernetes to Amazon EKS, achieving zero downtime while serving over 40 million customers. The presentation features Ian (AWS Principal Solutions Architect), Mans (Director of Global Platform Engineering at NBCUniversal/Sky), Pete (Head of Platform Infrastructure at NBCUniversal/Sky), and Manish (AWS Senior Technical Account Manager).\n\nMans introduces the scale of \"Global Streaming,\" which encompasses Peacock (US), SkyShowtime (Europe), Showmax (Africa), and Now/Wow (Europe), all operating on a single codebase and homogeneous infrastructure. He outlines the problem: the platform team was spending 30% of their time on \"toil\" (maintenance, upgrades, patches) and 70% on development. The goal was to reduce this operational overhead to free up engineering time for innovation without disrupting the thousands of daily deployments.\n\nPete explains the migration strategy, driven by four key principles: engineers are customers, protect customer interfaces, deliver capabilities not just technologies, and reliability by design. They chose a middle path between a full rebuild and the status quo: migrating 1,600 apps from self-managed Kubernetes to EKS within 12 months with zero downtime. The migration had to be invisible to development teams, who continued to use the same declarative interfaces (kubectl, Kubernetes API).\n\nThe migration followed a six-stage process:\n1.  **Pre-flight validation:** Reducing DNS TTLs.\n2.  **Build EKS cluster:** Including a dedicated end-to-end test suite and Valero for backup/restore.\n3.  **Snapshot:** Freezing pipelines to capture the state of the self-managed cluster.\n4.  **Workload Migration:** A 50/50 scale-up strategy to balance loads.\n5.  **Traffic Shift:** Delegating DNS zones to the new EKS cluster.\n6.  **Cleanup:** Re-enabling pipelines and decommissioning old clusters.\n\nCrucially, they built extensive automation and a test suite involving synthetic load testing, functional zone validation, and deep network analysis using VPC flow logs (analyzed via VictoriaMetrics) to catch firewall issues. This rigorous approach allowed them to migrate the entire global streaming platform in just 4 months after 6 months of tool building, with near-zero incidents.\n\nThe results were impressive: toil dropped from 30% to 10%, upgrades became 6 times faster, and they removed 50,000 lines of code. This shift has accelerated their ability to adopt new technologies like Carpenter and Service Mesh. AWS Enterprise Support and the \"AWS Countdown\" program played a vital role, providing expert guidance and heightened support awareness during critical migration phases."\nkeywords: Amazon EKS Migration, Kubernetes, Platform Engineering, Zero Downtime, Global Streaming, Operational Excellence, AWS Enterprise Support, Automation, Chaos Engineering\n---

So, good morning and welcome to IND 3325. 0 downtime at scale, migrating Peacock's global streaming to Amazon EKS. Uh, so my name's Ian. I'm an AWS principal solution architect. And joining me here today from NBC Universal Sky, Director of Global Platform Engineering, Ms. NBC Universal's Sky head of platform infrastructure Pete. And my colleague here from AWS Minish, senior technical account manager. So I don't know if you've worked in technology for maybe a few weeks, a few months, a few years, but I think you'll realize that um migrating anything at scale, to like Kubernete's clusters at scale to Amazon EKS doesn't come without its challenges. So, after a short reel, I'll hand you over to Muns to talk to you about how er global streaming were able to achieve this. The NBA is streaming on Peacock. games Peacock NBA Monday, coast to coast Tuesday and one epic 2026. Plus all new ways to see more than the score. From the court to the culture. NBA is on Peacock. Thank you everyone. Um, thank you, Ian. Um, so first of all, great to be here in Vegas. Um, just want to welcome everyone, especially on behalf of AWS, but also P and I from NBC Universal on Skype. So I'm gonna give you all a just a very high level view about what we call global streaming technology, which is essentially an engineering team that spans across effectively two companies, so that's NBC Universal and Sky. So just to give you a little bit more kind of context, so currently we have over 40 million customers here for, for Peacock, which hopefully everyone's aware of. Um, that's in the US, but we also have other propositions um in Europe and in Africa, which I'll talk about in a second. So we consider ourselves, obviously Prime might disagree with us, but we consider ourselves one of the leaders in, in, in live sports, uh, here in the US but also across Europe. And hopefully most of you sports fans will know we've got obviously got major deals with the NBA, NFL, Premier League, and recently, um, announced the MLB. Um, one of our biggest events we had, I think it was a year and a half ago and that was a wildcard, um, game where we accounted for over 30% of the US bandwidth, which is a huge achievement. Um, a little bit more about GST in terms of where we're based. So we've got kind of engineers based here in the US, um, in, in the UK, um, also of course Czech Republic, Portugal, and, and India. So we're a truly global, uh, team. So just talk a little bit about our kind of infrastructure and our stacks. So multi-tenancy is absolutely core to the way we work. And that's not just from an infrastructure perspective, but also from a front end. So if you look at the, the, the diagram on the right, we, we cover 4 propositions using a single code base, both from a front-end perspective, but also from an infrastructure which, which Pete will go through in a second. So from left to right, we have Peacock here in the US we have Sky Showtime, which is a joint venture between ourselves and Paramount, mostly in, in Europe. We've got Showmax, which is another joint venture between us and MultiChoice in Africa. And then finally we have the Now the W brand, which is um a streaming service in Italy, Germany, UK, Austria, and the Republic of Ireland. So as you can see, we have one single code base from the front end, as I said, but we also, the key for us is a homogeneous infrastructure layer that again covers all of those different propositions, whatever regions we are in around the world. For us, because of scale, we really wanna maintain consistency across all of our development teams, and that's really important for us. The reason, the main reason for that is how fast we want to move. And as I, as I said, we have one central platform team which Pete to my right, um, manages, which we'll talk through in a second. So the problem statement and what what, what we're here to talk about. So if you look at the lift, the platform team or the platform engineering team, um, right now we spend around 70% of, of our time on what we consider kind of development tasks. Um, and, and over 30%, which is kind of BAU toil. So an example could be a Cuban is upgrade, a security patch, the ones we all love to do. Um, and what we, what we want to do is obviously bring that down. If you look to the right. Because it's such an uber multi-tenant Cuban cluster, we have hundreds of different development teams across multiple time zones. And if you look, if you look at the graph, the number of deployments we do day to day is, is only just growing. So the key for us is how do we free up more engineering time for, for Pete and his team. Um, from a development perspective, but at the same time, what we cannot do is disrupt our development teams, especially when we're talking about, you know, thousands and thousands of different name spaces and components that are running absolutely critical services for those propositions I mentioned. So I'm now gonna hand you over to Pete, who's gonna talk about the migration strategy and the planning and how we got there. Perfect, thanks man. So platform engineering is the cornerstone of infrastructure for global streaming. Um, and it, we really focus on trying to take away the work that engineering teams ultimately don't need to do or um in often cases don't really want to do. So we really want to focus on pulling a lot of that heavy lifting away from them so that we can do that for those engineering teams. So I look after the platform engineering department across global streaming. So really what we're focusing on doing is doing that heavy lifting for the engineering teams. So what I now want to do is sort of focus on Four key principles that we have as a department and link that to how we approach the EKS migration. So the first principle, engineers are our customers. So what we really try and do, and this is really one of the key methodologies of platform engineering, is, is we want to internally market ourselves. Engineering teams should actually want to use our products. It shouldn't just be a case of they have to use our products because they happen to be plugged into our platform. The second, protect the customer interfaces. So we have an obligation to limit the number of changes that we make on our platform because we can't just suddenly make a change to an important interface that many, if not hundreds of thousands of developers have plugged into. We suddenly make a change to that purely because there's a technical evolution. We should be focusing on moving slowly in terms of interfaces, but rapidly advancing our technology that we're using under the hood. Third, deliver capabilities, not tech technologies, so this is really doubling down on that, on that aspect. We don't simply want to deliver a technology that may be the latest, greatest tech that might come out of the industry. We want to really understand how our customer, and when I say customer, in my case, that means development teams are going to use those products. So we want to work backwards from how we think they're going to use it, and then we leverage the technology under the hood to offer that. And finally reliable by design. So as Mant's already said, we support market leading streaming services globally, so even a few seconds of downtime just isn't acceptable. So taking those 4 key principles in, I now want to sort of explain to you how we approached the EKS migration project. So before I delve into it, you might be thinking, why are we talking about an EKS migration in 2025? And I'm going to give you a bit of history as to why that is, but before I jump into that. When we were first looking at EKS, we could have really taken two traditional paths. We could have either said, look. This is a significant change, we're gonna build a brand new version of our platform and we're then going to ask all of those development teams to shift over. That's not really doing the heavy lifting. Alternatively, we could have said, you know what, our platform's working, let's just leave it as is, and let's hope everything continues to work. We were really tasked to do something in the middle, which was quite daunting for us as a team, it was pretty scary, but I'm going to run through how we approached that and some of the lessons learned that we had. So, just to give you a bit of history. So we started using Kubernetes right at the inception, so this is pre-V1 Cubernetes, so back in sort of 2014, 2015. We as a department, and we were quite a small team at this point, we were exposed to a lot of bugs, a lot of issues in Cubernetes that we were really sort of contributing back to the community and Kubernete's code base, and we learned fast. But one of the key takeaways that we took from that really early adoption of Cubernetes was the controller pattern. And understanding the value of eventual consistency and being able to declaratively define what state you want and have controllers that will continually validate that state. So I could go in, I could change the environment that I'm deploying to, and we'd have controllers that would then go and reset that. It's quite different to sort of things like terraform, where you would sort of do a one shot. This is something that we continually want to validate. So that's something that was really embedded early on in our development of Kubernetes, but we were given quite an extensive period of many years of being able to really refine and hone in our understanding of Cubanettes. EKS was then launched in 2017 to 2018. But there was an issue for us. We would have wanted to immediately pivot to EKS at that point. However, Peacock came along and kind of changed the game a little bit for us. Our scale shifted from. A small user base within things like Now Proposition and Sky Go over to Peacock in the US. We were increasing orders of magnitude on the number of customers, the number of developers, the number, the amount of focus that we had on us as a platform engineering team. So over the succeeding few years, we then really tried as much as we could to focus on the EKS development and thankfully at the end of 2024, we actually launched our first EKS based streaming platform, which was Showmax, which is one of the propositions that Mant spoke about um earlier. But what we then needed to do was look backwards and go, how do we now take our platform that we've honed and built over the last 10 years and fundamentally a lot of significant streaming platforms are now integrated with, how do we now take that and move it to EKS? So just to give you a bit of more detail as to what happened for us throughout that same time period. So you can see on this graph here. We had a, a few years of, of, of fairly comfortable growth, it, it was great, we, we were able to really lean into our testing, our automation, we were able to really hone in on ensuring that our Kubernetes platform was reliable and stable, and I, and I think we really fulfilled on that. But you can see just after that EKS platform was released. We then had a surge in usage on our platform. So, how do we approach this? This is essentially what we ended up with, so this is what we were, were tasked to migrate at a really high level. So I'm sure anyone who's done sort of Kubernetes the hard way will have seen a similar pattern as this. You'd use things like terraform, Answerable, built on top of the AWS API EC2. NLBs, ELBs, all that other good stuff, so I, I think we had a pretty mature platform, but it wasn't managed. This was very, very much self-managed, doing it the hard way. We also had a number of. Engineering teams and a huge amount of business units. So we had over 1000 developers consuming our platform. We had hundreds of business units. These are all disparate business units, different organizations, different ways of working, different management structures, so there's not sort of one single individual or strategy that we could say we're going to now suddenly shift over to EKS. This is a significantly diverse set of individuals and teams that were using our platform. But the key part is 1600 apps. This is really what we wanted to drill into, because this is from the tech perspective, let's ignore the sort of business side, let's ignore the team side. We just needed to move 1600 apps from doing it the hard way Kubernetes to EKS. So let's take the sort of simplistic approach. So going back to that initial. You've got the two options, you either build fresh and then ask everyone to move or you, you leave the status quo. We were never gonna leave the status quo, we, we knew we needed to evolve the platform. So let's maybe hone in on that first option. So if we were to build a new platform based on EKS and we took let's say an average of 2 weeks per app, which is probably some apps significantly longer, probably a few less, but let's just assume it's 2 weeks, that that would give you 61 full-time engineers. Working on that for a whole year, that's a significant investment, and when we really take a step back for the engineering teams, they don't really care, right? It's really the platform engineering team that wants to move to EKS. It's less the development teams. They really just wanna focus on developing their apps, building their apps, releasing new features. They don't want to be focusing on moving to EKS going back to what Mans was talking about on that left-hand side. It was my team that was feeling the pinch of self-managed. We saw the increasing level of BAU, but we were thinking about shifting that responsibility onto development teams. This isn't doing the heavy lifting. So let's remove that, that's not really an option for us. So these were the 5 objectives that we laid out, and this links back to those 4 principles that we have as a department. So the first objective, no action for development teams. We offer interfaces, and it's a pretty extensible interface and it's a mature interface, in this case Q Control and the Cubernetes API. That doesn't need to change. That's how they deploy their app. We should keep that consistent. 2, migration must be done live. We don't have periods where we can just simply switch off our streaming platform. People always want to stream their favorite shows and live events. 30 downtime. So as with our principles, even a few seconds downtime just isn't acceptable. 4 Less than 12 months to complete. So this was really something that was largely driven by us as a platform engineering team. We weren't really given the timeline because ultimately from Mans's perspective, he was happy enough with Self-managed, wasn't massively happy with the increasing burden of the operational overhead. But we as a team felt that we were being held back and being able to evolve our platform, so we were hearing about Carpenter and all of these additional add-ons and auto mode and all these features that we couldn't utilize, and we were seeing the industry constantly evolve, if not rapidly moving forward, and we weren't able to leverage that and offer that to the business. So we internally said 12 months, full stop, let's move to EKS and let's make it happen. And lastly, and this is really the gold standard for us, teams shouldn't even know it's happened. So this is the gold standard, we want to revolutionize our technology whilst keeping our capability consistent. We want to be in and out without anyone even knowing. So I'm now gonna hand over to Ian, who's gonna talk about the partnership that we had with AWS to make this happen. Brilliant. Yeah. Thanks, mate. OK, so as an AWS solution architect, it's really my responsibility to help the customer with complex migration planning. And um I think over the last 8 years we've really been able to build sort of confidence that we can help the customer to achieve their, their outcomes. So every day we talk about Kubernetes, every day we're talking about AWS services and features. But for this it was a little bit different. I mean, we needed to to talk about the tooling as well as the services that would help with the, the migration process. So we're able to do this, but also we were able to talk about proven architectural patterns that would help the customer um and we're taking that right across a different sort of set of industries of how customers have approached um large scale migration. I mean this was really good, so we were then able to bring some of our er solution architects specialists, some of our service teams in to to validate the approach, to validate er validate the migration plan, and I think that was really useful, so it's all of the right experts at the right time that were really able to help the customer. And actually right from the delivery of Peacock, we'd always worked with the AWS well architected framework, and that had been really useful, not just as a, a sort of simple checklist, but actually a way for the customer to validate the existing architecture, but to really look into the future, look around the corner a little bit, and perhaps look at scale, to look at redundancy, resiliency, reliability, and of course cost is a big part of this. Um, but having done all of this, we were able to help the customer to just validate the approach, to feel comfortable that they were able to migrate. So when they were actually ready to make the decision to migrate to um Amazon EKS, I mean, we felt like we had all the relationships across the business. We felt that we had the relationships with, with AWS. So like when they were, you know, good to go, we were ready to, to really step up and help the, the migration. So then if we move into the migration phase, we've really started to double down on the tooling and also the AWS services. And then we spent a long time really just completely validating the approach and actually Pete and his team built out a six stage migration process. And I think what was really great about this is actually. They ensured that every single step kind of like moved it forward and validated and tested everything they needed to do. But if you've worked in technology for a long time, you realize that actually being able to step backwards and restore operational state is absolutely critical, so. It's great when things go well, but also you want to plan to restore operational state when they don't. So we really started to double down on some of the key services, like Valero, with Amazon S3 for handling the Kubernetes backup and restore, and Amazon EKS for now managing the Kubernetes clusters, Kafka for processing messages, Route 53 for DNS. But I think what was really critical was observability. So for the customer to observe the existing Kubernete's infrastructure, but also to observe the transition to Amazon EKS just to ensure that as the transition was was moving, the custom, it was migrating across, that the customers were having the experience that they should be. And actually, we'll come onto this later in the presentation. Manish will cover uh AWS Enterprise's support in um underpinning the, the migration. And but now I'll hand you back to Pete to discuss how global streaming went through the migration process. Thanks mate. Thanks Ian. So I'm now gonna go into a bit more detail as to how we did the migration. So this is really a view of a a slightly more detailed pictorial diagram of that answerable terraform uh cluster that we started with. So again, it probably looks similar to a lot of customers who are using self-managed or even EKS. So in this diagram we've got our. Different integration points into Kubernetes, that's our CICD tooling, that's gonna be a developer, Cube Cuddle, um, all of our CDNs and any third parties that are integrating into the platform. But the important part is that we control that route in. That's key for how we did this migration. And also key was the fact that we had quite an opinionated platform and how applications deployed into the cluster. Again, that was a really key part in, in having this be a a possibility for, for, for how we did this migration. So the first step for us was fairly self-explanatory, so spin up that new EKS cluster. This is in the new, in, in the same VPC, um, same egress nat so that any traffic coming out of this cluster looks and presents as if it's the same self-managed cluster and that sort of simplifies things like firewalling and, and fun stuff like that. Um, create a new Route 53 zone. Um, and this is so that all of the applications can still present themselves as the same layer 7 route into the cluster, but the important part here is we didn't want to delegate that zone via the parent zone. Step two, and as I touched on earlier, and this was something that we worked closely with AWS on because we really wanted to know surely other customers have done a similar migration, cause it can't be that unheard of, of moving from turnkey, do it-yourself Cubintis to EKS and, and Valero was the de facto choice for doing this. But again, it's a technology, not really a capability. We needed to build a lot of automation around it. But the first phase was to really shift across those core services, so that's things like core DNS, ingress, monitoring. I wish we had EKS add-ons for this, but we had to do a lot of hand cranking here. And then the next stage was to shift over the application workloads, and this is where we started to step a little bit further into the application team space. Previously we would have always said anything at the name space level is for the development teams to manage. We as a platform engineering team, that is not our responsibility, that is the development team's responsibility. But this is where we really needed to question that for this migration. At this point, we stepped into the application space. We had to do a lot of understanding in that space, we needed to understand every application. I'll go into a bit of detail, uh, later on on some of the more complex applications we needed to migrate. But we needed to really understand that and then shift it across, because we are suddenly, as part of this migration, taking on a lot of accountability in terms of the reliability and availability of those services. And then step 4 is delegate DNS. So this migration that we performed was a big bang migration. It was something that we had to do full stack. We could not do service by service because a lot of our architecture is based on things like Cuberti's service discovery and service routing. So it means we couldn't just shift over one app because any downstream maps may still not be up in that new cluster. So we needed to see the full stack in that cluster move over. And then lastly, and, and probably the easy bit and the very satisfying bit for sure, was to get rid of the old cluster. These are clusters dating back to 2014, um, which really was testament to the team that we were able to continually upgrade, patch and actually take tech from 2014 and and make it relevant for 2023, 2024, just when we were doing this, but um, yeah, it was sad to see some of them go, but um, ultimately it was, it was better for the future and we're we're able to leverage a lot more out of EKS now. So I'm gonna go into that 6 stage migration process that that Ian touched on, um, because what was key for us is to really lean on automation and controllers to do this migration. We could not rely on humans sitting there manually moving apps from one side to the other at the scale we were running at. It just wouldn't be safe and it also wouldn't take us 12 months. So the first step was pre-flight validation, so this is really where we look at things like Route 53 zone TTLs. We wanted to massively reduce those TTLs to give us more flexibility in being able to flick over to the new region or new cluster and then if anything does go wrong, we can pull it back. But this was something we did weeks in advance to ensure that all clients had that new fresh TTL set. Next was building the EKS cluster. Um, so alongside all of the ECS clusters, we also deployed a, a dedicated end to end test suite that we developed as part of this migration. I'll go into a bit of detail in a moment on that. And also all of the Valero mechanics, so the plugging in with S3 and all of the other stuff that we needed to set up in both the self-managed clusters and the EKS clusters. At this point, we're now ready to start taking a snapshot using Valero of, of that self-managed cluster, but we obviously need to stop any incoming changes into the cluster. Thankfully, because we have a fairly opinionated platform, we were able to centrally disable pipelines across all of our development teams to essentially stop them making any changes that would then allow us to take that snapshot into Valero and and start the migration. This is really the first time that developers might have been aware of something going on, um, but you'll be surprised as to the amount of messages we actually got, it was fairly minimal. People weren't really aware that this was happening. Now we've got the big stuff. This is where it's really starting to kick off. So this is where we did that workload migration and we decided to do a 50/50 scale up. So this allowed us to scale down our self-managed and up on EKS partly for cost, but also we wanted to ensure that there are certain issues that may arise by having two larger scaled applications, talking to some of the persistence layers. Again, I'll talk about that in a second. Stage 5, so this is really where we start to see traffic moving over to the new clusters. So this is where we do the zone delegation and then we bring that EKS cluster fully up to 100% and we bring the self-managed clusters down. And lastly, we re-enable the pipelines. So this stage process, so really from stages 3 to 6, which is really where development teams may be aware that something is happening, this is something that will happen in a matter of hours. This is single day activities that will be done so that we can do it quite quickly. We want to be able to take snapshots and we don't want to be blocking teams from making development changes because as Mans said at the beginning. There are a significant amount of deployments and changes continually going into production. We couldn't afford to take down the environments for too long. So I'm now gonna drill into two key areas that I think were absolutely critical for the success of this project. So the first is our is our test suite. So this is really the makeup of, of the different aspects of testing that we had uh across our migration, and some of these test suites remain even today and we leave in the environments. um but I'm gonna step through a few of them just to give you a sense as to what we were focusing on some of the key um performance indicators that we wanted to drill into as part of the migration. So the first is around synthetic load testing. So we want to continually know what is the latency, what is the availability at a P99, P99.5 across the entire platform at load. This isn't a functional test. This is continually injecting load into our platform so that we have very reliable data to tell us if there's something wrong on the client. But also you'll notice this is a service that we own as a platform engineering team, so we also have reliable data on the server side. So we manage the entire journey from request all the way down to response. 2, zone validation, so. I already touched on earlier around the fact that we have new zones that are created for EKS but they're not delegated, so we've taken on the accountability to make sure that those services are running. But dev teams can't access them. They don't know how to access these new pods that are suddenly running in EKS. So we had to take ownership of functionally testing those applications. We weren't doing full scale load tests. I don't think they'd be particularly happy about that, but we were doing very low level functional tests to ensure that things like all of their pods were up, that they were responding to health checks, that they were having a certain. Level of latency, that we were getting the right percentage of 2x, 3X 4 X 5X, and we were doing continual comparison between our self-managed clusters and our EKS clusters. So we were able to look at those metrics and essentially look at deltas between the two clusters. So even if an app has a significant amount of 204s or a significant amount of 404s because of the way that the application's configured or really the nature of the app. We would be able to compare and validate that there is no difference between self-managed and EKS. Next is node and cluster level checks. So again, we want to make sure that all of our nodes are healthy, we have a lot of services that run on them and we want to be able to run node and and cluster level checks continually. And lastly, and this was something that I think. Was probably the scariest for us, uh, and it's something that we couldn't really manage, and that is files that we uh we don't own. So we felt that the most likely cause of an incident or an issue with this migration would not be our firewalls because we've manage those and we can, we can essentially do all the due diligence around that, but it's. If any other team has a firewall that maybe only allows a certain 26 or 28, that potentially we weren't aware of, we can't validate everyone's firewall. So what we did was rather than look at those firewalls and reach out to all of those teams cos we sent the comms out, but was to lean more on PPC flow logs and cloudwatch. So we would look at any anomalies in spikes in potential dropped connections or anything. Essentially changing between what we expect on our self-managed versus what we see on our EKS. This was really key and we did pick up quite a few issues here where we were seeing significant numbers of packets being dropped coming out of our platform, which indicated a firewall issue. And underpinning all of this is Victoriatrics, so the scale that we run at, we could not simply rely on something like Prometheus or even really Thanos. We needed something more distributed that could scale to the size that we needed. So this was absolutely critical in us being able to do this type of analysis. So the next area I want to zoom into is our workload migration. So. I touched on it a moment ago around some of the more complex workloads, and I think the majority of our applications are stateless, so pretty easy to migrate. Um, there were some issues, but generally speaking, we were moving stateless workloads from A to state stateless workloads in B. Where it became a little bit more difficult um is this example that you can see here. So we have a number of services that use things like Kafka for queuing, um, and being able to have some level of producer and consumer um type uh architecture. The problem we had here. Was that A lot of the lafter clusters that we configure have a set number of producers and consumers defined on the cluster itself. So if we as a platform team suddenly doubled the number of consumers and producers, that could significantly affect the performance of Kafka, and we could get additional number of um. Essentially degrades that Kafka cluster. Um, so what we needed to do was essentially take a more fine-tuned approach to things like Kafka. You can see in, in that diagram that I was showing a second ago. We had to shift over by a delta of + or -1, so where we took a consumer and producer pair, we increased the EKS, we reduced incubilities. Again, this isn't something that we did one-off. We had to build automation to do this for a number of use cases across, across the board. But it was something that we worked really closely with a lot of our Kafka experts across global streaming, um, but it was definitely the more complex, uh. Issues that we actually hit, um, but thankfully we were able to sort of get through that. You'll also notice in that diagram that multi-region was key. So what we needed to do was actually shift some of our traffic away from the environment that we were migrating and wait for the queues to essentially get to zero before we performed that migration. I do appreciate a lot of organizations don't have multi-region, and we also have certain propositions that are not multi-region. So the way that we approached that was really focused on really quiet periods of the day to ensure that those queues are as close to zero as we can, and thankfully we didn't hit any issues at that point. So Where did we get to in that 12 month target, which was pretty ambitious at the start. So we essentially spent 6 months not migrating any cluster. Whenever I was giving an update, it would say, I haven't done a single cluster yet. Um, which wasn't particularly great when I was reporting that to Manz and, and, and, and Co. We, we weren't migrating clusters at that point. Uh, but we really had the confidence and backing of, of Manse and, and the team to, he knew that we were developing a lot of tools and automation to be able to rapidly migrate to EKS. So we spent really 6 months building that 6-stage process that I just ran through. And we finally migrated our first cluster at the end of Q2, which was kind of our internal target. But what we then were able to do over the succeeding 6 months, and really 4 months, was migrate the entirety of global streaming in 4 months. And that was all down to that automation and that testing that we'd built. And we had next to 0 incidents or issues with this migration, and that was because of the due diligence that we really put in. And I also really want to highlight some of the support that we had from Ian and Manish and team. We had a lot of support from AWS with this to ensure that a lot of the right people and the support was around us, and I'm sure Manish is going to go into a bit more detail on that in a second. So what's the key takeaway? Cos it's not really just about Kubernetes, to be honest, it's not, we're not gonna be doing an EKS migration again, thankfully. But the real unsung hero here was an interface, a declarative way of defining what you want with your platform engineering team. So what we're now looking to try and do is pivot as much of what we offer as a platform engineering team away from a plethora of APIs or interfaces or GitHub repos or all of that other stuff, and we want to build a declarative interface and spec that teams and our customers, our internal customers can declare what they want. So that today is their workloads in Cubanettes. But we're currently working on being able to define things like keyspaces. Any other database technology might have CDN configuration. Really, the options are limitless, and, and when I look at things like AWS controllers for Cubantis or ACK and um a lot of other tools out there, this is really where we're seeing the platform engineering industry going, and we're really trying to keep pace and ensure that we can offer that same interface for our developers across global streaming. So I'm now going to hand over to Manish, who's gonna go into some of the support he gave us. Perfect. Um, thanks, Pete, and uh hello, everyone. As Global Streaming's technical account manager, I would like to highlight how we are building operational excellence for global streaming, as well as how AWS Enterprise support helped facilitate this migration. But before we do that, could we have a quick show of hands, please? How many of you are AWS Enterprise support customers? All right, fairly good mix. Thank you. Uh, for those who are unaware, let me quickly talk about two key AWS Enterprise support engagements that were directly linked with this migration. So, NBC Universal and Sky are AWS Enterprise support customers, which means that they get access to a designated technical account manager like me, who works closely with the team. Through enterprise support we have built a technical partnership with the team. This also means that they get access to AWS expertise whenever they need it, whether it's for architectural discussions, technical deep dives, or operational challenges. For this migration, we had our AWS countdown initiated for the migration. So what we did was we worked in parallel with the customer team and we collated all the important information around their AWS environment and then we had our AWS experts look into the internal run books against the AWS services involved in the migration and come up with the detailed specific technical recommendations. In the next slide we will look how A Lewis count on help in this migration, but the real success story here is how global streaming team has leveraged all of these support mechanisms to make sure they meet their project migration goals. They take our inputs whenever valuable, but always maintain the clear ownership of the technical decisions. So, let's look into the timeline once again. Pete has shared this is what the project timeline looked like. The project got kicked off in January and soon after AWS countdown was initiated. Through the information we collated, we knew that AWS Amazon EKS is at the front and center of this migration. So this prompted a very good technical discussion point with the team. The team has been managing the self-managed cubein that is cluster, but with Amazon EKS, the control plane becomes the managed offering. So we discussed with the team around the best practices of the observ for the control plane, what are the key metrics that get exposed via cloud watch, and for this migration specifically, what are the specific metrics to look for, to to make sure that the control train is healthy. Similarly, we also had a discussion with the team around the control plane scaling aspect. There was already a good public blog post around it, around how Amazon EKS control plane scales in response to various inputs and metrics. But for this specific technical. Migration, we discussed with the team around what are the specific steps to take care of so that the platform scales in response to their migration. And last but not least, before the first progression cluster migration, we had heightened AWS awareness. So which meant that in case if anything goes wrong during the migration and if there is a need for AWS support case to be logged in, the support case engineer who is working on that support case gets the context of the migration very, very quickly so that they can help the customer efficiently and quickly. Through the rigorous testing, the proactive planning discussions, and this heightened AWS awareness, the project went smoothly without any issues. Now In the last slide for this section, I just want to highlight like what does what does the life look like after the migration. So operationally we are seeing already the benefits after this migration tracking the Cubans version has become very, very easy. A great process has become very smooth now, so this means that engineers are. spending time more on innovation rather than maintenance. Through our technical discussions, we also continue to discuss with the engineering team around various optimization opportunities. For example, we are already talking with the team around Carpenter for more efficient node scaling. As well as provision control plane scaling, and much more. Now all of these technical discussions happen organically through our enterprise support partnership. And at last, we are also exploring the different AWS Native services across compute, networking, database to add more resilience into the platform capabilities. With this, uh, I'll hand it back to Ms to talk about specific improvement metrics that we see that we are seeing after the migration. Thank you, Manish. Hopefully everyone's still awake in the back. Um, by the way, you all look super cool. It looks like a, like a scene out of Tron via futuristic, uh, pink headset, so it looks amazing. I'd love to take a photo later, but yeah, that's one for later. So, um, I'm gonna, now that I've given Pete 1212 months, um, to, to, for this project, what's the benefit? What, what do we actually get out of this? So let's, let's have a look. So that pie chart I showed you at the beginning where we had 30% as a reminder of kind of what we consider BAU toil, it's now it's dropped down to 10%, so that's just as a reminder, that's things like upgrades and security patches and all the things we love to hate. Um, so the benefit of that is obviously 50,000 fewer lines of code that we've just, you know, thrown away, which is, which is great. Our upgrades are now 6 times faster with VKS, so obviously that's that's a huge benefit. But what does that actually mean for us in, in, in reality? So what, what it means is that we can now accelerate what we consider development. So, so things like, you know, rolling out ISTO and Carpenter and, and Argo. It's not to say we couldn't have done that without this migration, but it's allowed us to accelerate it and do more in parallel. But ultimately that's the benefit that we've got. So, you know, great work to, to Pete and the team and, and obviously, uh, massive help and thank you to AWS. But yeah, I mean, obviously that it's, we could not have done that obviously without this great partnership. So yeah, thank you. I'm now gonna hand it over to Ian, who's got some questions for all of us, so up with you, Ian. Alright. Thanks, man. Um, so because it's a silent session today, it's really hard for us to do a traditional Q&A. So we actually um we were able to to talk amongst ourselves and and do more like a FAQ. So what we thought would really resonate with you are questions that we probably thought that you would ask, but we will be hanging around for a few minutes after the session if you wanna come and speak to us, and then of course you're welcome to do so. So, first question for Mans. um, so Mans, how has the Amazon EKS migration transformed Peacock's ability to deliver major live events like the Super Bowl, Super Bowl and the Olympics? Thank you, Ian. So yeah, I mean, as you can imagine, there's a lot less for, for Pete and his team to do in terms of, you know, whether that's scaling, whether that's testing, and you know, like I said, things like support, it's a lot easier. You also get the benefits of the new features that Amazon will rolled out via EKS. But it's actually a lot more than EKS and what we, what we're looking to do is kind of transform how, you know, how we work with kind of managed services like things like keyspaces as well. Um, yes, we have obviously large events like the Super Bowl and the NFL coming up, but. that the key for us is obviously the, the, the growth kind of internationally. You know, I could easily get a call tomorrow from, from my boss saying we've we've landed another deal, another partnership with another streaming service, and we've got another 12 months to do a, do a migration. So I think it's more about the mindset and the tooling that we've built as part of this that's ultimately, you know, given us that, that, that, that, that great, um, option going forwards. Yeah, brilliant, thanks man. Um, and I think Pete, over to you, you mentioned earlier. In your presentation that you're, you're hoping to expand the migration approach maybe to from Kubernetes, but maybe to like database or content delivery networking or something like this. So can you tell us a little bit more about that, that'd be great. Yeah, sure, so. As I sort of touched on in, in the presentation, I, I think the key for us is ensuring that we follow a declarative way, not just for application deployment, but for all aspects of the developer ecosystem and and life cycle. And I think traditionally what we've needed to do is build our own controllers and. We have a set of engineers within the team that uh this is really something that's their bread and butter, is to be able to build those controllers and uh and we've built a number of them over the years and this is pre-C DNS we were building controllers to manage Route 53 zone delegation through the Ingress resource and all of that other stuff. Obviously now the industry really is leaning in to the declarative controller-based eventual consistency, and I think we are really excited to see what's coming out of, in particular AWS so I think. Tools such as ACK or Crows, AWS controllers for Kubernetes, I think are gonna be game changing for us and that we can just use that, and I know it was actually announced a couple of days ago, the fact that that's now gonna be an, an EKS capability that we can just enable as well as think things like Argo CD. is just totally game changing for us, but if I pivot back to things like key spaces and CDN configuration, I think we right now are actively working on building controllers and contributing to controllers to be able to manage those aspects of the. Essentially the ecosystem, but the important part for us is making sure that the way that is declared is not technology specific. It needs to be declaring what the developer wants from that aspect. Aspect whether it is a relational database or whether it is the ability to expose their app on a CDN or adding caching, it should not be related to the technology under the hood so that in the future if we need to go and move to a different technology or a different CDN or we want to expose more CDNs, we don't need to then go and ask. 1600, 1800, however many developers that we may have to go and make those changes on their code base, that data's there, we can make that change on, on the platform engineering side. Yeah, and I think, you know, look, I think that's super interesting, and I think probably uh the people here today. And I think perhaps in the future, um, I'm sure you'll talk about it again, but I think it's a really interesting approach that I think people can really benefit from, and it, it kind of leads me nicely on to the next question from Manish. Um, so Manish, have you observed similar migration needs among other AWS customers in the way that Global streaming have migrated to Amazon EKS? Um, yes, in fact, we do, like, we do see other customers going through similar migration journey, but they all do in isolation. I appreciate there is not a lot of publicly referenceable documentation at this moment around these kind of journeys, so I'm really grateful that global streaming is sharing the journey with the wider audience, and I'm hoping that we can follow it up with probably like a further white paper or a blog post so that we can distribute it with the wider audience. Yeah, I mean, look, I think from my perspective there's a lot from people to learn from like yourself, Mans and Pete. Um, I think what you've done here is, is really interesting and kind of like super cool. I think, um, if we can blog about it in the future, or perhaps a customer use case, I think that I, you know, I think people would really appreciate that. Um, so changing track a little bit now, um, as Peacock expands months, um, how's the modernized infrastructure supported your overall growth strategy? Uh, yeah, I, I think speed to market is absolutely key and like, you know, like I said, we could easily, you know, sign a deal with uh another streaming service, you know, you know, in a matter of days and, and give it another 12 months. So I think speed to market and rather than us having to provision and write a lot of our own infrastructure code is using managed service, whether that's persistence, whether that's CDN, whether that's actual, uh, community clusters itself, I think it just gives us that speed of market, um. Key for us is a lot of these new kind of partnerships that we're making are kind of global. So you know, we could get a call, you know, next week to say that we've, we've signed a deal with someone in, in Asia or in or in Australia or in South America. So I think absolutely the key is the speed to market, and we only really get that by using more and more kind of managed services with, with, with, with yourselves. Yeah, I think that's amazing. I think obviously speed to market really counts, right? So, um, Pete, I just uh I got another question for yourself, um, you talked about your team's approach to testing and automation. I know we talked about application recovery controller and thought injection simulator and these kind of technologies. Um, how are you testing recovery capabilities and resilience for live sports again like the Super Bowl or the Olympics? Yeah, I think you can probably tell testing and automation is like at the heart of like my department. Um, it's something that we all pride ourselves in and it's always the first thing that we ask whenever we're starting a new project. Um, and that also means that it's built into our culture, so. When we look at our platform, we're constantly looking at ways that we can further validate and essentially break our platform and find those particular breaking points. And I think something like AWS false injection service or. FIS has been really valuable. We've been heavily using that over the last sort of 12 months or so since uh it, it got launched. Um, and that really allows us to validate things like zonal failures or regional failovers, all these sorts of things. Um, that's absolutely key, but I, I think another tool, and I know you mentioned it as well, is the AWS recovery controller, um, what we're now looking to do, and this is really only possible. Via us being able to free up that BAU time is we want to now pivot towards being able to be totally regional, um, tolerant, so we want to be able to use something like Arc to fully fail over to a different region. Um, but alongside that, things like chaos engineering and chaos testing within the Cubanettes cluster is again something that we really want to sort of try and focus on and build. Um, we don't want to have all of the testing and automation that we did as part of this migration lost. We want to continue evolving that it shouldn't just be a one shot. This has to just be something that we continually lean into, um, for essentially ensuring that things like the Peacock Super Bowl, um, actually works. Yeah, um, I mean, certainly we're we're hoping that is the case for sure. Um, I think that kind of segues now. So, um, Manish, what preparations are being prioritized to ensure the optimal viewer experience for large scale events? Um, yes, so as Pete said, um, like the teams across globally streaming to rigorous testing in preparation for all of these big live event games. Um, so what we have been recommending to the team is to go for. Another engagement within AWS Enterprise support which is AWSs Countdown Premium. So it gives you much more support while doing these testing phases. So our recommendation has been to get this Countdown premium at the start of your testing life cycle so that who whichever designated engineer is coming in, they have more context around the live event games, your scale, the anything that you hit on the roadblocks, so they are aware so that. When the big day comes, um, everything has been tested. Every AWS, uh, there is heightened AWS support awareness so that there is nothing that could go wrong on the day of the main live events. Yeah, thanks V. I think that's absolutely critical. Um, so I think we've got time for perhaps one last question. So let me throw a question back at you, I guess Ian, so obviously I've touched on kind of our global ambition, um, and obviously not just kind of global reach, but also kind of a lot of things we wanna do with Gen AI and kind of the features, but. Uh, can you give me, give, I guess, all of us, uh, kind of, uh, uh, an insight into what you have in your world over the next couple of years that will kind of help us push towards that next stage in terms of our, our ambitions globally? Yeah, I mean, I, I think that's a really good question for where we are at reinvent right. So, um, I think we're seeing new services and features turn up every day. Um, I think it was really, I think we're really relieved to see, um, Amazon EKS provision control plane turn up as a pre-invent item. Um, I think that's helped us out quite a bit, but look, I, I think. We'll have a roundup next week. I think there's so much to come from Reinvent and obviously I'd encourage you now to to attend as many sessions as as possible like this, but also er a lot of the service announcements. And then I think the same for everyone here today, um, you know, I, I'd really encourage you to get out there, really make the most of um reinvent and really look at all of the new services and features that are now being released on a, on a daily basis. So I think um with that now, we will close the session. And um but I'd really, really like to thank Mans and Pete for their time um and for their partnership. Um I think, I hope you found the, the story of their journey to Amazon EKS um as inspiring as, as we do. And look, I'd encourage you to get out there for the rest of reinvent, really enjoy your time here, and thank you so much today for your time and attention. Thank you very much. Thank you everyone. All right.