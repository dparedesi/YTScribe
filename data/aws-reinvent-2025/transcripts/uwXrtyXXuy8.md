---
video_id: uwXrtyXXuy8
video_url: https://www.youtube.com/watch?v=uwXrtyXXuy8
is_generated: False
is_translatable: True
---

Thank you guys so much for coming. I really appreciate your support in coming out to Reinvent and coming to see our talk. So if you've been alive in the last year, you've probably heard about Gen AI, you've probably heard about agentic revolution that's been going on and hopefully you've also heard about Amazon Bedrock Asian Corre and hopefully that's why you're here because that is the topic and we're gonna talk a little bit about what it means to build multi-tenant SAS agents in Amazon Bedrock, Bedrock, uh, Amazon agent Bedrock agent Corp. Ah, that's a good start. So with that. I want you to think a little bit about any SAS applications you've built or any that you've used and how they've evolved over the years and the best practices as we know them, and we'll get into a little bit about what it means to take those same practices and apply them to agentic solutions. My name is Bill Tarr. I'm a principal partner solutions architect and I'm lucky enough to be joined by Usual Buca, a senior, uh, principal architect, senior partner solutions architect with AWS. With that, before we get started, this is a 400 level session. So we're going to spend some time digging into code, showing actual examples. We will set the table a little bit, but we do have an expectation you'll have some background a little bit in terms of coding, in terms of perhaps a little bit around the agent core already. So with that, I wanna talk about something else really quick. I want you to know that there's also a workshop that reflects almost all of the practices as we've talked about them in this session. So if you like getting your hands dirty, getting into the code, figuring out what's going on, this is going to be today and uh tomorrow. It's listed as sessionful in the catalog. Don't hesitate to go wait in line. This is a great workshop. All of the things that you see today and even a little bit more are reflected in that workshop, and really it is the basis of this entire talk. And if you aren't somebody who can likes workshops or you don't wanna go do the workshop, you can just go out to GitHub and you can grab the code. So there's an actual code base with actual working code that is behind this. This isn't vaporware. This isn't us waving our hands. This is a real solution with a workshop you can attend and if you can't get to the workshops here, reach out to your AWSSA. They can help you get access to this workshop. So what is this workshop that we built? Well, here's the sample architecture. I don't wanna spend too much time on it, but I want you to see this in the beginning because what you're gonna see is a number of components that will be reflected throughout this entire talk. So you'll see that there's an agent core runtime. This is where our code lives, and this is where, where we're gonna execute our agents. We have several agents. There's gonna be an orchestrator agent that's gonna call a knowledge-based agent, and a log analysis agent, and a coder agent. Why? Because it's going to help us solve code problems as we as they occur. So it's actually going to be able to go to a knowledge base, look at our documentation, see how the code should operate. It's gonna be able to go to our logs through Athena and S3 and examine our logs and see if we can find a reason that the, you know, problem is happening, and it has a coder agent that perhaps can actually create code or review code as we wanna roll fixes out. Around all of these we have the tools that live in in gate in uh agent core gateway. We're gonna get it into what it means to manage tools in Agent Core. We're gonna talk a little bit about Agent core identity. What does it mean to manage authorization and authentication in a SA solution in a multi-tenant solution and an agent core itself. And then finally observability, I wouldn't want to do any talk around multi-tenancy without talking about observability and how we implement the ability to get visibility into the operations of our solution. With that, I said I was gonna talk a little bit about SASS. We're gonna focus in on 5 main SASS architectural challenges that we see with the customers we talked to who are building SASS today, before you even think about agents. One of them is challenges with tenant onboarding. When we say onboarding in SASS, what we mean is how quickly you can get your customers from learning about your product to getting value out of your product. This is one of the core components of SASS. If your customers have a bad onboarding experience, if it takes them a week to start using the software, there's a pretty good chance they've wandered off. SAS identity again, what does it mean to authorize and authenticate a user and how does tenant identity and tenant context inject itself into this process and how do we make sure that propagates through our whole solution? Data partitioning. What does it mean to properly manage our tenant data? How do we segregate our tenant data into either logic or physical buckets that keeps the data safe from other tenants' data? Tenant isolation, what are the policies that we use that define what a tenant has access to and what they don't? This tenant can access this service, this tenant can access this database, and they shouldn't ever cross those boundaries. Tenant resources should be specific and explicitly defined for the tenant. And finally, SAS observability, right? I've said that I would never have a SASS talk without talking about observability. The ability to monitor your tenant's health applies across the board to every SAS solution. In fact, I mean every solution. If you're building a solution and you're putting on AWS and you don't have any source of reservability, I wanna have a conversation with you. But if you're building an agentic solution where there can be many moving parts, I really wanna have a conversation with you. So with that, Usual, could you kind of kick us off and tell us how we get started with Agent core and agents in general? Sure, uh, so the topic of discussion today is, uh, building a multi-tenant agents using Amazon Bedrock agent core. So in there, the fundamental component, as you folks can see, is the agent. So let's start with defining an agent and then, uh, try to add more layers on top of it. So in here, uh, now, I'll try to obviously, uh, try to define what an agent is, uh, but by now most of the folks might be know what an agent is. So what I'll try to do is when I'm trying to explain what an agent is, try to highlight some of the architecture challenges you might encounter when you're trying to build an agent. To start off with, you'll write just a piece of a code, and then you have to deploy that code and run it in some kind of a compute. Here, when you see that an agent, assume that that is just a piece of a code running in some compute. Then you configure that agent with a large language model and then expose that agent to your customers so that they can interact with that particular agent. And then to extend the functionality of the agent, you need to write some tools code. Tool again, when I say the tool, it is just a piece of a code which will help you to reach out, which will help you to reach out to some external system or external resource. So, and again, at the end of the day, the tool code has to run in some kind of a compute through which the agent tries to reach out to some kind of an AWS resource or some other external resource. Now that you have this agent. The user might be interacting with this agent, so there is an inbound call, which you might have to authorize that inbound call, and the agent might be reaching out to an external resource. There is an outbound call, so you need to do authorization of that outbound call too. So what I'm trying to get to here is you need to have some kind of an identity which helps you with that inbound authorization of the inbound call and also the authorization of the outbound call. And as you folks know, when we talk about agents, there should be some kind of a memory management component which you need to have so that you could Store the agent's conversation memory so that the agent can Resolve a task assigned to it within that particular context. Last but not the least, as Bill was mentioning, we need some kind of an observability component, which will help you to have eyes and ears into the agent to understand what an agent is doing, why it took a particular decision, and all that good stuff. So hopefully you can see that in here when I define the agent, I also try to touch upon some of the architecture challenges which you have encountered. That brings us to the next slide where I would like to introduce you to the Amazon Bedrock Agent Core, which kind of like addresses the challenges which I've established in the previous slide. To start off with, you could build your agent code and deploy that in agent core runtime primitive to securely scale and deploy your agent code. And you can deploy your tools code in an agent core gateway primitive. Think of this primitive as just MCP as a service. And then you could configure the agent running in the agent core runtime with the agent core gateway through MCP so that it can invoke those tools. Now, again, here, the user invoking the agent for the inbound call, there is an inbound call, and the agent reaching out to the external resources, there is an outbound call. To help with those inbound authorizations and the outbound authorization, you could use another primitive called agent core identity, which will help you with those two calls. And for the memory management, you could use another primitive called agent core memory, which will have a short-term memory, a long-term memory, which we will talk in a bit here. And then, you could leverage agent core observability, which primarily, as I mentioned earlier, helps you to understand why a particular agent is doing a particular task in a certain way, and also helps you to capture any kind of a custom metrics related to that particular agent. Last but not the least, the agent core also gives you some custom tools, uh, which you could leverage if it makes sense for your architecture. So Bill, now that we have established some of these fundamental components, can you let us know how we could bring in a multi-tenant flavor here? Yeah, absolutely. So again, when we think about SAS in general, one of the concepts we wanna be thinking about is how we want to deploy our solution to get to different types of customers. It would be very rarely with a SAS solution find ourselves with a single archetype of customers. We might wanna go upmarket to bigger customers. We might wanna go downmarket to smaller customers. We might wanna have more configurability, but there's different concerns we have to think about. The first of those is how we apply multi-tenancy in those different models. When we talk about these, there's a couple of primary models that really start to emerge. The first of which you'll hear, hear us call either silo or dedicated. Now, in a silo or dedicated model, in fact, it's pretty straightforward. Everybody gets their own stack. You get your own web server. You get your own web server, you get your own compute. You get your own compute. Everybody has their own independent architecture, usually top to bottom. On the other extreme is pooled or shared infrastructure. And in here it's an entirely different challenge and in fact most of the architecture will be shared and at run time we're gonna have to make decisions. Who are you? What are you trying to do, and do you have permission to do the operations you're currently trying to do and there are trade-offs. And those usually boil down to a couple primary facts. One, sillo is a simpler architecture. Any of us could take a complete AWS stack, run it once, and then turn around, run it in another AWS account. It's not very hard. On the other hand, it's also not terribly efficient. If you think about it. When you have 2 accounts, it doesn't seem so bad. What about when you have 1000 accounts? It's gonna get a lot harder to operate and maintain that solution. On the other side, we have more complexity. To really properly do pooled or shared solutions we have to think deeply about what the multi-tenant concerns are and how we've handled them, how do we apply governance? How do we think about tenant isolation, how have we done data partitioning? All of those questions become very, very important, and these trade-offs all become in sort of different flavors because you won't necessarily make a clean choice between these two. You might just not have a completely siloed or pooled architecture. You might have a hybrid or bridge architecture. Where perhaps one piece is completely shared, other pieces are completely dedicated, even all the way down to a workload by workload decision, you might be deciding is this a pooled solution or is this a completely siloed solution. So with that we all hand it back to you and talk a little bit about multi-tenant agents and how those fit into these different deployment models. Sure, so, so far we can like define what an agent is and we also got you introduced to the agent core and then Bill has introduced us to some of the multi-tenant concepts. Let's try to bring all of them together and try to build our agents or turn our agents into multi-tenant agents. So that, let's start with silo agent. When I say the silo agent, it's an agent dedicated for a particular tenant. The way you could create that is you could have your dedicated agent deployed in a dedicated agent core runtime. It can be configured with a dedicated agent core memory, and then you could extend that agents with the tools core, which are dedicated for that particular tenant, which are deployed in a dedicated agent core gateway, which would reach out to your dedicated um tenant-specific resources. In this case, I'm just showcasing some of the AWS resources, and also it can be a third-party API. This would be the silo agent, whereas moving on to the pool agent, similar pattern using similar pattern where you have your pool agents deployed in a pool agent core runtime configured with pool agent core runtime sorry, configured with a pool agent core memory, and then. It is also configured with a pool agent core gateway where you have a pool tools which are trying to reach out to the pooled AWS resources. So again, here when we introduced a silo and pool model, kind of like you know we showcased two extreme cases, but in reality, some of the components of these components would be shared and some of them would be dedicated, so you would build a bridge or a hybrid model which Bill was trying to elude earlier. And again, I would like to point your attention to the agent core memory. This is the first time where I'm introducing short-term memory and the long-term memory. So let me briefly explain what those are. Short-term memory is a memory where um you could store some raw events on a per session basis like user inputs, agent replies, or any kind of system level changes. Long-term memory is a memory which you could leverage to store conversation summaries across the sessions, or user preferences, or any kind of, you know, um system level changes or, you know, system preferences. You could store these things in there. So now that we have established some of the fundamental components or some, you know, multi-tenant agents, let's try to take these things and start resolving the SAS architecture challenges which Bill has explained earlier. To start off with, we'll start with tenant onboarding. So for the tenant onboarding as SAS best practice, we have been always saying that or advocating that, you know, as a SAS provider, you need to build some kind of a control plane experience where it has a bunch of services like onboarding service, tenant provisioning service, or tenant management service, tenant registration service. All these services will help you to manage, operate, and deploy or, you know, onboard your tenants. When you're trying to onboard your tenant, that's when your tenant-specific architecture will be deployed in a SAS application plane. That's where your multi-tenant architecture or your secret sauce lies in. So now, leveraging the SAS control plane, you could onboard a tenant which would map to a silo model, and then you would provision this silo architecture which you saw in the previous slide. And since it is a silo model, you could pretty much provision these resources only when you're on boarding a particular tenant, because it is a dedicated resources, right? Similarly, you would use the same control plane experience and you would onboard another tenant, maybe the map to a pool model where you would provision this. Infrastructure for that particular tenant. Again, since this is a pool model which is shared across multiple tenants, you could upfront provision or pre-provision this particular infrastructure. Similarly, you could extend this experience, where if you could onboard a 1083, which is mapping to the pool model, then you would, you know, just map it to that particular model. And the way you would expose these models to your customers through maybe through tiers in a sense you could have a basic tier which is mapping to a pool model, or you could have a premium tier which maps to a silo model. So Bill, now that we have resolved one of the Sasar creature challenges, can you set us the stage here for resolving the next Sasar creature challenge? Yeah, absolutely. So yeah, we're clicking through architectural challenges one at a time. We talked about onboarding. Let's get into identity a little bit. Now SAS identity has always been true across all SAS applications. Some of the concepts still remain the same. Quick refresher, I wanna go over some of the primary tenants and just a little bit of a twist on why we think about this slightly differently when we're building Agent core. It's always been true we've got tenant users coming in and there's gonna be some form of an identity provider. We need something to manage all of these users. In our particular example, we decided to use Amazon Cognito. It is an identity provider that does provide tenant boundaries. The tenant boundaries, and we have choices incognito, but the one we chose to use is a user pool per tenant. So a user pool per tenant simply means we've got a nice tenant boundary. All of our users for each tenant are bucketed up nicely. This particular user right here belongs to tenant one. And as you can see they've got some custom claims or what we're calling custom claims here on this user, and the reason I say calling custom claims is in fact if you look incognito, we call these attributes incognito, but downstream when we instantiate and vend this job token, you're gonna find them called custom claims inside the job token. So we can have tenant status tier, any number of other met data about this particular user that we want to proliferate through the rest of our application. Now the jaw token is a vehicle for how we pass that data downstream, and I want you to keep in mind if you haven't done jaw tokens before it's OK. We don't have to understand this representation, but in fact they're just basically Jason in there and there's a couple different parts and this is kind of relevant when we talk about Agent Core. There's the identity token which automatically inherits all of the custom claims I was just talking about that can be useful for some applications. Sometimes you can simply rely on that identity token alone. On the other hand, there's another part to the token which is called the access token, and the access token actually doesn't inherently inherit all of those custom claims, and the access token is going to be passed around inside Agent Core. One strategy we can use is to use a pre-token generation lambda trigger. That can take all of those custom claims and copy them into the access token, which can be convenient for when you're passing it around and you're gonna see usual talk about how that access token is is passed around and reused at different layers of Agent Corp. So, could you walk us through what that looks like in Agent Corps with the identity? Sure. So now that um Bill has established uh how you know we need to create a SAS identity, now let me walk you through here how you could leverage agent core identity and let this SAS identity flow through your architecture and also we'll walk you through how you could leverage student context from that SAS identity and use the agent core identity primitive and enforce authorization. So earlier when I was talking about the agent, uh, when you talk about the agent, I mentioned that there are two types of authorizations you have to do one for the inbound call and the other for the outbound call. Inbound call comes into play when a particular user is invoking a particular tenant which is running in some kind of an agent or runtime environment or something along those lines. The outbound authorization comes into play when your agent is trying to reach out to an external system. So in here, I'll we'll first talk about how you could leverage agent core identity and do the inbound authorization. So, my assumption here is that you have some kind of an identity provider, you're configured in such a way that it provides you with a SAS identity, which Bill has explained earlier. So now, for your, when a user hits your website, you would redirect them to the identity provider, where they will authenticate and it generates a JB token. It can be an access token or an ID token, but it has the user information and also the tenant tenant-specific information. So basically, it's just as identity. Using the JDBT token, the tenants can invoke the agent's running in the agent core runtime. In here, I am just using agent core runtime and deploying the agent code and also the tools code here. So now, now you need to, now the agent, the agent core runtime has to authorize that particular GDB token coming from a particular user in equation. For that, you could leverage agent core identity, where you could configure your agent core runtime with the agent core identity, and also you could configure the agent core identity with your identity provider. So once you do this configuration, agent core identity will get hold of that inbound JWBD token, talks to your IDP, and authorizes that particular call. Once it authorizes that particular call, then the JWT token will be available within the agent quorum time. That's how the inborn authorization can be implemented using agent core entity, kind of like pretty straightforward thing. Now, let's move on to the outbound authorization. For the outbound authorization, you know, this piece of an architecture which we saw previously in the previous slide. As I mentioned earlier, the outbound authorization comes into play when your agents are trying to reach out to an external resource. Let's assume this external resource is an AWS resource. If it is an AWS resource, you could piggyback on IAM. You could leverage IAM to authorize that call where basically you'll attach an execution role to your agent core runtime which will authorize that particular call. But things becomes interesting when your tools are trying to reach out to an external resource or an agent which expects a worth token or an API key. In those cases, you could leverage agent core identity. First, what you could do is, for those external resources, they might have some kind of an IDP. You are some kind of credential provider, you could configure your agent core identity with those credential providers or IDPs. Since you deployed your agent in the Agent Corps run time. Agent core identity, inherently what it does is it creates an identity for your each agent, that's called workload identities. Just hold on to that thought. In a bit second, you will see how this workload identity will be leveraged by the agent core identity. Now, using, again, using the agent core identity, you could bring in a human in the loop where You could surface a form or something saying to the user that, hey, your tool is trying to reach out to some external resource. Will you authorize this particular call? Assuming that the user has authorized that particular call, now agent core runtime and agent core identity has to talk to each other and generate an access token, which will give access to the tools to interact with that external resources. The way it does is, the agent core runtime makes a call to agent core identity, saying that, hey, for this particular agent. Can you generate an access token? Agent core identity takes that request, maps that request to an workload identity, that is the identity of the agent, and then in turn maps that to the configured identity provider and generates a workload access token. Now, the agent tools, using this workload access token, can interact with the external resources. Again, since you have deployed these agents in the agent core runtime, all the steps which I just mentioned happens out of the box for you. All you just have to do in the code is use one annotation, that is, at the rate requires access token. Then you'll get hold of that particular access token. The similar process happens even for the API keys. So now, you can also bring in a multi-tenant angle for this outbound authorization. Let's assume that, you know, your downstream external resource or, you know, your this downstream external resource can be some third party API or it can be some other service within your organization. They're expecting some more additional information within that access token, that workload access token. That additional information can be metadata or tenants specific information. You could pretty much build that into it by leveraging agent core identity. The way you can do that is, since the agent core identity is already configured with the identity providers, that external identity providers can Customize the workflow which generates the access token, and add the custom claims. The thing which I'm alluding to here is the one which Bill has explained when he's explaining the assassinity. Assuming that you've implemented that thing here, now, agent core identity, when it generates this workload access token, it can also have this additional metadata, which you could pass into the external resources. So now so far when we talked about all this good stuff we are only talked in the context of agent core runtime. But again all the best practices or the things which I talked about here will still apply for the agent core gateway to the way it looks is this is the same architecture which you saw in the previous slide. The only difference here is that the tools code now is deployed in the agent core gateway and it is reaching out to the external resources. Now the agent code running in the agent core runtime can, using the JWT inbound JWT token, can reach out or make the call to the agent core gateway, and the agent core gateway can be configured with agent core identity to do that authorization of that particular request. Additionally, Agent Core gateway has a new feature called Gateway Interceptor, which can intercept the request coming into that particular agent core gateway and get hold of all the headers in it. And you could also get hold of the JWT token, and from that JWT token, you could make that available for your tools, or you could also get the tenant context from it and then leverage those things here. So Bill, now moving on to the next data partitioning, uh, SASar creature challenges, can you just do a quick recap and set the stage for us here right on. I told you the identity part was hard, right? That's probably the hardest part. I promise you none of them are gonna be quite as challenging. Data partitioning is actually pretty straightforward to think about, as we mentioned, it's really how we bucket up our tenant data. What are the challenges we face? Well, there's cost challenges, there's operational challenges, but there's only so many ways we can do this. You have to have some sort of separation, logical or physical, between the data. Now in terms of data partitioning, we've kind of seen some hints already, right? We've heard the term memory, so memory probably makes us think there's gonna be some partitioning somewhere, and we've seen some downstream AWS services too that might need to be partitioned. So usual, let me head it back to you. Tell us a little bit about how we can apply data partitioning in Agent Core. Sure, so when we say about the data partitioning, so it's basically you need to, you know, look at the areas where in this architecture you're dealing with data. One area is with the agent core memory where you're, you're storing the tenant specific data. And the other area is the AWS resources. In here, we are just showcasing, you know, a knowledge base and the Dynamo DB table. Now let's pick these two areas and double click into it and understand how you could implement data partitioning when it is a silo model or a pool model. To start off with, Silo agent core memory, where you have, uh, as I mentioned earlier, you have a dedicated agent core runtime. In here, you have a dedicated agent. The white box which I'm showcasing is, you know, think of this as an uh agent implementation code. And then. You would have a dedicated agent core memory specific to that particular tenant, and if you have multiple tenants, then you know you see something like this. Now let's say if the agent is trying to make or create an event in the short-term memory, the way it can create an event in the short-term memory is it needs a memory ID. Memory ID is a unique ID created when you are creating an agent core memory. And then it needs a session ID as I mentioned earlier, the short, in the short-term memory, you store the events by session ID. So each session ID for each session, you have a unique session ID. And then you need some kind of an actor ID. Think of this actor ID as a unique key for, for your agent or for the user. Other way to look at this is like, think of this as a primary key kind of a thing in your, you know, RDMMs database kind of thing. Here, what convention you could use is you could partition the data by tenant ID column subject. These values are coming from your input JWG token and then making this tenant ID, sorry, this actor ID as tenant ID and subject, you're making it unique for that particular tenant user. That way you're kind of like partitioning the data by that particular ID. For the long-term memory, the way the long-term memory stores the information is by name spaces. And what you could see there is the convention which you have to use for naming the name spaces. And in here, I would like to point your attention towards the actor ID there. So, before you, before I do that, that you see some variables with the curly braces, right? Those are nothing but the variables where you could inject the values in real time. So, leverage that same actor ID in this name space so that you have a unique name space for that particular tenant user so that the data is partitioned that way. But again, I would like to point your attention back to that actor ID is equal to tenant ID and subject, where my assumption here is that I want to partition the data by tenant users. But for some reason, if you want to just partition by tenant, then in in those cases the actor ID can be just a tenant ID. But again, in that, the follow-up question there would be, if that is the case, then this convention might not be a big help for you folks because you're already creating a dedicated memory for that particular tenant, so it might not be of much help. But this convention comes in handy when you move to the pool model where you're creating a pool agent in a pooling runtime environment which is interacting with a pooled memory which is shared across multiple tenants. And now when this agent is trying to reach out to the short-term memory, it uses this convention where you're basically partitioning the data by tenant user and similarly when you're. Interacting with the long-term memory, you could use this name space with that actor ID so that you are partitioned. For each of the tenant users. And now I'll quickly walk you through some of the AWS resources which you have seen, which you have been seeing in this uh architecture and talk about how you could partition the data there. To start off with, we'll do this knowledge base silo model, you create a dedicated knowledge base for each of your tenant, which is configured with a dedicated vector database, and pool model, you could have a pooled knowledge base which is configured with a pooled vector store which is shared across multiple tenants in here. The way the the data is partitioned or tagged here is that when you're talking about a pool knowledge base, you would ingest all the tenant-specific data. Along with that, it also provides a Amazon Be knowledge base provides you a feature where you could attach additional metadata to it. So what you could do is when you're ingesting that data into the knowledge base, you could also attach the tenant ID to it so that the data goes in there, even though it's living in the knowledge base, it is kind of like tagged with that particular tenant ID. You will see that how this will be useful for you later when you're trying to pull the tenants specific data from this shared or pooled knowledge base. Applying the same principle for another uh AWS resource, Amazon Dynamo DB table, silo model would be table per tenant, or a pool model would be single table shared across multiple tenants, but you would have a tenanted as a partition key, so that is unique for that particular tenant. Applying the same principle for S3 bucket. You would create a silent model would be a bucket for each of your tenants, whereas a pool model, you would create a prefix for each of your particular tenant within the same. Bucket. So now, moving on to the next architecture challenges. Partitioning the data is not enough. We need to make sure that particular tenant is able to only get tenant specific resources. That's what you know, the definition of a tenant isolation. So now what we'll do is we'll go back to the all the areas where we partition the data and see how we could implement tenant isolation for those areas. To start off with, again, we'll go back to the silo agent core memory. You have this uh dedicated agent. Which is interacting with your agent dedicated agent core memory and then um with the short-term memory and the long-term memory, it is using this name name space prefix which you have defined. Since it's a, since it is a dedicated resource, you could pretty much piggyback on IAM where you could attach an execution role to this agent core runtime which gives access only to the tenant specific resources. That's how you could enforce student isolation here. But again, The assumption which I'm making here is that if you look at the actor ID, which is equal to tenant ID, where I'm kind of like partitioning the data only by Tennant. If you wanna, even in the silo model, if you wanna do partition by tenant ID and the the tenant user level, then. You might, this might not be handy for you. You need to do a little bit more fine-grained authorization to enforce tenant isolation, which you will learn. When we are talking about the pool model. Again, for the pool model, you have the pool agent running in a pool runtime, uh, and then it is interacting with the pool memory, short term memory, using this actor ID and the name space. In here, since it is pooled, shared across multiple tenants, what you could do is The agent can. Assume this particular role. Basically you have to create some kind of a back role. This is nothing but an attribute-based access control role. In that role, if you look at the highlighted area, what I'm trying to do there is like I'm putting a condition saying that hey, give access to this contents in this memory to only for this actor ID. And then similarly for the long term memory, you could have another IM policy which has this condition which is giving only. Access to that particular name space. So now, trying to sum up everything together here. The pool agent from the JWT Token gets the tenant context. It can create the actor ID, the tenant ID and the subject, inject that actor ID into this IM policy, and then it can interact with a security token service. It's another service through which it assumes this particular role and gets some scope credentials which are specific to this particular tenant, and using those credentials, it can only access or interact with tenants specific data in the shared or the pooled agent core memory. That's how you would enforce Tenant isolation in this model. Now, moving on to the AWS resources silo model, you know, you have a siloed agents running in the silo tenant agent called runtime, which is interacting with uh. Siloed tools which interact with the siloed resources. Again, since everything is siloed, you could piggyback on IAM resources where you could create an execution role or IAM role which gives access only to retaining specific resources, and you can attach those execution role to the pool tools or to your agent quarantine which will give you only that particular access. Pretty straightforward. Whereas the pool ADWS resources, again, you know, you have the pool agent running in the interacting with the pool tools which goes to the pool resources in here. When you're implementing the isolation. For some AWS services, like, you know, what you saw in the Asian core memory, and for other services like, you know, Amazon Dynamo DPT, you have good integration with IAM where you could create an attribute-based access control role and then enforce isolation. But for some reasons it won't be the case. But let's pick how, in this case, let's talk about how we would implement with Dynamo DB table. Similar to what you have seen earlier, you could create an IM roll, a back roll, which gives only permission to that particular dynamoDB table item in that particular Dynamo DB table. If you look at the highlighted piece there, you're kind of like creating a condition there, um, saying that, you know, hey, for this particular tenant to only give only this particular tenant items. So Now, in this case, the, the pool tools from the GW token, get the tenant ID, inject that into the back roll, and gets the scope credentials. Using the scope credentials, it can interact with the 10 people. But if you move towards the pooled knowledge base, you cannot pool knowledge base as of today doesn't provide you a way to implement this ABA approach. But in turn, you can piggyback on another feature provided by the pool knowledge base or the knowledge base is called metadata filtering. So if you remember earlier when I talked about the pool knowledge base, I said like, you know, as a best practice when you're ingesting this tenant-specific data, attach tenant-specific metadata, like thin and tidy to it, and the data would be tagged to it. Now, Using this metadata filtering, what you could do is you could pull only tenants specific data by using your tenant ID. Think of this as like, you know, a SQL query you are making, and you need to attach a ware clause like tenant ID is equal to the tenant ID value. So to put it in the context, the pool tools from the JW token gets the tenant ID, construct that metadata filtering query with the tenant ID, and gets only tenant-specific resources from this pool knowledge base. So that's how you could enforce tenant isolation for this case. Now, let, let me hand it over to Bill to help us. Run through our next SAS architecture challenge. Right on. So you probably gathered already that I feel pretty seriously about observability and a SAS application. So let's get into some of the details of what we mean when we're talking about applying observability in a multi-tenant solution. So in general, the challenges around SASS observability haven't exactly changed in the deantic world. There are some specifics that do change, but overall we still have tenants and they're calling specific resources. In this case they happen to be agents. We could apply the same practice if we were using microservices or features. All of these calls need to be instrumented. Now typically you'll have logs, you might emit activity events, you might pick consumption events, whatever you're emitting out of your application, you need to inject tenant assess context into that, which includes tenancy. What does that look like? It might look something like this, right? It might be a simple, you know, a simple Jason that has agents and tiers, tenant IDs and regions. It could be any number of other items. Really this is an exercise in reaching out to our business users who will operate their business intelligence tools and finding out how they wanna slice and dice this data. Because especially with agents with the expense that can be added onto your solution, they're gonna have to be rethinking how their prices in the solution. They're gonna be thinking about how their customers are consuming the solution. All of those are measured by sending out metrics which will allow their analytics to operate the business more efficiently now, as we mentioned. Agent Core has an observability component, so we've talked about run time, we've talked about the gateways, the identity, the memory, all of these things we've sort of covered. I think probably a nice telling slide though is all of these things, each of the pieces we've talked about so far are directly tied into observability already. This is one of the really nice things that they've done in Agent Core because this is a built up from scratch product instead of observability being a bolt on that somebody thinks on after the fact like oh yeah I guess we should probably do some observability it's there from the start and all of the different components of Agent Core can actually send observability off to there and in fact it has some nice built-in tools already. Like an open telemetry, uh, implementation, open telemetry, uh, what we call the A dot SDK that we can use to send the tenant ID off of custom metrics. It has a nice integration with CloudWatch already. I think if you haven't used it yet, write an agent, have it do some logs, and you're gonna find a pretty nice agentic dashboard that already exists in CloudWatch that you, you can use to troubleshoot your agents and understand in general what it means to see the operation of your agents. Now this is a bit of a rant. Uh, 10ant ID is not a first class concept in observability. It isn't for cloud watch. It isn't for our third party solutions. It rarely is anywhere, and this is kind of makes sense. Not everything is a fast solution, right? So there are solutions out there that don't need 10 and ID. So when we're slicing and dicing our data and thinking about how we do this, it usually comes. Down to custom metrics and custom dashboards when we're building multi-tenant solutions because tenant ID has to be a first class concept when we're building multi-tenant solutions and this is just as true for agents as anywhere else. While the agent core built-in dashboards can be very helpful, you're very likely still going to have to complement those with custom dashboards that you create that revolve around your tenant ID. Now why is this so important? Why is observability so important in multi-tenancy? One example that we almost always end up talking to our customers about is understanding the unit economics of SAS. What are the, what is the cost of operating an individual tenant? How do we measure that? And the answer is it's actually not that easy to do. We have to build some custom observability and we have to omit custom metrics. Now, when we talk about this, one of the things we say is because this is hard to do, we want to prioritize what we want to measure. What's the expensive part of an agentic solution? Usually it's our models. All of those, all those invocations actually add up. The inference costs are probably the primary thing that your, your business is gonna be raising their hand and saying, hey, why does this thing cost so much? And the answer to that is to actually measure the tokens input and output out of the LLM. With your observability you can see this is actually pretty simple code. This is just a custom library recording metrics that do the model indications. It counts the number of input and output tokens. We're gonna collect those and we're gonna pass them downstream so our agents can send those off to CloudWatch and CloudWatch can collect those and you can start to see that the tenant specific metrics are flowing through and if we're using CloudWatch. We can certainly get to the point where we can filter those with Cloud Watch log insights, right? So logged insights is a simple query language. If you're using DataDog, if you're using New Relic, if you're using Honeycomb, they all have their own query languages. It's not like if you're using third parties it'll be any different, but this is ours, and if you look there, you don't even have to. On the whole thing, just look toward the bottom and you'll see some by 10 and 90, and that's really all this query is doing for us. It's taking all those indications, breaking them down by tenant, so that in turn we can pass that off to observe our observability tools or in the case of what we're gonna try to do here, use those to calculate the actual cost per tenant. So here what we're gonna do on the next step is we're gonna correlate our agent usage with the actual costs. So we've already done the first part. We've already actually created the cloudWatch insights, so maybe we use a lambda to occasionally invoke that. Take those costs and get the actual underlying costs, right? So we've got usage. What does the cost and usage report say? How much did it cost for each one of those invocations? Combine those two concepts and perhaps we could store them in Dynamo DB and they might look something like this tenant one having so many invocations. Now that we've broken down our individual inference cost per tenant, we can visualize those, you know, perhaps with Quicksight or another BI tool. And create nice dashboards that our business users can drive to understand a deeper understanding of what it means to operate our tenants. Cost per tenant might look something like this, and cost per tier might look something like this, right, where we can actually say, OK, this is about the cost per tenant, and tools like Quicksight with their own integrations with models now can also let those business users drill into those individual tiers and see how much it costs for the tenants inside those tiers as long as we provide them with that data. It's our responsibility as the builders of the agent to emit that data that unlocks all of those cases where business users can actually drill down into this data and understand what's happening under the hood. With that, usual, we started off with the sample architecture. Could you take us back to the sample architecture and walk through it with all of the different things we've talked about today and talk about how they apply to that architecture? Sure, yep. So now I'll present you the same sample architecture, but let's, you know, look from the lens of what we have learned or, you know, touch upon the things here to start off with. The architecture which I'm presenting would is pool model which is all shared across multiple tenants. So what you see is like, you know, we've created a bunch of uh agents which are deployed in a pool agent called runtime, and these agents are shared across multiple tenants and they're configured with a large language model and uh hopefully you could see that we are using SANs framework to build these agents and then we are using a supervisor pattern where you have an orchestrated agent which is configured with a bunch of subagents. Now, when you assign a task to this particular orchestrator agent, it will invoke the knowledge-based agent, which will in turn tries to invoke a tool, again, which is a knowledge-based tool, which is shared across multiple tenants, which is deployed in a pooled agent core gateway, which is interacting with a pooled knowledge base, which is again plugged in with a pooled vector database. So everything is shared across multiple tenants. And one, assuming that, you know, the orchestrator agent did not get a meaty answer for this particular request from this particular subagent that is a knowledge-based agent, then it would invoke the log analysis agent, which in turn configured to use this log analysis agent. To pull the data from the S3 bucket, which has the application logs, where we, you know, have the all the trend-specific application logs. And then in here, this whole architecture, again, you will see in this workshop that, you know, we have used a SAS control plane and tenant onboarding experience to provision this pool model into the SAS application plane. And then from the SAS identity concepts perspective again you will see that when you're invoking this agent, what we do is we ask for your credentials where you authenticate your uh uh you authenticate you against an AD ADP and then um generate a. JWG token, which is nothing but, you know, an access token which has the tenant information in it, and then the tenant information using the JW token, you would invoke this agent which is running agent core runtime. Then, as we talked earlier, we use agent core identity to authorize that particular request and make the JWW token available for the agents within the agent core runtime. And then those agents would. Again, when they're making a call to the tools in the Agent Corps gateway, use the GW token. Again, there to authorize that particular call, we use agent core identity. And again, here We don't, we are only resource, we are only trying to interact with AWS resources, but again, as I mentioned earlier, you could exchange the agent core identity concepts, apply for the outbound authorization calls also when you're trying to reach out to an external resource or external agent. From the data partitioning point of view. The areas where you need to do the data partitioning is obviously, uh, as we discussed earlier, the knowledge base. The knowledge base is shared across multiple tenants. We used, as we talked about using metadata filtering concept. We ingest the data with tenants specific data in it, and we, you know, partition the data in that way, whereas for the log analysis tool agent. We have a shared history bucket. We have partitioned the data with a prefix for each of the tenants, with the prefix name as a tenant ID. And then for enforcing the tenant isolation, again, going back to the knowledge base, we use a metadata filtering, where the knowledge-base tool from the JWT token gets the tenant context. And then uses the tenant context using metadata filtering, tries to get only tenants specific resources or tenants specific data from the Amazon Bedrock knowledge base. And whereas. For the tool analysis agent, the way we enforce tenant isolation here, since S3 bucket has a good integration with AA or attribute-based access control role, it basically has a good integration with IM where you could create an AA role. So log analysis tool, what it does is from the GDP token gets the tenant context, injects that into the AA roll, and gets access only to the tenant-specific application logs within that prefix of that particular tenant in the S3 bucket. That's how it enforces tenant isolation. And then last but not the least, as Bill mentioned, we used agent core observability specifically to capture the tenants specific metrics related to the number of input tokens and output tokens generated for that particular tenant. When the agents are executing a particular task and takes those things and logs, using agent core observability, we log it to the cloud watch logs. So hopefully you can see that, you know, where we started off, where we introduced you folks to all the different concepts, and we'll try to bring them all together and try to build an architecture um which would, you know, check all the boxes of what we talked about. So that's all we had uh for you folks today hopefully um this is uh helpful for you folks and don't forget about the workshop that that is today and tomorrow, right? So today and tomorrow definitely it's worth waiting in line for if you aren't signed up already. Thank you everybody. Thank you.