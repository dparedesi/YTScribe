---
video_id: 1HNUH6j5t4A
video_url: https://www.youtube.com/watch?v=1HNUH6j5t4A
title: AWS re:Invent 2025 - Introducing AI driven development lifecycle (AI-DLC) (DVT214)
author: AWS Events
published_date: 2025-12-04
length_minutes: 59.35
views: 995
description: "The AI-Driven Development Life Cycle (AI-DLC) represents a transformative approach to software engineering that positions AI as a central collaborator throughout the development process. Unlike traditional AI-assisted development methods, AI-DLC reimagines the entire software lifecycle through two key dimensions: AI-powered execution with human oversight and dynamic team collaboration. The methodology operates in three phases - Inception, Construction, and Operations - where AI initiates workflo..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

OK, let's get started. Hello, everyone. Thank you for coming to the talk on AI driven software development. It's an exciting area. A lot of development has happened, and uh we are going to share a lot of learnings that we had uh with working with several customers and with our own teams, building software using AI. Uh, my name is Anupam Mishra. I'm a director of Solution architecture at AWS focused on, uh, building AI engineering practice at AWS. With me, I have Raja. Yeah, thanks. My name is Raja. Um, I lead a team called Developer Transformation in AWS. My focus is developers, developers, developers. Glad to be here. Thank you. Thank you. We both have, uh, um, a lot of experience developing software. I have been with Amazon for 18 years now, uh, building software for several teams. Some of this is our own experiences. Some of what we are sharing our experiences from our engineering teams. Some of this is experiences which we had with our customers, so. Looking forward to having a great session together. We're also going to have some time for Q&A at the end, which we'll do outside this room. Uh, with that, let's start with the how AI is disrupting software development. There's a lot happening in this space. There's several tools coming every week we hear about new tools, but let's start with understanding who do we have in the room. How many of you are developers? Raise your hand if you're writing code every day. OK. How about product managers? Oh great. There are a few product managers as well. How about people who are working on DevOps infrastructure? Great. We have awesome diversity here. Uh, any roles which we didn't call out? Raise your hand if your role was not called out. Uh, what is it? Oh, engineering manager, yeah, of course, yeah, and CTOs and VP of engineering, like several leaders who are thinking about, uh, what, how do I navigate all the opportunities and challenges which we are seeing ahead of us. Um, how, how about, uh, how many of you are already using AI for software development? Raise your hand. About half of you. Oh, great. And how many of you are happy with what you're seeing with it? Great. Some of you are, some of you are not. So, I hope you will have a lot of clarity in terms of what is our learnings and we would love to discuss some of your learnings as well. But let me summarize some of our discussions which we have been doing with the more than 100 companies across the globe over the last 1 year. A lot of them are early stage startups. Some of them are Fortune 100 companies. Some of them are service industry companies. Some of them are product companies. But these are the three common patterns we see, depending on the person or the person who is talking to us. A lot of engineers generally say, Hey, we hear a lot that AI is disrupting everything, but what does it mean for us? Should we be doing a different training? Should we be using different tools? Should we be not doing what we are doing? So a lot of confusion. The second one which we see is a spree of tools. There's no shortage of tools in software being built using AI, so a lot of people are feeling, what does it mean in terms of switching from tool to tool? Which tool is best? We keep going from one tool to the other, and by the time we start using the second tool, we learn that the first one was better and we hope that the second one will be better. So a lot of confusion. And the third one is leaders who are leading large teams, they are saying we want to make our team AI native, but what does it mean to be AI native? What are the things that they should be doing? How do I take it to thousands of developers which I have on my team? So we are going to share some of our observations on these questions. But before we start there, there's something which is important to understand. While there is a lot of talk about productivity gains and what is happening with software, making software development getting easier with AI. This is a research which ThoughtWorks shared a few months back about the velocity gain actually is 10 to 15% when software is built using AI. This is in practical terms, so this is their analysis, but we also see there are some companies which are nonprofits doing evaluations. You probably have heard of a company called Meter.org, which is evaluating LLMs. They did an experiment where they had about 16 open source developers who were working on an open source repository. They divided them into two teams of 8 each. One team was given tools to use AI. Another team was asked not to use AI. So pretty interesting experiment. Similar set of problems were given to them, about 250 issues, and the team, the developers who worked on this with with AI, they were asked how productive you were, and their answer was about 23%. And then they, they worked on this further, and they, after working, they thought maybe we were 20% productive. But then when the actual analysis was done as to what did the team without AI did versus the team with AI. They thought that the team which used AI was actually 20% less productive. So it's not to say AI is not working, and this study was done almost in early, early 2025. So take it with a bit of understanding that LLMs have evolved a lot. At the same time, there is a paradox in terms of how do we measure productivity, what is perceived productivity, productivity versus real productivity. So there's a lot to be thinking about how do we measure it. Now let's let's think about why is it not working. Yeah, yeah, I mean there are a lot of anti-patterns, right? And in our one year research we kind of group them into two major buckets of anti-patterns, right? So um predominantly developers are using two broad approaches today when it comes to using AI in software development. The first approach is what we call AI managed approach. In, in this, um, it could be a very complex problem, a large core base or it's a very ambiguous problem to solve. Developers throw this problem to AI and expect that A should work autonomously and build the software end to end. This is the expectation. You know that's that's that's a very ambitious starting point. I believe this is going to be the future, but right now that is not the case. The starting points are always ambiguous, and AI makes a lot of assumptions, and what happens is that this kind of approach, throwing a problem to AI and single shot, waiting for AI to solve it. Seldom works except for very small prototyping scenarios, simple scenarios, but if it's a production grade application where hundreds of design decisions are made, people have to collaborate with each other, and so forth, this approach seldom works. And the worst problem is that there's a lot of code thrown at the developers, and they have to put their name on the source code as the author. The confidence level is low because there is suddenly so much code thrown at people and therefore this does not go to production. In the same velocity that we expect, and that's one of the reasons why these kind of studies reveal that, you know what, I can throw code much faster, but the entire STLC kind of slows it down anyway, so the productivity gain is small step increment. It's not the paradigm leap we are talking about. So that's one. The second approach is the extreme opposite of it. Um, senior developers, um, you know, having tried AI managed, what they do is, OK, let me take it over. I'm going to do the task breakdown. I'm going to plan it, um, and then I'm going to insert AI in some narrow areas, right? So code this function for me, do a security review on this small piece of code. So they narrow it down in that way. And then it seems to work very well, but the problem is that the intellectual heavy lifting is done by the humans, which is the same as before AI also, and that's where the velocity gain once again is not great. But also the processes they follow is the pre-AI era processes. There's a lot of human interaction, people throwing documents at each other. A lot of meetings have to happen to resolve issues and therefore the time saved in the portions that we use AI is wasted in. The scrum meetings and various other meetings which are not relevant anymore, right? So these are two broad approaches. We will later see a lot of anti-patterns, but broadly this is, this is what we discovered as the root cause of. How many of you are seeing this? Any of you seeing this? Yeah. Oh great. Oh great. Thank you. Thank you for confirming this. So, having seen this, yeah, what do we do about this, right? Yeah, so, so we started on a journey that, yes, these are the two broad patterns engineers who start, they start throwing all the problems. Generally we see two outcomes. Either they will say AI doesn't work, forget all the hype, it is not useful, or they'll switch to the second pattern, which is AI assisted, and they will start seeing some value. But we went on a journey to see how do we build the paradigm leap, like how do we build twice productivity, 3 times productivity, 5 times productivity, 10 times productivity. What, what does it mean? Like how do we get it? And we started on a journey where we started creating several experiments. Some of the experiments were internal, where we would take different types of business problems, build software. We'll get together as a team for 3 or 4 days, build the software end to end, and see how are we using AI. We tried different tools. All the tools which you can imagine, we tried all of them. We tried things like business context being clear. We tried green field problems where everything is new and a new startup getting launched. We also tried brown field problems where we take a large complex open source repository and see how do we add features to it, how do we fix bugs in it using AI, and, and those experiments led to a lot of conclusions and clarity. And then we started going to customers. We went to some of the customers. We said, hey, we have come up with some conclusions. We Uh, we started, well, we had already validated those, those, uh, uh, those assumptions, and we went to customers saying, can we solve a problem which you are solving already with you? And that was a great way for us to keep learning. So we've done about 100, more than 100 of those experiments where we go to a customer, work with them, solving their own problem, and then see how fast we can do the work which they already thought is important to them in a much shorter cycle using AI. So let us share some of the learnings from those experiments. So the learning we call the set of learnings as AIDLC, which is the session, AI-driven Development Life cycle, which is a set of rituals, tools and roles working together to create great outcome for customers, but also working with systems at scale, not just creating simple examples, but creating systems which really work at scale, production grid applications, working as if an engineer has written it rather than an AI has written it. And now let's think about what are the core principles of this method. The first one is how does AI work. We have seen that when you ask AI to do a work, generally it tries to be very, very helpful. When it is very, very helpful, it does things that you don't want. It will try to, let's say if you're asking it to create a system to create a shopping website, it will create an authentication for you. It will create a log management for you. It will create different applications which you don't need. Generally, it will try to create the entire application. But to control it, you need to understand how, how, what is AI going to do before it does it. So, this is a cycle which we do where AI, we ask AI to create a plan. We humans will validate the plan, and that's where we do course correction. These are the things which we don't need. These are the things which we really want to do more of. These are the assumptions which you shouldn't have. So this process brings, if you think AI has a brain. This brings AI's brain and human brain at the same level that human is thinking this, AI is thinking this, and now let it go in the same direction. So AI keeps refining the plan, AI executes the plan, and human again verifies the output. So that way when AI is doing the work, human is 100% aligned that this is the way I would have worked anyways. So AI is working as if a human has done the work. But before we go there, let me ask a question. Where does time go in SDLC? Like, how many of you think most of your day, those who are coders are writing code, most of your day writes in, goes in writing code? Nobody. OK, OK, that's good. Uh, how many of you think that most of your time goes in meetings? Meetings about half of your, yeah, I mean that's the reality of life. Nobody talks about it, but software development is not about just writing code, and a lot of people think that if we make coding faster, everything becomes faster. That's not true. And, and it manifests in many, many ways. In SDLC generally it is not by design, but this is how we have landed in a world where everybody waits for everybody. The security team is there to as a gatekeeper for software development team that where is your threat model and what analysis you have done. Operations team is probably waiting for the software development team to release some new code. QA team is waiting. For again a new release to be, so there are cyclic dependencies and and all of these lead to escalation meeting, alignment meeting, something planned, but then people saying, oh what you have done is wrong, now go backtrack it. So so much wastage which happens and and we thought, how should we think about this as a as a completely reimagined way because humans work better. When they're together, when they're coming together, they are sharing their experiences, the cost of interaction is low. Like right now, for example, if I send an email to my colleague, they may they may respond in 1 day, they may respond in 5 minutes, they may respond in 10 days. Now if I'm blocked on that information, how do we, how do we short circuit the dependency which we have in humans with different types of backgrounds? Some are business people, some are. People who who are very good at coding and technology, some are operation people. How do we create a very synchronous and very good way of communicating? So that is the other model which we thought that for AI to work well, we need to solve the human communication problem as well, because a lot of the time is getting wasted in humans not interacting in real time because code can be generated very fast. But how about everything else which takes so much time? And, and, and that's a very fundamental reimagination itself. I mean, if you really think about it, Agile in the last 20 years did not happen itself. Why? Because, you know, the sprints were longer, 2 weeks, 4 weeks, and that led to natural sequencing of work. So, um, a developer has to wait until the product manager releases the stories to that person, and then the rest of the people have to wait, and that's everybody waiting for everything kind of a cycle that Anapum demonstrated. But in the AI world, the prints should not be 2 weeks to 4 weeks. It should be hours or less than a day. That's how it should be. If that is the case, it's really possible for us to bring people together and use AI and make the decisions right then and there and then move forward. So one of the rituals using that principle is what we call the mob elaboration. So if a product manager has a has an intention. It's enough at a high level. If the intention is ready, that's enough. So we put product managers, developers, QA, operations, all of them in the same room, and in a matter of 4 hours or half a day, you use AI to refine the intention, use AI to create the stories, and everybody is offering their oversights and validation on what A is generating. And you are able to sharply end with everybody agreeing to what are we going to build, what are the stories, and more important, AII also get a great context. So that's the alignment of human versus AI and human versus human all compressed in the short span of time, and that's one of the rituals that we keep practicing with customers. And every time we do that, this is a new way of working. That's a jaw-dropping moment that wow, you know we can really do this now. So that's one ritual that changes everything. Yeah, just on this one, the outcome of this exercise is a set of stories divided into units of works of work. So these units of work generally take many, many months, depending on the size of the company. Sometimes it takes a few months. We have heard from Customers, the work they do in these 3 or 4 hours takes them multiple quarters depending on the complexity. So it's amazing practice. What happens next? Yeah, what happens next is, so after the generally about requirements are well analyzed, broken down into multiple pieces that can be built. Then the construction starts broadly and that's also a mob ritual. We, we don't let teams be separated and build on their own. Um, now again, you might have noticed who are practicing AI, um, we don't need two pizza kind of bigger teams anymore. The team sizes are shrinking and AI playing a bigger role in that. And therefore you have smaller teams, cross-functional smaller teams, uh, full stack developers, one business person, one specialist in, in one desk. It's more like a single pizza team. And then such teams grouped together who are tackling the entire system, get into a room and they're just building it very fast with the same methodology that we're going to give some more details on. So these teams interact with each other, through the APSs with each other. They're sitting co-located. I mean, co-location doesn't have to be physical. It can be virtual also. But the, but the important point is people coming together, synchronizing their calendars and doing it at the same time with AI, moving rapidly fast. This is the reason why you are able to go beyond 2x and 5X kind of velocity, and we have proven that 100 times this is possible to do this. So these are some of the rituals that are very important. So putting it all together now, we don't want to throw these best practices and rituals as individual recommendations. We kind of put them together into a reimagined new methodology which we call aid-driven development life cycle, and the differentiator here is today if you look at pure tools, tools-based approach, they are more geared towards individual developers hacking and building stuff. What enterprises need is a collaborative method. That different teams come together, make a lot of decisions, and build things very fast. And therefore you need a methodology, you need practices, you need, you know, it's a people process tools all coming together to do this and like any typical methodology, this is, this is end to end covering inception, construction, operation, and it's iterative in nature and that's how it is. And the important point is it's reimagined. We don't retrofit A into existing agile. It's a bit of a new imagined method that gives this breakthrough. Let's take an example. Yeah, yeah, definitely. So the method goes like this. There are 3 phases. Obviously you will, you will receive some resources later for you to study deeper. Inception, construction operations are broadly the phases, and you have some stages within each phase. Typically the 9 stages represented there are very typical of this number of stages, and each stage goes by the cycle that Anupam pointed. Each stage you are getting able to plan that stage, verify. Ask you to generate the output and then you verify validate and the phase ends and the stage ends and each stage is leading to the next stage. The context that each stage produces gets richer and richer semantically as you go down the stages, and the AI's output is far improving as you go down the stages, right? And one important point to note is that these stages are not one size fits all workflow. And it is adaptive. The reason is a simple defect fix don't have to go through all nine stages, but if you're adding a new function, a business rich function to an existing brown field that has to go through most of the stages, green field will go through all stages, so this is very adaptive. So it's a, it's a real AI first methodology where we get AI to plan this out, and AI is going to recommend what stages are important for a given workflow or given intention. And you will be able to apply your oversight, you know, make some changes if that is needed, and you can set that into motion. So that's the whole idea behind this. The way I think about it is if an engineer has seen somebody thinking about a problem. To that problem getting well defined, to that problem getting designed to unit tests being written, to the code being generated, if they see everything, they feel like this is my thing. They don't feel like it's somebody's else. So that is a great outcome of this as well, that people who work in this way. They can see that I'm using AI across the chain, but I understand everything. I can own it. I can maintain it. I can fix bugs on it. Let's talk about some of the stories from some of the customers which we have worked with. This is an example from a system integrating company called Vipro. They have been working in several domains, but we worked with them on one of the customers of theirs where they created 3 distributor teams in 3 different countries, and they had a few months of work planned. All the teams bought their work. They were, they were able, they worked for 4 hours for 5 days, which is 20 hours in total, and in those 20 hours they were able to finish all that work in an enterprise healthcare space. And they shared that it is not just a faster way to build, it's a better way to build, and better here means better quality, better understanding, the team having a much better feeling about the work, like a lot of excitement in the room. In fact, some customers I've heard saying, I wish we continue working in this way. Tomorrow. I don't want to go back to the old way of working because you're seeing actions happen rather than a lot of slow slowness which we see in organizations because of so many people not not being available, dependencies, and everything just happens here. So the other example is a is a fintech company called Dun. They, they are in a stock trading space and they, they built a completely new application using this. They had planned to launch it in 2 months. So, so they built it in 48 hours, a matter of 2 or 3 days, and they were able to release it next week itself. So, so great. There are several stories like this. We have worked in customers in Japan, customers in Asia Pacific, Europe, and across many industries. But let us also share that it's not just about method. It's also about how do we use AI because sometimes we think of AI or a tool that this is the best tool and I can throw everything and this is going to be the best thing, but working with AI is also a skill, and and we have learned several things over the last last many years, and let us share some of those learnings with you. The first learning is how many, like, probably everybody has heard of code. Anybody has not heard of wipe coding, I mean, it's, it's the term has become more popular than it should be, but I think some people misinterpret it as well that uh I can keep AI. I mean, I can ask AI to build something and I can keep asking you to fix it until it finally works. But you know what, if you don't know what the code is doing, uh, if you don't understand, uh, what dependencies it has, how much duplication it has, how would you feel comfortable owning that, so. I, I think the world is coming to a conclusion that it's not great for building production applications, and also people are redefining what it means. But what I, what our takeaway from this is if an engineer is working on an application, they should start with a goal that I need to understand each and every line of code. I should be able to debug this code. And if you don't understand the code end to end, how can you be comfortable putting your name on it anyways, because somebody will say person is did this check-in. You can't say AI did this check-in. So, so that is a fundamental premise of how you work. You have to understand it, otherwise there's no value of moving fast. Yeah, um, again, this is again the best practice that the methodology inherently follows. Um, for example, if I ask, um, Anupum's example of ask A to build me a complete e-commerce platform, if I make, given, give that task, what is the meaning of completeness here? Should, should there be payment? Should there be shipping? If so, what are the shipping channels? What are the payment options? There's a lot of assumptions to be made here, right? So the task for AI shall not be broader like that. But instead, if I say, hey, here is a piece of code, check whether there is an SQL injection violation happening in the vulnerability there. This task is very narrow. what to do, where to do is very clear. And therefore, what the methodology does and what you shall follow also is to ask A to decompose the tasks, right, and decomposition should be pretty non-ambiguous and narrow. And if AA executes the decomposed task, output is always better. So this is something that the method follows and it's the best practice. Sure, the next learning we had is context windows. How many of you feel that context windows should keep increasing? Nobody? Oh, some of you get it, yeah, because I think the bigger context window means, oh, I can throw more things at this and maybe it will, it'll understand more things. But our learning has been more things sometimes is not good. AI gets confused if you, I mean, that's our experience. If you give a very large code base and you ask it to make a change regardless of whatever tool, how many of you have seen it touches and changes so many files and goes to infinite and you feel like, oh, stop, stop, like, and that has been our experience as well. You have to manage context. Windows carefully. More context doesn't mean better. So we trim context. We think about what is the non-relevant information which is still sitting in the context, because you may have chatted or done an interaction one hour back, but what that interaction was, is it still relevant? Maybe not. So it's better to clear that context so AI doesn't get confused about what is it. Also thinking about when did I reset, like how much context is already there. By the way, we have a Uh, in, in Q and QO there's a slash context which shows you what is in the context. You can compress context, but I think it's important to think about every interaction you do, whatever interactions you have done so far, all of them are going unless you clear the context. So it's very important to understand, am I confusing the AI or am I simplifying it? And always, I mean, the, the ethos we have is simplify AI's job and it'll simplify your job. So, uh, it's super important. Yeah, asking to mimic uh existing code. I mean, fundamentally, the LLMs are attention mechanism is a, is a patterns uh system, right? So it looks at sequence of tokens and patterns lead to generation. And that's how it works. So it's rather accurate when I point to a source code. Here is a reference. Build another service using that as a reference. In that case, the authentication that is there in that code, your logging mechanisms, your error handling method, everything gets exactly followed on the new service also. So it's great to do it that way. Describe it by saying that build me a new service with the X authentication, Y logging and the XRF handling. Don't let's not do that. And that's inherently the case with brownfield. I'm sure you would have played AI on the brown field and you might find that it's so difficult to handle, but the idea is if you, if you have the right context built in this way, pointing A to that and to make things work, that's going to be very, very accurate. Let's move to the next learning. Uh, because we use AI, the release velocity also increases. And if you're really, really successful in this, and that has been our experience, if you're able to do, let's say one month of work in one week. Should your, and, and let's say on an average, you used to have one bug every month. Would you now have one bug every week? Because the same work is now done in 11 week, right? In, in theory, if you keep the same quality level, the, the quality standards have to increase for you to maintain the same level of, uh, level of bugs. So everything needs to change at the same time, the way we can fix it is having a very comprehensive set of unit tests and integration tests, uh, especially, it helps in two ways. One, When AI is working, it can use those tests to decide I'm going in a path which I shouldn't go, and it can course correct a lot of it itself, but it can also understand how do I know the definition of done, what are the things which I should be doing. So it's super helpful. It increases your release velocity. It also allows you to to produce more code, knowing that my tests tests are going to cover as much as possible. Yeah, again, this practice, um, semantics per token ratio, uh, as an example, if I say, if I tell you I refactor using builder pattern. That's just 4 words, and these 4 words are pretty rich in semantics. Refactor is rich in semantics. Builder pattern is great semantics. So here the semantics to token ratio is 100%, pretty much 100%. I can say the same thing in a very long way. Build me a utility to create a complex object by calling many setters, each returning that object, and finally build me a complex object. That's so many tokens, and the semantic meaning is very, very low in that, right. So part of the reason why AII struggles with large code bases is because there are so many tokens with the semantic meaning is low in that because of the boilerplate code we have. So that's another takeaway that every time we deal with the AI, whether we are instructing or whether we are building context, we need to compress. The the the semantics ratio in in that tokens are in that inputs. There are techniques to um to do that and and we can, we can, we can chat about it later. Yeah, instead of throwing code, the meaning of the code can be sometimes much more valuable. We're gonna share some examples maybe later. Let's move to the next learning. It's important to understand uh how does, uh, how was the model trained. It's, it's a, it's, it's some of this, our, our own experience in this. Uh, how many of you are, have programmed in Golang here? OK, and, and keep your hand raised. How many of you have used, uh, domain-driven design in Goleng? 12, No, nobody. OK, yeah, because what we came to a conclusion on is the languages which have come or introduced in the last few years and patterns which were much more popular in much older times are no longer popular. So while you can find a lot of code in Java using domain. In design for Golang, it doesn't exist. So asking AI to use domain driven design in Golang is not going to work. We tried it and we saw it tries to create some some things which a developer would say this is wrong. So it's important to understand what is the practical practicality of how these programming languages work, what type of design patterns apply to one versus the other, because force fitting it is not right. Yeah, I, I think this again, um, we have seen it multiple times, um, all the time consistently. Um, in the past when we were developing, you might have gone to Stack Overflow, Google to do your research, copy paste, and That's very distracting. You are out of your zone, out of your ID, doing all those things. It's very hard to be in the zone longer in the past, but it has given us this great advantage that you can pretty much get into the zone anytime you want because you are not going multiple places. You are just focused there. But then what happens is you need to have this contiguous amount of time. Um, to, to work with AI, give the right context, give a judgment, give a validation, verify the output, and it is real, real jamming happening there, right? So often what happens is because you need to cut and go for a meeting, um, as developers we lose the flow state and AI also lose the context in terms of continuity, and when we get back, it becomes an anti-pattern actually. So what we recommend is that whether it's an individual developer or teams, let's have a contiguous amount of time blocked without distraction, and that's a great boost for our productivity is what we have seen. We have seen some of our Amazon engineering team who use this have started doing no meetings in the afternoon. Of course it needs your leadership backing, but it works very well, especially because the pace at which you're working is much more different than what is without AI. Let's go to the next one. Uh, how many of you have a very, uh, like working developer environment, dev environment, where integration and everything? Raise your hand, uh, about half of you, yeah, it's pretty common in Amazon also. Not every team has, uh, a very, you know, very good dev environment, but it's very important actually, because when you're moving fast with AI, you would be releasing to dev slash integration slash uh pre-prod, whatever you call it. And you're going to test how, how does it interact with these dependencies, how does it work with this service returning some specific parameters. But if you, if you don't have those working, AI AI will go very fast, but you will not have a way to test it. You will not have a way to move it, move fast, and you will have unnecessary gatekeepers blockers where the space which you have gained will stop you. So there is some homeworks organizational change needed where everybody builds a discipline. That how do we have end to end working dev environment not just for my team but for dependencies as well and over time you have everybody feeling confident that I'm allowing my consumers to do integration testing slash dev testing and and that becomes a if that becomes a platform, your pace of release can increase a lot. Yeah, I think this is a continuation of the same also. I mean, the real reason to have well-oiled CACD pipelines is that you're going to get fast feedback from production, from staging, and all of that, and that's a very important DevOps concept. 15 years ago, 20 years ago, we know this very well, and yet what we see repeatedly is the CCD pipelines are not broken or it's not matured enough. Now in in the era, this is going to amplify the problem because the initial stages all the way to coding and unit testing is going to happen very, very fast and then your CCCD pipeline is going to block things going into production, but there is a there's a backlog building up from the development side. So when the feedback really starts to come from staging about a defect or production about something not working. Your Dove has moved multiple versions ahead because that's happening fast, so it's going to, it's going to really, really crush everything moving forward. So it's extremely important that we advise customers, save your time in, in overall productivity, but take that save time and really invest in QEA and CACD environments and make it smooth. If not once again. You'll only see a step increment in velocity, not the paradigm leap, and you can use AI to do with it as well. Yeah, absolutely. Moving to the next one, this is the most interesting one which we see from a lot of customers that AI works for green fields slash new use cases, but I already have so much code sitting in my company. Can it really work there? Can it use the, uh, the validator for email which I have already returned rather than an open source one? Can it use my existing libraries? And I would say it can. We have tried it many, many times, and it, it does. Uh, there is a, there's a big problem though as to just feeding a a big code base and pointing AI to that code base. We have seen some challenges, and this is getting better with time. At the same time we have seen how can we make. Uh, make it easier because I think I was asking this how many times you had a large code base, you're making a change, and AI goes in an infinite loop changing almost everything, and you feel all you had to do was one change and you went to like 20 files. What we have seen is to make it mix, to solve it, you have to build some semantic meaning of the code. And we have some utilities which we will share with you where you can build context about the code. So let's say if you have a large code base which doesn't even fit in the context window, you can make it work by having the semantic meaning of the code, which could be the call graphs, the classes which you have, what each function does, and AI can use it. To decide what should I load in context, how do I, let's, let's say for a shopping app and somebody's saying the order flow should change, how do I know which files really need to be changed rather than everywhere I see order I need to make some change. So we have seen all of that happen and also decomposing it is super important because if you have a large code base and if AI is working on everything. We have seen it it gets confused. So if you keep the scope narrow, it's very, very easy to make it work. So we have a workflow which we'll share which can make some of this simpler. We bake this as part of the methodology so that all the techniques of building that context from large brown fields is already already available as part of the method, and the tools will work accordingly also. Um, the, the, this one is again, I, I'm sure we're all using a lot of MCPs. Our teams are all building a whole list of it in Amazon. We have a flurry of MCP tools. Sometimes one MCP server will have 100 tools inside it. Um, unfortunately what happens in today's technology is that that takes the context away because every interaction between the agent and the LLM, the tool descriptions are also packaged inside it. Um, so that takes like 60-70% of the context away, and that's not good. It's changing, but right now it's important that you disable those MCP servers that are not needed manually. I'm sure in the future systems will be intelligent to block the unnecessary ones, but right now we've got to manage it. If not, your precious context is taken away by this, and you can't really do deep work. For those who are starting, every time you have something in context that is going as part of the API call to the LLM, so it's unnecessary wastage. Coming to the next one, a lot of people ask how should we measure whether AI is really working for us, and it's a much bigger topic, but what we have seen is traditional metrics don't work very well. I mean, you can say number of lines produced, code accepted, you can talk about the mean time to repair and several other things, but I think the best way, or at least the way which we have seen it working is you create a baseline metric which could be what does, how much time it takes from. Me as a business leader or a tech leader deciding to build something to it getting launched, and can we do an AB comparison of AI being used versus not and it gives you a very good starting point, especially when you're thinking about is this really a way to work, is it really. A better way to do work because there are still a lot of work happening in how do we measure how effective is AI and we have seen the end to end productivity gain is it removes a lot of metrics which can be gamed to make, make it work or make it not work. Yeah, rewrite fossil and patching. I think this to do with technical depths, right? So all of us are dealing with systems with technical depths that's taking your sleep away, you know, there's $2.4 trillion problem in the US alone is based on technical depths, right? In the past we had an inertia to rewrite applications to escape technical debt because rewriting is a multi-year problem, uh, and it's very serious, so we had that inertia. AI is giving us a new lease of hope that we are able to rewrite much faster. So when you deal with the situations where you are thinking of just patching, just doing upgrade, if, if you, if you have a lot of technical depths, um, please reconsider because in, in a matter of additional 2 weeks, you probably are able to rewrite and then escape from a lot of technical depths, right? Once again, methodologists like AADLC will help you to. To confidently move out, having accountability of existing rules and and the new systems and the new functions and so forth, so this is a time where we have to reconsider the old inertia. We've seen some teams building something from scratch much faster than repairing it and maybe you will find some cases like that. Go to the next learning, uh, how many of you feel that AI is like a senior engineer? It's not a senior. 01 of you, yes. Sometimes it feels like, oh, it's telling something very confidently, so yeah, it seems right, but at times it will say something very confidently, and that's wrong. And you shouldn't hesitate to question it. By the way, when you question it, it will in a very polite way say, Oh yeah, it looks like you're right, and I should change this. So I think what what we have learned is sometimes it'll go in directions where you don't want it to go. We think of it as an intern who has a lot of ideas and thoughts, but a senior engineer needs to help and Uh, and guide it. And, and you are the owner actually. I mean, as an engineer is the owner, because whether you use AI or not, your name is on the check-in. So, so it's very important to question and don't hesitate to question. And, and AI, when the question would, would work much better, then you're accepting everything it tells you. Yeah, I guess now we are in a different world. Things are changing very fast, so a lot of customers would ask us, Hey, has there been a company who has done that and experienced 20 x velocity, only then I will move. That's not the right approach. There is no gold standard yet. We all have to do it and learn and evolve. Although we have the methodology, and it's not like it's 10 years since we have the methodology, right? So we encourage customers to safely design our experiment. Um, safely design your metric and then just jump in, use the methodology and do it. And if you don't get the results, it's OK to iterate and change the ways of working and make it work, right? So there is no turning back, um, so let's not have the inertia to, to stand outside. It's about time to jump in. Yeah, and, and that is a very good segue into learning with AI requires real hands-on experience. You can keep reading books about how to code, but unless you start coding, you will not know it. So, so you can keep reading about AI or keep doing things, but unless you do it your own, you will not learn. So I would say. Uh, spend time, and you may say that, hey, I went to this talk. I learned these 5 things which are very valuable. Let me try them and maybe you'll come to your own conclusions that, oh, these 3 things were very relevant, but these 2 things I have my own own understanding. So I would highly encourage you to practice and only then a lot of these things will intern, internalize. It sometimes feels like working with AI is the same as writing code, but I, I have felt, I mean this is just an observation which we have. You will learn about AI more as you start working. You will know some of these like where it works best, what, what, like, for example, one recommendation Raja shared was, uh, give it small, small task rather than large task. Like what is small? And I think only by doing you'll know this is, this is small enough for it to do well, uh, and you will have those learnings. And the conclusion of all we shared is that ultimately an engineer or your name will be on the code. So if your name is in the code. How, how would you feel comfortable that whatever has been done, whether AI is used or not, how would I feel comfortable that I can defend this code? How can I feel comfortable that I, I will be the person getting a page in the night and knowing how to fix it? And, and I, uh, so far, I mean, I don't know about the future, but so far I think that is the best way to create something which is production grade where you understand whether you know that I would have done exactly what this, what this is. And gives you the confidence that whether it's done by AI or not, I know this is the way to do it. And the customers that practice a methodology like this where they start from the top and they verify validate every step of the way, they have a better affinity to the final code that's coming up. That that that is the barrier, the barrier breaker that lets this code to go to production, right? If you just throw a problem and let a generate everything, it does, it does not happen. That affinity confidence doesn't come in. Now that goes into um we'll cover this pretty quickly, um, a lot of questions come to us on how do we measure the effectiveness, what are even uh the outcomes. So uh we, we analyzed it and that seems to be uh a lot of outcomes that come from AI. Uh, these are pretty similar to the agile era outcomes that we expect. Um, the top outcomes that a lot of engineering leaders such as yourself and the developers expect are velocity. Quality because velocity, it doesn't matter if the quality is poor, that has to go hand in hand. Predictability is another top outcome that we expect. Again, predictability definition is that the number of sprints I commit versus the number I deliver. Before AI, it was pretty much like 20%. We commit 10 things, we only deliver 2 with AI era, the predictability rate is more than 80%. Uh, it's just the beginning. It's going to go 100% also moving forward. So those are some of the typical metrics, but these things will, will change also like Anupam said, once everybody reaches the same velocity, then we will move the metrics to business value or something else, right? For now those are the ones. And how do we really put an experiment in our organization, so we don't have to overengineer this. You take something that you have in the backlog. And then you use your original ways of estimating it, whether it's user stories or your own approach of estimating it, have that estimate done, and then do the same work with the AI with this methodology, and you'll be obviously able to compare where you are. Is it 1.5x? Is it, is it 10% or is it 10x? You'll be able to compare it. And by doing it repeatedly widely in an organization, you will average and you will know that, you know what, this is giving me 10x productivity or 11x productivity, you'll be able to conclude. So it's very simple. There's no need to overengineer this. Yeah, sure, yeah. So Raja, maybe let's move to a demo of fixing a problem. OK, let me do a very quick one, and this is, you know, usually these kind of demos are much longer, but we want to set the context in. Yeah, so we took a use case where fast API is an open source library. Which is on GitHub, and we took one of the issues which was submitted, which is can it support the query HTTP method and and this is, this is what is submitted by one of the users. We just took it and Raja is going to share, given this issue, how do we use the IDLC to approach it? Yeah, that's right. So the Foster didn't have this graph kind of facility, you know, where the output is a lot Jason, but what you want is a small portion of it, so you want to filter it. So first AI didn't have it and this was the request that came in. What we thought is let's use our methodology coupled with Quiro, our Amazon Que developer. Let's see if we can tackle that kind of a pull request, make it very fast, and deploy, right? So what we're, what you're going to see is we have distilled the ADLC methodology. Into steering files, uh, how many of you know steering files are rules? have you heard of it? OK, so basically it's a way to take an agenttic coding tool, customize it to follow our workflow, right? Without it, that agenttic coding tool might might do managed approach and whatever, but you can customize it. So these steering rules are open source. Later you will see the resources, the ideal steering rules, so we use that. And like I said, um, ADLC is a very rapid workflow. Typically it goes through many, many stages, but most of the stages are not needed. Uh, if it's a defect fix, you don't have to do all the stages. So, um, so the workflow will adapt itself. Later I will show you how that happens. Um, yeah. So this is how we start, and again, this is Amazon Q developer. It works the same for Quiro, Kiro CLI Kiro. It works the same. Um, you will notice the rules are downloaded. The Amazonque folder is where the ADLC rules are. Uh, later I will give you the resource. You can download it there. And then all you have to do is everything is set up. We have downloaded fast API code. We have cloned it, and then we're going to start giving, um, some, some instructions. Um, so that's the on the left hand side. That's what the intention is. Our intention is support this, uh, this feature request, and the URL is listed there, right? The first AP URL is listed there. That's all is needed. Now, what happens is with that as the starting point, um, because of the steering files, the custom rules we have, the system identifies that, hey, I use, I want to use ADLC process methodology to solve this. So it shows uh uh welcome, hey, I'm going to use the methodology. I'm going to follow a typical workflow like this. Uh, let's get started, right? So the first thing it does is it identifies there is something existing in the work in the workspace, and it has to reverse engineer it. That's the first step in the ADLC metrology, and that's the brown field problem Anupam talked about. You got to make semantic rich context from it, so automatically it's building um what is the structure of this application, how does the core structure look like, what is the data flow looking like, uh, and uh what are all the existing APs that are supported in it. Um, how, how is the business context looking like? Who are all the people involved? How does the whole thing work? Um, and then it will also go into a very important reverse engineering concept called, um, yeah, text stack. So you see that it uses starlet FA API uses that starlet to implement a web server, um, so it understands everything, uh, in a, in a, in a very sharp way. And then that's the library of components. What are all the important components and what functions they implement. And how these components work with each other. So that's a beautiful, rich semantic rich context. So you have now looked at hundreds of thousands of lines of code, and from there you are reduced it into a semantic rich context. From here on, AI will use this to understand. Where to make the changes, right? The biggest trouble today is that in a large code base, if you just find the code base, it is going to struggle. Where should I make this change. So it uses some grub search and all kinds of non-semantic tools, and it struggles with that. That's the biggest problem. And we're able to solve this problem by building this semantic rich context. The ADLC does it automatically. The moment it senses there is a workflow, there is a workspace, it does everything automatically. So with that context built, um, now you know it, it does, it has done in the middle panel you will see workspace detection, reverse engineering, it's all done, uh, and it's going to go into, uh, understanding the requirements a little bit deeply because, uh, like we said, like Anuppan said, AA has to give you a plan. Um, this is my understanding is what I'm going to do. So it's giving, it's for that it has to clarify with you what are the requirements. So it's asking a lot of questions. The HTP query method, what standard should I use to implement it, right? So it has a list of very, um, important questions to ask without making any assumptions, and I'm going to say, hey, use a particular standard, and I'm going to paste the URL of that. That's the URL where you can find the standard. And I'm going to answer all the rest of the questions, uh, and he is going to tell me, hey, you know, this is how I understood all your requirements. Um, shall I continue with, with all the understanding, um, so that's, that's what is happening right now, um, and, uh, you know, it is saying this is a reference I'm going to follow. That's a standard HDP query, uh, document, and your A is now. Instead of describing it, you are pointing it, right? This is, this is what works with AI very well, so it, it's going to locate the query method. It understands what needs to be built. So that's beautiful so far. Uh, it has understood existing code base. It has understood our requirements. Um, now it's going to do a plan. Now this is the context, this is the complexity of the requirement. What should I do next? Should I do functional design? Should I do um uh user stories? Um, so it goes into something called workflow workflow planning. So on the left side you notice workflow planning is happening. Um, so at that it's going to say, um, let me, let me propose to you. Should we do user stories or skip it? Should we do test testing or skip it? So it's going to do a bit of analysis. Uh, it's going through the analysis and it's going to tell me what are all the stages I should do, what are all the stages I should skip. And all of these are maintained as context in the same repository, and it's available anytime that you want. It's a bit of a tech issue. OK, so you can see in the middle panel, um, say functional design, uh, in the construction phase it's skipping it because it's a very technical requirement. You don't have to do user stories. Non-functional requirements, it's skipping it because it should be the same. Um, the query method should support the same non-functional requirement compared to anything else, um, so it's just going through, um, skipping a few and it's going to do a few. So that's the adaptiveness of the workflow. So today, a lot of tools will give you one size fits all workflow that doesn't work at all, right? Um, so that's beautiful. And uh, and next is it's going to skipped everything and going into code generation. Um, it's giving a plan, you know, exactly telling me where are all the changes it's going to make. That's most important because as Anupam pointed out, sometimes if you don't, uh, create a plan like this, AI will make 1000 changes and you don't know what's happening there. But here it's telling you this is what I'm thinking I should, I'm going to do. Can you verify and confirm? Um, so I confirmed this and then right after that, beautiful, beautiful code comes up with single iteration, it's working magically very well, um, so it's, it's going to exactly follow the same steps, um, and then the the code will come. So it's a little bit of a glitch happening but it's OK. So you, you can see uh it follows um the, the existing framework. It uses starlet that is already in the source code and and beautiful amount of code comes in and um and then you, you can see the code um. You notice there fast AP with starlet inheritance and it's, it's all perfectly working very well first time itself. So in the end it's going to present to me. I did all of that. I completed the code. I, I did the test. We're ready for operations and there we go. So in a matter of a few hours we were able to release a pull request by fixing, testing it, and that's magical and, and we have great confidence that the code is working also because we have seen, validated, verified every step of it. Now, that's available for you. Uh, you should be able to practice it. Later we will share um some of the, the resources. Yeah, no, that's an amazing demo, Raja, especially on how much can be achieved while understanding everything which is happening. And this is just a simple use case, but you can think of any complex complexity. We have seen a large, uh, very, very large code bases on which we have used a similar approach, and it works very well. We have converted PHP code to Golang for efficiency purposes. We have seen new applications built. We have seen new features built on existing applications. Uh, so we're looking forward to see how you use and what you share with us. So I'll share some resources on what you can do next. Um, we have a self-help, uh, learning resource, uh, which is a set of videos which you can watch to go a bit deeper than what we could go in the time which we have today. So feel free to share it, feel free to, uh, use it to learn further, uh, share with colleagues of yours if you think this is of value. Uh, the second thing which I want to share is a workshop. We have built a workshop where you have, you can practice. Um, what is special about this workshop? Yeah, I mean, this is, uh, it gives you access to kiro. It gives you access to instructions, uh, and we are beginning to pack industry use cases, right? Um, so you can build multiple industry use cases by following the instructions using the ADLC methodology. But it, it's just you're following the instructions to get oriented to. So it's, it's, uh, it's available, um, you should be able to play with it. Yep. The third one we have is the AIDLC workflow. This is an open source workflow. So whatever we're showing here is open source. So you can go to this, you can play with it, and feel free to share recommendations that, oh, this can be made better by this, or this is what I have. Tried and so we want to make sure as a community we make this better. We want to make sure we all learn from each other and we create a great way of working and using AI in a much more predictable way, a much more way in which it delivers the real outcomes. This is an alternative to Quiro's spectrum and development also. In ADLC, it doesn't disturb Spectrum. It takes a completely different path, ADLC path, and as, as we pointed out, if you are building collaboratively building software with making a lot of hundreds of decisions in a large enterprise context, um, production grade applications, ADLC path will take you through that, so you can play it with Quiro straight away by planting this into it. And I'll quickly share like how do we work with companies and what have we seen across these companies which I talked about. Generally our journey starts with a leader saying, hey, I feel like a lot can be done in space. I've already tried several things. I'm not seeing the outcome, so that is the executive backing, the feeling like this should be tried. When they try it, they see signature wins where they see the work which anyway my team was going to take a lot of time on. Can be done in much lower time which leads to wider trials where they will try this in multiple teams. They will see what this is, this is something which really delivers, and then we see people customize it. Some teams have created new roles. Some folks in Japan have created AIDLC Master as a role. Scrummaster converted into that. We have also seen some teams have created separate teams to spread this, so a lot of customization. Happening. Some of them are happening in terms of the workflow itself. Some of them have created MCP connectors to different tools which they have. So looking forward to see how you use it and what type of customizations you will do, but ultimately it leads to better customer experience and as a company you can grow faster because you can react to customer needs faster. You can react to business needs faster. And how do we bring it to customers? So we, we have an internal framework called Unicorn Gym where we go to a, we, we used to use this framework for learning internally. Unicorns are special employees of Amazon, and gym is where we practice our skills. So we modified that, uh, where we go to our customers and say, if you have a problem which you're already working on, uh, some of our engineers can work with your team. And solve that same problem together to deliver the outcomes which we talked about. So, it becomes much more tangible. Uh, we, we are happy to work with some of the companies. Uh, if you're interested, please feel free to contact us and we will, uh, we'll help you there. Uh, yeah, yeah, sure, I mean, uh, now we, we know that methodologies like ADLC are evolving very fast. Uh, there are a lot of still there are a lot of open questions what roles are merging with what roles, uh, you know, how big should a development team be? There's a lot of things that we need to still build perspectives on. So what AWS has done is to create something called the ANA to builders community where thought leaders can come in. And bring all the lessons and cross pollinate, discuss about it, and then talk about what what should be the native development manifesto, you know, what should be the be the roles of the future. So we are going to collect all this knowledge and centrally process it and release it. That's an open community that's created. So if you are a thought leader, you have perspective staffer, experienced staffer, please write to me. You will see our emails later and and we will be able to process that and let's work as a group and define. The future of we feel very passionate. Hopefully it was visible in our expressions. We feel that the world is going to change and we can play a leading role in helping change the world in the way which is good for folks, but also thinking about how do we work together, what are the learnings which we are having together, so we don't. In isolated ways, we bring all these learnings together. So looking forward to great interactions, looking forward to some of you taking these learnings to your companies, looking forward to some of you sharing your learnings with us, and that's how we become better. So thank you so much for taking this time. I know you had to wait in line, so apologies for that, but looking forward to some awesome work which you do. Thank you so much for coming today.