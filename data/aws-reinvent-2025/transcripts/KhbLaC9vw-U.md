---
video_id: KhbLaC9vw-U
video_url: https://www.youtube.com/watch?v=KhbLaC9vw-U
is_generated: False
is_translatable: True
---

So in the talk today, what we're going to do is look at how AI can help us become more effective at this troubleshooting, right? How we can restore service to our customers quicker, how we can uh find the root cause of a of a problem and then mitigate the problem easier and faster. Now this is a level 300 code talk you're attending today, so which means we're going to show you a lot of hands-on things using Amazon Open Search Service. We're going to show you code, but we're not necessarily going into the level 400 of, um, you know, explaining every single line of what we're doing. It's more meant to be an inspirational session of what you can do in this area and how you can achieve it using open search. Um, we, um, we have prepared, uh, a, um, a thorough guide for you that will, uh, link to in the end. It's a GitHub gist which will contain all of the references, all of the code snippets, and all of the material that you will need to dive deeper on this yourself afterwards. So there's no need for you to, uh, take photographs or, uh, note stuff down and, and research on the fly and so on. We'll all reference that afterwards, OK? So, uh, my name is Ulli. I'm a solutions architect, uh, based in Berlin, Germany, and my, uh, primary job is to work with software companies running SAS on AWS, um, but I'm also part of the open search community here at AWS, and today I'm very happy to be, um, accompanied by Ruben. Hi everyone, Ruben Jimenez, um, also a senior solutions architect, um, and dedicated to the, um, community of cloud ops, more of the observability, more of the optimization perspective, and very similar to Yuli, um, dealing with SAS customers and SAS customers at scale, so. So before we go into the hands on part, let me give you a brief tour of the demo application that we've developed for this talk. So when we talk about uh observability and we do a uh a hands on demo for that, we need some application that generates our observability data. And what we've done here for you to have as a background is, uh, we've basically taken the open telemetry demo application, um, and modified that. So the open telemetry demo application, uh, if you don't know it, it's an open source project you can go and check it out on GitHub, uh, and it's basically a web shop where you can buy telescopes, binoculars, and such things, and this webshop is backed by a series of different microservices that are all performing different tasks such as uh shipping, such as billing, check out, um, and so on and so forth, accounting and different microservices you'll you'll see later on. Now the main modification we did to that application is to make it into a multi-tenant SAS application. So what we're pretending here is that we're a SAS provider that offers these webshops to to different uh webshop providers um in a multi-tenant fashion. And what we did for that is we duplicated this demo application across our different tenants and when you see these animal names, uh, this is our tenant identifiers, right? So we have 10 of those and then we also extracted some shared services from these tenants, uh, which is a pattern that we often see in multi-tenant applications, right? So, uh, some things are run locally for each tenant, but then we also have shared services that are running things that all of our tenants have access to. Now this demo application is deployed into Kubernetes and Amazon EKS, and there is another component to be aware of which is this open telemetry collector component and the application, the um single uh so the different tenant applications and also the shared services export their observability data, so logs, metrics, and traces into this open telemetry collector. And this open telemetry collector then forwards this data into Amazon Open Search Service. Um, and if you're not super familiar with, um, OpenSearch and Amazon Open Search Service, the kind of the main database or the, the thing that is storing the data in OpenSearch is called an index, and then in open search service we also have this component called Open search ingestion, which is kind of an ETL tool, um, aimed at open search, right? So open search ingestion takes the data from Open telemetry Collector, transforms it, and puts it into Open search. And then last but not least, we have open search dashboards, which is an exploration and visualization component in the open search ecosystem that we're going to use to look at our data. Now what you see here is a very classic architecture for observability, right, so many AWS customers running their applications in Amazon EKS and running their observability using OpenSearch have such an architecture. So this is nothing new. What we want to talk about today is the addition of AI to this whole setup, and we're going to cover three different AI use cases again with the goal of helping us to speed up the search for the root cause of an issue. The first thing that we'll talk about is natural language query query generation in open search dashboards, which will help us um uh query our log data quicker so that we can get to the root of the problem quicker. Secondly, we'll talk about semantic log search which will enable us to find um pieces of log data that we don't know how they look so maybe you remember that if you look for a specific log line you have to kind of know how that log line looks. You have to know the specific words that are mentioned in the log line to actually be able to surface that, but sometimes you just don't know really what you're looking for, and this is what the second section will be about. And last but not least we're going to tie everything together using MCP or model context protocol where we have an AI agent in our case Kiro CLI to um do a semantic search on our log data and uh do natural language interaction for us to not only troubleshoot or find the root cause but also mitigate the issue that is uh underlying. So let me switch over to our uh demo environment here. And I brought up this uh same architecture that I just showed you here on the left, so this should be familiar. This is our hotel demo demo application running in a multi-tenant fashion. And I just want to quickly show you how this looks on a technical level. So, um, if you don't know Kubernetes very much, uh, CBCTL is the standard tooling to interact with, uh, with Kubernetes and Amazon EKS. So what we can do here is we can say, uh, CUCTL get name spaces. And then we'll see exactly what you see here on the right, uh, on the left, sorry. Uh, we have, uh, our different animals which are our tenants. We have this uh shared name space down here. And we also have our open telemetry collector name space up here. So this is all currently deployed in Amazon EKS and running. And we can also have a deeper look into these name spaces so we can say uh CubeCa get pods and then take one of our tenants, let's take the Falcon tenant and just have a look at what's running in there and you can see there are these different microservices that are powering this webshop that I talked about. So you see here things like uh the card and the the the product catalog and so on and so forth. Uh, same thing with our shared services. We can have a look at those as well. And you see here the shared services that all of our tenants are centrally accessing. Now you see here that I mean here we see an error but this has already been there for 44 days so it's nothing going on right now. Everything else seems to be running smoothly, right? Um, no, no big issues to see so far. But unfortunately not everything is good. In fact, just before our session we got a call from our CEO and he's really upset because our customers are really upset because actually the web shops are not working like any of our customers' webshops are not working at the moment. And he said that people are able to shop on the web shops like they're able to put things into their cars and so on, but they're not actually able to purchase anything which of course is is bad for for any web shop, right? So we should uh go and fix that uh ASAP. So as a first step in troubleshooting this, uh, I'll do what, uh, probably an ops person would do as well and try to reproduce or, um, uh, yeah, try to reproduce the problem on my own system, right? And the way I do this is, uh, using QubCTL port forward, um, so this is basically a tool to, uh, bring an application that is running in Kubernetes to my local host to be able to access it in a browser. So I'll do that again using the Falcon tenant and now I can access this demo application from the Falcon tenant at my port 8080 of my local host. So let's go ahead and do that. Local host 8080. And this is the webshop I was talking about so this is a kind of the classic hotel demo application and down here you have different products at your disposal that you can purchase. Let's take this telescope here. This looks good and then uh let's scroll down here to place the order. And Nothing happens, right? I click this. Nothing actually happens. Let's open the deaf console of the browser to see what's going on in terms of network activity here. We see that there's a call to check out. And this is still pending, so this is taking a long time so let's see if that comes back. Uh, it's actually timed out now with a 504, right? So there seems to be something wrong 504 if you remember, it's a, it's a gateway timeout HTP error, right? So something is timing out somewhere, but in a distributed system like this, it's quite hard to find the root cause of this now. So maybe Ruben, we can have a look at open search dashboards to figure out what the problem is. Awesome, thank you Gilli. So as Julie mentioned, um, what we can do is we can go into the console or go into the open search service, um, open dashboards, um, as you can see it's an open search UI or dashboards that specifically have connectivity into our clusters or a multi-tenant cluster. What we have here is an application or endpoint that we've configured. And what we'll do is we're going ahead and launch um this dashboard here we have our particular cluster we can go ahead and click on that. Can you zoom in a bit? Um, I think it's, uh, a bit, yeah. How are we doing there? Good. Maybe a little too much so what we have is we have the logs in terms of the the multi cluster showing up here. So what we have is we have kind of the look and feel for most of you that actually have been working with OpenSearch or open search dashboards. This should look very familiar to you really kinda have the histogram here really kinda showing the activity of those logs that are actually put into that particular um. Um, index. I'll shrink it just a little bit so we can get down here. And then from there what we're really kind of looking at is um how do we then take it and um really kind of look at it from signal to noise right? so we have a lot of logs that come through or that are there um and how do we how do we how do we make it. More palatable or or how do we really kind of start to search for what is that root cause or what it, how can we start, you know, looking for that needle in the haystack so if we were to just kind of peruse it here which we all kind of know and, and, and work with on a daily basis, especially from an ops perspective, kind of looking down and it's really kind of a lot of, a lot of information, right? Nothing that we can really index on or really anything that we can kind of put our finger on right in terms of that. So what we wanna do here and you kind of ask yourself, well I could just go ahead and I could do a query. I could put it together and I could kind of figure it out that might take a lot of time, right? And especially if you haven't done it in a while, it's like oh my goodness, where do I start? So what we wanna do is then kind of ask ourselves could we use AI to actually help us do that, right? Could we use AI to give us a little helping hand in order to kind of figure that out or what that looks like? So what we're gonna do now is we're gonna go ahead and put in a prompt. And then what we'll try to do is we'll try to narrow it down, right? Kind of narrow the logs down, kind of narrow overall what's happening or what's going on. So what I have here is I, I kinda have a cheat sheet from here, right, in terms of that. So what I'll do is I'll pop into here, I'll copy this, and this is really the prompt that I'm putting in for AI. So what I'll do is I'll pop it into here. And what I'm actually mentioning or what I'm actually bringing attention to is how can we, you know, how can we, we narrow it or, you know, get to the get to the cause. In the past I've, I've worked with this before and you know since I've been familiar with the infrastructure and things like that and working with teams I'm able to say you know this is really kind of um barks like a dog and and kinda acts like in terms of an exceeded rate limit, right? And so you know we've kind of had this in the past and I've kind of seen it right? so I'm, I'm heading towards that direction and see if I can get confirmation or not overall for that, right? So I'll go ahead and hit this prompt here. And so what's nice about AI as well, so not only did it actually narrow it down a little bit for me, right, in terms of the logs and things like that that are happening, what I'll do is I'll open it up a bit. Can kinda see a little bit of the instrumentation that went on here, right, looking for different things, um, ah, and it actually picked it up, right? rate limit exceeded and it looks like it's actually doing kind of the ship order right or the ship order maybe maybe being affected or maybe something going on with that in terms of um could cause some some problems with the other tenants right overall or it could be a tenant that's over oversubscribed and actually causing that shared service to have issues. What it also did for us, it actually went ahead normally, as I mentioned before, is we could do a PPL, right? And it actually did that for us. So not only did it actually do an AI or give us kind of some recommendations, but it also gave us a PPO that if we weren't familiar with really kind of how to craft that or do that, it gives it to us right there and then, right? So we can have a reference that we could actually do it and run it so it's very similar to the different PPOs that you would run normally in your operations. So if we go to the ice summary too, which gives us a little bit of help here. Uh, taking a bit to Generate so again it's saying hey we did a sample of this and it looks like it's the uh the shipping rate limiter right or shared name space that could be the smoking gun, right? So it gives us a little bit more and it says hey look it could actually affect or or affect these downstream processes right or these other shared services because of high traffic and not be able to process it overall. So we're now kind of formulating a picture of what does that look like now we're starting to really kind of hone in on what is that needle in the haystack, um, overall. So really we, we could do this and, and have some confirmation here, but what we wanna do is really kind of just look at, look at this, and you know, let's go through even kind of some visualization, right? I find it pretty kind of kick that through, but what I wanna do is I wanna kind of show you how this works. So what we can do is we can actually do an NOP also for a visualization. And so again this is kind of yeah it's kind of not really nice in terms of where it actually shows it and things like that I think it might be an upgrade or or not that's needed but what we're gonna do is we're gonna put in a natural response here and we're gonna see where that where that ends up or what kind of visualization that will confirm you know is our shared services having a problem with other tenants, right? So what I'll do is I'll come back here. And what I'm gonna do for this visualization here is I'm gonna log the activity, right? Give me the activity of these particular tenants and show me if there's a problem or there could potentially be a problem. And again, I like to think of it as a picture is worth 1000 words, and you can really kind of then see what's happening or what's going on. So as we can see here it actually generated a particular visualization that we can kind of see it so you can see the eagle tenant is really, you know, really, really busy, really trying to process, and you can see that the other services are really competing, right? So what we can really kind of index on that or really kind of just make that leap a bit is a noisy neighbor problem and a lot of us in terms of EKS clusters and the operational piece. You know how many times do we actually run in the noisy neighbor problem right in terms of that, especially in the multi-tenancy environment so now we're starting to say we really have this, this, this noise that's out there in terms of logs, but now we're able to really kind of make that funnel a little more finite, and then we have some answers or we have some, some areas that we can actually, uh, start to dive deep in. And Yuli mentioned these are kind of just a couple ways to go about, you know, how do we, how do we make a semblance out of chaos in terms of where we can start moving and how we can start troubleshooting, and it gives us a little bit of foray into the AI perspective. So I'll hand it over to Yuli at this point and then we'll go a little deeper in terms of the next kind of mechanism or kind of tool in our toolbox to do the semantic search and then really, really hone in on what could be the issue or what's going on with the actual multi-tenant clusters. Y, pass it over to you. So just to summarize this piece we've now figured out both the uh issue that is going on right there's a rate limiting problem in our shipping service in our shared name space and this is caused by a noisy neighbor problem caused by our eagle tenant, right? So it's apparently they're doing a big sale or something overloading our system and affecting everyone else. So, um, in, uh in this next section on semantic search I wanna take a step back so you, um, saw that Ruben in the very beginning. Uh, put in this prompt like is there a lock that is mentioning rate limit exceeded, right, which basically tells you that he kind of already knew what he was looking for, right? He said like he had this problem in the past and he knew a bit what to, what to dig after. But in lots of cases you don't really know, right? There's something weird going on in your system and you don't really know like there's this 5:04 gateway timeout and maybe you've never seen that before, right? So you may be uh completely blank and in this case semantic lock search could be of, uh, of good use. Now for a semantic log search I will pull up this uh architecture here um give me a brief show of hands who of you knows in general what semantic search is? OK, so that's like, uh, a third of the room. So let me briefly explain in general semantic search. So semantic search is kind of, um, uh, an evolution of search of a classic lexical search, right? So lexical search, what we basically do there, we compare words with with each other. We have a user query and we compare this user query word by word with what we have in our database and in this case it's log data, but it could be other types of data as well. For semantic search we're evolving this a bit and we're making search available also by meaning, which means that we're not only looking at the words that the user put in, but we're looking at the meaning of what the user put in and comparing that to the meaning of things that we have in our database. And semantic Search is powered heavily by these vector embedding models, and we have a couple of them running on Amazon Bedrock and for this particular purpose we chose the most cost effective one that we have there, which is Amazon Titan text embeddings V2. Uh, and I'll show you a bit how, how the output of those models looks, uh, later on. But for uh for log semantic log search we have to consider uh two let's say specifics about log data versus other types of search, and the first specific is that we have log data is typically very large in volume, right? So it's not just a product catalog where we have a couple of 1000 of entries maybe. But even in our little demo application we're ingesting thousands of log lines per minute already and of course in production systems it will be orders of magnitude more than that so we cannot just go and run every single log line against this embedding model because it's an AI model and this will like uh stack up quite a bill right if we do that for every single log line. So what we have to do instead is we need to work with a subset of our logs and just do embeddings for those uh the subset, and we're doing that using a technique called sampling and and I'll dive a bit into that. Now, uh, what's good on the other hand with the log lines is that log lines are typically semantically not very different from each other, right? So when you have, when you look at a uh at one specific microservices, uh, microservice and the log lines that this microservice is producing, it's typically repeating itself over and over again, right? It's the same log lines maybe with different IDs, maybe with different numbers. Numbers like the time something took or something, but this all doesn't change anything semantically like it doesn't change the meaning of the log line, right? It's it's it's only like a different, a different identifier or so which doesn't change anything in terms of semantics so we're good to do this sampling we we shouldn't miss out on too much information because it's the same things repeating themselves all the time anyway, right? So we we're good to do this sort of technique here. So let me first show you how we actually achieve this uh sampling uh and for that let's dive into uh open search ingestion which I mentioned mentioned in the beginning which is this ETL tool that we have available in the open search context. Now let's dive into the logs ingestion pipeline, which takes logs from our open telemetry collector, transforms them and puts them into open search. Now, um, the. The uh log data that we've been working with so far was our raw log data and you see that we have a little sub pipeline here that is basically just putting every single log line into open search and what Ruben has showed you before was working with with this raw data. But we have a different another subpipeline here which we call the sampled subpipeline and we, when we look at this definition down here, uh, we see that we've introduced this rate limiter in here and this rate limiter now does the sampling for us so it takes only 10 events per 2nd, 10 log lines per second per service. And we'll drop the rest, which basically means we have now a good kind of little selection of log lines from each and every single service that we have in our system. And then what we're doing with those sampled logs is we're ingesting them into a new index in open search called sampled logs. So we now have two indexes, we have the main one that stores all of our logs, and then we have the sampled logs that don't just contains a subset. But with the the the subset of of logs we're not able to do semantic search yet, right? We need to do some configuration on the open search site to be actually able to do this kind of semantic search. So let's do that together. The way I'll do that is using the open search dev tools and if you're not familiar with open search so much it's basically um a very uh lightweight developer environment where you have HTTP requests on the left hand side and the result of those requests on the right hand side and we can just kind of fire off HTP requests right and see what they come back with. Now with setting up semantic Search there are 3 steps involved and we're gonna walk through them one by one. The first one is we need to connect Amazon OpenSearch Service to Amazon Bedrock because Amazon Bedrock hosts the actual embedding model that we need, uh, as I said before, and OpenSearch will be the database and it will also do the communication to Bedrock on our behalf. Now the requests I'll fire off here are a pretty boilerplate. I'm not going to dive too much into details here. What we first need to create in OpenSearch is a is a connector to Amazon Bedrock, and we will specify the model here that we want to use in Bedrock. And then we're gonna take this connector ID that OpenSearch generated and put it into our register call here, but we also need to create this uh so-called model group and we'll also take that ID and put it into our register call. I said, as I said, I'm going, going through that a bit quicker. It's all in the GitHub just if you want to follow up a bit more in detail. Now when registering the model we get this model ID which we'll be using for the remainder of this section. We need to deploy this model first so it's known throughout our open search domain and then we can actually go and predict, which means we're testing this model now. And the cool thing is we're now talking still to the open search API, right, so through the open search API. This will call Amazon Bedrock under the hood and give us a response again through the Open search API, so the two systems are now connected with each other. And this embedding model, if you've never seen this before, comes back with this long list of floating point numbers, and this is the output that an embedding model produces, and this in a mathematical way encodes the meaning of what I put in. So I put in here what is the meaning of life, and this question is now encoded into this long list of floating point numbers which is our vector embedding. Cool, so we have established the connection now, but our sampled logs index is still not able to do semantic search, so we need to configure the sampled logs index to perform this embedding automatically on our behalf to be able to do the semantic search. So let's go ahead and configure this index. When I look at the index so far and I'm just kind of spitting out the definition of this index here, um, we don't find anything in terms of embedding in here, right? It's not in there, it's just containing all of our like plain lock lines. What I would do first is create a so-called ingest pipeline, which is a configuration object that will enable OpenSearch to automatically embed uh log lines that are flowing into the index. So I'm creating this embedding, uh, this ingest pipeline. And what this does is, it's just processing all of the uh the log lines that are flowing in, taking the log body. And producing a new field called body embedding that will store this floating point-like array of floating point numbers that we saw before, which is the vector embedding. And then we are going to create an index template for our sampled logs which contains this pipeline, so basically now saying please prepare this index for automatically embedding all of the log lines that are flowing in and you see here that I also specified this new body embedding field which is a vector field in OpenSearch. And what I'll then do is I'll just delete the sample logs index, and what's happening now under the hood is that open search ingestion will continue to ingest data into this index and open search, since the index does not exist, it will recreate it automatically applying the configuration that we've set up here. So now the index will be recreated using all this embedding pipeline and the new embedding field. So let's have a look at this sample logs index now, it's not yet found, so apparently open search ingestion has not ingested new log lines yet. Let me repeat that a few times. Should usually take just a few seconds, but sometimes it can take a bit more. And there it is back up and when I now search for embedding, we have this body embedding field here that uh is now part of my index definition. And when I go down here, I also have this pipeline as part of my index setting. So what's now happening exactly what I wanted, all of the new log lines that are flowing in are automatically embedded by OpenSearch using Amazon Bedrock. And this is the last piece of set up I needed to do. I can now perform semantic search on this index. Now we've been talking or I've been talking so much about setting this up that it's worth kind of recapping what the value of this is actually. So let's take a step back. Remember we have this 504 gateway timeout. I know that when there's a 504 gateway timeout. It indicates that something just takes very long to process, right, kind of that's what I know, right? I don't know anything of rate limiting. I don't know any exceeded or acute or anything. I don't know how the specific log line would look, but I have, um, an idea that something is just taking very long to process. So let's do that. Let's search so I can put in something very vague here. I can ask for something like uh something is taking too long, and I can do a semantic search for that to find log lines that are semantically relevant for this query. So let's have a look what it came back with. And it's exactly again the shared shipping service that we saw before, right? And it's found the log line rate limit exceeded request queued, so you see that lexically those things are not related, right? Some is taking too long is not contained anywhere in this log line, but in meaning in semantics they're very similar to each other. So I've now gone from a kind of a blank page problem. I have no idea what to even look for. I've gone to the log entry that is uh that is going into the right direction, right? So now I can have a deeper look at my shipping service to figure out what exactly that problem could be. OK, so having finished our semantic search, let's, um, let's do the 3rd part of the session and do everything together, natural language processing and semantic search, bring it all together using moderate context protocol or MCP. Cool, thank you, Yuling. So as Julie had mentioned, so it's really kind of the latest and greatest, right? So how can we have an agent, um, process these particular searches for us really kind of. Um, with a prompt, right, how can we use that same NOP or that prompt to get information back, right? How can we use it as a tool in order to, uh, interrogate, um, that particular open search service, right? And as we are moving into kind of that agentic framework, and I'm sure you're hearing throughout the, the, the conference, what is that agentic enterprise or what is that agentic framework look like, right? So we have standalone, um, agents, we have, um, autonomous agents and things like that, right? So what we're doing is we're gonna use Qiro CLI as our as our client. What we're gonna do is we're going to talk to you or interact with an MCP server that we've actually went ahead and and built out, um, in terms of Python. And then from there it actually has all of the scaffolding that Yuli had talked about in terms of being able to use the model, use the embeddings, and then go ahead and, and use that reasoning, and then bring back a particular response, um, and this was all kind of connective tissue, um, kind of leading up to this or, or delivering this piece overall. What I'm gonna do now is I'm gonna go into the configuration file in terms of the actual MCP server itself. But again, in the gist itself it has really kind of that that scaffolding of how you would set it up, how you would configure it in Kiro CLI, so you know if you're kind of wanting to dive a little bit deeper in those settings of that configuration, it's in the gist. So we'll share it up there, um, and you can go ahead and look at it and try it out or kick the tires on it. I'm a huge fan of Clear if nobody's noticed. Ah. So I'm gonna just go into the file itself in terms of the um where the actual downloads is or where. Where we have that MCP server. It's kind of painful for you guys to watch me type here. Goodness gracious. Let's see here. Uh, it doesn't say it's there. Sorry guys on that one. We'll pop in here. Just add a main main dot pi in the end. I'm a cat, not a CD. Oh, that's right. There we go, perfect. So what we have here is just kind of the set up, uh, in terms of where we're setting this up or actually the main dot pie itself. So what we have is we have our, our really kind of our import vote to 3 client right from there we're actually referencing the fast MCP for that particular component. Then we're walking down and kind of the open search connection. There's documentation that that's out there that really kind of brings this to bear or you can actually look at and reference. So what we have here is we have the host, which is the actual uh search or it's actually the open search endpoint that we're gonna interrogate or hit. We have the region that's there as well. Um, services, yes, of course, credentials we use the session and things like that which is in the AWBS account so you can access it, um, use that SIGV4 access and, and, and go ahead and being able to, um, uh, get that information or query that information. Then what we have is we use the open search client here based on the Python piece overall in terms of that client, right? So it's actually calling it, it's using the host and using the variables above in order to make that connection or hit that end point. Then what we have here is we're actually describing the MCP server itself. Uh, fast MCP seems to be, um, within Python the best way to go ahead and do that, uh, that query, right? Or do that interrogation of that endpoint, um, and then from here if we really look at it, it really just kind of boil it down. Is we can just put in a few lines of code um for that MCP server to actually do a really dynamic fetch of that particular model or that embedding model that Yuli actually set up right initially so there really isn't a whole lot of work we already have the plumbing that can actually make that connectivity or talk to that end point and what we have here is we actually have just a stanza here in terms of MCP tooling. That we can say hey look we want you to do that semantic search and we want you to go ahead and give this that body information coming back and as Julie displayed before we actually have that embedding that will actually return a response based on that very uh finite query on those 10 logs, right? Um, and then we also have the model ID so dynamically we're actually pulling in that that model ID so that we can use it and leverage it within the semantic search. And again, as you referenced before, we have the sample logs here that we could actually leverage and utilize. So what I'm gonna now do is I'm gonna go ahead and kick off our client right? um here the MCP client from there so we'll clear that out. So what this is doing is it's actually going into the MCP. JSON file or settings file, and what it's doing is it's setting it up, right? So it's reading all of the MCP servers that I have configured already and so one of them is the OS logs or it's the main pie, right, that's actually coming through. And as you can see it went ahead and welcomed and it actually loaded it successfully. You can actually go in and type MCP itself and you can validate that I actually did load or it did actually come into your environment or within your MCP client through Quiro. So what I'm gonna now do is I'm gonna just type in a a a particular uh input or an NOP input and see what I actually get right um from there so I'm gonna come down, um, my cheat sheet one more time here. What I'm gonna do at that point is ask it. Oh goodness gracious, no no no no no no. And I'm going back to a little bit to what I know in terms of the exceeded rate limits. Yuli said just sight unseen we can actually get it, but again, I wanna see what we're gonna get back with a very um targeted um NOP or or prompt. So again on everything here we'll go ask, do we really trust it or do we want it to do what we say it wants to do so we'll push it through here again. And here we go. So it's actually showing us here it found a 500 rate limit logs from the shared shipping service. So through the whole talk, you know, we've kind of been, you know, dancing around. OK, how do we really kind of, um, close the funnel or actually start to hone in on the particular root cause, and then we see it again, right? So it's more or less, it's, it's a problem with the shared shipping, um, and it's actually a noisy neighbor issue, uh, and causing that component altogether. So we kind of have confirmation at this point. So what we also wanna do is how do we remediate this right? so we can just ask it directly, um, remediate issue for me. And so Kiiro and and really kind of the whole AI component here will go ahead and just kinda give me suggestions on what they can do in order to remediate this right again you know if I didn't really know or understand what was actually going on um. With that piece, um, it would actually show me exactly what could be, what could be going on in, in that so what it's seemed really at this point, let's see. So it's actually scaling itself right? so it's actually go ahead and it's actually scaling this, uh, shipping, right? So it's actually going through and it's scaling it for me, um, overall so you know it actually went ahead and looked and says hey the recommendation is scaling that that that shipping shared service then if we go ahead and scale it. Will it go ahead and remediate the problem? So it went ahead and gave us that, or it kicked off for us, right? And we went ahead and said, OK, trust it, go ahead and do it, right? Very similar. Like, are you sure you wanna delete? Kind of the same thing, right? You wanna make sure that that you're adhering to it and, and kind of responding to it. So basically it actually scaled, so it actually gives you actions here, scaled shipping deployment from 1 to 5 replicas, right? And it did a few other things there. Um, so this is kind of what it fixed here distributions, and it gives you kind of uh an overview of what it did and what it didn't do at that point. So what we're gonna do at this point is we're gonna go back into the application to see if it actually did did prove to be successful or not, right? So, um, what we'll do is we'll go back out, um, on our command line and we'll see, um, what actually happened, if anything, so we'll quit, uh, that piece. You know, forward again, so we'll just take a look at the shared, uh, services real quick and see if it actually did. It did actually scale it so we can say yes, that actually happened and Kiiro did what we really kind of recommended it could do to solve that problem. That we wanna do is come over here we wanna port forward so as Julie had uh kicked off before we just port forward here. On one of these front ends and then we'll go and test the application to see if that application is functioning the way we wanted it to. So we'll go back here to the hotel, we'll go ahead and exit out of that, and we'll continue shopping. We'll come back in and we'll actually choose this particular product, right? We'll just kind of stay with what we wanted to before, um, of course our, our, you know, nephew wanted this particular thing, so let's see if we can actually complete it or get it done. So we'll add it to the cart as we did before. We'll walk it down here. Everything else is pretty much the same, and we'll see if we can actually place that order boom, successful. So now we've really kind of have full circle, right? So we actually went through and we looked at it from NLP, really kind of narrowed it. We went from the semantic search, not really understanding, um, what could be the issue, but then we're actually homing in more and more in terms of what the needle in the haystack would be. And then we used MCP. It's kind of an external or an agent tool to actually give us that same confirmation to actually fix it and, and move forward. So, um, with that, I'll go ahead and hand it back over to Yuli to take us home. So this uh already brings us to the end of the session. This is the GitHub gist we uh promised a few times during the session so do uh check this out uh it contains all of the code snippets, documentation references, and other references, explanations, and so on that we talked about during this session. Um, let's have a look at the time. So I think we have time for one or two spontaneous questions. So if any one of you has like, uh, wants to have a question for the room that we can answer, uh, you can do that now if you wanna raise your hand. Anyone here? OK, yeah. Uh-huh, um, is it the This is. OK, so just repeating the question for everyone. The, the question was for the connection between open search and Bedrock, whether we can have a cross region connection for that. And yes, in fact you can, yeah, you can, you can do that, uh, in our case we did it same region but you can have it cross region as well. Basically how it works is that, um, open search is assuming a role on your behalf and just, um, calling the Bedrock service, uh, end point on your behalf, yeah. Any other questions from the op side of the house? Go ahead. Just a question when you're demoing all those tools and I was showing you what part was that tools? Is that part of open? Yes, it's part is a standard part of open search dashboards. Um, so, uh, open search, uh, is fully HTTP API based, right? Uh, so, uh, in contrast to other search tools that are out there, it's fully HDP API, and this is, uh, kind of the. Main kind of playground that that people use in open search right to to play with different uh play with different end points yeah so this is not nothing new this has been around for pretty much ever yeah. Cool. OK, so, uh, with that, let's, uh, close off the session. Uh, thanks a lot for spending your valuable time with us. Uh, we can take further questions in the hallway, by the way, we just need to move out in a bit for the next speakers. Uh, happy to, to take any questions, um, offline. Um, please do fill out the session survey. I know everyone reminds you of that. Uh, please do for this session as well, especially if you liked it. We read every single piece of feedback that you put in there, so please, uh, do fill that out. Um, have a great rest of reinvent, um, enjoy the conference and hopefully see you all soon. Thanks folks.