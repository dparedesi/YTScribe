---
video_id: KZapt0fQA8k
video_url: https://www.youtube.com/watch?v=KZapt0fQA8k
summary: "This session, \"Context engineering and building better agents\" (AIM345), features Shirini Pendala from AWS, Udai Predi from Elastic, and Rotten from Tavili, delivering a masterclass on building enterprise-grade Agentic AI applications. Pendala opens by defining AI agents as autonomous systems capable of reasoning, planning, and acting, but emphasizes that their true power lies in \"context engineering\"—providing the necessary business-specific data and guardrails. He introduces \"Strands Agents,\" a new open-source, Python-based SDK from AWS designed to simplify agent creation into a single line of code that integrates a task (system prompt), tools (APIs), and foundation models (via Bedrock). To scale these agents from POC to production, Pendala presents \"Amazon Bedrock Agent Core,\" a managed service that offloads heavy lifting like runtime management, memory (short and long-term), identity, and observability. Udai Predi then explains how Elastic Search integrates with this ecosystem as a \"long-term memory\" and retrieval engine for RAG (Retrieval-Augmented Generation). He argues that 95% of POCs fail due to lack of context or \"context poisoning,\" comparing an agent without context to a GPS with a wrong address—no matter how fast the car (compute) is, you'll end up in the wrong place. Elastic solves this by providing secure, scalable vector search and \"Agentic Observability\" that tracks token usage and costs, preventing expensive GPU overruns. The session culminates with Rotten, CEO of Tavili, who redefines modern search not as human-to-web, but agent-to-web. He introduces Tavili as a specialized search engine for agents that doesn't just fetch links but processes and chunks web content to minimize tokens and hallucinations. He dives deep into \"Context Engineering\" techniques, debunking the myth that Context Window equals RAM; instead, he advocates for a dynamic approach where each step of an agent's workflow receives only the specific slices of context it needs. He details strategies like \"Write\" (using scratchpads for intermediate states), \"Select\" (retrieving precise data via RAG), \"Compress\" (summarizing or chunking data depending on the query type), and \"Isolate\" (splitting tasks into sub-agents with separate context windows to prevent poisoning). The session concludes with a demo of a sales agent that first queries an internal CRM (via Elastic) to understand a client's history, then searches the live web (via Tavili) for recent news like earnings reports, and finally synthesizes a personalized outreach email, demonstrating the power of combining private enterprise data with real-time world knowledge."
is_generated: False
is_translatable: True
keywords: Agentic AI, Context Engineering, AWS Bedrock Agent Core, Elastic Search, RAG, Tavili, Strands Agents, Context Poisoning, Agentic Observability, Enterprise AI
---

Hello, um, can you all hear me? Yes, I think so, right. Good evening to you all of you. I hope you're all having fun. Are you? Yeah, on an evening, right, in Las Vegas, right, you just wanna get through this whole thing and then you have more fun after, right? But anyways, um, if you are building agentic A applications and you're really struggling with your applications in terms of, you know, lacking context or in these models hallucinating, right, um, you are. You are in the right place to discuss that and that's precisely what we're gonna discuss. Uh, this is Shirini Pendala. I am the senior partner solutions architect here at the AWS and I'm joined by our esteemed, uh, partner Elastic represented by Udai and more even exciting is that we actually have our joint customer in the form of Tavili, uh, and represented by none other than the CEO himself, Rotten. And uh they we're all gonna together can help um further go on this discussion of context context engineering and how we can better build quality agents for our enterprise needs yeah thank you so much for your audience on this and uh really please accept our gratitude on this and gentlemen you know if I make a request and then as they come onto the stage they'll introduce themselves too perfect. So Technology, yeah. I want to take one step back, right? Take a deep breath and talk about. AA agents, right? We at AWS, um, clearly cut, you know, thought through this and then gave a definition for this, and it's basically agents are autonomous or semi-autonomous software systems that can a reason, B they can plan, and C they can actually act. To accomplish specific goals or the tasks that you're giving and throwing at them, right? In a sense that's what an AA agent is all about. Now for these agents to be able to do their job. They need many things, things like, you know, for example, you know, if you want to do, if you want to accomplish something, right, you need a crystal clear definition of what the task that you want to achieve. Then you also need tools right to to do those tasks and AI agents are no different and we're gonna zoom on to that in a second. Now why are we talking about the agents here? Because context engineering or, you know, building agentic A applications with the right context, specifically your business context is so crucial. Otherwise, you know, these models, you know, at the end of the day they're just models. They don't know what to do unless you give them the guidance, unless you give them the right context, especially in an enterprise set up. You have these data sets, data, I would rather than saying data sets, I would wanna call it as data assets. That's your IP. That's your data and only as long as much of the quality of the data that you actually provide as a context to these agents, only so much you can actually get out of it, right? So if it's garbage in garbage out, but if you provide quality data you get quality output. Uh, Technology again, sorry. Yeah, there you go. I think I probably need to eat more, not. So As the members speak. A revolution is happening. These are the numbers from the Gartner themselves, right? They are saying. By 2028, 1 3rd of the enterprise applications are already using agentic AI in some form or the other, right? And at least 15% of the actual decisions would be made by these with the help of these agentic AIS in, in a typical enterprise too. And again that's by 2028. These are the numbers that's directly in coming off the uh Gartner reports research. So what that you know brings us to we want AWS to be the best place to build. And deploy the world's most trusted and useful agents. That's the vision that we have and we would love to always, you know, collaborate with our partners like Elastic to actually make that a reality for our customers. Now the story starts. Let's actually take one more step back again and uh look at it from the from the choice of the framework itself, you know, purely from the developer's standpoint, right? You know, you have many agentic AI frameworks like, you know, land chain, Lemma index, Crew AI. Anybody used Crew AI? Anyone here? Land chain. Land graph, yeah, there you go, sir. I see a few hands, right, um, these are all great frameworks, no doubt. AWS also took one step back and created another uh framework SDK called strands agents. It's completely open source. It's written in Python, right? Any Python lovers? There you go. I see a lot of hands. Yeah, you're gonna feel super, super easy at home with strands as decay, right? You know, let's say what is the end of the day, what do you need to create an agent, right? It actually requires 3 components. 3, number 1, it requires a task. Throw a task at it, what you want to accomplish, right? Typically that translates into a system prompt that you actually throw it in. The second thing that it needs is obviously, like I said, tools. So you, you, you take your existing APIs, you take existing application logic that you already have, and with simple, you know, decorators and annotations, annotations, you can actually convert them into tools that your, um, agent applications can actually leverage, right? And the third thing that you need is obviously the foundation models themselves and where do you get this from, uh, where do you get these foundation models from again, Amazon Bedrock actually has. Plethora of foundation models from all the innovators like you know meta anthropic, you know, even Amazon itself, right? so bring all these things together with strands as decay and and believe it or not, you know, developers are gonna love this. The experience is gonna be super simple, super simple. It's actually one line of code. It's really one line of code to actually craft an agent. With all these three ingredients, system prompt, foundation model, and the tools. It's just one line. So it's that I'm, I mean you know if I fell in love with strands now of course I've I've got my hands dirty on the other frameworks too nothing against them, but there's another choice uh for developers too. So At AWS we give all the tools that you need to really make this AI adoption. Which could be a utopian dream, a reality, right? So at the very top you actually have ready to use right out of the box, ready to use solutions in the form of like for example for your enterprise productivity we're talking about uh Amazon Quick Suite or Quicksuit whatever, right? You have Quiro for um agentic software development, right? You have AW if you have legacy, um, you know, systems. That you want to quickly migrate them as they used to talk at modernization. Now we're talking about a modernization. So if you wanna do that, you have AWS transform, right? Um, this is all great, but then if you wanna do, if you wanna do yourself, if you want to, you know, craft these applications yourself again for the developers, I already uh zoomed in on this which is in the form of strands agents that's another SDK. Uh, that you can actually leverage and then if you wanna really do this in your own DIY style then, um, you know, of course the Amazon Bedrock itself becoming the foundation actually gives you the moral choices like I already mentioned, right?] Then it also gives you the capabilities of, you know, um, for example, you wanna do it in a more responsible way you wanna, you don't want to, you know, leak information that should not get into the wrong hands, right? You have the guard rails for that. And this is all great, you know, it all looks super easy, uh, on a developer laptop for a demo for a POC. It's all great, but then as soon as you decide to actually move your agentK applications into a, a enterprise scale production deployment, right. Then it's not gonna be easier as as we all know, right? You gotta think about security. You gotta think about um how do I manage, where do I in fact run these agents in the first place, right? You need a runtime environment for that. You need to manage, um, the contextual memory, right? You know, in the form of short term memory in the in terms of the long term memory, right? You need to think about, uh, enterprise A adoption requires enterprise security too. Authentication, authorization, all of that. So the answer from AWS on that aspect of to, you know, to quickly easy make it easy for our customers to do this is comes in the form of, uh, Amazon Bedrock agent core, and I'm gonna zoom in in that in a uh in a, in a bit and of course you know if you want to do the traditional AML we have, uh, you know, Amazon Sage maker itself you can do custom models building right from that, you know, you can do train them. With your own data, um, you can even leverage for your training purposes you can actually leverage, uh, in a trainium or inferncia and GPUs and all of that on Amazon. So you have a plethora of options here. Do it yourself, you know, ready to use solutions or even go deep dive, you know, you have plenty of options here. Now, like I said, building an agent is the start. Doing POS is easy. Getting excited is easy, but then production scale is the goal, right? You want to run these things so that the, the, the, you know, your enterprise can actually adopt, discover these agents, and then get the job done. That's where, uh, like I said, um, Bedrock Agent Corp is, um, is, is of help. In essence what we're talking about is we would want to um you know we would want to uh undifferentiated uh heavy lifting of your AA operations we want to take that on our shoulders so that you can actually focus on your innovation so that you can actually focus on your business applications so that you can actually focus on your application use cases and the application logic itself while we do the rest of the stuff for you, right? So uh it comes in this uh. In, in, in OK, it comes in the form of, you know, it's basically Amazon Agent Core is a combination of these modular services we're talking about runtime, we're talking about the memory, talking with identity gateway, and of course the other miscellaneous tools too. So let's actually zoom in on to each of these, right, um, the foundational service for running these agents actually start, um, with the run time itself, right? So you need a place to run these that's where agent, uh, agent core runtime comes into the picture. And uh basically think of like good old days a a serverless compute environment for you to run your serveless uh computations, right? Same thing here if you want to run your agent applications you need a runtime environment that comes in the form of agent core runtime and when you do that. You also need to be able to do this in a more secure way who can access your data sets, data assets who can access these agents who can do what the authentication, the authorization aspect of it comes in the form of identity, right? You need a strong identity, um, uh, services to be able to tie it up with your actual agents so that you can actually securely, uh, run these, uh, to, you know, agents. Similarly, you know, you, if you want, you know, we also provide the tools to enhance the, the, the overall experience, but you know, the needs, of course these are the actual fundamental needs of uh, an agentK application itself, right? You need memory. You need, um, you know, you developing these AI agents is one aspect of it, making them discoverable across your enterprise so that the rest of the other agent AI applications within your enterprise can also take advantage of. If you go back a decade, 15 years back, right, we were when, when this whole notion of API first strategy started, right, we wanted, we were building API bus so that, you know, this, these APIs can actually become discoverable within your enterprise, right? Same thing here, you know, as you build agent decay applications, you want them to be discoverable. You want them to be used, otherwise, what is the use of building super, you know. Super fancy agents, but nobody knows that they even exist. That's where the gateway comes into the picture, right?] And of course, you know, if you want to simulate user actions using web tools, you have the browser tool, you know, if you want to, you know, automate workflows using, you know, software, you know, you actually have the code interpreter to do. So long story short, they all come together, right, um, to, to make your agent applications enterprise ready. And this we're doing this completely, you know, it embraces any framework you you are on the planet literally, OK, any most of the, most of the famous ones, the crew AI, you know, land graph lama index. Or you know, our own agent strands agent itself so you can use any of these frameworks. Your, your, you have a strong affinity to any of the protocols like whether it's MCP or A2A. Please bring it on. You know, it, the idea is that we want your agent A application super easy to be deployed on, uh, agent core and that's the, that's the goal here. Right, so I am. Wanna quickly um mention something here real quick um in. New York summit right last summer, uh Elastic has. Uh, an MCP offering, you know, if you haven't tried it, you know, you should get your hands dirty. It's super easy to use. So, uh, we announced together, Elastic and ourselves, we actually announced, um, and Elastic is one of the esteemed partners who actually went, um, and you know, GA their MCP offering on agent core runtime. What does that mean to you if you already have your enterprise assets that are indexed in Elastic. With simple natural language, you can actually interact with the data using the MCP server on the agent core securely. Makes it super easy for you to craft these agent applications using Elastic as a uh data source. Next is uh about this is about the agent core gateway memory and uh browser so now I didn't actually zoom in on this one, right? Agent core memory is is a crucial important ingredient specifically in the context of actually context engineering itself, right? You wanna manage your the memory of, uh, memory of, you know, the information that you're actually feeding to your models for your agents. Um, there is short term memory. There's long term memory. I'm very happy to again share with you all that, uh, we using strands SDK with one single API call. You can, if you already have Elastic, right, you can one single API call, you can actually use Elastic as your data source for agentic memory. One line completely manages for you. Developers love that. More importantly, business users also love it because you're actually shipping covered much faster. Your, your value to the, your time to value is shorter, so you can actually, you know, reap the benefits of your investments, right? So and the third thing is. Yeah, this is my favorite. I was actually, you know, uh, myself and today we actually got our hands dirty a couple of days back to make sure actually this actually works and it does because Agent Core. Um, observability module, right, right out of the box. With 0 code, with auto instrumentation. Emits Hotel. Open telemetry data. And Elastic also being in the forefront of uh open telemetry they actually in fact they have their uh Elastic actually has their own distributed uh open telemetry contributions. It's called EDOT. I'm sure majority of you already are already aware of it. So with open telemetry data right out of the box it's zero code Eastic can actually give you end to end, right, right from the time the agent actually gets invoked all the way to the end that you get the final response. You can actually collect the logs, you can actually collect the metrics you can actually collect even the traces and see for yourself what's happening. It's basically. you know, it's one thing to have this, but you know it's another thing to actually observe this, right? So agent core observability works right out of the box and if you want to go more fancier, if you want to expand these traces with more custom spans, you can also do that using with using your custom instrument instrumentation, uh, logic too. So that's again a possibility with elastic. So elastic today in summary is in the agent core runtime. Using their MCP offering, they are directly plugged into strands as decay as a long term and short term memory and then best of all, everything is that you can actually observe right out of the box again with zero code practically, um, you know, the end to end of all of your agent decay applications. So I love elastic, you know, and I'm not doing this as a lip service. I really love this because I've seen our customers actually reaping the benefits of this technology. So that's observability, right?] Uh, we already talked about it. In essence, that's Amazon Bedrock agent core. You have all the tools that you need to really, you know, deploy your agent care applications at enterprise scale. You, uh, and we do that, you know, by being very open to MCP, being very open to A2A, any kind of protocol, uh, that you subscribe to, and of course, you know, we love any framework that you can bring on, uh, you know, onto this as long as you can actually containerize your agentK applications, the doors are open for you, right? And Elastic, uh, if you use Elastic as a, as a data source, no, the, the game is super easy for you all. So with that. I'd like to take a pause here and I'd like to request you all if you wanna, you know, get some inspiration, get some motivation, some impetus, please take a snapshot of these resources. I want to leave this with this idea here. Agentic AI revolution is happening. There is no more. It's an option for us to be part of it or not. It is happening. And it's, it's super easy. I would like to very humbly challenge each and every one of you to think big, innovate, and use these tools. Strands as decay is super easy to adopt, and then you can easily bring. That your workloads to Agent core and if you're not believing it, no, one of these, uh, you know, the SDK and the tools and the samples, especially the samples, uh, uh, uh, thing that you're looking at is actually going to point to, uh, a GitHub report. You got all the plethora of, of, you know, samples that you can actually look at, get inspired, and then build your own, make, make AI dream a reality and, uh, with that, I would like to actually invite Udai from Elastic. And Elastica has been doing a lot of, uh, you know. A lot of innovation in the space of context engineering, so I'd like to have him, you know, further double click on that. So thank you so much. Appreciate it. This is the next. This is. Hi, guys. So, you're able to hear me? My name is Udehi Predi. I'm a cloud architect at Elastic covering AWS Solutions. I closely work with the Srinivas, uh, to solve the complex problems. So that's my job. Uh, actually, you, you, you know elastic search already because elastic search is pretty popular and as elk stag, right? So elastic last t is Kibana. So you may be used in, uh. Log analytics, so maybe, you know, maybe security observability in somewhere in the ship. So, but, uh, Elastic Search is a very, very popular vector database. So it's like, it's, it's most most popular vector databases downloaded in the last one year. It's like 5 billion downloads, so it's like pretty popular vector database. 50% of the Fortune finite. Companies are using for different use cases, so we have a platform, Elastic Search platform, and we have solutions on top of the platform like observability and security. So we have integrated all the AI things in agent core strands and also bedrock prints and everything, part of the Elastic Search. You can run Elastic Search in your laptop in cloud as a serverless offering. So we are giving that option because. You can, you can deploy it anywhere, so you can run in a container. It's up to you basically, but the most easy way to run is a serverless offering. It's on elastic on AWS serverless. So if you, if you maybe you use the elastic in some way, other way, use cases wise, maybe you took an Uber from your hotel to Venetian to attend the, you know, session, are you checking the restaurants and Yelp to get some good food in the evening, to get, you know, relaxed day, so you are using elastic in many ways, like maybe you're searching for a date, like Tinder, so it's just many ways. It's like search, wherever the search is there, elastic is there, so. So, I know Elastic started uh AI journey almost a decade ago. So if, it's not a, AI is a, you know, AIML is like a big platform. So when, when we started AI. In a log analytics or security detecting the anomalies, so we started with the supervised and unsupervised machine learning. So, machine learning is basically we, we use the machine learning to detect the anomalies. We use the machine learning to detect the algorithm, all that clustering and, you know, all these things in the last decade. So we know machine learning very well. We give that as a value to all the solutions. So for the phase one, right, so after machine learning, all the, all these like models and everything. Recently, the LLM explosion came, so everybody started using foundation model as Srini mentioned. There's like a lot of foundation models like Anthropic, Nova. There are plenty of foundation models from Amazon and even like some of the other third party likewell Labs and a lot of people using different models for different use case. So the models are great. So there's no issue with the model, but the only thing is that models have some kind of a memory that only works for 80% of the time. It's. So the second use case is rag. So if you are working with your enterprise applications like maybe banking or like finance, maybe anything with the insurance, right, so you have to store those vectors somewhere in, you know, retrievable augmented generation that's in elastic. So that's a rag use case if you chart, if you ever use a chatbot application and maybe booking travel or like, you know, calling the customer care, so that will go to the rack system nowadays. Everybody's building that. Behind the scenes, like RAG uses vector database to basically give that, give the prompt to, you know, make it like a context aware prompt. That's where RAG comes into the picture. We have the vector database that inbuilt the models. We have ESA and S3 and some other models. You can bring your own model also custom models, but RAG is a pretty popular use case. Elastic do RAG very, very, you know, frequently. That's that's another. And then also moving to the agentic one. So more and more elastic use cases are increasing because elastic as a long-term memory, as Srini mentioned that strands, right? So elastic work as a long-term memory because in the enterprise grade applications you're building, so you need a security, you need a scale, you need a reliability, you need durability. It's not easy to basically build in a laptop or like in a container, right? So that's that elastic provides that long-term memory and the agentic memory. So that's. And also if you look at the same content but it's a different way, rag, you use it for the cushion, but that will go to the elastic, check the context first, and that context may be coming from your email, or Word docs, maybe Google Docs, all that vectors you're storing. So if you are getting that context from vector database and giving that prompt to LLM, then you most probably get 99% of the. Correct context, right? So otherwise LLM thinks if you think, uh, example, like if I put it like a NASA, so NASA, maybe that's like, you know, popular, uh, you know, airspace like you like this, you look at like NASA is inelastic context it's like a North American sales organization, so something like you don't know the context right. So basically if you think about that, so the context is important for getting the right answer from LLM. So LLM cannot give that. Right answer by itself because it needs because all these elements trained by the reinforced learning, so it gives the right answer all the time, but it won't give the right answer the first time. But you know in the business you cannot wait for the right answer because once you get the wrong answer, your customer may not come back to your website, may not use your app. So that's where you have to be very, very careful building this application with enterprise grade. That's a rack. The second thing is the same thing the agent AI. So you need a right memory, because your LLM cannot, remember everything because it's just only 50, 60% of the time, and then after that, you have to rely on the long-term memory. Because we're not talking about, uh, we're talking about petabytes of data. It's not like, uh, you know, terabytes of nowadays, data is like exploding if you use in the right context and right data, so then you, you, you will go next level to the, you know, uh, in your competition. So the next hype in the AI, so AI is not a hype, but I'm just saying like a lot of people are thinking it's hype, but it's real. But if You build it correctly. The only thing is that AI will come up with their own challenges. The next AI is coming the autonomous cars or maybe robotics, something like that, maybe humanoid robots. For that also they need data to operate. If you put your workflows, your tools, and your agents with the right context and the right data, then only it works as expected. So the challenges with the, you know, agent is basically the lack of context. So the lack of context is the same thing that Sweni mentioned that even traditional AI, right, if you don't give the right data set, right inputs, and you won't get the right output. Same thing for the Gen AI agentic AI that's a lack of context will give very bad experience to your customers. So that's where you need a lack of context and also durability. If you have a context, you Have everything, but you don't have security. You don't have the right system, then it will fail because you need that durability built in the system. So that's where elastic provides, and 95% of the POCs fail because of because of this, because you don't have a lack of context, and context window you're not setting the right context, then you won't get the right answers. Simpley is basically GPS. Even if you enter a wrong address in GPS prompt, you will end up somewhere else, right? So even your car is good, your GPS is good, it's the same thing. Your LLM is good, your compute is good, but still, you're getting a wrong address. So that's where you need the right context window and you need an enterprise, clean data. Clean data means your data quality and data governance. Everything is like. Have to create that in enterprise grade databases like Elastic and all of the tools like MCP, like Stey mentioned, and MCP is great, but only it can communicate with other agents. It can get the data, but the only thing is you have to massage the data. You have to clean the data. You have to put it like the data in a format. Even the agents understand that. G application understands. That's where the enterprise. you know, data stores like Elastic comes into the picture and you know, same thing with the complex fragile development if you build that app, a app or agency app in a rush without using all these guard rails or the right data store, then you end up managing that. You cannot, you cannot operationalize that agent in the market, so that's the problem. So Elastic is one stop shop for everything. It provides all the encoding models inbuilt. It's sparse encoding model, dense encoding model, and multilingual embedding models like E5. All that is, you don't have to work on the, you know, undifferentiated heavy lifting, you know, in the data stores. It comes with it. I know some of the data stores, you have to build your pipelines. You have to work with external models. Do your chunking strategy. Build your embeddings, all that you have to manually do it. That's like a, it's, it's not easy because you have your day job and you, you are managing that all that yourself. It's like time taking. So multi-modal embedding models like we have uh multi multi-models like, you know, text, audio, video, so those are all supported and GPU based inference if we, we have automatically included GPU-based inferencing and. We have tight support with Amazon Bedrock, anthropic models. We have tight support with other models like Sagemaker. If you want to open source model inferencing, you can still do with elastic influencing with Sagemaker. So that's all possible. And like I said, chunking is part of that. We have close to 200 plus connectors. The data onboarding is easy, so you don't have to build that in the pipeline. We have that APIs built. We have all that taken care of, so you don't have to. Work to get the Salesforce data or like some other data from a Google Drive. All that is part of the Elastic and all the web crawlers built in. You don't have to work on the web crawler stuff, so everything is part of that, and we do auto parsing and now that we are using AI to parse your data, put it in the right format so that it's clean, it understands the data. So the retrieval side, it's like more on the vector search. You can do vector search. You can do lexical search. You can do hybrid. Search all that is possible. And in fact you can also do geospatial and graph retrieval also. And then like I said, we have our own models. We recently bought a Gina model from recently acquired a Gina company that's come up with the re-ranking models, very popular in the industry, so it's it's, it's taken care of the re-ranking and it gives the relevance to context. So that's, that's another one. So we are innovating on behalf of you, so you don't have to do heavy lifting. And it can work with the rag workflows and LLM integration. All that is part. Now we are moving to agenttic side. So we actually recently released the Agent Builder, so that actually provides gentic workflows and you can build the tool selections and all that is part of the. So that's just a thing. Another thing is Srini mentioned once you build it, you need to monitor that so that agenttic observability is a key one. So we recently integrated with agent core observability. You can monitor agents, invocation logs, error logs, and agenttic memory because GPUs are very expensive. If you, if you're not careful. With the GPU consumption, you end up with a with maybe millions of dollars, right? So you, you have to optimize your GPU. So that's where elastic comes into pictures to show you how many tokens you're using, what kind of invocation lacks your agent to agent communication, everything is part of that. So we also do all this token tracing and everything publicly. So next one. So this is the agent builder that we recently launched. It will provide. Agentic applications building very easy because you have data sitting in Elastic Search that is already clean and then also it provides the right context to your LLM and Agentic stuff so that's where you can chat with your data. You don't have to write complex sequels, joins all that is like natural language processing. You can chat with your. Data as if you are talking to a human. So that's easy that we make it and then also custom if you want to customize the tool or workflow, you can do that with the ESQL. You can actually define the workflows. You can, you can do that you can control that agentic flow. You can define that in a custom tool. You can attach to the agent, and that will manage your flow and we have external integrations like MCP. And Amazon released their own MCP server, right, so you can talk to other tools and other agents like maybe SRE agent or something like a security agent. You can build with that and maybe you can build a financial services agent. So it's, it's endless opportunities and opportunities are limitless. You can try anything, but that's and like I said, if you look at this Venn diagram like context engineering and Agent builder elastic City in between so it can provide all that ingestion data on boarding and security re-ranking all that enterprise level grade data store it will support and an agent builder will build the custom agents for you if you want to because the. Production. Everybody can build the POC apps, but if you want to build a production ready and POC to production, that's where Elastic gives an easy way. It builds powerful custom tools. You can you can easily check your agent right there. So the agentic builder is basically provides the accelerated agent building. That's what I said you can build in instead of going months and months, you can try within like days. That's, that's possible. And then you can customize the workflows. You can actually do native context engineering and build a reliable AI development, right? So that's like. It'll come up because privacy is very important in agent building. So if you build without privacy, there's no point, right, because industry comes with all this governance, maybe the HIPAA or like SOCs. We have to manage this data in a correct manner, otherwise you may end up getting bad experience from your customers. So that's, that's the thing. And with that, I will actually hand it over to Rotam. He will show you how he built. Agents, real agents, uh, in the, in, in the industry with the Elastic and Amazon services. Uh, thank you very much. All right. Hi everyone. Excited to be here. It's my first reinvent, so, uh, also interesting, yeah. Right. What we're gonna do today, I'm gonna introduce Tavili cause I think not all of you know what Tavili is. I hope you do. Uh, then we're gonna do a dive into what context engineering is. Uh, more practical, I'm gonna show you some concepts and things that you can just take home and do. Um, and then we kind of finish it up. Hopefully we're gonna have time with a very simple demo. It's just gonna show how you utilize all of the technologies that were mentioned, Elastic, AWS, and also the ability to build a very simple sales agent. It's simple, not because I was lazy, it's simple because we have great technologies that reduces a lot of time to developers. Awesome. So Waterville is? If you think about the shift that is happening today, uh, up to now, people used to communicate to the web directly, right? You would go on Google, you're gonna send emails, but what's happening right now with the agents is that people communicate to agents and they communicate to the web. What it means for companies is that. We're becoming this gateway for everything that's gonna go in and out from the organization, and that requires a lot of governance, security. They see it as the new firewall and obviously scalability to really allow, uh, allow them to build all of their agenttic system and connect them to the internet with Tavili. Who knows Tavili here? Anyone? Perfect. Awesome. So what it started with is really, if you think about a year ago, you'd ask any LLM or SGPT, you would ask what's the weather today or what's the score of the game last night, and the result you would get, as all of us know, is, hey, my, my knowledge is cut off 2021. I cannot answer that. So that was the first thing that we kind of bridged. That was the gap, um, but. It turned to be much more complex, right? It turned to be much more complex because it's not all about getting the right knowledge. It's really going into the web pages and extract the right snippets. And now when we're gonna talk about context engineering, you're gonna understand why it's so complex but also so important. Awesome. So what context engineering is, uh, We talked about it before, but I'm going to recap. It's really what you put inside the context window, and it's what you put inside the context window at each step. So, we have knowledge, which can be your proprietary data, it can also be your emails, your Slack, maybe web knowledge. You have tools, uh, like the Tavili tool, um, maybe it's gonna be a calculator that's gonna do calculation and return the agents some feedback. Any instruction, which is prompt engineering. Uh, it's really to give you the tool descriptions. It's to tell the agents exactly what you want to achieve, give you some restrictions. Uh, each one of them is an art by itself. Awesome. So, This is very important because you're gonna hear a lot context engineering or context window equals REM. Which kind of makes sense because it's really, you wanna put inside a context window what's gonna allow the agent or the system to operate better. But why, the reason it's not correct is because if you think about it, if you're just gonna bombard the context window with redundant tokens, you're gonna be penalized. You're gonna be penalized with hallucinations because. The agent can get poisoned, right, with irrelevant data, and it just costs you more, and there's no reason for it to cost you more with redundant tokens. I will take it to what we do at Tavi. So think about the question like what's Leo Messi's wife's name, right? You're gonna get a Wikipedia page, but if you're just gonna throw the entire Wikipedia page into your agent, into your prompt, you're just losing money. You're. Getting a lot of redundant tokens there and it can cause hallucinations. So our job at Tavili was not only to get you to the right links, it's also to go inside and do processing that we're gonna talk in a bit to make sure that we give you the minimum amount of tokens to allow you to answer the question or allow the agent to answer the question and at the scale of the internet and the scale that we're seeing with AI agents. It's a difficult job to do cause you've gotta do it fast. Now, I'm gonna take a step back about the video, just so you understand how fundamental web search is and where it comes. So, you're gonna see a lot of verticalized agents, right? You're gonna see sales agents, you're gonna see marketing agents, HR agents. I'm not sure which one you invested in. I hope you're gonna get rich, but eventually all of them gonna need web search. The HR agent's gonna go to LinkedIn, right? It's gonna find talent, uh, and the legal agent can go to the web to check for presidents online. But essentially the scale that we're seeing right now, and we're seeing with the growth of the company. It's insane and we're just at the tip of the iceberg because every company is building their agenttic system and all of these verticalized agents gonna go into the same functions that people do today. I mentioned sales s are legal. All right. Let's talk about the dynamic split. So, as we said, the context engineering is about inserting the right relevant context at each step of the agent. Now, we also said that it's not RAM. You don't need to put 100%, to feel 100%. Of the context window. Every step can be different. Let's take a research agent, for example. So, you're gonna start with doing a lot of web research, right? You're gonna collect data about the topic that you're exploring. But then you're gonna take that and all you wanna do is to create an outline. So, first step can be a very long knowledge, right? And very short and simple instructions. Just build me an outline. The next step, maybe you're gonna split it and you're gonna write paragraph 12, and 3. So for each one of them, the knowledge is gonna be much less cause you're gonna split it, right? It's gonna be less knowledge about the topic, and maybe you're gonna have a lot of instructions. You can say, you want MLA APA, Chicago, uh, yeah, I just graduated. So, um, you're gonna. Give it a lot of instruction, less information, and it's the same agent, just different step. So you gotta make sure that when you're building context engineering systems, you need to allow dynamic um split between the three components. Awesome. So that I actually took from uh Lance Martin from Lenshain. Uh, if you didn't read his blog or watch his YouTube video, he does have a great explanation about context engineering. Uh, let's break it down one by one. So, Writing, writing in context, you should think about it much like people. So if you're gonna listen to a lecture, and you're gonna have a, I know, exam after that, you wanna write notes. You wanna make sure that you're gonna remember certain things that you listen. And what it allows you is you don't need to fill in all the context window. You can write it into your file system, and then you can retrieve it at certain different steps, right? So you can think of scrap, uh, scratch pad as something that. Runs with you throughout the same process, the same agent. You can also have long-term memory. So, you know how sometimes if you use chat GPT or any different chat application, it suddenly remembers something you, you talked about a few days ago. So, that's really the long-term memory, which can be memories that are relevant or facts that are relevant for the system as a whole. Maybe it's gonna be a different agent, different user at a different time. So think about a sales agent that is doing something bad and it lost the deal. If there's gonna be something similar later with a different salesperson or a different client, you wanna make sure that the agent remembers what went wrong and it can actually take it into consideration when writing the output. So that's how you should think of the longer, uh, long memory. Let's go into select and that's reg, right? With select, you're gonna retrieve information from the same component we mentioned. Either it's gonna be a scratch pad, or it's gonna be long term, and it can also be web data from Tavili. Um, and here it's, it's an art, and I'm gonna say that for companies, The most important thing is to learn how to do it, uh, because system can be very complex. You can use different databases, you can use different communication methods, and no ones gonna build it for you. No one gonna build something off the shelf that knows how to retrieve from whatever you want. It's not gonna work, and that's why companies should put a lot of effort into just. Learning how to do it on their system for their agents. It's got to be very customizable to really get good results. The third part is compressed context, right?] So, with compressing, that's probably the fun part, all right? And I'm gonna tell you something that happened to us at Tavili. You can make it simple, right? You can just, if you have too much context, you can summarize the page, right? It should work, but it's not always working. Uh, let's think about an article, a soccer game article, right, which we retrieved from Taviri. And I can know, based on the data we have that 99% of the questions that's gonna be asked about this page is what's the result of the game, right? And if I know that, why would I just summarize the page? Maybe I would just choose the right chunk to return. So you have many different methods for many different use cases, and again, not everything is, again, you, you cannot always do semantics. Maybe you have financial data, maybe you have, you know, maybe you're working about people. Not everything is semantic and everyone just usually fall back to this, uh, embedding methods and chunking and embedding. It's not always the solution. Um, think about coding agents, that's not everything is semantic. So you gotta think and be very creative with the ways you compress and trim. Isolate. Um, so I actually started with an open source called GPT Researcher. It was one of the first agents that Went to the web, collected data, turned it into a coherent report. Uh, what we did there, which was pretty unique, is that we took a lot of data and then we split it into a lot of mini agents, sub-agents, and we allocated each of their, um, each of the content of the knowledge based on the paragraph that each of the agents was writing about. Why it's so good? Cause first of all, you're gonna get, uh, if you're gonna have 5 subagents, you have 5 context windows, right? So you can actually do much more in parallel. It also prevents um context poisoning. So if you're gonna retrieve knowledge from a knowledge that only discusses about what you want. You have less risk of poisoning the agent with irrelevant data, and that's something that's not only efficient because you're doing parallel compute. But it also prevents a lot of hallucination and speeds up the process. Awesome. So, we're gonna do a very simple demo. Um, so what you're seeing here is A master agent, the AWS, uh, Bedrock, uh, we're gonna use cloud. You're gonna see two tools, right?] Two sub-agents. One of them is gonna go, it's gonna use elastic, it's gonna go to a prefilled CRM, um. A lot of dummy data, and it's also gonna go into Tavile. And what we try to mimic here is an agent that's gonna get a question about a client, right? It's gonna be a system for salespeople. It's gonna check first if it decides to, it's gonna check on the internal data to check communication, it's gonna check notes from salespeople, um, maybe it's gonna talk about recent deal or updates. And then it's gonna take all of this context and it's gonna go outside to the web. Why do you wanna go outside to the web? Maybe there was an announcement here at Reinvent. We saw a lot of announcements. So we wanna make sure that we're on top of everything. Um, you can generate an email, hey, congrats on the recent fundraise. We want to sell you an office. So that's the reason you want, um, real-time data. And yeah, that's it in action. Awesome. All right, so what we're doing here, um, we're gonna set up two tools. One of them is the Tavide web search. Uh, as I mentioned now, with Tavio, what we're trying to do is really to simplify everything for the developer. So the tool is one line of code, the agent already knows exactly what to do. Um, and that's all you need to do when you define a tool with Davile. The second thing is we're gonna define the elastic search tool, right?] Uh, that's a method, uh, that land chain are using. You can define the, uh, details about the tool right here. So it knows exactly, the model can know exactly what to do there when it goes into, when it chooses the tools. A lot of things that you shouldn't worry about, but that's how we define the tool. Uh, and we're gonna set up using AWS Bedrock, we're gonna set up the clouds on it, right? Now look how simple it is. Not that simple. All right. So what we're defining here is a very coherent set of instructions, uh, that really tells the model and the reasoning model what to do. Um, so we defined. Everything. So we define the system prompt your research agent that can conduct comprehensive, accurate and up to-date research using a combination of real-time public data and internal enterprise knowledge. So we're gonna define the Tavia web search tool. We're gonna define the vector search which is uh the elastic, and we're giving it all of these guidelines. Awesome. And what you're seeing here is also pretty important, not pretty, very important uh concept, uh, about instruction, which is a few-shot learning. This is a one-shot learning, uh, but you, as long as you can, uh, you wanna provide the model with a good, roughly relevant example. On the process on what to do, which directs the model much better. I'll give another example from real real life, so. We're trying to build a good search for uh coding agents, right? And what coding agents need is fresh data from documentations, right? You want to see the latest API updates if there was any change, so you're gonna reduce a lot of hallucinations there. Uh, what we did, which was pretty smart, usually with APIs, if it's a new API, which we're seeing a lot today and it wasn't trained on it, it's gonna get, it's gonna be really difficult for the model to uh do it the right way the first time. So we really wanna make sure that you provide some example, and the more similar, the better it's gonna get. So what we did was pretty interesting. So if there was a failure, we had a collection of examples from APIs, different APIs. If it failed, we would make sure to take it in the background and to generate another example that is very similar to the one that didn't work. And then the next time it ran, it would run a vector search on the collection of this example of coding snippets from each API. And slowly learning was able to improve over time and get the right fuchsia learning in the top 3, and that allowed the model to always improve the execution with the new APIs. So that's what you're seeing here. It's an example, a question, thought, action, observation, and now let's see, it's running. So what we ask here is check the Amazon deal size and find the latest earning report for them to validate if they're in the spending spree. So it's gonna go soon we're gonna see all the thought process, but it's gonna go into the CRM, into Elastic. It's gonna find the relevant information from Elastic. Um, from an Amazon account. And then what it's gonna do, it's gonna take it, it's gonna go to the web to find the the recent, um, Warning report and it's gonna generate a robust report based on that. We have a great Wi Fi here, so it's uh. Yeah, no Wi Fi. We're gonna upload it, so you can run it. So sorry for that. Uh, we're just gonna look at the output of the first one, just because it's there, um. So what you can see here, the question was, search for the latest news on Amazon relevant to our current CRM data on them. So what they did eventually. He thought, I first need to search our internal CRM data to understand what information we have about Amazon and then search for recent news. Action, it went and understood it needs to go to the Elastic Search. And after it got the results, the feedback which went into the knowledge, uh, it decided on an action based on the other tools that it has in the instruction. And the next tool was the Tavili search. So it, uh, decided to search for Amazon's latest news, AWS Cloud AI Enterprise Partnership, 2025 months advanced news, uh, got the information, decided to do another search, and ended up with a coherent report. About AWS. Awesome. So that's it, um. Yeah, we're on time. Thank you guys.