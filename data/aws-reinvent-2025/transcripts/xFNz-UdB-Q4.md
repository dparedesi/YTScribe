---
video_id: xFNz-UdB-Q4
video_url: https://www.youtube.com/watch?v=xFNz-UdB-Q4
is_generated: False
is_translatable: True
summary: "In this customer spotlight session, Vadi, the Worldwide Tech Leader for Migrations at AWS, introduces Tempo Labs, a rapidly growing Canadian startup and Y Combinator alumnus. The session explores Tempo's journey of migrating from another cloud provider to AWS to support their AI-driven design and engineering platform. Vadi sets the stage by discussing the evolution of cloud migrations, noting a shift from cost and efficiency drivers to a focus on business transformation and rapid innovation. He highlights AWS's own success in modernization through \"AWS Transform,\" a service born from their internal efforts to upgrade Java stacks at scale, saving nearly 800,000 developer hours.\n\nTempo Labs' leadership team, Kevin (CEO) and Brandon (Head of Growth), present their platform, which is designed to bridge the significant gap between design tools like Figma and development environments like VS Code. Tempo allows designers and product managers to interact directly with a production React codebase visually. Unlike traditional \"vibe coding\" tools that generate throwaway prototypes, Tempo integrates with existing repositories, effectively enabling non-developers to push production-grade code. They demonstrate the platform's capabilities, showing how a user can drag and drop components from a Storybook library—like a testimonials section or animated tabs—directly onto the canvas. The tool automatically generates the underlying code, which can then be committed to GitHub. A key highlight of the demo is \"Tempo on Tempo,\" illustrating how the complex platform itself is built and maintained using its own tools.\n\nThe narrative then shifts to their technical challenges and migration story. Originally built on Google Cloud Platform (GCP) leveraging BigQuery, Tempo faced a crisis when they went viral in early 2024. Their infrastructure, initially hosted on massive single machines, became unscalable and expensive. Simultaneously, their use of Anthropic's LLMs led to a staggering $250,000 monthly bill that exceeded their revenue. These factors drove them to partner with AWS. Through the Migration Acceleration Program (MAP) and a partnership with OpsGuru, they containerized their infrastructure on Kubernetes over a 6-to-8-week period. This modernization effort reduced their cloud infrastructure costs by 5x, from $25,000 to $5,000 per month, while improving security and scalability.\n\nA critical factor in their move was Amazon Bedrock, which allowed them to apply their Y Combinator credits toward Anthropic model inference—a capability not available on other clouds. They also praised the AWS team for providing critical support with rate limits and rapidly addressing technical blockers, such as API compatibility for prompt caching with the Vercel AI SDK. The session concludes with a look at their ongoing R&D collaboration with the AWS GenAI Innovation Center. They are co-developing a \"PRD Agent,\" a novel tool that uses LLMs to parse entire codebases and extract a knowledge graph of product functionality. This essentially creates a \"senior PM\" agent that understands every nuance of the product's capabilities and limitations."
keywords: Tempo Labs, AWS Bedrock, Migration Acceleration Program, GenAI, React, Design Engineering, Infrastructure Modernization, Anthropic, AWS Transform
---

Awesome. Hello, everyone. Thank you for joining with us today. Um, so my name is Vadi. I'm the worldwide tech leader for migrations at AWS. Uh, today, we have a customer with us. The session is a customer spotlight, so they will walk us through the innovation that they are doing with Amazon Bedrock, and they will also walk us through the journey of how they migrated from another cloud provider to AWS, and how they are doing continuous modernization and doing innovations in Gen AI. So before I introduce the customer, I would also like to give an overview of how the landscape has evolved for migration and modernization, so as GAI over the years. So I, I joined as the very first migration SA in the team and still remember the very first large-scale migration that we did. The primary driver for the large scale migrations back then. Where cost and efficiency. So it is cost as in the total cost of ownership, efficiency as in about operational excellence. So it's primarily around having better security, reliability, performance, and scalability. The conversations have evolved over the years since then. Especially the board of directors, customers like you, they want outcomes that is much more than just cost and efficiency. They want business outcomes. They want to transform both the business and the customer experience. They want to deliver a digital transformation and modernization outcomes. They want to The, the thing is they also want to reinvest some of the cost savings that they have and be able to achieve innovation. And with NAI they want to do this at 10x the wear velocity, 10x the agility. See, on the Gen AI space. So back in 2023, it was, it was completely a new landscape. Everyone was trying to understand, mostly the concepts, the basics, and the potential of it. So 2024 was more about the implementation. They were asking the right questions to get into the implementation mode and getting into the practicality of it. So 2025 was taking it to taking things to production and making it a core essential for their business business excellence. A good example I can walk through is AWS ourselves. So we had a lot of legacy Java stacks. It needs to be upgraded. They were nearing end of life. They need to be upgraded. And back, back in 2023 we were exploring Gen AI on how best it could be. Creating the slow touch ability to upgrade the Java stacks at scale. In 2024 we productionized it internally and we got a lot better at it. We were able to upgrade Java applications in a matter of every hour, we were able to upgrade Java stacks. And today, 2025, fast forward, we have our first gente service called AWS Transform, which can do this at scale. Every customer can leverage it, being able to, not just for Java, including .NET, mainframe modernization. AWS transformed so far has saved about 800,000 hours of developer productivity and also be able to look at a billion lines of code from a modernization perspective and it's not a small feat. So this is not just true for AWS or cloud providers or enterprises in the space. It is also true for startups who are there in the space. They have they have been given the same. Access to the accelerators and technologies, and they are also innovating at the same rapid pace and building the primitives that the enterprises can readily consume. So with that said, we're excited to have amongst us Temple, who is one of Canada's fastest growing startups. And a trusted AWS customer. So Temple has been using AWES to power their next generation design and engineering platform. The thing is they have the ability to transform a spec into a real-time production application that is ready to use. They are also a great example of what builders can do with AWS. So they will, they will demonstrate the innovation that they are doing with the GAI applications, and they will also walk us through the journey of how they evolved, migrating from another cloud provider, continuously modernizing and innovating using AWS. I welcome the Temple Labs team to join us and walk us through the journey that they had. Thank you for being here. Thank you. in the corner here. Like, go sit down. It's fine. All right, so we have a few fun things planned for you guys today. It's a little different given the silent room, so we're gonna do our best to kind of echo through that but before we really jump into things, I wanted to do something a little bit fun and play a video for you guys that really introduces the scope of tempo but also hopefully hypes you up in the sense of understanding what we are doing today. And then I think audio should be good. All right, so I hope you guys enjoyed that little, the little clip there. We have a few things planned for you today that are gonna take it beyond the traditional, move a little bit into the danger zone as we actually show you guys what this is all about. But before we really jump in, let me briefly introduce myself. I'm Brandon. I'm the head of growth here at Tempo. So what exactly is Tempo? Tempo is the best design tool for design engineering. It's a visual platform like Figma or Webflow, but under the hood, it works on top of any um production react code base, just like cursor, VS code or any other ID. I like to kind of simplify this and say it's almost like Figma and your favorite ID had a baby. We bridge that gap between those two. Tempo completely eliminates the traditional handoff between designers, PMs, and your development team. Where effectively your design team or your PM is able to open their own PRs and push production grade code. What this has represented is that teams that are deploying and using Tempo effectively are seeing on average an acceleration of shipping features at 40% greater than what they were before. But given that we have Kevin here, our co-founder and CEO, I'm gonna let him dive into that a little bit more because frankly speaking, he is much better suited to talk about that. I like to call him our chief product, if not prompt officer, because frankly that is what he does best. So a little bit about Tempo. Tempo is a venture backed early stage seed company. Um, we released our product in December of 2024. Since then we've had over 160,000 people use that product with an average growth rate of about 22% month over month. I'd be remiss to not give a shout out to Kevin here to add a little bit of credibility to the team, but also our other co-founder Peter, who operates as CTO. Before founding Tempo, they were part of the founding team at a company called Perpetua, which sold for 100, 150 million. I can't remember the exact number, but somewhere right around that. Um, and lastly, I would definitely be remiss if I didn't do the shameful shout out. That we are a YC company, so in true traditional fashion, if any of you are from the Bay Area, I can guarantee that you've seen almost every single founder have this picture and. It wouldn't be an introduction if I didn't take the opportunity to at least show that. So before I really jump into this, you know, today I actually saw a tweet and someone said, vibe coding is dead. What is happening in 2026? Frankly speaking, this is what I want to differentiate ourselves from. Tempo is not just another vibe coding tool. We're not out here marketing, you know, what if you could just build or design a SAS dashboard? Cause frankly speaking, We already know that's very possible. This has already been highlighted. There's a lot of tools out there that can get you that surface level interaction. What we built is a solution that goes beyond the superficial. Tempo is a space for designers, developers, and PMs to actually collaborate. And more importantly, enable your teams to ship production grade code. AI has really gotten good enough for PR PMs and designers to ship real code, but this isn't happening at an enterprise level. Why? Because, frankly speaking, designers like using Figma. PMs, they like using the traditional scope of linear or Jira. But that isn't helping you at a business level, that's not helping you move faster. And so, this is the problem that we are solving at Tempo. To show you aren't kidding, we're actually gonna take this a little bit of a step further, do something a little bit dangerous, which generally speaking, you probably shouldn't do, um, when you don't control all the factors, but we're gonna do a live demo. So we're actually gonna show you guys right now. I'm gonna call Kevin up here, and we're gonna pull up our app, and we're gonna do this in real time. Kevin was supposed to record a fallback video in case the internet didn't work. He didn't do that, so we are living extra dangerously. Um, Kevin, are you ready to go? We're good. OK, swap it over to the other one. Alright, thanks Brandon, nice to meet you everyone as Brandon mentioned, my name is Kevin. I'm co-founder CEO here at Tempo, um, and when it comes to Tempo, like Brandon mentioned, there's really two key differentiators, uh, versus a lot of the other sort of vibe coding tools that are out there in the market. Um, the first is that we integrate with production code bases, so it's not just about generating sort of prototypes in a silo that are effectively throwaway, but we actually sit on top of your existing production react code base, um, and so what you're seeing here again, it, it looks sort of very, uh, very similar to a design tool, right? You have an infinite canvas on the side, you have a, uh, a layers and properties panel, um, and you have a, sorry, let me just get rid of that right there. And you have a, you know, styles property editor on the side over here. But the most important and interesting thing is that this is a live running React app and it's running on a Docker container in the cloud so that this whole environment is actually collaborative just like Figma, right? So just like you can send a Figma link, multiple people can edit things, um, at the same time. You can actually send a link in tempo and a designer, a PM can actually edit the same code base because it's, uh, it lives on a container in the cloud and so you know we have a lot of cloud infrastructure we'll, we'll talk a lot, a lot about which, um, powers this experience, um, but. That is the high level scope of Tempo now when you go to Tempo, you'll notice, you know, just like a traditional, um, sorry, let me just refresh that here oh we gotta log in. Just like a traditional vibe coding tool, of course you can uh start from scratch or you can enter a prompt and build something from scratch, but the most uh value and and our biggest power users all import their existing code base from GitHub which you can do in one click. Now Of course you can vibe code. You can use AI, but the, the second key differentiator with Tempo beyond the fact that it integrates with production code bases is that we give designers and PMs familiar tooling and we give them the level of control that they're used to from a tool like Figma. So designers can use Tempo to control every single pixel by hand, and I'll give you a quick demo of what that looks like. So here in this case you can see, let me just, um, refresh this here. Let me just give me one sec. Well that that might happen to me. So here you can see I have a, or you'll, you see I, I have a sample Airbnb code base right now let's say I want to add to this home section a testimonials page. Um, what I can do is I can actually go to an online marketplace, uh, which, uh, such as reactcomponents.com, which is another property that we own, and I can look for something like a testimonials page. I can look at what other designers have created in the community, and what I can do is I can actually just copy, uh, this component. Let's say we wanna use this testimonials component looks pretty good. And now what I can do when I go back to Tempo is I can paste it on the canvas just like I would in Figma. And so there you go, here's my testimonials page. Now if I want to add this into my code base, I can just drag and drop it and what you'll notice is if I go to the actual underlying code, uh, you'll see that Tempo has added the code for this UI component in the correct place under the hood. Now, obviously this doesn't look that great uh because I, I need to customize it and make it look better for my, my application. So I'm gonna reduce uh some of the padding that's here. I'm going to edit a, uh, the copy here and actually it's duplicate. We have another hero section, so let me delete that altogether. Uh, one sec. Let me delete that altogether, let me edit this copy to say come to Vegas uh for reinvent. And then I can make changes such as uh for example let's say I wanna change the uh color of this element. I can change this to amber and maybe I can add a. Just for fun, a, a blur. OK, that's too much blur. Uh, let's dial it back down a bit, uh, and when again when I look under the hood, you'll see that every single change that I made, even though I haven't used AI once, I haven't prompted once, I've just been controlling every detail by hand, you'll see that tempo is editing the code correctly under the hood, and all of the code changes are actually staged here for me to be able to just enter a commit message. I can push this up to GitHub, or if it's not all the way there, I can just share the link with an engineer and they can sort of take it from here, um. And then like I mentioned, the beauty and the power of this is that you don't have to, like, for example, this is not like it's sort of like you're not ready to push this to production, but it's as good as it's much better than a sigma design, right? But about what we're seeing is about 60 to 70% of the code that PMs and designers generate in tempo is actually being able to be reused by front-end engineering teams, and that is a is a dramatic change to the traditional design handoff and teams are shipping about 40% faster, um, using Tempo. And like I mentioned, the whole thing is collaborative so I could send this link to Brandon and you know I'll show you another uh you know I sent this link to a friend of mine, a colleague of mine earlier and they created this beautiful animated tabs component, uh. I won't go into the full demo today, but you know, Temple also builds very deeply on top of storybook. So if you have an enterprise design system that's in storybook, every single one of your stories is ready to be drag and dropped and reused just like an asset in in Figma. And so here in my storybook, like I mentioned, someone from my team added an animated tabs component and so again now, uh, I'm sure some of you guys remember the uh beautiful Airbnb release that they did. I think in the summer I can just drag and drop this new animated tabs component. And uh boom you know here's my my beautiful new animated tabs component so the whole thing is uh collaborative the whole thing is uh built on top of your production grade your production code base and your production design system uh built in storybook. And lastly, just to sort of end the demo, there's a lot more that I'd love to show you, but just to end the demo, um. Some of you might and a lot of customers come to us saying like, hey, this looks great, you know, a lot of the vibe coding tools they're good for little toy apps, but can it actually handle a production code base, right? Can it actually handle a large enterprise grade code base? The answer is yes. We have been working extremely hard for months and months, and my favorite demo to prove that is actually. Tempo on Tempo. So what you're seeing here is actually the Tempo editor recursively inside of itself and so Tempo, as you can imagine, is a very heavy, very complex front end app and our team and our designers and PMs actually use Tempo to uh contribute code directly to our code base. And so if you can handle our code base, most likely can handle yours if it's not too legacy. Um, so yeah, that sort of wraps up the sort of demo hopefully that gives you a taste of what is possible in tempo, um, but what about AWS and the migration to Bedrock? Let's talk about that, um, and so with that, do you have the clicker. And it's not there so I'm not covering you. OK, perfect. Um, so now it's where, where does AWS come into play? So like I mentioned, you know, we have these cloud dev environments, right? We're spinning up containers that are running your production code base in the cloud where multiple people can edit similar to Figma, right? With designers and PMs, you know, they don't necessarily wanna keep their git up to date and set up their end variables and maintain the dev environment. It's a lot of friction. So, um, we always believed from the beginning that it had to be a cloud hosted experience that felt like Figma. It's just a link that you could send so that we could make. Collaborating on code feel like Google Docs, you know, so people can just, uh, code is just like it's a relay race, right? It's something a PM generates something, a designer generates something, and then hands it off to an engineer, and they take it from there and ship it to production. So cloud was a big, uh, there's a lot of cloud infrastructure that powers Tempo, and our team was previously on GCP, um, because we built our previous company on GCP. We love GCP primarily for big query. We were in the ad tech industry, um, in our, in our previous startup, and so we were building on GCP. Uh, and like any startup, you know, you do things that don't scale and so this infrastructure that I mentioned, you know, you could imagine you want to spin up, uh, pods and containers like very dynamically to handle, you know, large changes in load and manage costs and stuff. We didn't do any of that. Uh, what we did is we just had like a massive, massive, the biggest machine we could possibly find on GCP and we hosted all the containers basically on one machine. And then when we grew we were like oh snap we were like OK we're gonna you know fix up our infrastructure later we didn't and then we went kind of viral in March which I'll talk about and we basically just uh. Ultimately got 5 huge machines and just routed users between them, but we were at this point in March where we were basically completely screwed. Like our infrastructure was costing us like way too much money. It was not scalable. It was a real pain in the butt to to manage and more importantly, our cloud inference bill for LLMs we hadn't even optimized our AI agent for. Economic costs and so like you know how much a customer pays us and how much it actually costs us in terms of LM costs I'll just say we weren't very, very disciplined about that in the in the early days and in March or April we basically got hit with a $250,000 LLM bill and we were not generating $250,000 from our customers in in that month and um we were really, really like scratching our heads being like, OK, what exactly is our strategy gonna be? Now most of that LLM spend was with anthropic and as I'm I'm sure you're aware, um, anthropic and AWS have a very, very special relationship um much deeper at at least at the time. Uh, than really any of the other cloud providers and so that's really what brought us initially to Bedrock and two of the challenges that I mentioned that we had were, were, were costs, but the second was also rate limits and so we were looking for, you know, how can we scale up our availability, our capacity. Anthropic was giving us first party support because we're a YC company, but still, you know, even they were struggling to scale and whatnot and so that's sort of how we got involved with, uh, the Bedrock team and Amazon and we were kind of like, hey, help, uh, where do we go from here? My turn again. The risky part of the presentation is over. All done in a live demo. Um, OK, so transitioning now in terms of talking about the AWS journey. And it's really important to consider uh Tempo under the scope of the startup lens simply because a $250,000 LLM bill for an enterprise company probably isn't going to cause that much of, you know, anxiety, so to say, but for a seed stage company, that's massive. And so when we think about the bigger picture here, and when we think about cloud computing. It's relatively become a level of a commodity. Access to cloud computing is quite vast. There's a lot of different resources, options available, um, especially being a YC company, we're afforded a lot of luxuries in the sense of promotional kind of levels of pushing, pushing us in a certain direction or them trying to give us credits. And so what led us to being such an outspoken supporter of AWS it's honestly fairly simple. Um, we chose to work with AWS because they were willing to invest in the future of our tech. But more importantly, they were willing to invest in us. And I'm not talking just about credits or resources. I'm talking about the fact that when we have a problem, and we're texting our AWS rep at 10:00 p.m. at night saying, hey, there's something wrong with our rate limits, they're responding that night. They are truly operating like a partner. And not just from a technical perspective. Being head of growth, obviously I'm looking at the bigger picture. How are we deploying, how are we growing? Working alongside the AWS startup team has been instrumental for us, especially as we start to think about our more upmarket trajectory and, and working more on the enterprise level, which I'll talk a little bit about more here in a second. So like I said, it's more than just credits and resources. It's opened up significant doors for us in terms of our go to market motion. Um, as we improve the product and as it's become more stable, like I said, we first launched it in December 2024. The video that we showed earlier was actually that initial video, and one of the things why we want to show it because it's a tryer beta product. But what happened was, within 3 to 4 months, we had literally tens of thousands of people testing that app. And we quickly realized that we needed to grow up. And as we've grown up and as the product has stabilized, as we become more confident in it, we've started to expand our scope in terms of working with more enterprise customers. Having access to the AWS marketplace has also enabled us direct access to some of those groups. Not in addition to obviously the networking events, the conferences, um, but a concrete example that's a really big, big opportunity for us is the fact that we're now working alongside a big four consulting firm to deploy this at scale, and a lot of the facilitation for that project is gonna be with access to the AWS marketplace. All right, now to actually talk about some of the more technical details, gonna pass it over to Kevin. Um, awesome. So migrating to Bedrock again, uh, was very easy. Again, frankly, number one, we had a ton of YC credits, uh, that we wanted to use, and Bedrock was actually the only big cloud, uh, sorry, just to be clear, as a YC company you get about 250 to 350K in credits from each of the different cloud providers, but, um, Bedrock was really the only one that allowed us to apply those credits towards anthropic LLM inference, which, as you can imagine is like a huge financial incentive again if you look at all the data, Anthropic is by far the best LLM. for enterprise API oriented use cases. So that was the initial huge value add, um, but the second thing like I mentioned was, uh, support in terms of rate limiting. So now again, the support that we've been receiving from, uh, the AIS team has been incredible. Whenever there's a new model released or anything like that, we're working very closely to ensure that we have a, a smooth, timely, and swift transition from, you know, an older model to a next gen model and that we have the sufficient rate limits to be able to achieve the capacity that we need. And lastly, and this is another really cool thing is, you know, we build on top of the, uh, Versal AI SDK, which is an open source SDK, and you know, even in the earlier days there were certain, uh, new features that were getting added to, uh, AI, uh, LLM APIs like for example prompt caching again for those of you that aren't familiar with prompt caching, it's, um, something that you can use to really reduce, uh, your cost of LLM inference and the Bedrock APIs at one point in time when we transitioned, uh, didn't support the exact prompt caching. Uh, APIs that were needed for, for use within the Versel AI SDK, we contacted Amazon and, and really within a few days they were able to rectify that whole issue, so we were able to continue building on top of the Versail AI SDK on top of Bedrock. So I have nothing but, you know, uh. Uh, good things to say about our migration to Bedrock and really the, the Amazon AWS team has just been incredible and again I've worked with both of the other cloud providers as well, um, for these types of workloads and, and really no one compares, um, and a large part of that is, is the sort of very deep relationship with Anthropic, um. Beyond that, as I mentioned it, you know, so we had this huge LLM inference bill, but we also had this huge cloud infrastructure bill, um, for our, you know, containerized Docker, you know, uh, cloud-based collaborative dev servers, and Amazon, uh, as part of their migration program actually invested, uh, quite a large, uh, sum into helping us build that out, so they said, hey, not only are we gonna give you incentives to come over to. Uh, Amazon, AWS, but we're actually going to invest in resources to help you modernize your architecture and make it truly scalable and so we, we move from sort of like these, uh, you know, routing between huge machines to a proper, uh, Kubernetes based, uh, containerization. Infrastructure that allows us to scale up and down and auto heal and set really good you know pod level permissions and security and all of those and high availability, all of those different things and that was actually code that we didn't write. It was actually one of the Amazon partners that basically we invited them into our code bases and they worked really closely with our team and. They did all of that work over the course of about 6 to 8 weeks which I can talk about and not only did it allow us to be more scalable, more secure, all of these different types of things, but actually reduced our cloud cost by 5x, um, so we went from about 25K in sort of monthly cost to 5K in monthly cost to power the infrastructure, um, uh, for our user base which was again just I mean incredible. I have nothing but good things to say it was like it was incredible that they they invested in helping us, uh, do that. Um, And Ops grew, some of you may or may not have heard of them. OpsGrew was the specific partner that Amazon, uh, invested in that helped us, uh, build that whole migration and. Now you know the whole, like I said they completed the whole project in about 6 to 8 weeks. Our team then took on all of the code, all of the infrastructure we now own it and maintain it and continue to build on top of it ourselves and. Uh, it's funny, you know, most of our competitors use, use like more like managed cloud solutions like uh fly.io or um some of those other types of solutions that you might be um familiar with. In our case we decided to, with the help of Amazon, I mean we, we were always planning to basically handle our infrastructure by hand. That might be either the best decision we've ever made or the worst decision we've ever made. Time will tell because it takes a lot of investment and a lot of maintenance, but, um, we're already starting to see really cool benefits like, um, for example, we, we run, uh, TypeScript language servers and tailwind language servers to power things inside of our app, so. For example, if a designer wants to change a prop on a react component right in Sigma, you just have like a drop down of like here's the different props that you would want. We actually run a TypeScript language server which is what powers like IntelliSense and VS code. We actually run a TypeScript language server on our pods, and that's what we use to then, uh, power, you know, uh, sort of Wizzy Wig props editing experience that feels like Figma and so like I, I. Again, we believe that the scope of things that we need to build won't be possible and sort of the, the, the fully managed, fully hosted solutions and so we did feel it was important to own and and manage our infrastructure in-house and Amazon, uh, and Opscu really made that possible, um, with speed, uh, and, uh, with speed and, and, and, and tremendous cost efficiencies, um. And that's like again the partnership with Amazon has only been getting like deeper and deeper and deeper so I talked a lot about sort of our cloud infrastructure migration but they've actually been. Co-building with us like these true R&D projects that are are focused on innovation. So one of the projects that we did was around a basically uh what we call the PRD agent, which I'll explain in a second, but essentially a new type of agent, um, to help product managers specifically, and you can see like there's a it's a screenshot of a paper that we actually wrote together, Amazon and Temple, um, explaining essentially the work that we did. Uh, and, and so you know, beyond just migrating and making our cloud infrastructure scalable, we actually work together with Amazon to do R&D on what's truly like a, a cutting edge, uh, work flow within the industry. So what is this PRD agent? Essentially again it's a lot of information there, but essentially at a, at a high level. Every product manager or every person who's an expert in a certain product has a knowledge graph in their head, right? This is my project. Here's the, sorry, this is my product. Here are the features that exist. Here's the scope of the functionality. Here's like the limitations. Here's what's supported. Here's not what, what's not supported. There's all of this context, right? And very, very few organizations other than maybe like very large disciplined enterprises have all of that knowledge and that context written down somewhere in a way that it can be parsed by an LLM, right? All of that context lives in your code. But there's so much noise around the implementation details and it's sort of a different layer of abstraction from product knowledge and so the project that we kicked off with AWS was hey can we take a code base and basically run. Like a dramatic amount of LLM preprocessing on top of that code base in order to parse out and extract out essentially each of the PRDs that explains the scope of functionality within this application, right? So, uh, and we did a test with a large open source code base and the output from that code base was essentially a set of I think it was 15 or 20 PRDs that in detail. Detail explain what's implemented, the limitations, but at the right level abstraction and that output is now used as context for a PRD agent, a PM agent that we're building such that you can now interact with an agent that knows everything about your product, right, as, as if they were like a senior PM that was there from the beginning of your code base, uh, and that level of context I think just. Accelerates the nuance and the types of tasks and the types of PRD writing that you can do with with AI and so uh anyways super super exciting um project that's gonna be released early next year um but we've like I said we've been working with the the AWS Gen AI IC team to develop this out we've had, um, tremendous, uh, tremendous results that we've been seeing and please give it a try early next year or or contact me if you'd like to be part of the beta, um. And you know I just put the screenshot here, but like the level of depth that the uh AWS uh R&D team was able to go is much farther than us as a startup would have even been able to go ourselves. And so not only is AWS helping us do this, but they're taking the project further beyond what we would have even been able to do, um, just on our own. Um, so. With that, that sort of describes our migration journey from uh to to AWS to Bedrock and our partnership with AWS and and with that I'll pass it over to to Viidi. Awesome. Thanks. Thank you, Kevin and Brandon. Uh, that was an excellent walkthrough, yeah. Um, so the, the thing is, we would allow everyone, every one of our customers in here to have the same journey as Temple Labs, right? And we want to provide you with some of the accelerators that can get you started in the same journey, be it able to get in the path of migration, continuous modernization, and innovation, and innovation at scale. So one of the accelerators I would love to introduce, if you're not already familiar with is the migration acceleration program or called the map program. So AWS over the years, uh, we, I would say 19+ years of migrating tens of thousands of customers, we have compressed the experience that we have learned into a framework to be able to support you on large scale migrations and continuous modernization. To be able to move at scale, you definitely need multiple aspects. You need a proven framework or a methodology on how you go about carrying out a large scale migration and a continuous modernization so that you can unlock the innovation side of things. You also need a set of purpose-built services, whether you are dealing with compute, storage, or the latest G AI services. How do you able to move from one to the other without having to risk and also be able to kind of And remove some of the undifferentiated things as you are carrying out doing this. There is also some of the services involved and as Kevin mentioned, there there are certain specialists that are involved and arguing about moving from one technology to another or moving from one provider to another, so it could be a set of AWS professional services, the experts that are in-house who would be able to come provide the blueprint. And there are also a larger partner ecosystem who would be able to take some of these blueprints and be able to apply it at scale. And the thing is, uh, I know both Kevin and Brandon mentioned it multiple times, any migration modernization journey means you are keeping the lights on while you are carrying out the transformation. So there, there could be a bubble cost, double bubble cost, triple bubble cost if there are licenses involved or not, and you'd need to be offset it. And that is where the AWS funding comes into play. So we, we look at the long term and be able to offset some of the, uh, some of the cost that goes into this transformation. So AWS as an organization is investing in the long term to be able to be successful in this journey. So one accelerator is map. You can reach out to your account counterparts on how to leverage map when you have migration modernization projects. The 2nd accelerator that I want to talk about is the GAI Innovation Center. The thing is, as you heard from Kevin, A lot goes on, especially right when you have Gen AI-based workloads. The first thing is around the model selection, the right model that you need to choose, and doing a whole evaluation around, hey, is this the right model for the use case that I have? Do I need a race car or would the utility car be good enough for the use case that I have in hand? And after the model choice and the model evaluation, you would have to take the model to the production. So maybe in timing to the market, the race car would have addressed the use case. It might not work well from a cost and efficiency standpoint. You would have to either in a race car maybe the braking system was really good and that was the only thing that you need for the use case so maybe you could use the braking system from a race car and apply it in your utility car, be able to save cars. So that is the whole thing around model distillation, right? So could you have a teacher children training model be able to get in to do this model distillation so you can take some of the models to production, right? And there is also this, the fine tuning. One of the things that uh Kevin mentioned was around being able to use the knowledge that you have and be able to have the LLM agents be able to use the knowledge base. Um, so the, the thing is this is around fine tuning. So taking to the model, doing fine tuning, being able to do the model distillation. And doing this continuous optimization, there are new models coming out every other week from some of the key model providers. So how do you stay on top of these purpose-built models for some of the use cases that you have and how this end to end journey is going to look like? So the thing is GAI Innovation Center brings together this one team, a team of data scientists, a team of specialists, a team of essays who would be able to work with you either hands on or through advisory, strategic advisory or technical advisory in a 5 to 8 week time period at no cost for you to go through, work with to make your use case a reality using Gen AI. I would recommend this as another accelerator for your teams to consider, to engage with us, reach out to the account team on how to bring in GAIC. Um, so the way they engage is, it is two prompts. One is the use cases where you're using primarily the model inference. Another use case is where you are actually doing the model building from the ground up. So there are based on the two use cases, there are multiple engagement models that are available. It could be just at the advisory level where it is a couple hour workshop, getting into things and providing you with the right next steps or the pathway on how you can self-service the use case implementation, or it could be a deep dive on a 5 to 8 week engagement. Thank you, yeah, and everyone, we'll be around here if you wanna come chat for a few minutes or whatever. Awesome, thank you. Thanks. Thank you.