---
video_id: XmBDchavCW8
video_url: https://www.youtube.com/watch?v=XmBDchavCW8
is_generated: False
is_translatable: True
---

All right. Welcome everybody. So, uh, my name is Jordan Ratner. I'm a senior general of AI strategist, uh, at AWS. Uh, and I have a question for you. How many of you have made a bet since being in Vegas this week? Oh, almost everybody, OK. A lot of risk takers in here. Well, we have some really great news for you, uh, with FanDuel's AAI, a general AI betting assistant, your experience for bets just got a whole lot easier, and I have here a couple of friends with me, Rohit and Santos. You guys wanna introduce yourselves? Hi folks, I'm Rohit, uh, senior software engineer at FanDuel. Hey folks, I'm Santosh Waurplia, senior solutions architect with AWS Betting and Gaming Team, and I work with FanDuel very closely. And so we know a lot of our customers have questions about how to go from nothing to a full scale production generated AI solution, uh, especially in such a highly regulated environment like FanDuel is, and we wanna walk you through that journey step by step in this talk. We'll start off with a baseline of who FanDuel is, what their goals are. We'll walk through the entire history of AAI from inception, prototype all the way through to production rollout. And then we'll dive in, dive into how we did it. So what does the architecture look like? What challenges did we go through, and how do we overcome them? And finally, of course, the metrics. Is this working? Uh, and how do we know? So why don't we start off with that overview of FanDuel. Thanks, Jordan. Um February 9, 2025, Super Bowl Sunday. I was in Porto in Portugal. The office was buzzing with excitement, and after the game, we sat down and we looked at a few figures. We had over 16.6 million bets just during that game. Nearly 3 million active users. During the during the time of the game and We peaked at 70,000 bets at the very, very height of it. FanDuel is America's number one sports book, and there's a reason for it. Uh, I want to talk to you a bit about Before we get into AAI, I want to talk to you a bit about what FanDuel is, what we do, and also uh what are the challenges we face generally. And We can think about FanDuel as a sports book, as an e-commerce platform. Uh, you have to have. You have to give the customers the product, list it out to the customers. You need to make sure that the customers can. Use the product, pick the product up and order it. You have to have an order processing system. You have to wait, you have to have a way to fulfill that order and make sure that the customer gets what they want. So this is the same thing in in a betting platform as well. We have Jalen Hurts is going to score that touchdown. That's the product. The ability for the customer to pick up that product, bet on it, place their wager, see how the wager progresses, and finally settle that bet so that they get the money. This is what we have to do in an e-commerce platform. It's, it's similar to that. We can also liken it to a trading flow. Prices change constantly. Imagine a basketball game. It's so quick, it's so fast. Um. We need to make sure that the prices are up to speed. Jalen Hurts is going towards that goal line. We need to make sure that. The probability of that the event happening is so much closer now we need to make sure that the prices are correct at that time, so. These are the two Extremely challenging. Scenarios and that's what we face every day in a sports book as well. Uh, some of the challenges that we see otherwise in general is sports betting in the United States is very regulated, a highly regulated sport, um, industry. Um, We've got regulations within the state itself, so a state, one state would allow. College Betting. Some other state might not allow that college betting. Um, some states allow college betting for only, uh, sport teams which are not within that state itself, so it's very complex. Some, some states don't allow some sports at all. And we've got to make sure that these rules are centralized so that they're available across all states. Then there's the data requirements as well. In terms of data requirements, they're strict they are. Mostly regulated by the Wire Act. The IW Act makes sure that we have to. We have to ensure that all bets placed within the state are within within the confines of the state itself, so we need to make sure that infrastructure, the data, uh, the bet transactions all remain within that state. So the data requirements are quite. Uh, important as well for us. And you can see, you can think about the latency. The central rules lie in a different place. The, the state-specific rules lie within the state itself. So there's a, obviously, there's a latency problem, and that's where AWS helps us a lot. Um I've spoken about the fast pace, so I won't bore you again, but there's one point that I'd really like to hone on is responsible gaming. Uh, FanDuel takes responsible gaming really seriously. Uh, we, we aim to be, to set the standards as a responsible gaming operator. Uh, with that, I would like to go to the next slide where we talk about a few principles of grandeur. Don't worry, I'm not gonna bore you with all of these. It's just the three that I really want to talk about because that's what we use for, for ACI and, uh, they kind of like relate to ACI as well. Uh, one of them is everything begins with the customer. Uh, and that's really, um, ingrained in our culture. We need to make sure that everything is for the customer, that the customer is safe, again, coming down to responsible gaming, and also that the customer is entertained. Uh, the second thing is. ACI wouldn't have been possible without the support that we have from our leadership. Anything is possible is is there within. Uh, within FanDuel, everywhere, uh, and the, the support and, and the, the push that we got for when LLMs came out, when, uh, when generative AI came out, uh, from the leadership team was, was phenomenal, and that's how we got AAI as well. Uh, and again, lastly, winning with integrity is, is important for us, um, because. As I mentioned earlier about responsible gaming, it's winning, but with integrity, making sure the customer is safe, making sure that they're entertained. Uh, with that, I'm gonna give it to hand over to Jordan to talk about how we went from inception to prototype. Thanks, Jordan. Thank you, Robert. Much appreciated, and Rohit was serious when he said that FanDuel took Jenny I seriously. Badal started in 2024, very early on, and they brought together a global hackathon with hundreds of people from all different walks of life, engineers, data scientists, lines of business leads, and all the way up to their senior executives and C-suite. Everyone was a part of this hackathon to try to find the best opportunities to apply GEI to help their customer experience and to improve their internal operations. And of course being the customer centric company that FanDuel is, they landed on this idea of AAI, an autonomous betting assistant that would help customers create a much more seamless experience than they ever have had before, uh, in their gaming life. Now, there was a challenge. In 2024, especially early 2024, very few people actually knew how to deliver a generative AI solution. And so to build that muscle, this is where FanDuel reached out to AWS's Generative AI Innovation Center. Now the Gen AI Innovation Center is a global team of strategists and scientists who are fully dedicated to delivering generative AI solutions at scale. That's all we do all day long. AWS has actually invested over $200 million in this team to make sure that we had this capability to support our customers in delivering what they needed even when starting from absolutely nothing. Now FanDuel started from zero. They had incredible engineering talent, but they didn't understand the GEI tool sets. They had never built out prototypes before, and so they needed a little bit of support. And for any of you wondering how to get started, you don't need hundreds of millions of dollars. You don't need a team of 50 people or 100 people. FanDuel actually. Started with just one engineer that's it. It was 1 engineer and 3 or 4 data scientists from the GII Innovation Center helping to guide them through in an advisory engagement which you can think of like office hours, meeting and chatting about, uh, what the solutions look like, what the services would be used for, how to set up the solution, doing research. Sharing code, uh, and that's where it started, so really humble beginnings going from this idea, uh, bringing in that single engineer, uh, to learn about this work and that engineer actually became the start of a snowballing effect within FanDuel and very quickly this engineer became the, the trainer so we had the trainer, the, the train the trainer moment. And we've snowballed this just from AAI at the beginning, all the way up to 15 engagements with FanDuel, 10 of which are already in production, and we have another 20 or 30 on the docket for next year alone. So we're really excited to be partnering with FanDuel to help them build this muscle and continue forward. So something else that I'll mention is uh that we have a bunch of different programs available to our customers. So you'll notice there's a ton of them on the screen here. I'll focus you on advisory and hands-on. So advisory is what we did with ACI. Uh, it was just, uh, the engineer from FanDuel Building and us guiding. We also have a hands-on engagement where the Gen AI Innovation Center actually goes into your environment and builds out a solution alongside your engineers. OK, so how do we walk through this entire engagement? How do we go from, from nothing to something? Well, the first thing we do is we validate the idea. This is so important. If you have the wrong idea at the start, you're setting yourselves, setting yourself up for a lot of challenges that are just unnecessary, and it doesn't take forever to do it. We did it in a single conversation. So we made sure we had what we call a diagnostic call for 60 minutes where we brought in all the senior executives and the engineering leads, the data team, and we walked through what were the challenges, what were the opportunities that they wanted to tackle and why. And within that single conversation it became very, very obvious that a betting assistant would be really valuable for their customers. And so with that one conversation we sprinted forward and we decided to take this on, uh, to continue to build. The next step was the discovery workshop. So this is where we introduced the GII Innovation Center's incredible data scientists. And they came on alongside myself uh to walk through with FanDuel's, uh, entire staff of engineers, data scientists, business leads to identify the customer, understand what the challenge was, determine the KPIs of how to measure success, uh, determine the challenges of getting, uh, getting this going, understanding all the requirements, sharing screens, looking at data, uh, and really diving in to understand how to make this thing successful. Once we had all the requirements, we then started off with the engagement. And this is when that engineer went to work alongside our 3 to 4 data scientists. Now of course once we were ready, we handed it off to Rohit's team to actually productionalize the solution. So what did we find out? We did all of this work. We walked through with all the different stakeholders. What is it that we found out about the customer? Well, the first thing was there's a consistent repeatable format that the customer walks through. They first research what they want to bet. They look up all the information, they understand what is the record of Federer versus Nadal over the past 10 years, and in recent months. Then they go through the discovery phase. OK, now I have all this great information. What can I actually bet on? What is available for me to deploy this knowledge? Then they construct the bet. They say, OK, great, uh, we have all this information, we know what we wanna bet on, we see the bet in front of us, uh, how much money do I wanna put down? Uh, maybe I wanna build a parlay which is much more complex, and then finally executing, pressing that button, uh, to actually put down that bet. And what we found was that experience was actually filled with friction because when somebody determined that they wanted to make a bet and they wanted to research something, they would leave FanDuel's app, they would find their favorite search engine, and they would ask the question of how many points, how many of the top, uh, who are the top 5 leaders in hits, as you can see in the questionnaire, and they would get this information from Google. They'd get a notification on their phone from TikTok, and boom, 5 to 10 hours is gone. They look up and all of a sudden their bet has expired. So this is a really, really common occurrence and so we knew in order to make that customer experience better, we had to create a single in-app flow that the customer could walk through all entirely within the FanDuel app. That was really, really critical. Now having that single experience that handled the entire of the journey was great to drive conversion, but we made sure to keep it an assistive. Uh, a assistant, we did not go to full autonomy and when you're considering what to build, you're gonna hear a lot about autonomous agents that handle things end to end. That's really, really great and when you wanna deploy that in certain scenarios, uh, where you don't have any business with human judgment in the decision, that's great, but in this case our customers wanted to retain control. So throughout the betting experience they wanted to be able to do the research themselves, stop, consider things, do the discovery themselves, stop, consider things, and onwards. But what they didn't want to do was the hours that it takes to look up all the information and do the physical searching, putting all the information together and doing that analytics. And so we focused this assistant on making sure that all of those elements were seamlessly handled while maintaining that control at every single step of the journey. Uh, and of course we wanted to measure how we were successful with this solution and we did it in a couple of different ways. So one was an event-based way. So this is something that's just being counted on its own. The system is understanding how many times somebody's actually used this AAI program, uh, what are their drop off rates? Uh, did they leave something in the cart and not actually continue forward on that bet and let it expire? Then we have time-based considerations. So how long does it take to go from the beginning of the journey all the way through until the execution? And then how long did it take at each individual subsection? How long did it take to construct the bat, uh, and all of these things are really important when you're starting to understand, uh, how successful are you with the customer experience. And the last one is interactive. So one of the best ways to get feedback from customers is just ask them, and this is where something like a CSTAT would come in. So what does this prototype look like after all this work? Well, you can see here it's really rudimentary. It's not that pretty, and we ask a point here how many points has LeBron had in the past 3 games? And the answer is this paragraph. It's unformatted. It's a string of metrics. It's really kind of hard to read, but that's OK. That's the point of doing these rapid prototypes. Your goal is to handle the tough job of proving customer value. That's it. And so all these edge cases that you might be thinking about, uh, you don't need to worry about that in this first stage. So if you ask this, uh, AAI in the prototype stage to make a poem about your favorite tennis player, it would gladly do so, and that's perfectly OK. We wanna make sure that we're proving customer value, uh, and once we did that. We handed it over to Rohit's team to actually go to production. So Rohit, do you wanna walk us through what it took to go from prototype all the way through to rollout? Yes, thanks, Jordan. Great. And then Video uh takes, takes us back uh. Good memories. Um, yeah, when we started the rollout, we had 3 things in mind. We wanted to ensure, uh, that the customer gets the most accurate information. Um. Thinking about how GN AI was working at that time, we were very unsure, so we wanted to make sure that that information is, is correct for the customer and accurate, and we want, if not, then we needed to roll back and we need to find that strategy as well. Second thing was. To learn because we didn't know a lot about this. We don't know how our customers are going to react to it as well. So we had, we wanted to learn about this as well. And the third thing is iterate. And that's why, as you can see over here, We went one sport at a time, one state at a time, so we slowly moved out into the bigger United States. Uh, the Alpha pilot was a very small number of customers, friendly customers that we used to give us feedback as soon as possible, understand how the chatbot works and how they work with the chatbot as well, and then we slowly went out to the beta release earlier this year in quarter 1, and then in quarter 2 we went to the beta 2. And then so forth, and now we're in the beta 3 release. Um, yeah, so that, let's look at what ACI looks like today. Uh, this is a short video. As you can see. Much richer experience than the prototype. Yeah this looks beautiful, right? Yeah, uh, and you can see how the suggestions come out. The customer types in something very vague like bets. Yeah, that's interesting. So, so our customers putting in individual keywords instead of full sentences? They do, yes, they treat it much like a, like a search engine. So there's just type in something vague and. Expect answers really quickly. And I noticed here that you have a lot of rich UIs, so these are dynamically being pulled based on what the customer is looking for. Absolutely, yes, yeah, um, so it's, it's more conversational like think about you as a, as a person talking to another person. What would you think when you say bets, for example? Yeah, well, I would have a question and I'd have a follow-up question, but what do you mean? Yeah, exactly that. Yeah, you can see it's come out to stats for a, for a player. Uh, no, this is interesting, so build me a parlay. So. I can see here as it's thinking about building that parlay it, how long does it typically take for a customer to build a parley? Uh, internal research shows that it takes a customer hours to research and build and construct a parley for hours to construct it wow, some of some customers, that's amazing. And, and we saw here it only took a couple seconds. Yeah, wow, that's an incredible that's an incredible improvement in the experience. This is quite an extreme example of responsive gaming, um, and we do see things like this, but ACI is very sensitive to these kind of questions and will respond immediately with a link to the responsive gaming page so that you can go there. I know I've been hopping on about responsible gaming for the last 3 slides, but this is very important for us in FanDuel as, as a responsible operator. Yeah, people, people do unfortunately in betting take risks that they can't afford, and so it's really important that we make sure that AAI is responsible. Uh, now we, we also. Mentioned in the prototype we didn't handle edge cases so in the prototype if somebody asks, say hey I'm I'm gonna put all my money and and lose the house, uh, it would say great good luck uh it, it doesn't handle that and that's OK in the prototype stage, but once you go into a customer facing realm it's really important that this responsible gaming is properly handled. One last thing I wanted to mention, um. ACI is not a recommender. It researches for you, it allows you to prompt it, to provide and discover the, the bets that you're looking for, the opportunities that you're looking for. Um With that, let's get on to some of the meaty stuff, uh, a little bit of a technical deep dive, great, and Santos, why don't you join us up here and I'll absolutely. Let's do it. Um I'll take you to the architecture. Sportsbook FanDuel.com up there is, is the, is your device or your application. Uh, when we first, when the customer first gives us a prompt, the first thing it hits is, is the AWS API gateway. And obviously, the gateway, um, will help us rate limit, will help us do the authentication and the authorization, etc. Uh, and then it comes down to the lambdas. Now, lambdas. have been like they've been the best thing for us because when we first started out, we didn't know about the volumes and the traffic that we're going to see. We didn't know what kind of volumes we're going to see. So they're very, they were the ideal solution because They're serverless, we can, they can launch when we want to, uh, memory is very low as well. Lambdas serve as our main compute here. Uh, they do the routing, and I'm gonna talk about that in the next slide when I talk about the data flow, but they do the routing for us, uh, and also they do, uh, the data collection. Um, on the left there you can see, uh, a couple of few EC2 instances and also, um, EKS clusters there. Now those form our main. Uh, services and data that we get for, for, for the, for the LLMs. Um, those are the sports book services. Um, yeah, so, essentially those, that's the data, those are the tools that we have. Um, Lasty Cash, Reddis, and Dynamo DB, um. Offers the um the caching for the data and also the memory for the LLM so I'll talk about that a little bit more when when we talk to the go through the data flow. The RDS over here is is is where we store the stats for players and teams, etc. So that's where the, the main stats for the players and teams come from. Um, bedrock is the shining star over here for us. When we started out, bedrock was phenomenal because we could quickly get Uh, the team working on it, it was pretty easy to, as a learning curve to come up and start using Bedrock. The choice of models was phenomenal. We could use any different models we, we wanted. We tried, um, Cloud Haiku. We tried, uh, Metalama, but we finally settled on Nova Light and Nova Pro because that. Delivers what we want for us and it's very cost effective as well. And Scaling is, is not a problem as well. Um, being serverless, Bedrock scales very well, and if you speak to Santosh very nicely, he'll put up the limits for you. Uh, and costs obviously are your tokens. So as long as you control the token, uh, your costs will be controlled as well. What do you say, Santosh? Yeah, I think, uh, you know, we've heard my buddies Jordan and Roe talk about how AA really started off as a hackathon idea, you know, went into rapid prototyping, you know, experimentation and the production rollout, and this just happened in a matter of a few weeks, you know, we're not talking months, years, quarters, it was just a few weeks. And so to be able to have that kind of rapid consistency, rapid application development cycles, it was really important for FanDuel developers to have consistent APIs, you know, constant uniform platform that they could experiment with, deploy it, test it, and then move on to production. And that's where Bedrock has really shined really, really well. And so you just heard Rove talk a few seconds ago that they've experimented with a bunch of different LLMs like Haiku, Sonnet, anthropic models, Amazon models, and they finally landed on Novo models. But then before that final decision there was a lot of prototyping, testing, evaluation. cycles that they had to go through in the absence of Bedrock, what it meant for FanDuel developers was they had to use several different SDKs that were proprietary to all these LLMs, build custom authentication frameworks around that observability is like another huge pain. There was all kinds of cycles that the developers would have otherwise spent a lot of their time trying to deal and solve, but then Bedrock. It just provides you with a single consistent API that you can use to then interact with hundreds of different models we host on Bedrock. All it takes for the developers is to just flip a model ID and then boom, now you're talking onto the LLMs of your choice. So that was really, really amazing for developers to rapidly prototype, understand what LLM works best for their use cases, and then quickly move on, focus on something that's more important. And also Ros just talked about scaling and cost, you know, as you've just seen, FanDuel went through several different rollout cycles. They had alpha stages. They had beta stages. Several new states are coming live. I'm sure AAI would probably end up there. And so it was really important for FanDuel also that the model or the medium where they're actually deploying these LLMs. Should also scale seamlessly, you know, FanDuel doesn't want to necessarily invest a lot of time in maintaining that infrastructure, and patching that infrastructure, keeping them always updated. And so that's where Bedrock, being so less seamlessly scales to whatever your requirements are. All we had to do with FanDuel and Rose's team was just ask them what are you expecting as your new. To to uh to be after your rollouts, how many tokens are we talking about? A RPMs requests per minute, and that's it. We just put in a request, we got the service limits bumped up and Bedrock did its magic. So Bedrock has really, really, really reshaped the way in how the Gen AI development life cycles happened at FanDuel and then also especially for AAI. Yeah, thank you, Santosh. That folks is a very high level view of the architecture. Um. Let's go on to look at the data flow. Uh, the data flow is a typical rag flow. Um, so you've got your, your prompt coming in to the lambdas, the ACI backend services, what I'm going to call it. Um As soon as it hits there, the first thing we do is we, we make an intent call, the get intent call that goes to the bedrock immediately. And that tells us what's the intent of the prompt. What does the customer want? Uh, and that goes back as a response to the lambdas which then decide what tools to use to collect the data from and that's where it would go either to the RDS or the EC2 instances or the EKS instances to pick up that information. Once that information is picked up, it'll then send that back to to Bedrock. Uh, to formulate, to enhance that information, that goes back to, uh, the lambda, which, which goes back to the customer and, and is rendered back to the customer in, in the, in the form that you see earlier. So let's take an example. We saw in the video, which was quite an extreme example, I'm going to lose my house. If that kind of a prompt comes in, And it goes to To Bedrock. Bedrock responds with This is a responsible gaming intent. Uh, once it finds that it's quite a static response to go back to the customer, so we, we put, put that page quite quickly. This kind of information is, is quite easy to, to send through because things like customer service or responsible gaming, we send it through really, really quickly. The second thing is Um, let's take the other response. Uh, so, there was a question from the customer on the video for stats about Donovan Mitchell. And if that, if that kind of uh uh question comes through, a prompt comes through. Bedrock will say, right, this is a stats response. It'll go to the RDS. Pick up the statistics that the prompt has picked has requested goes to to Bedrock and then Bedrock will formulate it and enhance it and enrich it for the for the prompt according to the prompt and send that back to the LLNs which then rendered to the customer. So the data flow is quite simple. It's essentially a Multiple intent-based workflow. Acquiring information from many tools and sending that back to the customer. Um, both, that is. In a nutshell, the architecture and the data flow of the system, let's look at some of the challenges we faced as we went through, uh, and also, um, how we solve them. Um Context is Quite key to to LLMs. In the video earlier, you might have noticed, the customer said, give me stats about Donovan Mitchell. The LLM responded with the stats. And then, The customer asked the LLM, give me. create a parley for him. What's the hymn is Donovan Mitchell. In a conversation, we know this, we know the pronouns when, when a person uses a pronoun, but the LLM doesn't know. We need to provide that context back to the LLM. This may seem quite naive, but at first we didn't realize that. But as we went through the alpha and the beta stages, we realized what was happening over there. The LLM would work for one player, one team, but then when there was multiple teams and multiple players, it became difficult for the LLM to understand what the context of the prompt is. Um, so, with a lot of experimentation and iteration, uh, we Managed to put in the Dynamo DB uh from, from my previous um architecture diagram, if you remember. And Dynamo DB now serves as the memory for the LLM. So when a new prompt comes in, we pick up the previous prompts of the customer and provide that to the LLM so the LLM knows what the customer is talking about. What I want to say here is, Gen AI is, is a very different paradigm compared to our normal developed test. Uh, fix bugs and release cycle, it's, it's quite, it's quite different from a functional perspective we need to think about it differently and we need to understand how the LLM works because it hallucinates, it gives out wrong results, etc. Um, the other thing is, Betting is quite a fast-paced. Um, Industry As you know, Imagine a basketball game. LeBron James is, if, if it's live, LeBron James is gonna score that basket for, for, for 20 points, and he gets 20 points. Is he going to meet that bet? How quickly is that gonna happen? That's difficult for the customer and difficult for the for the LLM to to understand as well. So when a when a person in a live environment asks for a parley, it becomes really difficult both for the customer to type that all out, but also for the LLM to respond back as well. So In this situation, what we've done is When it's the game is live, we provide them only with singles. Even if the customer asks for a parley, we provide some answer back. So we, we inform the customer that. A, a live parley is not possible, but here's this, here are some singles for Donovan Mitchell or for LeBron James. That's how we, we work it out. And in terms of latency. We had very high latency earlier when we first started out. Um, typically, we have this 1 2nd rule. When somebody asks for some question or searches for something, we need to respond back in, in, in 1 2nd. But for LLMs, that's not possible because they take a long time. So, we had a latency of about 12 seconds when we first started out, and you can see that on the graph over there. The P99 was 12 seconds. We did a lot of work on traditional work like trying to cache things. We saw the reddish cache that's where most of the data is brought together. So for example, if, if I want to bet on tonight's game, Jordan might want to bet on that as well. So the data available in the reddish cache makes it really easy for, for the LLM and the lambdas to pick it up. Um, most of the work that we did somewhere around August, uh, was traditional work, but we also did a lot of work on, on the models as well, and AWS was really helpful in figuring out where we could fine tune the models to get response back quickly. Um So then we, we rolled that out and we reduced our, our latency. We've come down to around a P 99 to 5 seconds now, but Over here, what I wanted to say was that it's not, the models will respond back to a prompt, but you can do only so much with the model. You need to do some work behind the scenes on traditional architecture so that you can get faster responses as well. Um, moving ahead. For these 3 points that I'm going to talk about. I, I'm going to talk about the challenges that we faced, but. I'll, I'll talk about the, the solution to that in the next slide. Testing is again slightly different in the in the Gen AI paradigm. It's not the traditional way we can test. We can't have. We can have test suites, but not, not the test suites that we have right now. Functional testing, exploratory testing is difficult because we don't know how the AI AI will respond back. So how do we do that testing is, was, was interesting for us. Um The other thing was, customers are quite vague when they ask questions to the LLM. Jordan asked me a question earlier about when we were going through the, through the video. What how do customers behave? Customers behave with. The Gen AI just like they would behave with any other search engine. They think it's a search engine. They ask for questions and those questions are. Pretty vague. They could go um. If you look, if you remember the video, they, the customer just asked for bets. What kind of bets? And, and that's, that's where we need to think about the LLM as a person. As a person, what would you ask back? So when, when the customer says bets, You would probably ask them which. Which sport do you want bets for? Is it NFL, NBA, NHL? And that's exactly what we did. So the LLM responds back with a suggestion, saying, hey, these are the suggested best that you could get to. And when the customer clicks on NBA, then again, that's a vague question. Do you want NBA bets for tonight's game? Do you want for the season? Do you want for the game next week? So that's The vagueness of the customer is you, you need to prompt the customer and suggest more things to the customer so that you get the information out from the customer. And finally, the customer goes to, I want the bets for Donovan Mitchell, I want the stats for Donovan Mitchell, and that's where they wanted it to be, really. So, we need to prompt the customer as well as customer prompting the LLM. Um, the last thing is, um, in, in vagueness is, is the slang and the colloquialisms that people have to have within the sporting, uh, industry. So, people will say something like, give me a bet for LBJ. Which is LeBron James. So, we need to be able to translate that and understand what that, what that means, because the LLM doesn't understand. Or maybe a parley for the birds, the birds being the eagles, uh, how do you do, how do you deal with that? It's not an exhaustive list. So, uh, this is something that we're still working with, we don't have a perfect solution to it, but, um, it, it's happening. Um Models are also very sensitive. I want to bring. Two points over here along with the models. One is they're sensitive to changes in the data, so the tools changing any kind of data. Tools are used by not just the lambdas for the LLMs, but they're also used for other services. So when these services change, that affects the LLM as well. So we need to be careful about that. The other thing is the prompt. If the prompt changes and the way customers start evolve and start looking at the prompts and and creating better prompts, that is also something that we need to track. So how do we do that? How do we make sure that we have an accurate response and how we track that accuracy? Let's look at that in the next slide. Um, when we talk about metrics. We, we have a lot of metrics. Obviously, if you, if you're a data scientist, you'll have your precision and your recall, and you'll have your Uh, false positives, your false negatives. But for a product, we're in FanDuel, we've picked up on accuracy as our, our metric for understanding what the response is going back to the customer. And evaluations have help a lot in that. So evals is, is what we use at the moment. Um, The traditional development tests and release cycles, they don't work very well with With evaluating and understanding the response from from an LLM. So what we have done is We have created a test suite and we've created thresholds for Every response that comes out we've got categories, and each of these categories have got thresholds for any kind of response that comes out. So when we have any change, any change to the model, any change to the prompt, any change to the data, or even the code, if any of these four factors change, then we tested against these thresholds, and if the if. The response that comes out has gone below the thresholds, then we've regressed. So obviously we need to make a change. We need to understand where we went wrong. If it's progressed, then that's great. It might be a new threshold, but we never know. We need to keep understanding why it has progressed, and then we have a new threshold. So that's what we do today we have thousands of test suites, uh, sorry, thousands of tests in our test suite, and we run those every time we make a release so that we know that. Our benchmarks are met. We know that we have the right. Responses from the models for whatever changes we have made. You'll ask me, what about What about how the customer interacts? Because this is just the test, this is just the changes that we do. What about customers? Customers, as I said, they keep changing. They're, they keep learning, they keep evolving. So how do, how do we track that as well? Our data scientists and our data team, they work with understanding the feedback loop from the customer, so they use evals as well, uh, and they make sure that we have, they have a list of static and dynamic tests where they're constantly testing the prompts that are coming in live as well as statically behind the scenes. Uh, they have ground truths on these and, and with these ground truths what we do is we make sure that. The information that's coming through is evaluated against new categories, so we discover new categories, uh, and new, new ground truths, and then that feeds back into the test suites that we have for our testing as well. Um Yes, um Moving ahead That's about the key metrics and how we, we test, um, so going back to the, um, learnings and the challenges testing, that's how we take care of it by using the evaluations and the models as well. We make sure that when there's a new model coming, so for example, uh, we had uh Nova Light 2 released uh yesterday. We can test that now, make sure that it's, it meets the standards that we, we have, uh, put out, and that we don't regress from, from what we have right now. The future. So we saw how We started out from inception as a hackathon idea, um, how we went ahead, the partnership of the Gen AI and the innovation Center, um, how we implemented and rolled out. In iterations, the architecture, and you saw the data flow as well and the challenges that we faced along the way. Uh, we continue to learn from customer behavior and understand, uh, how our customers are evolving and how we need to evolve as well. Um. And we're going to use new things as well, like MCP for example, for, for internal use. Santosh wants to talk a little bit about Bedrock Asian code as well and what we're doing with that. Yeah, absolutely. So we've seen how Roe talked that lambdas were really central to how AAI was actually built. You know, there's a lot of different nuances that lambdas handle really good today, gets the job done properly. But then we all know that JNA applications are unique by themselves, you know, they have different requirements. They have different characteristics to them. And then you also need to have a platform that is efficient enough to support all of your agentic A applications and LLM applications, you know. And that was the reason why at New York Summit, I believe in July or August, uh, you know, uh, uh, this year we've launched Bedrock Agent Corre. So Bedrock Asian Core has a lot of different primitives onto it such as runtime, identity, observability. It's it's a very helpful feature of Bedrock that a lot of our customers are experimenting with and same with FanDuel as well, you know, Lambda's got them so far. It's got the job done really well, but then with Asian Core is what FanDuel is now evaluating to see how that actually fits with AAI runtime platform. You can now run workloads as quick as a few seconds or a few milliseconds, and then. Have those workloads run until many, many hours. I believe 8 hours is the absolute limit that you can run. So, as AAI is exponentially adding more features to it as it's getting more and more robust, uh, we're currently actively we're evaluating agent core runtime as a way of scheduling all the work, all the workloads. And secondly, how many of you are following up on all the launches that we've had in soulless space? We've got quite a few, very good. I see some hands. Yeah, we've got some quite exciting announcements with lambdas and API gateways, and one that is very much of interest for us here with the FanDuel team is API gateway now has the capability of streaming responses. So when you're interacting with Bedrock, when you have lambdas running in between, you can actually stream the response as it is happening live and then send it out to users instead of chunking it or batching it. So you know, to make the user experience even more better, there's a lot of things that we're doing, and API gateway is one such area that we're investigating and and experimenting with right now. And I'm sure we can talk a lot more here. You know, we're also looking at SDKs, strands SDKs, what you know we've launched as well to provide you with a consistent manner in how you can build your GA applications. There's a lot more to it, and we're very excited to see the future of Ace AI and how Amazon Bedrock is going to help shape that for FanDuel. So Sorry, I was gonna say I, I think uh the. API gateway streaming is quite, oh yeah, absolutely, it's, it's really quite powerful. Our customers have asked for the features and we're very excited to have launched this as part of Pre-invent just uh and uh I would also encourage all of you to try it out, you know, give it a spin. Let us know what you think about it. Yeah, Oh. All right, it was really great presenting to all of you. Thanks for taking time to be with us today and I hope you found this session helpful, you know, thank you very much. Thank you very much, everyone. Thank you. All right, see you folks.