---
video_id: pq3jxZ1AMKE
video_url: https://www.youtube.com/watch?v=pq3jxZ1AMKE
is_generated: False
is_translatable: True
---

Right, good afternoon, folks, um. You're here hopefully to learn about how AMD Epic CPUs can help you across a broad set of general compute workloads as well as AI workloads. Um, quick bookkeeping, I'll talk really loudly until you wear your headset. You're gonna have, you're gonna need your headset to hear me, um. Quick intro, uh, I'm Madhu Rangarajan. I run the server product team at AMD, so all of the Epic CPUs, product planning, product management, and product marketing, and I'll let Mike and Kyle introduce themselves as well. Hey everyone, Mike Thompson here. I am responsible for AMD's public cloud products out in the world across all the hyper scalers based in North America and soon to be neo clouds. Good afternoon everyone. I'm Kyle McLaughlin. I lead our fin-house practice at CVS Health and uh talk more later. OK, cool. Thanks everyone for coming. I'll see you in a sec. All right, so I'm gonna cover the first part and then Mike and Kyle are gonna get into progressively more detail, um. If you look at AMD as a company and how we've evolved, right, uh, we are looking at everything as end to end solutions because we understand our customers aren't just looking for a piece of silicon. Uh, we've got CPUs, we got GPUs, we got networking, but more importantly, there needs to be a software layer that ties it all together, and then, uh, there are solutions and cluster level designs that now we have the ability to do through our acquisition of ZT Systems. So we're really looking at ourselves as an end to end solutions provider. Uh, I'll be diving a little more into the CPU today. Yeah. Now jumping into the CPU, if you look at the spectrum of workloads, right, you've got general compute and small AI workloads and AI pipeline workloads and classical machine learning and things like that that have been traditionally done in the CPU and continue to be done on the CPU. If you go all the way to the right, you got the AMD Instinct GPUs. So if you're doing any AI at scale, LLMs at scale, you're looking for real-time inference, really large generative AI models, and so on, the instinct GPUs work really well for that, um. And of course the CPU continues to be relevant throughout this because AI doesn't live in a vacuum. AI is a pipeline that includes data input, cleaning, preprocessing, model training, and then deployment. So there's always the system level interplay between the CPU and the GPU and the networking and clusters of all of these, OK. Now, jumping into inference a bit, if you look at what's been happening just over the last two years, in 18 months, the cost of inference has gone down 280x, OK? The. It was $20 per million tokens in 2022 and it's 7 cents in 2024, so this is like a new version of Moore's Law where the cost per token is going down exponentially. And what that means is for those of you familiar with Jawan's paradox, uh, Jawan's paradox says when any resource is, uh, becomes low enough in cost, there is a significantly higher consumption of that resource, and that's exactly what we are seeing with all of the insatiable desire for AI and all of the silicon growth that's happening. And what does that mean, the growth of influence? As most of the big labs and all of the companies were doing training, uh, it was a little less complicated, quote unquote, where the CPU did some data preparation and then you had clusters of GPUs doing all of the training and there wasn't as much interplay although the CPU was involved in it, but as you transition to a world where there's a lot more inference happening than training. Uh, you, it's a much more complex workload now. You've got a lot of preprocessing. You've got things like rag pipelines, uh, and then you do the inference itself, and then you have to do postprocessing, and, uh, and there's an interplay where there can sometimes be iterative loops going back and forth between the CPU and the GPU between general purpose tasks and these, uh, more AI or inference focused tasks and. Agentic AI takes that to an entirely different level, right? Previously you just had a bunch of users accessing a bunch of compute of different kinds, whether it's CPUs or GPUs or networking. As agentic AI grows, what you have is a bunch of users deploying a whole bunch of agents, and all of these, and these agents never sleep. These agents are accessing compute resources, uh, thousands of times more than human beings, and they're accessing all of these web servers, databases, media, CDNs, uh, then they're running all of the, uh, they're accessing AI accelerators. So this is what's driving that insatiable desire for compute. You're looking at all of these giant data center buildouts you're hearing about all of the constraints on power you're seeing all of these huge data center buildouts that are happening, uh, it's happening exactly because of this, and we don't see any signs of this abating because as these agents get more and more useful you're gonna see more and more people deploy this. Now, getting further on Epic CPU side of things, uh, in general, if you look at the Epic CPUs and what we've been delivering consistently for a few generations now. Extremely high performance across a broad set of workloads. Uh, very power efficient, one of the most efficient, uh, CPUs out there, and TCO, total cost of ownership, which we deliver through a combination of performance and core count as well as efficiency. And if you look at the evolution of the AMD Epic CPU roadmap, uh, we started with the Zen1 servers in 2017, and when the Zen1 servers came out in 2017, we probably had low single digit percentage in terms of market share. Now going all the way to the Zen 5 servers on SP5, uh, the product we call Turin, we are now at 41% market share and Zen 6 or Venice is coming very soon next year and we expect the trajectory of growth and market share growth uh that AMD has to continue for a while. Now this is a quick snapshot of our 5th generation Epic or Turin which we launched late last year. Um, at the time of its launch we again delivered significant performance over competition, right? If you look at all of these metrics, whether it's just benchmarks like SE CPU or you're looking at a broad set of enterprise workloads or HPC workloads like 23, and 4X more performance than anything competition has to offer. Even on CPU-based AI workloads, uh, just the core count and the memory bandwidth and everything else we had to offer gives you up to 3.8x on, uh, uh, various AI workloads. And now even if you are using a GPU. It is actually extremely important to pick the right CPU, so we created a part called the EPIC 9575F. It's a high frequency processor, and the main job of the processor is to get out of the way of the GPU so that the GPU can process things and not bottleneck the GPU. And what that translates into is on a GPU server 20% more performance out of the GPU just by minimizing the time spent waiting for the CPU to do stuff. If you look at that in the context of, I don't know, $30,000 to $400,000 GPU server, that's tens of thousands of dollars of performance per dollar that it translates into. So whether you're running your workloads on a CPU or a GPU, uh, the CPU is kind of ubiquitous there, and it's extremely important to make sure that you pick the right CPU to run those workloads. And again, if you're running AI inference itself on the CPU and Mike will get into more details on this, uh, broad set of AI inference workloads run really well on Epic, uh, for example, if you look at machine learning, uh, decision trees, support vector machines, and so on. Those workloads use high precision vector engines. They do really well on the CPU. When you have LLMs and generative AI, especially if you have a small and and smaller language models, small scale deployments where you have, for example, a chatbot that's not busy all the time, the CPU actually does really well, and the large memory capacity that a CPU has to offer is very helpful there. And then just recommendation systems as well as AI apps where you have a mix of, uh, like general compute with AI embedded in that overall pipeline. uh, CPUs tend to be really good at those workloads. And of course that's the hardware piece and it's extremely important for our customers to be able to consume this easily in terms of software so what we are AMD is extremely focused on making sure the open source ecosystem is enabled. So if you look at how we're gonna make sure you are able to consume the performance easily. We are enabling the standard uh framework Sytorch, tensor flow and uh and onyx. Now all of these will work right out of the box and in general they actually do pretty well. Now if you want to kick your performance up a notch, we got this plug-in called Zen DNN. They plug into these frameworks pretty seamlessly, and you can get more performance out of it. It's, uh, it's not required, but it can help tune your performance further, and we're actually working through upstreaming a lot of those optimization directly into those frameworks, so you at some point you don't even have to use a plug-in. And again just to sum it up before I hand over to Mike, uh, why use Epic CPUs for AI workloads? The first is accessibility. CPUs are omnipresent. You can find it across regions, across data center, across clouds, whether you're on-prem or in the cloud. It's ubiquitous, and you got the standard X86 software stack that you can leverage across and without any porting you can get, you can run your workload. Utilization opportunities. A lot of customers have servers in either in their data centers where they're sitting at 20-30% utilization or they've bought a whole bunch of reserved instances and those reserved instances are sitting idle a lot, so you can actually do batch processing of AI workloads on the downtimes and increase the utilization and make more effective use of the hardware that you've already got. Uh, third one is scalability, especially when you're in the cloud. Uh, you can have a baseline amount of capacity and you can scale your capacity up and down. That's the, that's the value prop of cloud. That's why all of you are here. And the last one of course is if you want operational simplicity where your AI workloads are a little more small scale and you wanna have a relatively uniform fleet without having to manage multiple types of infrastructure and figure out how many of each type of infrastructure you wanna deploy and so on, uh, with the CPU you get a unified platform you can get a single type of cloud instance to support a broad set of workloads. And with that I'm gonna hand over to Mike who's gonna kind of get you go get you through more details. Awesome, thank you, Madu. Hey everyone, Mike here. Uh, again, I manage AMD's product, uh, Global cloud product, and I'm here to talk to you today about some of the cost efficiencies that you can extract by just making the simple choice of selecting your processor, your compute platform wisely. And I'll also go into some details about when does it actually make sense to do inference on CPU. A lot of people think AI is GPU, and to some extent that is true, but there's, we've been doing AI on CPU since 1950, so 75 years, and especially as you know, we finish up this huge era of training, or at least we've done enough training to be able to start actualizing those. Models making them available to the broad public, you should really consider carefully where you deploy those. There's some pretty significant benefits that you can get from an availability and cost perspective. Now, to take a step back, choosing your compute platform really does matter. Historically, a lot of folks have thought about their compute platforms, the CPUs that they're renting in the cloud, as a commodity, and May overlook some of the benefits that you can get from performance. A little bit later today we'll have Kyle from CVS on stage and he'll talk a little bit about how they were able to get an OE return by choosing the compute platforms wisely. This is an example. AMD Epic CPUs are. Have a really, really good architecture. They have really good implementations in AWS that enable them to have incredible performance, up to 2X performance, and that comparison is comparing Ice Lake or a 6I to a Genoa or a 7A. So one generation difference, you can get a 2X performance uplift. If you're just choosing your compute platforms by whatever has ever been done before, you could be leaving an awful lot of your OE or your TCO on the table. So with that 2X performance, you can drive almost 50% lower cost for the jobs that you're running. The jobs that are most suited for that are ephemeral jobs or containerized jobs or batch jobs where you spin up the servers when they're ready to be used and then you spin them down when they're done. Through that mechanism, what we call performance driven cost optimization. Literally with 4 clicks of your mouse in the AWS console, stop your instance, hit the instant selector drop down, select an instance with an A in it for AMD. You'll see the little lowercase a for EC2 and start your instance again, you can drive your, your burn rate 45 down 45%. And so that's the impact of performance driven optimizations. And then do we have anyone here from Europe today? Anyone from AMA? Couple, OK, so what I've observed is the concern with energy consumption in Europe is pretty significant. There are some existential challenges there, and so I see a lot of the decisions about deploying infrastructure in Europe. One of the primary concerns is actually energy consumption, and so. By the same token, by choosing AMD powered instances, whether you're doing them on-prem or in the cloud, you get up to 2.2x better performance per watt. So every watt you burn, you get more than double the work out of it. Sometimes those numbers can be a little bit hard to tease out of the hyper scales, but you get both of those benefits. You get performance and hence a lower cost, but you also consume a lot less energy while you're at it. If anyone is interested in diving into more details about that, please stop by afterwards. We'll be out here to chat for a little while or swing by the AMD booth on the show floor. All right, so, AMD CPUs can really help address a couple of the key AI challenges. One, When most customers or most organizations, they set their budgets at the beginning of the year, whether it's calendar year or fiscal year, you get your budget, you allocate it to whatever applications or business units, and then you go and execute. Well, there's so much innovation in AI happening right now that you can't anticipate what you're going to need in August when you set your budget in January. And so one of the ways that we've seen customers take advantage of these performance driven cost optimizations is to go into their, maybe just their general IT infrastructure, do that 4 click transformation to drive roughly we see 27 to 45% cost savings that can come from just simply switching to AMD powered instances. They free up that budget and they can take that and either invest it in a lot of times it's. Innovation, so AI applications or advanced analytics, but also for business growth. If customers are having a significant amount of business growth, they need more capital to deploy more infrastructure to drive more revenue as you're growing, right? And so that's kind of a secret way that you can unlock budget that you might not otherwise think you have those efficiencies in the infrastructure they already have deployed. So higher performance means faster runtime. Smaller footprint or smaller instance size, all of those can drive lower opex, especially for instance size. Like for instance, if you get 2X performance and you're running SQL Server, which is licensed per core, and those software, those licensed software applications, that license cost tends to dominate your TCO. It's usually 80 to 90%. 80 to 90% of your TCO is wrapped up in those license costs. And so if you take advantage of that 2X performance, that means you can cut your instant size in half. That means your number of cores or VCPUs goes down by half, and that's what your software is often licensed on. So you can dramatically drive down your TCO, including the licensing cost, and in that case, The infrastructure is a small portion of it. You roughly can expect about a 45% TCO return when you're doing that based on SQL Server packaged with the instance from AWS. 45% cost return for those software applications. And then finally, since we're mostly here today to talk about inference or AI, um, inference on CPU is very cost effective. I'll show you some ways that that is enabled today. CPU instances are highly available. They tend to be more broadly geographically available than GPU instances. There tends to be more pool depth. They're highly portable as well. Let's say your AI workload shuts down. You can put any general purpose workload on a CPU. You can't always do that with GPUs. Don't get me wrong, GPUs are great, also very important. We need those, but CPUs tend to be more portable. Uh, and you can also repurpose those instances for other applications. Um One of the things talking with dozens and dozens of customers, one of the challenges that customers face in terms of getting the most out of their investment in infrastructure, and this is honestly on-prem and cloud, but here what we're looking at is what is the typical CPU utilization over the course of 24 hours. You'll notice from midnight until early in the morning, a lot of the compute isn't getting used at all. Then it peaks when everyone comes to work, then it drops off around dinnertime. Then after dinnertime you can see another little peak. And then it drops off again. So that's kind of dramatically under underutilized compute resource, especially if you've paid for like a long term fixed price contracts. You're literally burning money doing nothing. And so one of the things that you can do for AI applications that aren't necessarily real time, there's a lot of analytics associated with it. Maybe like let's say you run a streaming service and you want to come up with new movie titles to recommend to your users tomorrow. Well, You can insert those AI workloads during these non-peak hours. If you're doing inference that is not latency sensitive, that's really, really well suited to CPU and it enables you to get more out of your investments, so you can deliver insight during off hours. You can maximize the value of your reservedense fixed price contracts, and I'll show you a little bit more in a moment. You can leverage the really, really good performance of AMD Epic CPUs. Again, look for the A in the instance name. To run these inference applications. Now, here are some examples of why you might want to do inference on CPU. First, I'm showing one of the leading GPU instances for doing inference versus an M7A instances. This is from our Genoa Epic processor, 7A, M7A versus G6E. You can see that the CPU instance in blue is about 1/3 the cost of the GPU instance. Consider with that utilization curve that we just looked at, why would you want to burn 3 times the rate when you're only using about half of it? So there's many. Applications for inference where using CPUs makes a lot of sense. If you're not latency sensitive, it makes a lot of sense. If you have small and medium models, it makes a lot of sense. Has anybody in the, uh, has anybody in the audience had like a meeting summary agent join any of your calls? Afterwards, 5 minutes after your meeting ends, it sends you a summary. What about, has anyone had, uh, what do they call that, the transcript where it just does the voice recognition and sends you the transcript. Does anybody read the transcripts in full? Nobody does. Does anybody read the summary? I do. I read the transcripts once about a year and a half ago, I think, and then when the summary agents came around. So now, those summary agents, do you really care whether that summary turns up in your inbox immediately as you exit the meeting, or if it shows up 5 minutes later, is that fine? It's fine, right? And so that's an example of a case where CPU inference makes a lot of sense. You do it at a 66% discount. It's much more highly available and more geographically distributed, so that's one of the reasons you might consider inference on CPU. And so now I'm going to start diving into some of the technical details. We've run loads and loads of benchmarks for inference workloads. I'm just selecting one of them here. Here we're looking at a model that does natural language processing, um, like something that a model that can deliver a customary service agent, customer service agents, um, or interact with you to understand, you know, understand the language that you're asking. It's a way to interact with humans to process language. Just making a simple choice in your compute platform can have a dramatic impact on your cost. And here we're looking at an M7I versus an M7A. And because of the performance uplift, if you look at the instant sizes under the chart here, you'll see the gray. When you move from gray to blue, you cut the instant size in half. We either go from 4 XL to 2 XL or 8 XL to 4 XL if you speak instant sizes in terms of EC2. That higher the higher performance on the M7A AMD epic powered instances enables you to achieve similar performance at roughly 40% lower cost. You know, right now we're going through a fairly decent economic cycle, but the next time an economic downturn comes, that 40% difference in your burn rate can make a tremendous difference in, frankly, the number of people that you can keep employed, right? So. It also can free up a lot of capital to invest in other applications, new applications, business growth, and so forth. So that's the impact that performance can have on your applications with a very simple conscious selection of an optimal platform for your applications. All right. And so now looking at when does it make sense to do inference on CPU. So on the left here we're listing a range of applications from classical NML, which you know technically don't really use models to recommendation systems. That's what your Netflix movie recommendations come from. Natural language processing, we interact with that all the time. Uh, Gen AI for large language models, as well as mixed workloads. A lot of applications now have a mix of traditional software and AI driven software. Now, so for the model size, generally you're gonna see, um, CPU inference works well for small and medium sized models, except for classical ML and MLP and NLP natural language processing. CPU can handle all the model sizes. Um, and then offline versus real-time. Offline is a way to express not latency sensitive or not real time. So generally, if you're not latency sensitive or not running real-time, CPU makes sense in all of these cases. In a few of the cases, you want GPUs and AMD has great GPUs as well, though today we're here to mostly talk about CPUs. But for classical ML and recommendation systems, you can even run those on CPU. There are some large streaming services today that are doing just that that I would imagine almost everyone in the audience here experiences every time they go to use their favorite streaming service. And so net net, why use AMD CPU powered instances for inference? It really boils down to performance, which drives cost savings for you and can drive much better user experience for your end users. You get over double the price performance, meaning performance per dollar or whatever currency you happen to be operating in, just by choosing instances with the lowercase a in the name. Um, and that is what enables you to drive on average across a wide range of inference workloads 44% lower opE. And so the, the fundamental concept there is consider the platform that you're deploying your applications on. If you just deploy it wherever you have before, you're probably spending more than you need to. Um, this is one of the main. Interest that we see broadly from our customers today. And up next we're going to have Kyle McLaughlin from CVS Health telling you a little bit about how in CVS's FinOpps practice they've taken advantage of these kinds of performance-driven optimizations. So, Kyle Thanks, Mike. Good afternoon, everyone. See if I can figure out, remember, there we go. All right, cool. Uh, so quickly, just a little bit about myself. I'm Kyle McLaughlin. I lead the FinOs practice at CVS Health. Uh, I've been there for about 18 months, but been in the FinOs space for about 7 years now. Uh, previously worked at the Finns Foundation, which sets the standards and best practices for FinOs. It's a project of the Linux Foundation. Spent several years as well in a number of, uh, FinOs startups, so. What was really exciting for me about the opportunity to come to CVS is I could take uh startup experience working with different organizations all over the world. The industry best practices and go back to where I started my career, get back to a big company and see about how we do finops at scale. Enough about me though. Let's talk about CVS. Most people know CVS for the retail presence. Uh, you see the stores on every corner. What a lot of folks don't know is CVS also owns Aetna Insurance. We have a giant pharmacy division, and we also have a healthcare delivery division. Combine all that together in one package, you have a US based healthcare company that has a full end to end solution about how we can treat our members at every step of their healthcare journey. We're a Fortune 5 organization and we're on pace for about $400 billion in revenue this year. Combine all that with us going through a major tech transformation, uh, we are trying to modernize and enhance our tech stack while still serving 100 million customers a day. So safe to say we have a fairly significant cloud spend, but at the end of the day for us to support our members, stability is our overarching priority. So where does FinOps come into play for CVS? Well, to manage all that spend, make sure we're adding value for our member experience, it's really important that we have good visibility into tracking the application spend, understand who those owners are. It's also not just about spend, it's about what's the business value that we're getting out of that incremental dollar. So, well, we'll go backslide. Who here was not familiar with Finns before they showed up at this do or has never heard of Finns? Love it, my friend. You are going to get baptized by fire here. This is the FinOps framework. I no longer work there, so if you don't like it, uh, I'll tell you who you can take it up with. But a lot of content here, work with me, folks. Being in the space for 7 years, I have found everything boils down to, I'm gonna pick the smallest graphic on the screen, which is great for a presentation, inform, optimize, operate. We call this the phases of FinOs, and really everything that we're doing ties back to these 3 phases. It's impossible to go save that money if we don't know who's spending it in the first place. So everything always boils back to how's our visibility, how are we doing in the informed phase. From there we think about optimizing. And then once we've optimized, there's frankly too much work to do that we can't just keep sitting there doing the same things. We need to find ways to operationalize that work, automate it when possible, and then we enter right back into the wheel again. And we talk about a lot of work to do in the fin-off space. We touch a lot of different personas. See if I make sure I got the laser right. There we go, uh, a fin-offs practice. Executives care about it. Our friends in cloud engineering, we are heavily dependent on them and a partnership in order to have success. Procurement's extremely interested, finance is extremely interested, you have to work with all these different personas across the organization. And in the framework we have these ideas of domains and capabilities. A domain is really what you should be doing. In this case, how do we quantify business value. The capabilities are how we go about doing that, how we measure the business value is concepts like unit economics. It's fine if your cloud spend goes up by 10% as long as you're serving 20-30% more customers, and that's really how we think about cloud spend and the value that we're generating out of it. So I go back again. There's a lot of content on this slide. I really want to talk through what we've been doing at CVS, what I think are our biggest priorities, and we're leading to some of the successful results that we've had so far. So let's start with in 4. Again, everything comes back to knowing who's the resource owner, who spun this thing up, and then what are we going to do about it. We are able to map all of our resource owners through our CMDB. This contains the technical owner information, app owners, all that good detail. Not only do we need this for security purposes, but we're also able to charge back all of our cloud costs to those application owners. So we put all this emphasis on our CMDB to make sure we have the proper visibility of who that owner is. And then also hold them accountable for the spend that they're generating. We also believe in operating with complete transparency on your cloud spend. I have engineering teams that are interested in knowing what they're spending in cloud. We make sure they have the appropriate visibility through different tools that we have internally. Optimize the fun stuff. One of the biggest wins we've had is really trying to focus on standardizing our SKUs that we have engineering spinning up infrastructure by standardizing the SKUs that really allows us to take advantage of the price and performance reasons and implement those spinoffs best practices at time of resource creation. Then we can focus more of our time on how do we chase down, you know, resources that were created before automation went into place. Consolidating on SKUs gets back to what Mado talked about. It allows us to really optimize and max out the savings that we're getting from reserved instances and savings plans. We're able to manage all of this with a combination of tools, some of the folks that you might see out on the expo floor, and we're fortunate enough to have some engineering teams that build custom solutions for us, you know, when we can't quite meet our needs. And we again give that total visibility to teams internally. So if people want to go and see their costs, they want to optimize, they have tools available to them to get that done. But again, going back to the first slide, uh, we can't just keep doing the same thing over and over again. We're a relatively small team. Uh, how do we operationalize this? A couple places where we found a lot of great results, uh, specifically partnering with our largest application teams. Generally, it's just like any 80/20 analysis, 80% of your spend is probably going to come from about 20% of your total applications. And what we found is by partnering closely with these large application teams, building the relationship, investing the time again to know what they're trying to solve, typically leads to great results down the road. We had one application team in particular where we just approached them about a fairly simple, fairly straightforward SKU change. Fast forward about 9 months, we now have that engineering team refactoring applications, completely rearchitecting how they deploy and even finding really nuanced optimization opportunities that are far outside the skill set of our finoffs team. But because we built that relationship with them, frankly got them a little hooked on the savings, they've really leaned into it and we have a great partnership and we've been able to replicate that across other application teams. We were talking earlier, CVS is, we have 300,000 employees. It's a large organization, and we're spread across 4 major business units. So we've also been working to establish working relationships with all the different BUs and make sure that we're evangelizing and spreading these best practices throughout the organization. And then where possible automate. So we have automated communication channels making sure we're getting the right information to the right people at the right time. We're constantly exploring new ways to get that information to our app owners and the people who are passionate about managing their cloud resources effectively, and that's really been a key for how we scale Finoff's program at the size of CVS. So where are we going from here? Containers continue to be a growing presence in our overall cloud portfolio. It's going to be a major focus for us for the next few years is the same thing, going through the same three phases of the FinOps life cycle inform, optimize, operate, make sure we're getting best in class visibility and granularity of what's going on within each of those name spaces and pods, and then exploring different optimization opportunities about how we more efficiently pack resources and use the right VM for that workload. We've done a good job so far of implementing standards, but we really need to start transitioning to a place of policy and governance, uh, and for an organization of our scale, it's also how do you do real world mediation with real exception management. You know, it's easy for anyone to go and say we need to go do this one thing, but at a company as large and as complex as ours, we need to be able to handle different use cases as well. And you're welcome. I am saving my last bullet point at the end to say the word AI. Uh, we are also looking at ways to leverage the latest technology for FinOP's best practices, and it's really two lens. There's FinOs for AI. Everyone knows the spend's growing there. The tooling's still relatively new in the market. We need to get more granular visibility into what's going on with these AI workloads, and then we can have the fun optimization and operational conversations. At the same time, AI can also be used to help you generate your insights. The introduction of chatbots and agents, we can now be leveraging AI to help us generate our insights at the end of the month, as opposed to fully relying on dashboards and analysts diving into the data. So, I'll be here after the talk. If anyone wants to talk more, happy to answer any questions and I'll pass it back over to Mahu. Oh. Right, thank you very much, Kyle. So just to wrap it all up, um. Tying it back to the overall theme of this, right, how do you build an excellent platform that can span a broad set of workloads including AI inference Epic CPUs provide that platform. I think Mike walked you through various examples. Uh, and then again, the CPUs are just the means to an end, the instance is how you're consuming it, and Mike, of course, walked you through the EC2 instances that can help you, uh, realize this TCO value and in terms of the ease of use. Getting started with an AMD Epic Power DC2 instance is really easy regardless of what your starting point is. You're starting from an old, uh, Intel instance. It just works, right? I think for those of you with long memories, you might remember that back in the day there was this whole uh IA 64 versus X8664 instruction set uh debate and AMD went down the path of X8664 while Intel went down the IA 64 path and. The market spoke and X 8664 is what's arrived and that is why all the applications just work in most cases and uh once you uh move over to an AMD instance most cases you're gonna get more performance most cases it just works and then you can get even more performance by doing, uh, further tuning. So really, so, so the Epic kind of gives you the ability to essentially just lift and shift and. Then optimize you don't have to do a whole lot of porting work to get there uh with that I wanna thank Kyle and Mike and all of you for attending we'll be available to answer questions we have about 20 minutes, uh, and so we can hang around and answer any questions you have. Thank you very much.