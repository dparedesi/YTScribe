---
video_id: 9HTSa_JsoAQ
video_url: https://www.youtube.com/watch?v=9HTSa_JsoAQ
is_generated: False
is_translatable: True
---

Good morning everyone. uh, can everybody hear me well? Give me a thumbs up, OK? So remember this is a silent room. Don't forget to put your headphones on. Um, so welcome to Las Vegas. Welcome to Reinvent 2025. We make it. Uh, I can't rem I can't, uh, believe it's already December, um, and today this is the session CNS 417 for network observabilities and strategies, and on this session we will be guiding you through a journey to achieve observability. And improve your overall security posture on Kubernete's environment by simplifying overall operations and application life cycle, right? So we want to simplify our daily tasks so we can be more achieve more velocity and faster time to market. Uh, one thing to keep in mind is um. We want To to make sure that all the features that are coming out is uh have that goal to simplify your overall operations and today we are announ seeing a very nice feature that I wish was there when I was a platform engineering managing a lot of Kubernetes environments. So how many of you are using Kubernetes in production today? Give me a show of hands. So nice, basically everybody. How many of you are aware of EKS Auto mode? Nice. So we'll be covering some features that cover overall EKS overall Kubernetes, but also how EKS Auto mode can help you simplifying those, those daily operations. My name is Joricoberza. I'm a container specialist solutions architect at AWS, and my goal here is to help customers, uh, overcome challenges and build solutions to simplify their, their daily activities. And I have the pleasure to have here with me. Master Luke, he's not Luke Skywalker, but he's also a master Jedi in Kubernetes. Thanks, Rodrigo. Hey everyone, I'm Laconde Mula. You can also call me Luke, and I'm a product manager on the Amazon EKS team. So I wanna start by asking a quick question. By show of hands, does this architecture diagram resemble your environment to some degree, at least at the fundamental layer. I see a couple of hands going up. OK, more going up, that's great. So this is something that we put in front of a lot of customers, and many of them were able to resonate with this. And again emphasizing on the fundamental layer and what we have represented here is an application environment that has Amazon EKS as the foundation for its platform and this is what a lot of customers are doing now. It's not the entire picture and that's something that we're going to be taking you on a journey about. You have a lot of Kubernetti's workloads or to be more specific, microservices that are running on EKS, and a lot of these are communicating with each other in order to drive business functionality. These microservices are speaking to each other within the scope of the cluster, so there's a lot of east to west traffic. Now, in addition to that, a lot of these microservices also rely on AWS infrastructure. For example, you could be running some kind of storage application that relies on S3 to actually store its objects there, and then in order to have some metadata associated with those objects, you could be storing it in Dynamo DB. Or perhaps you're using a traditional three-tier application and you're using elastic cache as the caching layer, RDS or DocumentDB for persistent storage. We also have a lot of customers that are training their ML models and using S3 for checkpointing. But again, it doesn't stop there. A lot of customers are also integrating these environments with additional services and systems that are running on-prem or on the internet. Does this pattern sound familiar? Again, any show of hands on that? So what we have here, in the bigger picture, is this widely distributed environment, with a lot of threads that essentially tie back to EKS as the hub or one of the primary hubs. And the core fabric that ties all of this together is the network. And the more that companies or organizations continue to innovate. And introduce additional enhancements that will typically translate to new microservices that land either in your EKS cluster or perhaps on prem or some other service on the internet but still ties back to this foundational layer. And so in such environments it is becoming increasingly critical to understand the landscape of the network, to understand its security, to understand its performance and its behavior, primarily so you can measure how well that aligns with your internal standards. Because the goal at the end of the day is to optimize it so that it's serving the ultimate purpose of driving the kind of business value that you want. This should ultimately be serving that bigger purpose. But that becomes especially difficult if you have limited visibility. You can't accurately optimize what you can't see. Some additional challenges to be aware of, especially in the context of an EKS environment. When you're running a Kubernetes cluster in AWS, you have to be mindful of the fact that you have a broader network plane, and that is the VPC. But in addition to that, Kubernetes also has its own network plane. So you essentially have two landscapes or two network planes which introduces additional operational complexity on your part. And if you're trying to achieve that objective that I pointed out earlier of optimizing for security, performance, and ensuring that applications, traffic behavior aligns with your internal standards or designs, you need to be able to consolidate these two planes and easily understand the data points between the two of them. And as I've already spoken about, Network observability is multidimensional. You're concerned about network observability with respect to security, performance, cost optimization. And ensuring that behavior aligns with applications that you have designed are the right applications talking to each other as expected. These are key things. And so the main thing that we do want to draw out in terms of the problem definition to begin with is that when you have a lack of visibility or a limited visibility, you end up with imprecise data. And the challenge with having imprecise data is you have inaccurate conclusions and slow decision making. And finally that leads to suboptimal actions. Any platform engineers that can relate to this? And you get uh you get stuck in a loop of trying to figure out exactly what a root cause is. And fundamentally it's because of a lack of visibility and that also eventually ties to a negative impact on the business. There are many scenarios in which the business is ultimately disrupted because of some kind of network issue. It could be congestion at the pod layer, specifically an ENI issue, but without sufficient visibility and data, it's hard to actually detect that in a quick way. What you're interested in is reducing the amount of time it takes to quickly detect an issue so that you can fast track resolving it as well. Or perhaps maybe it's a bandwidth issue related to the specific worker node, but again, that requires having sufficient network visibility at both a pod level and a worker node level and having enough context so that when you detect an issue, you're also quickly quickly able to tie it to the relevant metadata. What cluster is this coming from? What name space is this coming from? What is the workload associated with this, and exactly what pod is this tied to? What are the ENI's associated with this pod? There is also the challenge of increasing costs when you have insufficient visibility. A lot of customers try to adopt different strategies and techniques to manage the flow of traffic. Anyone can relate to that here? Yep, one of the key things when it comes to cost optimization is trying to ensure that when traffic is flowing from a particular workload to a certain destination, you try to ensure that that traffic ends up in the same AZ in which it originated from, and there are different ways of doing this. You can use native um functionalities to Kubernetes like topologywa routing. We also know that a lot of customers use ISO for this, for locality-based routing. Or you can even use envoy for this. But again, how can you quickly verify that those optimizations that you've put in place actually are working as expected? How can you review the topology set up in your Kubernetes environment and verify that the flows are working as expected based on certain techniques that you would have put in place? And then finally there's the impact of security as well, which is fundamental and Rodrigo is gonna be diving into this even further, which also impacts the time to market again as I mentioned earlier, the point is to ultimately deliver business value faster and it's hard to confidently do that. If security is unsure about how well the network has been optimized to meet certain internal standards that it complies as expected, are traffic patterns aligning with the designs that were set up for specific applications? Is workload A speaking to workload B as expected? And based on the designs ensuring that workload C should not be receiving any traffic from workload C, how quickly can you verify that so that you have sufficient confidence to move from dev to stage to prod. Rodrigo, where do we go from here? You still have one slide. Right. Almost missed this one. Another reason why this is so important is because this is an increasing trend. We have a lot of customers that are using microservice architectures. And of course it introduces its own challenges, but we don't want to take a step away from it because of those challenges and so we have to be mindful of these hurdles, but at the same time, the key thing on our part is how do we simplify operations on your part so that as you adopt a usage pattern that certainly has a number of advantages in this case microservice architectures, how do we ensure that the kind of way, the ways in which we are simplifying the operations in your life helps you to continue the trend. Or leaning into specific usage patterns that you're trying to adopt in your EKS environments. And that is going to be a key thing that we'll speak about here and Rodrigo already alluded to earlier. So where do we go from here? So how do we translate those kinds of business challenges to technical solutions, right? Because if you think about it, they're not really business challenges. They are technical challenges that are leading to business challenges. So let's bring back that architecture that Luke just showed us, but let's dive deep on the core center or our Amazon EKS cluster, our Kubernetes cluster, because if you think about it. The communication through storage or to a database, it, it was always there. Like it doesn't matter what kind of application do you have, it's running on EC2 or if it's containerized, but the growing trend of those containerized applications and multi multi-tenant applications is what's causing this complexity to increase over and over. So how did we get to like this very complex microservices scenario? Let's start from the beginning. So the multi-tenancy evolution started with a monolith. So you had one team or two teams managing one specific application. There's nothing much to do. Like it's just one piece, a black box. They don't have like too much interaction. They don't have too much connection like microservices. With the growing trend of Kubernetis, we started containerizing those monoliths, not. Not really in the right way. We just make those big applications, uh, containers, and we put in a cluster, like a single tenant cluster. So this is what we call like hard multi-tenancy where you have like one application running in a specific cluster, and this is not wrong. Like this is may be required if you have like a very PCI compliant environment or if you have like specific security restrictions. But then we started learning that Kubernet is very, very effective on managing multiple workloads, so we transitioned basically to two patterns. We have one cluster with multiple applications segregated by these logical pieces called name spaces. As we call it soft multi-tenancy, and the other pattern is when you have like several applications running in the same name space sharing the same name space, but how can we ensure that we have security among all of them? We usually start seeing that this is like an overly permissive environment because we're not controlling this. Are we sure that the application A can communicate to application B? We're not. And then these are concerns about the development or application teams, and they're not really to keep security is not really their their main focus or to keep this access control. So there's one team missing on this picture which is the platform team or the security team. The security team and the platform team, they are responsible for all of those environments to help control access, to help make sure it's secure, to avoid outages, whereas our application teams want to develop fast, they want to experiment, they want faster time to market, they want to speed up application life cycle, and as Luke mentioned, if the security team doesn't have like clear visibility. It's really hard for them to create for the platform team to have a secure environment. And and then it creates like an overly permissive environment like I said because if the security teams put like too much restriction they block the application teams and it's not good for anyone. So how do we strike this balance to have control and security that's required for like high regulated environments and have team autonomy so. The teams, the development teams can be efficient, usually, like I said, we fall back into an overly permissive environment because there's no like visibility across networking communications and we don't want to block the development. Does this resonate to like anyone's environment here, anyone's application life cycle process? OK, but where do we really want to go is here. So we want to have assertive controls so we are secure and we are efficient at the same time. And how can IWS help? Uh, like I said, our goal is to simplify overall operations and overall application life cycle. So we started tackling a layered approach to cover all of that. So starting from the bottom, we want to simplify overall infrastructure management so platform engineers can really focus on what's important. We want to give you full clarity and full visibility of all the network paths and everything that's happening in your environment so you can achieve an improved overall security posture, which is like one of the most critical pieces of everyone's environment. So starting from the bottom again, um, last year at Reinvent we introduced it EKS Auto mode. Remember I talked about auto mode? I, I, I know some of you are already using it. And what auto mode gives you, so it's basically one click create cluster and all of the production ready, best practices capabilities are out of the box, so you don't need to be worried about deploying any of those. It provisions all the, the, the cluster infrastructure and automatically does compute scalability through uh a a a key piece component that's called Carpenter. Everybody here knows what what Carpenter is. Nice. So just a quick recap for those who don't know, Carpenter is a node out scaler that scales out compute research based on research requests. So you have like pending pods. Carpenter will read those and in a few seconds you spin up any compute that's required for those workloads to run based on cost efficiency and performance efficiency. And it uses like basically two components that are completely customizable and it's managed entirely inside Kubernets. You don't need to be worried about managed node groups, out scaling groups, whatever. So together with that, like carpenter also continuously optimizes for cost, so it just doesn't scale resources, it optimizes your overall environment. And then the part that's most important for this session. Auto mode comes with core components for networking, and we'll be talking about those 3 net those 3 components in a bit. But also it keeps like all of those components updated to patch and security fixes on the AWS side, so. This way is like where we started creating a simplified infrastructure baseline for you. So let's talk about like those three components. Start from the edges. So Cordiones and Cer proxy are very well known, uh, components from the Kubernetes world. So cordions basically name resolution inside your cluster, so you don't need to like be memorizing IP addresses, of course, and Cerxy is. The component that understands the egress and ingress communication and knows how to direct the right communication to the right pods because you have like a couple of ENIs probably in your nodes but you have like 100 pods so Q proxy will do like this transition and direct the traffic to the right pod. And in the middle we have like the CNIKS CNI is the is one of the most important parts for for our session today. Uh, so CNI, there are a lot of CNIs out there on the open source world, uh, but last year, uh, in fact in 2023, we released it like native support for network policies on CNI where in the past you need to use like a third party CNI to have access to all the network capabilities in your Kubernetes cluster. How many of you are actually using network policies? Just a few. So that's, that's, that's really common because doing network policies is not easy. Why? Because you don't have that kind of visibility like east to west communication, pod to pod, name space, who communicates to who. All of these components run inside the node, and you don't need to manage all of them. So let's talk a little bit more about network policies which will improve the security posture of the cluster. So in the past before auto mode, there's a custom custom setup required for you to enable those network policies. You need to be worried about roles and how to enable that, and this could cause like a cluster level blast radius, where in EKS auto mode it's much more streamlined. Uh, network policies are enabled by default. You just need to tell them to use it. So basically you, you need to create a config map and say, hey, I want to use network policies. Boom, it will be there. Then one of the components for Carpenter that helps you manage the nodes is the node class, right? So you have the node class, it's pretty simple. It's baseline infrastructure, and then you can define the default behavior of your network policies. By default it comes with allow uh policies, but you can change that to default deny if you want to. Like I said, if you're running high regulated environments, and also you can separate specific node classes. So I want to have one application that's fully isolated from the rest of them, uh, the, the applications that are running in my cluster. You don't need to create another cluster. You can do that like in the compute level in the same cluster using EKS auto mode and network policies. So don't take my word. Let's just create a really quick one EKS auto cluster here for the sake of the presentation. Like I said, we're going into a very quick journey. So let's create like one cluster. I just put the name. I see we don't have the role specific for, for that for for that cluster. You can just click a button, it will come like with all the best practices, granular least privilege for the cluster, and you just a few clicks you have the recommended role to create a cluster in your environment. Secondly, we have the node role. So node role and cluster role, they have different personas, so they need to have different level of access. So we can create the same way that we did for the cluster. So a few clicks, create role, we have the node role running, and then. Let's go to the network part, so. Because of the way that auto mode runs, you can create a very specific network for your cluster. Like all the best practices will be there. Just click create VPC. The only, the only things that I'll be changing, I want to run in 3 AZs, not just 2, and I want to use this new feature for regional net. So with that, like, I'll be like in a few minutes, I can create like a cluster, roles, and VPC all together for my environment. As you see, like, all of that will be created in a few minutes. The only thing that takes a little bit is the the that gateway to be there and propagate it, but then we have like now. IM roles, networks with subnets, everything ready and also Auto mode knows that we want to run the pods just on the private networks. We don't want to have pods on public facing networks with public IP addresses, so we are protecting our pods by design. And if you want to dive deep, there are like there's some specific uh configurations that you can go like uh all the pods and services networks are already there and there's some stuff that you can opt out after you create your cluster. So pretty, pretty easy, pretty straightforward. Now we have a cluster running, uh, in a few minutes like we'll have like the cluster activating, uh, then creating one thing that I want to mention is. The only add-on that will be created is the metric server. So all the other add-ons like for core network capabilities, carpenter, pot identity, everything is embedded in the cluster to simplify your daily operations. The only thing that you see there is the metric server that's not embedded, but metric server is required to have like that scalability and the cost optimization that we talked about. So we see clusters there already active. Now let's enable the network policy on this cluster. So the cluster comes with the network policy already there as a capability from the, but we need to tell them on the, like I said, on. On that specific config map that we want to use network policies, so we just have like those two pods everything is is away because the goal of EKS auto mode is for you to just deploy your applications. So like, like you said, like you saw like there's nothing there other than the metric server and the nodes where the metric server is running. So let's see the note class uh where I show the that configuration for um. For the default behavior of the network policy, so let's take a look on that configuration file. I'll just filter the output so it's easier. We don't have like a big one. So we have network policy as default allow and events logs are disabled by default. Event logs is just to everything that happens, everything that matches a network policy will be logged on the Kubernetes events log. So I have like this uh manifest already done and I'll just create it, just double checking it's not there yet. I'll just be enabling. That specific config map so we can have network policy support fully enabled in the cluster. So we cover like this this bottom part for simplifying overall infrastructure with those core components and being able to enable network policies. Let's talk a little bit more about network policies. Like I said, this was introduced in 2023 as native feature for EKSCNI, and it works a little bit different from the standard network policy that you can get from the community. So it's a single plug-in, fully integrated. You don't need to deploy anything else and uses EBPF to be performed as it is today. Quick recap on what's EBPF. So EBPF means like extended Berkeley package filter, but it's basically a kernel engine or a Kernel module that's enabled and filter all the SIS calls that's happening from the user. So you have like an application or anything that does a CIS call EBPF will. Inspect that package and then go back to the Linux kernel after inspected. This is like very common use case for load balance steering, security, and tracing, but our goal here for network policies is security. But we'll be talking, but we'll be using for tracing as well, but I don't want to spoil the surprise that look will be presenting. And how does it work under the hood? So a user can create a network policy. The network policy controller will read it and trigger an endpoint CRD that's exclusive for ECI, and then the EKI using EPF will inspect that package and decide, hey, should I apply a network policy restriction or isolation here or not? When we do like this, this kind of changes and we apply network policies and change the default behavior for that node class, a specific rollout happens. So we cover the the the the AC2 node class in the the the previous slides, and the other component from from Carpenter is the note pool. So the noteb defines the behavior, whereas the the node class defines the baseline infrastructure for the nodes. So we're just getting like this one isolated. The disruption that I'm highlighting on the on the slide is just the way that the rollout will be will happen. So for example, I want to disrupt just 10% of my nodes at a time. Let's say I have like a huge environment. It will not disrupt and roll out and apply network policies or any other behavior for more than 10% at a time. So this is this is just an example you can do like 1%, 50%. And the node expires from time to time, uh, to renew the, the, the nodes and apply new patches and new configurations like I said. So let's see this in action. This is the cluster that we just created. Now what I'll do, I will deploy an application here. So this application consists of like a few name spaces, a few deployments. I'll just use like uh customization for the sake of time, of course we should be using like GitOps approach for this, but this is a talk for for next year maybe. So I created all of the the research components, we have our application running and because the policies that fall out, everything communicates to everything. Remember overly permissive environment. So we can just get like all the pods. Now let's let's get like the the URL for us to access this application, see if it's, if it's really running, and if it if it's there and behaving as expected. So let's get this. Let's paste on our browser. We have just a specific secret shop. We can buy like anything here. Let's just explore. So we have on this application we have this UI. We have a catalog for all the products. We have the carts and we have the orders. So all of those pieces need to be communicating together in order for this shop to really work. So now what we're going to do, we're going to apply a default deny policy for this environment and let's see what happens. So remember, today is the fallout. Let's change this to a default deny policy and wait for Carpenter to do the rollout of all the nodes for the sake of time. Like I, I cut the part of the the the rollout because it takes like a few minutes, uh, but you see like it, it won't take more than 3 to 5 minutes on the. The pod history. So let's change the network policy for default deny and enable the event logs so we can see what's actually happening on the event logs if we need. So done, let's watch for the nose behavior. We didn't trigger like the nose have like around 20 minutes. Now let's watch. In a few seconds everything will be triggered. And you see like new nodes running with the new configuration, so this is what the rollout's about. So see, we have like a new node, like this is the time that Carpenter takes to spin up a new node. It's really, really fast. Now let's go to our application and check. It doesn't seem to be running. So on this time you receive a call from your CEO or from a financial person that will say, hey, our application is offline. What you have done, just roll it back. We have the new nodes there and the applications are not really communicating, so what we are going to do. We see, you see, like the applications weren't even able to spin up because there's no communication. Liveness probe, redness probe, everything just failed. And how do we investigate this if we don't have like proper visibility on the network, even if we have access to the logs, it's not traceable. We don't have like a clear visibility on who's communicating to who. To to approve like this configuration, so let's do what like our other team asked and then go back to an overly permissive environment, and this will sit there forever. Like the application will go back running, but then how can we fix those things to be back to a controlled environment with our application running in an efficient way. Right? So how do we control access? Let's say we go back to the default deny and then we need to apply a network policy to open the communication. Uh, to the orders. Uh, name space from the catalog from the UI name space, right, so you can see like this is a this is a quick example of a network policy. So on the, on the spec we have like the pod selector, uh, so all the orders, uh, pods on the order name space is allowed to receive ingress communication from the UI as you see in the bottom, but then you need to open for the checkout as well. And then you need to go to the UI and say hey UI, you need, you can go to these two name spaces, and then you need another policy and another policy and then what about DNS resolution? Do I need to open communication to the core DNS so I can have name resolution? Do the spots need to access the internet? Who talks to who? And all of these questions falls into the platform team because they don't have visibility. Even if the application teams know where their application is going, they usually don't know who's accessing them. So how do you overcome that if we don't have proper visibility? So we cover like the top layer to improve overall security, right, but if we go to an over overly restrictive environment, things break. Something's still missing. Can you help me out with this one? Look. Absolutely. So as Rodrigo pointed out, it's very hard. To accurately optimize without sufficient visibility, you can have the capabilities. But normally what ends up happening. a lot of customers end up bouncing between two extremes overly permissive and overly restrictive, and that's what impacts their environments. So there's a critical layer that is needed, and that's the missing piece from the diagram that Rodrigo just showed, and that is essentially observability. And we're really excited for this new feature that we recently launched, container network observability in EKS. And this is a key or fundamental aspect to achieving the right kind of optimization you want for security as Rodrigo's been emphasizing on but also for performance and also for cost and ensuring that traffic behavior in the cluster aligns with design. This allows you to essentially have your EKS network environment a lot more observable, which is very important when it comes to successfully scaling in widely distributed environments. You need to be able to track east to west traffic within the scope of the cluster, but also you want to be able to track traffic between pods and certain AWS services. In addition to that, as I saw some hands raised, a lot of you are also running applications in the cluster that are speaking to services outside the scope of AWS as a whole. Now there's some key ways that this new feature can essentially enable you or empower you in this particular regard. I wanna focus on the use cases before we get into the specific feature sets with container network observability. So the first one that I wanna touch on is measuring network performance. So this is, this is especially important because this is something that a lot of customers run into to be more specific platform teams, being able to quickly pinpoint where an issue originated from. A lot of teams have already standardized on their observability stack, and a lot of customers will essentially be sending data from their Kubernetes cluster or their EKS cluster to some kind of observability stack that may be managed solutions within AWS. You could be using CloudWatch. You could be using Amazon managed Prometheus and Amazon managed Grafana, or you could be using open source solutions and managing them yourself with Prometheus and Grafana. And so this was especially important for us to ensure that if we're going to enable our customers in the right kind of way, then we need to align with your usage patterns. And so we're providing more key metrics now that are a lot more granular at the pod level and the worker node level that are related to networking, allowing you to quickly pinpoint issues like um network congestion at a pod level, seeing the ENIs that are associated with that, being able to quickly determine if there was a bandwidth exhaustion issue in a a specific worker node, whether it was related to ingress or egress, being able to track the flow count for pods. And being able to see in a very quick way. What the particular issue is, and that reduces the meantime to detection as well as mean time to resolution, which is extremely important when incidents arise, measuring your net the network performance to detect anomalies. And in many cases customers essentially have thresholds that they'll set. And once that threshold gets breached, they can proactively respond to these issues before they get worse. Now in addition to that, what we've seen is that is normally the starting point. You'll have some kind of observability stack where some alert, alert or alarm will notify you. But after that, there's a tail end of the investigation. You want a way to scope down the particular area where the issue originated from. You want to double click on it essentially. And then carry out your investigations. Now sometimes this can be especially painful if you're solely relying on TCP dump in the cluster. Can anyone relate to that? That can be a lengthy and painful and costly process as well, and so we thought it was especially important to provide additional capabilities for the tail end of the investigation as well, something that complements proactive anomaly detection with whatever observability stack you're already using. And so there are now native visualizations in the cluster as well that you can make use of, being able to track uh east to west traffic within the scope of the cluster, traffic between pods and AWS services at launch we support traffic between pods and S3 as well as Dynamo DB, but then also being able to see pods that are speaking to uh cluster external destinations and more specific destinations outside the scope of AWS as a whole. You can also leverage these native console capabilities to track the top talkers. That's something that a lot of customers are particularly interested in. What are the workloads responsible for the most traffic that is being sent within the scope of the cluster or outside the cluster? Being able to see traffic that is traversing different availability zones. How do you verify accurately that the cost optimization techniques that you're adopting are actually working as expected? And so a lot of what I've been speaking about is essentially captured in this diagram over here. We essentially took a closer look at the usage pattern among our customers, and we didn't want platform teams to have to deviate to fit into a new norm, but rather how can we make the EKS network environment more observable, aligning with what customers are already practicing. And so what you'll see is teams will essentially be relying on data. That they can leverage to measure performance, that is a key one for a lot of customers. How do we accurately measure the network performance in our environment? And so you've already got some observability stack and so the new metrics that we're providing are available in open metrics format and EKS is using Amazon Cloud Watch network monitoring. So this is a feature that was launched last year. And so we partnered together to further align with EKS use cases, and so Cloud Watch Network monitoring has an EBPF agent that runs on the worker nodes and it captures the network flows, to be more specific, the top 500 network flows on every worker node. As well as the flow metrics associated with each of those flows, and so we have two sets of metrics. We have system metrics as well as flow metrics. The system metrics can be scraped directly from the worker node and sent to your preferred monitoring solution. So if you are using Prometheus, for example, then you can directly scrape these from the uh from the NFM agent. Alternatively, you may be using Amazon managed Prometheus and you could use a scrapeless mechanism as well. To to essentially ingest those. Now, the tail end of it, as I pointed out, you'll see that there's an arrow from the platform team that's also going to the EKS console, and that is where you'll essentially find the additional capabilities that we are now providing to help you in accelerating and having more precise troubleshooting. Now I think what you're Looking forward to seeing is what this actually looks like in action. And so if you're using the console to enable this feature. You would essentially just click this one button over here or this check box rather to enable network monitoring and and it will automatically create the underlying dependencies that are in cloud watch network flow monitoring and we've documented these clearly for you to see exactly what will be provided and we'll also guide you through the process of installing the relevant NFM add-on. Now in this case over here it's already enabled and so you can also see the capabilities over there. There's a service map and a flow table, and those are provided in the console. The performance metrics endpoint is something that you can leverage and directly scrape from. We can see the status over here of the monitor. So from what we gather, everything looks like it's already set up and enabled. And so we're going to follow the normal workflow that a lot of customers already use, and so the starting point is typically you've got your own observability stack, as I pointed out. So this is a grafauna dashboard over here, which is something that a lot of customers rely on already. They're already standardized on using grafauna, and so we fit right into that. And so over here this is a dashboard that actually visualizes all the key metrics that we're providing now, and you can see the top 10 pods by Ingress bandwidth. And this allows you to actually track these things. We've had a lot of customers speak to us to say, how can I easily understand the amount of bandwidth being leveraged by Core DNS, for example, because that's a critical component that lots of pods are essentially communicating with in order to resolve their, their communication with other pods. We can do the same for egress traf egress bandwidth as well. We can look at the ingress bandwidth allowance exceeded. And detect any anomalies associated with the worker nodes and we've got the metadata that you can capture that is captured here as well and you can see that in your Grafana dashboard now. Everything consolidated in one place fitting right into your existing usage pattern. We can also see the packets per second allowance succeeded, connection tracking allowance succeeded, and you can see all the worker nodes that this is associated with. And the key thing here again is to further enable you to easily detect an issue and trace it back to the specific point of origin without having a long stretch in getting to what the issue actually is, detecting it as well as resolving it. You also have the ability now to track the egress bandwidth for your pods, as well as the ingress bandwidth, ensuring that this aligns with what you expect, or if you need to be modifying your environment, To align with usage by your customers as well. We can see as well for the TCP flow count for the different pods. We can see the packet rate from ingress rate as well as egress as well for the pods. Drilling in more and more and giving you more granularity so that you have sufficient information. And as Rodrigo pointed out, before we get to the point of optimizing things, we need to have this detailed layer of visibility that can give us informed decision making and that empowers it, empowers our customers or enables them even better to be able to respond quickly to issues when they arise. Now the next thing that follows after this is you may get some kind of page or alert that would happen. And you would have sufficient information. To then zone in on a specific cluster. Now in this case we have a very basic e-commerce application that uses the graphQL API, and this is just showing again that this is already enabled. So I'm gonna switch back and we're looking at the service map now. And this is specific to the e-commerce name space. We've got 3 microservices here. We have a GraphQL API that speaks to an orders application and a products application. You can change time if you need to scope it to a 1 hour session or if you want to reduce that to 5 minutes to see what's happening. We can click on any one of these flows. And then what we see here is forward direction as well as reverse direction in the case that there is bidirectional traffic. We can zone in even further because this is the kind of granularity that becomes extremely useful, being able to see the flow between pods that are actually talking to each other at a specific replica level. And because this can be a bit noisy, for example, for the last 1 hour or the last 30 minutes, you can also change this if you want it to be for the last 5 minutes. And again you can zone in further and see the amount of data that was actually transferred on these specific flows, and this is where the flow metrics come in. You can also see retransmissions and retransmission timeouts if you're trying to detect latency, for example, the higher your retransmission timeouts, uh, the more you see a significant impact of latency between specific pods, and you'll be able to use these kinds of capabilities for that. Let's drill in even further. We can go to view details, and what we have over here is actually a flow table that allows us to see at a more granular level the flows for the different pods that are talking to each other in this specific name space. You can filter for more granularity and precision. Say we wanna look at flows for where the local pod is is a graph QL pod. We can scroll to the side there again. We can still see each of those flow level metrics retransmissions, retransmission timeouts for remote pod. In this case, I'm going to choose orders. And there we see the amount of data transferred. And you can see right at the top there we've also got events so this links to the uh resources for you to be able to see any pod events in the in the event that there were particular issues that you saw as well. Just sort of consolidating things even further for you to have this um additional information when you're carrying out your troubleshooting or debugging of issues. And next up we're going to look at a photo gallery application. So over here we currently see that there is nothing. Might take a little bit of time. We switch it to 15 minutes and there's nothing going on here. So we're going to switch to the flow table view. And what this photo gallery application is actually doing is it's essentially just uploading images to S3 and then storing metadata in Dynamo DB. So if we go to the flow table, you'll see that there are 3 perspectives over here. There's the AWS service view, which is pod to an AWS service. There's the cluster view, which is east to west traffic within the cluster, and then there's external view, pod to a destination that is outside of AWS. Now in this case we can see here for our photo gallery application, we can see the pod is actually the pods that are speaking to S3 as well as Dynamo DB. We can also see the volume of data transferred. We can see the local pod IP and the remote pod IP. If you want to modify the columns that you're seeing, you can also do that and customize it for what's specific and more purposeful for the way you carry out your investigations within your team. Data transfer is one that is typically really important for customers to be able to see. I'm going to switch to the cluster view so you can see this, and for the cluster view, I'll switch back to the e-commerce name space. At the moment here we'll be seeing things for the photo gallery application, but we switched back to the e-commerce and you'll see that now we're seeing about 300 flows that are represented over here. And again you can change the time range, and of course the greater the time range, the more flows that you're going to have for this particular demo. You'll see it'll probably only go up by 2 additional flows, so we'll see about 302 flows represented. There you go. If we switch back to 15 minutes, it's about 300. And similar to what we saw when we were going through the service map, even here in this flow table you can filter further if you're looking for something specific again this fits within that workflow that we pointed out that I pointed out earlier where you would have started from your observability stack you would already have some kind of alert that's giving you sufficient information to know what you should be looking for so that when you come to the service map or the flow table you have enough context to just accelerate the troubleshooting process. As I mentioned earlier as well, this is something that can also inform the cost optimization techniques. Customers are typically interested in seeing what what uh traffic is going from AZ to another AZ. So that's something you can also leverage here. And something that's also specially important in this case and very relevant for this particular session is the fact that you're, you're able to actually track the traffic behavior within your cluster, and you can now see is this aligning with the with the network security policies that I've put in place. This can now be the key thing that informs how you define your network policies and is especially important as you're shifting left and you're doing this essentially in the dev environment instead of having to find out in a in a production environment when things are going wrong. And what we have over here is the historical explorer which essentially launch pads you to the cloud watch network monitoring dashboard where you can see this in the broader spectrum of things, and it is especially important to kind of see that you now have that consolidated transparency between the Kubernetes network plane as well as the VPC network plane as I pointed out earlier. So you can see over there in corresponding network path you can be able to correlate a specific flow that is taking place within your EKS environment or your Kubernetes network context and see how that relates to the underlying VPC constructs as well, being able to see the specific EC2 instance and the ENIs associated with that flow's particular path. And here in zoning in, particularly on the Isteo Ingres Gateway. And we can see the corresponding network path and each one of those flow level metrics are also visible here. You'll be able to see the retransmissions, the retransmission timeouts, as well as the volume of data that was sent. If I switch back and now we can see the photo the photo gallery is now generated in the service map and similar to before as I showed with the e-commerce application as well we can see the API gateway is talking to both the upload service as well as the gallery service and similarly if we want the more detailed view where we're seeing flows between the specific pod replicas, we can have that represented as well in the service map. So all of this is in the tail end of your investigation but meant to help in accelerating things. Now a key thing that we also see a lot of our customers doing is making use of a service mesh implementation. And the reason why we wanted to highlight this as well in this session is we wanted to know, wanted our customers to know that what we're introducing with container network observability is complementary as well, and in many ways allows you to be less reliant on a service mesh implementation for observability. We've heard a lot of our customers say that they would adopt the service mesh primarily because of observability, and now it no you no longer need to use it primarily for that, but rather you can leverage it for other features as well. And in addition to that, we found that there would be sometimes an inconsistency because not every workload may be running in the mesh and so you may have a certain level of observability at the network layer for certain workloads that are in the mesh, but any workload that is outside of the mesh doesn't have the same level of observability and now that can change because you're using container network observability that will surface the sufficient data whether an application is in the mesh or not. You can still use your service mesh for advanced networking, for fine grain traffic control, and an advanced load balancing mechanism. You can still use it for end to end encryption like MTLS, which is a common use case. And as I already pointed out for detailed observability, there's some key metrics that you can get from service mesh implementations like ISTO. Envoy emits a lot of great statistics. But it is also worth noting that adopting a service mesh implementation does typically introduce some operational overhead as well. Managing a service mesh at scale can be something that's particularly challenging. And so by introducing container network observability, how this helps our customers is they no longer need to adopt an entire service mesh just for a single piece of it. But rather they can make better informed decisions as well and seeing what capabilities they can actually get out of the mesh, knowing that they also have additional native capabilities provided by EKS that can help them in their journey. So we've seen the different layers that we've been taking over here. We started off by highlighting how we were making it easier for our customers in an operational layer with EKS auto mode and taking on that responsibility of managing those core networking components and then Rodrigo also highlighted the network security capabilities that can enhance the security posture of your of your environment at the network layer. But we've also been trying to drive home the point of how this is especially difficult without sufficient visibility. And now with container network observability, you can make more informed decisions and more accurate optimizations as well. How cool is that, right? So now that we know what's going on in our underlying underlying network, we can shift that security. So using that service map we can understand what are the pod to pod communication, what are the east-west, or even north-south communication, and we can start narrowing down the access control. So let's go back to that cluster that was broken with the applications not running and let's enable that. Default deny again, but, but at this time we will create the network policies based on the rules that we observed using the network observability capabilities. But then you can tell me, hey Rodrigo, I can use GAI to create the network policies for me, right? I tried that and I have to use the network observability feature to really understand what was going on because the network policies that the model generated to me didn't work. The application was still broken, so I was able to create the network policies now. Everything is there. You have the cluster running. The application is still running, but we still have default allow policy. Now we're going to change that. So this is what we have done. So we observed you can use that in a development environment. You can. And understand your networking map you can create your network policies and then change to a default deny and these applications will be ready to go to production already with with network policies already embedded on that. So that's the shift left security and is a shared responsibility between both platform security and application teams, right? So now we have like the default deny policy set that. We can have, we can see that I have several network policies running on the cluster, each one to with specific things, and I have the application still running. I can explore the catalog. I can put things on my cart, and I can finish, uh, buying. So this study from CNC demonstrates exactly that. If we shift lab security, the cost to fix a security issue in a development environment versus a production environment is 640 times more. So that's why we should shift less security on this case. So we covered all of those layers, but then I was talking about like application development and shifting lab security and putting everything together with the application, so there's more. Yesterday we announced the managed capabilities for EKS. So this is a brand new feature that was just announced really yesterday at 4 p.m. So these new capabilities are managed capabilities leveraging open source tooling like Argo CD, KRO, and ACK where you can use EKS to manage your entire platform and application life cycle stack. So this will be overall increasing all your velocity and time to market using all of these managed features from the baseline with EKS Auto to network observability, network policies, and now managed capabilities. So what are the key takeaways for us today, Luke? Thanks Rodrigo. So as we pointed out and I've been emphasizing throughout the session is what we're seeing is our customers are creating complex environments that are increasing in their distribution, they're growing in their overall scope in their heterogeneity, and the network is a core fabric or a critical layer and in order to sufficiently optimize according to whatever your standards are, you need the right kind of visibility. In addition to that, we've pointed out how having the right level of security in your network environment is actually going to enable you to move even faster, so we hope you see this tie or this connection between them. Informed decisions that can help empower you in making the right kind of tactics for the security policies that you're going to be rolling out with a lot more confidence knowing what the expected behavior will actually be. And as we've highlighted just in the previous session with uh in the previous slide sorry that Rodrigo just pointed out with EKS capabilities, uh, our commitment is to increasingly simplify operations for our customers and and so taking that layered approach we can see now how EKS capabilities fits into that as well. So we're very excited for you to try out these new features container network observability and EKS and of course what was just launched yesterday EKS capabilities. Rodrigo and myself will hang out a little bit after this. Uh, we'd love to chat to any of you that have particular questions and look forward to hearing about how you use these features in your environments. Thanks a lot for coming for the session.