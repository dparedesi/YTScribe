---
video_id: soZfsq46bbA
video_url: https://www.youtube.com/watch?v=soZfsq46bbA
is_generated: False
is_translatable: True
---

Today we're gonna cover the topic of Amazon, uh, managing capacity, to capacity and availability and the trade-offs that come with it. Uh, with me, um, I got Brian Flippe. Brian, would you like to introduce yourself? Sure. Hi everyone. My name is Brian Flippe. I'm a principal product manager, uh, at an EC2. I've been with AWS for about 8 years now. Uh, fun fact about me, today is Cyber Monday, uh, so don't forget to, you know, go to Amazon and, and. And check out the deals, but I used to be, when I first started at AWS, I used to plan capacity for Cyber Monday. So I, I live, eat, sleep and breathe, uh, capacity, so I'm really excited to be here with you. And in my case, I'm Carlos Manzaner Doreda. I'm the tech lead for compute. I'm an essay. I help organizations to find those trade-offs and especially on optimization, um, and everything that comes with the trade-off for that optimization and obviously capacity capacity and availability also together with price, performance and cost is um among top of those things. Before we start with the agenda, I would like to cover a couple of things. First one, Um, please show hands or raise your hands if you are part of the financial operation of Fin-ups or have been doing fin-ups. OK. I was expecting that. How, how many of you are coming from a platform engineering team or basically managed capacity as part of either an organization or a platform team? Perfect. So you see again. Um, part of the audience with you on that one. And finally, who of you are application owners or perhaps developers and are basically looking at things as availability and so on? Awesome. So you see that more or less that covers pretty well the audience. I probably left a few personas, but. I'm sure that you have to balance those trade-offs between those things, and this is what this talk is gonna be about. We're gonna cover first what is the challenge, and again, with the framing that we have set up, you more or less know what it's gonna be about. We're gonna cover what are the usage models. Is it the usage model that will help us to define for different types of workloads, what is best for your application. Starting with on-demand, on-demand capacity reservation, capacity blocks. Sport instances. And finally, we're going to introduce a few things that have been released in the last 2 months, or even in the last week. That are perhaps a bit new. This is a 300 level session, so there's expected some level of hands-on proficiency on the topics that we're gonna cover. For those of you that perhaps there's new things that we're covering, this is gonna give you the pointers to do the homework and learn how you can apply as part of basically your day to day activities on that capacity planning availability and so on. We will end with some key takeaways. And let's start with that challenge first. We were talking about these trade-offs and how we apply those trade-offs. I'm sure that we have heard that, hey, cloud and capacity is infinite and we know that that's a reality and that reality is. Uh, those physical limits of and how we manage that capacity internally, which is what we will cover over here, and there's a trade-off that comes with that, um, that comes with how much capacity is available and therefore what's going to be the pricing model and what's going to be the low prices that we can get with those, how we can maximize that efficiency. Just to give you an example. Just to give you an example of that. You probably have heard within that balance of spot instances. So this is a spare capacity that we got in our data centers, availability zones and regions. And we put those spare capacity at your use for a step discount of up to 90%. Um, but the trade-off is that those instances can be interrupted. So there's a trade-off with that availability as you see. We're going to be exploring. All those different options that you got with the different usage models. So you can understand what that level of capacity versus availability versus utilization. Um, goals. Let's deep dive first on on demand instances. Probably this is the usage model that you are more familiar with. On-demand instances are basically for your elasticity, just to get that capacity when you need it and at the point that you need it. So it simplifies a bit with the pay as you go model. When do you need that capacity and when are you going to get that? We've got more than 1000 instances at this point in time. My recommendation is go and pick the latest generation of those instances. Typically they will come with the best price performance, so that way you basically get selection of instances that is easier to manage. And we offer those instances in more than 120 availability zones and 38 regions. We also offer them on local zones, so you can manage also that capacity and availability. In places where latency is key for your customers. Now the way that we use on demand businesses is typically on stateful workloads that have that scaling patterns, so cyclical patterns, or perhaps you have some um time of capacity that you needed like spurios or temporary short term workloads that you need that capacity. And for those times where you are not ready to commit for a longer term, uh like saving plans and you like to basically testing and wait until fingers scale so you're ready. Uh, to that commitment. It's important to understand how those capacity pools for on-demand work. At some point when you are selecting a particular instance type, um, that pool can go down and you may suffer from insufficient capacity error. We on the back do lots of things to make sure that that doesn't happen and we present to you that elasticity that we were talking about. Behind the scenes, we are definitely doing that forecasting to get the capacity up front. We are doing also a placement in the regions where that capacity is needed and removing any bottleneck to rack and stack as fast as we can. Behind the scenes, we're doing things like pool balancing, for example, where you have an excess or we have an excess of 16 X large instances for a particular type of instance. Um, if at some point there's a need for 4 Xcel, we will split those instances and we'll we'll create 4 out of those 4 Xcel instances. So we'll be creating that supply and demand, but still, there's going to be kind of uh at some point if you select only just one instance type, a point where a particular pool may dry out. Just to give an intuition of that capacity, we just released an M8i, so the new Intel software rapids. About 2 months ago. About Months ago we released also the M8A, the Turing version. I would recommend you just taking a look at those. As you may expect, even if we are racking and stacking as fast as we can. These are new instances. So there's going to be less capacity of those. Even if those are just preferred instances, you need to take into consideration that capacity availability factor that you were just mentioning. One way of doing that is being flexible and we are Vegas, so let's use the analogy of affair um you may want to go for the ravioli. And perhaps at that point in the buffet there's not gonna be ravioli, so, so bad. Um, I'm sure that if you are flexible and you like other pastas, you are gonna be still getting the right meal. And not only that, if you are flexible with the rest of the food, you'll get even a better meal out of it. So there's that concept of flexibility that improves how you tap on different pools of capacity. You can expand that to protect yourself effectively when you are using on-demand instances and still go to your preferred instance but know that you can always protect yourself with ice if you are a bit flexible. The way that you apply that flexibility is either using different in instance families or perhaps a different CPU manufacturer in XX86, Intel and AMD. Different generations or different sizes. I understand that a few of you for especially large organizations may do some level of qualification. Of instances just to verify that those instances match the service level objectives that you need. When you have Deterministic workloads need deterministic performance needs, this makes a lot of sense. I would like you to challenge those conceptions. I think that in most of the cases, even when you qualify, what you're after is not a deterministic level of performance. You're after a baseline, and after that baseline you can use other instances. For example, if your baseline gets met with a generation 7 instance, probably generation 8 is going to be still equally valid, OK? So you can think about those levels of flexibility that we just mentioned. Applying that flexibility is easier than you think. We have embedded all the ways to do the heavy lifting, both on easy to fleet and out of scaling groups. And those integrates with the rest of our service. For example, behind Carpenter, you're gonna find easy to fleet making that selection and allowing you through the note pulls to create uh that exactly elasticity and flexibility through multiple instances. And the same thing goes for auto scaling groups that integrate with AWS batch. With ECS and so on. Now, the way that you use it, super simple, you will define. Or create a set of fleet auto scaling group, apply that flexibility that you are after and effectively let that fleet auto-scaling group manage the instant life cycle for you. We introduced it about 2.5 years ago, 3 years ago, the concept of attribute-based instant selection. This is a way that allows you to define what is your flexibility and your instant selection through properties or attributes. For example, you can say, hey, I would like any type of instance that offers me this many BCPUs, this much memory on a latest generation or generation 6 onwards. And on the storage type of EBS. And that will define the type of instances that create that flexibility group for you. You can still define perfectly, that's perfectly fine to define the instances manually too. When you define those instances, for example, just to give you a pattern that you can use for on-demand instance to protect yourself, you can start defining with a prioritized allocation strategy. A group of instances and you will list them in order, you will start saying, hey, this is my preferred instance. But if I cannot get that instance, I'm OK to get my 2nd preference and my 3rd preference and my 4th preference. The main idea is that you are protecting yourself and making sure that that scaling group, if you cannot find your preferred instance, it will jump onto the next one, so you will not suffer those eyes, right? So this is a pattern that I would recommend you. Allocation strategies can also be used in the case of spot, for example, as capacity optimize allocation strategies that will pick instances from the deepest pools, so they will avoid frequency of termination or an increased frequency of termination. And if you are price sensitive, we have allocation strategies also that are driven by the price optimized. So you'll list all the instances and we'll pick the ones that are optimizing for price for you. As an example of this attribute-based instant selection and how you apply it, you got in this case, hey, I want 8 CPUs, uh, 8 gigabytes of RAM of type graviton, and on EBS storage. And that automatically will create that flexibility group that includes M8G 2 X largest, up to, as you see on the left corner down C7GN. So these are network specialized instances and we'll pick up from those. So it's uh easier effectively to plug that uh model. So if you want that deterministic level of performance or specific instance types, we're going to move to on-demand capacity reservations, right? Thank you. So like Carlos mentioned, uh, if you really need to optimize for a specific instance type and you need to have the capacity there, you can use a capacity reservation, which is kind of on the other end of the spectrum of, you know, cost and availability, uh, capacity reservations, you essentially hold or you pay for the capacity the entire time you hold the reservation. So let's talk a little bit about what what capacity reservations are. Um, so kind of the acronym for capacity reservations are on-demand capacity reservations, or you might hear me call them ODCRs, uh, and essentially with these you're actually reserving a specific instance type and availability zone combination, uh, so we look kind of at the capacity availability hierarchy here, uh, we have on-demand and on demand I would say is world class in terms of the availability that we can give to you with all the different num number of pools that we have. However, as Carlos mentioned, You can actually go up in terms of your availability by accessing multiple pools by being flexible and then finally, if you really, really need to have that capacity assurance there, that's when you would use the capacity reservation and the reason the way it gives we give you assurance is we're actually setting aside that capacity for you, but because we're setting aside the capacity for you, we're not letting anyone else use it or or or run instances on it. And so we charge the entire time you hold the reservation. If you're running an instance within the reservation, no additional costs at all. It just looks like a running instance in your cost and usage report. But if you're not running an instance, that's when you'll be charged for an unused reservation, which charges basically the same thing as a running instance. But the good thing about capacity reservations versus especially if, if any of you have been with AWS for a long time, things like zonalRIs with capacity reservation, there's no commitment period, you just, you can cancel it anytime you want to. Uh, there's no, no 1 or 3 year commitment period, I should say. So when, when should we use an ODCR? First is if you don't have flexibility, and that picture is a picture of me, uh, getting up this morning. Um, but when you, when you don't have flexibility, if you're, you know, you're at the buffet, like Carlos mentioned, and, and you really need ravioli, you know, at 6 p.m. on a Friday night in North Las Vegas. That's when you would wanna, you know, call and make a reservation to make sure that you, you're able to get your food, and that's the same thing with the capacity reservation. If you really need a specific instance type at a specific type of time of day in a specific availability zone. You probably need to use a capacity reservation. The other time that we recommend using capacity reservations is this if your workload is just so critical that you absolutely need to have, you need to be able to spin up instances or bad things happen, then that's we, we also would recommend uh using a capacity reservation. So there's kind of two different ways you can create the capacity reservation. You can create it on demand, which means creating it based on what capacity we have in the pool. So when you create it on demand, we'll take capacity and set it aside for you, or you can actually create it at a future date. Um, so for on demand we often see customers creating the on-demand portion of the reservations if they have a workload that takes a while to scale. So if I have a workload that it takes me, you know, I need to provision a single instance at a time or a couple instances at a time to be able to get up to my scale, it can be really frustrating to get 90% through my deployment and then run out of capacity. So with the capacity reservation you can actually get a synchronous call to say is the capacity there. It basically says yes or no, is the capacity there? And if it is there it creates the reservation for you so that you can just spin up those instances at your leisure and know that it's going to be successful. The other time that we see customers using the on-demand version of capacity reservations is when they have already running instances and they want to cover those running instances with a reservation. Like I mentioned, there's there's no extra cost for covering a running instance with a reservation, but if you need to do maintenance or bounce your fleet, uh, you'll be able to do so and make sure you can get that capacity back when you, when you do the the the reservation. So the other type we have is future data capacity reservation. And there are a few different use cases for future capacity reservation. One of them would be like a peak event like, uh, Q4 peak like we're having right now with, uh, with Cyber Monday and Black Friday, making sure that we have capacity for a big event. Uh, Another would be if you're performing some big migration and you want to switch to a new instance type, uh, a future data capacity reservation can help you do that. And then finally if you're increasing your baseline stair stepping it up on a future capacity reservation also can help. And the way that it works is it's just kind of like you're, you're placing an order like, like we talked about you're calling the, the, the restaurant, so you request for a capacity reservation for the date that you need it, uh, so it could be anywhere between 5 and 120 days in advance of, of what you need. Uh, and then you specify how long you, you are planning on holding the capacity typically because we, we might be racking and stacking new, uh, new servers for you or setting the servers aside, we ask for like 14 days, uh, of usage out of, uh, after the reservation. But you'll get an answer or you'll get a reservation created right then and there and it'll be in an assessing state, uh, and then within about 5 days, uh, we'll come back and, and schedule, put that in a scheduled state, uh, for you and then you just need to wait for the capacity to become available and, and you can launch your instances. All right, so we've talked a lot about, you know, what ODCRs are, how to use, or, uh, how to get them, what they're for. Uh, so now let's talk a little bit about how we use them. Uh, so with an ODCR we kind of, it kind of works on this principle of matching. So your instance needs to match your reservation and then the two will marry up and, and, uh, you'll be able to launch your instances with it. So there's 4 kind of primary things that you'll need to match with your instance launch and your reservation. Uh, so first is instance type, whether it's, you know, C6I.exlar or M7I.exlar, whatever the instance type that you need, the, the, uh, the instance that you're launching needs to match the reservation. We also have availability zone, uh, platform or operating system, and then finally tenancy. Tenancy is whether it's single or shared tenancy. Most of our customers just use the shared tenancy. Uh, now, optionally, if you are using a capacity reservation with a cluster placement group, uh, also aligned with it. Uh, you can, you, you'll need to match that, that placement group, uh, ID with your, with your ODCR. Basically cluster placement groups, they help with, uh, things like high performance compute. We put all, makes it so we put all of the instances in a really close, uh, co-location with each other so that you have really fast latency, uh, between the instance types. Now finally, uh, you have to think about instance eligibility. Now I would say for probably 80-90% of use cases, uh, your instance eligibility is just gonna be kind of the default or open instance eligibility, and, and what that does, it kind of acts like a public pool of capacity that anybody can tap into. Uh, the default when you launch an instance and the default when you create a capacity reservation is always gonna be open and so it always just kind of works and works like magic. However, if you have like a multi-tenant account where you're trying to protect capacity from somebody else in your account from using it, uh, you would wanna use targeted and targeted basically means you, the, when you launch an instance, you have to specify either the reservation ID or a resource group that the reservation is in, and then that, that way you can kind of protect and create a private pool of capacity, uh, versus that public pool that the openness. But typically it's multi-tenant accounts that that are using the targeted. All right, so going to, to kind of back to what Carlos said, you know, if you architect for flexibility, you still may want to use capacity reservations from time to time, especially if you have spare capacity reservations that aren't being used but you wanna save them for another, uh, for another time. So with an auto scaling group, you know, how do we do that? How do we make sure that when the auto scaling group picks the reservation. Uh, say, say that it, it wants to pick M8G, 2 extra large, the top left there, but I have an M7G, 2 extra large. How do I make sure that the auto scaling group is marrying up with my capacity reservation? And so last year we launched something called ASG Capacity reservation Preference, and with this you can actually set that preference so that you get better utilization of your capacity reservations. So you can either use capacity reservations only, which basically means if you have a capacity reservation that matches something within your auto scaling group, it will launch the instance and it will launch it using the capacity reservation. Um, but if you don't have a matching capacity reservation, it will not launch in an instance, and you'll get an error. You can also use capacity reservation first, which is kind of similar, but instead of when you don't have a capacity reservation, it'll actually try to use on demand. So it'll first try to look for a capacity reservation to use. If it doesn't have one. Then it will fall back to on demand and go back kind of through that list that Carlos was talking about where you know try to find the the most uh highest priority instance. So really what this does is it helps you improve efficiency because that's if you wanna talk about cost and availability, uh, the way to save costs when using a capacity reservation would be to use it fully and get the most bang for your buck out of that reservation. So let's talk a little bit more about how we can do that. First, uh, the one of the best ways that you can, uh, get better utilization out of your capacity reservations is through sharing. Um, so imagine, you know, you're high prior a high priority workload that only runs during the day and maybe has a small baseline at at night, but in the middle of the night there's not a lot of usage. So in order to kind of fill in those gaps, you can actually share the capacity reservation with other lower priority workloads that can be interrupted and that's a really great way that we see for our customers to increase the utilization, especially if they have spiky workloads that need that capacity assurance. Uh, so when you share a reservation you can actually share across accounts. You can share across organizations, so you can share, you know, one to many if you'd like, or you could even share outside of your organization with, uh, with another account if you, if you want. And the way that you do that is you use Resource Access Manager or RAM, uh, to set up the share. Now, by default, uh, if I own the reservation, so say I'm owning the reservation and I share with Carlos over here. If Carlos does not use the capacity, then I, as the reservation owner will be charged for it. So, if he does use the capacity, he's just charged for whatever he whatever usage is happening, and I don't get charged anything for, for a used reservation. However, say I'm, uh, a central, uh, a central team, and I want the ability to kind of control the reservation and modify the reservation, but I want Carlos to pay for it, you know, so one way that you can kind of, what, what we launched last year was, uh, the ability to assign billing ownership. So now I can assign billing ownership to Carlos because Carlos is the one that came and told me he wanted capacity. I, I, as a central team can manage it, but I can give it to Carlos and make Carlos pay for any of the unused because he was the one that asked for it. And then just as a side note, um, we just launched this year, we launched the ability to share, if you have a cluster placement group ODCR you're, you're able to share those as well. Um, so in the past reclaiming that capacity took a little bit of manual work. Uh, so if, if we're talking about like the high priority workload, if I'm, if I'm that high priority workload in the middle of the night and I realize, oh, I need that capacity back, you know, in the past what we'd, I'd have to do is get on the phone or page in the on call for the low priority workload. Uh, on call gets up, I've been on call. I'm sure many of you have been on call and. You have to get up in the middle of the night and log in and it, it can take a lot of time and manual effort to, to go in and, and, and take the action to kind of terminate those running instances so that the high priority workload can, can get the capacity back, uh, and so that's, that's, that's what it used to be. But, uh, so now when they, uh, they need the capacity back, uh, they can interrupt it with a 2 minute notification and, and then. You know, we take, we do all the automation, we interrupt the instances, we give it back to the the right reservation and then you're able to launch your instances. Uh, so I kind of like to, uh, Set it up like uh outline it like this, so the ODCR is almost like the parent reservation and the interruptible ODCR is like the child reservation. There's a connection between the two. I also like the parent-child relationship because I have 4 kids and my kids are always interrupting me, um, but you can then share that just that interruptible capacity reservation using the Ram share to your organization, uh, and then take it back when you need it. Now just one quick note on on this is there's producers and consumers with capacity reservations, so the producers obviously the high priority workload, but the, the consumer or the low priority work needs to make sure we need to make sure that they know what they're getting themselves into because they can have their instances interrupted. Um, so in order to launch into it, the, the low priority workload first needs to target that capacity reservation like we talked about, but then they also need to set the market type to interruptible capacity reservation in order to, to kinda tell, tell us that, that they're OK being, uh, being interrupted. But you can set this in a, uh. A, a launch template and kind of set it and forget it and and it all works and then just launch your instances and then wait or listen for uh that interruption notice and if once it comes you just save your uh save your progress and you shut down um. Shut down gracefully So finally, um, you know, we've talked a little bit about these two models, the on-demand and on-demand capacity reservations. There, there is a way you can actually tune the cost of this, this capacity, uh, tune your costs and lower your costs, and that's through something called a savings plan. Uh, and with the savings plan what you're really doing is you're committing to use some sort of capacity for a certain amount of time. Uh, it's basically an hourly commitment for at least 1 or 3 years, uh, and with that hourly commitment you get a discount on the usage that fits into that commitment. Uh, so you with, with, you know, our top 3 year instant savings plans you can get up to 72% off. Um, now there are two different flavors of savings plans. So there's compute savings plans and instant savings plans. I would say the compute savings plans are kind of the ultimate in flexibility. So if you need, uh, say you wanted to run a workload and have it be 8 hours in the Americas and then 8 hours in Europe and then 8 hours in Asia, you can use a compute savings plan which allows you to get a discount on any usage, any instance type in any region. Uh, to kind of get that, that discount and, and, and save, save on your costs that gets you up to 66% off of the on-demand price or if you're, you know, less flexible and you wanna have a certain, you wanna get a bigger discount but, you know, stay within a region or within an instance family, you could use the, uh, the instant savings plans. So with that, I'm going to turn it back over to Carlos to talk a little bit more about a couple of different uh usage models. So let's keep going with the usage models and different trade-offs. We cover basically deterministic um performance and on-demand capacity reservation and on-demand. There's one particular one which is capacity blocks for the male that target. GPU instances or accelerated computing instances including Training These an instance that probably first thing, they are in high demand and being in high demand, as you would expect, there will be some, in some cases, a higher level of scarcity. Second part is that they're harder to be flexible on, especially for training. There's lots of improvements that are going on on how you manage that kind of flexibility, as in if you check for training or libraries or frameworks, you'll find things like Qutile or JAX or plenty of things that are compiled time will try to basically make the most of some DSL, some domain specific language or language, and try to make sure that it compiles toward a specific target platform. But the reality is that there's a huge level of performance that you can get out of GPS. And that level of performance comes from maximizing and getting the low level things right. So in the case of GPUs like Nvidia, quite a lot of this training is done using Cura and maximizing and squeezing down the bottlenecks to get the right price performance, which makes the whole thing much more difficult to get that flexibility across instances. These instances are also expensive, which means that getting into a savings plans might not be the right way to go without doing any experimentation. Capacity blocks is the contract that help get that kind of block for reserve capacity from 1 to 26 weeks, so that's half a year. We extended that quite recently. And for that duration of time. When we apply that capacity block, when we request a capacity block, if we get that capacity block, an on-demand capacity reservation, a reservation will be put on top of it. So if at some point, for example, we are not using that capacity blocks with our own account, we can use those techniques that you just learned from Brian to hand over to other parts of your organization and make the most of those. When do we use capacity blocks, as you would expect, training, if you know that there's going to be a big exercise with training coming, that's the right place to think about capacity blocks. Make sure that you have that availability and that capacity. If you have also a campaign, a seasonal campaign, and that thing is coming, and no product launches and stuff like that, same thing, and obviously for some level of experimentation, you want to have that capacity over that experimentation without doing that saving plans commitment over time. So those are the places where you think about capacity blocks. You can get up to 64 instances, from 1 to 64 instances when. You multiply that in GPU that's 512 P6 200 GPUs, for example, which gives for the moment that you put that together with a placement group, cluster placement group to get all the bandwidth that you need, gives for a really damn big cluster for training. Makes sense? And obviously most of these go um on Linux systems, any of the Linux systems. The same concept of capacity blocks for ML applies also to Sagemaker. So Sage maker training plant is the equivalent of, it has some some differences in the in in the in the way that you can even apply uh 30 minutes in advance and if you that capacity is there, it will come directly to to you. But it's used for pretty much the same aspects. It will plug and integrate within Sage Maker, with Hyperport, you can ask for Ultra cluster service as well. So you can get also those P60s, 2 100s that we were talking about. And that experience um will be both exposed through the dashboard and through um uh an API. So you can get access to any of those. You can, as you would expect. Use it for predictable access to accelerated compute as we were saying, including training 1 and training 2, and primarily to make sure that you have all that capacity and in the place where you have the data as I was saying, most of these workloads are less flexible than general workloads because of first, the reasons of compiling to the target architecture, but also the number, how much data you need to process those training models. Right, Let's move to the other side of the spectrum. You probably by now are acquainted with what the spot instances are. I already mentioned in the first part of the session. Um, these are spare capacity that we got in our data centers. They are sold with a step discount of up to 90%, typically 60, 70% is what you'll find. You can uh come in this path. People forget that we have also, if you if you can run opportunistic workloads on GPUs, you can also access GPUs on spot instances. Now the question is where to find them, obviously. Um. If you haven't heard about the spot placement score, we'll talk about that in a second. But let's cover the basic four spot instances which are mostly used with C, M's, and R type of instances. So just kind of regular type of instances. The key for those usage is two things managing that 2 minute termination notice. So have some level of fault tolerance. And second, the key thing will be being flexible. Being flexible across many different dimensions. You can use dimensions of the instant sizes, uh, generations, types, or you can be flexible also on time shiftable workloads. So you can basically, typically there's more. Capacity, spare capacity overnight. So you can basically move some of the batch workloads to overnight to find the capacity and the skills that you need at those lower prices. If you are flexible towards region. You can also move that regional availability or find that regional availability using a spot placement score. So Let's cover the aspect of fault tolerance first. We were talking about those interruptions. We have still that 2 minute interruption notice, which we are extending to up to 30 minutes for GPU. We also are providing something that is called a balancing recommendation, which is effectively a notification that we give in the worst cases only 2 minutes in advance, like the spot termination. But typically we tend to give it 1015 minutes in advance, telling you that that instance is going to be put at risk of termination. And in fact, we embed the replacement of that instance with some other instances that look similar um in upscaling groups. Oh, in Carpenter as well. You can even use 40 injection service to create a. Termination, deliberate termination from your site as part of your testing model. So you can verify that your application actually is fault tolerant towards these terminations. Verify how it works, and even build it as part of your continuous integration. So I was saying the key thing is flexibility. And aside from flexibility, is that um finding out which dimension of flexibility might be key for you. I don't know how many of you have heard about the spot placement score. We released it about 2 years, 2.5 years ago. Spot placement score is an API and it's in the dashboard as well. You can select a diversified configuration, similar with the attribute-based instant selection that we were just showing before. You can also set up a target capacity. For example, I would like, out of that attribute per system selection example that I use, 10,000 instances, or 50,000 instances. It will give you a score from 0 to 10. A B 9, you're going to find that capacity and you're going to get low level of terminations, or a 3, the opposite. You're probably not going to find that capacity and the frequency of termination is going to be quite bad. And it's going to give you effectively a number for the different regions that you are trying to get access to. All the different availability zones. So if you have something like a Spark workload that is very chatty and you are trying to avoid that inter availability zone charges, it can tell you which is the right availability zone for your analytic workloads. OK. You can always do that manual selection that we were talking about. So we've been covering all the different usage models and now even how to put them at the scale. The reality is that most of our customers are going to have a mix of those models. We just released something that I'm super proud of Brian's team on everything that they do on how we can bring and put all of those things together. Brian, would you like to? Sure, thank you. So like Carlos mentioned, we see all these different usage models being used throughout, uh, throughout organizations, and, and there are different reasons that we've gone over of why you would want to use those, uh, those different usage models. And at a small scale, it's pretty easy to manage the capacity if you're just looking at a single region, uh, or a single account, it, it's, it's not too bad. You can, you can use the, uh, the console to, to see exactly what's going on. However, if you're in a large organization trying to manage capacity across the entire organization, you're having to deal with thousands of instance types, potentially thousands of, of accounts that you have to manage, hundreds of availability zones, dozens of um of regions, and a mixture of all of these different usage models. Now AWS has a lot of different tools that kind of help, could help with this, uh, like Costausa's report, describe APIs, or even CloudWatch. However, what we found when talking to our customers is that they had to kind of keep going back and forth between all of these different tools to kind of figure out what's going on. It could take them even hours to kind of get a good picture of what their capacity looks like in order to be able to manage it. So back in October, uh, we actually launched and released something called EC2 Capacity Manager, which kind of helps bring all of those tools together into a single uniform, uh, view. So the idea is now you can see all of your on-demand usage, your ODCR usage, capacity block usage, and even spot data across all of your regions and across all of your AWS accounts in one place and it's absolutely free. So we're really excited about this because we think it's gonna help you to get a better picture, to be more efficient, and to, to be able to, to manage your capacity more effectively. Uh, so let's jump into a little bit of a demo here. Uh, so this is the kind of the capacity manager dashboard, and the idea with capacity manager is to always start at a really high level, kind of the 10,000 or 50,000 ft view, and then give you the tools to kind of drill into your data. So up top we have the dashboard and then all of those other tabs are, are tabs for you to, to dive into your usage, dive into your reservations or your spot. Uh, you can also set your date filters so you can set up the date filters up to a few hours ago, uh, all the way to 90 days ago, and you can have granularity of a daily granularity if you kind of want to see trends or if you really want to get into the weeds, you can see hourly granularity. Now within the dashboard we have an overview section and you'll be able to see really quickly how well you're doing with your capacity reservations. When I, and when I say capacity reservations, I mean both ODCRs and capacity blocks, and you can see how efficiently you're, you're actually using the, uh, those reservations, meaning, and when I say efficiently using your utilization, I mean is an instance on or off. I'm not talking about like this or like CP CPU utilization. Uh, you'll also be able to see kind of the estimated cost of all of your underutilized capacity reservations so that you can go in and see where are, where are you, uh, spending money that is not, you're not getting the best bang for your buck. Uh, you'll also be able to see usage and, and what percentage of your usage is actually covered by a capacity reservation and then even spot, um, normally I'm gonna just give a, uh, an aside here. This is one of our test accounts. So normally you're gonna see a lot better than 44%, uh, utilization, you're gonna see a lot better than uh 1 hour of uh of interruption time. That's just us testing things out. But you can also see, OK, where am I getting interrupted and, and where am I getting good, uh, good usage of spots so that I can, if I'm not getting the, uh, the runtime that I want, I can go add more flexibility. Uh, we'll also give you usage metrics, and you can break out those usage metrics by VCPU and VCPU hours, by instances, instance hours, and even estimated cost. Uh, you'll be able to see OK, what percentage of your on-demand usage is covered by a capacity reservation, what percentage is not covered by a capacity reservation, and then what's also running on the spot, and then you can see kind of the usage trends across time to see how things are, are changing. Uh You'll also be able to see kind of deep dive into reservation metrics and see what's used and not used and then we'll have this unused capacity, uh, section which basically gives you your top, uh. Instance type and AZ kind of cross sections where you're, where you're where you have the most unused, uh, unused capacity reservations. And then finally, you'll be able to actually track out spot. So let's look at what it looks like to deep dive into one of these things. So say I wanna, you know, I, I, I have some reservations that I'm not using as efficiently as possible as I'm looking across all of my, uh, my organization. I can click into reservations and kind of see some of the same things. So we see the date filter, we see new things like dimension filter and aggregations. The aggregations is where all of your data is going to lie. And right now, like I said, it's, we start at the top and then let you dive deeper. And we'll, we'll go into that in one second here. Uh, but you also will see different statistics about your capacity reservations and then, uh, trends. So say now I have, you know, 715 reservations is a lot for me to, to kind of think about or grok in my head. Um, so if I wanna kind of break out those reservations, I could choose something like instance family or availability zone. And then when I do that, it breaks everything out into, now I have my instance family and availability zone there so I can see exactly, OK, I have 4 reservations of M5 in US East 1 AZ1. And now it's a lot easier for me to kind of go and dive into each one of those reservations. Um, you can also filter the reservations, so if I only want to look at M5, I could, I could click that little plus button, or I can, I can dive even deeper into the reservations and, and kind of apply those filters and see, OK, what are statistics about just that line item, see different trends about what's being used or not used. And then even dive into specific reservation IDs and I would, if I wanted to, I could even, I'm not going to subject you to that, but you could even click into the individual reservation IDs and actually make changes to those reservations. Uh, there's a few other features that I just wanted to point out because I know we're running short on time. One is usage. So the usage tab works very similar to the, the reservations tab, uh, where you will be able to see, OK, your total usage trends, what's covered by a reservation, what's not covered by a reservation, and even your spot usage, and you, same thing, you could break it out by instance family, AZ, account ID, whatever you'd like. Uh, then you would also have the spot, and this is where you can kind of dive into some of your runtime metrics, so you can find out, OK, where do I need to spend some more time making my spot fleets more flexible so that I can get better runtime. Uh, and then if you wanted to, uh, so this is available through the dashboard, through APIs, or if you want to set up a data export, which is basically just an hourly dump of all of your data, you can set it up to, to go into your an S3 bucket, which is the only thing that you would ever have to pay for with, with capacity Manager is the, the storage cost for S3, but you can have uh data export set up to. Uh, every hour just give you all of your data and then you can use something like Athena, Abis Athena, or ABS glue to, uh, to do transformations on that if you'd like. So kind of wrapping it all together, uh, capacity manager is a fantastic way for you to take all of the different things that we've talked about today and wrap them up into a single bow so that you can make sure that your, your organization is using everything as efficiently and effectively as, as you possibly can. You can either set it up as an account level, uh, which is just mean all your data in a single account, or if you have access to the payer account or a delegated admin of the payer account, uh, you can set it up at the org level. And so that would be kind of the call to action today. It's completely free to to go and set up capacity manager within your accounts within your organization, because it's going to give you a much better view of what's going on without having to kind of go to all the. You know, 10 different tools to figure it out. So just to kind of wrap everything up here, some of the key takeaways that we, uh, we wanna make sure that we highlight on is one is, you know, EC2 has a bunch of different usage models, uh, not one, there's no silver bullet or or singular usage model that's gonna work best for everybody. Uh, oftentimes we see most organizations use all of the different usage models because they have different workloads that are gonna, they need to optimize so. Uh, it's gonna be a balance between the two and, or between all of them, and, and finding that balance is incredibly important. Second thing is if you want to kind of continue to tune things like cost and availability, you can add savings plans which if you're willing to commit for that 1 year or 3 year term, you can get large discounts on your on-demand usage or if you add flexibility to workload, you can improve your capacity availability. And 3, if you want to manage your, your EC2 instances at scale or your EC2 capacity at scale, use capacity Manager. Uh, it's really easy, and, uh, just wanna say thank you so much. Sorry for all the technical difficulties, but really appreciate your time. If you want to connect with Carlos or myself, uh, here's, here's our LinkedIn profiles. Thank you very much. Thanks everyone.