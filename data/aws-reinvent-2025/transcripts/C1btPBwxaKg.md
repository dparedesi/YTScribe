---
video_id: C1btPBwxaKg
video_url: https://www.youtube.com/watch?v=C1btPBwxaKg
is_generated: False
is_translatable: True
---

Excellent, um, folks, I just wanna start off by saying, well, Justin and I were quite nervous, as you can imagine, but seeing you all here bright and early, 15 minutes before the session started, means a lot. So I just wanted to say thank you so much for being here so early, um, and hopefully. We can do justice for the time you've invested in us, all right, um, just on that note, I wanna do a quick raise of hands if that's OK. Um, how many of you experienced a log 4J event that happened a couple of years ago? Awesome. Half the room. Excellent. Um, how many of you were on the front line on MI bridges having to answer CISOs and CIOs or CTOs, you know, around exposure, blast radius? Any of you on the front line? So 123, I can see a few hands. How awesome was that experience? I'm being sarcastic here, but you, you get the drill, right? Um, it was chaotic, it was absolutely nuts. Pardon my French here, but it was, it was insane, right? And that's exactly what we wanna talk about today is how can agentic AI help you and your organization be better prepared for situations like the Lock Forger event. Everyone, I'm Praveen Bhatt. I'm a principal solutions architect coming all the way from Down Under, or Australia as we like to call it. And with me today I have. Hello everyone. Uh, I'm Justin Thomas. I'm a senior cloud Support Engineer, and I spend my days working and helping customers with cloud operations. Um, so the Lockford event, Praveen just mentioned, I was there at the front line, and I was helping customers, many customers simultaneously. And what I witnessed was chaos. It was organized chaos at scale. And I would be on a call with a customer and helping them trying to find the exposure. And suddenly I would see 5 more customers coming in with the same problems, same questions. So I want to take a look at an example of the same experience. So this is a fictitious company, Manual Everything Corporation. And imagine it's Friday afternoon. The team is ready to wrap up for the week. And suddenly Enormous list A critical CV gets announced and now thousands of the instances need patching. So an emergency bridge call is scheduled. Security director joins the call. And us All right everyone, are we exposed or not? Do we have exposure? And Warsaw blast radius. So first few minutes, nobody answered. It's silence. The platform team hasn't even joined the call, right? And after some time, after we page the platform team, the platform engineer joins, says like, I'm still trying to figure that out. I need to log into the instances. I need to log into the accounts. See how much number of instances are uh non-compliant right now. And also, I need to test this patch that just released before we start deploying to the production. The compliance manager, the IT compliance manager, jumps in. Ask, are we doing an emergency patching right now, or can we do that? Can we wait till our patch maintenance window? So the CTO of the company. It's listening to all this conversation. It's frustrated because this looks like a scavenger hunt for the right data to make the decision. And Which acknowledges that we need better automation, we need better intelligence. This conversation is something that I constantly see. 1st 30 minutes to 1 hour, we are just waiting. We are waiting for the right people to join the call at the emergency uh bridge call. And when the right people joins the call, it's the same questions. These questions, which instances are affected? When are, when are we doing the patch? Do we need. A downtime Right? So, imagine like this single assessment, this conversation is for a single CV and this is going to be for hours. Now imagine doing that for multiple series. Uh, in a month. The team simply cannot scale to do it. And that's way, what if we add an intelligent agents. Which work alongside with your engineers. To help identify and assess these vulnerabilities, check the compliance requirements. And also do the remediation. So your team stays in control, but it is now augmented by these agents. To do all the heavy lifting. And that's what we want to show you today, the intelligence agents that we built. And we can move to the demo. The demo, uh, the demo setup that we have is pretty similar to an enterprise setup, so we have. Three different environments. Devastating and broad beach. Uh, environment has different schedule, patch schedule, and also have different patch compliance requirements. This existing services already configured. Amazon Inspector, AWS Systems Manager, AWS Confi, these are the services which are already detecting your vulnerabilities already remediating. What's missing is an intelligent orchestrator that leads the way. When a critical CV gets released. That's where we add our intelligent patch automation. This is a multi-agent pattern. So there are 44 agents that we create. One is supervisor, and which have control of 3 specialist agents. So these agents are built using SANs. Now, SANs is an open-source SDK. That makes it easier to build and run agents with few lines of code, and what it brings is a model-driven approach, which means we allow the LLMs to think on its own. To think autonomously and make a decision to reach the results. And this is deployed in our account, in our demo account using Amazon Bedrock Agent Core. So, Agent Core is an agentic AI platform, which makes it easier to deploy and operate AI agents uh at scale and safely. Cool. So, let's, I'll switch to the terminal now and we can see the patch intelligent automation that we built uh in action. Folks, can you see the, the font, OK, on the back especially? Yeah, awesome, thank you. Thank you. OK, so we call this Patchy. That's the name that we've given for the system, the agentic system that we built. And you can see we have uh access to the specialist agents. So first thing during a OK. So, for what you're actually seeing is a simple chat client that we built to interact with the agent. Um, Naturally, you will build a proper web app in production, uh, which can even integrate with your Internal change systems like Jira or ServiceNow. But here, we just want to like show it like simply how the agent, like how we can interact with the agent. So, chat client seems pretty simple. OK, so first thing during a critical event, what I want to know is understand the environment, understand what's running in my account. Right? So, even the platform engineer is taking some time to log into the console, log in, like, what my production instances, production environment looks like. So I'll ask the agent just to summarize what's there in my environment. Um OK, so I'm gonna ask it to also give me the information about that compliance requirements for those production instances. So what we are doing is we are interacting with that supervisor agent that we saw, and it's making the necessary API calls. It's reading the EC2 tags, it's creating the AWS configt service, and within seconds, we have the big pic the complete picture of your production environment, right? So we get the in society, we get a common. Configuration information about the production, but most importantly, we see the compliance framework for each in society, and that's useful information for us to make the decision. So at manual Everything Corporation, if you see, it's, it's this, this information are spread across spreadsheets, Excel sheets, CMD be tools, but here we got it with one prompt. Now, do you remember the, the first question security director asked? It was, are we exposed? What's our exposure? So let's ask the agent to find if there are vulnerabilities in our production environment. So I can just say pro honey. Cool. So this time, uh, the specialist agents, the vulnerability Alice agent, the supervisor consults that agent, and that's being built to use to query the AWS inspector to get these findings. And imagine if you're a security engineer, you would do that, uh, by logging into the AWS Amazon inspector, download the findings. Check the CV details. Check if there are patch available for that CVs. Then download everything, get a priority list, and we're talking like hours of work, right here. Within seconds you will see exactly 3 37.6 seconds you will see that we got all those results. So we, the agent. Summarizes that there are 50 high severity vulnerabilities that's there in the production environment, and also summarizes which uh the top critical series based on the CVS's score. So and which instances are affected by it and which compliance. Is at risk if we don't pass the CVIDs. Now here comes the interesting question which was asked by the IT compliance manager. Is that when is the right time to patch this? Right, so, there is an SLA requirement of the company, there's a patchment manager window. How do we come to a decision when to patch this? So, I will ask the agent to come and see uh and tell us, like, when is the right time to pass this. Also, I'm just, if you note this, I'll just write, when should we patch this. I'm not specifying in society, I'm not specifying vulnerabilities. It's, it's just, it's, it's because like with Agent Core, we are using agent core memory, which remembers all the conversation history that we have. So it's a contextwa agents, and we'll go through that once we see the architecture. Uh, but this time, like you see, uh, it's consulting the patch manager agent, that's a specialist agent, and it's checking that, uh, what's our patch manager window. What's our SLA and it gives you a decision. Like, it says like patching decision needed 24 days until next window. So your maintenance window is 24 days uh from today, right? And vulnerability status is. OK, it's actually says SLA data unavailable for some reason. Let's actually check again. So, the patch manager agent, um, will see the code, and the, in the strands, we give this, uh, the patch manager agent an access to a tool which can calculate the SLA requirement. And when we ask this, when should we patch, it actually go and check what's our SLA policies for those specific severities. And then see a delta with the maintenance window, right? OK, so here it goes. So it says emergency patching required. There's an SLA violation. So there isn't necessarily deadline because your instances are PCIDSS requirement, so you need to patch within 48 hours. But your maintenance window is 25, uh, 24 days away. So there's like, you are, you are at risk, right? So do the immediate patching within 48 hours. So that's the recommendation, right? So the agents helped us avoid an SLA violation and gave a clear recommendation that you need to patch this now. So, up until now, we identify which instances are affected, which vulnerabilities are there, and we came into a decision that we need to do and apply, we need to like do the patching now. So let's ask the agent to do the patching. But I want to like start with Dev first, not we, we checked everything for the prod right now, but we need to like start with the dev and then staging and the prod. So. That's gonna ask, OK, apply. Emergency patching. Or the Devinsons. So this time also, the patch manager agent will come into the picture, which is built to do all the systems manager API calls. So, when, in the patch manager, you deploy, you run a, a run command, and you use the AWS Run patch baseline document. OK, so it's actually asked me to clarify uh which environment should I patch. OK, so let's batch the dev environment. Yeah. So, it, at the back end is, is going to generate a run command ID. It's going to target those dev instances, the 5 dev instances, and uh it will use the AWSR patch baseline document for that. Um, just if I may add, Justin very quickly, I think quite interesting to notice is the smartness of the large language model to be able to understand the fact that you've been discussing about production the whole time and all of a sudden you're saying you wanna swap, you wanna switch to the dev environment, actually trying to get a confirmation, um, which in a lot of instances doesn't really happen, um, and again, it's. It's something that you'll notice the more memory, um, the more context that the large language model has, the conte uh, the more aware it is about your organization, your processes, the better it becomes with such things. Yeah, thank you. So we can see like a command is, um, command ID is generated, the status right now is pending, uh, so 5 instances are affected, sorry, targeted. So we do have some next steps that provides, uh, by the agent that we can check the status. Uh, also we can type proceed to continue the the staging environment. So, basically, agents understand that this is a phased rollout. So we started the dev. I didn't specify that it's going to be like staging abroad, so it understands that if I type proceed, it will start patching the staging environment next, or we can wait and check and verify if everything's fine, and then we can do that. So. We avoided an SLA violation, did the emergency patching, right? But what about the times that we were not able to catch that SLA miss? And that's what the CTO of Manual Everything Corporation is thinking right now. How many times did we, how many times we were non-compliant just because the team were busy doing the scavenger hunt for the data? And this is something even the auditor's gonna ask, what was your compliance history, right? So, Manly Everything Corporation does not have a tool to get this. So let's ask the agent to get how many times did we miss SLA. So this time is the last specialist agent, the compliance analyst agent, uh. Is, is consulted by the supervisor agent, and what it does is, it fetches the reports from estuary bucket, and then do the math, it checks when was the vulnerability released, when we did the patching, and, Um And also check the SLA policies, yeah. So it's actually checks the SLA policy and see like how many times did we did the patching, which was, should be like done within the SLA timeline. So, it says there are a total of 6 breaches in the last 30 days and also gives the breach details, like these were the CVIEDs that were like breached, and these were the team that were affected by it. OK, so Wow Yeah, so, next, we want to like explore the architecture. We want to explore how we build the score and how we come into like the decision principle that we took when we were building this. And for this, I will hand over to Praveen. Awesome. Thanks, Justin. Um, a couple of things, um, that will kind of highlight as we get into the code is the patch manager or the patch manager agent itself. Has the ability to verify or do health checks as part of its patching process as well. So, the reason why we can do it in just in the interest of time, but under the hood, it's basically checking your application, your infrastructure, and we'll go into detail of how it actually does it, and once it completes, um, it's patching, it's uploading all that information to an S3 bucket as, as Justin kind of pointed out. All right, now that we've seen the solution in action, let's have a look at the underlying architecture and code that drives the solution. As you've already noticed, you've got multiple agents in the mix. This is based on a pattern called as a supervisor pattern, um, wherein you've got a supervisor agent that manages the end to end orchestration of the whole workflow while it consults or coordinates with specialized agents, each of which have a specialized role, set of instructions, a set of tools that it can use to be able to complete a given task. Now, you might all know this, agents by themselves don't have the ability to understand the user intent, cannot reason. And for that intent, we have clots on it 4.5 as a large language model running on Amazon Bedrock that gives it the brainpower. Now even with all the brainpower, the agents need to be able to interact with AWS services. For that, we have specialized tools that are exposed to the specialized agents. Justin's mentioned this, but very quickly in terms of the AWA services we have in the mix are Amazon Inspector for vulnerability findings, AWS config to give the agents an understanding of the resource dependencies, so when it actually makes a patching decision, it understands the business criticality that might have an impact on the whole patching operation. Right, if it's, it's a, it's a, let's say, um, a web app, a front end facing application that has extremely high business impact, you don't wanna do it or you don't probably wanna patch it, right? So all that decision is factored in by the, the patch manager agent. This in your case could be something like a CMD like like CMDB like a service now or a Jira that you're using, but anything that is able to give the, the resource dependency view, the large language model will absorb that information and make it, um, available for the decision making process. Systems Manager to understand the patch compliance state, to apply the patches, um, and then you've got S3 to store and retrieve the compliance reports, and finally EC2 to apply the patches themselves. We're using strands SDK Justin's mentioned this, but the one thing I wanna highlight why strands is for one key reason. Strands makes it very easy for users to be able to build agents that can interact in a continuous manner with the large language model and the tools in what's referred to as an agentic loop. What that means is, think of a scenario where a user puts in a request. Right, patch my senses in dev environment. The agents don't understand it, so they've got to understand the intent. This request gets routed to the large language model. The model says, alright, I understand the intent, but I need access to tools that can interact with AWS services. So the request gets routed to the tools, the tools go, talk to the relevant services, come back with a response. That response is fed back to the large language model. It reasons that and goes on and on and on until the user's original request is completed. Now all of this happens within what's called an energetic loop. You don't need to code it, it's just made available to you, and I'll show you when we get to the code itself. And very quickly in terms of scale and security, it was super easy for us to just deploy it on Agent Care. Agent Core comes with um 7 primitives, if I'm not wrong, 6 to 7, but we're only using 3 of those. Run time to actually host and run our agents. Memory for making the agents context aware, and finally observability to be able to understand the state of our agents uh by using tracing, metrics, logs, all of which are very, very useful when you're trying to debug a situation, when you're trying to iterate or evaluate your prompts, um, you're making changes to your large language model parameters, etc. all of that observability makes it super easy for you to get that information. And that's it. Um, now. When Justin and I came up with this idea, this is like 4 months ago, we didn't start with this whole idea in like this, this sort of architecture at all. We thought we'll start off with something super basic, super simple, understand the capabilities of large language model, understand what enterprise users would prefer, uh, in this capability. So we thought we'd start off pretty small. And then we continue to iterate. Get feedback from customers, and then you trade it again and then again and again and again. So we thought in the, the next 30 odd minutes that we have left, we'll kind of take you all on that accelerated experience of how we started off with something very basic. And then how we traded and how we got to the stage where Apache is today. So hopefully you can leverage, you know, the tips and practices that we followed, uh, the architecture patterns, um, to get you and your teams to a stage where you're more comfortable to be able to handle the Lofg event. Sounds OK? Cool. I'm gonna swap quickly to my demo screen. All right. Excellent. OK. Um So what we started off with first was, was a super simple EC2 agent that could just list all your EC2 instances, get the compliance state. Right, so let's just start off with that. OK. If it's OK, I'm just gonna get rid of the screen like this. A little bit on the left, yeah, perfect. Can you see OK, alright. So I'm gonna start off with um. Some Importing of some uh main class. So the agent class is the main class because it's the core interface into your large language model, um for your conversation management and all of that. So this is the most important um class that you need and then for your agent to be able to, wow, I'm a terrible typist, um, for your agents to be able to interact with your AWS services you need a tool. And Strands has uh this built-in tool called as use AWS uh that's available with the Strands library. And then I'm gonna just quickly define the agent. Start off with a name. Yes, agents have their own identity now. Uh, and then give it a system prompt. System prompt is where you specify the role, the persona, the traits, the instructions that you want your agent to have. In our case, I'm gonna keep it very simple, you're an EC2 assistant. That's it. And finally, don't forget the tools. You know, so use AWS, which is a tool that allows the agent to interact with AWS services. That's it. 6 to 7 lines of code within strands that allows your agent to be able to interact with the EC2 services in your account, get the compli uh get the list of EC2 instances, and then interact with systems manager to get the compliance date. That's it. So I'm gonna quickly. Start off, I'm gonna say, list all my EC2 instances. In Prague. And their compliance state. In US East one. That's it. I'm gonna run this. Uh, all right, Python. So hopefully I didn't make a typo. Yeah, it's just, so what you're seeing, um, at the very top is I'll help you list all your EC2 senses. That's a large language model actually understanding the user intent, and it's figured out that it needs to do a few things, which is get the EC2 senses, then get the compliance date. And then you see the tool invocation, the response that comes back, the intent is being or reasoned uh by the large language model and goes on and on, right? So you'll see multiple tool invocations happening. And then it comes up with the the final response, which is like, here are all your production instances are they noncompliant compliant. And a bunch of other information that I probably didn't need. But anyways, this is, this is just a basic agent, right? What I do want to highlight is the agentic loop again. The large language invoked the tool, got the response, and as you can see here. I don't know if the highlighter is working, but the invalid parameter value exception. And we've reached a stage where large language models can autocorrect themselves, figured out that the syntax that it followed was wrong, goes back to its knowledge base, figures out what's the right in uh in syntax, and submits that again. And it probably tries AWS convict, there you go. Because we said compliance state, it goes to conflict for some reason, to get the compliance information and then figures out conflict doesn't have it, and then autocorrects itself and tries to get that information from Systems Manager, which is here somewhere. Yeah, I'll use the Systems Manager compliance API to get it. So that was it. So we realized like, we can do a lot, but the performance was not optimum. The fact that we had to go 5 tool invocation sometimes, sometimes 7, it wasn't ideal. So we thought, how about we make the agent more richer, more powerful, give it more tools, uh, and tweak our uh system prompt. So at this point in time, we went with. Something like this, which had, just gonna remove this. The same agent, but with slight slightly more enriched prompt and better uh tools that it has access to. And I'll explain to you a bit more of what's happening under the hood. So you've got bedrock model defined. When we didn't define one with strands, by default it uses clots on it for today in US East One, and also sets up all the parameters like temperature, top P, which controls creativity, and, and a few other things like max tokens as well. It has default parameters which might not be suitable for our use case. So we had to control that as well. And in our case we just tried like can you use Nova which gives us better cost of performance um to see how it works. So this was a 2nd iteration, literally this was it, um, and then control the temperature element to control the creativity of the large language model to make it as deterministic as possible. Now, for all the Python aficionados, you can see the tools here. I've got two tools, one to get the EC twin sensors, and the other to get the patch compliant state. It's just a Python function leveraging Border 3, that's all it is. The only difference is the fact that we use a tool decorator, which exposes a Python function to be used as a tool by the, the large language model. And the way it does it is by the dock strings. The dock strings, as long as you're clear and structured in terms of what the, the, the, the tool does, it knows when and how to use it. So in this case, you just say get EC2 instances, optionally filtered by environment, so on. That's all it is. And same thing with the, the, the get patch compliance, we're doing the same thing here as well. Right? So that's the, the tool bit. Now coming to the prompt, previously we saw just the you're an EC2 instances. Wanted to change that because you could see all that additional information coming through which we didn't need, so you could control that using what's like a behavior, which are basically a set of instructions that I'm giving to the agent to ensure that it follows a few things that I wanted to, which is like, give me the exact summary sort of to begin with, and then the details, mark down tables for multiple instances, use the default region if it's not mentioned. But here comes the interesting bit. I'm building an EC2 assistant, I don't want anything else. So you can control the scope and the role that it needs to perform using a prompt. Right, in this case I've just said, if user asked anything about non-EC2 related queries, just respond with saying I don't handle it. Simple as that. And then being explicit with your tools. Again, you don't need to, it's a personal preference because the dock strings itself help the large language model understand when it should use the tool. But I'm just pedantic, I prefer to be very explicit, make sure the agent understands what it needs to do. And then the output format, we're using a chatbot on a terminal, so had to follow a certain set of instructions from an output point of view, and then the tools. As you can see we're not using U AWS anymore, we just have two tools. I've got a very basic why loop just to start thinking about a chatbot experience, and that's all it is, right. So, let's invoke this one. I'm gonna say Cla, yeah. Python, EC 2. But with tools. Comes with a request, so I am going to copy paste my previous prompt itself, so, so you can see exactly the behavior, but this time with a more smarter agent. So you'll see the streaming information, what's coming through, and this is one of the reasons why we built a chatbot on Terminal just to show you the streaming behavior. We probably could have done this with a web app, but it was just a bit more easier with the chatbot. Um, so you can see, it figures out the intent. And it makes only 2 tool calls. That's it. If you remember, actually, before I go, just, just observe this behavior as well, this is quite important. The fact that it just sort of types one line at a time. And I'll just explain what's happening under the hood and our experience, why this is not ideal. Um, it just did two tool calls. If you remember the previous agent did 5, sometimes does 7, sometimes 2, it's unpredictable. With tools you start getting more predictable, deterministic behaviors. Now, to the point again of that single line being printed, that's because the tool that we had for patch compliance state only allowed for one instance at a time. So the large language model is basically making 5, there's 5 instances, yeah, 5 API calls to Systems Manager to get that information. Not ideal. Batching is the way to go, but we realized that the hard way. Not the hard way, it's just, I think we were being lazy. Um Actually, one last thing around the scope, uh, list all my lambda instances. Now, if you remember. This is just an easy to. Agent should shouldn't give me any information. So from a security point of view, I can build in guardrails. It's just one way to do it. There are other ways as well, probably some things that are more harder, but this is one way to actually achieve it, um, especially when it comes to injecting, um. Um, um, um, for lack of a better term, I'll say crap into your prompt as well. So, uh, there, this is one way to actually achieve it, uh, is having controls in place within your, uh, prompt itself, um, within your system prompt, sorry. So it just says I only handle EC to related queries, uh, ignore the duplication, but it's, it's just saying I only handle easy to related queries, and the same would apply if you, if you keep trying. Um, So that's it. So we reached a stage where we realized, OK, we're able to do a lot more than what we use AWS, so we started building more tools. But the agents started getting heavier and heavier. So we got to a stage where it started becoming monolith versus microservice sort of debate again. Should we go with a single agent? Or should we start splitting into multiple agents? And the reason why it was becoming more obvious for us to go down the latter path was because of the hallucination, the confusion. So simple sort of philosophy we started following was, if an agent is not doing one specialized task, Then it should be broken up. Right, so again, it's a philosophical debate, again, monolith versus microservices, not gonna get into it. I'm pretty sure we're gonna spend all day, maybe all week on it, but we'll, we're not gonna get anywhere. But the reality was for us, we started getting better results by breaking it down and evolving it and adding more tools. We got to a stage where Apaches today, and I'll, I'll just walk you through the Apache code base now. Um, alright, I'm just gonna close these things. I'll close this as well. So just very quickly in terms of the project structure, all of the, the agent related code is within the agent um folder itself. Ah, we have a helper function within the helper folders and I'll walk you through that as well, but just very quickly, the Bedrock agent core YAML file that you're seeing in the docker file, these are pertaining to the agent code deployment, so they are the configuration file and the docker file that's used by the runtime to be able to deploy agents. Um, everything else, which is the four agents, if you've got the patch manager, supervisor, vulnerability agent, and compliance analyst. Right, super simple. In the helper functions, very simple ones, uh, we just have memory, uh, global, globals, and, and a few other things, but the key thing I wanna highlight is the, the tools file. So you can either have the tools embedded within your agents. But the fact that I come from a software engineering background, it was easier for me to just externalize and have it all in a separate file, just made it easy for maintainability and readability and and a few things, and you can always import it, right, as simple as that. So, I've already walked you through a few tools, I'm not gonna bore you with that again. I just want to show you one tool and just what does, Could look like from our perspective, I guess, um, so we've gotta get vulnerability findings here. I'm gonna just remove this so you can see the whole screen. Again, tool decorator But there's probably 23 key things that I wanna highlight here is one, the dock strings. Again, if you're have been running Python code, you're familiar with this. But being as clear and structured as possible makes it that much better for large language models to be able to understand when and how to use a tool. If you, it's garbage in garbage out, it's as simple as that. So in our case, we define what the purpose is. The arguments, the CVID severity environment limit for example, being very clear in terms of the default values, ah giving examples where possible, and what it returns once the tool's invoked. Try and return dictionary objects where possible. The more context that the large language model has, compared to say just a string output, the better it will be in terms of, Providing you with more richer information as an output. OK, now, one thing I do want to highlight, this is probably my 2nd thing or 3rd thing I can't remember, is the fact that you have optional and required or mandatory arguments. You can basically control the behavior of agents using this one thing. To give you an example, let's just say in get vulnerability findings, if my severity was mandatory or required. When the large language model is invoked, so let's say Justin in his prompt says, give me all the vulnerabilities, um. And that's it. He just says, give me all vulnerabilities. He's not mentioned the severity at all. At that point in time, if this was required or mandatory, the large language model, rather than assuming that, hey, the user is asking me for all of it, so he probably assumes. They assume that it is critical, because critical is high, so I'll just go with the critical findings. There's an assumption being baked into the large language model. However, if you enforce it, then the large language model will question or check with the user again to say, do you mean critical, high, medium, low. So you can control the behavior using just the arguments itself. Other than that, it's not rocket science, right, it's a simple Python function, we're using B 3 clients to be able to get that information for vulnerability findings. That's it. Um, alright, moving on to supervisor, I'm just gonna have a quick look at the time. OK, we're good on time. So this is the supervisor agent that we have, the, the the basically the one that controls the entire orchestration for the workflow. Um, now for the supervisor to be able to interact with the agents, we're using a pattern called the agents' tool. Um, if I scroll all the way down, you'll start seeing what I mean. Actually, I'll just mention one thing very quickly. So here it is. So you'll see these 3 lines, we're importing the 3 key agents from the different files that they belong to, and we're making them available as tools by using the tool decorator. So I'm gonna scroll down very quickly. Here it is. So this is just to consult vulnerability analyst, which is the agent itself, the specialized agent. We just pass the large language, sorry, the natural language request in as a string, allowing the vulnerability analyst to be able to understand the intent of the user. So all the supervisor agent is doing is understanding the intent, passing that request onto the specialized agent based on their capabilities, and I'll walk you through that as well. And then let the specialized agents do the rest. That's it. Um, so in this case, we've just got the three tools. We're also using the use AWS tool if in case a request is outside the scope of any of these 3 agents. So I'll walk you through very quickly the system prompt itself, yeah. Um, you're defining what the role is, so which is a patch automation coordinator routing request to the specialized agents, and I have some response guidelines in terms of what are the things it needs to follow, like verbosity in terms of the output, being very direct, uh, what we actually need, what format do we need it in, in ofagination, truncating the data if it's more than, you know, 10, as you might have seen, said 50 severe, 50, um, uh, severities available but only showing 10, for example, and those kind of things. And then the routing priority. How do I want the priority to be, how do I want the request to be routed, which is understanding the user's primary intent. If it can't find that information, go to the context from the memory itself. And if it can't find that as well, then route to use AWS. That's it. I just provide the details around when to use the specialized agents, which is analyze security vulnerability, CVs, etc. etc. So, you can use semantic uh semantic routing, ah, keyword routing, or capability-based routing as well. So this is just one way we could do it. Um. And that's it, um. We mention the tools again. Now, if you recall in the 2nd agent from memory, when, when the tool started printing out one line at a time, this is what I was trying to get to. With tools that you've defined, you can control batching. Right, so if you've got um an API call that has to be made 10 times, 20 times, then you can control it. But something like use AWS, which is a built-in tool, you can't control it. So you're using the prompt itself to be able to control the large language model's behavior of how it should use or leverage that tool. So in this case, batching related queries, using filters to reduce data transfer, this all helps in reducing the token consumption. Yes, we have to pay for tokens, and we realized that again as the cost started going, started going up. Um, and that's it. It was just output format, like, you know, emojis, etc. to make it look appealing. But that was it. That's the supervisor agent, um, um, like, just, just to give you an overview. Um, I'll show you Patch Manager very quickly because that's the most complex one, and every other specialized agent follows the same sort of format. As you can see, we've got a whole heap of helper tools available. It's just because we wanted very specific capabilities to come out of it. We want it to be deterministic, we want it to be consistent. So, just walking you through it. We defined the scope very clearly because what was, what we observed was when the scope wasn't defined, and you might have observed this as well, the supervisor agent, when a request comes through, routes it to multiple agents. If a specialized agent is not able to perform the set task, and it's because it's not in its scope, it comes back with an out of scope response, and the supervisor agent now needs to correct itself and figure out which other specialized agent that it needs to route the request to. So this is extremely important and something you might want to keep in mind. Now, as you might all appreciate. Patching as a process is extremely rigorous, it is very controlled. You don't want it to just be done in a hillybilly way. You wanna control that whole form. So we, when, when we started off, patch Manager was doing things in paddle when it shouldn't, like it was trying to do devan staging at the same time, trying to be more efficient, um, but we realized that that's not the way it should be. So we had to bake in this concept of workflow within the prompt to control its behavior. So, the way we kind of started off was, like, get the CV severity first. So either the user provides that in the prompt itself, saying, hey, give me the critical severity in the dev environment, and if that's not available, if it's available in the memory, extract it from there as well. So it has different ways that it can actually absorb that information, uh and use that for its calculation, and we'll get to that in, in a minute. Then get the patch compliance. So we have multiple applications running within these instances, each assigned a compliance framework that has an SLA associated with it. So, get the patch compliance state based on the environment and the severity that's been provided. Now, the next step is to get the maintenance window for the said environment, which is when is the next execution from a patching perspective gonna happen. Now comes the the smart decision making ability of the the patch manager agent here. Now that it has the severity information and understands the SLA of your application, it needs to make a decision on if it has to patch it in an emergency form, which is straight away, or can it wait for the next maintenance window. That all happens with just a few lines of code, rather than a, a pretty large ifL statement that we would have probably written too complex for, for anybody to understand. And that's it, and then just display that information which is, hey, Windows within or beyond the SLA deadline, so we need to either patch it now or leave it for later. That was it. So that whole workflow was baked into the prompt. Could we have optimized it? Absolutely, but we reached this stage based on a lot of tests. Last week we had a a release, as we're getting closer to reinvent, we're actually in revent already, but there's a lot of new announcements. So one new announcement was strand's SOP or standard operating procedure wherein a lot of the workflow sort of. Uh, implementations can actually be externalized, which makes the prompt a lot more smaller, which means less token consumption, more faster larger language, more faster agent responses. Um, Alright, then comes the instance ranking, so it's done the calculation on emergency versus not emergency. It also has the ability to rank your instances based on business impact and blast radius. And then actually go through a phased execution. This would have taken a long time, hence we couldn't show it in the demo and as I mentioned that previously, but under the hood, it has awareness that it needs to go in a phased manner, even if it's an emergency patching situation. And it does that, keeping in mind that health verification is extremely important, which is mandatory after each environment. We've all observed this when you patch, applications are, you know, experience something and they just go down, you're impacting your up time, etc. So we kind of built a very simple health verification system, you could build a more complex one obviously. But the logic is the same. We've got two layers of checks. One is a simple pink check with SSM, which is for the infrastructure layer. The second one is for cloudwatch alarms. Now in your case, you could integrate your agents, uh, as a web hook with Splunk, uh, Neuralic DataDog, whatever you use, um, to see if any alarms have been triggered as part of the patch, and if it, if it does happen, it can roll back. How useful would that experience be where you can just roll back capacity to apply because it's broken the system. And it's also smart enough to figure out that it only rolls back that environment, so that you can compare a delta between a previous or a lower environment that your passion was successful compared to this environment as well. And finally, all this information needs to go somewhere for compliance reporting so our CTO is happy, and that happens with a specific tool and all that information. And that's it. We had rollback policy output format, but it's, this is pretty straightforward. Couple of things I want you all to take away from a prompt perspective, um, I'm sure you've all experienced this. As much as I wanna say prompt engineering is an art, it feels like there's a lot of science behind it now. A few key tips that have helped us is keep them as structured and clear as possible. Keep them concise, the more robust they are, every time the agent actually like is working with your specialist agents, they are taking a lot of this with it, makes it really hard, becomes more expensive. And keep in mind that you don't have conflicting instructions at the top and the bottom, right? It's happened a few times where you've said something at the top and the bottom is completely different. Agents confuse quite easily in that regards, um, exemplify, show them an example of what good looks like so that they use that as a pattern moving forward. Um, yeah, that's pretty much what we had in terms of the code itself. Um, what I'm gonna do very quickly is, Oh I think I had, yes. Can I just change that to PPD? I think We've all seen, like, Justin's already shown the demo, I've gone through the code base with you, and look, if you have time, we're more than happy to take questions, uh, either offline, depending on how, uh easy is Romi gonna be on us, but we're happy to take questions, or just, just go out of the room and, and answer any questions, but we've just seen how agentic AI can help. You, manual everything cooperation, be better prepared, right, it's technically and operationally viable. It's not if, it's, it's just a matter of when. Not saying it'll just like magically solve it tomorrow, but I think we're very, very close to a stage where answering questions for the security director, what's a risk a blast radius. That was just done in seconds, right? You no longer are dependent on application owners and application teams to give you that information. You can get that. Within a matter of seconds, platform engineers now have the confidence and the comfort knowing that if they apply a patch, they can roll that back. This is especially useful in a centralized environment where you have application teams, platform engineering teams, cloud teams, etc. Super useful. Compliance managers are feeling a bit more at ease, knowing that they don't need to rush through a critical vulnerability just because it's critical. If it fits within the maintenance window, they can make that decision sooner. And the CTO is super happy that they're not gonna be in the press because they're more compliant, they understand the health of their, their business from a, from a, from a, from a, a patching perspective, and they can continue to help the teams evolve and get better. So if there's 3 key things that we want you to take away. Of these three things here? Tools versus prompts. I think we get to a stage where um again it's getting a bit more philosophical, but what we've experienced is tools is more deterministic, more consistent, prompts is more creative, it lets the large language model think on its own, it works sometimes, it doesn't work sometimes. So keep that in mind when you're trying to build a particular, when you're trying to get to a certain outcome, think about it when, when you think about tools versus prompts. Augment or replace I think a lot of people think like AI is just gonna, ah, this is a controversial statement, I think AI is gonna take over the world. I, I think it will eventually. We're not at a stage there yet. Um, whatever we've shown you today is showing augmenting capabilities of an AI to make your teams, make your systems more smarter, more, more, more powerful, so they're meant to augment or replace your existing systems. Agents will give you the scale, the agility. But we, they need humans to provide the governance and oversight without which it's gonna be a really bad situation for all of us. And finally, build small, test and it trade. We didn't get there on day one, we've taken a few months to get here. Again, we've got our day jobs, but I'm sure if we had put our mind in, just like you would, you can accelerate through the whole experience, but start small, test in a trait, use your observability tooling to get to a stage where you understand what it can do, what the responses look like, um, before you tweak it. Right, take your time. And one key thing if I could highlight here is, Even if you're not a software engineer, please follow all the software engineering practice in terms of version control, look at tools that help you with model evaluation, prompt evaluation as you tweak your prompts and models. The parameters, they're super useful. We are planning to publish the code soon. But whatever we've built has been based on patterns and samples that are easily available, so these are the QR codes for bedrock agent, co code samples and strands agent. Hopefully we'll have our code published soon. But in the meantime, you can try and take the lessons you've learned today, hopefully, um, in terms of the patterns, the architecture principles, and apply them with these samples. And everyone's here for swag, I'm pretty sure. If you're not, you can come for the demos. We'll be there as well, uh, but please do visit the Cloud Ops kiosk, um, at the AWS Village, um, but yeah, just on Justin on my behalf, thank you so, so much. Uh, we do have 5 minutes, so we're happy to come around and take questions, uh, if that's OK with Ziggy, um, but yeah, thank you so much, really appreciate it. Thank you.