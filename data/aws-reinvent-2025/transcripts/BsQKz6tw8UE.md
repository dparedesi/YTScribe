---
video_id: BsQKz6tw8UE
video_url: https://www.youtube.com/watch?v=BsQKz6tw8UE
is_generated: False
is_translatable: True
---

Hello everyone, good afternoon. Uh, welcome to CNS 427, uh, supercharging the testing with Quiro. My name is Aarti. I am based in Singapore and I work with customers in Southeast Asia. I've got Thomas. Hi, I'm Thomas. I'm based, based in Sydney. I look after startups across OIPJ focusing on agentic coding. Alright, I hope you've all had a good Reinvent so far, um, almost the end of Reinvent. So in today's talk, we are gonna talk about how you can use Agentech AI across your surless development life cycle, specifically to simplify testing. Now testing is a vast topic. Uh, so today we will focus on automated functional tests for Servalli, which is unit tests, integration tests, and end to end tests. So this is a Level 400 talk, so we assume the audience is familiar with Servallis and also some of the basics of coding. So to first understand the challenges with Serveli, let's start with what's a bit different about Serveless applications. Um, Serveless applications are highly distributed or modular, uh, which means they have a larger number of integrations as opposed to traditional apps, and they also make use of a lot of cloud native services. Now this has implications for the testing process where it becomes important to test the integration layers as well. And one of the questions that comes up very often for automated tests is handling dependencies for isolated tests, that is, should you mock them or should you emulate them, or just use AWS services. And finally, your Lambda functions themselves may not be complex enough, so how do you think about things like coverage or where should you actually focus your testing efforts? So today we are going to see how Quiro can help us with each of this pillar, starting with how you should write your applications to make it easy to test, how you can use Agentic AI to generate the tests, and finally how you can combine the power of NCP servers with agents to kind of use historic data to continuously improve the quality of your applications. So because our focus is testing in today's talk, we've actually pre-built the application which is just a task management API that uses Amazon API gateway as a REST API, Lambda for processing, and Dynamo DB for the persistence layer. There is also an asynchronous component where any task events are published out to Amazon Event Bridge, and then they are consumed by a notification service. So throughout the next hour or so, we are going to evolve both the application and write the test for this application, for this task API. Uh, so with that, we are good to start and we're gonna switch to the IDE. I It's stopped mirroring, I think. It's not mirroring. It's not mirroring, sorry. Sorry, just give us a second. Um, so I'm gonna start off by taking you through a quick code walkthrough of the current application. Now, we have chosen Python in this particular case, but a lot of the best practices we talk about will apply for any other programming language as well. Now as we go through the code base, this is a slightly trimmed down version of the code. So we have published the full version of this application to GitHub and we'll be sharing the links with you later on. Uh, so for, for this talk, we just want you to focus more on how the code is structured and the actual flow and how we're gonna evolve it. Um, so real quick, if we look at the directory structure currently, um, we are just gonna focus on the task API behind API gateway. Uh, all of the code is in the task API folder and our tests are in the test folder, and we'll start with just the unit test for now. So, just a quick check. Can everybody see the code? Is it big enough? If not, just raise your hand. We'll, OK, cool. So let's start with our lambda handler first. Uh, so we are using power tools here to simplify implementing some of the serverless best practices. The handler itself, we've gone down, we've chosen to kind of combine all the task CRD operations into a single lambda function as against a micro lambda, but Power tools makes it really easy to route the request to the correct function. So for example, if you get a post on the slash slash task endpoint, it'll end up invoking our create task function. Uh, so it's fairly straightforward, we first parse the event to extract what we want, which is just our task details. We build the task object. Then we process the task to our database, in this case Dynamo DB. We publish the event and then we construct the response and there is some basic error handling here. Um, our task handler also enforces some of the business rules. So as an example, if you're updating a task and you're defining dependencies between the tasks, you don't want to end up with circular dependency. So in, in this case, when you update a task, there is a check here where we are, we've created a helper function that validates dependencies, but the rest of the flow is the same. You update the database, you publish the event, and then you construct your response. Um, our helper function here, given a task, it basically queries the database to build the existing dependency graph, and then it's going to just pass through the graph to identify if you're going to violate the rule or not. Model's file is pretty straightforward. This has just our data classes. Our domain logic has business rules. Again, in the real world scenario, you would have a lot more rules here, but to keep things simple, we will simply focus on the circular dependency check for the demo today. So the idea here is given a task, it's dependency and the dependency graph from the database, this is just doing a depth-first search to identify circular dependency. And then the very last file we have here is the integrations where we define the integration with AWS services. So you can see we've used Boto 3 initialized our clients here. And this is the class that, this is the module that has the implementation of the methods invoked from our handler. So our safe task to Dynamo DB ends up calling the put item API and likewise, uh, this is where we actually publish the event to event bridge. Um, let's take a very quick look at how we've written the unit test for our Task Handler. Now, because our task actually persists tasks to Dynamo DB and publishes events, if we have to unit test this, we have to mock out those dependencies. So in this particular case, currently we are using Moto. So Moto is a Python library specifically designed to kind of mock out BTO 3. So that makes our life a little bit easier in a couple of ways. So our P test fixtures are just the reusable setup and tear down for our tests. As a best practice, we have set set the AWS credentials to dummy values for our unit tests. So with Moto 3 you just need to use the mock AWS context Manager, and the advantage is because it's designed for Boto 3, you're still using the same Boto 3 client calls. What this is also doing transparently is that it's monkey patching the Boto 3 calls during runtime. Basically it'll intercept calls to Boto 3 at runtime and replace it with our mock. Uh, we also need to mock our event bridge. We have created a mock test context, and just a quick look at two tests. Our first test is a successful scenario where, given a task, we create the task successfully within the database. So we need to pass our our fixtures as arguments to the test case. We import the handler, we create our test event. We invoke the lambda handler with the event and the context, and that's that we validate the response, you can optionally test the mock state as well. And then let's take a look at another test case uh that enforces the circular dependency rule. This test is a little bit more involved than the first because this depends on the dependency graph existing in the database. So as part of setting up this test, we need to create a few tasks, in this case task one and task 2, and we persist these tasks to our mock Dynamo DB database. And the rest of the steps are pretty much the same. We create the test event, invoke the handler, only in this case we expect an error response. So I'm gonna run this test in a second to show what happens. Uh, but this is our current code base. So kind of the question here is, you know, let's say our Business rules change in the future, the needs change, and maybe we need to replace Dynamo DB with something else, maybe DocumentDB or maybe we need to replace EventBridge with something else. So can you think about the implications of this? So, of course, we'll have to update our integrations file to talk to the new services, but just from a testing or developer experience perspective, do you see any challenges with the way the code is written right now? You can just shout out the answers and I'll just run the test suite. How many of you ran into difficulties when you were trying to change the integration architecture and then you had to change all the testing, all the related code afterwards, just give us a raise of hands. There you go. Yeah, so, so just as we did the code walkthrough like you, so our test cases, for example, that's testing the handler code, and it's, you know, just basically testing the response status and the details of the response returned, that test will have to be updated because now the mock will have to be changed to work with the new services we pick. Um, so this kind of creates, so although you did not change the actual code that was tested by the test case, you need to rewrite all those tests. So this kind of, you know, creates extra work. So I have just run the test here. Um, so for those not familiar with Python, so poetry, it's just a library commonly used for packaging and dependency management, and PyTest is a really common testing framework in Python. So I've turned on the timing for our tests here. Um, so you can see that there is a slight overhead in our setup where we are initializing the mock functions and it varies between 400 to 500 milliseconds. Um, so in this case, we've only got 2 tests, um, the actual runtime depends on whether the dependencies are cached or not, so we've run this a few times in our laptop, but there's kind of, you know, room to probably improve and make our tests a little bit faster. So just given this code base, Let's quickly summarize what we have seen. Um, currently, the way we've written the code, this is very tightly coupled to the infrastructure choices. So if you have to change anything on the AWS layer, you end up rewriting test, uh, that would, that should not really be affected by the change. There is a little bit of friction with the developer experience that you do need to know for mocking exactly how the services work. And then of course there, there's room to potentially improve our tests and make them faster. So now I'm gonna pass it over to Thomas to see how we should address these problems and what's the best way to write the tests. Thanks, Arie. Before we proceed, how many of you are Python developers? Just give us a raise of hands. Good number. For those of you that, that don't work with Python, don't worry, all these principles are applicable across the board for. We just picked Python just because we are familiar with it, but all the principles essentially can. Uh, be applied to anything and everything. Uh, just give me one second. Oops. All right. So. Arie walked us through our current architecture, uh, we saw some pitfalls in there. Uh, now let's have a look at how we're going to fix it. So what I'm going to do, I'm going to use Kro CLL for this particular task. Now just give me one second, I'll just fire it up. And You may notice that I'm not using the traditional uh invoke command for Kira CLI. I'm using something that's called custom agent, and I'll tell you what a custom agent is just in just a minute. I'm just going to pass a prompt which essentially is asking Kira to do a review of my current architecture and propose, uh, or suggest what are the problems and also propose how to fix them. So I'll just execute that and we'll get to that in a minute. I'll just leave it running. I just want to show you. What's under the hood. So if we go to a Kiro folder in our project folder, we'll have a section called agents. If I opened it up and I'll. Just remove this for a minute. And make this a little bit bigger. So this is essentially a configuration of a custom agent. Now, what a custom agent is, it's an instance of Kiro agent or Kiro CLR agent rather, that we can configure for a specific task. So you can notice that I have a specific set of MCP servers just for this particular operation, so, and I'm also passing a description and a prompt to uh the agent which is used alongside of my prompt that I passed to it. So this is think of it as a specialized agent for a particular task. So in our case it's going to be evaluating our architecture. You can have a specialized security auditing, uh, agents. You can have compliance agents, etc. agents, etc. So it depends on your use case, uh, that you would use. Uh, we also have a built-in agents that we released this week, agreement, uh, such as the, uh, AWS Security and AWS AWS DevOps agents which are on the side of Quiro. They're not inside of Quiro at the moment, but. Uh, these are capabilities that you can configure inside of the Kira or Kiro CLI brow. We also configure tools, uh, so it's, it's quite customizable. You can configure steering files that actually tweak how the agent operates and what kind of output it returns. So, with that said, let's go back to our terminal. And let's double check what the response is essentially. So we can see that Kiro on the scroll all the way up. Kiiro did a review and it did use. Uh, did read our whole project essentially, uh, at points here and there it used the MCP configuration just to enhance its answers and generated a full hexagonal, hexagonal architecture audit report, uh, that we can either read in this terminal format or we can read it in a proper MD format, uh, which I'll show you and it's probably easier to read. I'll just flip here. So there's a few sections there. I'm not gonna go through that in detail, but this just showcases how you can leverage agentic AI to help you with current review and kind of also look around corners because you know we are developers we know what we're doing, but sometimes we don't account for every single scenario. and we may not see everything that's related to our application that might be potentially needing updated updates or or being, uh, need to be improved so we can see already that it it asks us or it suggests us to fix, um, the domain logic dependencies. Uh, there's going to be more, uh, section essentially related to the pattern how we design the architecture of the application, etc. Uh, as I said, I'm not gonna go through this in a whole heap of detail, but, uh, you can see there's quite a bit of information there. Now, why is this useful? Because we can essentially go to Quiro and we can ask, ask it to. You can notice I mean a spec kind of uh spec uh flow which creates spec driven development essentially uh spec specs that we can use to quantify the requirements, design and implementation of individual tasks. It's particularly useful for for feature or general software development, but in our case we're going to use it to create a plan to integrate the changes that Kiiro actually suggested us to do. So I'll just reference the file. That has our. Evaluation our audit and I'll ask you to generate. It's just while Thomas is doing this, so, uh, I think he mentioned hexagonal architecture. So the thinking there was because our code base was kind of tightly coupled with the different concerns, so the idea was how do we decouple it, and hexagonal architecture is one way to do that. So the idea is you have got code or ports that only deal with the interfaces, and you've got the core business logic, and then you have a glue layer that connects the interface to the business logic. So we have kind of used that as a baseline to uh rearchitect the code. Exactly. So we can see that this was a little bit faster than usual just because I actually have a spec spec created just for the sake of time, but normally Kira would go in and create the whole spec from the ground up with requirements, design, and tasks, we can see those three files in here, referenced and actually gives us a description of what it did. Now, I'll show you how those files actually look like. So if I go back to my Kiro folder, I'll just minimize this. We have a folder called specs, and in there we have a hexagonal spec. If I open the requirements, I'll just get. This here and, Oops, sorry. Where am I? Here, OK, so, um, in the requirements file, oops see Daisy, hold on. There's always something with live demo. OK, let's minimize this. There we go. So this is our requirements file. You can see that we have introduction, we have glossary to kind of understand all the terms, and we have all the requirements and especially acceptance criteria that are needed for every single project, any single change that you're running to your project, be it a feature, be it architecture, be it anything essentially, it's very, very useful when working with AI especially. Now the next file that he created for us is design. So this is the full design of changes we can actually see the diagram of changes that it'll be implementing into into the process. This is the target architecture, how it's going to augmented or rather decouple, uh, uh, features to make them more flexible, more versatile, and, uh, easier to test as well. Uh, I'm not gonna go through all of this in detail. I just want to highlight the process. But the most important thing here is the task list. Now here we could go to Quiro and ask it to essentially start executing these either in, in sequence 11 by one, do the whole project ess essentially in one go. Depends how much time you have. Depends if you're doing something on the side, but you can essentially delegate this to Kiro to go through on its own. Observe it at some point, um, verify that it's doing the job that it's supposed to do, uh, but essentially do the migration or rather refactoring of the obligation on the go on its own. So, um, I'm gonna make it easy on myself, I'll just fast forward. Normally this would take a little bit of time, let me just. Make this a little bit bigger. I hope you can see everything now. Uh, OK. So, just to illustrate the original state that we had when Arti was going through the application, we had something like this. We had everything kind of coupled together. Yes, we had some files that were separate, but it was all kind of bundled together with tightly coupled references. So all the HTP passing, business logic and, and all the integration calls were kind of stuck together. Now, after the audit and the implementation of that, uh, of those findings of the audit, uh, we would use Quiro through Spector and approach to modify our architecture, refactor it. This is how it would look like. This is the current state of our architecture. So we can see we still have our handler, but it's much, much leaner. Uh, it's only processing HTTP requests. Then we have all the business logic kind of offloaded to our domain layer, and I'll go through this in more detail in a second. Uh, and then also through interfaces, uh, we are communicating with the, uh, adapters that are, uh, invoking our services, so the integration. Now how does this actually look like? If I go to the task handler again, and I'll just minimize this side. So, in our main handler, uh, we have a method called create task, right? So The important bit here is that we're no longer coupling anything, we're calling a delegated task service. Now if I scroll up here. And I go to the site, essentially, so. That action is calling the domain logic. If I flip to domain logic, essentially, and I'll go to the top of the task service. We can see that this particular service requires a repository and even publisher to be to process. So there's two kinds of streams that we, we see. Uh, we have. One way that one stream that essentially uh does an operation when we don't specify them, and I'll show you what that does, but if we if we do specify them we can point it to specific integration that we can manage. Now what does this mean? So if we would, uh, essentially not specify these these uh parameters, our application would default to what it has under the hood and we'll use interfaces. To use what we call protocols now protocols are specifically for Python, but what they essentially do, they create contracts without the need to, uh, set implementation, and this is what you can see here because the, the methods in our class for Tel repository protocol are pretty much empty. There's nothing really in there now. Why this is good, because we can point it to anything, right? But in our case, If we go back to the logic, essentially we can see that the repository in this case is set to none. Which means that we are calling integrations. And through integrations. If I go to integrations we are calling our integration to general MoDB or the even bridge, right? And we can manage this we can modify this we can point it to something else if we need to, uh, this is for the case where we did not specify the repository and the even publisher. If we do specify it, it depends on what we specify. The route will be different essentially, and this is particularly useful for tests, um, so back to our architecture, just wanna recap here. This is how the decoupling would work here. And in terms of what we covered essentially in this part of the the session, uh, we essentially uh showed how the handle performs the the validation. um I'll just take this off. Uh, we, we saw how the domain logic is essentially containing all the business logic and business functionality. We saw how the integration layer works. And then, uh, also we saw how the abstraction works with the handler its own, so we are obstructing the logic, the business logic and the integration from the handle itself, and we are relying on the domain to kind of interface to the repository or the publisher to create a contract and, um, afterwards the integration implements, uh, through the interface straight away services. And back to you. Yeah, I, I'll show you the test. So just to recap, the task, our domain logic just expects a task repository that offers the safe task method or a delete task method. It does not know whether it is implemented using Dynamo DB or Aurora or whatever might be the service, and the exact implementation logic is contained within our integration layer. So if you go back to the problem suite statement of swapping, say, Dynamo DB with something else, your integration. changes, but as long as the database is exposed through the same safe task method, we don't need to change the tests in our handler and the logic code. So I'm going to now actually code out the tests and then we'll see how that looks different or simpler than before. Now, we'll start with the task handler, uh sorry, the test for the task Handler. Before I move to the test, there's one thing I wanted to call out. So if you saw when Thomas walked us through the code, we did not directly initialize the task service and the handler. We actually used the get task service method. Um, so what we are doing here is setting up our code for dependency injection. I'm gonna talk a bit about why that makes testing easier. Again, those of you familiar with Java, this will probably seem intuitive, not so much in the Python world, but we'll see how we can do it. So if we actually look at the Get task service, um, it looks for a service level uh variable taskservice. If this is set, it's gonna return it as it is, but if it's not, it's going to initialize the task service, and this is the flow that would actually kick off when our lambda is actually invoked in production. So, The key thing I want to call is at runtime our application does not rely on dependency injection because we have provided default ways for the flows. So this piece of additional code was specifically written to simplify testing, and we'll see how that simplifies testing. So let's go back to our test task handler. At this point, our handler still relies on the domain logic or the task service, so we still need to mop that out to test this in isolation from the rest of the code base. So the way to do that would be. Let's say we start off by mocking, get tasked. All right, something like this. I'll, I'll accept this and I'll explain like what this is doing. So now to unit test our task handler, we need to mock out only the domain logic, so we are no longer concerned with the actual AWS services in use. We just want to validate that our task handler works well and returns the correct response code to our end client. So we need to do a couple of things. One is we are going to mock the Getar service to return a mock object. Um, so in this case, that service is called the MC task, MC Gta Service. If you remember when we spoke about motto for Boto 3, motto is automatically understands BTO 3 API calls, but in this case, this is a custom mock, so we need to configure the behavior of the mock task service. So within our test case, we will have to go ahead and say configure mock. So we, I'm gonna just keep this simple, but basically you will have to specify the return value. I won't accept this because we're gonna do the test a bit differently, but just to give you the picture, first, I need to create what the return value for a successful create task call looks like. And then we also need the monkey patching because the patch now has to be managed by our tests. So essentially every time get task Services call, we want to insert our mock into the picture. That's basically what this test is doing. Now the thing with this approach is, remember we just wrote two sample tests, but we are gonna write tests for all our different resources and methods, and at times we want to simulate the error. So depending upon the behavior we want to simulate, our mock will either have a return value or a side effect, which would simply be raising the exception. So what happens, it's all our test code is now riddled with a whole bunch of mock code, and we also need to patch the code at runtime. Now the tricky thing with patching at runtime is it it can get brittle because if we change the logic of the domain service, it can break in unexpected ways, and also patching can sometimes leak states across tests, and this is where we are going to use dependency injection. So I'm not, instead we are not going to use the mock service, so let's see how dependency injection simplifies our life. So the first thing is we want a highly configurable mock whose return value can be changed depending on the test we are running, whether it's an error or a success scenario. So what we really need is a configurable fake task service. Um, so this is just an in-memory fake at this point, this is not a mock. So what we have done is you can initialize the fake task service with a bunch of flags that tells whether the service raises an exception or just works as expected and follows the happy path. So for example, it has the same methods that our task service offers, but when we call create tasks, the first check we do in this in memory fake is to check whether the exception flag is set. If set, it'll raise an exception, if not, it'll go ahead and simply return the task object. So in scenarios where you want to highly customize the behavior of of your dependency, it is kind of easier to do that with in memory fakes rather than a mock. So the complexity of the mock configuration has now moved here, so we are not essentially writing additional code, but it's just where the complexity goes. It's now in the in-memory fake. Now this is the first step. Now we of course need a fixture again, uh, because the second step is we, we need to replace the original task service call with this mock. So how are we going to do that? We said patching is brittle, so what we are going to do instead is we are going to use dependency injection here. So within our pi test fixture now, we initialize the fake task service. Now if you remember, our get task service looks for the module level variable, whether it's set or not. So in our test, we are basically setting that module level variable to our fake task service, and we have basically injected our in-memory fake into the test. Um, and that's it, so anything before the yield is set up and after the yield statement is your tear down in Python. And contest is a file that PyTest automatically loads, it's just where your reusable code goes. So how does this actually make our test cleaner? So the first thing is we, we want to use our fake, all right, we will probably go to, we don't need this, this was generated. So I'll stick to the previous um structure we had for the test and let's see what changes. The first thing is we are now going to make use of the fake tasker, so we need to pass this as a variable. We still create our test event. Now remember, the behavior of our fake task service when the flag is not set is to just return a successful task response. So all we need to do to run our tests here. is we import our handler code. So this should actually look similar to what we were doing. In the old tests. I then invoke this. All right, the autocomplete is a bit laggy. But basically, we're going to do the same thing that we did before. I called the lambda handler directly with the event and the mock lambda context. And then all we need to do here, it's all right, it generated this time. Uh, we can optionally validate the response, but I'll keep it simple for the demo. So if you look at it, that's about it for testing the happy path for the test task handler. We have used the fake task service, which gets injected into our task domain and it is simulating all the different scenarios that our task service can raise. So as an example, now if we take the case for the circular dependency error. If you remember the previous code, we had all this logic to basically create the dependency map in the database and all of that. That goes away because now I can simulate an error simply by Configuring the should raise circular dependency flag here. The rest of the steps are gonna look similar to what we did uh with the previous test. So you can see here that our tests are now vastly simplified. So what this means as a developer in the future if our domain logic changes or you change the behavior of the task service, so this file is the only place where I need to make changes to get my unit test to work. So having said that, it doesn't mean that you should avoid mock at all costs. So for example, if we take our notification service that's processing things from the Event Bridge, let's say it is sending out an email when your task is due as a reminder. There, there's no need for me to fake an entire email server or an SES because that's a third party dependency. All I need to know for that test is that the send message method was actually invoked and I'm good. But for this particular case where we own the code for the domain logic and we want to highly customize the behavior in memory fake combined with dependency injection makes our code a lot cleaner. But then that leads to the question, what about our domain logic that actually validates that the circular dependency function works? So this becomes a whole lot simpler now because this is just a pure function that given a task and a dependency map, it is just going to check and return true if there is a circular dependency or false if not. So if we look at the test case for this, let's say. Uh, this is our current dependency, except I, task one depends on task 2, let's say, and task two has no dependency at the moment. I'm going to try to force a circular dependency by calling this has circular dependency, and I passed this mock dependency graph I've created, and all I need to know is that this is going to return true. The negative scenario is equally simple, um, so I now have an empty graph here. And at this time this should basically say that there is no circular dependency. So that's it, and these tests again, don't need to change. In the future, I mean, if you change the logic for how you calculate circular dependency, all you need to do is rerun the test. So the effort kind of goes down. And the third thing we called out was, um, what is the impact of this on the timing of the tests, so. Um, so both of these are in the unit folder, so we'll stick to this. So for, uh, so mock basically Moto 3 is a uh library that mocks all of the AWS services, so it has a slightly bigger overhead. But for our particular case, the in-memory fake, as you can see, is barely 120 milliseconds, it's a lot more lightweight, and our tests are that much more cleaner. So we, we have now wrapped up our unit tests, but kind of this takes us to the next question. Um, when you're using agentic AI or spec driven development to build applications, uh, let's say you use the requirements, build the code, then you ask the agent to write the test for the generated code, um, and it might do a great job of comprehensive tests. But there is no way to validate that your requirements were correctly translated to the code in the first place. You know, your tests are validating the code that's generated. So how do you solve this problem, and Quiro recently introduced property-based tests to do that. So while Thomas explains that, I'm gonna run the test, but I'll explain what I'm doing after he's done to save time, yeah. Who has heard about the property-based testing in Quiro or in general? OK, uh, I'll use an analogy to kinda explain this. So think about a case where you're building your new tests. It's kind of like building a bridge, right? You build a bridge, you wanna test it, that it actually holds the, the load that it's supposed to be holding. Now, would you rather test it with your own car only, or would you rather test it with your car, my car, Arti's car, a truck, an ambulance? An elephant, right? So that's the difference between the traditional way of creating tests and the property-based testing. With property-based testing, you essentially define properties. And then the system runs not just one test. It can execute 100 tests during the same execution, and Arthur is going to show you how that actually looks like, uh, using some menu modules, but it's, it simplifies the process because it makes it simple to define but also faster to execute. Yep, so, like, like Thomas said, it's great to kind of also detect edge cases in your business logic. So the idea here is we'll just see, I've, I've run this test and I'll, uh, explain this in a minute. I'll start with the scenario I'm trying to explain. So if you think about a circular dependency check, that's a good example where property-based tests actually works well because we just want to verify the algorithmic correctness of that code and whether it does what it is supposed to do. That's really it. So to do that, we are going to kind of take a slightly Um, all right, I. Where is the code? I think I might have the wrong file open. Hang on, just give me a second. OK, it's just doubled. Why does it split it like that? Yeah, I think I did that before. All right, that's strange, so, OK, bear with me. Yes, so actually this is the first time Thomas and I are running this together, so I'm not used to his laptop, so please bear with me. So, all right, so in this case, let's say this is the scenario we want to test, OK, for algorithmic correctness. So, when you saw the unit test we wrote for the domain logic, we created these simple dependency graphs and we tested it, you know, like we said, task one depends on task 2, get task 2 to depend on task one, and it, it should detect. But let's say that there are complex kind of dependencies that exist in our database where, uh, you know, each of these nodes represents a task essentially. So let's say this task depends on 1, this depends on 2, and so on. Now, if I try to force this dependency of this main 2 back to main zero, now if there was a bug in the logic and let's say it only travels the right side of this branch, it might incorrectly conclude that, you know what, this doesn't set up a circular dependency, let's allow this to go through. But if my logic is written correctly, then it should test all branches that it encounters on the way. So this is a good use case where property-based tests might help us. So I've basically actually run the test. So, a few quick things to note. So Python has a library called Hypothesis that allows you to do property-based tests. So obviously with property-based tests, instead of fixed inputs, we are generating a large number of inputs from a given problem or input space. However, the data is not completely random. So in our case, task IDs are UU IDs, so that's why I've used the strategies here. In this case, to specify that I want to create a bunch of UU IDs for my testing because that's what I'm using for my task. Next, let's just focus on this branching test. Um, you see the given decorator here defines the input space to be used to generate the test. So I'm just gonna refer to them as main chain and branch chain as I showed you in the diagram. I'm just generating a list of task IDs. I've specified a few constraints as to how many, and the ID should be unique, because, of course, each task is unique within our database. Similarly for the branch chain, and then I'm also specifying that we randomly choose a point on the main chain where we want to create the branch. Now if you look at the test case. We also define a few additional constraints. For example, we don't want any overlap between the two lists, because we are using UU IDs there's a good chance, we won't actually breach this constraint. And the test itself is pretty simple. Basically I'm looping over the list of task IDs generated for the main chain and I'm building this left side of the branch first, and then I do the same for the second set of the list and building this right side of the branch first. Then I actually pick the point where I want to define the branch, so if there are any existing dependencies there, I want to preserve the dependency, and that, that's really it. I've built at this point, the dependency map, uh, in the database, and that's it. I just. You know, loop back the last node back to the first node, and then I expect that it'll detect the dependency every time. So the way we have written the test has changed. Second, I ran this test already and. You can see that for each of the tests there is about 0.34, 0.32. There is a little bit of an overhead in running the test, although these are still unit tests. So what exactly is happening behind the scene? So to understand that I've installed this open source plug-in called Tikiy. Sorry, I'm just gonna have to run this again because. All right, got it this time around. So this, this plug-in makes it easy to visualize what's happening under the hoods with property-based tests, specifically. So what this is showing us is, I, I specifically chose just the test that we did. Uh, the important thing to note is it did not run a single test for this function. It ran 100 test cases, each with a unique combination of inputs. But you'll also see here that the generated number of samples is 104. That's because it created 104 sets of inputs, but 4 of them were discarded because it did not meet some of our constraints, and, and that's OK. Our test itself, actually each test runs pretty fast, but it's just that it's running 100 of them, so that's what it's adding to the overhead. Now this visualization in, in our case, the good news is we wrote the logic correctly and you can see all 100 of them passed and that's good. But when you have failures as a developer, it's good to look at what was the input for which the thing failed, so it's easy to troubleshoot, and that's where this helps again because if you click into this you can actually see every single input. In this case, of course, everything was successful, but you can see what was the main chain, what is the branch chain, what is the branch point. It even tells you the actual code coverage, what were the lines that were tested by this, and it makes it easy for you to troubleshoot things. So that kind of wrap wraps this up. So kind of where this is useful is anywhere algorithmic correctness or, you know, pure functions, business logic, really good, especially to catch edge cases. But the moment you're thinking about say end to end tests or so on, they're not so good because for end to end tests you need specific inputs that will actually trigger your end to end workflow and random data might not really help you there. But that's a good way or even if you don't run it as part of every CI, it's a good way to validate correctness of core kind of business critical business logic and whether that works as expected. All right, so that wraps up this one. So we're actually good to move on to the integration test. So we've finished our unit test. So for the integration test, the first one I wanna uh discuss is just uh validating the schema for asynchronous integration, which is our event bridge part. Uh, so what exactly does schema test do? So the goal here is as a publisher of an event, if I modify my event such that I maybe remove fields that my subscribers depend on, I'm gonna break the logic of my downstream subscribers. So that's really what the schema test is for. So if you have any breaking changes in your event schema, it's going to pick this up. Now our task API is super simple and I think it's fair to assume that it's the same team that owns both the code bases and maybe you can coordinate the changes. But the moment you have like a central event bus, multiple publishers, multiple subscribers, where publishers often don't even know who the subscribers are, it becomes more and more important to validate uh the contract of the test. So again, there are a few different ways to write this. It depends on how you're defining your event schema. For example, Async API is one way to define your schema or Open API, then you can use tools specific to those. But because we are in Python, we're gonna do it the Pythonic way, and we're using Pedantic, which is just a data validation library. So defining a schema is as simple as inheriting from the base model, and I've basically defined what a task, create event or update event will look like. If the task is deleted, of course the schema will just have the ID. There's another important thing we are checking because we're using EventBridge, and EventBridge expects a few mandatory fields, otherwise you can't publish to it. So we are also validating the compliance to EventBridge schema. Now actually running the test is easy. I've, I've got a helper function here, so validating a schema is as simple as just calling the model validate on the class with the event, and that's it. So my actual test cases are generating the create event, delete event, and so on, and just calling this helper method, and, and we're good to go. There are a few advanced other ways to test this, for example, consumer-driven contract tests and so on. So but if you're interested, just find us after this, after this talk, and we can talk about it. Now, let's come to just integration with actual AWS services, um, which is Dynamo Devi and EventBridge. So our recommendation here is for AWS services, test against the actual services. Uh, that way you also get to validate the other integration properties such as, you know, whether permissions are set up correctly or networking is set up correctly, and so on. Uh, the other thing to think about also is with mocking libraries is whether the libraries fully support those services. So as an example, Moto that we, you know, used, uh, in the initial version does not support global tables for Dynamo DB out of the box. Then you will have to manage that replication within your code. So that's another thing to think about if you are relying on MOOCs. Um, so let's quickly take a look at Dynamo DB. Like we said, we are going to test this against the real, uh, database. So our test fixture is straightforward. So if you remember the integration code that uh Thomas walked you through, you initialize the task repository with the actual table name. We are going to modify the UU ID because we don't want to leave the test data around so that we can delete this. So we are gonna fix what is the ID of the task. Uh, there's the cleanup that basically deletes the task. Now there are two types of tests here. One is the happy path scenario, you create, you know, and you make sure that the tasks are persistent. I create the task object. I kind of use my P test fixture task ID for it. I simply call save task, if you remember the protocol that he mentioned, save task allows you to persist the task to the database, and that's it. Then we retrieve the task from the database and we're good to go. So again, remember in the future, if you swap Dynamo DV with something else, this test case actually does not change at all. You simply rewrite your integration code and run this as is, and, and it should work. Oh well, you will have to rewrite the code to retrieve the data based on the database you're using, but a large part of the test remains unchanged. Now, the more interesting part of this test is actually the failure scenarios. So the first thing to call out here is, again, we are simulating errors with services. Once again, in memory fake comes to our rescue, so we have a fake class. I won't go through it, it's similar to what we did with the unit test. But the question here is, what exactly is it we should be validating with failures? So there are two things we should be uh checking for. First is, are we surfacing the correct error code or exception code and message so that the client knows what to do with it, or are we just collapsing everything into a 500 and then troubleshooting becomes hard? The second thing to test is does your application behave as expected when these error scenarios occur. So to make it concrete, let's again go back to our application, like, uh, let's say for our task app, we have an offline mode, so people can work on tasks offline. And probably when they come online, it's possible that you might end up with conflicts in tasks. So how are you going to resolve the conflict? So we have used a simple strategy of first right wins. So we have a version which is the Unix timestamp of a record, so when a client reads it and then they write back, if the timestamp is changed, you know, the right will be rejected. But the error message should convey enough, so the client knows what to do with it. So the way we're going to do this is um In this case, we should return a 409 conflict error. Um, so what we will do is, now this is going to start to look similar to the unit test. So the question is, first, is this an integration test because we're, we're just using uh MCs. The second question is, are we repeating what we did with the handler test? So for the first question, in this case, we are evaluating a failure with the integration layer for Dynamo DB so it logically kind of sits as part of the Dynamo DB test validation. So we have left it in here with the integration test. The second thing is it's not an exact repetition of the handler test, um, because if you remember in the handler, we mocked out the entire task service logic layer, we injected our fake there. So that part of the code was never tested. But if you look at the fake that we have created here, So the first thing we are doing is we are creating the task repository, but we are using a fake. So once again, dependency injection, we injected the fake database into task repository. We are using a mock for the publisher. So here again, mock is handy for us because when I'm testing database failures, I don't want to publish any events to EventBridge, that's all. I don't need to actually configure the behavior of EventBridge, so here mock is super handy for me just to prevent events from being published to EventBridge. And then I actually instantiate the task service with the fake repo and fake publisher, so we are testing all of the code that has been written in the task service, so it's not a repetition. You could choose directly to validate the task service and not call the handler. The reason we chose the handler is that we are also mapping some of the error codes. That is, for example, when you have an IAM error, which is a permission error, and Lambda doesn't have permission to write to Dynamo DB, this permission error has no meaning to your end client. So here we actually check that your error message does not have, um, oh, I'm looking at the wrong test case. Right, that's the test case, sorry. So here we actually check that your error message does not have permission or access in the message and you're basically surfacing IAM errors as internal error. So that's why we chose to test the handler. And then the last one. Uh, for EventBridge, the main difference with the EventBridge test is unlike Dynamo DB, you can't really query EventBridge because you publish an event and it's gone, unless there is a subscriber. So testing EventBridge requires a little bit of extra work where we are setting up a test harness, which is just another lambda function that's listening to test events. It adds a bunch of metadata and persists it to another Dynamo DB and then we just read back from the Dynamo DB. So if we look at the Happy Path case for this, we initialize the publisher with the real event buzz, we create the event and we just call published task event. Because there is extra work in kind of lambda receiving the event, processing and persisting it, we do introduce a wait time here. Uh, to allow for that, and then we basically query the target database and we are, we are done. So I'm going to just run these tests, which actually take a little bit time to run, uh, but we'll have this running and I'll pass it off to Thomas to finish the last part. Um, so, OK, while that runs, very quickly summarizing, what we did for the test was MCs are useful, but use them sparingly, and we saw at least 2 or 3 places where we did use the mock. However, when you want to customize the behavior of your domain logic, uh, in memory fakes work better. Dependency injection makes the code cleaner, but remember at runtime we have provided defaults, so the code at runtime does not require dependency injection, so we're not talking about, you know, full dependency injection frameworks. It's a good idea to validate your event schema. Integration tests, run them against real services, use fakes to simulate error. And very important to validate the handling of integration failures, like are you surfacing the correct exception codes, messages, so your client knows what to do with it. And that actually wraps up our test section. We have the last, last part of, oh, the tests have. OK, the tests have run, not surprisingly, it's almost 17 seconds because the wait time I specified for EventBridge was 5 seconds. So that kind of adds to it. So what we recommend is unit tests should definitely run on every commit, but some of the long-running integration tests, good idea, and only when you explicitly change the logic or for major releases, so you can optimize your build time. All right, so we're moving to the last part of our session. Let me just flip to another window, make this a little bit bigger. OK, um. I'm sorry, uh, I missed the phone here. There we go. So We, you may have noticed that we kind of followed certain flow, uh, throughout the session. So first we had our application in a certain state, then we figure out what needs to be changed and modified. Then we perform the migration using the AI and, uh, afterwards we kind of drilled deeper into things and kind of figure out how to modify it even further. So this kind of flows into the traditional way of thinking of development life cycle but powered by AI and this can be elevated. We were touching lightly, to be honest, on AI in this case, uh, most of this stuff you can use Kira or or any other uh coding assistant essentially to help you out analyze the code, uh, but what I want to emphasize here is that we followed essentially the, the. The thinking, uh, approach that corresponds with the framework that AWS just released a few months back, which is AIDLC or, or the AI AI driven development life cycle which kind of brings structure to the chaos. So if you've followed the news for most of the year, a lot of people talked about white coding. Uh, I built this in 5 minutes. I build that in 5 minutes, which is great. But once you get to a higher level of complexity in your application, especially in existing applications. You can't just white coat your way through stuff. uh, this is where we kind of need structure and where spectrum and development fits into the picture very, very nicely, but, uh, to kind of bring that to the whole team on a, on a larger spectrum, this framework is particularly useful and it's, it's very simple. To implement actually it has 3 stages. So as we did ourselves, we kinda went through the inception phase where we kind of thought about what we can improve. We had AI perform a review to give us a list that we can follow, analyze, tweak, modify, uh, or process otherwise. Then we went to the construction where we actually performed the refactoring, uh, performed additional changes, and then deployed. And afterwards, Now we're gonna cover a bit of operation. So operation essentially is the part where you push the production or at least some kind of uh traffic loaded environment where you can monitor and evaluate your application across the span of time. So think about a case where we would, um, essentially evaluate our application, uh, for let's say 3 to 6 months. It's already running, and we want to kind of collect, uh, the, the, the bugs or the, the issues essentially that we have, um. So what I'll do right now, essentially just run this prompt in here. I'll tell you in a minute what it does. 4. So, um, we do have a list of bugs, uh, let me just minimize this so it's visible, uh, that we collected over time. You can see it's in Jason format, um, it's hard to read, it's hard to go through it, you know, let's make it easier on ourselves, so. What I'll do, I'll run this again actually, I'll just get a summary. And let me fan this out a bit. So this is the summary of the Jason file that we have. So it essentially gave us a total sum of the bugs. Uh, we can see there's 1 critical 17 high severity ones, etc. We can see individual components. So this is essentially from the decoupled perspective, uh, that we already, uh, talked about. We can see which file has, uh, which, which problems, uh, and we can go deeper. We can, we can see which one relates to validation, uh, serialization, etc. Um, now, we asked Giro if I scroll a bit higher, to actually analyze our application and create a, a risk heat map, essentially for us to understand which parts of the application need to be modified and how. Alright, we gave it some extra input just to follow the hexagonal architecture, essentially, and it's already working on it. Uh, it's creating a heat map, uh, now just for the sake of time because we have 3 minutes left, I will use the pre-created one, so let me just flip to that one. Uh, here. It actually is modifying this precreator one, so you can see it's flickering, uh, it just modified it, and let me just scroll all the way up. So, you can see it's been just, just updated, Kira just finished updating it, uh, if the the file wasn't existing, it'll create a new one, of course. But this is similar to that uh evaluation that audit file that we created before essentially we got a full report about what's happening based on uh based on our collective bugs or bug report. It can be locks. It can be anything. It doesn't have to be just in the simple form as, as we, as we saw, but we can see the highest pin points essentially. So this is quite graphical, uh, and Quiro can do this in even in a higher level depending on the configuration. There's steering that we can use to kind of fiddle with this and make it more granular, more, uh, customized, uh, and this kind of brings us back to the. Uh, previous approach where we right now imagine we are in operation, right? So you would imagine that's the end of it we'll just collect it and. That's it, right? Now, we're gonna do something with it. So we could do the same thing that we did in the inception phase where we took our audit, created a spec, and refractor application. So, in this case, we can take our heat map again. Have Cro analyze it even further if we want to create our spec and feed it back to the inception, so we're essentially making a full circle from the operations to inception, then apply through construction, essentially apply our findings, improve our architecture, improve our application, eliminate or minimize or mitigate those bugs that we're running into, and, uh, that essentially creates the full circle of the uh AI driven development. PPD. In take Yep, so just summarizing the things we covered today, uh, we looked at how you can basically use Gen AI throughout the development life cycle. So instead of approaching it just as writing tests for Servalli, but how do you redesign to simplify testing, and then we saw, so just so you know, all of the, including property-based tests and the application itself, we built it using hero, a lot of the spectrum and development. And then of course, towards the end, Thomas showed how you can continuously iterate, uh, based on historic data to improve your application. Uh, so some quick resources. The first one is a talk from 2023, but it's a really good breakout that talks specifically from a Python perspective, some of the best practices. Uh, the second one is from this year, but this is a recorded one. we now have really good integrations to, uh, debug functions live, uh, using VS code. And the last one is the completed version of our task API that follows all of the best practices that's published out to GitHub, and you should be able to access it there. Then really quick if you're looking for service and advent resources this is the place to go and that was really it. We thank you for spending time with us for spending your last day agreement with us.