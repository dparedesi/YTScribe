---
video_id: H02dc_AV_Vo
video_url: https://www.youtube.com/watch?v=H02dc_AV_Vo
is_generated: False
is_translatable: True
---

Good morning, everyone. Welcome to Reinvent 2025. What a way to start the reinvent talking about agentic AI and modernization using agentic AI. I'm Dina Alan Triana Saandham, head of modernization. AWS Proso from Australia and New Zealand. Joining me on stage is Ash Mullin, General Manager of Cloud Acceleration, Commonwealth Bank of Australia, and he's also the acting CTO of CBA India. Today, we'll take you behind the scenes of one of the most ambitious modernization program in the financial services industry on how agentic AI has helped us to transform modernization from a slow, manual process. To a scalable high velocity engine. Yes, we are going to talk about how CBA and AWS Proso have reimagined modernization using agentic AI. And it all, it all starts with a problem, um, a problem of scale. At CBA, um, we were working towards an end of support applications running on Windows 2012, and we needed to migrate at scale to cloud. And we wanted to do it the right way, not just lift and shift. We wanted a bunch of cloud goodness out of this. Um, as we went through this, um, we found it like very time-consuming, very labor intensive, and often very difficult. And the punchline of today's conversation is going to be that we build a multi-agent tool to help our engineers accelerate in how we did that modernization. Um, what we will cover today, so Dina will talk to you a bit about industry trend on modernization and best practices, and then I will follow in and explain how we apply that at CBA. And we will also have a live demo today, so we'll see how we go with that. Dina, over to you. Thank you. Let me ask you a question, like, how many of your workloads are still in on-prem? And Do you have any challenges in maintaining your legacy code? And what's your average enterprise transformation time? I think based upon the market research from Gartner, McKinsey, and ISG. About 70% of the workloads are still in on-prem with a code written about 20+ years ago. And an average transformation time takes from 1 to 2 years approximately. In reality, this is the reality we are operating in. The question is, how do we accelerate this without sacrificing quality and also from a security perspective. So enterprise transformation journey is broadly classified into three major types. One, infrastructure migration, where you would upgrade your VMs or you upgrade your SAP environments, or you want to upgrade your storage, whether it is a NAS or a SAN. Then the next one would be application modernization, where you want to. Modernize your uh monolithic application to a microservices architecture or you want to reduce your operational cost by moving more towards the managed services, or you want to reduce your licensing cost uh by moving towards more open source, say from Windows container to a Linux container to save the licensing cost. The last one is the risk remediation. With the increased cybersecurity threats in the new agentic AI world, how do I protect my software endpoints? How do I ensure that like my softwares are up to date and my CLIs are protected from the external threats? And with these 3 transformation journeys. There are a couple of transformation challenges which all enterprises face on a day in, day out basis. One, all this transformation are slow because of the limited resources or it is a very labor intensive process. 2, it is a limited scalability because the monolithic applications are resistant to a modular change. The last one is a scarcity of expertise. A person who has written the code has become a head of cloud or they would have left the organization. And in the new modern world, uh, in order to modernize an application. You need some specialized skills like you need a full stack developer who knows both infra modernization to app modernization. And how do we come to market quickly. With that in mind, let's understand what AWS hasn't. So this is a proven AWS migration framework where we do an assessment. So as part of assessment, we create a business case for a change. We treat our applications with different seminar patterns, and once the business case is approved, then we go ahead and uplift or upskill our team. Then we uplift our landing zone and we create patterns that is mapping to a particular application what we want to migrate or modernize. Then we pick up a sample workload and then we run through the process to ensure that the patterns are validated. Then we have multi-squad approach to. And to go ahead with the modernization at scale. There are challenges in this approach. Let's understand how we can leverage agentic AI to modernize this particular framework. But before we talk about agents. What we've done at CBA, we've worked very closely with our partners at AWS Proserve, and we've made the modernization framework relevant for CBE. So we've customized it to make it work for our organization. The big thing for us is we have our internal DevOps hosting platform, which is our platform engineering capability that we use to deploy applications to cloud at scale, and we will show you that in the demo a bit later. And so what we've done is for our VM that we needed to migrate to cloud, we use the migration service, MGN service, we image the VM and then we pipeline that through our DHP application. What DHP gave us out of the box is your deployment automation, your continuous delivery, your evergreen environment, your immutable environment. On top of that, we've also layered our migration factory that does your in-place upgrade, your automated patching, your AMI production, and then your continuous patching of that AMI to life cycle that over time, so that we can continue to remain evergreen. What DHP also gives us, from an immutability perspective, we also side push all our DHP agents onto all of our VMs. What that means is if someone logs on, And tries to do something on the VM, we detect that, we go, ah, that machine is tainted, we go and destroy and burn that machine when that user logged off. To make sure we keep the pristine environment of fully immutable environment in production. So you'd think, if you've built all of this pipeline, you've got your capability, you've got your platform, it's gonna be easy. We're just gonna be migrating application to cloud at scale. It wasn't. So, what we found out over the last year, we've assessed over 370 applications to migrate to cloud. I If you think about an application that was built about 10-15 years ago, like Dina was saying, knowledge is broken, people have left the organization, you've had like 1015 projects over the years that have built that applications, you've got like a big project that designed, wrote your solution document and all of that. And then there's a separate project that came in to add a feature, and then another project later that added another feature, so there was a lot of documentation archaeology, to go and find out all the various pieces of documentation across the organization, and then piece together a timeline and you'll find missing pieces. So it was like, very difficult to go and figure out like what an application does when you don't have the SME. Um, the technical debt was huge. Um, outdated packages, packages where you've lost your, uh, binaries from your artifact system, um, as you work through your rehost plan, you, you need to do your framework upgrades, um, all of that cost us a lot of time spent in just figuring out what the application does and making it ready before we could even start to migrate the application. So our baseline before we started our AI journey to assist us was about 10 applications a year. Where we ended up was about 20 to 30 applications per quarter. So we saw like a massive uptick in our velocity in order to work at scale. So, how did we go about doing this? So, we took a step back and thought, how do we put AI at the center of everything that we do? From your understanding your application, understanding your requirements, understanding your networking flow, your ingress and your egress points, understanding your code, producing your solution artifact, producing your cyber position on your application as you migrate, um, actually making the code change and transforming your code, which most people that I've spoken to in my organization think the best thing you can do with AI is just code conversion. There's a whole thing around it in order to deliver safely at scale in production. And then you talk about testing. You've built your application, you've deployed. Your application was built 1015 years ago. You'd be lucky if you have unit test. We absolutely did not have automated UI test. Um, and then how do you build that continuous feedback in the loop so that you can continue to maintain that application. Hm Do you know? Ash talked about putting AI in the center of modernization. Let's understand the evolution of agentic AI and the patterns of modernization. It's been fascinating to see how GIS evolved into agentic AR. In earlier days or in the first wave, GA was used to be a content generation tool where you give a prompt, it generates you a code or it generates an image or generates a content based upon the prompt. It is simple, but it's powerful and intelligent. Over the period of years we saw the emergence of AI agents where it leveraged a reasoning framework called RAC. It is able to reason. And break down your task into smaller steps and then able to perform the particular task. Then over the last year, we have started seeing AI agents evolved into a fully autonomous multi-agent system that can seamlessly execute a multi-step workflow in a dynamically adapting, changing environment. But nowadays the agent, uh, agent AI or the AI agents are capable of tackling a more complex real world use cases like autonomously diagnosing and remediating an IT incident, or it can generate, deploy, and test an application. With a single prompt. Let's understand the different patterns that are available in the Asian AI space and how it can be applied to modernization. At the foundation as the basic reasoning agent. It is the simplest form of all the agents where there is no external tools or no complex memory. It it reasons purely based upon the context. So example is like you have your policy document or you have your licensing document, so you want to understand when my version of the software needs to be upgraded, what are my do's and don'ts, so you should be able to give that to an agent. The agent should be able to interact and uh inform you about what you can do with that particular context or the particular prompt we are trying to understand. The Next one is the tool-based agent. It bridges the gap between uh thinking and doing so LLM doesn't just produce text, it decides to call what is the API I need to call or what is the lambda functions I need to call or if I need to make a database query and retrieve some information so agents will have that intelligent capability to call it. So if I put it from a modernization context, uh, it's all about, OK, I have a code modernization agent, but I have my compliance rules provided by my governance body. And I have my own APIs in terms of formatting, which I've been following it for the years. I want to continuously follow it to ensure that like my risk team or my cyber team approves it. So agent will automatically call those APIs, retrieve the results, and based upon it, the code gets generated aligning to your enterprise standards which you want to deliver. So now we have talked about. Agent that bridges the gap between thinking and doing. There is still a gap about how do I persist the contest. So it's about persistent and how do I do the learning. So there are two types of Memory augmented agents. One is a short term memory, another one is a long-term memory. About the short term memory, it's all about. I want to understand what are my previous 10 comments and what are my peer review comments that have been provided and based upon what is the action that has been taken. And so whenever I generate a new code, so I look into these contexts and then ensure I generate a code which will minimize my peer review comments so that it aligns with it. So that's like a short-term memory of the agent which we would use it. Then we are talking about the long term agents, long term memory. Uh, it's about bringing in intelligence across the sessions. Say, I've modernized my applications in the past and learning to a standard. So APIA standards which you want to follow it or an APRA standards from an Australian perspective. So how do I ensure that, OK, I implement the best practices. For a session that I'm doing it now after 6 months. So that's, that's where the long term. Memory that comes into the picture. So having talked about memory, having talked about tools. Then as we have talked about the agent multi-agent workflow, so this is where. A multi-agent workflow orchestration that comes into the picture. Like, once an agent needs to manage the complex multi-step task, that's where you move to this particular pattern. So, example is, like, we have a modernization workflow agent. So that automates code migration. So it is able to orchestrate between a discovery agent which discovers the repository and refactors your code, and then it tests the code that has been refactored and then able to generate a compliance doc about what are the changes I have done. And then it produces the results about the testing output that we want to provide. So it's like an orchestration of agent which you can orchestrate to do it. And this is what we're going to see as part of the demo which Ash is going to demonstrate. Let's look at the modernization workflow, how this gets implemented. I'm a business user, and I want to modernize my application. So what I would generally do is I would engage in modernization SME and the application SME. So they go on for weeks to assess the particular application to understand what is the next steps. Then they do a dependency core dependency mapping and they do a code decomposition. And they generate to be architecture and to be cyber document which they want to do. Then we will engage the testing SME to do the baseline testing, like whether it is a performance testing or whether it's a system testing, we will engage them to do it. Then we engage a modernization engineer to do a code refactoring, and once that is being done. Then we'll again do the testing to ensure that it maps out with regard to it, and this goes on for cycles. What if He remind you imagine this particular workflow with agents. So, where we still have a human experts, I'm going to augment them with agents so that the agents can do the heavy lifting the engineer is going to do. So it, it makes them even more powerful. And what makes this overall thing powerful is The AI agents are supported with deterministic engine, so the deterministic engine provides the facts. The AI engine provides intelligence to the facts so that it increases the accuracy or it increases the reliability and auditability of the change that is going to be done as part of the agents. So in this way we can avoid hallucination or any other things that we want to avoid as part of agent. Giving us some information which is not relevant to us. Having talked about it, like how CBA has leveraged the AWS suit. It all starts with Uh, AI compute that AWS provides in terms of our inferentia or the The AI compute which we have, then we have an AI intelligence layer where we are able to host our models with the agent core runtime. Then we leveraged The agentic AI framework like through AI, pedantic, or lunch chain or strands. To build our agents. Then for each of the capabilities which we want to provide, whether it is a code insights, which Ash was talking about, or the network analyzer or a cyberdoc generation, so we leverage this framework to build a specific accelerator catering to our need. Then we'll go ahead and customize it. With a CBA specific requirement talking about, OK, we will use agent to agent protocol or a moral context protocol. To ensure that like we are able to get the compliance details or get the enterprise specific things which we want to pull it out and then. Get the results what we want to do. Just hand over to Ash to talk about. So How did we do it? What we've done, we've created our own modernization portal at CBA that we've nicknamed Lumos. A little bit of a shine the light on legacy codebase, if you will, Harry Potter, I don't know. Um, um, essentially we've broken that into three key sections, um, analyze and design. Transform and test, and then deploy and operate. Throughout that, it gives us the full life cycle of how you migrate and modernize your application to cloud. If you remember back that diagram a few pages ago where we had AI at the center, and then the end to end life cycle around it. So, analyze and design. Let's get to the fun part. Introducing Lumos. Nope, wrong button. Introducing. Lu more, huh, OK. It's not a live demo if it doesn't work, hey? There we go. OK. Screen loaded. All right, so, this is Lumos. So we build that in-house and it goes through every step of that modernization journey. And it starts with understanding your application. For us, that was an interview with the application owner to understand, tell us a little bit about your application, tell us your intent, let us know a little bit how it works. We need to understand whether you've got a CICD pipeline, what database you use, etc. etc. That usually happens over a team's conversation, and we recorded that transcript. And so what we do here. We anchor everything against our CMDB application CI. We give it an application name, and then we upload the transcript. Um, While the AI does its thing, I'll tell you a little story. We had this application, we were migrating. The business owner of that application told us. No one uses that application, we've got like 20 users left, I want to turn it off. My migration engineer. Went in and said, but tell me about your connectivity, tell me about your database, tell me about your CISCD pipeline. So we had two people with completely different intent. One person wanted to turn it off, the other person's job wanted to migrate that application to cloud. And so what ended up happening was that application started its migration journey. It got deployed to the dev environment on cloud, and then the my uh head of engineering picked up on that and go, why are we migrating this application when the business wants to turn it off? And so what this tool does, it gives you a breakdown of that application into your requirements, breaking down into user story in your Gherkin format, so that that can go straight into your engineer's backlog. In this particular case, as an example, um, it's telling us this application is legacy, it doesn't have a CIC pipeline. So now your engineer knows it has to go and create a pipeline for it. The next step then was to understand the network connections within the application. And by the way, like we've called it sample lab, but that's a real application. We've renamed it because presentation, but that's a real application. we've migrated. So it's not fake data. Right. So this taps into our VMware environment, connects to the NSX API and pulls connectivity information into our tool. It tells us information about the application owner that it pulled from CMDB. It'll tell us about our infrastructure. For the savvy cyber people, we've anonymized the IP address, they're not real IPs. Um Gives us all the integration point, ingress and egress, port numbers, IP addresses, and if you're like me, a bit of a visual person. And draw a bit of air, and zoom in on that. Bit of a live diagram that shows you the flows between each nodes of your application. So now we're starting to build the knowledge of that application, right? Earlier Dina was talking about short-term memory and long-term memory. All of these are being stored in our long-term memory, and you'll see how we use all of that. Next step, let's analyze our code. Oh This will take time to load because I have access to a lot of repos, let's see. Boom boom, boom, boom. I've preloaded it. I thought that might happen. OK. I have access to a lot of repositories at CBS so it takes time to load. OK, so, um, I've picked a repository. 1st. 1st up, let's do a quick breakdown, very high level, like what's my main text stack that I'm dealing with. This is a Java application, a little bit of JavaScript on the front end. Let's configure that analysis. So, one of the things we've done, very mindful of AI cost, if we've done an analysis in the last 7 days, we'll stop you from doing another 17 days is good enough. After that you can try again. And so in this case, I'll retrieve a previous assessment. We've done that recently. OK. First up, now, this is what the AI looking at the code base thinks what this application does. This is one better than what we have in our CMDB system. So if you go in and you say like, our net banking application in our CMDB, it says, this is the CBE online banking application. It allows you to transact online. Great, but not helpful for an engineer. This goes into an actual breakdown of what the application does. And then we break it down further into actual functionality and features within the application. Very detailed What modules are in the application. What's the network connectivity we discovered from the application? Any API endpoints we're exposing. What are the primary tech stack Springboot application? Any client-server framework and any database libraries the application uses. We're slowly building knowledge about that application. We then need to perform a cloud readiness. Can we move that application to cloud? What are the various components we're gonna be looking at? Quite a bit. Any connectivity to network storage, because we'll need to move those to an S3 bucket, right? Any dependencies and integration on MQs and all these sort of things. What are some of the critical issues you might want to fix in your application ahead of migrating to cloud, so. Critical issues, passwords, security and compliance, any suggestion for future enhancements. Version upgrade. So we're running an old version of Java here, so making a recommendation to upgrade. Repository structure, so breakdown for all the files which ones have issues and which ones don't, that you'll need to fix. Config XML files that were ignored, we captured that as well. I probably should have preloaded this. Um, dependency matrix. And then we generate both a class diagram and a sequence diagram. So the class diagram gives you the first view of all the various components and how it's put together, and your sequence diagram shows your interaction between. Takes time to load. Uh, maybe I have that preloaded. So, sequence diagram and class diagram. All right. So far, we've been building a lot of information. Now let's use all of this to generate a solution document and a cybersecurity position as we migrate this application to cloud. So. Again, I'm going to anchor that against my Application CI. Now, what I can do here, I can provide it additional context that I haven't done before. So if you have like a knowledge system in-house, Confluence, for example, you can extract, save as PDF and upload that additional context here. Any current architecture that you know about your application, network analyzer, report analyzer, any other documents that you think will be helpful in this context. And then you hit generate all section. This whole page takes about 10 minutes to fully generate, so I've pre-generated it. In Here. OK. So We've generated the overview of that application. Build additional information, components, tags, server details, any reference materials, so all of these are what we uploaded in the file upload section. What's our target architecture, target environment, assumption and risk, integration flows, DNS service accounts. Now this is fully agentic. And let me show you what the agents are doing behind the scenes. So we have an orchestrator agents that will coordinate between two agents at play. There's a content writer agent that will write your solution document, and then there is a content reviewer agent that's going to review what the content writer has written. And then based on, and then it will score that what the content writer has produced. And it'll do up to 3 iterations and provide feedback back to your content writer agent for improvement. As an example here. In iteration one, the content writer wrote a whole bunch of things. The reviewer agent came in and said, Well, not good enough. I'm going to score you at 30%. Go try again. And it gave it feedback on what it needs to change. So it had another crack at it. It took the feedback in. It was reviewed, and again it was deemed not good enough. I was given additional feedback. And then it had a 3rd crack at it, and then this time, it scored much better. So that's the multi-agent system working in terms of generating your um solution document. You can also interact with it. Um, you can provide inline feedback. So if you know specific bits about that section, you can interact with the agent, take the feedback in, and dynamically update that. In the end Um, You can view your whole document in one piece. What we do with that is we export that as markdown, and we put it straight into the code base. So that when you grab your code, you've got all that information with you next to your code. We then go into exactly the same thing, but now instead of a solution angle, we have a cybersecurity angle against that application. So we assess it against dimension like your service details, solution overview, scope, information assets, and then we build a threat table. We build our nodes diagram, nodes table, our flow flow table, and then we build a security zone diagram. That will tell you what are the various pieces and where it sits within the CBS security zone model. What are some of the risks that we've identified? Any policy exemption we need to apply to migrate this application to cloud, and what are the non-negotiable that we have within the organization that we have to comply with. OK. Um, That was a prayer recording in case the demo fails, so I'm gonna skip that. Cool. How does it work behind the scenes? So, one of the key things that was super important for us, because we're building multiple accelerators, we wanted to land on a pattern that's extensible. Any engineer building one of these accelerators or building a new accelerators, we wanted them to have a familiar paradigm that we're working in that scales. And so, Our UI is hosted in the front end, built in XJS hosted in a container on ACS on Fargate. That makes a call to a an agent running an agent call, which is our orchestrator agent. And then behind the scenes, you have your multi-agent workflow that then interacts across each of those accelerator pieces, whether that's code analysis, whether that's cyber document generation, HLSA generation, etc. We're storing that in our short-term and long-term memory. We're also calling existing MCP servers that are available within CBE so that when we go and build solutions, it's relevant for our organization. Our MCP server and our AI agents uses OpenSearch serverless as our vector store and keeps all our knowledge base using AWS knowledge base ragged in an S3 bucket. Containers are hosted in ECR and we use the Bedrock model and Pedantic as our crew uh Pedantic as our orchestration framework. Right, Now we need to transform the code, and then we need to test if it works. Yep, thank you. All right. What we've built here is we create the concept of an application. In this particular case, it's only one repo, but you can envisage your standard three-tier application, web tier, app-tier database, etc. So you can bring multiple repos into this. So in this particular case, there's only one repository. Our agents will have a couple of tries at modernizing this application code. In the first instance, it's going to set a baseline of the application. And then it triggers um open rewrite to go and modernize, try to convert that code from older version of Java to a newer version of Java. And you'll see the quality was poor. It failed. So at the back of this attempt here. We had to build failure. It didn't work. And so, the orchestrator agent went back and said, hey, open rewrite failed, I need help. And what it did, it did another iteration. Let's go back here. And it brought in Queveer. It said, hey, open rewrite can't do its job properly. Que developer, you come in and fix that code for us. So in the 3rd iteration, Que developer comes in, starts making changes after Open Rewrite has done its job, until it's successful. So what's happening in the background here, if you look at the execution log, there's a few things that happens. We instantiate a container at one time, we pull the code base into that container. We download all of our build tooling, and we compile the code. We run any available unit test if it exists in the repository, and we set the baseline. And then when we run open rewrite and Q developer, we iterate until we get a successful build. And then at the back of that. We will raise a request. Now there's a human in the loop that will come in and review what the AI has produced. We want to absolutely make sure in this case, that there are humans in the loop that does those reviews. We also score the confidence of what we, what the agent has produced on multiple dimensions, number of lines of code change, number of filets, number of libraries that you've changed, etc. In one of our application. The bill was successful, but the confidence score was low because the AI agent went in and changed too many things. It started upgrading our libraries. And so the orchestrator agent came back and said, hey, you're touching too many stuff, it's not necessary. Try again and touch less things. We also had a bunch of SQL Server 2012 in our environment. That was also nearing end of support, and we wanted to do two things. Short-term, we wanted to move those to SQL 2019 or above, but really we wanted to move that to Cloud Native using um AWS Glue. And so, what we've done. We've built this little accelerator here. We take our SQL Server integration services job. We loaded it and we do a first view of the package structure. And then we asked Bedrock, go and analyze this package for us. Tell us if there's any issues, compatibility, and upgrade requirements. It's doing its thing. OK. So, what we're seeing here, here's some required compatibility changes to make that old package compatible for SQL 2019. Here are the things that need to be updated. Here's some deprecated features that we use that have been identified, any security consideration, etc. etc. and remediation step. In an older version of this accelerator, we also had a button that fixes all of this. Where we are now in our journey, we're no longer moving from SQL 2012 to 2019, we're going to cloud native. And so we've started our glue assessment. So that works with the AWS schema conversion tool. It does an assessment of your glue jobs. That takes about 10 minutes to run, so I will not bore you to it. And then we allow the engineer to upload his QCLI token, and there's the uh. The the code transformation. And so at the back of that, you can download your Glue package to upload. Um, Cool. Now, finally, we have to deploy that application to cloud. If you remember at the start, I was talking about DHP our DevOps hosting platform. Let me show you what that looks like when it comes to deploying to cloud. So I'm picking the organization domain where I want to deploy that within CBA and I'm gonna pick my dev environment. I'm gonna load an application that's already uh pre-deployed. Um, I've got many things running. Let's pick this guy. OK. So it's it's reading our metadata. So what CBA does, or what DHP does I should say, we take our infrastructure as code and we simplify that quite a bit, so that the engineer provides the parameters that are just enough for them to get to an outcome, rather than having to write hundreds of lines of terraform or cloud formation template. And so in this case, It's defining the environment, it's applying the CBA zone model, your web tier, your app tier, your internal control zone, your database tier. It's provision your ALB. It's, it allows you to visually configure the parameters that you can set, your tags, etc. Your ECS clusters, your lambda, and all of that. But it also allows you to then configure your CD so you could also deploy your artifacts. So in this case, I have 2 containers running on that ACS cluster. You know, pick one of them. That's our image name. It's the Lumos MCP server that we're deploying. And just enough parameters. There's built-in validation so that. Cybersecurity rules are enforced. We don't want to run our AC to be internet facing, so it stops you from doing that. Behind the scene You can see our infrastructure is good. So this is how we define our environment, how we define our load balancers, so very minimal minimal parameters that gives you your load balancer name, your back and pull, your certificate and your tags. Just enough to do that deployment. Push a button. A pull request gets generated. GitHub action workflows kick in, and your infrastructure is provisioned and your application is deployed onto cloud. How does this work? Surprise, surprise, exactly like I showed you before. That was the point. We wanted to build a repeatable, repeatable mechanism by which every engineer building a new accelerator can use and scale. We've brought in um QCLI and open rewrite in this particular accelerator, but the design remains the same. Oh, there is one more demo. You can do it. OK. Cool. Earlier, I was talking about codebase with no tests. And so we built this UI test generator. For our application, we put the internal URL of that application. So CBS sample app, but in this case, I'll pick a public website. I'm going to say Load the homepage and navigate to a new page. When the new page loads, I'm going to consider that as complete. What's happening now is we're using AI to look at the website and generate selenium scripts. If that was uh the website that we had code analyzed before, the code breakdown and the features get imported in, so that it builds an understanding of the application so it knows what to test. OK. It's generated the test case. It's got the test configuration. We're gonna run the test. Successfully loaded the homepage, and now it's trying to navigate to the banking link. I didn't say anything about banking in my, in my prompt. It's figured it out. Awkward silence. As the page loads, so we have used the selenium agents, as well as we have used computer use for any internal applications to start doing the testing simulation part of it. And as part of this, you'll see it's captured screenshots to attach as your test evidence of successful completion. So, for that sample application that I was talking about earlier. That's that application there where it logged in. Um, it started taking a trade, produced a report, and taking test evidence of that application successfully being tested. Do you want to talk about this one, you know? Yes, definitely. Thanks, Ash. This represents the overall architecture where everything comes together, so. When we started this CPA journey, it was, we started as a generative AI one, and then we leveraged deterministic engines like a static code analyzer or which generates. Sequence diagram and class diagram, which has been acting as a fax to the AI agents that it uses intelligence to understand, OK, what are the things we need to do starting from code analysis to the cloud readiness and we leveraged for the code transformation, we use still open rewrite, which is a deterministic part of it, and then we use Q, which is now being called as Quiro CLI which you'll be talking about, uh, where it would use the AI agents to. To expand and then show the transformation code of it. So, this is where we build accelerators, horses for courses. For example, for code analysis, we specifically created a code accelerator, then transformation accelerator, testing accelerator, and documentation accelerator with regard to it. So, one thing, one challenge which we faced is like how do we ensure that the reliability is brought into the picture. So that is where the deterministic engine and that's where the. evaluation agent came into the picture and the continuous compliance agents came into the picture to ensure that whatever code that has been generated or whatever document that has been generated, it's evaluated to ensure that it is 90 to 100% accurate. We talk more. Is the job done? Who built it, right? No, it's not. Um, we, we're working for what are the next phases of Lumos from continuous analysis to getting Rather than being human triggered to start your modernization, how do we make the agent start first? So, building the agent that will scan your code base, start the modernization, and then the agent then prompts the human and says, hey, I'm trying to migrate this application to cloud. I'm trying to modernize this. I don't know enough, I need more information, go get that for me, please. Um, making that fully automated, um, building cross repo analysis so that you can start to build a dependency map between multiple repos and work out if I'm touching this function in this repo, what else upstream and downstream is impacted as part of my modernization stream. How do we make the agent self-improving? How do we make it work for more than just 23 languages, from .NET, Java, Node JS, JavaScript, iOS, um, Android, etc. And then lastly, how do we build opinionated, orchestrated workflow that we call modernization pathways? And do you wanna show the mod OK, let's pull up the dev environment. Um, so modernization pathways in every organization. You'll have specific microservices system that will host all of your microservices. We have one in CBA that hosts our Netbank application, um. Is it loaded? It's loaded, cool. Taking all of those accelerators and build that into an end to end value chain to modernize an application. Full disclosure, this doesn't work just yet, it's a work in progress. That's why it was in the next step slide. So, this is our Netbank application to DHB. There's different technology stack within our Netbank environment. .NET, older version of .NET Core, .NET framework that we're migrating, some of them will need to do a .NET Core upgrade from old to new version, some of them will need to go from .NET framework to .NET Core. Some of those applications will be hosted on a Windows container versus a Linux container. So we've made this very specific. And what you can do then Is run through a wizard, where we've assembled all the specs of the accelerators, and then augmented that. So you'll see, for example, your business requirement, your meeting notes analysis, producing your design, um, specific transformation that is relevant for that code base, your containerisation agent that will go and containerize that application. Do the hosting, testing, cyber document, network flows, etc. So a guide a journey on how we do this modernization. We'll be back. Why do we do all of this? Ultimately, for us, at CBA we wanted to build solutions that are better, safer and faster for our customers. What that means for us is we don't want to run legacy applications in production. Because it comes with vulnerabilities, the Australian community places a lot of trust in CBE to run safely in production. That was super important in our modernization journey, so that we continue to keep the trust that our customers place in us, in our organization. Finally, um, maybe a little bit of stories from our engineers, um, some of the challenges they had as part of that modernization. Large ripper analysis was very difficult. Like we broke context windows so many times, it was like not even funny at one point. And so like, Um, the way we tackle that, we've taken our, uh, repository, we've broken that down into step functions that triggers a bunch of lambdas that breaks your big repo into smaller chunks and then reassembles it in the end. Like, um, I remember like before we started doing this, we tried it out a few times on the applications we're migrating, it was quite doing quite well. And then um Dean Bailey here, our friend from Bankwest. He goes, Hey, Ash, can I try this? Sure. Boom, 10 minutes later, hey, it fails, it doesn't work. Keep trying, keep testing, build new solution. Um, cohesive solution in versus individual accelerators, what I've shown you before. Like, we build all of those accelerators, when we have conversations with application owners, it just gravitates back to, I just want to do the code transform. That end to end piece was completely missing, which led us to build that modernization pathways, getting an outcome versus just upgrading your code. And then the last piece is talk a little bit about future value versus job done. So, how do we get those agents to continuously run in the background? As your engineer then takes that application, continues to work on it, pushes new commits into the repository, how do you make the documentation self-updating? So have agents that's continuously monitoring and conti continuously updating the documentation for you. And that is a wrap.