---
video_id: cqxUeDYpCao
video_url: https://www.youtube.com/watch?v=cqxUeDYpCao
is_generated: False
is_translatable: True
---

Welcome to today's session. My name's Stephen Leeddig. I'm a principal solutions architect with the Servius team um out of Australia and New Zealand. And um uh thanks for joining us this morning. Um hope you're all having a nice time at uh Reinvent Learning loads. Today what we're gonna really be focusing on is um how to design surplus um architectures, um but also with resiliency in mind. So I wanna get started with a bit of a quiz, just to warm everyone up. I know it's the first session of the day probably. What's more important? You're in a company, you're selling widgets. Is it more important to send 1 million $1 widgets, or is it more important to sell $11 million dollar widget? Now if you're, if, if you're, if you're looking at this from a, from a numbers perspective, maybe losing 1 or 2 or 3 or 4, you know, $1 widgets, that may not necessarily be, you know, a big deal. But you definitely don't wanna be the person that loses that $1 million dollar widget, do you? And this is just as sad to say, there's an actual a cost associated with resiliency. Right, in 2023, Gardiner actually, you know, put some numbers towards this. There's a significant financial cost associated with um message loss, with downtime that is associated through unreliable systems. And that cost is not just financial, that cost is also um impacts your brand and the confidence that your customers have in your products. And so what we're going to be talking today about is resiliency from a server perspective. Quick show of hands, who's building serverless apps today? Wow, a lot of you, OK, awesome. So we're gonna look at um some of the lessons that we've learned over the years, and particularly those that we've derived from the well architected uh serveless lens. I'm gonna show you some of the, um, you know, um tools that we have available for you to be able to go, to be able to build more resilient systems. We're gonna look at some of the serviceless specific, or service specific I should say. Um, resiliency strategies that are already built into the services that we have, as well as looking at some patterns around, um, some, uh, building surplus, uh, building resilient, um, applications. Now let's level set our definition around what resiliency means. So this is, uh, this is the definition that you'll find in many, in, in very in, in, in many places, and it's the ability really to resist or recover from disruption related to any kind of infrastructure, um, dependencies, any kind of misconfigurations or transient network issues. And if you're familiar with the um the shared res the shared responsibility model for security, you may be surprised to, um or not surprised to learn that we also have a shared responsibility for resilience. Now with this shared responsibility for resilience, it's very much the same as the security one, right? AWS is responsible for all of the infrastructure that um all of your services run on. You're responsible for really performing all of the necessary resiliency tasks around configuration management for things like EC2. You need to worry about, you know, multiple, uh, distributing those applications or those uh instances across multiple availability zones. So you get high high availability. You might be looking at self-healing strategies around auto-scaling and potentially looking at um highly available um um RDS instances or databases that um that can nicely recover from failure. And a lot of this, a lot of this stuff is things that you have to worry about. Now, from a services perspective, AWS is still responsible for all of the infrastructure that um that that runs the services and that are offered in the cloud. But here we're working with managements, managed, largely managed servers that do a specific task. And that changes the landscape a little bit from my perspective. It puts more emphasis on the actual applications and how you compose them, because you don't, because all of the resiliency stuff, um like the auto scaling, the high availability, multiple AZs, etc. all of that is already taken care of for you from um from an um from the managed service perspective. And that leaves you the ability, that gives you the focus to um to focus on the application architecture and the code that you're running on um on, on, on top of those services. And this is fundamentally to say that service is really about building solutions. It's about taking those managed services. And composing them in a way that helps drive business outcome. And it's important to really point out here that surplus infrastructure is actually an extension to your architectural design patterns, right, it helps you implement the design, the, the patterns that make your applications work. And that's what's fundamentally distinct from a, from a, from a serverist perspective. Now true resiliency still require and that brings us to fundamentally, the topic that we're talking about today. True resiliency really requires deliberate design choices at an application level, not just at the infrastructure level. Now, who's seen this before? Everyone's seen this, uh, this is our typical serverless architecture which, you know, we've been telling customers about, this is, you know, quintessentially a microservice. Right, and I feel that this is a bit of a naive example, but it's one we're gonna be working with today, just to sort of drive home some of the point. Because really the reality is that customers today are building far more complex systems. And um and when we look at all of these lines, um there's a lot of stuff that we, we kind of like take for granted, and I just wanna make, dive into this a little bit more. Now this is a um a bit of a, This is a toolbox, like I look at, I look at architectures, um, quite a lot from lots of different customers and lots of different industries. And this is the one thing that I really come back to. So if you don't take any, if you don't learn anything else today, just remember this. Because this is the, this is the one thing that's gonna help you understand, or actually like just raise questions about uh some of the assumptions that we make about the lines in our architecture. So This is the two box in the line architecture that uh Gregor Hope um popularized um a couple of years ago. So you may have seen this before, but it's a really good exercise to ask yourself, what assumptions can you make about how these two services or how these two components are actually integrating with or or interacting with each other. I'll give you a little moment to think about it. You might be asking yourself, What's the interaction model? Is this an asynchronous or a synchronous integration integration? Is this representative of a point to point or a pub-sub kind of architecture? It's pretty hard to tell, isn't it? What about, is B polling A? Are these two components distributed? Are they even in the same account? We, we don't even know. What about the data format? What about the conversational state? How is that being managed? What about error handling? What happens if, um, you know, if B is calling A and something goes wrong, right? Do, are there retries implemented? Do we have partial failures? Do, are the, do we have exponential backoff? Like what are the, what are the assumptions? Does B need to be item potent? We'll talk about item potency in a little bit. So Yeah there's a lot of questions that you could ask yourself about how your services are interacting, and when we're drawing lines, we can't always make assumptions, and we'll get into a little bit about uh that uh in a bit. Now if you're like me, um, one of the things that are constantly hammered or hammered home to you is that this whole idea of loose coupling, right, and making sure that your applications are loosely connected, however, um, you know, have good cohesion. Um, and when we dive into this a little bit more, we realize that, you know, there's lots of different facets to coupling, right? We've got, you know, technology, there's technology coupling, we've got spatial dependencies around where do, where do things live. Do we address them through IP addresses or DNS? How, how, what, what impact does that have on our applications? Um, we look at things like temporal coupling, right, this is probably one of the biggest things that you're going to be dealing with in a, in a, in a ser from a surplus perspective because of, you know, our ability to deal with, um, asynchronicity. Um But it's probably fair to say that there is no such thing as loose coupling. In fact it's, well it's not in the extreme, right, there's no loose or tight coupling, it's really just a, um, you know, a, a function of multiple dimensions where you're making trade-offs. And so you need to make make some decisions around um where you want to, um, you know, where you want to put the most effort in terms of making your applications more loosely coupled. So, going back to our um our example around what microservices looks like. Is this a resilient architecture? Now, when I'm looking at this, maybe some things you might wanna talk uh think about are, you know, does the client really need to actually, uh, you know, um, uh, wait for a response. Um, what kind of, um, you know, what happens if there's a surge in traffic in the API? How do I protect my backend? Um, lots of little questions that, um, that come up. What about my database, right? Is it, have I configured it correctly? What happens if there any, what happens if there's any failure in the database transactions that when I'm writing, um, my records to that? So You know kind of always leads you to these questions like, Am I, was that the $1 million.01 million dollars widget that I just lost, understanding what the impact of these questions are. So in order to, to get to this point we need to start thinking about what could possibly go wrong in servers, right? One of the first things that, you know, we, we as developers we need to know is like, what are the service limits and the and the and the quotas that are associated with that. Quite often you might, you know, think you're on the right path. You might be going, OK, I've got a lambda function implementing, uh, writing something to Dynamo DB but the um the function is timing out. Or there's some error rates uh that are that are unknown and maybe look like are coming from lambda, when in fact what you've really done is misconfigured the read-write capacity for Dynamo DB. The other thing you might wanna look at is um the other thing that causes errors um within servers are things like misconfiguration. Incorrectly defined IM permissions, probably one of the, one of the bigger um, one of the bigger challenges for, for, for many of us. Resource availability is potentially another one. And state management is also an area where we can potentially, um, you know, get into, into strife. Um, but one of the things that I kind of like to focus on is making sure that we've got appropriate design decisions. Sometimes our assumptions around how we connect the, the dots and how we put solutions together is really what the main problem is. Things like function chaining, for example, right, having one function connect to another, uh, call another function calling another functions. These are anti-patterns with within Servius that you should really try and avoid. Now Anyone who's familiar with the uh well architecture program knows that it's based around, you know, these 5 pillars. We've got 6 pillars now, sorry. And um from a resiliency perspective, um, you know, what we're really gonna be focusing on is the reliability. Now the well architected framework was, is, is, is a great tool to help you sort of better understand your operational um environment and what you're doing from a, from a governance perspective, um, with the idea of just focusing on particular applications. But sometimes we don't go deep enough, right, and that this was the case for Servius a few years ago, when we um introduced the servalus lens, which was specifically designed to try and dive in a little bit deeper into what is the actual uh what is actually happening at a, at a application level. And we've come up with a whole heap of best practices around um some of the more common, um, Uh, common pitfalls that customers might fall into, um, and, um, provided a whole heap of guidance, which includes things like this, around, um, you know, around, uh, observability, which is really what sort of kicked some of this stuff off, you know, giving you advice around, um, not just trying to, Measure the nuts and bolts of all the mechanics of how an application is fit fits together, but also understanding its behavior, and providing you with advice around tracing, distributed tracing, and structured logging and all of those things. And all of that was great, except you were forced to do all the work. And some and customers were asking us, how do I implement this consistently, have I done it correctly? And everyone was trying to reinvent the wheel. And this is where this led us to the um the development of power tools for AWS. Now if you're not familiar with this tool, um, I'll definitely check this out. Uh this is basically what we've done with power tools is we've taken all of the best practices guidance that's in the erless lens, part of the well architecture program, and we've codified it, so you don't have to reinvent the wheel. And now you've got best practices around observability, um, and things like batch processing, item potency handling, which I'm gonna mention in a little bit. And all of the other sort of common type of activities that you might be um doing as developers um and doing it in a, in a standard way with best practices applied. And doing that across multiple languages as well, Python, Java, TypeScript, and .NET supported. And now with the release of um lambda managed instances, you've also got full thread safety within those libraries, so you can use that in those for those applications as well. Let's have a look at some of the servius resiliency um uh features that we have. Now A good place to start would be at the at the at the beginning, right? Let's look at the front door and let's see what we can do at an API level to see, help us, um, you know, um, protect and make our um all application a little bit more resilient. And that really boils down to regulating, um, inbound uh request rates. And so you can use the throttling capabilities that API uh API gateway comes in to help you build more resilient applications. Now there's multiple levels of throttling that you can apply at an API gateway level. Um, you can obviously set the account wide um levels, um, but you can also, and you probably should, as a matter of, uh, as a good practice, um, you know, really um really align, um, appropriate throttling levels um to each of the stages that you have in your environment, like dev, staging, production. But you can also limit, because not every function, not every API um endpoint um has the same requirements, you may want to also apply those at a method level. Now usage plans are also a really, really useful tool if you want to be able to um uh segment based on usage. So for example, you can create usage plans. The simplest example that I can, you know, give you is something like a t-shirt size. You can have customers that are extra large, large or small, and then you can assign particular throttling rates to those usage plans, and then align and then give those customers or give those users API keys that then, that, that then determine what kind of throttling rates that are applied to the requests that they are making. And that is a super useful tool, particularly in um if you're building SAS applications or you're um or you're exposing public APIs. OK Now of course, throttling is not the only thing that helps you build resilient architectures. Um, one of the, one of the probably lesser known features of API gateway is the ability to have method level caching, um, for your, for your get requests. And this is really simply taking off back pressure, making so you don't have to make unnecessary calls to your back end for frequent requests for the same information. Now if you need more fine-grained, um, if you need fine-grained um control over this, then uh uh obviously another um strategy you can do from a caching perspective is to put uh uh is use um AWS Cloudfront distribution. And making sure that the requests that are being sent are actually valid requests. Again, this is a little bit more effort that you would put into defining your APIs, but it just ensures that you don't get any bad actors sending you payloads that you're not expecting. And you've got other, other features like making sure that you clearly define your integration timeouts and that you obviously secure your API through some kind of um you know, mechanism um that uh that that's relevant for your application. Right. We're gonna get into some lambda stuff, but before we do, I just wanted to level set our understanding around what synchronous and asynchronous means. So these, these are very common terms, sometimes misunderstood. For us today, um, it basically means synch synchronous, um, requests are basically where a caller waits for an immediate response before continuing. Right? It is basically expecting the client, uh, the client is basically expecting the service to finish its work before responding. OK, now some of the um I guess drawbacks to this approach is that you have all of the latency, all of the failures and all of the back pressure fundamentally become the responsibility of the caller. Now with asynchronous uh invocations, what we're really talking about is, uh, is, is managing sort of the temporal nature of our interactions, where the caller doesn't have to wait for um the, the service to finish, it just continues on and does what it needs to do. And at some point in time, the um the the the service may may or may not provide a response. And this is, um, this is a much safer way of building surplus applications, but it also um brings in some complications around making sure that you've got explicit correlation between um for requests, so any asynchronous um uh interacting components or services knows it can be traced back to the original call. And you might look at things like eventual consistency as a, as a mechanism for, you know, defining how your how your users interact with your system, as well as, um, you know, looking at providing separate error paths as well, for potential failure. So when we're looking at lambda and lambda's resiliency implementation, there's a number of different ways that we interact with that service, depending on the type of integration that you're performing. Now from a synchronous perspective, this is something we call the push model. This is basically uh you invoking lender directly, right? And the client as we were just defined is waiting for a response from that function. And it will also return immediate success or response depending on the outcome of that invocation. And we've also got asynchronous invocation, which is really driven by services like events that are being um published by um S3. You may have SMS notifications, cloud watch events, uh, EventBridge is also considered to be an asynchronous invocation type. And it's important to understand the difference between this and the next one, but fundamentally, um, here, AWS is, uh, lambda is actually fundamentally curing all the requests, allowing the allowing the caller to continue on, um, with the, um, and and allowing the caller to continue on processing and doing what it needs to do, um, and the, um, invocations are in. Uh, asynchronousity invoked, which means that there is no response back to the original, um, in uh the caller. Now from an event source mapping perspective, this is where you have integrations with SQS. You have um uh Dynamo DB streams, there's also kinesis streams. This is where we are Lambda fundamentally polls the event sources and um invokes the functions on your behalf. So it's important to understand that these 3 diff different invocation types exist, um, because they will they will determine how you can potentially architecture solution. Now from a um synchronous invocation perspective, here lambda does not automatically retry any of the any potential errors, right? The functions are fundamentally stateless. And there is no durability in these requests either, so if anything goes wrong, the um the the your ability to um rehydrate any kind of state or get back to where the error occurred um is, is very, is very small, in fact it it doesn't even exist. So this is probably one of those cases where you might want to go, hang on a second. Could this be a point in time where that $1 million.01 million dollars dollar widget got lost? I'd be thinking about whether or not the synchronous implementation actually um is, is, is a good idea. Now from an asynchronous perspective, we have a, a slightly different approach. You can notice here that I'm not using API gateway, I'm using EventBridge to invoke my function. And what's happening here under the hood is that EventBridge isn't actually invoking your function directly. It's, it's sending that request directly to an internal AWSQ. And then we have a polling process within lambda that takes that message and invokes your function. So this is that part of that surus reliability, like this is all happening under the hood without you actually needing to do anything. And um and this gives you the ability now to be much more resilient in the in the sense that, you can either use the existing retry mechanisms that exist within this asynchronous invocation. Because lambda gives you the ability to uh retry up to 2 times. Or you can then configure that lander function to use either a dead letter Q or a destination. My advice would be to look at a dead letter cues, and if you have any questions about destinations, I'm happy to have that conversation afterwards. But it's also, uh, so this is a great, but this is not obvious when you draw that line that we talked about before, because all you're doing is drawing a line between event bridge and uh and and and lambda without realizing what's happening under the hood. Now you know, now this is something that you can reason about when you're thinking about your next architecture. Now from a polling perspective, You uh we do something totally different, so this is actually the same thing as you just saw, except now you own the queue. And this is where um you have the ability to define the parameters around that queue, and, you know, how many retries um you you might want to configure for that, um, and various other properties that um will help you perform, um you know, the, the necessary throughput capabilities that your function um needs. But what's happening under the hood is this, again, you don't have to create polars. In fact, you can't even configure lambda today to actually, you know, do this yourself. You need, you don't have to create polars, we create polars for you. And those polars are basically pulling those messages off the queue and then invoking your functions in batches. And so, um there are some trade-offs with this approach, in fact, this is actually, although it kind of looks like an asynchronous invocation, it's actually considered to be synchronous in the sense that um the the polars are actually invoking your function directly. Even though you're using a cue, which might lead you to some confusion. And so in this sense that you're you're making some trade-offs where there is no um lambda destination support, but you do have far greater control around the inter the integration with lambda. And you can then use those um use that configuration to process many more messages uh in in batches. Now, one of the things with this integration. Is um that basically with SQS at least, the event source mapping um basically treats, um, your each request as a batch, so you have a batch size that you define. And that batch is um either succeeds or fails in its entirety. Now, and that, that continues and if your if your function fails for one of the rec one of the records in that batch, the entire batch gets put back onto SQS. And this continues up until that point where you've been able to process all the messages successfully, or you've um reached the maximum retry attempts or you have a re or your retention period for those messages fails. But there is a way that you can look at partial failures within SQS as well. So this is where you would con um configure um the partial batch response configuration, which is allows you to effectively where you need to then look at all, create an adjacent object. That allows you to, um, which is called the batch item failures, um, which is effectively a list of all of the message IDs that you would then return, as a, as as a response back to SQS and only those messages that failed would be back on the queue while keeping everything or while processing everything else. Now one of the things you have to, this is something that you would have to do though, right, you would have to create this list and you would have to send that response back. But what if there was a better way? This is where you might wanna start unleashing the power of AOS power tools and using the um the batch batch processor utility that does all that stuff for you. So in this instance, effectively um the when you cre when you define or when you use um a particular batch processor type and lambda, the lambda power tools actually supports multiple different types of event sources like kinesis and the Dynamo DB streams. So you can use the same library irrespective of what event source you're actually using. And then basically um all it's doing is taking the um item identifier and building that list for you, so you don't have to. And that's a huge saving in terms of like the coding and testing that you need to do um and it's a simple configuration. This leads us onto the next sort of topic around um item potency. So item potency fundamentally means that no matter how many times you process the same message, the end result is the same. But why is this important? Well, basically in a loosely coupled, um, distributed architecture, a producer and a consumer can basically become temporarily disconnected. Um, um, for, you know, for, for A number of reasons, but but but mostly it's related to network related issues. Think about sort of like the partition um uh the partition element of the, the cap theorem. So when a customer might success, a customer might um successfully process a request, but the acknowledgement never gets back to uh the producer of that event or that request. And then the producer is likely to want to retry that implement uh that request, sending the same payload back to the um back to that um back to that function. And so this can put you into an inconsistent state. Again, this is where power tools can help. Power tools has an item potency utility that, Mitigates the need for you to actually build a solution yourself around how to treat or how to deal with um potential retries and help you with deduplication. So in this case, basically, um you would get a client invoking a lambda function. At this point in time, the utility takes, um, uh, will, will process, will make a record of that request. It will process and keep a and manage the state for the response and if anything went wrong, And then a subsequent retry happened, it would simply either return that um request that it had stored before and giving, giving the, giving the um the request at the same um the same response. Uh, or it would then continue processing on, uh, processing that message, uh, in a safe way. This is really useful for any kind of uh event source really that you might have, such as um you know, asynchronous invocations or event source mappings that we've just been talking about. And works great in conjunction with the things like the batch uh processing utility. Alright, now let's look at Eventbridge. EventBridge also has a built-in, um, a built-in resiliency capability. In fact, by default, um, EventBridge will actually retry sending messages to its intended target for up to 24 hours or 188, 185 times, and it does this, um, with exponential backoff and some genetic, uh, uh, rationalization in there as well. Now 24 hours might seem like a long, is is is a pretty long time, but it's there, like you don't even have to do anything and already you're getting um uh automatic retra built into the service. But you might want to actually control this um a little bit more fine-grained, so that we can, you can now configure your targets to be able to set um the number of retries that you want to have in your applications or how old the messages should get before um before those are sent to a um uh an AWS sorry, an Amazon SQ SQSQ that you configure. So you've got far more control over how this failure, um, these failure scenarios happen. And the nice thing about the way that um those errors are then um stored within your SGSQ is that you're actually, once you've looked, you you actually are given error codes and error messages that help you troubleshoot those um those potential integration issues that you have. And it could be, you know, someone may have deleted a resource, someone may have, you may have misconfigured your IM permissions. This is sometimes hard to troubleshoot, um, and now you have the ability, and, and this gives you the ability to do that more quickly. Alright, So those are just some of the um ways that AWS helps you implement resiliency out of the box. And I've shown you how um I've shown you how you can leverage some of the power tools capabilities uh to help you build more resilient implementations. Let's have a look at some patterns for resilience that that you can then you apply yourself. Alright, where are we at now? So far we've looked at. Uh, a couple of different approaches, we've taken our kind of naive sort of example and we've started looking at ways that we can turn this into an asynchronous implementation. And we've got a queue now, we've got we've got some durability in the sense that even if we do fail to process that $1 million widget, we know that it's going to end up in the dead letter queue. That request never got lost, and so you're not going to have a bad day in the office. Now, one of the things that API gateway, I just wanted to call out um is also probably something that's lesser known um with um with, Uh, with customers is the ability for API gateway to actually invoke other services directly. So this is really useful where you may want to be able to um bypass the need to have a computational uh uh a comp service like lambda in in as a middleman. So the ones that we've been looking at so far is API gateway to um to lambda, and that's very typical and that's where most people start. But when you want, when you're starting to build asynchronous architectures, you might wanna consider using an API gateway to SQS integration directly. So that your original HTTP request ends up in a queue, allowing you to um then process those messages um when um in a in a in a in a in a time frame that that suits you or your application best. Or you might be wanting to invoke an SQS uh directly, wanna be invoking an uh a step function, right? Without actually having to invoke a function beforehand to then call um the SDK in order to invoke that. So this is a really, really useful tool um to be able to bypass some of that logic necessary and effectively what we're doing here is we're taking, um, the, the raw API calls. And then literally just mapping the requests from API gateway into uh in this example, a send message API call that then creates the record on ESQSQ. Now another popular approach um and I'm, I am gonna, um, I'm gonna do this with an air of caution is, um, API gateway to Dynamo DB. Right, you might, or all you might wanna be doing is, um, you know, manipulating some data or, or, or processing some data, sorry, reading some data out of a database, which in this case, it might actually be a really useful tool, but I would, I, I would, I'm cautioning you here to only don't, not to do this with things that manipulate state. First of all, this is not a very testable implementation for your API gateway, particularly if you're, if you're um manipulating state in the database. OK. Um, the other thing is, is that anything that manipulates state tends to be, um, you know, if things go wrong, whose problem does it become? It becomes the client's problem. And what can they do about that? Not a hell of a lot, right, so they typically maybe want to retry those requests, persist that, it's just a bad experience for everyone. Right, now if you want to read information, that's, that's non-mutating and I think that's perfectly OK. But if you, if you're uh looking at this uh as an as a as an approach or as a means of minimizing some of the compute logic that you need, I would just caution you to reconsider that for right operations, um, perfectly OK for Reed. OK, While we're on the subject of reading and writing, let's have a look at this from a transactional perspective. Now what we've got here. So we've got API gateway, we've got our um API gateway integration with SQS we've got our dead letter queue, we're calling our function and we're persisting some information. Is this still a resilient architecture? We're getting there, I think. So from the first, the, the first transaction is this, right, if a client. If a client request fails, right, the error from an API perspective is, so what I mean by that is is that the message actually never ends up in the queue. Now this is a, this is a API sorry, this is an AWS managed integration, so the likelihood of this happening is relatively small. But in case it does, right, the good thing is is that you haven't actually manipulated any of the systems, uh, the system is still in a consistent state. You haven't done anything with the back end. The client maybe isn't is is a little bit unhappy because they got an error, but at least you've got some stability in your um in your environment. Now if it succeeds, then um the message will end up in the queue and the client gets a 200 response saying thank you very much and you're basically telling them we'll take it from here. Alright, which is actually a really good place to be because anything that goes wrong potentially after this is not the cust is not the client's problem, it's your problem. And you can then build mechanisms to deal with those problems. So our next transaction is actually writing and taking that record and actually putting it into um into the Dynamo DB table. Again, because we've got we've got a cube, we've got a temporary buffer, so any um any potential um uh failures that occur around um writing that information to the database. We'll only just put that request back on to the queue. So you've got some safeguards there around potential failure. And if, and if the um, you know, if the problems persist, then you know that message will end up in the dead letter queue and you're um you've you've basically rescued your um your $1 million dollar widget from uh from from losing that sale. Now You're not gonna just be writing stuff to a database. In a, in an event driven architecture, um, you might be wanting to actually notify someone that that $1 million was uh uh record was just written to the database. So in this case, you might be looking at maybe publishing an event as well. Now, this is potentially a bit of a problem. What do you think's wrong with this picture? What we've potentially introduced here is a distributed transaction. OK, where the function is actually doing two things. Now if anything goes wrong either by either by uh within your within uh within your function, that's, um, you know, maybe it hasn't quite written the the the the record to the database, um, you've published the event letting everyone know that something occurred that actually didn't occur. Or you successfully wrote something to the database but you failed to notify other people that it actually happened. Again, it leaves the sys other systems or other interacting components in an inconsistent state. And so what we wanna do is try and rearchitect this a little bit to make this a little, to to introduce a third transaction that guarantees us um some or put some guarantees around the eventual delivery of those of that event. So rather than having the lambda function actually doing both things, what we want to do is we want to um enable Dynamo DB streams, which basically is a time-ordered series, uh sequence of items that are modified in your database. So if you create a new record, or if you modify an existing one or delete something, there will be a fundamentally a change data capture event uh in on that stream pertaining to that um particular record. Now what you can do here is ideally you can do a couple of different things. You could have another lambda function read off that stream and then process that. But we're now in a server space, so we're gonna look at event bridge pipes which allows you to um select um sources from multiple from from uh uh a range of different services as a as a as something that will invoke uh something that you can use to actually transform the message. Now why would you want to transform the message? Well, like I said before, this is a change data capture event, this is, this record represents a a a service level change in your database, but that service level change is not very useful for other customers or other, other services, right, it has no business representation, and nor has it got the context necessary to tell those uh those recipients of the um of um that that change actually has some kind of meaning. And so what we wanna do is we use event bridge pipes to actually transform this information into something useful. So here we can use some of the filtering capabilities for our pipes um to um only process those events that we're interested in processing. And then using the target transformer capability to convert that Dynamo DB record into an EventBridge event that has some kind of business representation. And this allows us then to share that information across um across our um environments and allow us to um uh and allow those recipients to understand the meaning of what just occurred. Now what I just showed you. And what EventBridge, I mean the names kind of give it away, but effectively what, what this implementation, uh, what the EventBridge pipes implementation is, is a representation of the pipes and filters pattern that um was popularized by Gregor Hopper in his integration, um, enterprise integration patterns book. And this is basically just an architectural style that allows you to divide a bigger problem into um into smaller independent, um, um, steps. Um, without, you know, and in this case, the recipient really only needs, sorry, the, the, the, the message sender really only needs to know around where, what transport, uh, or what pipe to send messages to. It doesn't know or care what comes after that. So this is a nice way to start breaking down problems rather than thinking more sort of like everything has to happen in one sequence. I think that's a really important concept, especially when you start dealing with distributed architectures. Now another implementation of our pipes and filters, arguably is using step functions as a means of building resilient workflows. Now step functions is, uh, you know, if you're familiar with step functions, it allows you to build um state machines that uh you know follow a sequence of steps that allows you to, um, you know, fulfill over a long period of time, um, you know, some business outcome. And with that comes um a whole range of capabilities that will help you build more resilient workflows and implementations for your applications. Such as error handling, it's got built-in error handling that allows it to actually look at your um, your, your, your function level um uh uh and custom level exceptions that you emit from your function, and it allows you to then use that to either for for graceful um uh, Uh, uh, degradation or allows you to choose alternative processing flows. It also has automatic retries and um exponential backoff capabilities. These are things that you don't have to program, you don't have to come up with the algorithms, they're already built into the into the into the service, so you can then gracefully or you know um manage how you're interacting with other services or components in your in in your workflows. It also allows you to fine tune those interactions through um timeouts and heartbeats. To make sure that those services are available, um, and also manage um potential um potential issues around time out. So this, this still leaves us with a bit of a, bit of a challenge around how do we deal with distributed transactions. OK, where we actually need to complete, complete a series of steps that need to be, need to um need to work or um as, as a single unit. Right? Now we don't have distributed transaction coordinators in the cloud, in fact I don't think anyone has, and it's really hard to, so what we need to do is we need to simulate what this looks like. And so this approach. It's actually um you know, effectively it's a rand, it's it's, it was established by um um Garcia and Molina um uh way back in 1987, this idea of a saga, or um what's commonly known as a saga today, but they refer to as a long-lived transaction. And what this what what this allows us to do. Is effectively build compensating transactions for um for our workflows, so we can get the entire workflow back to a consistent state. And this is a hugely powerful mechanism for, you know, building resiliency and durability into your architectures and making sure that you really understand what are the consequences of failure within your business processes, and how do you roll those any changes that you may have made to um a system um through various different transactions in your workflow, how do you roll them back? And so this is where you build these long um long-lived transaction implementations, these saga implementations that have specific steps to do that. Um, and And so that's a, that's a very powerful mechanism there. Alright. We're getting close to the end. What about um what about validating resiliency? We've looked at some patterns, um, we've, we, we know that our services have capabilities that may not be obviously, uh, may not be obvious, but we know that they exist now and we can be reason more explicitly about that. But how do we validate that? Now, of course there's lots of different ways that you can test your applications, um, but one of the things that uh become really popular and now also for servers is the ability to do things like chaos engineering. And the chaos engineering capabilities here. Um, fundamentally they're supported by our service, um, for, um, through, um. Through the um hang on a second, there you go, the fault injection service. But ultimately what we want to try and do is look at ways that we can manipulate state or the behavior of our underlying system. By introducing things like timeouts and cold starts and working, uh, manipulating the network so it actually creates errors or, uh, fallbacks on our in, in our environment, we might wanna mess around with some of the co-dependencies that our functions have, and also to determine what is the outcome of all of this. How can we, if this happens in production, um, how can we, um, get back to a safe space. And this is where the um the fault injection service can help you um you know test some of that. And so it's got built-in capabilities that help you um fundamentally um manipulate latency. um you can do HTTP response manipulation and you can also force particular failures to see how the overall your system behaves in a uh when when things aren't going down the happy path. Alright, so the key takeaways for today's session really are. Um, the, the main things that we sort of started with. Now this is a, this is a, a, a quote from Tim Bray that who used to work for AWS who used to be, um, and, um, is now an independent technology and author, but he's popularized, he's got a popular saying saying if your applications um are large scale and distributed and don't have any messaging services, there's probably a bug. This is also a really good, uh, a really good key indicator that you may not be building resilient architectures. If you're looking at an serverless architecture and there aren't any messaging services in there, you might wanna consider or be more explicit about how you're gonna deal with failure and where you're gonna get durability and reliability from if things go wrong. And so going back to our original question, what's what's more important? Every message, every widget is important. Right, it's, we can't make choices around specific types of requests that come through. We can prioritize and we can, you know, say, we can potentially reason around um um you know, the potential importance of where we put things, um but ultimately every message is important. And what makes, and it's not just about what connects the services that's important, but the actual services themselves, right? And again, take note of this two lines, uh two boxes in a line architecture. Think about this the next time you're looking at a, at a, at an application architecture and go, hang on a second, what, what assumptions am I drawing, um, when I'm looking at a line being connected from one service to another and ask yourself those questions. Now we've got some other sessions that you can check out that are along the same theme here. Um, we've got um uh we've got a uh a session, um I think it's a breakout session around um understanding how to implement circuit breaker, sagas and strangler patterns. Um, we've also got the Serless developer Experience workshop, which will give you actual hands-on experience, um, you know, dealing with some of the, um, some of the patterns, um, as well as a lot of event-driven approaches to, um, building resilient architectures. Uh, that's this afternoon here at the MGM. And we've also got integration patterns for multi-tenant systems, um, that's also worth checking out, and that's tomorrow. Um, finally, here are some resources around, um, application integration, uh, and servalus resources. Um, all of this, uh, and more, um, is available on servalless land, um, and we've got a huge amount of patents available for you to consume on that website as well. And with that, I say thank you very much. And enjoy the rest of your day.