---
video_id: _gnj3ornavE
video_url: https://www.youtube.com/watch?v=_gnj3ornavE
is_generated: False
is_translatable: True
---

Good morning. Uh For many of you, this might be your first reinvent or maybe first day at reinvent or maybe the first session. Uh, I'm not sure whether this is first silent session or not. Everybody's comfortable with the Hudson? You know how to control volume. Great. Thank you and welcome again. Uh, thank you for joining us today for the next 60 minutes as we unpack the agent AKI story. And here is the thing, you are hearing the story from humans and not from the agent. Maybe at least for now, we don't know what will change. Imagine a world where AI doesn't need to wait for instruction. Uh, it can take initiative. Uh, it anticipates your need, uh, make decisions, and act on your behalf. Agentic AI is bringing in that shift. Agent EKI is transforming an organization like yours from automation to autonomy. Today we will show you how Agent TKI Vision is delivering. Meaningful business outcomes and and and real business results that matters the most. My name is Sujal Shah. I'm senior customer solution manager with AWS along with me, Hamid Shahid. He's director of AI and data science at Site Improve, and Pradeep Shriran, uh, senior solution architect at AWS. In this session we will focus on 3 most critical elements that That actually unlocks the power of generative AI. First and most important is move beyond the hype. Today AI landscape is filled with noise and hype. We will explore how to cut through that noise and identify the AI initiatives that aligns with your unique business goals and strategic differentiation. This is important rather than chasing every new technology trend or or focus on shiny use cases. Once you identify the most important AI initiative, the next important thing is journey, how you will take those initiatives from idea to production at scale. Hamid will walk us through Site Improve journey, and he will show us how Site Improve has converted their bold ideas into production ready solution that is already delivering results to their customer along working alongside with AWS. And third, Pradeep will walk us through. How AWS Gen AI Services is providing everything you need to be successful with agentic AI from foundational capabilities to production ready workload, it enables you to build and manage enterprise grade agentic solutions at scale. Let's start with something fundamental. What is the most important thing for your business today? Most likely your answer will point towards the core business principles, for example, customer focus, operation efficiency, product innovation, strategic differentiation, personalized experience, and so on, right? None of these fundamental business principles change, but what is changing and changing rapidly is how you are delivering on those principles. Generative AI and agentic AI is fundamentally transforming. How value is created and delivered. It is shifting from reactive workflows. To a proactive outcome-driven system. Agentic AI systems empower businesses to innovate faster, uh, respond in real time, and stay ahead of changing market requirements, market dynamics, and customer needs, and that puts you towards, towards a profitable growth. The level of empowerment and enablement it creates for businesses is definitely sounds powerful. Uh, but the main question is what do we really mean by agentic AI? And how it is different from traditional automation system that many of us have been using for decades to, to deliver on our core business principles. Traditional automation relies on predefined rules. Generative AI responds to prompts. Agent EKI goes further. It plans, reason, uh, take initiatives, take initiatives independently to drive outcomes. Agent TKI systems are designed with intent. They think and focus in terms of objectives and not just executing instructions. Agentic AI acts autonomously, it sets goals, dynamically adapts strategies for the given context. It has been interesting to watch how G AI is evolving into agentic AI. Gen AI generate contents. You provide a prompt. It produces an output, either a paragraph, image, or line of code. Definitely it is very powerful, but it is wait for your next instruction, GI agent. Took this further by breaking down, breaking down tasks into smaller steps. Agent TKI system has evolved into fully autonomous multi-agent system. Asian daycare systems are capable of executing complex multi-step workflow by dynamically adapting strategy to changing environment. Most importantly, it can do all these things without any human or external intervention. Definitely agentic AI sounds transformative and, and, and it definitely offers real potential. Understanding what is agentic AI is maybe a relatively easy task. But the real challenge and the most important thing is how, how organizations are adopting it, how do they operationalize agentic AI with scale and with a clear path to measure value, with a clear path to outcomes, with a clear path. Ultimately towards monetization. Agentic AI adoption is definitely accelerating fast. Gartner predicts that within just 1 year, 40% of enterprise applications will embed task-specific agentic AI. Fast forward in 3 years. Autonomous AI will handle up to 15% of daily work decisions. And in the next 10 years by 2035. Uh Agentic AI-driven software is expected to capture approximately 35% of enterprise application revenue, and that is roughly surpassing $450 billion US dollars. Now think about this When we say 40% of enterprise applications will have agentic AI specific embedmentment, that is not a pilot matrix that talks about the real adoption. And when we say 30% of enterprise revenue will be driven by agentic AI solution, that is not just an add-on, that is product itself, and that's how the industry is adopting Agent AI. There are 2 powerful ways where businesses are leveraging Agengen TKI and Gen AI. These two powerful ways are operational improvement and product innovation. Operational improvement. This is basically streamlining how you build, market, sell, and scale your products, right? Agentic AI reduces friction and, and it enables efficiency gain. That free up capacity. Product innovation, product innovation is, is transforming what you do, or, or what you do to offer your better differentiation, enhance customer experience, and greater adaptability. ANT unlocks new capabilities and competitive advantage, and that brings in new or net new revenue stream. Together this creates a very powerful business dynamics. Cost saving from efficiency plus new revenue stream from innovation. This is how you sustain growth in AI first era. In the early adopter and most, most of the things what we are seeing today, businesses are focusing on operational improvement, but the real power comes from product innovation, and, and we need to balance both operational improvements and product innovation. The balance between this renewed efficiency and new growth creates investment opportunities. And, and you know that there is no shortage of AI investment ideas, right? You do any hackathon, or any brainstorming session, you will get that laundry list of ideas. So the real challenge is not to identify the ideas and opportunity. The real challenge is identifying which opportunity truly moves the needle. Truly moves the needle for your business with the high impact use cases that advance your unique business goals that advance your Business outcomes and not just spreading investment thin on low value experiments. So the question becomes, how do you prioritize your AI investment? We know that fundamentals don't change, and one of those fundamentals is working backwards. Working backwards is Amazon's mechanism for prioritizing investments by starting with customer outcomes and then decide which one is truly worth building and funding. Not all opportunities are equal. Start with start with business outcomes and metrics. And then you work backward. Evaluate each opportunity. Each opportunity's value potential against effort required to deliver those. Let's break this down. Into a framework you start with. You start with quick win, high impact opportunities with low effort, and this will help you build momentum and achieve early result. In parallel, you focus on your most strategic bets and initiatives that holds the highest potential. Avoid investing resource heavy, effort intensive projects that can only deliver moderate results. As your AI and data maturity grows, keep an eye on emerging opportunities and only prioritize those. That can, that can help you build scale. And after all, measure all these opportunities against three important metrics trust, speed, and adoption. Trust. Ask this question Do these opportunities help you, help you build and sustain customer trust? Speed, can you move fast at scale and not just solving today's status quo? You are not just solving short term goals, but whatever, whatever you decide to do, can you, can you do it at scale? And adoption, as you know, adoption is the most important matrix. Without adoption, trust and speed. They don't matter, right? Adoption means whatever capabilities, uh, product innovations you are doing, are your customers are using it, not only just using it, but are they delivering any value? And once they start using and start delivering value, do you see a path towards monetization? So the structured approach keeps AI initiative focused on what truly moves the needle. You can maximize your Maximize your business with managing risk. Naturally, There are, there are concerns about converting upfront investment into growth potential. Here agentic AI acts as a force multiplier. It enables organization to unlock. Unlock powerful growth trajectories with meaningful investment. Think, think of agentic AI as your strategic fulcrum. With the right foundation. Investment in infrastructure and talent. You gain leverage With a clear growth vision, new revenue stream, differentiated products, sustainable competitive edge, you set the direction. But the real impact comes from speed, how quickly you can convert this opportunity and stay ahead of market dynamics and customer needs. Yes, it is a practical reality that agent TKR requires off investment. And maybe some short term trade-offs, but with clear strategy and disciplined execution, agentic AI becomes force multiplier. It takes you beyond incremental gain, unlocking transformative growth from every major investment. So invest in the right foundation, unlock transformative. Invest in the right foundation, set the clear transformative vision. Prioritize quick win that can help you build momentum and achieve early result. That will put you on a path towards turning investment into realized value and then slowly you can accelerate on your on your go to market. This leads towards lean investment, an exceptional result, and this is how winning enterprises are adopting agent EKI. Now we know how to stay focused, avoid distractions. Avoid distractions from every new technology trend. Avoid distractions from. Uh, new announcements happen every day, right? Framing the opportunity that aligns with your unique business goal and force multiplier effect. And that's not the end. The real game is how to take these opportunities through the implementation funnel and land solutions on your customer hands. Let me invite Hamid to walk us through the site improve journey, transforming the bold ideas into production grade solutions, leveraging AWS capability. Hamied, thank you. And. Hello everyone, this is Hamed, and I'm very excited that I'm representing Sight Improof today and share our AI journey from generative AI to agentic AI. A short introduction, Sar Improve is a software as a service platform and a company that helps organizations ensure their digital presence is accessible, compliant, and highly performing. We combine digital accessibility, analytics, SEO, and content strategy into one integrated platform called Site Improve AI. And we typically serve medium to large enterprises, government agencies, educational institutions, financial services, healthcare, and any organization with substantial digital footprint who cares about the quality and performance of their digital content. But what do we really mean by a unified platform for agentic content intelligence? So we have AI agents and solutions in 4 different areas as I mentioned, accessibility, analytics, SEO, and content. Our accessibility platform and agents bake continuous compliance into your workflows and ensure accessibility across web, mobile, social, documents, applications by design and not by accident. Site Improve agents assist every designer, developer, editor to make sure content is accessible by default. We help identify and fix accessibility issues, maintain site quality, and meet critical standards. Ensuring a seamless digital experience for all users. When it comes to analytics agents, we want to stop sifting through raw dashboards and focus on what matters most. These agents translate traffic and page insight into clear next step prioritize and tied to your business goals. In other words, advanced analytics, as well as the user behavior turns into actionable insights. And our purpose here is by continuously improving the performance with our AI-driven intelligence, your business stays ahead of the competition and maximizes ROI. So we have SEO agents. The purpose is to outshine the competition in search engines, LLMs, and AI search. Each agent supports SEO leads and content owner to basically by offering real-time guidance and optimization through the content creation process. To boost visibility and discoverability, this is very important. We harness power AI driven tools to ensure that your content not only aligns with your business goals but also is discoverable across different search engines. And last, content agents helps planners, writers, and strategists through their every phase of content ideation, planning, briefing, and optimization. And basically tries to close the gaps between your content, but this is not the end. We also need orchestration agents to work across the site improved AI platform and agnostically integrate with your ecosystem, such as CMS. So Agentic AI adoption, and why this is very important to have a successful agentic AI adoption. We all know That AI is super easy when it comes to PC. You see high school kids are playing with these tools, plugging them, but it's very hard when it comes to production and at scale. There are two key challenges here, we believe. One is driving non-deterministic AI models to deliver more deterministic and consistent output, and second is to and the second challenge is when people start avoiding basically the error rates. We should not underestimate the error rates we are talking about when we are thinking about scaling. As an example, you may think, oh, my system is working 99% correct and perfect, but then we are scaling to like 100 million or more requests per month. That translates into 1 million requests submitted to AI getting error, and that is the big challenge. So now I should say the way that we see this AI adoption is not a big bang moment. It's more like a trust curve, and at Sight Improve we do this in 3 steps. In the first step, human is in the loop, and that means AI and human work side by side. AI suggests, explains, and validates. But the human is the final decision maker, and it approves. This is where we prove the value of our agentic solutions with higher quality, faster delivery, fewer errors, and measurable impact. The next step is to go from human in the loop to human on the loop. Here we still have the human in the, in the, we still have the human in the picture, but not as an operator, it's more like as a governor. So it overlooks or over the AI basically does most of the job and the human is only governed the human only governs the overall process. The next step is human out of the loop, and this is when we are fully, we are fully automating and we are fully automating our process and when the trust is there, but generally speaking, we don't have to, we don't, we should not move forward to each next step before answering two main questions. Do users trust the system and has the system proven its value? Because if you skip the trust, adoption stalls, and if you skip the value, trust never forms, and this is the biggest challenge that we have. So this is the three layer agentic AI adoption that we are trying to do at Sight Improve. But what agentic AI really means at sight improve, let's get into this. It's very interesting if you all search for agentic AI definition. You probably realize there is no single definition for agentic AI, and that's OK, but everybody agrees that intelligent agents are built on core abilities like plan, retrieve, act, remember, understand, create, specialize, recall, and customize, and these are the core building blocks that turn an agent from a chat interface into a reliable digital teammate. An agent is more like a special assistant. Designed for a specific task. Operate on those ability to deliver consistent results. But there is a key point here. Having an agent or even having multiple agents doesn't automatically mean that you have an agentic AI system. Agentic AI AI truly begins. One system orchestrates multiple agents and technology towards the same unified goal, and they work together in autonomous loops like a plan, act, observe, adjust, plan, act, observe, adjust. It chooses the right sequence of tasks, tools, models, and makes decisions without human. Intraction as a constant communication with the agent, with a constant communication with the agent. So in the past 12 months, We started from bringing generative AI and now we are at the agentic AI stage, and now I want to share part of our journey and some use cases with you. At first with generative AI it's like having a brilliant assistant who can create things for you, but only when you ask. This is very similar to a product that we have which is called brand consistency. When the user comes and asks, match, for example, this content on this page with the tone of voice of my brand. The prompt is fixed, the ask is fixed, the the generative AI model generates the output, or when we have some accessibility rule checker goes and checks based on some prompt and generates some fix. Next is the AI agents. In addition to generation, now your assistant can make suggestions and even take actions. Agents are capable of taking initiative, using tools, integrating with other parts of your system, and get the work done. This is what we are currently doing and we have different systems such as conversational accessibility remediation or conversational analytics. And the last phase is basically the agentic AI phase where your assistant becomes more like a collaborative team that can plan, coordinate, and adopt autonomously. Here, multiple agents communicate and work together to solve problems dynamically. You don't have to micromanage them. They understand the goal. They figure things out along the way, and that is the agentic AI. And this is our mission to get these agents across all of our platforms that they can talk to each other, communicate with each other, and get things done. Before continuing this, I want to take a moment to mention that Side Improve Sight Improve was recognized as a leader in the Forester wave for digital accessibility platforms, and this was a big achievement. The Forester Wave measures digital accessibility vendors across the strengths of their product offering as well as the strength of their strategy, and we advance into leader category in both axes. And Basically we could achieve the highest possible scoring 13 criteria, including our innovation and vision. This is the quote from Forrester. Site Improve is unique from other leaders in this market because it provides accessibility as part of a broader unified platform that includes SEO analytics, content strategy. So our journey started from content intelligence platform and now we are going to agentic unified platform. But before I jump into the actual use cases and technical architecture, I want to say why accessibility engine is very important for us if you don't, if you don't know. So it's about ensuring accessibility is about ensuring digital content works for everyone. Including 1.3 billion people with disabilities, visually impaired users navigating with screen readers, as well as users with motor impairments relying on voice commands. Let's assume a page title is welcome or a link says click here. The experience is broken here and fixing these in production is not as easy as you think because some of some of our customers have like 50,000 websites, sorry, web pages that needs to be crawled, needs to be checked, and there are some timelines. So this is not necessarily an AI challenge per se. It's more about an orchestration and infrastructure challenge. Today, I'm going to present 3 of our interesting use cases, and the reason I selected these 3 use cases is each of them has its own pattern, and we show how this pattern is implemented in production. The first use case is async batch processing use case, asynchronous batch processing. When we try to automate potential issue reviews for accessibility. here we need to process millions of pages, up to 100 million pages per month, for accessibility rule check. And obviously if you can do this in a costly cost with an acceptable cost and meaningful scale, this saves a lot of time and effort from your manual work. Let's assume that you can submit everything in a batch and you get the response in 24 hours. You can submit up to 100 batches, each batch 100,000 requests, and in one day you can achieve 100, 10 million requests done. And I show you the architecture that we implemented for this use case. The second use case is conversational AI remediation for accessibility issues. This use case is a synchronous conversational use case when the user comes and tries to fix a specific issue, and if the user is not satisfied with the response, then it starts a conversation with the agent and gets the work done. The third use case is ay again. But now it's not batch because not necessarily everything can work in batch. Some of our tasks have higher priority, and we need to deal with high priority async requests to be able to get the response as soon as possible. This use case focuses on contextual image analysis, which means we want to understand the context context of that image is relevant to the image like an alt text or the heading on the top, the caption on the bottom. So before I continue to talk, I want to show you a one minute video, and then you get a better understanding of what I'm talking about, for example, for this conversational AI remediate. So the user, this is an interactive experience. The user can go to the portal and see the potential issues, click on one of them to understand what's going on. The AI tries to understand the context, and here it says no title for this page. It reads the page yet makes sense. Make sense of the page and try to come up with suggestions alongside with some explanation and best practices that you need to do something and then if you're not happy with the results and if you have any further questions, you can ask the agent, OK, can you tell me a bit more about this situation? And the agent starts again going through the history, the context, and get the response back and send it back to the user. Now, Things are getting more interesting. Before we start developing and implementing any system, it's very important to understand the business requirements, and this is even more important for AI agents and AI and agentic AI systems. Sometimes some nonfunctional requirements can change things significantly. I will review some of these business requirements with you so you have an idea of what's going on and why we decided to go with AWS and Nova model, and I will show the architecture next, so. We are operating in both the US and the EU, so we need enterprise grade security, governance, and compliance. We need multi-region support for compliance, scalability, and resilience. We need multi-modal, so we should be able to have agentic solutions as well as generative AI solutions. It's very important for us to have access to a variety of leading malls including cloud, Lama, Nova, and so on. Support for both batch and interactive workloads and a good support for deep and seamless integration, but the most important thing is flexible pricing. We need to make sure we can optimize the cost when we are running at a scale. When we looked into the possible solutions, Bedrock was a great pick because Bedrock runs in the same environment and the same VPC as the rest of your architecture. You have multi-region support automatically. Bedrock supports both agentic and generative. You have a variety of support for both batch and interactive, deep and seamless integration. And when we were looking into bedrock solutions and Nova models, at some point we realized, oh, some of the models, for example, Nova Micro at that time when we checked. We had 75% cost reduction compared to the leading malls at that time for some of the use cases that we have. So it's very important to select the right model for the right problem and not take the best model possible for a very tiny problem. This is Sight Improved AI accelerator. We designed this in a way to satisfy all of our business requirements, as well as all the possible use cases that we are aware of. The top layer is sight improved applications, and the bottom layer outside of the box is our AI services. It's the AI services that we use like an Amazon Sage Maker, Amazon Bedrock, like in foundation malls, Bedrock agents, and even Agent Core. What is inside, it's very high level, and I go to the next slide which shows shows you the details. But then here we have 3 main big components. One is our batch manager. One is our business logic manager, which includes all the prompts and parameters and all the details about the actual problem. And the last layer, which is shared between almost everyone is AI service adapters. So our Based on the case, we want to talk, for example, to a specific third party model so we will have an adapter there to talk to that model or have an adapter for Converse API on Bedrock. So this is the high level one, but this is, I want to have it. Closer look and what do I really mean by async batch processing and I'm not gonna talk about too much too many details here, but it gives you an idea of what's going on like we have an async batch processing I already told you and examples are, for example, we want to know if the HTML. Page title is descriptive of the page or not, or the heading of a section is descriptive or not. By batch processing, I'm talking about millions of pages per day, up to 100 batches of 100 cases turns to 10 million pages per day, and we will have input and output queues. Starting from the top left, that blue, purple circle ALB, the request is coming to the system. We have a lambda grabbing the request, putting it in a queue right away without any further processing. There is another lambda after we have enough requests in the queue, grabs them, put them, and group them in a necessary bucket, make them ready for batch processing bedrock. Every few hours or a few minutes, depending on your load, we have another lambda submitting these batches to Bedrock. We know the turnaround time is 24 hours, so we watch for an event to get triggered, and then we have two lambda to grab the results. Each one lambda grabs the whole batch response and the other lambda processes each of them, clean them, do the post processing, and submit the results back to another queue. With this architecture we could achieve what we what I just explained like in millions of requests. And then think about what I mentioned at the beginning. I said I picked 3 use cases for you to say, to show you some patterns, and this is what I mean. This is our AI accelerator architecture that's getting extended on the very right side. That section is exactly what I just showed you, and that's a batch processor unit. In the middle part, whenever we have some sort of an interactive conversation with some agents, we have some lambdas to talk to the bedrock agents around the conversation, having conversation, having revision for the conversation, having remediation. And on the very left you see the 3rd use case for contextual image analysis. I, I take some time here again, so the request is coming to this lambda. The image gets pushed. The URL of the image gets pushed to that queue. When we have enough requests, the Bedrock requester grabs the request, submit to Bedrock. It's async, but it's not batch, so it submits back or sends back the results to the output queue, and the user is using it. And now think about this. I have 3 patterns established in my infrastructure. So if I have like 5 more use cases in the batch, I follow the same pattern. And if I have 2 more use cases high priority, I follow this pattern. So we are trying to build this architecture to support all possible use cases that we have in our business. I want to double click on a very recent development and highlight that we are now extending our agentic framework with Agent Core for conversational analytics and why this is important. I already mentioned we are thinking about having a unified platform and the user comes to our platform and can ask questions from different aspects of the system. Think about this. User comes and says, what are the issues with my top visited pages? We need to go to the analytics of the site, of the website, understand, or when I say we, the agent, needs to go and understand what are the top visited pages and what are those pages, basically. Then the agent calls multiple agents to say, OK, what's wrong with the accessibility part? What's wrong with the SEO part? What's wrong here? How can the user can improve. So with this agent core architecture, we try to connect the existing agents into more. Scalable solution. We have real agentic AI. You'll see our agent called RT is running on a strand agents. It has some internal local tools for query interpretation, for formatting for another formatting for JSON and UX. We can call other agents through Agent Core Gateway. As an example, we call our own server through the gateway. Each of these tools can have bedrock models for their internal tasks, and they both, they all have access to short-term memory and long-term memory. Short-term memory is usually for session management. Long-term memory is usually for actor management. You want to keep the preference of the actor. And in the next part of this presentation, Per will go and explain the architecture in depth. Now I want to start concluding my part, and I want to share our strategic milestone. And the reason I'm sharing this, I believe this is, this is common for many other companies. It's very important and when you want to walk into this journey, you first prove values of your AI agents. Lay the foundation with core use cases in each pillar that you have. Try to have reactive agents which are the which are the simpler agents. The next step is then to automate and amplify, expand agents to take on more repetitive, complex, and high volume tasks. This is the time that you enter proactive agents and multi-agent collaboration. And usually the long term goals for many companies at this point is to have more or less an autonomous orchestrated intelligence to be able to create agents, data, and workflows across the entire platform. During this journey we learned a lot of lessons in collaboration with AWS and I want to share some of these. You may get benefit. Some of them are pretty technical, some of them are more kind of an administrative, but it's very important that you use cross region inference if you can. It helps you to reduce latency, improves resiliency, scales throughput, optimizes cost, and basically you're serving your workload from multiple regions. The other challenge that many people have is effective, prompt engineering and optimization. Bedrock has an optimization tool that you can give your prompt and the model that you want to optimize for, and it optimizes the prompt for you, and you should not underestimate this. For example, when you are working with one specific model, maybe the best is to have XML tags to structure your prompts. But when you are using for Nova, Nova, for example, prefers markdown language to structure your prompt. If you do not optimize your prompt for this specific model, then things get messy when it comes to this scale. The next is understanding regional throughput and quota variations. This is super important that you have an understanding of your quota ahead of time, especially if you are scaling. Don't assume if you submit 200 million requests in a month, it gets processed. So you need to understand, for example, what is your quota for mall inference per minute, what is your quota per region, what is your quota for the number of jobs that you can submit, and then estimate and get help from AW's team to get what you need. Not when you are going to production way ahead of time. The next lesson learned for us was how we can mitigate failed responses by using different models. You can have your architecture to dynamically select the models, especially if you are using one family of models like Nova. If the response gets failed from micro, you can bump to light. From light you can go to pro. This can be done in real time if you have multiple malls or it can be done when you do your prompt engineering and your analysis, so you can have multiple molls running at the same time for different use cases or have a real-time escalation to the next mode. Mitigating model hallucinations and model hallucinations and enforcing restrict output formatting. This is very important. At the end of the day, you want to show the AI output to the user, so you really need to have probably a JSON output or some structured format that your UI at some point can process and show it to the user. There are several techniques that you can force your agent to come up with a. Very specific format and this is an ongoing challenge I believe for everyone when I'm talking to different leaders. Like you can find guidelines like in prefilling is one technique when in the assistant part of the prompt you start the beginning of the output that you expect and that forced the agent or the generative AI model to continue on that. There are multiple techniques that you can learn, but you should not underestimate this one and the last. Part is handling contextual information batch processing. When we have batch processing, we are dealing with millions and millions of requests. By contextual information, I mean metadata, for example, the digest ID for each of the requests. So there is, if you want to stay stateless and keep everything within your Within the call to the agent or and get the response back, you need to somehow pass part of this contextual information to the agent and get it back unless the architecture that you're using supports for these kinds of metadata, which is usually not the case, at least to this point to the best of my knowledge. What is the challenging part here is the hallucination. I'll give you a very concrete example. We realized we were using a very specific hashing or encoding algorithm to hash my output, to hash my digest ID, and whenever the agent gets to the semicolon. It was thinking oh it's the end of the line and dropping the rest of the ID and then we had a lot of hash mismatch. So you really need to understand when you are working at the scale, how you deal with your contextual information that you expect the agent to send it back to you. Now I want to pass it to Pradeep for the next part and his presentation on Agent Gore. Hello all, my name is Pradeep Shridran. I'm a senior solutions architect at AWS. At AWS we have a simple but ambitious vision to provide the best place to build and scale the most useful AI agents. Not just a prototype, but actual production agent and not just any agent. The most useful agent that actual that delivers actual business value. This was, this is what drove our work that we did with Site Improve, and this is what is driving everything that I'm about to show you today. But before we go there, let's levels set on what an agent is. In a nutshell, an AI agent is a system where an LLM. Receives goals, maintains context, creates and uses tools, learns from observations, right? It's a what we call is a continuous loop. Observe Reason learn and act. It's very simple in concept, but when executed well, can be very, very powerful. But executing this well is not without its challenges, and here is why. Because agentic AA systems are autonomous in nature, even implementing a simple agentic AA system takes a lot of moving parts, right? You need to have a way to do orchestration. You need to have a way to do tool execution, and you need to have a way to manage state, right? Luckily, We do have frameworks today that helps with a lot of this uplifting, right? We have trans agent from Amazon, Lang Chain Land Graph Crew AI, and so on and so forth. But this open source framework makes it easy because they provide developer abstraction for some of the pre-built codes, but still, Many organizations struggle with getting from this state to actual production. System. In fact, we have a name for this. We call this the prototype to production chasm. You can see on the left you have an, you have a POC that is working, right? You have excitement and potential, but you want to eventually get to where that, that very POOC is running in production, delivering actual business value. But still, this is not without complexities, right? You have to manage memory systems because LLMs are stateless. You have to manage security. You have to manage governance, and all of this needs to scale as you as you as your enterprise scales. Gartner predicts that 40% of enterprise agency KI projects will be canceled by 2027 due to soaring costs, unclear business value, and security concerns. This is the reason why we built Amazon Bedrock agent coal. Bedrock Agent Core is our answer to the production chasm problem, right? You, it's a fully managed service that handles everything you need for running production production agents. You get all the leading models that's available on Amazon Bedrock. You also get access to orchestrators, the open source orchestrator I just talked about Landgraf Langwin, so on and so forth. But what makes Agent Core a real production grade application is what you really see under the. The way we line, right, you have secure runtime management. Memory systems that manages both short-term memory and long-term memory. As Samit mentioned, short-term memory is primarily for session management, long-term is for actor management, secure authentication and token storage, secure tool communication. And out of the box observability so that you can understand what your agents are doing. So this is not just about running one single agent, but this is about running a production grade agent that can scale to millions of users, recover gracefully from failures, and adapt to your needs as you grow. I'm going to pull back a little bit and show how this services work together, and this particular slide can be a little bit technical being the weeds, and I know this is a silent session, so if you guys have any questions, I'm happy to answer some of the questions outside once the session is done. But I want to highlight a few major components of the agent core, right? You have the agent run time, the agent gateway. Agent memory, agent identity, and agent observability. Agent runtime is a secure server like runtime, purpose built for deploying dynamic agents. It's a remarkable runtime. It supports multimodal agents, scales to thousands from 0 to thousands of concurrent users in seconds. And it also provides complete dev isolation, meaning you, it has isolated computer environments, so data leaks are not a problem. Agent court gateway. Converts your existing APIs, lambda functions, and MCP servers into tools that agents can use. It provides a unified interface and has pre-built IAM authentication. Agent core memory provides you a name space. Encrypted namespace-based storage. That supports both long term and short term memory, and it's also quite powerful. It supports multiple extraction methodologies like semantic search and namespace-based extraction. Agent core identity solves enterprise authentication challenges, right? It provides a secure delegated access control mechanism for your agents for third party applications like GitHub, Salesforce, so on and so forth. It also reduces development time because it can, you can reuse existing identity providers like Microsoft Entra ID, Octta, or Cognito. It also uses a secure vault storage. Uh, that reduces constant authentication, fatigue, and improves, uh, provides a frictionless user experience, right? Now, Agent core observability. It's a comprehensive end to end solution for observability. You can use it to understand your agent action. Look at. Reasoning and input and output locks. You can also build pre-built dashboard. You can use pre-built dashboards and uh because agent core observability is hotel compatible, it integrates with a number of hotel compatible application monitoring tools. But Agent Core is just part of our larger portfolio, right? Amazon provides everything you need to build and scale your agents. I'm going to start from the top. At the top we have applications. You have Amazon QuickSeed. This is a purpose, purpose-built business application for enterprises. Quiro, our newly launched ID for software development. AWS Transform is again an agent tool that is used for modernizing legacy workloads. Amazon Connect with the agents for customer support, and we also provide through our AWS marketplace, a number of agents and agent applications provided by our trusted partners. In the middle you see our development platform. You have Amazon Bedrock. Now Bedrock provides access to a number of leading models from providers like Anthropic, Meta, Mistral, and we have our own models, Amazon Nova, which are essentially models designed with Agen Agentic A architecture in mind. We also offer Nova Act, which is essentially a model that can take action within a browser context. We also provide agent core. I just talked about it. It's essentially the best way to run and scale your agents in production. Uh, we offer strands agent, which is the open source Python-based SDK for developing your agents as well. From an infrastructure front. Uh, Amazon provides infrastructure that can scale from prototype to production. Uh, the primary infrastructure is offered to Amazon Sage maker AI, uh, and we have our own chips as well, ranium and influentia. Now, apart from this, we also support open standards like Model contest protocol by anthropic and A2A agent to agent by Google. Now, all of those could be uh a little bit complex, so I'm gonna leave you with a simplified version of it. There are really 5 key takeaways, right? We have pre-built agents so that you don't have to start from scratch. We have tools like code interpreter and browser. We have models like anthropic and meta models that we offer. We have infrastructure to scale from prototype to production. And we also provide expertise, which is essentially AWS folks like me working in concert with your team to achieve your business goals, which brings me to the next slide, which is essentially how we are going to engage with you. Now at AWS we don't just provide technology, we also provide you a complete support ecosystem. Our one team model puts customer you at the center of it. Uh, we have 4 distinct teams. We typically start with AWB's account team. So this is a team that actually works with you day in and day out, understanding your business goals and vision. Uh, I am actually the account team SA for Site Improve. Uh, and account team typically advocates for you within the larger AW the core system to get you more, uh, more access. Then we have the Gen AI Innovation Center. So this is a team that provides deep ML and AI expertise. This is a team that actually worked with Site Improve, uh, to help with some of the uh batch processing challenges and prompt optimization. Then we have product teams where we give customers access to Engineers working on Bedrock, Nova, and Agent core platforms. This is so that you'll have early access to features as well as provide roadmap influence and build and support team, support your endeavors and provide technical account management as well. And we engage with customers in a variety of ways. I've highlighted 4 here office hours, which typically means recurrent calls that we handle quick technical discussion. Deep engagements. These are engagements like what GAC did with Sight Improve to kind of accelerate. Uh, experience-based acceleration. This is another interesting mechanism by which we work with the customer on an over the shoulder basis, uh, particularly when they are beginning something to accelerate outcome for them. And technical enablement is when uh we provide training and certification for you and your teams, right? So if you're ready to start with your journey, Uh, I'll leave you with a framework to begin with. Uh So for any agentic AI innovation you have to start with business outcomes in the mind, right? Like define first your use cases and start from there. The reason why Sitempro was so successful because they picked up accessibility, which is, which was essentially making the most sense for them, right? Once you have your use case defined, then you can pick your models and agentic AI framework and remember with Bedrock, you're not locked in. You can change your mind any time. And once you have that information, you can then have to consider security, compliance, and responsibility up front. Uh, it's not an afterthought. So, once you have this, create a proof of value, not just a proof of concept. And uh from there you can go in uh to development, deploy and scale. And remember this is an iterative process. You can learn and accelerate. I'll leave you with 4 key takeaways. Prioritize investment by starting with business outcome. We don't have to chase every trend. Like I said, to focus on accessibility and compliance. Agent transformed business beyond technology. Yes, there are early challenges, but the growth potential is enormous. Third, bridge the prototype to production chasm through systematic value delivery. Use tools like Agent Core when possible to achieve this. And 4th, build for enterprise success with AW's one team collaboration. You don't have to do this alone. With that Thank you very much. We are excited to help you build and scale the most useful AI agents. Thank you.