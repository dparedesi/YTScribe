---
video_id: vcYT6INWpa4
video_url: https://www.youtube.com/watch?v=vcYT6INWpa4
is_generated: False
is_translatable: True
---

Hello, everyone. Good afternoon. Thanks for joining us today. Welcome to SAS 305, using GEI to profile, scale, and optimize your multi-tenant SAS architecture. So in the session today, we're going to look at how you can fast-track your SAS maturity using Gen AI to introduce best of breed SAS attributes into your SAS product. Now, this is a level 300 session. Uh, what that means is we're not going to be diving into demos and, and code, but we will be doing a lot of architecture patterns and showing you real-world use cases that you can take away and use yourself. My name is Dave Roberts, and with my colleague Damakari Ananda, we're both solutions architects here at AWS and we've been working with SAS for the last couple of years and in the recent year or two with Jenni I as well. So what are we going to look at today? Well, first, I'm going to look at what makes best of breedsA and how you can use GEI to incorporate some of that into your control plan. Then Damica is gonna look at cost analysis. So things like cost per tenant, pricing, forecasting. Then I'm going to look into customer behavior, the insights that we can get out of those. Customer personas, packaging, that sort of thing. And finally, Damaka is gonna round us out with looking into resource efficiency, so solving the noisy neighbor problem. So let's start off and look at a very common situation. You've just bought your SAS application. It's awesome, right? You're proud of it. You've put a lot of work into there. And you've got customers using it. And to operate that, to manage that, you've got some business model going on as well. Cool. It's been a couple of years and, and hopefully, you're getting some revenue and making some money. And where you want to go from here is to, to grow your sass. So the question is, what do you need to do to grow your SASS? Well, there's 3 main areas that we can look at. One, we can get more customers, right? More customers equals more revenue. More revenue equals more profit. Great stuff. The other way is that we can look at getting more revenue per customer. So moving customers onto higher margin packages, that's going to give us more profit. And of course, the other way is to lower our operating costs. So get more resource efficiency into there. So these are the things that we need to do, but how are we going to do it? Well, let's look at best of breed test vendors and what they do. And when we talk about best of breed, we're talking about those Sass vendors out in the market. You probably know them. They, they are standing above their competitors, right? You can see them and they, they always seem to be one step ahead and keep pulling forward. So what's their secret? Well, it's not really a, a, a secret. It's they use data. So they know the cost to serve their customers. They've got things like cost per tenant, cost per feature metrics. And they understand what their customers are doing. They've got insights into their customer behavior. And of course, they focus on their resource efficiency because when you first go live with a, with a SAS product, you're focused more on just getting out, getting it going live, getting products going, and you're not really thinking about building the most, uh, efficient architecture. Now, the problem with that is that you get some resource wastage. And as you scale your SASS, that resource wastage scales as well. So Best of breed knows this and they, they focus in on efficiency because those efficiency gains are going to scale as well. And this will give them the edge because they're able to use this data to create packages and pricing that the customers are going to want, right? Because if, if you're choosing a SAS product and you've got a couple of competitors and one of them has exactly what you want at a price that's fit for you, well, that's the one you're going to choose. And because they've got these insights, because they have this data, they're able to forecast the revenue that's going to be coming in, right? That's pretty useful as assessed business. And because they're ultimately more profitable, they have more profit to reinvest in themselves. So you get this nice cycle, right? They have more profit to reinvest, they can pull further ahead, and so on and so on. So if it's so simple, Dave, why, why aren't you doing this right now? Well, Odds are that you probably have other things to do, right? You've gone live with your SAS, you're onboarding new customers, you're adding new features in. You started off as an architect, right? Then you turned into DevOps as you've got the customers going on, more and more customers. Now you're just ops. You're not even doing architecture anymore. So nobody's got time to think about implementing these sort of features. And Engineering will think this is a business problem, right? So until we have some business people, until you assess company scales that you have these business functions pushing engineering to deliver them. You're going to focus on what's important for you. And ultimately, there's a cost to doing this. There's a cost in terms of engineering points. There's a cost in terms of your time. So It, it gets put onto the back burner until you reach a level of sas maturity and then start incorporating these features. Now, the problem with this is that as you operate your SAS, you have to make decisions. And at the moment, you'd be making decisions without this data. So you're already behind on the game because your competitors could potentially have these insights. If your competitors understand what your, what the customer base wants, well, you're behind in the game. And even, even more dangerous, you may have unprofitable customers. It's a common scenario that I see with SAS vendors where they've been going for 3 years, they start looking into their cost per tenant, they find out they've got these customers that are unprofitable, that are hurting their bottom line. It's not a place that you want to be in. Now, lots of doom and gloom, right? But there is a light at the end of the tunnel, and that's why you're here today, right? This is what the session is about, because GE AI can really help in this regard. Because all these things, these data that we're, we're talking about, it's all about looking into our data and finding patterns and finding insights. And guess what? This is exactly how LLMs work. They're great for identifying patterns. And not only that, You can use them to get some value really quickly. You can get an environment going up, look into the data you have, and get some insights in an afternoon. This is really awesome, right? You don't have to take months to build up this environment. Straight away you've got some value or not. And if you don't find something else. And you get this lovely natural interface, right? You can use natural language to query for these insights. You don't have to be this intermediate translation from business question into technical requirement. And it also gives us the opportunity to move beyond just solving these normal use cases and looking at what we can do beyond what's currently now. So things like real-time or predictive analysis. So, let's look at an example SAS application, right? You've got your application plan there. This is where all the good stuff's going on, right? The customers are getting their value. And this is going to scale over time and you're going to need to operate and manage that. So you, you're going to introduce a control plan. And inside the control plane, we've got a few components. We've got some data sets. So our application metrics, for example, our usage metrics. We've got a data set which will handle our tenant metadata. And when I talk about tenant metadata it's information about our tenants, about our customers, like which resources are they allocated to, what sort of entitlements do they have, that sort of thing. And to do all of our heavy lifting, we have some compute logic, right? These are our workflows. So onboard a new customer, this is where it's going to happen. Change in entitlement, update our resources, etc. And to interface with these, we've got this lovely integration layer. And we can see that we have a lot of tools, a lot of resources that an agent could utilize. So what would, what would this look like as an example? Well, let's say you're inspired. You come and you do the session, you're excited about this and you rush, don't even think about dinner. You just rush back to your hotel room and you want to start playing. So you, you get up, you're a nice career environment. Get onto the AWS lab site and see, hey, we have this cloudwaatch MCP server. All my, all my logs are in, in CloudWatch, so I'm going to use that. I'm gonna go ask some question, right? Do all my tenants have that same usage pattern? That could be interesting. So, our little LLM friend is going to go and use the MCP server and hopefully look in the right place. And I say hopefully, because your SAS architecture is probably pretty complex and it's hard enough for us to understand, let alone an, an agent. So we can expose the the code base, and look into there and find how you've done your architecture, but it doesn't really tell us much about how you operate as a business. So we want to be kind to our future robotic overlords and give it a bit of context. Tell us a bit about how our SAS application works. Because the better context you're going to provide, the better results that you're going to get. So now we add that context, ask that same question, beep beep beep, we get a result. This is really cool. We, we should be able to see some insights that potentially we didn't have before. And you're saying, well, this is great, right? It didn't take me long and I'm getting some value out of this. So where can I go from this? Well, we can go back to AWS Labs and say, hey, what other MCP servers do we have? There's one for Cost Explorer. Well, that's interesting. Now I know I have got resource to, to look into the usage of my customers. I know how to find the costs for serving that. Let's find out our cost per tenant. So we can say, hey, let's just divide that total cost by the number of active tenants. And boom, just like that, we've got some data, some insights about cost per tenant. And, and I talk constantly with customers who have been serving, uh, the SAS for years and they don't have an idea about the cost per tenant because it seems so hard. Well, we've done it in less than an hour. Easy, right? And the thing is it's not perfect, but you've got, now you've got some figure that you can work with. And, and we can look at improving the accuracy, right? Because at the moment, we're just looking at the number of active tenants that we have. Maybe we have some inactive tenants. That's going to change our cost per tenant values. And we have information on the number of tenants that we have and their subscriptions. It's sitting there in our tenant metadata. We just need a way to expose that so that our agent can use that. So we can go and extend our integration layer, add some MCP servers there. And add these into our agent, and now we can get a more accurate cost per tenant, right? And this is awesome, and all we had to do is expose the control plane a little bit. And, and that's really not so hard, Matt, because for example, look at our tenant management. This is, this is the tenant management service inside our control plan. We can see our crud operations there. Or to expose these as MCP servers, we can use something like agent core gateway. It makes it really easy, right? We create this tenant management gateway and add some targets through to our API methods that we want to expose. And we can repeat this pattern with our other control plane capabilities. So resource management, entitlement management, billing, anything that could be useful for us to get insights into. And once we've done this little proof of concept and seeing that we're getting some insights that are valuable for us, either as architects or as a business. Then we can decide that we want to keep this and make this a permanent capability inside our control plane. So, one option is that we could just lift and shift what we have and, and, and put it in, and deploy that. Now, the other is that we could make it a bit better, a bit more accurate. So we want to think about some sort of architecture pattern that we can use because we're going to be deploying these things over and over as we get these new insights. And it's basically going to look like this, right? We're going to have the agent itself. We're going to have some deterministic tools, because if you want to do X divided by Y, well, create a lambda, right? It's easier, cheaper than getting an agent to do it and it's reliable. We're going to have a dataset or datasets. This is going to be the results from anything that we're, we're deriving in terms of insights and some sort of gateway to expose those for our end users or other agents. And for example, we might have a, uh, deterministic tool that says, well, I'm, every night I'm gonna go off. I'm gonna look through our metrics, aggregate some results and put them into our dataset. Now I can expose that to our end users, nice simple web interface or maybe some, some BI tool, Amazon Quickle type thing. Awesome stuff. Now our end users, our business are able to see the insights that we've, we've garnered. We can also expose that to some other agent and some other agent can then come through the gateway through some MCP server that we've exposed and they can access that data. So we can see as we've created this capability, we're also able to share those insights to have a data set that another capability could utilize to get even further insights. And we can do a similar pattern when we talk about invoking the agent. So we can provide a natural language interface for our business to, to query, or we could expose it to some other agent, and this is awesome. And then our agent can use its own tools or some external tools to go and do its thing. So we can see by having this nice pattern. It lets us be modular and scale out our insights more and more so we can get more and more information about that. And, and this is a nice pattern. You're creating value and you're creating more value and more value. And to give some examples of how this would look in another context, I'd like to pass back to my colleague Damika, who's going to look into cost analysis. No Awesome. Thanks, Dave. Um, everybody, welcome again to the session. Uh, now that you have a better perspective about how GNAI capabilities can be leveraged in SARS applications, let's take a look at some of the examples starting from the cost analysis. Now, in general, cost analysis starts with instrument in the application plane to meet the meter the consumption matrix of the tenants so that we can aggregate them in the control plane to get the infrastructure cost per tenant attribution. And that helped us to understand the tenants who are not making so much of a profit in the business so that we could do something about that. Traditionally, this is where we all got settled in most of the SARS applications in terms of the cost analysis. Now Gen AI fundamentally changes this equation where while it can provide the automation for the cost analysis, it can also provide the access to advanced cost intelligence much easily. Now, before getting into the cost analysis, our story still starts with the cost per tenant attribution. Now Dare explain how we can get some of the quick wins by leveraging GAI tooling in the cost per tenant, but here we will see how we can make it more permanent design in the control plane architecture by using GI technologies. Now, in the control pan we already got the consumption matrix and the logs from the tenants, and now, instead of having hardcoded cost allocation rules and bulky microservices, we are going to introduce an AI agent. Uh, for, for, for the picture, and that agent can be triggered using a prompt, and that will be, uh, kind of orchestrating the entire workflow of the cost per tenant attribution by using the, uh, reasoning capabilities of using the large language model. Now, during the workflow, it will look at the current usage of the tenants and then aggregate the matrix out of that so that it will know the list of AWS services that has contributed to the cost and then work with the cost explorer MCP server. To get the unit cost of these matrices so it can calculate the cost per tenant accurately. Now whenever you are building this for your SARS application. Uh, you will feel that, hey, we, we need more tools, more steps in the workflow. That's totally fine. The agent will be able to orchestrate them all for you. Now when I know the cost per tenant, I can easily get the margin value as well in the same time, and that's great because now I can have the cost and the margin both in the same data set and then save as a in the control plane for the future use. Now this is going to be the base for our cost analysis architecture. Now if you look at where the cost analysis analysis uh implementation really get fit into the control plane architecture, there are multiple ways of doing that. One of the ways to think is to extend the SARS admin panel and introduce a cost analysis dashboard so that we can populate the Insights, the patterns and the recommendations we are extracting from the cost analysis module in there so that the SARS admins will have access to them. So with that setup, um, We already have the historical cost standard margin data set, came from the previous agentic workflow. Probably one of the low hanging fruits for us to start with is to look at this data set and identify the insights and the patterns, so me as a SARS provider, I have a better perspective about what happened during the last few months in terms of cost and the margin. So for that, we'll have a cost insight agent here, very simple workflow it has. It'll look at the last 12 months of the cost and the margin data set. And and then leverage the LLM and ask for the insights and the patterns. There's no business logic in here, just the power of the LLM. So during this analysis, I can ask for things like trends, margin variations, and temporal and behavioral patterns, and this can be entirely customizable because this is what will go to the prompt that we send to the AI agent. Now, the results of that would be Um, very much data-driven, deep insights that will show me how the margin and the cost has been varying over the last 12 months. This is awesome because now I, I can even understand some of the opportunities that I could have done better to improve my profit margin as well. Now another feature that we can add to the cost analysis is the cost forecasting. The idea is simple. You look at the historical data and then predict the future values, right? So for that, we're gonna have a cost prediction agent. It will look at the pretty much the same data data set in this example, like previously, the last 12 months data of the cost and the margin, and then they do the prediction for the next 6 months. Now, as a SARS provider, I really want to have this prediction as accurate as possible so that I can use them with a high confidence for my, uh, let's say, business decision making and things like that. So in order to do that, I can either go for a library or a business logic of my own that will give me the accurate, you know, prediction as well, or I can plan to deploy this agent in the Amazon Bedrock agent core runtime and then let it know what kind of prediction algorithm and the simulation I want to have, so the agent will be able to generate the code. Using the LLM and then run that within the agent code interpreter so that it will provide the prediction data set. So in that case, I'm not even managing this business logic that will be literally generated by the agent for me. So likewise, there are many options with the agentic set up like that. So when you've got the predicted uh data set, you're gonna save that for the future use and then it will leverage LLM asking for the insights and the patterns. Now, during this predictive analysis, I can ask for things like forecasting trends, uh, growth patterns, and the cost and the margin variations. Now I have a better perspective about what's going to happen in the next 6 months in terms of the SARS economics as well, which is awesome. Now we can actually get credit for, for the business in different, different forms. Now, this is great. I only had the cost per tenant value some time ago, and now with the help of a couple of uh agents, I now have the historical patterns and the predictive analysis as well. Now, before moving ahead, we want to improve our architecture because so far we have been discussing what individual agents can offer to us, right? So if you, if you put them all together into a SARS architecture, it's important to look at how they work each other uh as an extendable design. So here we got the insight and the prediction of agents that we discussed before. So one of the changes I would do here is to introduce a supervised agent. So it'll kind of orchestrate these two agentic workflow and anything that is coming up down the line. To extract the response for the prompts coming from the front end, in this case the inside dashboard. And also as a technical decision, probably I'll have the cost per tenant workflow separately so that I can individually trigger that when I want to get the latest cost and the margin values into the control plane. Now, with that setup, it's all, it's, it's really interesting to look at another area uh that is AI recommendation. Can an agent provide me some recommendations uh to optimize the resource utilization to maximize my margin, for example? To check out that, we'll introduce a resource optimization agent here, and then I'll provide all the knowledge about the infrastructure of the system. And um the, the tenant usage of the infrastructure, that dataset as well as a preloaded knowledge for these agents, so it's already configured. And then When that agentic workflow got triggered from the supervisor agent. So this agent can leverage its own LLM now also with the historical and the forecasted cost and the margin data set as well. And asking for the recommendation. Uh, to For the, for critical uh infrastructure changes I need to do to maximize my profit margin. Now this is awesome. Now there's an agent telling me what are the things I can consider. Uh, in, in terms of the infrastructure changes to maximize my profit as well. Now, if your main focus here is to improve your profit margin in your business, which is in in every SARS application, This is just one side of the story, optimizing the infrastructure. The other side would be the tier tier pricing. You've got to have the tier pricing optimized as well. But if you look at our architecture, there's one important dimension that is missing to have recommendations for tier pricing. That is the unit cost for the features. That stands for the average infrastructure cost required to host each and every feature in the architecture. Why it is important? Because most, like often, most of our SARS applications are driven by product features, and then we categorize or group these features into different tiers and we provide a pricing value. Because of that, the unit cost of the future will have a direct impact to the tier pricing. But how can we get it? It's actually simpler than we think. We already have the consumption matrix in the application plane that have been metered by the tenant ID. All now what we need to do is to identify the features and their workflows along the along the architectural components and then inject the right feature ID. Uh, to the consumption matrix based on the future workflow. That's it. And then send them onto the control plane and in the control plane, I'm gonna aggregate the matrix by the feature ID now, and then calculate the cost, get an average, and then I will get the cost per feature data set. Now I can talk about the new pricing strategy agent that we are going to introduce into the picture. So this will give us the recommendations to optimize the pricing strategy. For that, I will provide all the knowledge about the uh current tiering strategy, packaging structure, and the cost into this agent. And then when it, when it gets triggers, it will ask for the recommendation and now it can have it, it has a rich set of data sets to bring in uh for for for getting those recommendations such as all the costs of the margin data set and the new cost per feature data as well. Because of that, these recommendations are really comprehensive. It can come in terms of the pricing adjustments where the, you know, agent will tell you exactly what are the prices have to be changed to maximize the profit. It could be packaging recommendations where it will tell you what should be the list of features that each and every tier should have to maximize the margin, even the new tier opportunities where we can introduce a new tier, what is the price of that, what should be the list of features that it has as well. Now, this is awesome. Now we can start optimizing the pricing strategy as well. So if you look at our cost analysis, you know, architecture, the right hand side you've got a nice uh agentic workflow that is growing. We can keep on adding uh new capabilities. You've got the supervisor agent orchestrating them all and then extracting the response for the, uh, uh, prompts coming from the front end, which is the dashboard in this case and like easy in in uh easy feature that we can actually implement in here is the, of course, the natural language interface. So now that SARS admins are not limited by what has been shown in the dashboard, they can even have a conversation or interaction with the agenting workflow and get to know more about the insights and the recommendations as well. Now, whenever you are building this for your SARS application, I have two recommendations. Number one, you don't have to start, start with everything, right? Start small but have an extendable architecture so that you can keep on adding new features, new agents without being worried about how do you integrate that to the SARS architecture. And then work backwards from your business requirements so that you know what you need from the front end, whether it's a dashboard or an AI bot or something else, so that you can prioritize the relevant agentic workflow at the back end. I hope this will help you to start off with your cost analysis workflow in your SARS application. So like cost analysis, custom insights takes high priority in SARS application where Gen AI can help a lot as well. To talk about that, let me turn it back to Dave. Awesome. Samaka. So customer insights, knowing what your, your customers are doing. It was one of the big promises of SAS, right? You, you were all on-premise, you didn't know what your customers were doing. Well, let's operate and manage this on our own environment and we're gonna get insights into our customer behavior. We're gonna understand what they're doing. Awesome, let's do it. Now, the reality is, I see this with a lot of customers is they're not getting these insights, right? And this, and this is a, this is a bad place to be because our customer insights are really important. And, and why are they so important? Well, basically, they grow out fast, right? Because if, if we know what customers are using, well, we can spend our engineering points and make our strengths even stronger. And something that I've seen more than once is where we have a customer starting to do, to look into their insights to find out what, what their customers are using and they find that their customer base are using features which aren't really the core product. And so they decide to, to pivot and focus in on those aspects of their, of their product and they become more successful that way. And the other side is it lets you sell in the way that customers want to consume, right? Because if I'm going out there and I'm looking for sales products, well, I'm gonna choose the one that has the package and the pricing that relates to what I want to use. And you can use this packaging and pricing to create these conversion journeys as well. So creating a, a way to get someone from a basic tier that gets value from your core product into a more premium tier with higher margins. So we can explore customer behavior if we, if we go back into our, our nice care environment with some context like we had before, or we can ask some, some basic questions, right? Do we have different customer personas based on our usage? And our little LLM friend is going to do his thing, beep beep beep and give us some data. And these insights, could be useful. Like for example, we can see here that we've got a bunch of customers here with low usage. Now, that could be interesting for our business because those customers could potentially leave. We don't want that, right? We don't want to lose the revenue. And on the other side, we've got a bunch of customers with high usage. Also an interesting thing from a business perspective because these customers could be moved into a higher paying more performing tier. So we'd like to get a few more insights into this, but we need to add some more dimensions. We need to look into, to feature usage, right? What are, what features are the customers using? So, we come up with a problem of where do we get that feature usage from? And a common conversation I've had with, with SAS vendors is that they're, they're going down and looking into feature usage, but they think that they have to instrument their whole application and this it becomes this roadblock, right? It's a lot of instrument. You never get data out, and when you do, it's incomplete views. But it doesn't have to be so hard, right? Because we're building our applications on the cloud, right? And you need some sort of way to access your application. And this ingress is the perfect place to take feature usage. So let's say, for example, if I'm using an API, well, most of our, um, APIs are going to have some opportunities for access logs, and most likely each feature is going to have their own path. So we can use a path per feature as a way to get feature usage dimensions. Boom, nice and easy, right? Didn't take any time at all, and now we're getting insights into our feature usage. And we can get more granularity this way as well, because our operations within that, that, uh, service feature are probably gonna be different based on the HTTP method. So we can use this as another method of getting some more granularity in there and straightaway, we're getting a lot of insights into feature usage and didn't really have to do much there. Now, this doesn't apply to every use case. Maybe your product is taking millions of requests per minute or second, and you don't really want to turn on those access logs, right, because your bill is going to explode. So then you need to look at actually instrumenting your, your code. Now, this is going to give us a bit more complexity, right? Because we actually have to go into our application, touch the code, and if you've got multiple feature teams, you have to think about how you're going to standardize across the teams, right? Because you want some sort of standardization because standardization scales, fast. But the benefit to this is that you're able to get much richer insights, right? Because you can instrument for the dimensions and the metrics that are relevant for your business. So we have to think about that trade-off and balancing that higher engineering cost with these deeper insights. And there's a place for both approaches, right? And my general advice is to say start off by getting that feature usage with your ingress logs, right? Get some data on the table, stuff that you can work with straight away. And then when you need deeper insights into particular areas, then start instrumenting your application code from there. So now we know how to get feature usage. Let's add that as some context for our little agent friend. And now we can ask some questions, right? What features are my customers using? Does its thing beep beep beep and gives us some, some results. And because it's a natural language interface, we can follow this up, right? We can, we can say, well, based on that feature usage, do we see some different personas? Because customer personas are really strong. And when we talk about customer personas, what we mean is, it's just a group or a subset of our customers that we can market to because they have similar attributes. They use our product in, in a similar way. Because it would be nice if all of our customers were the same, but just like the real world, customers are different. So we need to find out what sort of different customers we have. And once we know what different customers we have, we can find out their profitability. Because it would be really interesting to understand which ones are profitable or more profitable or unprofitable, because we can use that data to set our strategy, right? If we're, if we're planning our customer acquisition for the next quarter, we're gonna be spending this sort of money, well, what personas are we going to target? I'm probably gonna choose for the ones that are more profitable personally. But now we have some data to do that. And we can explore these personas by building up these capability sets as well, right? So I've created this persona analysis capability, right? This, I'm getting good at it, right? I've created ones for usage and cost analysis. I mean, the agent's just doing half of this work for me anyway. And now when I go and query that, what's that cost per persona, it can use these other capabilities recursively, right? It's going to go and ask what personas we have, what sort of feature usage those personas do. And then we can use that cost per feature to get some insights there. Ask which ones are profitable. Ask, do we have any unprofitable ones, right? So we're getting this, this information that we can use as data for creating more decisions. And part of those decisions and their strategy is looking at our packaging. Right? And, and here we're creating packages for the personas that we've discovered, right? Because you can target your customers better. And We can plan these packages to foster an upsell, to foster a conversion journey. Another common, common conversation I've had with customers is they, they create this awesome SAS product, right? It's, it's solving this great use case, but they only have one service tier and it's really expensive. So they can't get new customers on because their, their market base is, is a small subset of enterprises that have all that money. And you, you need to think about, well, how can I create this basic value tier that gets customers onto my product. Getting value out of that integrated into their business workflows, and then pushing them into more higher value for the customer and higher margin for you as a SAS vendor packages. In the premium tier. And once you're planning these packages, you can forecast the cost per package as well. And this is going to be really important. If we go back to that customer acquisition example, right? If, if I know what, which personas I'm going to target, and I have an idea on, on what it costs to serve that and some, some sort of conversion rate success, well, I can, I can forecast what sort of return on investment I'm going to get by spending in, in customer acquisition. Or maybe you have some RFQs. If you're a public sector, right? RFQs are really common. Well, you need to understand how low you can go in terms of your offer and also make an acceptable margin. Well, now you've got that data, right? And, and you can increase your revenue with this approach. And there are other ways of increasing revenue per per customer, right? And one way is by reducing churn. So when a customer leaves, it's a bad thing because not only are they, they leaving and they're taking their, their revenue stream with them. But they probably still have the, the core need why they came to you in the first place. So they're probably looking at a competitor. Now that competitor has that revenue stream, they're reinvesting that to get better than you. Don't want to be in that place, right? So this is why Best of Breed vendors have these, this as a, a core mechanism to, to retain customers. And the other way by increasing revenue is upsell, right? We talked about that, planning this conversion journey, upselling. Well, you need to look and find the, the customers that have potential for upsell. And again, Best of Breed SAS, they know this as well, and they build this as core components inside their, their business models. So if you're in an early stage, you don't have big teams that can do this, well, what, how can our little agent friends help us here? Well, if we come back to, to these capabilities that we created, let's look at how we can detect churn. I can go and ask, set up some, some scheduled automated, uh, request to say every night, go and look for low usage customers, right? It's gonna go check our usage metrics, and if it finds some, it needs to know what to do. Now this is, this is then a decision for you as a business, right? Because this is ultimately a business process, right? What do we do with potential churn risk? It could be you say, well, how important is that customer to my business? Because if, if they're really important, well, we need to get feet on the ground, right? We need to be out shaking hands, finding why they're not happy and making sure we can retain them. But that doesn't always scale. So we end up building in some sort of automated workflow, right? Reach out with some templated email. We've, we've seen these in our inbox, right? You get this, this, uh, email saying, oh, please, please stay customer, here's a 10% discount, whatever. Very non-personal, but it's got a rate of success, right? That's why we do it. But we can improve that, right? We can invoke our, our usage analysis agent and say, hey, this is the customer that's at risk of leaving, based on their persona and their usage, what would be a good way to try and retain them? So we get a, a personalized response for the customer and this should yield a better retention rate. So, this is cool, right? But we can also improve this as well. We can turn that customer reach out into a customer success function. And that customer success, this capability is going to do the same thing, right? It's going to create that personalized response, reach out to the customer, but now it's tracking these customer interactions. It's seeing which, which ones are more successful than others. Basically learning from itself by, by, uh, over time. And we can do a similar approach by looking at new customers, right? Because when a new customer comes on board, we want to make sure that they get a short time to value. They want to get them using your product, getting that value locked in. So we can, we can do the same thing, that same approach and just say, well, if, if they have low usage and they're a brand new customer, we'll send them something that's maybe more of a quick start than trying to retain them, right? And we can use these same tools for upselling. Except now instead of looking for low usage customers, we're looking for high usage customers or maybe customers that use specific feature sets. And again, Once we see an opportunity, we follow that same approach, reach out to our customer success capability and create this personalized approach, except now we're looking to upgrade the customer instead of retaining them. And these capabilities, these aren't brand new things, right? I didn't invent customer success. If, if you've been in the SAS field for a while, you know that customer success is a standard business unit, a standard capability. And, and things like usage analysis, that's been done by product owners since product owners started. But the difference is that now you're able to implement these with agents without having to build up headcount, without having to build up physical teams to do this. And this is, this is really, really awesome because that means that you can start getting these best of breed, these mature sass attributes straight from day one. And we can see that not only is GEI fundamentally changing how we build our SAS products, it's also fundamentally changing how we operate them as well. And to give another example of this, I'd like to pass back to, to Damaka, who's going to look at resource management and resource efficiency. Thanks, Dave. So, resource management in SARS is always a balancing act. You provision too little, it will impact the performance. Uh, if you end up provision too much of infrastructure, it will kill the uh profit margins, right? So, so that's why it's critical but also very challenging control plane service to have. So this is, this is where GAI can help a lot, uh, to, to automate, to improve the operational efficiency and also the agility of the overall module. Resource management always starts with the resource monitoring. If you look at how we used to do that in traditional SARS applications. Um, we already had the consumption matrix and the access locks of the tenants in the control plane, and then we decide defined the service level objectives for the solution, SLOs. This stands for um uh the infrastructure level thresholds that the tenants are allowed to use at the maximum level. And then we had the monitoring microservice constantly looking at the tenant usage and checking if there is any SLO violations. If so, there is a notification to the DevOps team. So the dev ops team comes in, identify the issue, identify the fix, and then verify once the fix is done, a whole bunch of manual work down there. So if you really want to bring some acceleration and quick wins using Gen AI, you can always lean into a preloaded AI agent like Quiro CLI, provide the context, let it know how the solution works, and, you know, brainstorm about the notification that you may have received or about anything directly about the attendance as well. It will provide some guidance. But if you really, if you really want to make this automated as a part of the SARS architecture, one of the first things that you want to do is to understand how the SARS resource monitoring and the management uh workflow would do, would work, like is and so that how can you automate that. So everything starts with the resource monitoring. Why? Because then we can understand the anomalies which are the uh suspicious tenant activities that are ideally there's still a violation, violations, but With Genia you will see that there is a broader scope than that. So once we know the anomalies, then we need to figure out whether they are actually, if they are leading into anything critical, any critical conditions, anything concerning, such as noisy neighbors or resource misuses or or API spikes and things like that. So for that, with Gen AI we can leverage, you know, pattern recognition. So once we know the potential critical condition, then we can get focused GI components, looking at them and identify the corrective actions, which are the infrastructure level changes that we need to do to mitigate those conditions. And then we can go and fix them, uh, monitor them, and then verify if the condition has been resolved. Now, when you want to automate uh this for your SARS application, certainly there will be more steps coming up, but I would consider this as the bare minimal that you need to automate anyway. So let's see how this flow can be implemented using gene technologies. Now for the anomalies, we are going to implement an anomaly detection agent that can be triggered using a prompt, simple workflow. It'll take the current usage of the tenants, and then SLOs and leverage to the LLM ask for any anomalies. So, in the prompt, you can ask uh to cross-check the current usage with the SLOs and also any additional considerations as well, such as, you know, consider on the on the business impact, consider on the tier configuration of this particular tenant and the impact to the other tenants of the tier and things like that. So, based on your SARS application, you can keep on adding more and more consideration. So what happens is now the agent is having better and bigger scope to unders or identify the anomalies without missing out anything else. So once we know the anomalies at this stage, we are going to log them into the resource analysis data store and then move on to the next step. If you look at these anomalies, they are, they are filled with data points that justifies why the agent thought this would be an anomaly, such as the statistical analysis of the threshold violations, and also the LLM analysis will also provide some level of details. So once we know these anomalies, next is to identify whether these are leading into anything critical, any critical conditions. So for that, we're going to add a pattern recognition agent in here. Now, for the pattern recognition agent, we will have preload or provide the knowledge to the the agent upfront about the potential critical condition that could have happened in the SARS architecture. For example, how do you define a noisy neighbor and how do you identify a resource misuse and security issues and things like that. So it'll, that knowledge is already there with the agent. So with that, When it gets executed with the workflow, it first takes the anomaly that was identified and then and then get the tenant ID or tenant. And then get all the current and the historical usage data from the control plane and then leverage to the LLM and then ask for potential critical conditions. Now, through the prompt, we can, you know, specifically mention uh to conduct the usage, temporal and the behavioral pattern recognition of this given anomaly against the uh historical or the usage data set, so the agent will know how many times this anomaly or something similar to this anomaly has been repeated in the past and what is the criticality and the importance of that so that it can map the anomaly into a critical condition that's given. So once we identify any potential patterns like that, we will lock them in here in the resource analysis data store and move forward. Uh, and these potential patterns will clearly mention what is the condition that, uh, that is concerning. For example, in this case, tenant 004 has been identified as a potential noisy neighbor. Why? That anomaly has been repeated about 100 plus times during the last few days. 70% of which has been during the last two hours and also the agent has predicted that 80% of that will likely to happen in the next 3 hours as well. It relates to heavy CPU and the resource usage during the off-peak time. Certainly someone has to look into this immediately. However, if you look at our architecture, we are not yet ready to process those critical conditions. We just have a couple of agents only, so it needs some improvement. First improvement, we are going to have a supervisor agent for the orchestration, and then we will introduce a condition-specific agenttic layer where there will be a dedicated agent per critical condition. One for noisy neighbors, one for handling resource misuses, maybe one for security as well, who will be handling cases like Unauthorized cross-tenant data access, and so on. So the advantage is that now I can provide focused knowledge, the tools and the capabilities agent-wise based on their focus area, based on the condition that they are focusing on, so that their decision making capabilities will be much more advanced and accurate. So maybe the final enhancement to the architecture at this stage is if you look at the resource analysis data store, we have been keep on adding all the anomalies, the patterns, critical conditions, and down the line you will see we'll add all the corrective actions and the actual fixes that we are doing into the same data store. So over time this data source is going to be the single source of truth that represents the entire SARS monitoring and the management. Uh module of the SARS application. It's a huge amount of knowledge. So what we can do is I'm going to convert this into a vector store and build a knowledge base out of that and then inject that knowledge to each and every specialist agent that I have, so now they are more capable, the decision making would be much more accurate with the historical knowledge as well. Now with this, if you move on to the uh the architecture and then see if you have a noisy neighbor, potential noisy neighbor uh to handle, like in our previous example, the supervised agent just needs to trigger the noisy neighbor agent, which is already configured with the knowledge and the tools, and it will refer to the potential pattern and then ask for two things from the SL LLM. Can you confirm whether this is actually not's neighbor? So the agent will Use the existing knowledge to decide whether that is a noisy neighbor or not. If yes, what are the corrective actions we need to do to mitigate the situation. So if you look at the output, It will clearly mention the confirmation and also the corrective actions that we need to do at the infrastructure level. So this format will be the same for all the critical conditions that you're going to have in our solution. Now, this is great, right? This is great, but the big question is, who's going to verify this, who's going to do this? Traditionally, we had our DevOps team coming in and doing these infrastructural changes, but now we have an agency player doing most of the stuff for us. So, to, to where we have to lean in, in this case. So there are many options. So, my recommendation is, When you have these kind of corrective actions or critical conditions found in your SARS application. Prompt that to a natural language interface and have a human in the loop. Ideally, an expert in the house on DevOps who can come in and review these critical conditions at the corrective actions. And then maybe, you know, ask questions from the agenttic layer about these options, get to know them more and verify what needs to be executed out of these, if not all. And then use the existing DevOps and the GI tools to do the fixes and let the supervising agent know about it. So that it can self-generate a prompt that will trigger the existing pipeline, but this time only for this particular tenant and only to verify the fixes uh for the critical condition. That's it. So that way we are. Kind of reusing the same workflow, same pipeline, without having any, without needing to have any additional instrumentation for monitor and verify. What we can do is we can even have a verification window during which we will run this prompt a few times just to absolutely make sure that the fix worked. All right. So, again, when I, when you, when you want to kind of adapt this to new SARS application, I, I highly recommend to have the human in the loop option that will improve the confidence of the infrastructure level changes that you are doing in your production system and also it will help you to understand. Overall accuracy of the output of the agentic layer so any improvements can be identified. So you can keep on modernizing the agentic layer until you have a better confidence about the output, so that you can even plan to pass or hand off some of the responsibilities to the agentic layer as well. Hope this will help you to entirely automate your SARS monitoring and the um management layer by using gene technologies. All right, so we we we talked a lot about GAI today and how we can bring the GAI capabilities into the SARS control plane. Um, we took a lot of examples in explaining that as well. So, in terms of, in, in summary, as key takeaway is, if you are early into the process, you know, start small. There are enough preloaded AI agents and MCP servers out there that you can use to put together, get something up and running that shows some results real quick in your control plane. So always lead your teams to that. To start with, if you are there already. You know, identify a few areas that you can modernize in your control plane and bring the agency capabilities in. I highly recommend you have a closer look at Amazon Bedrock Agent Core service and its features. It has a lot to offer to you. Feel free to bring the right tools and bring the right capabilities to the agents so that their decision-making capabilities will be higher, and also always start with the data. If you don't feel don't see the right data set that an agent requires, start from instrumenting the application plane and bring the data set first into the control plane so that you can then talk about the agentic workflow. So finally, take the leadership to kind of provide, uh, you know, the guidance required to your team and the organizations to bring the generic capabilities into this control plane. Now that you know it's not only the automation and the operational efficiency, but it can also help things like maximizing the profit margin of the SARS application as well. Now, we went through these three use cases much deeply in terms of the design and implementation just to showcase to you what is possible and how it is done. So, I highly recommend that you plan to implement them in your SARS application and in general, my call to action for you all would be identify the remaining uh control plane services as well that you want to modernize and feel free to bring the agents and the GI capabilities into the implementation as well. All right, everybody, so this is the kind of content that we want to share with you all. I hope this is what you expected from the session as well, and I believe now you have a better perspective, better confidence about bringing GAI and the agents, uh, agency capabilities into the control plane. Uh, just a quick couple of things, uh, before wrapping up. These are some of the SARS sessions that our team is, uh, you know, hosting during the next few days in the reunion as well. So, uh, if you're interested, please take a note of it. There are some workshops and the, uh, builder sessions that will be very hands-on as well. And finally, there should be a survey link for this particular session in your mobile app. So please let us know your feedback. Super important. We'll be looking at them with a high interest to learn from you and also to shape up our future reinvent sessions as well. All right, everybody, a huge thank you to all of you for being here in this evening. Thanks, Dave for cooperating with me as well. I wish you all have an awesome remaining part of the reunion, everybody. Have a wonderful evening.