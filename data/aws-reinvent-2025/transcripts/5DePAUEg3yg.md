---
video_id: 5DePAUEg3yg
video_url: https://www.youtube.com/watch?v=5DePAUEg3yg
is_generated: False
is_translatable: True
---

Good morning everybody. Hi, good morning, uh, great to see you all this morning. It's 8:30 a.m., uh, we're in Las Vegas. It's the start of Reinvent. Thank you all for showing up today. Um, it's great to see you all in here in person. I can't see much because of the lights. I'm trying to spot people's eyes, um. But yeah, here we are, an exciting week ahead. Um, so you've all got a plan, you get really excited and you start moving and migrating your first workloads to AWS. Everything's going great, your teams are getting more confidence, you're high fiving each other because of the success that's happening, um. But you start bumping into problems, er, we see a couple of different scenarios of working with customers over the years. Um, one of them is that you give your technical teams all the keys to AWS and they just start building things without a particular plan or strategy in place, um, and they bump into issues and things get closed down, uh. One of the other situations we see is that, you know, you start building things without really a clear business outcome defined, and again, things get uh get closed down. And we call this uh the great stall. uh and we really want to avoid this happening uh because, you know, crowd success is not guaranteed, uh and we really want you to be successful on AWS. Uh, so this brings us to our talk today, which is from Ideas to Impact, uh, architecting with Cloud Best Practice. My name's James Beaumont. I'm the director of enterprise support for Amir. Um, and my co-presenter, who, where is he, he's over here, uh, is Paul Moran, who will come on stage in a bit. Uh, he's a, uh, principal technologist based in Amir as well. So, 10, it's 10 years since we launched Cloud Adoption Framework and well architected, um, so it's pretty big for us in 10 years of a journey. But what is the cloud adoption framework? Um, so it's your business and organizational guidance for your cloud adoption journey. And the well architected review is your technical architecture guidance for building workloads on AWS. So they work really well together, um. Oh, pressed it too early, um. So they work really well together and the cloud adoption framework is geared towards your early stages of your, your journey to AWS and the well architected framework is when you get into the technical implementation phases. Now, as I said, this is, we've been going for, for about 10 years now, um, but actually started before that. So back in 2012, the well architected framework got created in the UK team, so it was created by the UK Solution architecture team, and it was. Led by a guy called Fitz, uh, and it was based on us having conversations with customers. So we'd go and sit with uh customers about how they're using AWS and we'd see common questions coming up. So we built a bunch of questions to walk through customers with, um, and it started off on a spreadsheet, as all good things do, um, and there'd be a set of common questions that we'd run through customers with, and there'd be guidance out of the back of it. Um, and that was the original well architected framework. Uh, in 2015, we published the framework so you could see what the guidance was and the questions were to run through. And then in 2018 we published it, so we made it self-service so you could run through well architected yourself. Uh, since that time, we've seen customers run around a quarter of a million well architected reviews, which is pretty significant. Um, we also see, we've also developed other capabilities, so we introduced something called lenses. Now, there's two rough types of lenses. We get industry lenses, and in industry lenses, if you're a financial services customer, it will be how you view the well architected, because there might be certain particular things that you want to focus on as being a financial services industry. One of the other lenses we have is uh workload specific lenses and one of the more recent ones that we did was the generative AI lens, so if you're running generative AI workloads, how you wanna think about well architected. But why do you want to use the cloud adoption framework? So you want to reduce business risk, um, you wanna lower your risk profile through improved reliability and business continuity. You want to improve your ESG or environmental, social and governance. You wanna be able to monitor what these aspects are and make sure you're improving over time. You wanna create new value streams or revenue streams, so most of us work for businesses, um, and you know, when we're building something, it's around, often around creating more value for customers and more revenue, so understanding what that is and how you're gonna get there is really important. And then we've got increased operational efficiency. Um, so this is around, uh, reducing operational cost, increasing productivity, and increasing employee and customer experience. Now, why do you want to use a well architected framework? So you want to learn from AWS best practice. There's a saying which goes, there's no compression algorithm for experience. But we've kind of compressed 10 years of experience and understanding of working with customers into the well architected framework. So rather than you have to start from scratch, you can go and get that 10 years of experience to learn from. You wanna make informed decisions. So this kind of is an obvious one and goes without saying, but you know if you're not making informed decisions, you're probably gonna make mistakes, so knowing what you're going into and how you're gonna get there is really important. You wanna build and deploy faster. So when you're sitting down with customers or the business and you're understanding what the customer needs are, you wanna get what they're talking about and what they're asking for into their hands as fast as possible. Um, and so your ability to be able to deploy and release faster is really important, and we call that uh time to value, so customer time to value. And then you want to lower and mitigate risk. Obviously we don't want risk, so understanding what the risk is and being able to mitigate it where possible is really important. And there's often a trade-off between going fast and having risk and understanding all these different components and how they work together is really important. So the guidance framework for cloud optimization, uh, the CAs that I experienced over this last 10 years, and it actually looks at 6 different perspectives. And they're down the bottom here. I'm going to talk about two of those specifically. So one of them's the people component, and it looks at some really important things around your culture, how does your leadership function, uh, what is your org design, and how are those things kind of structured to make you successful? One of the other parts is governance, um, and that looks at things like uh uh risk management, product management and data governance. Because, you know, we talked around speed and how that's really important. Some of these things, if you're not set up in the right way as an organization, can introduce, yeah, slowness. So you wanna make sure you're getting the right guidance and and and following the best practice. Now, from a well architected, we also have 6 things, but they're, they're uh pillars, um, and the well architected's more around the tech when you're getting into the technical implementation of your journey rather than getting the cloud adoption framework, which is your, your strategic portion and and the earlier stages. If we talk about operational excellence, um, it looks at monitoring systems and continually improving processes and it builds on the operational capabilities that we talked about in a cloud adoption framework. If we look at the security pillar, it focuses on things like, uh, data security, user management, um, and being able to detect security events. AWS, we say that security's job zero and the true should be the same should be true for all of your businesses, so the security pillar's really, really important. Um, and then as I mentioned, we've got the lenses, so if you're from a certain industry or you've got certain workloads, you can take a view through the well architected framework. So in summary, the cloud adoption framework is for your early strategic stages in looking at your organization, so how your organization's structured and how you go about getting success on the cloud, and a well well architected framework is when you're moving to the technical implementation phases and taking the best practices for your uh your workloads that you're either looking to build or you've already built an AWS. And the frameworks work really well together and they take a joined up view of your full cloud optimization journey. Now with the well architected framework, it looks at what we call a workload, and understanding what a workload is, is really important because that's how you, you understand what's what it's meant to do and how it's functioning. Now there's two types of ways to understand the workload. You've got automated discovery where you use uh like technical tools to go and do discovery of what you've got running, or it could look at your infrastructure as code. And then you have conversational discovery, so it's sitting down with the relevant stakeholders who understand the workload to to see how it's built and how it's supposed to operate. We actually find those two things in combination give you the fullest picture of what that workload is. I'm going to switch gears and talk a bit about culture, and this is kind of why I focused on those two specific aspects of cloud adoption framework and a well architected review. Um, there's a famous saying which is culture eats strategy for breakfast, um, and we believe this is really important, so I'm gonna go and talk about some of the aspects that AWS utilizes in how we run things around our culture. Um So one of these is what we call the service ownership model. Um, you might have heard the term two pizza teams, but this is how we operate and our teams, uh, function around our AWS services. So a team is responsible for an AWS service. They build it, they deploy it, they operate it, and they continually improve it, but they have full ownership of that service. In terms of how we deploy, rather than taking a release cycle that might be released on a monthly basis or a quarterly basis, we take the approach where we want to continuously release in in small chunks rather than doing big releases. In in that release process we do testing, we, uh, you know, do phased rollout, so rather than doing a fleetwide rollout, we'll test in smaller chunks and then we'll monitor and make sure the progress is going OK. Um, we'll have auto rollbacks built into it. But again, this is a culture that we take where we're continuously doing releases. And then we have something called operational readiness review. So ahead of any service going into production, we have an operational readiness review that has to be completed. And this is built based on our experience of understanding common pitfalls and things that you should think about ahead of uh launching a service into life. So this might be, you know, what happens if you suddenly 10x your traffic, what's gonna happen? Uh, if there's an issue and you have to move away from availability zone, what happens? What's your disaster recovery? Um, what are the key metrics that you're gonna monitor? So if you're monitoring for a certain, uh, errors, what is your error rate that you want to alert on? But it runs through all these questions and and the team that are responsible in this service ownership model have to answer all those questions ahead of it going into production. We then have a couple of different review mechanisms, so this isn't uh this is pretty common practice across the industry, but design reviews. So, you know, have peer reviews of the code that's being done. If it's something meaningful and significant, we'll put on our principal engineers that we have at AWS to review what's being done and the approach that's being taken. And I already talked about the operational readiness review. Now, I've been at AWS for just over 10 years now, and one of my favorite things at AWS um. Which might sound a bit strange, is, is what we call the correction of error process. Uh, so if we have a customer impacting event, we do something called a correction of error or a COE. And it's a review of what happened during the event, so there's a number of different sections in a correction of errors. One of the sections will be a description of what happened, so it's how, what was the customer impact? Did we impact a certain number of orders, was there latency, um, there will be a timeline of what happened during the event. The, the timeline of the event will be down to the seconds, uh, of, of full flow of how the event from start to complete finish when it was mitigated. One of the other sections in there is what we call the five whys. Now the five whys is really cool. So the first why will be why did the thing happen, why was there impact? And then you keep asking subsequent why's that drill down into the details. Now it's not always 5 why's, I've seen some where it's 12 or 13 whys, or there might be 3 whys, but you keep asking why until you can't ask why anymore, there's no more whys and the bottom why is really, really the reason what happened, what caused the event. Also there will be uh actions at the end, so what are the things that you have to do to make sure this doesn't occur again. And then those actions are tracked er until completion, and they get quite loud and they escalate internally with an AWS if they're, if you're not hitting the dates. Um, there's also peer reviews, so when you have one of these series, you can't just write it and publish it, you do it in a peer review setting which comes back to the one of the earlier slides. Um, there's a QR code code here that you can scan. Um, you can go and read more about COE, but it's, yeah, as I said, it's one of my favorite things at AWS. We also do something called the the weekly operations review. So every Wednesday it's afternoon, I'm based in the UK so it's uh afternoon for me, but morning if you're based in Seattle. Uh but we have a weekly call that is all the engineering teams from all of the AWS services are invited to. At the start of that meeting, we always celebrate successes. So this might be that you've improved the er the latency of an API or you've increased the operational efficiency of the service, you've made it cheaper to run, or you've increased the reliability of the service. Um, we'll discuss COEs, the, uh, you know, the bigger ones as a group, and, and the reason is we wanna share that so other teams get to learn about what happened and how they can learn from it as well, um. We'll also do uh this spin the wheel, we'll we'll pick certain services, so AWS has a bunch of services, we'll pick certain services to drill into. All of our metrics that were defined originally in the operational readiness review and that we've iterated over time cos we've been doing continuous improvement, they're all available to everybody, so we'll pull up the metrics of a specific service and that'll be reviewed as in a group setting. But it's a really good experience where you've got your engineering teams across different functions and across your different services, seeing how other teams operate so they all can learn from each other. But now I'm gonna hand over to Paul, where, there he is, um, I can't see with the lights to talk about building um foundations. Hey, thank you Paul. Thank you James, thank you. So, um. Yeah, I, I'm gonna dive a little bit deeper into the well architecture side of things. Uh, James talked uh uh a lot about the, the cloud adoption framework. Um, uh, both of us work in support, so we spend a lot of time working with our customers around well architected solutions because the, the, it's the long-term goals that we're looking to achieve. Uh, and as James described before, operational excellence came in in 2016. Um, but before that, when we originally published in 2015, there were four pillars, and those four pillars were the security pillar because it's job zero, reliability, performance efficiency, and then cost optimization. And then, uh, back in 2022, we introduced sustainability as well. And, and we see these as the end to end approach in terms of how you think about your architectures in AWS and also we see these as uh complementary to each other. And one of the reasons why We placed operational excellence as the first pillar is because it's the foundation for a lot of what comes later and particularly security is made better when you have good operational procedures in place. So I'm gonna dive in a little bit deeper around operational excellence for the next wee while and talk through that. So Each pillar itself has its own set of design principles that we work with, and those design principles for operational excellence, there's 8 of them. Uh, and they're there to be those kind of non-negotiable components when you approach how you deliver your operations, your security, your approach to reliability, all of those kinds of things. Uh, and that they, they help you create that mental model for building out your applications and your workloads. Uh, these ones, uh, are based on laying those foundations for everything else. So things like implementing observability for actionable insights is really, really helpful because it's not just saying, uh, put monitoring in place, it's saying have something that provides business value. You're, you're building this out for a business reason, have something that can input back into the business on the back of it. Safely automate where possible, it's really a great example of what I'm human, I'm fallible, I've made more than one or two mistakes over the years, and if I can automate out my own fallibility, I will go about and do that. Um, until recently there were areas I was thinking that that we we're just never gonna be able to automate, uh, around those, and I've got a bit of a demo later on to show how automation has helped massively, particularly in the operations side of things. And that whole concept of making the small, frequent reversible changes, I, I think that really helps from a risk perspective as, as James was talking about before in terms of that, that whole risk mitigation and uh and the, the long-term approach. Uh, and constantly refining, constantly improving, always really, really helps with, with your evolving workloads, your evolving business outcomes, and your operations need to, to run with that. The anticipate failure one seems like a, a fairly obvious one to a lot of IT people, but it's, it's difficult to anticipate all kinds of failures. Uh, and there's ways of being able to understand what those different failures could be for your workloads, and there's ways of building up the muscle memories within your organization. I'll go, go through some of those examples a little bit later on. Uh, and those going back to the, the, the COE, the correction of errors process that Jane talked about, that's how we think about learning from all those operational events, updating those metrics. And then the used managed services piece is, well, actually operations is, is hard and complicated. Can we use this to take some of that operational burden away from your teams and place that on AWS that we're doing that on, on your behalf as part of our part of that shared responsibility model. Now some of the ways that we do that. Are by building the infrastructure on your behalf. So we currently have 38 regions around the world. Each region has the, the same kind of makeup. There are multiple availability zones in each region, uh, and they are a meaningful distance apart. So that's many, many miles is, uh, is the, uh, the, the long and the short of it. Uh, and each availability zone will have multiple data centers within it, and those data centers will be geographically closer to each other with super low latency connections between them. And very often you will have, uh, an instance running in one data center, an instance running in another data center. Center and you can't tell that they're not in the same physical building or same physical rack. So there's a, there's a lot of thought gone into the design for that so that we're we're taking on board some of the uh the the operational challenges there and but what that does is it lays the foundations for your architectures and allows you to think more broadly in terms of the, the different approaches that you can take. Now each of those regions have a control plane and a data plane wrapped around them, so each region can operate autonomously. Now there are a few services that have global constructs, er er so they're eventually consistent within a, a uh a primary region globally. That primary region also has uh failover options uh on the back of it for the, for the list of those services. Um, and we've got a really good document about fault isolation boundaries that explains that, and there's a QR link at the, uh, at the end of the session for, for that. But the idea behind the, the control plane versus the data plane is kind of born out of networking. It's that whole, uh, how do I control what's going on within my network, but still serve high quality data out to. The services that need to get that data quickly without having any of those conflicts in there. So the idea is that your control plane is where you handle the complexity of the different services operating with each other in the background, and the data plane is where you're you're moving your data around, where you spin up your EC2 instances, where you interact with the services that are run on there. Now when building out an architecture, we think in terms of shared responsibilities. There there's, there's a number of shared responsibility models across AWS. This is the one for resilience which leans into a lot of the operational outcome uh type situations. Uh, and the, the idea here is once again that we're, we're doing a lot of the heavy lifting on your behalf. We're making, uh, as many of the, the complex choices as possible, as easy for you to consume as you can. But there's parts of that that you still need to think about in terms of how you deliver your services out. And from a, an architectural perspective, one of the reasons why any business will uh be running in an application in the cloud is because you, you, you want it to be something that is trusted, that's reliable, that your customers can can consume in a consistent way, that you know that the that the business can, can really. Move forward with, uh, and that's why building out things like change management, uh, operational resilience, observability capabilities, all of those kinds of things are, are a key to your own success as part of the, uh, the, the building of your broader approach. Now moving your workloads into the cloud means that you've got quite a lot of different approaches that you can take towards resilience. So, um, everybody here got workloads in AWS at the moment or at at all, so I'm seeing a lot of nodding heads. Still got a lot of data centers that you're still having to operate as well? Few, few folk nodding, OK. Um So when we think about the, the whole er approach to resilience, there's a lot of terms that are conflated very often uh and one of the ways that we uh I I I see quite a lot is um the reliability pillar is conflated with the resilience pillar. Reliability is about that consistent set of outcomes, that consistent set of capabilities that you have that consistent, reliable set of engagements with your customers at the end of the day. Then there's high availability. So resilience and high availability must be the same thing, aren't they? But, well, actually, yes, you can continue to operate given something has gone down, but that's not necessarily the same as it it being resilient to a set of outcomes. And then you've got your operations themselves. And in the middle of all of that, actually that's where your resilience lives and that's where we think in terms of building out those architectures to allow us to to think about the, the, the longevity and the life cycle of the applications that you build. And that leans into an uh the the different architectural approaches that you can take. So when you've worked with the the data center type things, you can have that traditional enterprise failover type scenario scenario where you can say I've got a primary, I'm gonna fail over to the secondary, I've tested it, everything works, and I can fail back and it's all good. And, and let's keep on going. Now you can do that also in uh AWS in a region you can have um uh multi-zonal failovers. Now that's, that's an approach that you can use for a lift and shift for a legacy application that that follows that traditional pattern. But also as you modernize, you can be in a situation where you can say well actually I can be multi-zonal, I can be active, active all of the time. er and what that means is that I've got more options in terms of er survivability, so I'm making my workloads more highly available. But I, I also have a situation where I have a slightly more complex operations. So how can I automate some of these capabilities? So it's things like putting in, um, low bouncers, auto scaling groups, and spreading the workload across all three of those availability zones in this example. You can then have multi-region failovers, so you can say I've got a primary region, um, and part of our business continuity planning is that we will have a failover to that secondary region and we can continue to operate with uh with that approach. You can still have that high, high level of availability of service because you're operating across those three availability zones. But you still also have that option should, uh, we, we have an event that means that that region's no longer available for a period of time. You can move whilst the disruptions on and fail back when the time comes. Increasingly we're seeing patterns where it's multi-region active active as well. So patterns where you can say. Uh, I, I, I can distribute my workload, uh, and I, I test my BCP and my DR on a day to day basis by saying, well, I'm sending some customers in this direction and some customers in this direction. I'm, I'm moving my traffic around to accommodate where, where they are, it's closest to them, it could be a follow the sun type model. But either way, it's the, the situation is that you are testing your resilience across those, uh, regions as well. And as you get more and more mature in there, you can have different approaches to your architectures. So common approaches we we see cell-based architectures, anyone familiar with cell-based architectures? Any hands up for that one? There's a few nodding heads, so cell-based architectures are uh very much enclosed portions of your, your, your architecture where they can operate a, a portion of your overall workload. Uh, and if that cell has a problem, fails for some reason, that there there is an issue with a piece of code that's deployed and it, it only affects that cell. And the other cells around it can continue to operate whilst you're spinning up other resources in the background to to accommodate those kind of things. So cell-based architecture is a really strong way of thinking about those more sophisticated, really highly available, really, really resilient solutions, but also very, very scalable as well because you can start to deploy them across multiple availability zones, across multiple regions. Event driven architectures are about making sure that you're uh moving your workloads around in a meaningful way so that they they're following the events themselves, not necessarily just responding to the, the individual, you know, application flow as well. Um, and they're really good from a, a kind of a deterministic perspective, you know that when your workload is moving through, you know what it's gonna be doing, uh, and you, it lends itself really well to things like uh a surless workload and consuming surless services. But all of those things are are really, really fundamentally beneficial when you can test them well. And that's where the chaos engineering side of things comes in. um and I'll talk a little bit about that later on. Um This is an adage that has existed since we started AWS. Werner said that a very long time ago, I think in 2006. Um, anybody heard it before? Any hands up? You familiar, familiar? Yeah, there's a few hands going up, that's good. Um, and for a, from a, a pers the perspective of somebody who works in support, um, that, that's my day to day is, uh, the, the idea is. Everything fails all of the time. How can I make sure that when it does fail, we have a plan for it to, to work out the way we need it to from a, a, a business perspective. And one of the ways to think about it is, is trying to visualize what the different causes of a failure can be. Uh, and I like to think of them in terms of the five causes, uh, as, as, as we talk about them. First one is dependency failure. So you've probably all come across this before, you have, um, you, you've got an application that's running, something that you depend on is, it has failed, so it could be that it's a, uh, an upstream service that you're consuming from, it could be a database or a, a, a corrupted cache. It could even be a third party service that you're reaching out over the internet to and, and, and pulling data in. Something has gone wrong, but you need to know about it because it'll have an impact on your business. System and component failure, this is the one that I, I used to come across the most when I worked in data center world, and that's that, that's the, I, I, I think has broken, a hard drive has failed, uh, a motherboard has gone pop, the, the CPU is uh has overheated, that, that kind of thing is the, the, the components that they tend to think of. Um, but in the cloud you can extend that to, well. Uh, I do containers. Is it one of my pods in my cumulettis clusters has gone pop? Do I care and how do I handle that? What kind of data do I wanna get it on the back of that? It could be something as significant as, as a whole availability zone has gone. So you've got that, that component failure is that, that, that the access to that availability zone has been closed because of a major event or because simply somebody put in a firewall rule that that closes things out. It's that kind of thing that can occur. Then there are constraints. So it's the, the things that you bump into that, that aren't always anticipated. These constraints can be something as simple as, well, I'll ask a question. Anyone here ever had a certificate expire on them, um, and it caused a bit of an outage? Uh, yes, there's a few chuckles there. Yeah, yeah. Uh, it's that's one of the most painful ones because, ah, yeah. Uh, those are the kind of constraints that we can bump up to, a, a, a date, a time. It could be something as simple as you are, are growing your estate really, really quickly, uh, and forgot about quotas and asked to get more like EC2 in there or increase the, uh, the, the, the concurrency within your lambdas, those kind of capabilities. So being mindful and, and, and keeping on top of those kind of things is really important. Now traffic spikes are always a big one. I, I think of these in terms of, uh, some of your inputs have in cha changed within your environment, so, um, it, it could be, uh, something as simple as there's been a marketing campaign and on the back of that you've had a surge of traffic. It could be that. Somebody has consumed your service, they love it and it's gone viral because they're an influencer. That, those kinds of things tend to drive, uh, unexpected or unanticipated outcomes. So being able to stay on top of understanding why that can break things and how you can respond to it from an architectural perspective using, well architecture is a great way of, of, of visualizing what it is that you're looking to do and how you can go about scaling the architecture that you've got to. You know, anticipate those traffic spikes and demands, and there's ways of thinking about those failure scenarios that I'll go into a little bit later on as well. But in my experience, the biggest, the biggest one is always some kind of change has occurred. Uh, it's the uh code has been pushed and there's something wrong in the code. Um, the, the, the other one is, is config, uh, either, uh, that the config that you intended to push didn't have the desired effect or the ones that I, I've experienced a lot in the in. Pat is it's bled through from the wrong environment, so you've got a pre-production environment with uh a conflict that shouldn't be there in, in production. Those kinds of things are really, really, you know, impactful. So how do you go about finding out about these things from an operational perspective and how do you use that data to improve your architecture over time? So we think of that in terms of observability. Um, that the one thing that I would say is there's uh there's an observability workshop this week. I think the first one is this morning, I think there will be another one later on on Thursday. Um, if you're into observability, definitely go for the workshop because it's, it's a, a, a strategy building workshop. So it's not necessarily about the hands-on of how do I implement my, my metrics, my logs, my tracers. It's about how I build out my organisation's mental model for observability and what am I working backwards from, who are the personas that I need to care about. It's not always just gonna be about the metrics for a developer or the logs for an operational team. Uh, it, it's also about, well, is there a dashboard that the CFO wants, needs to see? Is that, does the CTO need to have the insights at that high level that they can then drill down into, uh, and how do you keep track of those things and, and get the, the right information to the right people. Increasingly though, these three separate aspects of observability are becoming a lot more accessible. Uh, so you're no longer having to mine just metrics and logs and traces to tease out the, the, the information from the data, because I think very often there's a lot of data that we're collecting. But having meaningful information on the back of that is really, really hard. So one of the things that uh we we talked about in the, the design principles section earlier was around uh safely automate where where it's possible. And for, for me, observability was one of those areas where that I, I didn't think you could safely automate where possible. You, you have your network team looking at the network stuff. You have your infrastructure team looking at the infrastructure. You have your application team seeing what's going on with the application side of things. But latterly with the advent of er Agentic AI that has changed a lot and you've got the opportunity now to be able to dive really deeply into those things. And I just wanna go through a little bit of a an example from Cloudwatch. This is er a Cloudwatch investigation. And with Cloudwatch investigations what you can do is you can say um we know that a thing has gone wrong, we can see that it's unhealthy. Let's let's have a little dive into what's what's happening here. We'll create the name of our, our er investigation. Uh, and then what we'll do is we will click investigate. Uh, and what the AI does on the back of that is it does all of those things that I talked about before. It will say, uh, right, OK, I'm gonna go off, uh, and I'm gonna go off and do that investigation. I'm going to be the network team, I'm going to be the infrastructure team, I'm gonna be the applications team, and I'm gonna pull everything together that I know about in terms of the context of the, the, the error messages I'm seeing here, uh, and I'm gonna make this a part of the investigation. Uh, and here's all the, the, the data on that, uh, and actually this is starting to bleed into the realms of here's all of the information. You, as the, the person driving this, you can go through and say, well, actually, uh, this bit is more meaningful to me than some of the other stuff. I'll accept this as a part of my investigation. Uh, there are things that I, I won't necessarily wanna be a, a, a big part of it, but ultimately I'm pulling in. And all of the information that I know is of value to, to, to my business. Uh, and on the back of that, what you end up with is a, uh, a set of insights that you can, you can dive into that's really summarized in a, a, a much deeper and more meaningful way. Um, so this goes through all of that information. Uh, on the back of it, you, you get all of the, the, all the key links of to where, whether in the logs you can see the, the, uh, different components. Um, but at the end of it, you can say I can accept or or reject that. Uh, but you can also say how do I get my hypothesis, and this is what this bit is. This is how the hypothesis has come out on the back of the AI saying, I, I, I think I have a good idea of what's coming out on the, uh, uh, as a result. Now this is broken up into a bunch of components. First one is the confidence. This one is a high confidence, um, hypothesis that we've got here. Then there's a breakdown of what what it thinks is going on. There are some possible next steps in terms of recommendations. You can actually dive into the reasoning behind it as well if you want to look at that. Ultimately you can accept or discard this hypothesis and and run through it again. Say I want actually I need to pull in a little bit more information, rerun the scenario. But the thing that I, I like about it is it gives you the information that you need. It's saying, OK, uh, going through all of this data, I've spotted that you, you've got a uh an IM role that's er problematic in there, and you get there really quickly. This is pretty much real time in terms of that. So you're saving a lot of time, you, you're taking that networking view, you're taking that infrastructure view, you're taking the application view, uh, and you're, you're distilling it down really, really quickly. And on the as a result you get a bunch of recommendations, uh, and those recommendations you can uh you can, you, you can implement them if you wish. The, the one that I like the most is this last one here. Uh, it's kind of uh the AI kind of wagging its finger at me saying, Paul, you should know better. Put proper change management in place cos uh I, I'm not seeing anything here that's uh that that's reflecting that kind of change management side of things. But how do you then take that kind of information and pump it back into your applications, uh, and make it more meaningful and much more useful. And in that regard, well, success is intentional, so. You can say, well, my untested architecture is not necessarily well architecture, so how do I go about testing these things? And as James said before, we've got this adage of there's no compression algorithm for experience. Uh, and unfortunately in my experience, um, experience comes in, in one of two ways. One is, oh no, I've done something wrong and, uh, it's, it's all fallen to pieces. And the other is, well, how can I gain the experience without that. Everything's gone wrong type situation. Uh, and the way we think about that in AWS is around chaos engineering and and continuous resilience assessment. And this is really useful from an architectural perspective because you can use this to, to take your learnings in terms of the the five causes we talked about before. Uh you can bump up against the edges of those types of scenarios and say how do I start to build out a mental model of what happens to my application as I go through different conditions because no matter what you design. That there will always be something that happens in your environment that you, you bump up against. So you, you create your hypothesis, you run your experiment, you verify it, you make sure that there's improvements in there, you get back to a steady state once you've done your well architected review, you have updated your architecture, you create a fresh set of hypotheses and you go back to running that experiment again. And in that scenario, we like to think in terms of you know, injecting entropy into your broader environments. Uh, stimulating hardware failures, simulating things that can go wrong within your software, um, things like the, the scaling events that I talked about before, uh, and pretty much anything that's capable of, of creating disruption within your services from that, that steady state being. Um, and what that will do is that will create the chaos engineering will go hand in hand with your resilience testing, and it'll give you a bunch of outcomes that are really, really useful. And a great way of doing that is running game days. Um, I'm a, a massive fan of game days. Um, and they, they, they're composed of a, a, a few really critical features that, that help you think in terms of, uh, of what the long-term success looks like. Game days are reasoned, you have a set of criteria that you're, you're setting out, so that's part of that hypothesis thing. You say, I want to prove that we can, uh, survive the loss of an availability zone or that we know how to migrate from, uh, one region to another. That, that's the, the, the goal that you're setting out ahead of you, and, and these are the things that you, you, you, you're testing your overall architecture against. They have to be realistic. It's not gonna be a, a, a thing that is, is not the data that you get on the back of it, you wanna inject back into your broader environments. You wanna use those as the learnings and then you wanna be able to say, right, OK, we, we discovered a thing. That thing we've fixed through an operational change or through an architectural update or through consuming a new service. Those kind of things will allow you to then say, right, we, we've got the right data on the back of this. Now we're, we're providing the business value by making sure we're avoiding future troubles. And they need to be controlled as well, so all of the outputs that come from this, they need to be collected. You need to make sure that you are saying, right, OK, we, we've we've discovered through, uh, a bunch of traces in our observability that, uh, when a certain condition is met, the application just, it, it falls flat on its face and we can't continue to operate it. Take that data, inject it into your observability, inject it into your operational uh outcomes, but also do a well architected review against the application with this in mind. Use the well architected lens tool, um, and use that to, to say right, OK, how can I focus in on an application that can survive this? What are the changes that we need to make to make it more survivable in a, a, a set of circumstances. And regular, these things need to be frequent as well, um, and why would they need to be frequent? Well, um. Uh, one of my customers does a DR test every September after everybody gets back from the summer vacation, and one of the challenges there is that everybody is back. So what they're not detecting is what happens in that DR test when a key person dependency isn't there. So somebody goes off on holiday in April and they're not there. They don't know that because they're only testing it in September. That key person dependency means that if they test it more frequently, they will unearth those kind of things. So that person could be sick, they could be um on vacation, somebody could get hit by a lottery ticket and they're no longer in the business. You wanna be able to pull in all of the right information so that your operations can survive those kinds of outcomes. And the center of a good game day as well is the uh a means of injecting entropy. Uh, for, for me that's using the fault injection service from AWS and there's a bunch of scenarios that you can use there to be able to say right, OK, how do I test, push the boundaries of my, my infrastructure architecture, my network architecture, the application stack itself. Recently there's been a couple of updates to uh the font injection service. Um, these ones are particularly of interest uh from the perspective of they simulate gray failure scenarios. So a gray failure is, is something where it, it's not necessarily obvious from the, the beginning. You have a failure of a, a particular situation, it can be quite binary, it works and it doesn't work. A gray failure is where you're in a situation where, well, you've got increased latency, the application is going slower. It's building up traffic in the background. You've got unhappy customers, you have scale events because you can't get the resources fast enough, so you're spinning up more instances. Um there's going to be a cost aspect to that. Using this kind of capability allows you to to look at the, the mechanisms where that would happen, what your applications, what your infrastructure, what your network will do. By seeing what that increased latency is within an availability zone, but also simulating it across availability zones as well gives you that insights that can to say oh well. We, we didn't expect that when the database was called across availability zones in these circumstances that the, the response would be so slow and a huge chunk of our customer base won't be able to get access to the services they're trying to consume. So being able to manifest those capabilities early on means you're saving the business time and money in the long run. The list of capabilities, the, the two that I just talked about there, the list of capabilities for font Jackson service is quite extensive. Um, I, I particularly like the um power interruption, so simulating the loss of an availability zone. So you can say, well, what does my application look like when we lose one third of our estate. Um, and how do we recover from that? How do we, how do we absorb the new resources in the other availability zones, or actually how do the people in our teams respond to it? Do our automations do what we expect them to do? Are we finding the right sets of outcomes that we, we need as well? And similarly these other types of situations are, are really good. So, so from a. Uh, like a, a Cubanetti's perspective, the EKS stress testing, really, really useful. You'll see what's going on with your, your, your different pods, how they respond. Uh, it'll also help you gain confidence for the use of services like Spot. So anyone here familiar with Spot compute? Any hands going up, there's a couple of hands. So Spot is the excess, um, compute available after on demand has been consumed. Uh, and the way that you access it is it's vastly reduced price, but if we need it in the on-demand pool, you have a 2 minute warning to say this is going away. In conjunction with state workloads like a Cubantis deployment or any. UTS deployment, it gives you the chance to, to be able to say, well, I can have a highly scalable environment at a much reduced cost. And one of the nice things about having the confidence to see what happens to your pods when uh when you lose an instant is you're testing your resilience. So using things like Spot will help you from a cost perspective. It'll also help you from a resilience and operational perspective. It helps you redesign your architect architectures of your applications as well to have that mental model of, well, How, how do we survive in all of the different circumstances? Doing things like uh game days with chaos engineering or fault injection have a bunch of benefits. They help with that automation, they help you determine where things are going wrong, where in your environment you can see. Uh, improvements and is there a process that's currently handled by humans that you can then say, actually I, I can now automate this and like before with the, uh, the observability, I didn't think we'd be able to automate observability. Now with the use of AI, that's a much more meaningful set of outcomes and a much faster set of outcomes that you can do. And as a result you improve that visibility, you get better insights, and one of the big things I think about the, the improved visibility is that the traditional mental model of monitoring and observability tools is that it's a sunk cost that you are spending money uh as part of an insurance policy in some regards. But what you can then say is actually we've taken the the visibility angle of this and we're creating business insights. We're able to see what's going on within our broader environments. And it helps with that improved recovery. Going back to the um going back to the, the game days and the, the chaos engineering. Did any golfers here? Anyone play golf? There's 11 hand going up, a couple of other, yeah, a few hands. So I, I'm, I'm from Scotland and the weather's terrible uh and everybody's obsessed with golf, but I, I, I'm not. So forgive me if I get my uh my pose wrong in a, in a minute. I'll give you a demo of it. There was a, a golfer in the 60s who um. He used to go around the professional court at the professional courses, and he would, he, he'd try and find the hardest shots. So he'd be in there in the bunker and he'd be, this is where my, my demo is not great, he'd be chipping out of the bunker, uh, onto the green and he'd keep chipping and it'd be onto the green and chip and a trickle in into the hole. And the apocryphal story is a guy came up to him and said, uh, you're the luckiest golfer I've ever seen. You've done that 5 times in a row. And his response was, well, actually, I found, the more I practice, the luckier I get. And, and I think that's the thing about the game days with regards to that improved recovery is the more you practice, the luckier you get, the more your ability to recover from different scenarios, the more resilient your application architecture becomes. And one of the other nice sides of it is things like uh font injection service provide you with compliance reporting as well, so you can actually say at the end of the day, here's the list of the things that we've tested, here's the list of the outcomes that we've delivered, uh, and we can move forward with assurance that we know that we can operate against a particular set of regulations or that our customers can, can say we meet the bar for, for their sets of criteria. But in the midst of all of this, Going back to the whole, the, the well architected side of things is you have that opportunity to drive improvements as you go. You use those game days and you use the fault injections that you're, you're putting into your environments to to create learnings. You improve your design on the back of that. You measure the outcomes from those and then you take it back to well architecture and do a review against those outcomes again. Then you start to to run those game days, you start to continue the the er with the learnings on the back of it as well. Uh, I said earlier, I provided a few, uh, QR codes. Uh, there's a couple there well architected in the cloud eruption framework. The fault isolation boundary one is a really good read, um. It, it's, it, it just breaks down the mental model that we have in AWS for how we build our regions, but also how we build our services in terms of how you can consume them. Uh, and that, that comes from the perspective of, uh, we've got global services, we have regional services, and then we've got zonal services themselves and how you build your architecture around those and applying the well architected principles to the consumption of them can make a really big difference in the long run. The Builder's library is uh a a library of best practice advice from some of the most amazing people in Amazon. Uh, the, the, the work in there is fantastic, and I just definitely advise reading through some of that. The solutions library is uh kind of a companion from that where there's pre-existing code and examples for you to be able to draw down from. And finally is the link through there to the injection injection services as well. So um I just want to close out now by saying thank you very much for coming along on a uh uh a Monday morning um and. Yeah, really appreciate your time. Enjoy the rest of Reinvent. I hope it's a fantastic week for you. If you could provide some feedback in the app as well, the only way we make these things better is by your feedback. So uh if you can give us a a rating in there and and some constructive uh feedback, it'd be fantastic. Uh have a great week and uh see you around. James and I will be out outside if you wanna have a chat with us at all after this, so thank you. Bye.