---
video_id: 6cuUx_KfZK4
video_url: https://www.youtube.com/watch?v=6cuUx_KfZK4
is_generated: False
is_translatable: True
---

OK. Hello everyone, thanks for joining our session. I actually wanted to double thank you guys because let's face it, it's the 3rd day of the summit already. It's Las Vegas. It's late evening. The happy hours are already started. We're already aware of that. I mean, as Lady Gaga says, you had millions of reasons to ditch us, but I guess you had your one good reason to be here, and that's migration, I guess. Um, by the show of hand, how many of you are from overseas? Do we have anybody from overseas? Welcome. Uh, I assume the rest of you are from the United States. Do we have any locals from Las Vegas? I mean this light doesn't let me anyways, welcome. I mean how many of you have planned your trips in person you did it yourself. Really, only you, so I guess the rest of you just had the luxury of an executive assistant doing the traveling for you. Good for you. I'm really impressed. I mean, regardless of who did this for you, there are a few things that you needed to clear out before being here. First off, you needed to know where you're going, why you're going, when are you going, how are you going? Um, how much money are you, are you allowed to spend? What's your budget? Do you need any accommodations? All of those answers regardless of whether you're planning for a fairly short trip like this or a rather longer journey, need some decent answer before we can actually have a meaningful and successful meeting. And today I Sosan Araullija, a senior AWS solutions architect specialized in, I mean, infrastructure modernization and migration. Along with my peers Mangesh Bukle and Audie Simon are going to show you how you can get data-driven answers to the same questions when it comes to your digital journey of migrating your workloads to AWS with the focus of Microsoft-based workloads. OK, first we're going to talk a little bit about a generic portfolio of what the typical migration would look like. We're going to talk about discovery, then we're going to go deeper into each categories of portfolio of workloads that you might be running on your environment. The demos are all blended. We're going to have an engaging session, so let's hit it. You guys here because most probably your organization is one of these organizations that still have workloads that includes the 70% of workloads that are still running on premises. Why do you wanna move to the clouds again, you could have a million reasons to move out there. The latest addition to this mix is embracing AIML in general and generative AI in particular. But we know that you need answers to the question that I brought up again, and we know that those answers they're not easily got. The main thing is resource scarcity. That resource could be human resources. It could be budget. It could be timing. It, it could be the skill set. We know that you need to be able to figure those out before you can have a successful migration. I mean, when you look back at the journey that people have already been been gone through, you see some milestones out there. First, I mean, in the late 90s and early 2000s, everybody was just running the workloads in their server room in their office. Then they decided to embrace the commercial data centers and the concept of co-location. Moving forward to 2005, it was the early days of cloud. People started to embrace more of what cloud had to offer. And we had different milestones moving forward and right now our milestone is Gen AI moving to the cloud for AI and by AI. Maybe in 202023 when you were thinking about Gen AI with your projects you were just thinking of having your prompt somewhere, getting some feedback, getting some recommendations from your model, and then implementing it manually. Then came 2024. Now you could get better, more contextual prompts to the, uh, AI model using systems like rag and augmented retrievals and more, uh, streamlined experience. But still everything was manual and now at 2025 we just we want the AI to not just tell us what to do we want to do things for us of course under our supervision and under our security guidelines and that's what uh agentic AI mean. And we're going to show you how we can leverage that for our migration project. I mean for a typical journey, one of the trivial questions that you already know the answer is uh where am I coming from you already know where you are but in a digital journey like migration that is actually a question that needs to be answered uh what are we migrating and why do I have. And that would be the concept of discovery. Our primary go to mechanism to answer that question has always been and still is this mechanism that we call optimization and licensing assessment or OLA as the acronym goes. Through this process, us as AWS along with our partners, they would, we would come to your environment, we'll scan your environment with our tools we see the relationships that you had with different vendors, and we would come up with recommendations including licensing recommendations from different vendors, whether it's a good, uh, choice for you to do to use the bring your own license scenario versus going and purchasing license included. These are the answers that we will have for you. If you go through the OLA. If you're interested in this topic, we have some resources that we're going to share with you towards the end of the session. The question is, is there an alternative way of doing it? And the answer is yes. I mean, you probably have already heard about this service that we put a lot of emphasis on, and it's called AWS Transform. It's an umbrella of services and modules that will help you. At each step of your way towards your migration, uh, it can actually help you with assessments as well. You would feed in your inventory data. Many of the common and popular formats are supported, but it's a generative AI. Most probably, uh, it, uh, if there's the format that you understand, most probably our gene AI will support, will understand it as well. So it gets your inventory data and based on the other supporting data that you possibly can fit into it, you will get uh some insight into it. Let's actually see what it works in action, how it works in action. So starting with the familiar face, your vCenter. I have a cluster of two ESXI hosts with a bunch of virtual machines running on them. They're hosting a plethora of applications with different environments. Like, for example, this one is a, uh, travel expense application with three tiers of. Architectural uh element into it. I'm going to use AWS uh discovery tool. This is our own, our own tool for discovery. The first time I'm going to run it, it requires a new password. I'm going to put my password here. This is the interface for it. The first thing I'm going to do is to, uh, give it, give it access to my vCenter environment so that they can inventory my virtual environment. The moment I type it in, as you see at the bottom, the agent starts scanning the environment, but that I want more. I want dependency mapping data as well as SQL versions that I can later on optimize, and this is why I'm going to give credentials to the guests via virtual machines so that they can go deeper and talk to the virtual machines themselves. Again, the moment that I do that, the, the database discovery and network discovery agents, they all kick in too, and as you see, you see, each one of them has different refresh cycle every hour for VMs, every 24 hours for databases, and every 15 seconds to be able to grab that network dependency mapping data. Already we got some information here. Let's see what we have for our inventory. This is the same environment that you saw, uh, through the vCenter. Some of them, as you see, they have errors because the credentials were different, but for whatever reason, uh, we can actually download the inventory in a zip file. If you look at the name, it says 28 days. That's how long we recommend the you run, keep running the store. Logging into AWS Transform. This is identity center. Plethora of identities, uh, providers are supported. This is the front page for AWS Transform, as you see at the bottom, we have some customer testimonies and some what's new on those topics. First thing we need to do is to create a workspace. A workspace is just a space that holds on to all the activities you need to do for a specific migration project. OK, we created our workspace. Now we go in, as you see, we have some suggested prompts on the right with the categories of migration tasks that you can that you can select. I can directly type in the category, the action that I wanna do to the model and it will open up a proper job for me. There it is suggesting I accept this suggestion for the job names and all that, and here's where the actual action happens. Humans in the loop now it's asking me for the inventory data that I got from the previous step. I'm going to upload my zip file. And I'm going to choose my destination region as the possible target for my migration. And hit analysis. Uh, while it's processing my data, I can see that I can see what the model is doing through the workload. Uh, it's, it's showing, showing me what the model is doing. Uh, we have the artifacts, everything that we fit into the model, as well as whatever the model generated for us will be in the artifacts. Um, there are some collaboration features in within AWS Transform, and of course the main panel will be the job panel. Each step that is done, there's a check box. The yellow means human in the loop, and this one is still working. While it's working, I can ask for some, for anything for a status object, for example, in this, uh, example. Um, it's giving me some example. That these are the actions that have been done. These are the left ones and of course in the middle this is done. Let's see what this assessment has provided for us. The first thing is a PDF report of an executive summary pretty much of what is our. Uh, prediction of the savings we'll get on the cloud, what are an overview of our recommendations, all that in a nice human readable format. You'll get the same information as part of a PowerPoint presentation as well that you can use as a basis for your presentations if you're a partner to your customers or to somebody in your hierarchy. All the data is already populated there, you can use that. And the meat of our uh assessment is the Casela spreadsheet here. You would see that all those inventory data that we fed into now we have a line by line analysis of them, the server name, the storage attached to them, uh, the specs, the recommendation that we have for you on which instance like to move it, and of course estimate of each one line by line with the separation of licensing versus compute. All those will remain in your workspace even when your job is done and you can come back to it and use these artifacts again when uh you're creating other jobs for the next steps of your migration as you see here the the dependency mapping and all that that we can use them later on for other steps of our migration. Great. Now we know that we have the, we have discovered our environment and we know that our workloads most probably fall under one of these categories with regards to specifically Microsoft-based workloads, servers and VMs, anything generic, you think about your, uh, infrastructure servers as well as generic application servers. If you're a Microsoft shop, 95% of chance that you are using Active Directory as your identity and access management. SQL databases has always been the uh database of choice for Microsoft-based development. Same thing with .NEET, but when it comes to Windows and web applications, you might already be on your pathway for modernization, maybe through some some containerization features that you see out there. And the file storage. For those of you who are familiar with our terminology, we have this concept of 7 R's of migration. The real ones are those that are highlighted, of course. Re-host, just lifting and shifting it to another host, re-platform using the same technology but maybe offloading some management elements to a third party provider in our case AWS, and refactoring means just decomposing the application and creating a new to be able to grab as much of the cloud native features as possible. Now let's dive deep into each category. First one, servers and VMs. By the show of hands, how many of you are not using VMware? That's what I, oh, actually we have 2. That's great. OK, so for the rest of you who do, we have plenty of sessions covering this new service that we have called, uh, Amazon Elastic VMware Service or EVS. For this session it's uh it's just enough to say that we will give you the ESXI ESXi host in your account and then you can use the technologies you're already familiar with to migrate your via virtual machines to AWS. We're not going to go deeper than this into this. The generic service that we have for migration and for the lifted shift, it has been around for a while and it has been called application migration Service or MGN. This is a service that installs a little agent on your on your source servers, then it transfers blocks of zeros and ones to your staging environment in your own account. When you're ready for the cutover, either test cutover or a final production cutover, we will hydrate EC2 instances based on the launch templates you defined for each server. And then you'll have a replica of your on-premises servers running on AWS. There is a minor revision to that as well. In that case, and for the on-prem environment, if you're using VMware, you don't need to install agents individually on each server, but rather you rely on a central appliance to do the replication for you, and we rely on VMware's snapshot technologies to get those deltas of zeros and ones to us. Regardless of which one you would use, MGN is just the engine. It, when you're doing the migration at scale, you need to be able to orchestrate, to the wave planning, and make sure that the dependencies are met. So far these were the services that you would be using to track your, uh, scaled migration project. But now we've taken all those experiences and put them in. The AWS transform for VMware migration category and our mighty Mangesh here will go deeper into that. Thank you, Sassan. Hello, everyone, Mangesh here. So let's continue the server migration journey. So we are living in an artificial intelligence era. So speed matters and how we work. Large migrations are complex, takes time, it's error prone, and require a lot of efforts and coordination. So raise your hand if you believe the agent ETI has the potential to transform your traditional migration processes. Wait, you're right. With that, I'm excited to talk about our first agentic AI service, AWS Transform for VMware, which simplify moving your servers from your environment to Amazon EC2. With minimum human oversight, using our goal-driven specialized agents, which we have built with the 70+ years of our experience. This new UI provides you. More flexibility and AI for share driven experience throughout your migration process. So let's see how it works. So flexibility starts with You're onboarding the transform using your own choice of identity, whether it's OTA, Intra, or maybe AWS identity Center. It has a key four phases, starts with the discovery, where you can provide your VMware information using your choice of tools, whether it's RV tools or maybe if you're using a partner like a Cloudamize or Modalize IT or the Sasan already explained our tool called AWS Transform for Discovery tool, that is what we recommend you. Once transform agent has this all analysis of your environment. Then that agent will group your servers into different waves and applications. Now, you can work with the agent to guide them, to group them based off your business or technical constraint, like example, application, owner base, functions, department, or maybe OS or IP subnets. Transform agent also understand your source network and it translates that into AWS specific networking components like VPC, subnet, and more, and provide you with the cloud formation and CDK template which you can download, make the appropriate changes as you want, and you can do your deployment. If you're using any third party tools for the firewall like a Cisco ACI or maybe a FortiGate or Palo Alto, which is normally customer used, or maybe NSH driven your networks, you can import that information in Transform and for the more context. Transform agent also give you the different design options when you deploy, whether it's a hub and Sport model or it's an isolated VPC and now it supports the multiple flexible IP address mapping, along with the different multiple accounts deployment, and that is very much required for large migrations. Once transform agent has this all networking information and waves and planning and everything, it goes to the next phase, which is a migration phase, where it will orchestrate end to end your Uh, migration using behind the scenes AWS MGN service which Sassan explained. Now, Transform can migrate your servers not only from VMware, but if you have a Hyper-V, uh, KVM or maybe your physical servers, it can do that for you. Important thing is about human in loop. It's very important where it creates that balance between end to end automation and human oversight, so that you can provide the user input on every critical task throughout this migration process. And it creates that uh uh unified web collaborative experience so that your team's enable to do the communi streamlined communication, resolve the blotters quickly. And avoid these coordination delays which normally comes across all large migrations. Now, with that, do we have any alternative method to do the large migrations? So as I told you, large migrations are complex, time taken, but also large migrations. Communicate with different data sources, different services, tools. They have a number of pre-migration tasks, post-migration tasks. These are the normal scenarios actually. So, how do you overcome that? We have already one proven service called Cloud Migration Factory, which customers are using to migrate thousands of servers at scale, at a time, right? But the question here is, can we take this to the next level, considering we are now living in an AI era? So how it works, how we work matters the most. Example I'm saying Can I use AI agent 2? Draw ask AI agent to analyze my flow chart, migration flow chart, which I have in my mind, right? And ask agent to analyze this, work with the cloud migration factory, and create these all pipelines and automation jobs to do the end to end migration. And I will be as a human expert, will make sure that the agent is doing right. Yes, it is possible. With Qiro CLI as an agent, now in this case, my migration agent assistant can work with cloud migration factory to create all these pipelines and everything which I draw in my picture as a migration, right, which I want to do with the help of MCP server, which is a model context protocol server which has all the required tools to communicate with the cloud migration factory, and cloud migration factory between, if you see the automation engine. That is nothing but water. That is already pre-built automation using MGN. So you can understand MGN services now so critical, which Sassan explained. It's used in all our services. It's a behind the scenes. Can you take this enhance more? Yes, you can take the advantage of AWS transform now, which I think Sasan explained as an assessment to do the assessment for the large data centers, or you can use the wave planning and everything out of that and then import that into. A cloud migration factor to consider. Now, you may have a question, what about the pre-task and pro task, right? So you can take the advantage of our Amazon Bedrock agent core, where you can build your own migration agent to handle the task. For you, example, ticket creation, Jira stories, or maybe after migration DNS updates, or many more things with your email communication. All these things you can handle through our secure platform agent core. So now let's see this in action. Yeah. So this is the WordPress application which we want to migrate. Yeah. This is just a servers, multiple servers. One of them is obviously web uh uh WordPress servers. This is just on-premises to AWS. This is the cloud foundation factory which I already deployed to save the time, but this is a blank factory. If you see, it's a 0.0 servers, 0 applications, everything. I launched my Quiro as my AI agent, and now this is the MCP's tool, which is having all the tools for the cloud migration factory, right, which it will use. This is the agent core where I have built migration agents, like, for one for ticket creation, one for email communication, and this is the flow chart I was talking about, like, there are some manual tasks, automated tasks, and simple flow chart, starting with the ticket and ending with the notification in between migration steps. Now, this is the prompt I gave to AI agent. Go and analyze this JPG file which I have shown you before, and do the job for me. Create the pipelines, work with the cloud migration factory and work with that. So that is what exactly now it is trying to do now. It is creating that all pipeline, behind the scene with the cloud migration factory using MCP protocol. As you can see now, it is doing that. And then it will also ask you for the inventory of your servers which you can take from Transform or your own inventory in the CVS format, you can give it to that. In our case, we have a 3 applications, uh, 5 servers and 2 waves created. So if you go on our cloud migration factory UI you see that 2 waves, 5 migration and uh 5 servers and 3 applications. This is the UI, but you can ask the status from iro also, like agent. What is my status of the migration? And now you can see that it will also try to create a wave one, because that is the one I'm showing in the demo to migrate the WordPress servers, right? This is the UI of the cloud migration factory where you can control the things also, but we want to control through AI. So if you go back now, yes, you can ask, what is the status of my migration. It will go, but if you see that migration has not started, why? Because there was one manual process previously, that ticket creation. It's asking, provide the ticket. So it's understand manual versus automated. Now we are providing that, yeah, I have a ticket. Now it's going to go all follow my flow chart and do the migration for me behind the scene using MGN. So if you see now, uh, in the UI uh that this is install the replication agents behind the scenes, started the block level migration, and step by step, it will follow all the process, test instant creation, test, and then cut over, and then. Deploy the production instances. All it will do for me, for example, if, if you see there were 2 new migratory servers are there. And now I can just ask, what is the status there? Everything looks good. It's demo, but there may be failures also you can ask, uh, possible. Now you can see all the pipeline here. Again, the main motive is what we don't want to do multiple UI's cloud migration factory, uh uh then uh uh MGN. I just want to check everything. You can see notification and I got the notification. This is the application from new migrated server, and this is overall status. So this is how fast you can work now with the AI, right? You can ask these questions and in this it's a uh everything works fine, but in case, reality, maybe it it failed, it will not be able to install the agents. You can work back and forth with the AI agent and ask why it's not, uh why it's failed, and you can fix it using natural language communication. That is the main intention of TED. So again, we have covered how you can do the server migration. We have multiple ways to do that. I will suggest work backwards from your own requirements, whether it's a technical and business, and see what solution best works for you. Example, AWS Transform, maybe Cloud Foundation Factory, or maybe our partners and services tools. OK. So let's see now, change the topic to. Core Service Active Directory. How many of you are using Active Directory? Almost, maybe all right. This is the most used application across industry, and a very critical application, we are thinking about the migration. So let's see what are the design options we have. We have a native 3 options. On AWS The first is a self-managed ECT domain controllers. When I say self-managed, it means in this model, you are responsible for managing this domain controller, whether it's managing, monitoring or maintaining uh throughout the life cycle of the DCs. The key point on this is, if you, you can extend your own domain to my EC2. If you have 50,000 or maybe 100,000 users, you can extend that easily with the AD replication and start giving authentication to your migrated application. This is the fastest way you can start doing and most of the customers do this. Second option we have is uh AWS managed as managed. This is managed by us. We deploy Active Directory for you and behind the scenes domain controllers, so you don't require to worry about the undifferentiated heavy lifting of the domain controller, we manage that for you. And this is an enterprise ready service, so it comes with the seamless integration with AWS applications, maybe uh enhanced features like uh directory sharing or replication and everything. But the key point on this service is, when you deploy the Active Directory, you deploy a single for a single domain new, new single for a single domain. That means what? You cannot extend your existing domain to the managed ID. So if you want to provide the authentication, you need to create a trust relationships, like one-way or two-way trust relationship, depends upon your use case. Third is an active AD connector. AD connector is not an active directory, this is like a gateway, or maybe I will say a simple term, it's like a bridge between your on-premises Active Directory and AWS specific application which supports the AD connector to provide the authentication and lookups. This is 1 to 1 relationship, so you may end up deploying multiple AD connectors, just to make sure that you use the unit's service account to avoid the blast radius issues. Now these options are used by our customers all the time. I, uh, across the segment, whether it's a government, education, uh, finance, uh, life science, healthcare, uh, I'm supporting these customers to deploy the Active Directory from long time and we get the feedback. They are looking for a solution, managed solution in AWS which is a flexible and easy to maintain, and also get the managed experience like the managed daddy, but they can be able to extend their own domain to manage Day, which is not possible right now, right? So that they don't require to do the trust relationships or maybe actual migration from their own domain to manage Day, which is a big project, as you guys know already. It's so difficult to do the AD migration. With that, as you guys know, most of our services and features come from your feedback. So I'm excited to talk about our new service, AWS Managed Ready Hybrid Edition. Now, this is, in this service, you can extend your existing domain. To manage data hybrid edition, so that you can take the advantage of managed experience because we manage the domain controllers this for you, right? And uh uh Same thing like the AC 2, you can quickly start giving the authentication to your migrated applications. How you design this, the, if you have a domain controllers on-premises, you can just uh extend that as an additional domain controllers on managed ID just to make sure it is on boarded with the system manager is one of the prerequisites. If you have an EC2 domain controllers, you just extend that now to hybrid ID. So, but in this case, hybrid ID domain controllers, which is your domain only, is managed by us, but EC2 and on-premises is managed by you. So it's a co-management now between us. So let's see this in action, how this works, actually, new service. So if you see now, these are the two domain controllers I already migrated from uh my own premises. This is onboarded with the system manager. This is the important thing to notice, the OU structure. You can see this is your OU structure, right? And these are self-managed domain controllers by you. This is the secret we need to create because we don't, we need some access to your domain to add the additional domain controllers, so we created the secret. And these are very specific name secret. This is documented in our website. These are, needs to be the same names. And once your secrets are there, your domain controllers are on boarded with the system manager, now you see the new option here in the managed hybrid edition. Along with the normal hour managed. Provide all the domain information in, uh, DNS, uh, networking is very important behind the scene. Provide all that. Now you can see the two domain controllers which you have provided, which is minimum requirement. Minimum 2, you need to providers to use that for the additional domain controller deployment. And everything looks good, but you cannot install, you we are not installing hybridated directly. We are creating the assessment. Why? Because we need to make sure that whatever domain controllers you have provided to us is healthy, working fine. So behind the scene, we are going to check 38 tests we are going to run, which is very simple, 80 specific tests like a resolution, replication to make sure they are healthy. And this assessment we are going to use successful as is another prerequisite to create the hybrid domain controllers. And now you have to provide the service account. You can see the service account which we have created, and then once everything looks good, all the information is good, now you can say that create the hybrid directory. So, behind the scenes, what we are going to do is now, we are going to deploy the domain controllers for you. In our account, obviously, and just going to manage by us, and you can see now IP hyphen. So the IP hyphen is domain controller deployed by us for your domain, but managing and the OU structure. You can see now new to you, new to OUs are added, AWS reserve. So that is what exactly managed by us. All other is your domain anyway, and 2 domain controllers by yourself and total, if you see now, there are 4 domain controllers. So we, you can see the 2 are managed by you and 2 are managed by us under the co-management with this. And if you go to the actual uh directory and see that, you know, it all looks like successful, everything, and the assessment is also successful. If you click now the domain, you will see this is a managed kind of experience where you can see the manage application management and all that. So you get that managed experience now. With that, I will hand over to Adi for the next journey over. Mm. Thank you Through Active Directory. Now, as part of customers migration journey, the next workload that is most commonly talked about our database and application. Can I get a raise of hand if you run Microsoft SQL Server today? OK, great. Most of you. All right, so my name is Adi. I'm gonna talk to you about your database layer and application layer. Let's get started with SQL Server. OK, now. Let's try to understand what are your options to run SQL Server on AWS. The first option is re-host onto SQL Server on EC2. Now this gives you the most flexibility in terms of SQL Server editions, SQL Server versions, and you have a very familiar admin experience and full control of your SQL Server. Now you can choose license included or BYOL if you meet the eligibility requirements, but full control comes with full responsibility. That means you are responsible for managing everything on ECI, except for the physical infrastructure. So, what if you're thinking, is there a way to offload this operational overhead? The answer is yes, you can re-platform to our managed database solution, which is Amazon RDS for SQL Server. With this, you get the managed experience, we optimize the architecture for you. AWS automates the patching backup. You also get the high availability if you opt for multi AZ and license is also included in this option. We also have a third option called RDS Custom for SQL Server. This is where if you have a use case where you need to access the operating system to install things like third party agents such as your security agents, your monitoring, blog forwarder. This can be done with RDS custom. Now that you know What is your destination when it comes to SQL Server on AWS? Let's talk about the approaches for you to get there. What are the migration approaches? I'm going to present to you 4 very common approaches, and the first one is to use MGM Application Migration Service, which Sassan and Manguest have already talked about, to do a full lift and shift from your SQL Server, be it on-prem or on other clouds, onto SQL Server on EC2. This does block level application, so that means everything. Including your database, operating system, data, configurations, they all come along onto EC2. Very simple, good for lift and shift, good for individual database. The next approach is going to be very familiar for all of you database administrators out there, native backup administall. So with this approach, what you do is you back up your secret Server as usual, then you upload the backup file onto Our storage services such as Amazon S3 or FSX, then you restore them onto either EC2, RDS or RDS custom. But one caveat of this is that you've got to take into account downtime because backup and restore undoubtedly involves some downtime. So what if you want um uh an an approach that minimizes the downtime? Well, if you run a clustered environment today and you're taking advantage of building high availability features, such as you're always on availability group, failover cluster instance, lock shipping, what you can do is you can spin up an EC2 instance with SQL Server on it, and then add that as an additional node in your existing cluster. Let the nodes synchronize all of your data comes along, and once you are ready, you perform a controlled failover on your source database. Finally, when you're ready, you can decommission your source database. So this is great if you run a clustered environment, but this only works if you're coming onto SQL Server on EC2. But the upside is this can also double up as your disaster recovery approach or solution. Now, finally. The one that gives you most flexibility is DMS. Database migration Service allows you to achieve continuous data replication. CDC change data capture from your source database to any of the options that I've just talked about. So you can come onto EC2, RDS RDS custom with minimal downtime. So this is great for highly available applications. But when it comes to database migration, it goes hand in hand with modernization. If you run SQL Server today, I would guess that you would run it on Microsoft's Windows Server. Is there a way to save on licensing costs, at least on the on the Microsoft Windows Server side? Yes, there is. With re-platforming assistant, this is a scripting tool. It allows you to export your SQL Server on Windows Server and restore it onto EC2 Linux, so you save on Windows licensing costs. But SQL Server itself is still quite expensive in terms of license. So what if you're thinking about moving the database, modernizing it to open source alternatives such as MySQL or more commonly Postgrad SQL such as on Aurora. You can use database migration service to migrate the data, but before you do that, you need to convert the schema, and we have a tool for that as well. As part of DMS we have SCT schema conversion tool that allows you to convert your SQL Server schema onto the Postgrad SQL equivalent of or any database of your choice. But up until now, you have to convert this manually using SCT, then orchestrate DMS manually, and what about your application code? Now modernizing database without touching the. Application is considered a breaking change for your for your application, right? So you're gonna make sure that you tweak your application to make it work with Postgrad SQL. Up until now, you have to orchestrate all this manually, but with that, I'd like to introduce to you AWS Transform for SQL Server modernization that allows you to use the generative AI to orchestrate and streamline all of these proven migration and modernization tools to modernize your database and application layer from. Microsoft SQL Server to Aurora PSquare SQL. OK, let's see this in action. So as you saw from Sassan's demo, the entry point to AWS Transform is to create a workspace, right? And you can create jobs within the workspace. The job that we are interested in is under this category called Windows modernization, and we're gonna need to select the SQL Server modernization option to create this job. Once the job is created, you can see that on the left-hand side, you really got a guided experience all the way from prerequisites to um to the actual. Wavefront. This is an interactive experience. You can converse with the agent and if you look at the prerequisites, it gives you clear guidelines. Now for AWS Transform to work with your database, it's going to need to access your database. It's going to need a user for it to use to log in. So this is our source database. We have a few tables and we have a user here. This user is going to be stored. Secrets manager and we're gonna tag it in a very specific way so AWS Transform can work with it. Within our database, we have a few tables, as I mentioned, and if I do a select statement here, you can see that we have some data in our table. On the application side, this is an entity framework, a secret server provider, and this is a web application that resembles a bookstore. OK. Now, back in AWS Transform. Um, in the prerequisites, it tells you how you should tag your secrets that contains your database credentials. So make sure that you tag it exactly. This is documented on our website. And as for the code, make sure that you have got code connections ready, set up onto your repository. This can be GitHub, Gitlab, Bitbucket, Azuro DevOps. Now, establish the connection to your database secret and the code connection from AWS Transform. Once you approve that, it will make sure to validate that all of the access is OK, IM roles are OK, and that it will move on to discover your resource. As you can see, it has successfully identified that there is one database through the secret that it can access, and it has also accessed and found successfully the repositories that is associated with this database. OK. Now after it's done, then it's going to move on to the assessment phase where it's going to look at the complexity of your database and application. So it's going to look at all of your database objects, determine its complexity, look at how your application accesses the database, making sure that it's able to do the wave planning which comes next. After the assessment is finished, you get a downloadable report. One of the aspects that you can download is the CSV file that contains a summary of what you have in your database. A well-formatted PDF report is presented to you as well. On the application site, you see your repositories, complexity rating, and high level lines of code count. Once that is OK. You will move on to the wave planning phase. I've only got one application and repository here, but if you've got multiple, the wave planning means that you will actually do application and database grouping. So make sure, human in the loop, make sure that you review this before you hit OK, because once you do so, it will move on to the actual wavefront. The first task in the wavefront is schema conversion, and you can get transformed to create a new database for you. Or you can select an existing database as target as we do so here. Once you confirm that this is the database that you want to write to, hit OK, and under the hood, this will orchestrate schema conversion tool to convert your schema from SQL Server to Aurora Postgra SQL. Now if you head over to PGSQL on the Postgrad side, we now have our bookstore schema, and if we expand that, all of the tables that you saw on the Secret Servers are now present here. But if I do a select statement, there is no data in these tables yet. Which is natural because that comes next. Data migration, 2nd step, and you can also ask AWS Transform to orchestrate data migration for you. What they will do is under the hood, instead of you manually having to create tasks in DMS, it will create and configure the DMS task for you to do the data migration. Once the data migration is ready, and if I select this table again, I will now see all of the data that looks exactly the same as the data that we have in the SQL Server site. What's next? Code, right? So it's gonna need to look into your code, it's gonna scan your your code, do actual changes to your code. It's gonna um uh uh write it into this new branch that you can specify instead of overwriting your existing code, right? So it's gonna change the entity framework classes to make sure that it works with the new database. Uh, if you have embedded SQL code in your application, it will change that as well. It will swap out the connection string to make it postscript SQL connection string. It will also validate the application. It will also resolve any build errors. If you have unit tests, it will run the unit test for you. Once that is done, as I mentioned earlier, it will write the results into this new target branch and you get a nicely downloadable uh uh downloadable report. This is quite frankly, a detailed report. You can go project by project. You can even drill down to individual file changes within each project as part of this, um. As part of this report. Now, let us switch over to our code repository. You can see that we now have this new branch created by AWS Transform. If you take a peek at the code changes that it has done, we can see that it has made all of the necessary class mapping from the entity framework side. If you scroll down, you can see that it has actually swapped out the SQL Server provider with a Postscript SQL provider. And it has also swapped out the connection string and links the connection string. So if you externalize your connection string, which is best practice, it will find those and it will swap out the secret for you as well. On the application side, before I show you the application, I'm going to make a tiny change to the data on the Postgrad SQL site to say that this book title now says 1002, instead of 1001 that you will see on the Secret Server site to prove that the application is indeed talking to Postgrad SQL, right? So, if I start this application now and browse through the book, it should now say 1002 instead of 101. So this application is definitely talking to Pore SQL. Now. Database, they don't exist in isolation. They go along with your application. So let's talk a little bit about your application. If you're a Microsoft shop, you might be running some legacy .NET framework application with a mix of more modern cross-platform .NET applications. Most likely you run this in Windows Server, maybe some Linux server, and you might have already started your journey towards containerization. You might have already started using this container. Technologies. So what are your options to run this on AWS? If you need full OS level access, you can run them on AC2 or elastic beanstalk. If you need automated patching, automated scaling. If you're on VMware, you can relocate your entire estate to EVS like what Sassan have already touched on. Now, if you want to run containers, we have a lot of options. ECS, which is quite simple but powerful orchestration platform. EKS if you want to run Kubernidis. If you run your containers on Redhead OpenShift, you can continue to run it on AWS on Rosa, Redhead OpenShift on AWS. If you're on VMware Tanzu, you can run VMware Tanzu on Amazon EVS. Or you can also refactor to serverless event-driven functions such as lambda. So how do you get from the left to the right? Of course, MGM, but it's less common when it comes to application. If you have access to your source code, you are most probably going to use your CICD pipeline to deploy to these environments, and code pipeline and code deploy can facilitate this for you. Now what if you want to use this opportunity to modernize as you migrate? Let's say that you want to containerize, re-platform running on containers, or you want to refactor, rearchitect to embrace the more cloud native architecture. This is what traditionally gets quite complex, because you have to manually orchestrate each one of these tools that solve one piece of the puzzle. Porting assistant for porting, after to container containerization. If you want to break down microservices, you want to use another another tool. So now we have AWS Transformer for .NET that is really designed for transforming .NET framework application and making.NET Linux ready. So this In combination with AWS transform for sequel modernization, gives you a full stack modernization capability that is assisted by generative AI. We also have Q Developer and Quiro, which is our coding assistant with agentic AI capability. Que Developer is available as an extension in most of the popular IDE, and Quiro is a full-featured IDE with advanced agentic capabilities such aspectrin development and agent hooks. So with that, I will hand it over back to Sassan to talk to you about the next workload. Thank you, Audie. Well, the last but not the, uh, least important one, the file servers. What are we going to do with them? I mean this section will be morally a review of the options you already have. First off, self-manage. Bring up your EC2. Make sure that you have proper tiers of EBS and the size of the EC2 that will support your, um. Throughput and uh transfer rates and the IOPs and you'll be good. You want to offload some of its management tasks to us, you would go to the FSX system. If all you need is SMB with Windows under the hood, then SMB, uh, I mean FSX for file servers is the service to go. And if you need to go beyond that, meaning NFS protocol, ISKZ, tier storage and controls as such, then it would be Amazon FSX for NetApp on tap, the service that you'll be looking for. How you can migrate to them you already have uh quite a few options. We talked deeply about MGN for the lifting shift. Depending on the target that you want to use, you might actually be able to leverage some of the native services in Windows environment, for example, DFSR, it it has some integration capabilities with our FSX for Windows system. Or if you want to go to EC2, your other option could be the storage replica component of Windows servers. I mean, and if it's actually a very small one, you can simply use robocopy or use a third party system and just do a manual copy. If it's too big to do it over the network, ship it to us from Snow Cone all the way to Snowmobile. We ship you the device, a secure device. You loaded the data, ship it back securely to us, and then we will put it on Amazon S3 for you. And from there you can decide what to do next with your data. And of course the most straightforward service that we have for grabbing all this data is AWS Data Sync. Depending on what your source environment is, when what Access Protocol it supports SMB, NFS or S3 compatible APIs, we would get those objects and we will sync it to our back end and then depending on the target storage type that you want your data to be, we will just put it there for you. Really quickly before we go to this summary, we just wanted to make sure that, uh, you know there are some programs and incentives available to you. Discuss these with your account managers to see if there are ones that you're eligible for, but they are there and we want you to be able to leverage them for your migration projects. Overall our whole purpose for this session was to make sure that your takeaway is that your questions might remain the same, but now, given the new tools and generative AI based tools that we have, we have answers for you, and we're by your side as a team of solutions architects, account managers, CSMs, and the, and the wide network of partners that we have. We're here to help you, uh, make your migration journey easier. Specifically you saw that how you can now offload some of your management uh to us for with the hybrid Active Directory uh service that we have out there. You already saw a wide, uh, a demo of wide capabilities that AWS Transform provides for you all the way along. And of course we just talked about the programs and incentives. If there's only one slide that we want you to take away from our session, it would be this one. Here is an exhaustive and extensive resource of, uh, list of resources that will go deeper into every individual subsection that we talked today. Uh, we didn't dive deep into, uh, VMware or MGM, those topics we have some resources. Some of them are from this reinvent, and right now they point you to the catalog to for you to attend. And later on when we have the sessions recorded on YouTube we will update this page to have the YouTube recording of of all these footages along with this one of course and you'll see some blogs, some technical notes, some training materials through our SkillBuilder so all of those we've combined here and we want you to bookmark and go deeper into them if you're interested. And with that I wanted to thank you again.