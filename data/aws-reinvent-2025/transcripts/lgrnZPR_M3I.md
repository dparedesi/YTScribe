---
video_id: lgrnZPR_M3I
video_url: https://www.youtube.com/watch?v=lgrnZPR_M3I
is_generated: False
is_translatable: True
---

Uh, welcome everybody. Thanks for attending our talk. I'm Ryan. I'm a sales engineer at Chronosphere, and if you don't know, Chronosphere is an observability company. Uh, we're focused on open source data collection, performance and reliability at scale, uh, and control over your telemetry, only paying for what you need. And recently we've had a lot of success at AI companies and in AI use cases. What we're gonna talk about in this short talk, uh, is an intro to AI observability, the different patterns, um, use cases we're seeing, um, and kind of how to use observability to prevent some pitfalls. Not going to be able to go super deep into product demonstration, unfortunately, but definitely encourage you guys to check out the Chronosphere booth, where we're demoing our AI guided capabilities, um, and some of our other new features there as well. So first, before we get into the use cases themselves, we're gonna kind of overview what we're seeing from a market perspective. So from our perspective, we've broken the market down into 4 core buckets. We're seeing model builders. These are people who are building foundation models that everyone else is building on top of and consuming, GPU providers who are tailoring GPU infrastructure, uh, around AI inference, model training, and fine tuning use cases. AI natives who are building products from the ground up around AI technology, uh, and then our feature builders who have existing products and capabilities, where they're adding AI functionality, um, into those existing product lines. And across the board, one thing to highlight, observability has always been hard, uh, continues to be a struggle, and AI is just adding complexity, adding a layer on top of that. All of your existing large scale cloud native problem patterns definitely still exist and are at the core of AI observability use cases. Going into a little bit more depth, um, on those challenges, what we're seeing in existing large cloud native, uh, workloads, massive scale, really mission critical reliability, high performance, uh, a lot of troubleshooting complexity across distributed systems, observability costs and data volume control, and then managing cardinality as your infrastructure changes. Some of the new AI specific challenges we're talking about are actually around model behavior, making sure the model's accurate, doing what you expect it to do, managing the token economics to actually get an ROI on the use case you're attacking, um, and then understanding complex dependencies, especially if you're using MCP, RAG, energenic architectures. And then lastly, if you're managing your own GPU infrastructure, um, that's largely a new component for many organizations. So we're gonna dive into our first use case, which is model training. Um, everything in this use case also applies if you're doing fine tuning. But what really matters here is training efficiency, model performance as far as the end result. Um, the model is the product of the training. And then GPU utilization, these resources are extremely costly. Um, so it's critical you're actually getting the right utilization from your investment. Uh, a build here, let me click through. So, a quick overview before we get into the observability side, um, of the standard model development life cycle. I'm definitely trivializing it a little bit here, but we're taking large data sets, putting that into a large compute infrastructure with GPU accelerators, and running distributed training jobs in that infrastructure, with the goal of producing a trained model, uh, that we can then put out into the wild and get value from. Once the model's complete, the next step is actually hosting your inference service, whether that's externally or internally, more as like a platform team. Um, but this is where as a user, I can say, hey, here's my description or image of a cat, and my model can infer or predict, yes, this is indeed a cat. Simple. And if we look at, um, you know, a basic architecture of how you might go about this, this is all about scale, reliability, and performance. Um, and what we're seeing in the market is, uh, the more training cycles, the more compute you have, the bigger and better model you get. And efficient training becomes a competitive advantage, especially when everybody has access to roughly equal compute infrastructure. Looking a little bit at where the problem patterns start to occur, and where we can start thinking about observability to prevent them, um, starts with our data sets. So understanding that a small amount of inaccurate or invalid data can poison your entitled, uh, entire training cycle. Uh, so understanding the metadata around your data sets, measuring how one data set versus another impacts the results you get, super critical. And then similarly, we have data ingestion services. Um, and if these are slow or have spikes and errors, it's going to bottleneck, again, your entire training pipeline. And then we have the model training jobs themselves. Um, this is very similar to a traditional service or any other job you might be monitoring. Uh, need to correlate infrastructure issues with the outcome of training. And then on the far right-hand side, we see the GPUs, the dollar sign on fire. Again, continuing to highlight, if you have downtime or low utilization, not only wasting money, uh, but you're slowing down your time to market and getting the value from what you're investing in. So at the end of the day, kind of asking yourself, are we maximizing our training efficiency to stay competitive in every way that we can? Uh, now jumping to Chronosphere and tying this a little bit more closely to observability, uh, what we're looking at here is a Chronosphere lens service page. This is interesting to us because what Chronosphere is doing is saying, hey, I'm detecting GPU metrics from the Nvidia DCGM, uh, Prometheus exporter. We're getting utilization, temperature, error stats. Um, but we know from our labeling strategy, this is supporting a specific training job. We're also getting the training metrics from our Hotel Python SDKs. That's giving us training accuracy, gradient norms, samples per second. And having all the information here lets us quickly understand kind of end to end what's happening in our training job. We're looking at this from the perspective of a human looking at dashboards and service pages, but all the same value grouping and analysis applies to our AI troubleshooting tooling, um, our MCP and Agentic integrations. Uh, so that's super critical to think about. And throughout, low latency alerting. If you have XID errors, and you have GPUs that are malfunctioning, the time from that malfunctioning GPU, um, to getting an alert in front of an operator who can remediate is absolutely critical. And then again, throughout, only keeping the data that we actually need to accomplish the use case we're pursuing. All right, and again, this is a scroll down of the same service page. And I'm highlighting because we have distributed tracing with Otel, we get out of the box this dependency map, and we can see right away if there's a spike in errors, a slowdown. Um, with a data ingestion service. And we have all of our telemetry in one place. So not only do we know there's an issue, but we have all the logs, the events, the metrics, the traces to dig into, um, and really correlate, identify the root cause, um, ultimately maximize the, uh, minimize, sorry, the training downtime and maximize your GPU utilization. Cool. So we have a trained or fine-tuned model. That's awesome, doesn't provide a ton of value, unless we can put it in front of users, uh, by hosting our model with an inference hosting use case. So what matters here, Service reliability. People are going to be building on top of this, they need it to work, or they're going to pursue other alternatives. Uh, on the same note, it has to be fast. If it's not fast, they're waiting around, they're going to use the next tool. And ultimately, we need this to be scalable. We're investing all this time and energy into training and hosting. Uh, we don't want to support small scale use cases. We want this to scale to many users. So another architecture diagram here, this one will feel very familiar to a traditional cloud native service. We just kind of have inference plugged in at the back end there. But users need fast and accurate responses across multiple client devices. Um, the services are relying on this, so uptime, performance is critical. Uh, and namely, this last bullet point, incidents and outages can be very high impact, high visibility when we're talking about inference. You don't want to be in the news because your AI is giving incorrect or harmful information. So again, looking at our problem patterns, front-end issues in our different UIs, uh, upstream dependencies, all of these supporting services can impact our reliability, network issues, and then again, keeping GPUs kind of always in sight when we're talking about AI use cases, a little bit less critical for inference, might impact only, uh, you know, a smaller set of users, uh, but still ultimately important to keep track of. So jumping back into Chronosphere, now we're in the perspective of a platform team, self-hosting some inference. We still care about all of our red metrics like we would with any other service, request, errors, duration. But we also want some way to evaluate and benchmark, uh, the accuracy and health of inference itself. And that's what you see here with our hallucination rate, biased response rate, and toxic response rates. So again, we have all the telemetry in one place, we're kind of correlating these different things. And one, piece of feedback, positive feedback we get from our customers is any graph in chronosphere, you can click into and access our anomaly detection feature called differential diagnosis. And for example, that spike down there, um, you're able to quickly identify which label is most uniquely associated with that anomaly. Uh, is that a build version, a cluster version, um, container or something else? That's the actionable piece of information that often gets lost in the noise, um, of a large observability implementation. So we're gonna start shifting gears. We've talked about training and fine tuning models. Uh, that's in our view, a smaller set of organizations. What most organizations are actually doing is consuming and building on top of inference. Um, so first, let's define this term AI native. Uh, I think it's definitely subjective at this point. But our view when we say AI native, is people who are building from day one, designing around the AI technologies. One fun way to think about this and kind of test it. Uh, is if you think of a product person or a founder, and they say, hey, what if we built a, and then you put any product category in there, it could be an IDE, an HR tool, um, anything, and you say, but with AI, most likely, that's going to be an AI native product. So we can see up top here our traditional architecture, we have strict schemas and data models, using CRD rest architectures. Implementing these capabilities one by one behind endpoints and then accessing them through our different uh client devices. But with AI right on the bottom here, we don't need to be as concerned or as strict with our data models, and we don't need to implement every single capability individually, because the LLM has the ability to reason, take requests dynamically that aren't pre-implemented, um, and use data that might not be structured. So what we're seeing now is functionality built around inference and tokens, uh, reasoning and rag capabilities, and then really optimizing around your prompt and context engineering. Uh, and then another thing you might notice, startup URLs are now maybe innovativeguy.AI instead of disruptiveproduct.io, right? Um, some other terms, we've talked about these a lot already. But just to make sure we're kind of level setting, when we say tokens, we mean essentially the word count going in and out of the LLM. This is used to gauge throughput, um, also to calculate pricing. Uh, some other key concepts are evaluations. This is taking the inputs into an LLM, looking at the output you got, and evaluating if it's what you expect, if it's healthy, um, And doing what you needed to do. And then lastly, rag or retrieval augmented generation, extending the knowledge or data available to a foundational, uh, excuse me, a foundation model, um, through external data sets. So, finally getting to the inference health use case, home stretch here. Uh, what matters? Typically, you're looking at my model accuracy or performance, and you're comparing that directly against the token economics and the cost, uh, that's required to achieve those outcomes. And ultimately, what we're seeing AI natives really strive for is product differentiation through the use of AI. Otherwise, AI might not be the tool to solve that problem. So we're gonna go through some examples, um, of some concrete inference health issues that observability can help solve. So the first one is probably the most common we all hear about, which is hallucinations. I think a great example of this, straightforward, asking it what is OTE? And the LLM tells me OTE is a 3D printed telescope. Wholeheartedly incorrect, but the LLM is confidently stating this as a fact. If you don't have a way to measure and evaluate this, um, you have no idea how often your users are experiencing that. Another one that can be, uh, even potentially more dangerous is if you have bias, and you're starting to use inference in things like hiring workflows, um, agentic, HR, right? Uh, and there's bias you're not aware of, which candidate should I hire? And it says you should only hire hockey fans. Maybe if you're hiring for a hockey coach or something like that, it makes sense. Um, otherwise, this could be really harmful. And then at the end here, less about the behavior of the inference, more about those token economics, um, but excess token consumption. So if I'm asking a simple question, what letter comes after A? And the model tells me the letter A is, the letter after A is B followed by C, you can count the number of excess words and characters, and at scale, that adds up to just wasted money and cost. So, kind of double clicking one level further, um, why do these things happen? If we start with hallucinations again, largely this is something that's related to your own prompting, um, and your own usage of the model. Could be inaccuracies in training data, and then the lack of tools for a rag, um, Lack of update information. It doesn't have the knowledge, so it tries to invent an answer, um, that's not available to it. For bias, if you have bias in your training data, they're going to have bias in your inference. You might not have evaluations or guardrails at all, or you might just have ambiguous prompts that's not protecting against any, um, uh, bias that does exist. Excess token consumption, if you've use an MCP server, you've probably seen a model spin its wheels and infinitely make requests, um, and just burn a ton of tokens. If you have that type of thing happening inside an agentic workflow, again, you can scale it. Um, that's a lot of wasted costs. You could be spending on GPUs or elsewhere. You also just might not have output filtering. You might not be specifying a response format. So the LLM doesn't know what you want it to do. So it's guessing, uh, and producing some waste. And then if we think back to our example where you might also be hosting the inference yourself, you can look at your temperature settings, um, inference and model configurations, um, always keep an eye on the quality of training data, and your GPU performance actually can affect how the model behaves. It can restrict tool calls, it can change the behavior, uh, and impact the accuracy of responses. All right, back to chronosphere one more time. How can we use observability, um, to kind of help us protect against these pitfalls? What we're looking at here is an OTel trace. Uh, this is instrumented, um, with a library called Open Inference by RS AI. What that does is give us everything we get in a standard OTel trace, but it also grabs LLM specific attributes that we can do a lot of additional analysis on. So anywhere in this trace, if there's a traditional service error or a hallucination, bias, etc. this whole line will go red, and you'll know right away where in your agent, uh, reasoning or request the issue is. And then we can jump in, create trends, use our anomaly detection again, um. But on the right hand side, we see the span details. When I talk about these LLM specific attributes, what do I mean? Uh, I mean stuff like this, we can see which model, which model version, the actual prompts, uh, inputs and outputs. We can feed those into evaluation systems like Phoenix, you can create your own, uh, you can even run evaluations at the code level. We also get all of our token counts, uh, and we get these assessment attributes for hallucination, bias, or toxicity. What this lets me do is drive all these useful trends, uh, and start analyzing the data. So if I'm an AI native product, What do I care about, right? You might care about choosing the right model for the job, so we can see what's the average cost per request broken down by model. We can compare that over time. So maybe one new release actually switches this up. And the way you're doing your prompting makes it so that Grok's cheaper, um, than an alternative, for example. These are the type of things you always want to keep an eye on, so you can make data-driven decisions and always improve your product and your implementation. Then at the bottom here, we've talked a lot about the hallucinations already. Um, but again, maybe you make a change, maybe the model provider makes a change, and all of a sudden you see a spike in hallucinations. You need to action that and pull it out of prod right away. This is something that I think traditionally AI and ML teams were focused on. But now if you're an SRE if you're a, um, support operator, and you see this happening in production, speed of actioning that becomes critical to stay out of the news stories. OK. And in closing, we talked about a lot of different data. Um, one thing to highlight, Chronosphere does not have a proprietary agent. All the data we used, uh, in this talk was open source from open telemetry, SDKs and collectors, Nvidia, DCGM Prometheus exporter, Cube State metrics, Prometheus node exporters, the Open inference SDK. Again, that's from, uh, Arise AI, uh, they're open-source products, Phoenix AI and then fluent Bit for our logs. That's it. That's everything I wanted to cover. Hopefully, it was valuable. Uh, really appreciate you guys listening to the talk and hope you have a great rest of your reinvent. Thank you. All right, thank you, Ryan.