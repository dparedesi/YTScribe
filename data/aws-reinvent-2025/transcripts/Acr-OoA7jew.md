---
video_id: Acr-OoA7jew
video_url: https://www.youtube.com/watch?v=Acr-OoA7jew
is_generated: False
is_translatable: True
summary: |
  Alex Graff and Sudhir unpacked how cryptographic attestation underpins confidential computing on EC2, then walked through a full multi-party LLM example that seals secrets with KMS and only releases them to trusted runtimes. They framed two isolation dimensions: AWS operators cannot access customer data (the Nitro stack exposes no such APIs), and customers can shield their own operators by running sensitive workloads in trusted execution environments. Every attested environment starts from a recipe (Kiwi or Nix) that produces an AMI and a set of measurements (PCR hashes). At boot, Nitro’s TPM regenerates those measurements, signs an attestation document, and exposes it along with a certificate chain so relying parties can verify the document is Nitro-signed and untampered.
  Because secrets must not be baked into images, they are provisioned at runtime through KMS. The presenters showed a KMS key policy conditioned on specific PCR values (e.g., PCR4, PCR7, PCR12) from the attestation document; only an instance presenting matching measurements can decrypt the wrapped data key and, in turn, the encrypted model weights. In their demo the model owner envelope-encrypted a Mistral 7B GGUF file, published the encrypted payload plus the encrypted data key to S3, and added a KMS policy that whitelisted the intended PCRs. On the consumer side, an EC2 instance fetched its attestation doc from the console, verified the certificate chain, and used the document to satisfy KMS conditions. The sample app also hashed the loaded model and extended an unused PCR (e.g., PCR15), so consumers could assert they were talking to the exact model version rather than any generic LLM.
  Packaging matters. Using Kiwi, they disabled interactive access paths (EC2 Instance Connect, SSM Session Manager, OpenSSH) to prevent operators from reaching the runtime, installed TPM tooling, and let agents hot-reload tools from a “tools” directory. A parallel Nix flake produced a reproducible CPU-only AMI. Both images benefitted from Nitro TPM attestation, support for GPU-attached instances, and online secret delivery. They contrasted this with Nitro Enclaves, which change PCR semantics, communicate via vsock rather than ENIs, and require a bootstrap script plus proxy processes to reach S3 or KMS. The enclave demo similarly extended higher-order PCRs with model hashes and showed how the agent serves OpenAI-compatible APIs from within the enclave.
  To accelerate adoption, they shared QR codes for sample repositories and workshops that package the multi-party LLM app with Kiwi, Nix, or enclaves. Key takeaways: generate and compare measurements, verify the Nitro-signed certificate chain, gate decryption on attestation via KMS, and design images to remove unnecessary access paths so the only way to use the protected assets is through the attested workload itself.
keywords: confidential computing, attestation, Nitro TPM, KMS sealing, attestable AMI
---

Hello, good afternoon, and welcome to um the session on demystifying attestation, how to make cryptographic attestation work and verify your execution environment, so you're only running what you want to be running. I'm Alex Graff. I'm a principal engineer in, uh, the EC2 organization, and I have with me Sudhir, uh, senior SA, uh, who's also working on lots of SA, uh, lots of EC2 topics including confidential computing just like me. So imagine you are. NAI company There you go. You're an AI company. And uh as an AI company, you build an AI model, right? So you are, uh, you're deeply vested, you basically spend all of your money onto, onto building such an AI model, super expensive thing, so you want to have customers go and use it. Well, How do you get customers to use it? First you get a customer, and they have data that they want to go and send to you to go and perform some operation that you and UAI model can do for them. Now Once you do that operation, you're influencing on on that data, the model spits out a result and your customer is happy. The obvious first thing you would do design-wise to build such an architecture. Is to build it as a service, right? So your customer can go send that data over to you and you process it and generate the result, which is really a happy case in most environments unless you realize that your customer actually has super sensitive data they don't want to hand to you. Right? So how do you solve that problem that your customer uh cannot give you that data into your environment but rather wants to keep it in their own. Well, easy, right? You basically just give them the environment instead. Why don't we just allow the customer to run our, uh, our AI model in their own premises or in their own account so that they don't have to go and really trust us for the execution? Well, the problem with that is that training this AI model was not cheap and it's kind of IP, so you really want to make sure that nobody can go and see that. So what can we do? Well, confidential computing to the rescue. If we put a box around it, because putting boxes around it always makes things look prettier. If you put a box around this. And we secure that box we put a a a nice shield around it so we secure it, uh, and call it confidential computing, we have the problem solved. Well, the reason we're in this presentation is so that we can take a look at um how that actually works underneath the hood and then understand what we can and how we can actually go and do that. So confidential computing in a nutshell means that we don't give anyone access to things that are running inside of an environment. So what we do with this shield, with this box and shield around it, really just means that whenever there's some communication going to the outside world, we define that interface to a point where data that we don't want to leak does not leak. Sounds very easy in a nutshell. Details the devil's in the detail, obviously, right? But the basic gist of what we're building with confidential computing here is that your account operator does not gain access into the instance or into an an entity into a trusted execution environment that is actually executing uh your sensitive workload. But there's more. The, this, this confidential computer environment is running on top of a stack of a technology stack. In AWS's case, that's the nitro stack. And in nitro we follow the exact same model we say we have APIs that are clearly defined to not include any access pattern access paths into customer data so that any workload that executes on top of that stack is safe from AWS operators that otherwise still need to operate the fleet obviously. One way to think of this um problem instead of this picture is to look at it in dimensions. So we call the uh environment where we say our AWS operators don't have access we call that dimension one so we including me like I do operate this fleet on a day to day basis and we do not have any API access that would allow us uh we don't have FD APIs we don't even have APIs that would allow us access into customer data, right? Um, dimension 2 on the other side is when it's about your applications fending off access from either your own operators or from somebody that's running this execution environment like what we were showing earlier in the um in the AI model example. And if you want to know more about how this whole concept works, um, there's a very nice white paper that we wrote, uh, on how the nitro model works, um, how, how the, the, the whole overall system actually is designed. We even went down and validated that whole design and the approach, uh, with the NCC group like an external assessor, uh, that validated that there are no gaps, uh, in the nitro system that will compromise the security claims I was laying out earlier. And we are so confident about that stance that we even included that we do not have operator access into UEC for instance in our terms of use. So confidential computing to recap, means that AWS operators do not have access to your EC2 instance data. It's always on, there's nothing for you to do. If all you care about is that. Can you can sleep uh peacefully but this one goes further, right? Actual confidential computing in my opinion is always when you also isolate the environment from some other entity that's not only A yes, it means that you want to go and isolate your workload from your operators or from other operators that could possibly access it. And that is part of customer responsibility. However, we provide some very nice tools in order to make this an easy, achievable task, which I will lay out in the rest of this presentation. Uh, there is one piece that I didn't mention until now, which is that while it's nice and dandy that you say, hey, um, I don't have access to this data, you want to prove it, right? You want to have some mechanism to say, um, yes, the thing I'm talking to actually is running this code rather than just only take my word for it. And that one, we can get into. So. Data uses, um, we do have the thing we looked at until now, uh, is when you are processing data, right? So you have some execution environment, some, some confidential computer execution environment, and that is processing data, but where does that data come from? You don't like Breathe data out of the air, it needs to come from somewhere. It usually comes from either somewhere at rest or it comes from somewhere in transit, which means you're talking to somebody else's life. So either you have some storage where you put data uh or you talk to somebody live and that data needs to go and be encrypted. With that encryption key only known to this execution environment so that you can be sure well execution environment and maybe another execution environment that can see data um so that you can be sure that only trusted entities can communicate either with storage or uh over the network. The um thing that you need to do uh in order to get there is you need to have some type of shared secret, some way to authenticate to to to ensure that there's a shared secret that's only available inside of this execution environment, not outside, right inside of the execution execution environment, but how do we get it in. If you see it as part of the launch, then it may potentially be a public secret because anyone could see and uh look at the the original image that that was executing so we need to do that at run time. So that we, let's take a look at how you actually build and execute these environments. Execution environments like the confidential computer execution environments always start from some description, so you have something that describes what this image looks like, what the the environment is going to look like you generate an image out of that description, and then from the description you spawn the actual environment. You can obviously spawn multiple environments, right? And ideally you want to have uh a a reproducible way from the description into the image so that you can always receive uh the same result but one thing that happens that is really important in this picture is that when you generate the image you don't only generate the image you generate the image as well as cryptographic hashes that describe the full contents of that image. And that's a really key point. The, the reason why we need these hashes is that later on when we execute the execution environment, our trustworthy nitro stack now also regenerates those same hashes and provides you with an nation document which you can then use to identify what was actually running in that instance at that given point in time in that particular environment. And if we then use them to compare. The image generation hashes as well as the executing hashes, we know that the environment we have is the environment we built, which is the main property we're looking for here. Now we still don't have a secret, but we only know what for now until now that we um have a an environment that looks the same as what we uh wanted to have. But we can get that one quite easily using a service called the key management service. So we integrated capabilities into the key management service, uh, which allow you to define a policy on KMS keys. So KMS is a is a service that allows you to store and um and and perform operations on on key material. Um, which then can say you are only allowed to operate on this key if your policy matches. So basically if your image is exactly if the execution environment is exactly the same as the image that you vetted is actually built with full trustworthy code. To recap, cryptographic attestation means that we are validating the execution environment against the image that we built. Um, validation of that is integrated into the key management service. You don't have to use AWSKMSs. It just makes your life a lot easier. Um, you use that mechanism in order to gain access to data in transit or at rest. Typically in these confidential computing environments we assume that code is public, data is private. And if you need private code, you treat it like data, right? If you need to try private code um that that you don't want to have anyone see, you would encrypt it so it's basically data addressed again. Uh, but in order to do all of this attestation and the thing we need an attestable image, and attestable image is not the same as just a normal image. It's a very special type of image that you, um, have to build in order to gain this property that the boot stack is going to actually give you these measurements, these hashes that allow you to reason about the contents. And for that we provide two mechanisms. One is super new. We just launched this earlier this year. I'm very excited about this new feature. Uh, it's called easy to insert a station. So easy to insertation um means you have a description. There are multiple different formats I can get into in a second uh of descriptions, but the description describes what you want to have inside of your attestable army. Think of it like a docker file. And then you generate the attestable army. It's a very special type of army that has a special boot flow which also generates uh a set of hashes. You can always generate these hashes postmortem like when the image is already there, but typically you would generate them while you generate the army. And then you can launch the army like you would any other EC2 instance. So who of you have already launched an EC2 instance manually. Anyone awake? All right, two hands, um, so if you haven't got an easy to instance, try it. It's really fun. Uh, it means you can basically get a virtual machine in any, uh, any point in time, uh, very quickly and easily, uh, and with this mechanism we are hooking confidential computing, including actual workload adaptation into the whole EC2 ecosystem. So you can launch an EC2 instance and this EC2 instance goes in and hashes the boot contents of that instance into nitro TPM. It's a. Uh, a component, a, a virtual trusted trusted platform module that we, um, provide for a while already in, in these two instances and. The new thing that we have launched this year is that this Nitro TPM now can also issue a nitro TPM meditation document which gives you all of that KMS integration I was talking about earlier so you can unlock your secrets based on your boot flow. And the secret part about why that attestable army is so different is that it contains a completely different boot environment. So you have the Nitro stack which starts up the virtual machine which executes a UEFI boot environment, um, low level fware which then launches something called a unified kernel image like a UKI, uh, which is a standard upstream concept like a normal Linux concept, but we're reusing it in testable armies because it gives us a very nice property. It is a single binary that contains kernel init ID and command line. Of the target system, which means that when UEFI launches this, it measures, it hashes the contents, the full, uh, contents of that UKI into PCR 4. Then the UKI sees PCR 12, and the kernel command plan that's part of that UKI. Contains a hash of the root file system, so by just only looking at that one single number at PCR 4, you can reason about all of the contents of that one single system because everything is drilled down into a single number. Thanks to the nitro TPM meditation document, you can also reason about the fact that you are running on a nitro stack because the document is signed with an AWS key that only nitro environments have access to. So by looking at basically these 2 numbers plus an AWS signature in the document. You can reason about the two dimensions I was explaining earlier. You can know I am running on nitro, so I'm running on an environment that AWS operators do not have access to, and I am running exactly the image I built and because the templates we provide by default don't contain operator access at all, and if you don't add operator access, it means you are running an environment that also does not contain any operator access. All right, so recapping easy to insert the station. The super amazing thing about easy to insanitation is that you get all of these properties with all of the features and capabilities of an easy to instance. You can use elastic network adapters, you can use elastic fabric adapters, you can use instant storage, you can use accelerators like your ranium, anything you want. All of the features that you're used to and love and easy to just work. Same as like auto scaling groups, right? Everything is just from an EC point of view, these things are just armies, they're just normal machine images. Um, the Nitro TPMation document proves the two dimensions, right? It proves that you are running on the nitro stack with no AWS operator access and you, it proves that you are running exactly the image that you built. Uh, the, um, this interactive access, uh, that I talked about where you would, um, have an operator that could possibly go and extract data, as long as you follow our samples and you don't add that access, that is, uh, excluded, like you know that you don't have that access and you can prove it again using the key, uh, the, the, the hash, um, and we provide sample descriptions for Amazon Linux 2023 as well as NIOS. Today. Today means this QR code brings you to the web page, you can just go and get started. I would totally encourage you to do so, it's actually pretty, pretty fun and easy to go and do. Um, this insulation is really, really useful if you have an application that you want to lift and shift. You take the full application and you put it into an easy to instance. That is fully attested and validated and you are sure that it's running exactly your code. But there's a second mechanism that we have, which is called nitro enclaves. So for nitro enclaves, which is a feature that we have had for quite a while, but I suppose who's familiar with nitro enclaves? That's more hands than EC2 instances. I'm amazed. Um, so nitro enclaves. Uh, is the feature we launched first, but it's actually the more complicated feature in a way. So in, in a natural enclave you would typically say, uh, my application is consisting of like a lot of different things. I have like third party applications running somewhere. I have a full blown operating system that goes and does a lot of things. Um, I have my like a big part of my application that goes and processes random data, maybe talks to to external entities and that I may not fully trust, but I have this tiny piece of code that I want to shield out. And want to isolate and want to make sure that this particular tiny piece of code is running securely in an environment that nobody else or nothing else can access. And that's what nitro enclaves are for, and that's where they really shine. So if you can take your application or you want to take your application and you want to split it into two, you want to split it into a trusted and an untrusted part, but they still work lockstep, they still work hand in hand together as a single entity, then nitro clase is the solution for you. So Nitro Enclave provides you with a separate tiny virtual machine that you can spawn on the fly out of your EC2 instance resources. That then can process highly sensitive data inside of an enclave. uh, the enclave only communicates to the parent using, uh, the VSOC channel that it has like a special, um, uh, low level transport. Uh, which means that you need to enlighten every communication that goes in and out, which is a special property of natural enclaves that allow you to ensure that you always reason about every single commun communication that goes in and out of this enclave, because, as I said, it's built to only communicate between your parent application and your enclave. So enclaves is the tool of choice if you want to perform ISA operations on behalf of a parent, so you're splitting your applications in two. If you look at building an enclave, it looks the same as the picture I described earlier. Instead of having an image description with like source cody um attributes, here you take an actual container image like a built container image, and you convert that, you generate an enclave image file from it, an EIF file. While generating it or postmortem, you can generate hashes uh of the um of this image so you know what to expect later on at boot. And from that image you also launch a nitro enclave. When this nitro enclave launches, the nitro hypervisor goes in and hashes all of the contents of the actual image and provides you with PCR values which then are fed into a nitro enclave attestation document. So it's the exact same flow as before. The actual measurement itself is a bit easier, um, so instead of having that chain of boot flow you really just only have the nitrous stack that takes the EIF file, uh, and then boots the EIF itself and seeds PCR zero value which just means all of the contents of the EIFL. There's more PCRs, but you can read up on them online if you're interested. But using these two values, the AWS signature you have again on the natural enclave ation document as well as PCR zero, you can reason about the same two dimensions. You know that it's exactly your image running, which means no interactive access hopefully unless you put an SSH server behind VSOC, uh, and the, uh, nitro stack itself as well because the document is signed with the nitro enclave attestation, uh, with the with the nitro attestation key. So nitro enclaves. It's super useful feature, but it does have a reduce restricted feature set, so you only have CPU memory and VStock available which dramatically simplifies auditing because you don't have a lot of um uh surrounding uh code and and access mechanisms available to the environment but it also means you don't, for example, don't get access to GPUs for that we recommend easy to insert the station instead because that gives you a full blown environment just like you would on a normal EC2 instance. The station document the nitro enclaves again proves nitro as well as the nitro enclave buty contents. And uh if you want to communicate to the outside world from a nitro enclave, everything goes through VSOC, including, for example, access to CMS. So it may be a bit more tricky to set up, but it's usually worth it. So nitro enclaves is what you use if you want to execute in a tested and isolated ephemeral workload that depends on a parent application, ephemeral because you don't have persistent storage usually unless you build it again on top of it. With that we actually have some uh really cool demo application that Sudius has built. He's going to show it to you on how you can use all of these technologies that I just described to solve the initial problem I solved to talk to you about, uh, which is how do I make sure that I can give you an environment to execute that launches and, and, and, uh, operates on my AI model without you having visibility into the model itself. So yeah, off you go. Now let's see, I think, I think I, I promise I have like a couple of slides and then we'll just get into the code, right, uh, so let's actually look at one of. I wouldn't say prevalent, but one of the cool use cases, uh, which you can use at a station to go solve for, right? So, um, let's see. All right, so here's an example and by the way, everything that I'm gonna show in, in this, uh, in the next few minutes you can follow along, grab the QR codes, the sample applications that I'm gonna show, the code samples that shows how to take the sample application and package it into an attestable AMI using. The tools that Alex mentioned Kiwi and NX, all of that's already open source. So grab these QR codes. But here's the, here's, here's the setup for the sample app, right? So imagine, um, and again this is just continuing what Alex uh outlay at the very beginning which is you are a AIML model owner so I'm just gonna take an example of an LLM model owner, right? So let's talk about generative AI applications. So let's say. I am in the business of fine-tuning domain-specific LLM models, right? And then I want my customers to use them. I have invested a considerable amount of resources in fine tuning, training those LLM models. So how do I make sure that my customers can use my LLM models without Me worrying about the model weight exfiltration, right? So that's one way to look at the threat, possible threat that I want to mitigate as a model owner. Similarly, model consumer, the other half of this multi-party collaboration. Um, set up, they would worry about, hey, what if I'm actually sending highly sensitive data in the prompts, in the query, in the context that I supply to this large language model, right? It's, it's pretty common these days you would want to enrich what you send to the LLM to get more context aware answers back from the LLM so you would send. Maybe your organizational data, enterprise data, personal information, what have you. So you do have a use case where highly sensitive data is being sent and you want to Um, make sure that the other party cannot exfiltrate away what you have said. So, Here's one way to go solve it. So let's look at a very simple setup. Um, here's how you deploy, and I'm not saying this is the only way to go consume LLM models on AWS. There are other services, but for the purpose of this conversation, since we're talking about cryptographic attestation and Amazon easy to instances, here is how, here is one way to go host up an LLM model. Served using a model server and then I have a UI or a front end that's consuming that LLM API, right? So far so good. So this is a classic and a simple example of what we call multi-party, multi-party collaboration, right? This is not multi-party competition, this is a multi-party collaboration. You have two parties here the party that owns the LLM model, the party that consumes the LLM model, right? I think I have a slide on that. There you go, uh, like I explained, you know, party, uh, the, the party that consumes the LM model wants to protect certain things that is being supplied to the LLM model. The other party wants to protect the model itself. So how would you go, um, solve for a pattern like this. Here is another visual which shows the again the prompt and query being sensitive. The LLM model is intellectual property of the party which owns the LLM model, so that's sensitive as well. So, um, there's, there's more patterns like I said, you know, this is not the only thing that we could solve. You would have more design patterns and generative applications where you could have, um, components of your applications stacked that. Anonymize pseudonymize the prompts and the sensitive information before sending over to LLM model you could have your rag components that append additional context. That is something that you probably want to protect because RAG has access to your organizational information, for example. Many more things, right? You extrapolate, but the primitives. To go solve a problem like this do remain the same, right? So let's take a look at those notices. The first and foremost. As a model owner, you would want to encrypt your LLM model weights. We call it envelope encryption using AWSKMS or your own key management system before you publish it to the other party. If you recollect, we started off with the problem statement saying that instead of asking your customers to send their sensitive data to you, why don't you invert. The deployment topology where you're packaging up your LLM model weights in such a way that they can actually be deployed in your customer's AWS account or at the point of consumption. Right, so, so that sensitive information that is being sent as input to LLM model stays where it is. It's not going beyond the boundary of the AWS account where it belongs to, right? So envelope encryption is one way to, uh, go package up your LLM model weights. To be made available to your consumption, but the next step is the most important step beyond just encrypting them because we're talking about confidential computing environments and because we have a nice little tool called attestation at our disposal, what we'll do is we'll. We seal the model weights to the measurements that you saw in Alex's talk. These measurements represent the conferential computing environment, the isolated computer environment that you have built, right? So you went through the build flow, you built an attestable AMI, and one of the things that came out of the exhaust in the build process is measurements that represent this AMI. Now imagine you got the execution environment, the EC2 instance, which also presents you the same measurements in an attestation document. So what I am gonna do as a model owner is I'm gonna not only just encrypt but see you. My model weights, only to be unsealed in an execution environment that Shows me or presents to me these exact same measurements. In that attestation doctrine. So let's actually look at the sample application. So enough of um design patterns and primitives. We'll actually see how these primitives and design patterns resulted in uh. Sample app and how we package it up and deploy it as. And easy to instance with instance attestation, that becomes your confidential computing environment. So, I'm gonna switch over to my Look up here. And let me know if some of this is not. Clearly legible, all right, um, so all right, so first and foremost let me, let me just give you a peek at the sample app itself. So here's the sample app, and by the way, does it look OK so far? Do you want me to zoom it up? Let me, let me just zoom it up real quick, right? So this is the same concept that we just looked at in the, in the slides, right? So I have two parties. One party is the model owner, the other party is the model consumer. The model owner wants to encrypt and seal the model weights, and then Publish it to the model consumer and the model consumer we're talking about an EC2 instance here with instance at a station, right? Um, they want to consume it so they will be sending their inference requests to this EC2 instance to the, you know, the to the application that we're gonna package up into this EC2 instance, right? And then out comes a chat interface where they can chat with it. So let's actually look how it comes together. So, um, for the purpose of this demo, We have pushed up a bunch of concepts like I said, we made it very, very simple, and I'm actually running. Both the front end and the back end of the sample application inside the same EC2 instance inside a single EC2 instance, right? So by no means this is well architected. You, you could go divide up these components into in each of its own execution environment with its own attestation with its own measurements. There's nothing stopping you from doing that, but this is just for simplicity's sake. So what I have here is two views, right? Here is what I can do as an LLM model owner. Which is, if you recollect what I just said, which is to envelope and encrypt my model weights and then publish them and seal them, right? Uh, I'm just gonna take some off the shelf LLM model weights available on hugging face. So just play along. Imagine that these are my intellectual property, highly fine-tuned domain specific LLM models or what have you. So I have this LLM model weight that I'm gonna use a KMS key ID. To go on all of encrypt and publish to an S3 pocket. Cool. I'm not gonna bore you with the loading demo, and sit here and watch it. I think I went ahead and published it already. So, uh, here we have it, right? The encrypted. Uh, model weight, which is the, you know, in this case it's a Mistral 7B model. On disk, it's about 4 gigs. 4 GB, uh, what I also have, this is the cool part, important part with envelope encryption, what you get to do is you have a backing KMS key which is what this ID represents and what we did behind the scenes, if you see these steps is we generated a data key. And that's what the fundamentals of on all encryption is, right? So you go go ahead and generate a data key using the master key, and then you use the data key to encrypt the GGUF file. And what you publish to your consumer is two things. One is the encrypted GGUF file, which is what you saw here. And again, I'm just assuming, you know, the terminology, excuse me, GGF is, is just one representation of LLM model weights and there's several, several others for simplicity we just used the most popular format that my simple LLM server, which is OLAMA, can serve. We'll see those details, uh, but, but yes, those are the two things that I'm publishing, which is the LLM model weights which are encrypted and the data key encrypted data key that I have used. To encrypt the LLM model weights, right? So, so far so good. We did one thing so far as a LLM model owner, which is publish my model weights. Now the ceiling step, what I'm gonna do is I'll take this KMS key ID and this is where attest attestation comes into the picture, by the way. Well, let me actually show you attestation before we go into the ceiling model right it's because you, you might be like, hey, what is the PCS? So, so if I go to the EC2 instance attestation tab, this is a pretty high level view or maybe a detailed view of how an attestation document looks like, right? This is, by the way, the nitro TPM. Generated attestation document which is the EC2 instance attestation attestation document. It has a bunch of things that might be of interest to you. One of those is the nonce. nonce is typically used in sessions, etc. to, uh, preserve freshness. You just want. To make sure the same attestation document is not replayed, but the important thing is one of the important things is the PCR values. We just talked about what PCRs are, how it, how some of them come out as exhaust from the build process. I'm gonna just show you, uh, show of the interesting ones, the PCR 4. PCR 7, which runs the secure board policy and then importantly the PCR 12 as well. So these are probably the three things that you would want to verify introspect in that attestation document at a minimum, right? There is a few other things that are also important to verify the integrity of the attestation document itself, which is the certificate chain that I'm showing here. So we'll start off with something like, hey, what is my leaf certificate that's representing the TPM itself all the way up to the root certificate that signed this attestation document. So if, if, uh. If an EC2 instance is presenting you an attestation document, you would want to verify the signature and wanna make sure that it is a genuine nitro signed attestation document before you consume these PCRs and then reason about them, right? Uh, so that's why these are important, which is exactly what we're doing. I'm just gonna show you that, hey, you know, there is a thing called verification, semantic validation, certificate chain validation. Um, the non-certification things like that, we went past all of this in, in this sample application, but if you go now as a model owner to your KMS key that you own that you have used to encrypt the LLM model ways, here is how the experience is gonna look like, right? Some parts I have done, uh, by the way, ahead of time. But here is a, a KMS key policy for this KMS key actually. And here I have simply, and this is because we did the out of the box integration for EC to instants attestation with AWSKMS just like we have done it for Nitro enclaves, right? You could do this exact same thing for your own microservices, for your own other components that want to interface with this. Instance with this contractual computing environment there's nothing stopping you um but here is why it is simple because we've done it out of the box right? so so KMS key policy what you could do is you could simply add a condition that says hey only allow this action which is decrypt right because we're trying to decrypt the. GGUF or the LLM model weights from a customer's perspective. So as an owner, I'm saying that I'm only gonna allow decrypt. For my model weights, if the request came from, uh, EC2 instance that presents these exact PCR values. If you try to issue a decrypt request from a regular easy to instance which does not present attestation document is simply gonna fail, as simple as that, right, um, now with that in place, that concludes what you need to do as a model owner. Before you ask your customers to go consume it, right? I'm not gonna go into details about, hey, how do you actually onboard my customers? How do I do the handshake on, hey, customer, here is the PCR values that you need to go trust. Hi, here is the recipe that I've used to go build this environment. All of that will be part of your customer onboarding on however you establish that handshake, but at a simple level, this is how it's gonna look, right? Um, now, from a consumer standpoint, here's what I'm gonna do. Again, the sample application just makes it easy, but the primitives remain the same, right? So what I'm doing here is, hey, the publisher gave me an S3 bucket where the encrypted files are available. So I'm gonna just point to them. I'm gonna use the KMS key ID provided by the publisher, and I'm gonna say, hey, will you allow me to decrypt? And you go through these steps, right? So you download the encrypted files, you attempt to decrypt the data key. And if the data key decryption is successful because you presented the right attestation document with the right PCR measurements, then you get a chance to go decrypt the actual model weights, right? So that also, you know, um, takes some time. I'm not gonna do this live right here, but the end result would be. You will It will result in loading the LLMRL to the OLAMA server that's running locally within the EC2 instance. And additionally, what it's gonna do And this is the functionality that I have baked into the sample app is if as a consumer of the LLM model, if I'm interested to know that, hey, I, I'm talking to a specific model and not just about any LLM. So in this case, if my threat model says that, hey, make sure you're talking to a Mistral 7B and not just some other LLM model, what I'm doing is I'm hashing the model weight. And extending those hashes into one of the available PCRs. So in this case, I'm actually hashing it into PCR 15. What does it mean, right? So, so there are some out of the box PCR registers that track certain interesting properties of your attestable AMI which is PCR 47, 12, etc. but you do have the flexibility to go measure things that are interesting to you, to your workload, and then use one of the available PCR platform configuration registers in the attestation document in the Nitro TPM. And extend those so that you can verify them later, right? One of those examples that I just showed was the LLM model hash. It could be more. In total, you, you have about 32 uh 24 PCR registers, um. Some of them are reserved for um um some of them are available for you to use for your own custom hashhes but you get that idea so that's the consumption experience and again both the publisher and the consumer's expe uh experiences hinged on the fact that there's attestation documents being exchanged. With the handshake with AWS KMS. But you could do this exact same thing with your other components of the architecture. So let's say you have a microservices, you want microservice A to do this attestation handshake with microservice B. Both of them can exchange at attestation documents to each other and get the work done, right? Um, that's the idea. I'm just gonna show, um, one of the capabilities that we mentioned is unlike enclaves, you do have the capability to consume a GPU attached, which is exactly what I'm doing here. Um, in this particular instance, I happen to deploy this to, um, a G5 instance which gives me Nvidia 810G environment, right? So, so there's that additional, so, so the choice of which path you take, whether enclaves or, uh, easy to instance at station. You can make the addition based on the features available to you in one versus the other, right? So, so that's the, um, quick walkthrough about the sample application. Now let's actually get into details about packaging itself, right? So here, um, and, and by the way, once you, you, you know, you got the QR code, so once you go to the sample application, you will see how each of those UI components and the backing API we have done things like on of encryption. Uh, fetching that frustration document, etc. etc. right? Now let me actually show you The kiwi recipe that resulted in that sample application being deployed into the EC2 instance. I'll just call out some uh key features here, right? So if I go to the, uh, kiwi recipe, you'll actually see here, uh, and again, you know, kiwi has its own declarative way of specifying what you think should get into the AMI as packages and what should be ignored. One of the things um that I'll show you here is these few lines of code, right? So now because one of the interesting property to go solve the multi-party collaboration problem that we are looking at right now, the sample application where you do want to host it in your customer's account, right? You want it to be an isolated computer environment where your customer cannot get into it and then. Potentially exfiltrate away the model weights. All they could do is just use the API and consume the LLM model using the LLM model server, right? So what I, what's interesting to me is I want to add these properties where I'm saying, hey Kiwi, when you go build this AMI for me, make sure you are never gonna include these packages. So that's what this ignore statement says, and I'm saying none of the interactive access should be possible. So things like, uh, easy to instance connect. Your senior console, your SSM agent, which gives you SSM session access, open SSH, SSHD, all of those interactive paths to get into the CC2 instance are mitigated and are taken away with these few lines of code, right? Uh, and you will see, again, in the sample code. Um, by the way, I'll give you, I'll show you the QR code for this one as well. But here I'm specifying all the packets that I need. You'll see some of the packets like TPM 2 tools. AWS Nitro TPM tools, things that you need to go get the attestation document, extend the PCR, things like that, right? So that's, that's uh a little bit about the uh kiwi recipe that resulted in the sample app that you just saw. Now, The same sample app can also be packaged as an attestable AMI using Nix. NI is yet another way to go build your machine images. Now, unlike Kiwi, NI does give you the property, a very important and interesting property where you could do uh reproducible builds. And what it means is. Uh, uh, again, let me just show you the sample app that uh Nix has created. It's gonna look and feel the same because all I'm doing is Packaging it using different tools. The application remains the same. I'm just packaging it in a different way. To result in an adjustable AMI. And just to show some variation, what I did in the next recipe which I'm gonna show is. I took away all the GPU drivers and the intention here is, hey, let's actually deploy the sample application in a CPU only instance type where there's no GPU attached, right? So if I go to um the uh environment, you'll actually see that there's no GPU. Um, the idea is the same sample application can be packaged and the AMI, if you build it the right way, can be deployed in multiple instance type, right? And, and if you look at Nitro TPM documentation, you will see that there's many more instance types available to you to fit the right size for your workload, uh, compared to Enclaves. Enclaves also has the flexibility to give you the CPU and the memory that is needed for your workloads, which we'll see in a, in a second. But yes, the sample app is the same. The attestation document again looks the same because this is also Nitro TPM attestation back. But if you look at the recipe in N, it's gonna be a little bit different, right? So what I'm gonna, uh, show here is some of the interesting pieces of code. Um, oh, well, uh, this is worth mentioning. Let me, let me show this. In Kiwi, the way I told Kiwi to. Bake in my sample application into the AMI was something using something called overlays, right? So I told you that, hey, here is my sample app, it's potentially in a different GitHub repo. Take that overlay it on top of what you already have in the base image that that you have the recipe for and then. Uh, output the AMI, right? So the sample lab is 3 baton in mix similarly. Or maybe a little bit different in NIS you have to now write out um uh something called Nick Flake where you tell Nick how do I go package and build my application. So for NY what I'm saying is, hey, here is how you go build my front end here is how you go build my back end. And the next one, go do it, right? Um. And, and it's the same experience. NI does along with creating the AMI give you a bunch of PCRs that you can later use in attestation as well. All right, um, Now, let's look at the last piece of the puzzle, which is taking the same sample application and packaging it to run inside an enclave, right enclave, right? So I did that ahead already, uh, which is this application, and of course the semantics are gonna be a little bit different when it comes to nitron enclaves, right. Uh, there are Differences in how you package your application for an enclave. There are differences in attestation as well. So if I start with attestation, here is the Night enclave attestation document. Might look similar. You still have the nons module ID, user data, public key, etc. You do have PCRs, but these PCRs mean different things, right? position only. So, for example, PCR 0 is the hash of the entirety of the knit play image. What I did do is I grabbed one of the higher order PCRs. In this case, looks like I loaded the model multiple times. So I took anywhere from PCR 16 to 22 and extended them with the model hash of my, with the hash of my model weights, right? So even nitron enclaves gives you that characteristic capability to go use a PCR to measure and extend what's interesting to you in your workload. It has the same certificate chain. And, and that's about it as far as attestation comes. Pretty much all the other. Concepts in within attestation pretty much remain the same for an enclave attestation as well. Um, model owner and consumer, none of those functionalities change. So let's actually look at what's different when it comes to enclaves, right? We mentioned VSOC. One of the things that you get with an enclave is you do not have any ENI attached. So if you want to communicate with the external world, which in this case you do have to, to talk to KMS, to talk to S3, to go get the model weights, and then finally expose, uh, your API, the OpenAI API that serves the LLM to the front end. So all of that, for all of that, I do have proxies here. Um, and, and, you know, so that's one difference in how you package this application. The other difference being when you do. Uh, write out an enclave. You have to give it a bootstrap script. You have to tell how to go run this particular application inside the enclave. So for that I have some additional scripts, right? So those, those are the some, some of those differences, but the idea here is you could take the same application, which is the multi-party collaboration application that we set out to solve for using attestation capabilities for both of these environments. You have seen how you can use Nix and Kiwi to package it up to consume EC2 instants attestation, and you have also seen. How to package up the sample application using nitron enclaves. So that's the demo portion. So I'm just gonna leave some QR codes for you so that you can follow along, uh, later this exact same applications. I'll switch back to the PPT. Um, All right. So here's the KiwiNG QR code. If you want to, uh, so this gives you the sample packaging code that shows how to take the sample application and package it up into an adjustable AMI using Kiwi. Oh, sorry if I'm just rushing through the slides, and here's a quick architecture that's available in the GitHub repo as well, so you don't have to worry about taking a screenshot. And, and here's the next sample repo, right? So this is the same again, the same sample MPC application that you saw, and this gives the next flake. That shows you how to go. Um, and by the way, we packaged up this into a workshop. So if you are interested to know more about, hey, how do I actually build attestable AMIs, we have a session tomorrow. It's CMP 410. CMP 410, it's a short and suite builder session. So please do attend if you're interested to know more and get hands-on training on how to go build attestable AMIs. We will use this exact same sample PC app that you just saw. And then finally. I have the enclaves solution, if the clicker works. Which is this QR code. So enclaves has been there for a while. This is a QR code that will lead you to a hands-on workshop that shows you how to serve LLM model weights using enclaves. So that's, that's the end of my demo. Amazing, um, with that we have, uh, learned how to do confidential computing, uh, in an EC2 environment with a different mechanisms, um. Let's go to yes um I would encourage you to uh have a look at the survey. Please fill it out um let us know how we did, uh, and I'm opening things to questions now. Thank you.
