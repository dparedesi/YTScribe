---
video_id: Ew3nqJQ4uOs
video_url: https://www.youtube.com/watch?v=Ew3nqJQ4uOs
is_generated: False
is_translatable: True
---

Welcome to OPN 310, Agentic Workflows featuring Salesforce. My name is Vikram Egaraman. I'm a solutions architect, uh, at AWS, and I work with some of our strategic customers that operate Kuberator clusters at large scale. Along with me, I have Shrikha Rajan here, senior director, sales force. All right folks, have some good news and bad news. The bad news, getting paged at 2 a.m. Troubleshooting 5s for a 5 minute fix is not going to go away. But if we can build an intelligent system that can correlate telemetry signals and help us make decisions faster, That's going to make us, you know, place us in a good spot. And that's where we feel agentic workflows or AI ops can help us. And that's going to be the focus of today's session. Uh, as far as the agenda, we'll talk about Kuberator's challenge landscape. We'll spend some time to understand the challenges in operating Kuberators at large scale. Then we'll shift gears. We'll talk about AI ops. We'll, we'll introduce you to AI ops, how it kind of helps us from reactive to predictive operations, and then I'm gonna hand it over to Shikan, who's gonna talk about Salesforce journey into AI ops, and any questions and answers, we can always answer them after the session. Sounds good to y'all? Perfect. Quick show of hands, how many of you here have 50+ kritis clusters in your environment? Quite a lot of you. How about, uh, keep your hands raised if you have more than 100. Any 500+ clusters, people who own 500+ clusters. Oh, I still see one person there. That's, that's awesome. I mean, that's exactly the problem statement here, right? Like if you have that many clusters operating kuberator clusters, taking care of the operations is not an easy job, right? I'm a big fan of analogy, folks, uh. Like Vegas, our clusters never sleep, isn't it? There's always constant movement of vehicles in and out of the city. There is weather inclements. There is crashes, you name it. Now put yourself in the shoes of a person who sits at the traffic control center ensuring smooth operations, right? It's not that easy task. Just multiply this chaos by 10 more Las Vegas. That's exactly what we talk about when we say managing 1000 clusters or how it feels to manage 1,000+ kuber cluster, folks. Now putting yourself in the shoes of an on-call engineer who wakes up at 2 a.m. getting paged to see like there are thousands of alerts waiting to be attended right now in order to get to the bottom of the problem, he has to maybe skim through 50,000 time series of metrics, 2 petabytes of logs, and folks, he's already racing against the time when a 5 minute fix takes you 5 hours to troubleshoot. That clearly tells us there is something wrong in the way we operate, right? This is not a monitoring challenge. This is an intelligent crisis where we are unable to correlate the telemetry signals. Just to summarize the list of challenges that he might have, he has to first isolate the noise away from the actual information, then he has to make sure that he collects all the information, telemetry signals as your request flows through your complex microservices architecture. Tracing can help, but again, you have to connect the dots. Thirdly, we do have a variety of monitoring tools at our purview, right? We have Prometheus for metrics, we have OpenSearch for logs and traces. He or she should log into these systems in order to get insights from the telemetry back end, and that's not an easy job. And finally, when it comes to applying the fix, it's not the application owner who wakes up at 3 a.m. to apply these fixes, right? It's the on-call engineers and lack of run book is gonna further increase your mean time to identify and resolve operation issues. And I think that's where the kind of AI ops or agentic workflows comes into the picture. We don't Google anymore, do we? No, we just prompt, right? OK, so AI ops or agentic work workflows, it's not something new folks. We've been doing AI ops in the name of anomaly detection, who here has like have enabled anomaly detection in their environment. See, we, we, we did what we call a little bit of uh AI ops even before, before this term became a buzz buzzword, right? And then we were using predictive order scaling or even intelligent automation where a sing single or a simple batch script kind of goes and, you know, fixes some kind of an issue, right? But there is a big difference between the traditional tools that I was talking about and AI ops. AI ops is powered by the so-called LLMs that gives you the intelligence to correlate telemetry signals, and that's exactly what we're gonna talk about, like purely from an operations perspective. So what is the fundamental block of your AIIs framework? Agents, it's not a new word, right? So what are agents? Put in a simple term, agents have specific goals, folks. This is exactly where you kind of feed the context to the agents. You tell the agents, hey, you are a cooperator's operations assistant. You'll have to go through metrics, locks, traces, correlate them, and identify the root cause of the problem, or go one step ahead and provide a possible remediation to the issues. Agents also has tools. Tools are nothing but how a human being would interact with the with different tools like you run a cube cuddle commands to get some insights or run a prompt cub to get a uh to get insights from your Prometheus back in, right. And agents also has memory, both short term and long term. Short term because it has to understand the previous responses and has to provide the results based on the previous conversation. Long term because the same agentic framework or the workflow is going to be accessed by different users, so one who's from a business intelligence team need not be worried about all the telemetry signals inside. So it has to preserve that user preferences and has to provide the response in a meaningful way. Agents also invoke actions. So if you're dealing with kuberator things like restarting the pod, increasing the resource of the pod, rolling back a particular version of deployment, so these are different actions that we as a human do, and the same actions is something that our agents perform on our compute platform or simply put Kuberti clusters. And all of these components have a tight observation loop where you constantly monitor the performance of the LLM and decide whether it's kind of responding in a way that you would anticipate it to be. Just to expand on the different set of agents that we've seen so far, there is a simple assistance where you prompt the agent, ask a question, and get the response back, you know, this is like the kids' play. And then you have the deterministic agent. This is where you define the evaluation logic for the agents, right? Like, especially when it comes to troubleshooting, you tell the agent that you have to strictly follow these, uh, you know, list of actions to get to the root cause of the issue. Things like as soon as you get prompted, check your audit log to see something has changed, isolate if it's an application issue or an infrastructure issue. And see if there are any dependent issues that are failing and and have that checked as well. So when I say deterministic agents, this is where it provides non hallucinated output. It doesn't look into anything else. It just purely follows your script and provides a response based on that. Thirdly, you have the autonomous agent where you don't have to feed quite a lot of context. It can figure out itself. It has its own evaluation logic based on the model training, and it's gonna respo give you the response based on that. And finally you have the multi-agent collaboration which is going to be the focus of today's talk. There are like different individual agents, and there is an agent of the agents that kind of communicates with these individual agents, get the task done and provide the response back to you. Sounds good, right? As soon as we build one successful agent, are we gonna stop? Definitely not. The success for having a good and robust AI ops framework, I'm sorry, and agentic use case is having a good agentic framework. Now this is where you kind of define the security guard rails. You provide the access controls so that the agents, you know, don't don't behave in a weird way. You also configure the runtime. You integrate the framework with observability environment so that you kind of monitor the agent's responses, latencies, and whatnot. And this is exactly the place where you also integrate it with knowledge base so that it has some kind of a context when it comes to troubleshooting issues specific to your environment. All right, let's get to the, uh, the, the real problem statement. So I've been working with Shrikha so closely, um, so when, when, when his team had challenges, so they managed 1,000+ cubs clusters, and you can imagine the operational challenges that they may have when, when dealing with those 10,000+ clusters. So they were having most of the time their engineers were spending their time on identifying the issues while the fix just took like a few seconds, right? So we wanted to come up with a prototype that kind of. Get insights from these telemetry signals and help them reach the root cause or or the real cause of the problem, you know, faster. They use Amazon EKS, so we started with a single EKS cluster, and this is based, this is built on top of Bedrock's multi-agent collaboration feature, which, which means that there is an agent of the agents which can communicate with individual agents. We had 3 agents with the prototype. There's 1 agent that communicates with Prometheus, gets insights from your time series back end. Now you can ask literally questions like, uh, is my CP what's the CPU utilization of a cube system name space? Do you see any kind of container restarts, um, and what's the memory utilization across the cluster looks like across the node looks like you can ask any kind of questions and all of these questions gets transformed as the prompt QL queries by the agent. So, um, the agent transforms the natural language to prompt QL, executes the query against the back end, and then gets the response back. And we also deployed the KHGPT operator to get kind of get insights about your QB events and real-time pod locks. So there is a second agent that integrates with the KHGPT operator and it's gonna give you information about what was wrong with the specific pod around a specific point of time, right? So you're gonna get those kind of insights. These two agents. Comprises your MTTI side of the equation where you, where you kind of understand the meantime to identify your operation issues. The third agent is the Argo CD agent. We also wanted to go one step ahead and would want to remediate issues like some of the well known issues, right, like what are the Argo CD operations might help us with, like increasing the resources, restarting the parts. So we build a third agent that integrates with your Argo CD controller and it can do all those basic actions. So this particular agent is, it's part of your MTTR side of the equation. So within a single prototype we were able to address both MTTI and MTTR of the equation. Put it in simple terms, this is how an interface would look like. There is this collaboratory agent that will act as your central UI for your ops team. They can ask any kind of questions like, Hey, are you seeing any errors in my Cooper is clusters? So it's gonna reach out to KHGPT, get the response back. It's gonna tell, Hey, yes, I do see 6 errors. And then when it comes to understanding the utilization of your deployments, you just ask questions about like what's my utilization like, and it's gonna reach out to Amazon managed Prometheus or the agent that integrates with your Prometheus back end. It's going to give you the response. And finally, if you want to make any kind of corrective actions. There's a human in the loop process. You tell the agent, go ahead and increase my CPU of the so-called deployment. It's gonna get a confirmation from you. It's gonna invoke the Argo city controller, and it's gonna take care of the action. Sounds very simple, isn't it? So one stop shop where you can just literally interact with the agentic workflows and get the job done. Now it's time for me to hand it over to Shikan to talk about how they evolved from this simple prototype and build a robustic AI ops framework that help them manage 1,000+ clusters and finally help them reduce meantime to identify and remediate Kuberator operation issues. Thank you, Vikram. So, good evening, everyone. I've been building software systems to manage infrastructure for the last 20 years. We've all built a lot of automation over time to keep our production system stable and highly available. Even though we built a lot of automation, but there still remained a gap where we are still reliant on human intuition to solve some of the complex production issues, be it a hard performance problem, be it a cascading set of failures, or, or, or even the most subtlest of things like figuring out why something that is working in production is not working anymore. I've always dreamt about the time that this whole infrastructure space can become completely autonomous and the machines can really manage themselves. That's why I'm super excited about the emergence of Gen AI and the possibilities it provides us to uh to build towards the autonomous future. I'm Shrikantan. I'm a senior director of software engineering at Salesforce. I'm excited to talk about our journey to implement an AI powered self-remediation loop, uh, to solve problems on a real, uh, large scale production community deployment. A quick introduction to who we are at Salesforce. Uh, we are the Hyperforce, uh, Cubunities platform team. Hyperforce is the next gen infrastructure platform for Salesforce. Hyperforce powers, uh, all of Salesforce clouds, including the Sales Cloud, service cloud, marketing cloud, Agent Force, MuleSoft, and more. We are the compute layer within Hyperforce. Uh, we provide, uh, Cubanis platform as a service for all, uh, internal application teams within Salesforce. We basically abstract all of the Cuban Redis life cycle management complexity away from the product development teams, so the product development teams can can focus on building their application logic and they do not have to really worry about the infrastructure. We run a wide variety of applications on top of our platform from huge Java monoliths to high latency, highly transactional databases to low latency caches, big data and streaming applications, networking applications, and more. Here is a simplified view of our tech stack. I have not listed all the components that we manage as a platform, otherwise the font size would need a microscope. So essentially we are a multi-substrate platform that spans multiple cloud vendors. As an enterprise platform, we provide out of the box support for all standard Kubernetes capabilities like storage, networking, auto scaling, load balancing, mesh and invest. We also provide easy integrations to Salesforce, custom infrastructure services for managing identity, authentication, secrets, certs, and more. Our secret sauce is to provide clean and easy to use abstractions to the development teams that allows them to build and ship their container containerized applications quickly and safely. Uh, in the essence, we exist to bridge the gaps with native Cubans, uh, enforce the best practices, and manage the cost and complexity of, uh, of, of running this, uh, Cubans fleet so that our product teams do not have to do, do it all on their own. A quick peek at the operational scale with which we operate today. We managed more than 1400 clusters across multiple substrates. We run hundreds of thousands of compute nodes and our platform scales millions of pods today. Uh, we, we also provide support for 40+, uh, operators, controllers, and integrations, and there is a ton of monitoring that we have built for, uh, observing this massive infrastructure. More importantly, we spend a lot of time today, uh, for supporting this infrastructure. More than 1000 hours is, is spent on, on support. So we are actually looking at, uh, we're at an interesting point as an infrastructure platform. We are looking at 5X growth for our platform in the next couple of years. So, uh, solving for the operational scale, so scaling our operations to 5x the size of this platform is one of the important business goals that we have today, and we are actually looking at AI as one of the ways how we can scale our operational support for our platform. Uh, I also wanted to To talk quickly about the evolution of our tooling in general, we have built a lot of tools to manage the complex infrastructure that we had over time. Many of this is in-house, but some of this we are open source like Sloop. We have, uh, we have tons of dashboards for visualizing our monitoring and metrics. We built sloop for visualizing the historical state of human resources like pods, deployments, and demon sets. As our fleet grew, we wanted some tooling uh to identify configuration drifts across clusters and uh and figure out uh anomalies. So, so we built something called Periscope that does cross cluster fleetwide analysis. We built Cube Magic Mirror to automate our Kubernetis troubleshooting workflows and uh get an auto generator RCA. We build CubeMagic timeline, uh, which can help correlate events across various layers of infrastructure, uh, to find anomalies and more interestingly, we build something called a pseudo pseudo API that basically streams, uh, Kuberne is data, the events, resources, and other data from live production clusters onto a secure database in, in, in an internal environment. We built a pseudo API server that mimics Kuban Eis API but talks to this database in the back end, so this allowed us to do cube cuttle on a read-only replica of the data from Kuban Eiss without even logging into production. So we had built a lot of tools over time. But it has not solved our operational problems fully. So there are still gaps. So what are those gaps? These tools are pretty much siloed. They don't really talk to each other. We have to manually pass context between them to troubleshoot issues. The learning curve is still high. Every engineer on the team needs to learn all of these tools and also know how to effectively use them, how and when to effectively use them to figure out problems. And the feedback loop is somewhat limited for this tooling though we tried as much as possible to implement periodic reviews process, we have not been successful like. Most of these tools remain static over their lifetime and and these don't get uh uh uh uh these don't get a lot of uh. Sorry, these don't get updated uh frequently. So all of this put together, um, the operational toil is still high. And we needed some solution with which we can scale our production support. That's where we saw the opportunity with AI agents. Uh, using AI agents, we, we thought we could solve some of these problems to improve our MTTR and also eliminate human toil in the process. So Gen AI is making waves. It's fundamentally transforming the way we operate in the platform engineering space. We are trying to leverage it across the various areas of our software development life cycle. Like we're already seeing the impact of this from white coding to spec driven development for developing features, uh, like new Kuberitis operators and also using agents for automating the QA functionality and, and the, and the analysis of complex, uh, release and, uh, build failures. However, this talk is specifically focused on how we are applying this transformation into the operational space, especially using agents for self-healing and infrastructure resiliency. So when we started looking at the agents, uh, we didn't go a big bang from the beginning. Uh, we, we chose to start small. Uh We wanted to first understand what these agents are capable of and how best to leverage them. So our first agent was the on-call report agent. So every week our on-call engineers uh prepares a report, uh, which summarizes the happenings of the past week, uh, the incidents that happened in the last week, what were the alert trends, what are still the open, uh, what are still the open investigations that needs to be handed off. So all of this is nicely captured in the report, but it was all done manually. Now we automated this process using AI agents. These agents connect to Slack, connect to our alerting systems, connect to our observability systems, get all the data and, and. And synthesize them and put together a nice report, uh, uh, so our engineers don't have to spend time, uh, creating this report on a weekly basis. It, it automated about 90% of the work that the engineer would do and the engineer can just do spend, spend 5, 10 minutes to summarize this, and then the report is ready. So this was an instant hit. As soon as we built this agent, our on-call engineers adopted this immediately, and it saved some toil for them. Then uh we had an idea to automate cubecuttle commands using natural language queries. We already had the pseudo API which can uh query the status of uh uh production clusters in near real time. So we built an agent that uh that basically looks at Slack, the questions on the Slack, translates them into cube cuddle commands, and executes them using pseudo API. This, uh, this was very handy with, uh, our troubleshooting. Uh, our on-call engineers can directly ask in Slack, hey, what is the status of this part on this cluster, and they would get the results directly on the Slack. And this also, uh, got a lot of adoption from our application teams. They were, they started using this for, uh, for, for troubleshooting their applications as well. So after the success with cube cartel agents, um, we, we then expanded our agency capabilities, uh, with the life site analysis agent which is slightly more complicated than the other two. So we have a rigorous weekly process where. where our engineers go and Look at the availability dips and other golden signals for all the components that we manage across the 11,400 cluster fleet. So engineers spend a lot of time looking at the dashboards, uh, trying to figure out which clusters had availability dips, and then spend time. Triaging them, why did the availability, uh, dip happen, uh, and, and fig and figure out the mitigation steps like how can we avoid this from happening again in the future. This was pretty laborious. Uh, the engineers were spending at least a couple of days, um, uh, every week. So we automated this work with uh again with uh with AI agents. Uh, it, uh, the in the AI agents perform anomaly detection across all the 1400 clusters automatically finds the availability depths, and it also does the first level RCA to understand what caused these availability dips. Uh, this greatly, uh, eliminated the human toil that we had to spend on the repetitive work on a weekly basis, and this was, this was again a huge hit, and our, uh, our uncle engineers really love it. So as we built more and more agents like this, we learned a lot. And we also clearly saw the potential that we could apply agents to implement a self-healing loop in production and which can greatly improve our production support problem. So here's a quick screenshot of our on-call report agent. This is a screenshot from Slack. So our engineer is asking the uh AI ops agent, hey, generate the on-call report for this time frame. The agent is able to generate that, uh, create the doc and share the, share the doc link uh directly in Slack. Similarly, here is another screenshot from the cube cuddle agent. Here the engineer is asking, hey, get me the events from this name space for this specific deployment. Uh, so the agent is then able to translate that question into a cube cuddle command, execute them using a pseudo API and, uh, able to get the events and show them directly in Slack. So after a lot of experiments, we landed on this multi-agent framework for implementing the self-healing. We heavily rely on the, the managed internal agentic framework and the embedded RAG database to, to power this. The agentic framework is responsible for the data governance, especially ensuring Data privacy and data security. Given that we are dealing with sensitive logs, metrics, and other telemetry data. The data governance becomes really critical. Fortunately for us, uh, this lang chain land graph-based agentic framework was managed by another internal team, and so we could just directly leverage it for our for our architecture. So let's talk about The self-healing loop. How, how does this work? It all starts with the alerts. Uh, when the alert comes, it lands in Slack. So Slack is the space where we collaborate and track the, uh, the progress of the investigations. As soon as the alerts land in Slack, the manager AI agent kicks in. It plays the role of the chief orchestrator. So the manager agent leverages the power of the runbook knowledge that we had ingested into the rag vector databases and the infrastructure and topology knowledge that we provided as context to the LLM. So, the manager agent then augments the alert data with the context that is retrieved from the rag, uh, runbooks, and it performs the reasoning with the LLM to uh To figure out the right troubleshooting steps for uh debugging the problem. Once the manager agent generates the troubleshooting plan, it then needs the telemetry data to correlate the problem. Uh, the telemetry data like logs, metrics, events, and traces that are spread across various systems. So the manager agents then delegates the task of gathering the required troubleshooting data to the set of worker agents. The worker agents specialize in talking to various data systems across the infrastructure, get the right data, and transfer it back to the manager. The worker agents use MCP wherever possible. Our adoption of MCP is somewhat limited. Not every infrastructure system has MCP, so wherever it's available, we use it. If not, it's a direct integration with that specific system. We also made a very conscious decision that uh. We want to reuse all of the existing automation tools that we have spent that we have built over the years, so we built specific worker agents that That talks to KMM, loop, pseudo API, Periscope, and all of the other tools that we built. So these tools can enhance the troubleshooting for us and, and, and we are reusing them as much as we can. So once all of the data is, uh, troubleshooting data is retrieved, the manager then synthesizes the data, again uses the power of the runbook knowledge and the LLM based reasoning to summarize the root cause. And once the root cause is ready, it is then passed on to the AI remediation agent. The remediation agent is responsible for figuring out the right set of actions to take to remediate the problem. These actions could be restarting parts, restarting nodes, uh, doing a rollout, restart on a deployment, or, or even changing some configurations. So the remediation agents use something called a safe operations to execute these operations in production. I'll talk about safe operations, what it is in a bit. So when, uh when, when we let AI to take actions directly in production, we wanted to have critical human oversight to make sure that it is doing the right thing. So we implemented a Slack-based. Human in the human in the loop approval process. Unless explicitly allowed, the AI remediation agents cannot take any actions in production without human approval, at least in the very beginning. We also implemented a Slack-based feedback loop, so when AI makes mistakes, when either with the troubleshooting steps or with the remediation steps. The on-call engineers can quickly click a button in Slack and provide that feedback. That feedback is then captured to improve our agents and run books. So essentially this architecture allowed us to reuse all of our existing tools and their own books and then augmented with the intelligence of the ages to mean agents to meaningfully connect them and implement uh uh a full self-healing loop. OK. So when we talk about letting agents uh take actions directly in production, the biggest elephant in the room is chain safety. How are we gonna make this safe? How can we make sure that the AI does not hallucinate and it makes the correct decisions all the time? What are the risks and how we can mitigate them? There are 4 things that we saw, uh, starting with unbounded access. If we let AI make changes at will in production, uh, it can be really catastrophic. AI can choose to delete your Kubernetes control plan, can choose to delete your application or your note pool, uh, and all of them can result in outages. So we wanted to limit The access of AI to a very limited and curated set of operations, and AI AI AI cannot do anything that we do not allow it to. This greatly mitigates the risk of AI making catastrophic changes in production. The second thing is the lack of guardrails. There are tools like Cube cuttle or cloud SDKs. They lack the necessary operational guardrails to ensure safety. We'll talk about these guardrails in the next slide. So implementing safeguards or guardrails for every single operation makes these actions safer in production and also wherever possible, we also need to make sure that every change that we do is quickly roll rolled back in case it causes additional issues. Number 3 is poor visibility. If we do not track the AI operations closely, it can increase the risk that we see in production. So establishing strict change management processes and auditing controls and making sure that we have a periodic review process to to track all AI-driven operations would greatly help reduce that risk. And lastly, even with all the guardrails and the visibility that we put. We still want to be sure, and we, we need to have a way to guarantee that AI is making all the right decisions in production. That's where adopting progressive autonomy can help. Uh, to begin with, every no operation can, uh, can happen in production without human approvals. As we gain more and more confidence with AI, we can relax the constraints and give more autonomy to AI over time and let it take actions, uh, one by one and let it, let it, uh, so we can allow AI agents to make more autonomous actions one by one as we gain more confidence. Talk a little bit about safe operations, how we implemented it. So we implemented it using Argo as an Argo workflow. Every single operation that we built uh is an Argo workflow that has the necessary guardrails. AI agents can access these Argo workflows uh through our in-house compute API with obviously with necessary human approvals and oversight, and all of these operations that the AI agents are doing are closely tracked via change management and, and we have built a lot of visibility with dashboards. So what are these guard rails in general? How can, how can it make these operations safe? For example, if you're restarting a part, are we respecting the, the port disruption budget? If we directly do cube cube cuttle delete on uh on a deployment, it can, it can cause a partial or full outage. Similarly, if you're restarting a set of nodes, how many of them are we restarting at a given time? If we frequently restart these nodes, it can again result in outages. If you're scaling up or scaling down a cluster, are we looking at the utilization of the cluster? This has happened to us a couple of times where we scaled down a busy cluster and it has caused incidents. These kind of issues can easily happen if we don't have the right guard rails in place and let AI take action directly. So essentially we looked at what our seasoned engineers do to make our operations safe in production and coded them as guardrails within every single operation that we allowed AI to take. So to to sum it up, the combination of safe operations with guardrails and the necessary human oversight at the beginning helped us gain confidence with AI agents, and, and we were, we were able to implement them in production. Here is a screenshot of our remediation approval process, again in Slack. So there was an alert um that a node had some problem or an application had some problem. So the AI agents investigated and figured out that this specific node has some disk related issues and draining this node is the right fix. So, uh, it asks for approval uh from the on-call engineer in Slack. The on-call engineer can look at the RCA. Uh, make sure that, uh, AI AI investigation is correct and it, and this is the right remediation step, and he can click yes to basically drain the node in production. In fact, we added one more layer of approval after this. So after the on-call engineer clicks yes, it goes to another engineer or a manager who has to approve before the action gets executed in production. So what is the impact so far with the implementation of the Asiantic loop? So as we deployed agents into production, we implemented a set of success metrics. We saw a considerable improvement on our troubleshooting time. It improved by 30%. And the whole agents, the agentic architecture kind of saved. Almost 150 hours a month, which roughly equates to one person or one engineer worth of bandwidth that we could apply on other areas. As we implemented This AI powered self remediation loop, we learned a lot. There are many takeaways. Here are some of them. The most important thing uh in the with this architecture is the run books. The, the structure of the run book and the accuracy of the runbook determines the success of this uh overall architecture. Like if you have AI is only as good as the data that we provided. If you have duplicate runbooks with conflicting information, we can't expect AI to make right decisions. Uh, uh, runbook structure also matters. We figured that the structure of the runbook also has a direct impact on the efficiency of the rag chunking and the retrieval process. So in summary, having a good run book strategy which clearly defines. When and how the run books are created and modified and kept kept up to date is critical for the AI agents to work correctly in production. So we obviously spent a lot of additional work to build safe operations, but we felt it's non-negotiable given safety and reliability are the most important factors, especially when we are letting the automation happen in production. Using strict LLM prompts can help reduce hallucination. We asked AI that for every decision that it takes, it has to be backed with real data. If some data is missing, you better not make any decisions, ask humans for input. Ensuring continuous feedback loops also helps, so the continuous feedback loops, uh, help us, uh, continuously train and improve our systems. Our run books improve, our agents improve, and the overall success rate of the self healing process improves uh with the feedback loops. And as I said before, uh, progressive autonomy helps, starting with full human oversight to review everything that AI is doing and slowly relaxing it over a period of time and uh. And, and giving it more autonomy and, and, and letting AI do independent actions is a viable way to scale your uh agentic actions in production. Also, leveraging existing tools helped a lot. It was, it, it's, it was at the quickest and the biggest wins that we had. Like AI agents were able to connect all of these tools meaningfully and we realized the value of using it pretty quickly. And last but not the least, the Slack-based user experience is also very powerful. We were, um, like, first of all, it, it, it made it very easy for us to implement the approval, the multi-layer approval processes and, and the feedback loops. Plus if AI makes any mistakes either with the troubleshooting steps or the remediation steps, the engineers can then directly collaborate on Slack. And continue the troubleshooting, fix it, and then, uh, the whole thread can be summarized and fed back as knowledge for improving the future decisions, so which makes the integration with a tool like Slack very, very, very powerful. And the added benefit is like we could then automatically trigger these agents on alerts as well as invoke them on demand, uh, from Slack using Slack, uh, Slack queries. So we have implemented the AI powered self remediation loop. Uh, what next? Uh, we feel that we have only scratched the surface of what the AI agents can do. That is, that is, uh, this is probably the first step in a long journey ahead. Obviously we wanted to scale our AI agents to eliminate 80% of the production support toil that we have today. That's our ultimate business goal. And to get there, there are There are 2 or 3 things that we are exploring right now to expand our agentic capabilities. So the biggest problem that we had with the current architecture today is connecting the dots. Um, if, if If there is a complex Production problem that goes beyond a simple pod restart or a simple node restart, our runbook-based solution struggles to really connect the dots. Uh, like this is a, this is a good example of it. Like assume that there is an application that is experiencing high latency because it's internal core DNS call, uh sorry, it's internal DNS calls are timing out intermittently. Core DNS itself, uh, which serves the DNS records, is running on another node that is experiencing network bandwidth exhaustion. There is another part that is running on the same node that is doing high volume network transfer that's causing the. Uh, the network bandwidth exhaustion. So, how do we meaningfully connect from the pod that is experiencing the application, uh, latencies to the core DNS that is running on a different node altogether. To the node that is running the core DNS to the NI card, and then to the application that is really causing the problem. It is very difficult to write a run book that that that solves this problem even if we manage to write this, it is very specific and for a very single use case. How can we do this meaningfully across the entire scale of the infrastructure for all kinds of problems that we can face? That's a very humongous challenge. So we are trying to or we are attempting to solve this using knowledge graphs. Essentially, we wanted to teach AI uh the same way how we would teach humans, like, hey, this is how the infrastructure is defined, these are the components. This is how they are related to each other. And this is, these are the failures that can happen at each component and if it fails, this is how it would impact other components. If we are able to capture this knowledge in a structure, structured way in knowledge graphs, we can then let AI traverse that knowledge graph and figure out, uh. The root causes for some of the complex problems which requires human oversight today. So this is one of the things that we are exploring. The other thing, um, the second thing that we're exploring today is like how best we can use the feedback, uh, uh, the knowledge that we are getting from our feedback, um, if we can record our successes and failures in a way. Where we know that for this given problem, the most, the most probable root cause and the most probable remediation step is this we can then let AI use that information to speed up the diagnosis process and also improve the accuracy of the diagnosis. So how can we record the history of success failures in a very meaningful way that is that can be used by AI is another thing that we are exploring. And lastly We wanted to explore the possibilities of what AI can do beyond just the runbook executions and beyond this, what we tell it to do. There are so many hard, hard performance problems that we face today, that even humans don't know what is causing it, and we struggle over weeks, days or months to find the root cause. There are millions of metrics and data points. There are terabytes of logs. Can we throw it all to AI? Can AI help fish out the anomalies from this vast amount of data, and can it tell us where the problem is? Like this is one of the areas that we want to explore. We don't have a good answer yet, but, but hoping we find something useful from this exploration. With that, uh, we are at the end of the presentation. Yeah, thank you everyone for listening to it. Hope, hope you all have a very good rest of your days.