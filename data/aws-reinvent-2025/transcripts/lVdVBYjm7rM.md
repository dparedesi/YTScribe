---
video_id: lVdVBYjm7rM
video_url: https://www.youtube.com/watch?v=lVdVBYjm7rM
is_generated: False
is_translatable: True
---

Hi everybody, welcome, welcome. Thank you for the patience. We are ready to go. This is a 20 minute, uh, lightning session. Um, in this next 20 minutes or so we're going to talk about work that we at AWS have done with AstraZeneca running their genomics pipelines on the cloud, specifically on our latest generation of FPJ instances. Um, I'm Marissa Powers. I'm a high performance computing specialist solutions architect focused on life sciences. I spend most of my time working on drug discovery R&D with our largest pharma customers like AstraZeneca. Um, and we are fortunate to be joined with by, uh, Sean O'Dell from the AstraZeneca Center for Genomics Research, and this is work that we did together. So. Um, we're going to cover why we did this work, why do we run these genomics pipelines, um, on latest gen FPJ instances on AWS. Um, Sean's gonna, gonna do that piece. We're gonna cover what we discovered when we compared to previous gen, and because this is reinvent and we like to go into the details, we'll talk about, of course, how we built it, the architecture, and we'll end with a demo before we go through why, what, and how. Let's cover who. So I'm Marissa. This is Sean. Manu Pillai is a fellow HPC Life Sciences solutions architect based in the UK. He built out the AWS batch and Nextflow infrastructure that we use to run these pipelines. If you don't know what those things are, that's OK. We'll cover it in the next few slides. We also had lots of help from Hijun and Shamal from Illumina, Natalia, Eric, and Omar from AWS, and Gabriel Hernandez at AstraZeneca did the concurrence testing which Sean will cover. OK, so with that, I will hand it to Sean to talk about how AstraZeneca runs these pipelines on AWS. Great, thank you for that, Marissa, and thank you for joining us today. So at AstraZeneca we think about genomic processing kind of in these three categories. First, we develop standardized workflows and pipelines across different modalities. For example, for exome, we'll run a slightly different workflow than we would for a whole genome, and it's really important that when we do a migration or when we upgrade these workflows that we're able to demonstrate. That the results after the upgrade are exactly the results before the upgrade, so that's really important, as you would imagine in genomics processing. Where some of the results might be used to impact drug discovery or clinical trials, so we need to be able to go back at any given time and be able to reproduce those results. Oops, and I jumped ahead by accident. So that's the first kind of thing we look at. The next is that this type of processing produces massive amounts of data. Like petabytes of data, tens of millions of files, and we need to be able to store and retrieve that in a cost-performant manner. We need to have some type of catalog that keeps track of all those files, and then eventually we need to make this, we need to take the insight that we extract from the data and make that available to scientists in downstream analytics. And then the third point I wanted to make here is that we. This workload is very spiky, so we receive large batches of samples during the course of a year, for example. It's not a steady state type processing. So when we receive these large batches of samples, we want to get through those as quickly as possible using Alummina using Dragon from Alumina so that we can get benefit from the costs that we've invested in that. So we need to do it very quickly. And so when we when we think about this migration and going from F1 to F2, we need to have some kind of test case, right, because I just said we need to prove that the results in F1 match the results in F2. So in this case. It's very simple. We just take samples from each modality like we have exome sequences and whole genome sequences. It's a known set of data. We want to run exactly the same dragon command line on F1 as we do on F2, and we want to use the exact same reference files. So it's a pretty easy thing to conceptualize in the sense that F1 should give us exactly the same results as F2. But we're hoping that we'll get performance gains and we're hoping that we'll get cost reduction. So when we, when we did this testing, you can see here that we have up to 62% speed up and the only thing that's different is moving from F1 to F2. The Dragon software version is exactly the same, as I mentioned, the samples are exactly the same. The reference files are exactly the same. And you can see here this is this performance speed up is, is consistent across whether it's an exome or a genome, so we're we're quite happy about that. We can do the same amount of work in half the time, so to speak, and from a cost point of view. We see a 71% reduction in cost, which is quite significant. It means we can take that money, we can reinvest that into different parts of the business, into different things to drive different aspects of science. So that is the speed up we got, the cost reduction we have, and then as I mentioned, we did, we wanted to do equivalence tests to make sure the results from F1 match exactly the results in F2. And I'm happy to say we can confirm that with bioinformatics tools and can confirm that exactly the variants that we see when we run on F1 and F2 are the same and all the metrics that Dragon produces are exactly the same. I mean minus the stuff in the header that depends on date and so forth. OK, so with that I'm going to turn it back to Marissa and she's going to go through how this was done and the architecture of AWS. So as Sean mentioned, we see a 62% speed up with next gen FPJ based instances and a 71% cost reduction. So let's talk about why that is. So these are the specifications for the two instance types. So the first row is the F1, which is prior gen. The second row is F2, which is latest gen. So we just want to point out a few key factors here. So the first one is the number of VCPUs. So the F2.6X large has 24 VCPUs as opposed to the prior gem which has 16. So that's 50% more cores. If you look at cores per chip, you'll notice that the F1 has two chips versus the F2 has one. it's actually a 200% increase in terms of cores per chip, and these dragon processes run on a single FPGA chip. So the number of cores per chip is the largest contributor to the performance increase that we see. Additionally, the F2 instances have up to 16 gigs of high bandwidth memory, and as we mentioned before, there's a 62% performance speed up, but even a proportionally larger cost reduction. Why is that? It's because the later gen instances are actually have a lower per hour cost. Um, and so you can get, even if they had the same performance, you'd have cost savings, so they perform better and, and you have proportionally higher cost savings. So this is helpful, um, as Sean mentioned, it's helpful to do, uh, capacity planning. It's helpful to know when you have to process tens of thousands of samples a year as they do to, to understand what the cost implications of that are going to be, um, but let's talk through in more detail how we did this testing. And how many of our customers architect for running genomics pipelines at scale. So the first piece we'll talk about is AWS Batch. So if you're not familiar, AWS Batch is our managed service for HPC orchestration for containerized jobs. So when you think about Batch, think about containerized jobs at massive scalability. When we say massive scalability, what do we mean? Um, this is a chart from a reinvent talk a few years ago. It's a virtual screening run we did with Dana Farber on batch. What you see on the x axis here is time in hours, and the y axis is number of VCPUs. So we were able to scale up to 2.2 million VCPUs and back down to zero over the course of 4 hours, and that was done on batch. So again, batch containerized jobs, massive scalability. So we talked about batch. We know that we ran these jobs on FPGA-based instances. Two other key components for this testing and this work is Alumina Dragon and Sacara Next flow. So, uh, Dragon is a set of FPGA accelerated tools and pipelines. It's commercial software published by Alumina. It allows you to run in 35 minutes the same pipeline that would take over 8 hours with commonly used open source tools on X86 hardware, and it's available on AWS as a marketplace machine image. For Sara, uh, for folks who aren't familiar, uh, Saarra Nextflow is an open source workflow orchestrator. I'll talk through in the demo how it integrates with Batch, um, but it has advanced container support for reproducible workflows, as Sean mentioned, that's really important for not only AstraZeneca, um, but also for, you know, many of our pharma customers and just scientific customers in general. We also use Sacara platform, so that's a really nice web UI that they have for, um, submitting and monitoring and managing jobs, and I'll show you that in the demo as well. So from an architecture standpoint within AWS batch there's a construct of compute environments. So compute environments is where you specify the compute that you want your jobs to have access to. So this is where we specify that we want the FPGA instance types, specifically those F2.6X larges. We have two compute environments because, as Sean mentioned, we're running on two different versions of Dragon. So Xomes run on 436. That's a 436 AMI. Genomes run on 378. That's a 378 AMI. And so we have a batch que mapped to the exome compute environment and a genome queue mapped to the genome compute environment. Go ahead from my local machine. Um, and from your local machine if you're running it this way with Nextflow you can just submit a single command Next flow run point to the run script in this case main.nf and in Nextflow your top pipeline is defined in main.nf and then you specify some parameters that you want to run with. In our case we have a flag for whole exomes that tells Nextflow to submit to the batch whole exomeQ so that those jobs land on that 436 version that we need to run on. Um, we're also going to specify with tower with report that's going to send metadata to the web UI from Sacara that I mentioned. So I just submit this single command. Next flow is actually going to create an AWS batch job definition and job, and it's going to submit that job to the queue as specified with the flag. On the back end, batch is going to dynamically provision an EC2 instance in my account. The data sets are staged in S3. So once the instance is provisioned, it pulls a container down from the container registry. Input data sets are copied to that instance. The pipeline runs. The instances dynamically, uh, output files first are copied back to S3, and then the. Since it is dynamically terminated, so it's all it's pay as you go pricing, um, and this type of automation and dynamic provisioning and termination is nice when you're running one pipeline at a time, really important when you're running tens of thousands of samples as they are at at AZ. So, um, just to show, uh, and then in parallel we're sending again metadata to the, um, Sacara platform launchpad so that we can monitor jobs through that UI. So when I want to submit a genome sample, I just change the flag to type WGS, and again it just submits to that queue. OK, so now we'll go through a quick demo video to show what this looks like in action. So from my local terminal, I have my single command next slowruma.nf. I'm going to specify that I want to submit it to batch. We're just gonna go over to the console and confirm there's no FPGA instances currently um in the console in my account. I'm gonna submit the job. Nextlow is gonna output some information about the run first my Nextflow version, and then notably it's gonna output this unique string identifier, in this case Stoic Noble, when we go over to the web UI. This is the Saara launchpad Web UI. There's no jobs. I hit refresh and you'll see the stoic noble tagged job running. And when we click on it, this is just a quick snapshot of some of the metadata you can see for the job. So it shows you the container that the job is running based off of. This is really helpful for that reproducibility component again, um, and it also show you just for example, the paths to your input and output data sets for that job. Again, this is all just built in with Saarra, uh, Nexlow functionality, um, and Saarra launchpad functionality. So if I go over to batch in the console, I can see there's a batch job submitted. It's in a runnable state. That means that the resources are being provisioned on the back end. Um, the instance is still not there. It's gonna, it takes 2 minutes to provision. In the meantime, we can go over and look at what these run scripts look like. So there's two main scripts. There's main.nf and next flow config. So main.nf is where you actually call Dragon from. So we're going to specify the parameters that we want to run the pipeline with. We're going to, including the input and output data set paths, and then here you can see within the script section that's where we're calling Dragon and where you specify the tools that you want to call from Dragon. So, um, this is main. NF and then, uh, this is pointing to some more, uh, key files. These are reference files. In this case, these are from Jeanie in a bottle, um, which is an open, uh, data set. Nextflow.config is where you specify more so uh the batch infrastructure components. So, um, these are the, um, this is that I want to submit to batch. You can have different profiles. So if you want to run your pipelines on batch versus a different orchestrator versus even on-prem, you can configure that all in Nextflow.config, um, and this is where we set up our logic for that parameter flag. If I say WES, submit to the batch queue for exomes. And now if we hit refresh, we can see the EC2 instance got provisions dynamically just after submitting that job. Batch has a couple of different back ends. So this is using uh ECS, our elastic container service, which will provision EC2 instance in your account. If you used a servius option like Fargate, um, it would take seconds to actually start the job. So there's different options for the back end. Kubernetes is also supported as a back end for Batch. Cool. So quick, uh, whirlwind on how these pipelines were executed and that dynamic provisioning of resources. Again, it's, it's nifty tooling if you're doing testing. It's crucial tooling if you're doing production runs at scale. Again, tens of thousands of samples per month like AZ. So now we're going to cover quickly a few learnings from this engagement and I'll hand it back to Sean. Right, OK, um, yeah, I don't need it actually, so I only have this uh few points here. Excuse me, um, right, so thank you, Marissa, for that. I, so there's two things, two lessons or two points I wanted to pass on which you probably know already, especially because I think anyone who's coming to this is probably in the genomics or dragon world, or you're just waiting until the next one starts, uh, but basically, um, you know, equivalence testing builds trust with the scientists or with users. It could be any end users, so it's really important. That not only do we show oh this is so much faster and this is going to cost less, but that we do have exact horns or we do have exact. A quick, can you hear me now? OK, so that we, we, that we do have, uh, equivalent. So that's important to build trust with the end users and the, uh, the second point I just wanted to make is that, uh, it's very important that we keep data and compute in the same region. There's regulatory reasons to do that, data residency reasons, uh, there's also egress, um. You know, you don't want to incur a lot of data egress or S3 egress and the architecture and the demonstration that um Marissa gave in my view, you could apply that easily to a multi-region architecture. So if you put some flag at the beginning, where is my data? Oh my data is in Virginia, that means I need to start my processing Virgin in Virginia as well and I think that the architecture that was demonstrated, uh, facilitates that. Thank you. Right. Great, um, a couple learnings from, uh, the AWS side. Um, we, if you go through the aluminum documentation for running these pipelines, you'll see that for AWS they recommend actually setting up a 2 terabyte EBS, which is our elastic block storage, um, volumes, a 2 terabyte raid zero configuration with 4 500 gig, um, volume. We did some comparison testing for that versus local MVME storage and found that they are very similar, almost exactly the same from a performance standpoint. So talking to Alummina, um, essentially the guidance is to use the local MVME storage. There's almost 1 terabyte of local MVME storage with the F2.6X large instance type. So if your data set fits within that capacity, use local MVME. If it's larger, then you need, you set up the, the ABS volumes. Um, we also found the publicly available guidance is helpful. I'm sure all of us have had that experience. So we're in, in a, in a nod to that to contribute back to it, there's a blog post coming out probably in the next couple of weeks that has all the detail on the benchmarking, um, and also on the concordance testing performed by AstraZeneca. So keep your eye out for that. OK, and uh with that I wanna thank John for the collaboration. It's been, it's been a joy um and yeah, thank you all for joining the session. Please do fill out uh the survey in the mobile app so.