---
video_id: 0sg7KtCuhh0
video_url: https://www.youtube.com/watch?v=0sg7KtCuhh0
is_generated: False
is_translatable: True
---

Welcome everybody to our session. We're diving deep in the evolution of AWS load balancing and new capabilities. I'm gonna go over the agenda with you and introduce my cohorts here. Um, first up will be Matt Lewis, our senior principal speaker and senior senior principal SA, and he's gonna take you to dive pretty deep into nitro and all of the things that power our ELBs. He's gonna explain why they're so fast and how they're so performing and how they scale. Then I'm gonna take you through where they fit. So we'll be concentrating on ALB, NLB, and gateway load balancer, and I'll start with a, uh, uh, a simple three-tiered web app architecture, and I'll show you where they fit into those places for security, enhanced speed, and of course L7 processing. And then lastly, Melin Kulkarni, uh, the PM of NLB and Gateway Load Balancer, and some ALB will be taking you through some of the new juicy features that have been released lately, uh, recently, and we can go ahead and, uh. See what we've got. So I'm gonna hand it over to Matt. All right, thanks, Jamie. Exciting session. Thanks everyone for making the hike out here to Mendalay Bay. Um, if there's, if anyone takes any photos, feel free to find us on LinkedIn and, and share them, we always love that stuff, so, uh, I'll be talking about the architecture behind, uh, modern AW or modern load balancing. Not specifically AWS AWS is one part of it. And so AWS load balancing is basically made up of these three products, the application load balancer, network load balancer, and gateway load balancer, and we'll dive into each of those. To talk about how those architectures came about, we need to take a bit of a step back though and talk about what we call our classic load balancer, and this is the first form of uh load balancing on AWS so we called it our elastic load balancer. And actually uh when I first started working on AWS back in about 2013, Uh, elastic load balancing was kind of the aha moment for me. It was about elasticity, about how, how do you deploy many instances in a fleet, thousands of instances. You put a load balancer in front of it. Before that, I was working on physical hardware in data centers, building big metal boxes that were load balancers that had virtual IPs and we'll talk a little bit about that, uh, when we dive into on-premises load balancing and just talk about some of the functions you need to build a load balancer. So first, Let's talk about on-prem. Basically you've got 3 components here, you've got the central load balancer and then targets. Traffic comes into a central point and then it gets shared across the multiple destinations. Now, in a typical on-prem environment, the on-prem load balancer has things like ASIC-based packet processing, so it's highly performing, it's generally a lot of functions put into one. Device into one piece of metal that sits inside a rack. There's typically one IP destination called a virtual IP, which is the single destination for traffic. So you you have a DNS request, it resolves to that VIP or that virtual IP. The load balancer then spreads load across those three targets, and a load balancer will do things like security policy and routing policy. One of the more famous load balancer vendors has, Uh, a pretty robust way of doing TCL or I don't know if it's TCL or Tickle scripts, but basically you can define how traffic is routed through this central appliance. Now the reason why you need this appliance in front of targets, the targets are typically just commodity servers running a typical operating system like uh a Linux or something similar. General generally running some kind of application, uh like HA Proxy or something similar or Apache. And they're typically CPU and memory bound, so there's only a certain amount of CPU and memory you can fit inside that target, right, so you need to spread load across those many targets, and we'll talk about how that looks in AWS. Typically, redundancy is done doing things like TCP session synchronization across multiple physical devices. And so what we've got here is an active standby pair, where we're sharing the TCP session data from an on-premises physical single box to a secondary box, so that if that physical first box fails, the second can immediately take up the load and send traffic to those same targets, which hopefully are still online. Again, all physical stuff in physical data centers. And so there are some downsides to that. Basically what you're seeing here is a rack, it's a rack I drew, so it's very uh artistically, uh there's a bit of artistic freedom there, but basically you're deploying physical servers in racks of compute uh next to your load balances, and then you're scaling those racks, and I used to do this, I used to crawl around data. Centers and deploy new racks of commodity load balances and um servers that would run our applications, and we would look at the load on these servers and say OK we need to deploy another couple of racks in another couple of months, and eventually you get to a point, and I did get to this point in my previous careers, of building whole data centers worth of these kinds of uh componentry. Now the good news is. Here at AWS we've already built the data centers. So 38 regions worldwide. I don't know how many data centers are made up, uh, make up those regions, but basically we have these components where you've got the data center, uh, that are combined into an availability zone, and availability zones are combined into regions, and then we have this magical, amazing thing called a VPC. The VPC virtual Private cloud is a logical construct that allows you to deploy in any one of those physical pieces of compute. And then we bring in EC2, which is a VM or a virtual instance that runs on a physical server in a physical data center that Amazon manages. And so what a typical architecture looks like is something like this. You've got EC2 instances sitting inside subnets. You might have something like a net gateware or a network firewall. These are all pretty standard functions that run inside AWS inside the VPC. Now, let's go back to our web app. We've got multiple EC2 instances, because again, software bound, CPU memory controlled in a single VM space, also multiple VMs across multiple subnets, across multiple availability zones, that's how we define whether you're in one availability zone or another. And each of these instances gets an IP address, different subnets across AZ's, and you've got a web application. Great. Now what folks were doing back in 2013, kind of prior to elastic load balancing, was using DNS and DNS is actually quite a great load balancing mechanism, and so what you basically do is have a single DNS host name which resolves to multiple A records, and those A records each represent one of the IP addresses of your web application. Now this isn't true load balancing in the sense that you're not spreading the load based on a per packet or 5-tuple basis. Millan's gonna talk about 5-tuple and why it's important, but you're spreading the load based on a DNS query, the response is an A record, the client chooses that A record and that's used as a destination. And so there's some downsides to just purely using DNS here. I say that, but we use DNS internally for a lot of stuff and I'll talk about that. Um, you can also deploy in AWS instance based load balancers and a lot of customers do do this, particularly on products like our AWS Outpost products that don't have, ah, our network load balancer and gateway load balancer, which I'll talk about. But here we've got instance based load balancers, and again we've got our, A Records, or you could have a single public IP elastic IP and at the internet gateway, you can monitor your second load balancer, first load balancer, and shift that elastic IP assignment. And this is a common architecture to do active standby for instance-based load balancers in AWS. So what you're basically doing is saying, I've got a single load. Answer, it's a, on an instance, it's quite a big instance, 16 X large, 32 X large, so you can handle the amount of traffic you need, and you're shifting an elastic IP from one instance to another across availability zones. Interesting architecture because the elastic IP is actually not AZ bound. Now, we start talking about classic load balancer. And this is where we basically said, OK, these architectures are cool, but how about we build this as a service and offer it to customers so that customers don't need to do the heavy lifting there and build instances with lowbs, etc. Now the truth is, I talked about DNS we're actually using DNS on the front end. And so what we have here are two endpoints that, one in each availability zone, that constitute the classic load balancer or elastic load balancer service, and the DNS name, or the alias record for that, uh, classic load balancer is resolving to each of these two A records. So we're using DNS for that front end again and having instances inside our account, which I'll talk to in a second, doing the load balancing. Now, if you had an an internet gateway again, it didn't have public IPs and you'd have this elastic IP map, pretty straightforward. OK, what's actually happening under the hood is these elastic IPs are a little bit special. They're a cross VPC ENI attachment, which you can now do as a customer inside your own account, but we have a Amazon Service VPC, and we drop those ENI's from instances inside that service VPC into your VPC. And so we actually operate standard EC2 instances for our classic load balancer service inside our VPC and your traffic is then hitting those instances and being load balanced across the targets that you see in the bottom of the diagram here, across the web instances. And so one of the things we can do here is we can scale up those instances, we can scale them down, and you don't even notice. And that's where uh we used to say you'd have to do things like pre-warming a CLB if you think there was going to be a large peak in traffic, if you had a big event going on. Uh, that goes away with NLB and Gateway Low Bouncer, I'll talk about that. But, you would actually see if we scaled out, because what we would do is add multiple A records. In the alias record or the the C name for the classic load balancer, so here we've actually added two more in each availability zone because the traffic dictated that, and we add another 2 more, A records for each AZ, so 4 more total, so you would actually see that happen. Now there were some typical architectures where um, you would build what's called a uh uh ELB sandwich, where you'd have a set of appliances with an ELB in the front and you had someone uh like a security firewall vendor, monitoring this DNS name every second to see if the CLB scaled up or down, so they could send traffic to all of the nodes, so there's some interesting stuff that would happen there with the classic load bouncer. OK, let's move on to modern day low dancing and AWS now. So ALB, NLB, and gateway load balancer, and we'll start with ALB. So ALB's actually a very similar architecture to CLB but there are a couple of key differences. And the main one being AWS Nitro. So our AWS Nitro system is where we basically took uh what we called a uh it was a Zen based uh hypervisor that did a lot of the network operations in software, so we allocated CPU cores on a physical server to that function, and we moved that down onto the network adapter. And so the network adapter itself, which is part of the Nitro system, now does those operations on a CPU that sits on the network card itself. And so systems like ALB are using Nitro to get more performance than what they would have previously. Think about the software bound processes versus hardware bound processes. And so this is a a quick diagram of what the Nitro system looks like, we've got the Nitroc itself. There's a very small KVM SHI as the Hypervisor firmware now, and we have a nitro security chip which basically controls the security of the nitro system and ah makes sure that the Hypervisor firmware is actually what it should be. So think about if it was, And outposts, the same technology sitting on premises, could someone put a firmware in there? No, there's actually the nitro security chip that that has that that check and balance, um, to prevent that. Now you can see here, our nitro enabled instances started in 2013, and our bandwidth per instance has gone up quite considerably, so we're up to 50 gig in 2021, and, As we step through our different versions of our nitro chip on our network adapters, you can see here our CHGN can do 400 gigabits per second. Per network adapter, and that's enabled us to scale, for things like AIM AIML workloads up to 12.8 terabits per second per EC2 instance, so we basically stack these nitro cards next to each other and can get that kind of performance. Now ALB and NLB aren't specifically using those large uh instance types because we don't need it, but um you can see what niro really brings. Now Nitro has encryption by default as well, so always on encryption for version 3 plus. And it's basically end to end in transit protection for your traffic, uh, and you don't need to change anything on your application to enable that. Now we recently, I think about a week ago, released VBC encryption controls, which is an account wide VPC enforcement mechanism to say, I only want to use these new versions of Nitro, uh, so I have, in transit encryption for all of these instances. And so what will actually happen here, um, ALB, NLB and things like Firegate will auto upgrade or auto migrate to these newer families of uh Nitro to support VPC encryption. So when you tick that I want encryption within my VPC we'll go and upgrade your or update your ALB etc. to those new versions of Nitro that support in transit encryption. OK, let's move on to NLB here. NLB's a, a very interesting one because we fixed one of the key problems that I was talking about. Hyperplane is basically the solution to the DNS thing that we were seeing earlier. So basically what we do now is, regardless of the size of the fleet inside the service VPC, we give you one IP address, one ENI per availability zone. And so that means that we can scale up and scale down that fleet as much as we want, and it has no effect on what you see within your VPC. So hyperplane is basically a, it's an umbrella term for many services that operate within that space, but it's a massively scalable infrastructure that uh is just presented to you as one ENI per availability zone. Pretty amazing. So we can scale up and scale down and you see no change there. Now gateway load balancer is also built on hyperplane. A little bit of a different load balancer here. I think about it as a layer 3, layer 4 load balancer where basically it's built for appliances, so you've got traffic coming in, an internet gateway hitting a gateway load balancer endpoint. And then hitting a gateway load balancer in another VPC, this is for if you want to offer appliances in a central security VPC. You can now drop those endpoints in many VPCs and offer that same security fleet, um, and have it shared or have it as a multi-tenant service. Some of our, um, firewall partners actually offer this as a service as well. So you can drop a gateway load balancer endpoint in your VPC and use say a Palo Alto or a checkpoint, for example. Now, one of the key callouts here with this architecture is, we still have hyperplane, it's for the gateway load balancer endpoint, but also the gateway load balancer as well. So you're actually going through two hyperplane fleets to achieve this gateway load balancer architecture, and Jamie's gonna dive into what those architectures actually look like in practice. Now lastly on hyperplane, it is an internal load balancing service. It's not something that's public that you need to really worry about. You look at NLB gateway load balancer, etc. and know that it's using hyperplane as the scaling mechanism under the hood. Uh, and again, it's using standard, uh, regular EC2 instances, but it's built for immense scale and multi-tenancy. Lastly, We have a bunch of services using Hyperplane and there are probably more by the time you read this from when I wrote this last night, because there are new services being released all the time, uh we just released one in preview, uh our proxy service which actually uses Hyperplane as well, so hyperplane is an inherent function that's built into a lot of the new services that we deploy. Alright, I'm gonna hand it over to Jamie to talk about load balancing in action. Thanks, Matt. OK, so I'll be, I'll just concentrating myself on ALB, NLB, and Gateway Load Bouncer GWLB as we lovingly like to call it. Um, and a couple of things I wanna, uh, just set the stage for right away is that I'll be doing things through an architecture, right? So the first thing we're gonna start with is L7 processing, and the architecture that I'm choosing is gonna be, as I mentioned earlier in the introduction, a three-tiered web app, and that is basically giving you all of the tools and information of how the application load balancer runs. So let's take a look at some of the features that it has. So this is an ALB at a glance, right? So I've got our targets, and these are the targets that the ALB can go for instances, lambda, containers, and IP addresses, which is pretty cool. And here are some of the features, and I say some of the features because if I listed all the features, two things would happen. One, this slide would be a mess, and two, Melinda would have nothing to talk about. So we're gonna leave some of the newer released features for Melin to, to speak on. But a couple of things I just want to point out here is we have WAF integration. We have authentication offload, right? We have, of course SSL, TLS and MTLS with pass through as well as verified mode, and we have something called slow start, which is pretty cool. So if you anticipate a bunch of people coming into your site at a given time, you can set a slow start timer to say slowly ramp things up so my back end doesn't fall over, all sorts of cool stuff. So let's take a look at the architecture we're gonna be talking about. Standard three-tier web app. I've got a transit gateway in the middle connecting my back end to my front end. I've got my ALB or my ALB in the front. I'm using security groups at this point. I've got a fleet of Engine X servers, let's say EC2 instances. It goes to my back end. My databases are read databases, right? Well, they're clusters, so we're gonna basically be interacting with the read part. And of course I've got some Kubernetes clusters and, and some lambda and that. But before we get into messing around with the architecture, the one thing I wanna talk about is an unsung hero for ELB, and that's the target groups. Each of the of our, of our, uh, ELBs require you to configure a target group because you have to put your targets in something to send traffic to. So let's take a look at that. First thing you're gonna get when you wanna go ahead and and create an ALB or an ELB or NLB, any of those types is what type of target group they have. Now how many of you here are using ELBs? Good. Most of you. So you get a choice, right? You get instance based, IP based, you get lambda function, and then of course you get the application load balancer, and at the bottom you can see, and this is exactly right off of the console, it kind of tells you which target group can fit for which load balancer. We also have a couple of other, uh, top, uh, excuse me, a couple of other options like protocols. You'll see some newer protocols in here, right, that again Melin will talk about, but, um, all the standard culprits are there and it also lets you know which ones that you can assign to your load balancing experience and then of course the versions that you can use for protocols so you can set all of that stuff up. We also have health checking. Now I'm gonna go on my soapbox a little bit for health checking. How many of you use TCP 80 to health check? Not one hand. Perfect. I did ask this question a couple of reinvents ago and I had half the room put their hand up, so thank you. So I'm gonna preach to the choir here a little bit and say make sure that you are actually using the URL for or the status page instead of just TCP 80 because you're, you can get into a gray failure with your application and your port could be open, but traffic can still be sent through. And here we have a bunch of advanced, uh, topics or advanced features that you can set for your load balancing as well. We also have options for other things like different attributes such as your, your targets, um, draining, if you want to do min healthy failovers, we have an option so that you can say, hey, if I lose 20% or half or whatever percentage you set of my targets in my target group fail it away, or I can fail it open just to make sure that I can handle whatever I can if there's an issue and then go ahead and take care of it. And we also don't want to forget the fact that target groups also let us do auto scaling. So let's dive into a couple of advanced options that you can do. One of them is weighted target groups, right? So I can have one target group going on just accepting my traffic and let's say I wanna build a canary for something new. I can just set my weight so 95% of my traffic goes to my main app and then I can canary my other with 5%. So you can do things for ads, moves, changes, blue-green deployments. It's got multiple applications as I'm sure you can, you can figure out. But how about algorithms? We've got a couple of options here round robin, right, which is your standard load balancing, uh, Route 53 DNS type, just go right after the other. We also have last connection, right, or last outstanding requests, and that's basically, hey, who has the most requests? Let's not send it to them. Let's send it to the targets that have the least requests right now. And we have weighted target groups, and then we also have our slow start again that I mentioned. But I have the weighted target groups kind of selected, and this is a kind of a cool feature. It's something that we can do that we call ATW or automatic target weighting. So let me give you an example of how this works. So we have a bunch of targets, right, and they're working just fine. We've got traffic coming in, and ATW or the automatic target waiting, if you select those two pieces, look at three different things. They look at 5 XX errors, TCP connection failures, as well as TLS connection errors. And throughout that we run an algorithm or basically run it against our peers to see if there's any anomalies. If an anomaly is detected, we don't just pull it out, right, because this could be a temporary situation, but what we won't do is we won't send it any new requests. It'll just handle the requests it has. Now either you or the system sorts things out, and when it does, the system recognizes that, again, constantly running that comparison and says, OK, it's, it's fine, we can send targets, we can send our traffic to our different targets. Now we don't hide all these metrics from you. If you go into cloudwatch, you can go ahead and look at these different pieces here. To see exactly the counts and how automatic target weighting is being sorted out. Let's do a couple of callouts features that make life a little bit easier, right? You may be using some of these yourself. For ALB we have something pretty cool, WAF integration. Now WAF doesn't live inside of the ALB. The ALB actually connects with it on the back end, and WAF allows us to do a couple of things, of course. You can use WAF for getting Shield Advanced, right? So if you have a DDoS attack or something's going on right away, and you have Shield Advanced with WAF on ALB, you can kind of pick up the bat phone, if you will, and say, hey, AWS, help me out, and they'll go in and help you edit your rules, sometimes even on the fly. We also like to go in and remove any known bat actors to keep them from getting to your infrastructure. Another piece we have is the authentication offloading. So instead of having to put all this stuff into your application, you can go ahead and integrate with Cognito. Incognito gives you a couple of options. You can do user pools using IM, right? We're all familiar with it because we use IM to log in, right? If you use any advanced features such as VPC lattice or anything like that, you're using IM for your policies. No difference there. You can go ahead and use it in your user pools or SAML connections or OIDC providers like OTA, AuthZero, all of those folks, um, all can be handled on your application load balancer in the front end. So where does it shine, ALB? ALB shines in a few places, not everywhere, right? I'm not gonna tell you that use an ALB for all of your workloads because it wouldn't be a very good part of the presentation if I'm telling you where the best place to put these load bouncers, but e-commerce, retail, like if you have an experience of a checkout, right, and you have your shopping cart, or you also have your catalog, and you wanna route that traffic because we saw that we can do any type of host-based or path-based routing, um, you have good news and publishing, if you're releasing a new story. Matt showed us how ALB can elastically expand and contract for the scale of what you need. Social media platforms, you got your movies and your stories and all that fun stuff. We're HIPAA compliant and PCI compliant, so it's a good fit for healthcare as well as finance and of course government for any critical applications. A lot of governments use ALB. So now we've looked at that, let's take a look at our network load balancer. Now, I like to look at the network load balancer as our connection. Load balancer, it helps us connect different parts of our workloads together and also serves as a connection mechanism to other things like VPCs and hybrid. So instances, ALB containers, IP, one of the interesting things about IP though, again, it can be on another service as long as you can route to that IP you can use it in your target group. So I mentioned we had read databases, right? So we're gonna use our network load bouncer for our read databases. We're gonna go ahead and slide that right in there. And now I have my own VPC. I used to be a database person, right? I'm a networking person now, and we're both paranoid. You know, those are the two main things that if they go down the network or the database, usually everyone feels it and you have tons of pressure. You're not like, you know, you're at your kid's birthday party and you're getting phone calls, so they want to be sure that we're gonna make things a lot easier. We're gonna make things a lot quicker, and then they don't have to worry about anyone messing around with their stuff, so they're putting their databases in their own VPC. So why is this good? If we're looking at NLB, it has TCP-based connections. Remember, they have long-lived connections as well. These connections will not go away unless you stop sending traffic and your idle timeout goes away. Uh, my friend Milly had uh coined the phrase flow hash algorithm, and it does distribute our connections to our databases evenly, right? So instead of going and hotspotting one read database versus the other, now I can be sure to spread the load out from the rest of my workload. High throughput, of course, NLB being hyperplane as Matt mentioned, it can handle a tremendous amount of traffic, as well as a tremendous amount of spikes. So if your traffic is extremely spiky, NLB is really, really good for this, and of course, the ultra low latency. So now how can I use my NLB to increase my speed, lower my latency, and also increase my security? Well, one of the things that I love about NLBs is private link. So you'll notice that I've also removed my connection from the transit gateway to this VPC. I really don't need it if this is my paradigm of how I'm going to my read databases. Now we can imagine the rest of this infrastructure, there's other connections populating the databases, but we're just talking about the reads in the front end. So I'm using Private link here. The reason why this works is that Private link, it takes the local IP address of the subnet that is an endpoint is sitting on, so it looks like your machines or your workload is just talking to something else on their subnet. You can go up to 100 gigs. Actually you can go a little farther, but when you do, we start talking to you about doing more advanced things like sharding your NLBs, um, TCP UDP support, unidirectional stateful flows, so I know that no one's gonna be able to go ahead and reach into anything else other than what I want. It goes in one direction and it responds to that one direction. And of course, it's all private. So this connection between these two VPCs is happening over the AWS backbone, and there's no internet access whatsoever. The last thing too is Private Link inherits all the goodness of NLB. So we get uh questions a lot about cross account, right? So. Yeah, it works. Let's say our, our, our database folks are even more paranoid and they wanna have their own account. They wanna have their own limits, their own payers, all that stuff. What we've just set up will still work, right? Because private link with NLB will go across town. And it'll go across region. So now we've got a new region we've got some container workloads in there and we wanna give them access to our read databases. This isn't as um worrisome for performance as we would like, but it's still quite performant, right? All this again going over the AWS backbone, all of our own fiber that we had laid out, so we don't have to worry about going to the Internet, dealing with what we call Internet weather. So one thing that we see here is we see that I have a bunch of containers as well as uh uh EKS system. Where does ELB help there? Well, we always recommend the load bouncer controller. The load bouncer controller for ELB allows you to configure your, your ALB or NLB with the configuration of EKS, so you don't even have to become a master of these two load balancers. All you have to do is just know which commands you need to put in and which configuration you need to put into EKS to get this to go. So what we're gonna do is we're just gonna show our three nodes. We've got our pods, and we have our ports for our pods, and then we're gonna use an ALB ingress controller. We can use an NLB as well, depending if we want to go more performant or not, but we're gonna do a bit more path-based routing and, and handle things at L7. But that doesn't restrict us only from using ALB. And as your application scales out or if you add more containers, you can go ahead and add multiple paths, and it'll go to the ports as you need IP preservation, all of that fun stuff. But let's say you need something a bit more performant, low latency, all the goodness that I mentioned of NLB, you can go ahead and do something called Direct depod. And what Direct depod gives you is the ability to just take those IP addresses and add it to your target group, and again, you configure all of this within your Kubernetes configuration. So if you need more direct depods, you can go ahead and do that. Remember though, you do not want to get into IP exhaustion and do everything direct depod because then you can come up with a bit of a mess. So that's why we have the two different options. Also, there's a lot of work being done on the load balancer controller for a lot of the new features that we're coming up with, so I highly encourage you to go ahead and take a look at this. So before I mentioned that NLB does hybrid connectivity, what does this look like? So I said that we have IP addresses as an option for our NLD and sure enough, here we are. I'm using these IP addresses now granted, the target group does not actually live in the data center. I'm just doing this to kind of show more of a logical way of this is done, but it does live in the VPC that the uh NLB is in. So we've grabbed a couple of, uh, IPs from on-prem because our Kubernetes clusters need to reference something on-prem. We've got a direct connect, uh, connection to our transit gateway. Metal works just fine, so I'm using that to connect up to those pieces. Where does NLB shine? Now I can tell you number one, I am a gamer and I love the fact when our our gaming customers use NLB because it helps keeping the ping times quite low and the connections are again as long lasting as you send traffic through it. Finserve or financial services is also great in the side of needing to have. Latent performance. Think of like ticker tapes or transactions that need to be done in a very, very quick amount of time. Same thing for ad exchanges as well. IOT is another great one for IP sending in, uh, TCP configurations in, uh, from the field like, you know, your Samsung fridge or, uh, your toaster maybe GE toaster, um, and then of course media and streaming when we wanna sit down and watch a movie at home, we don't want that movie to, to buffer or or lag and stream. We want it to be perform it and play just like we're in the movie theater. So the last load b we're gonna talk about is gateway load bouncer, and as Matt mentioned again, it's more for our security systems. So if we look at it at a glance, we have instances and IP that you can go ahead and do. We use it a lot with bumping the wire, and again, these are not all the features that you can do, just some of the more important ones I want to bring up. You do yours custom health checks, 3 different couples, 53, and 2, so source port. Destination port source destination, all of that fun stuff and then again Matt had mentioned it uses Geneve encapsulation so when you're sending packets in and out to the targets of a gateway load balancer, that packet is encapsulated by Geneve and then when it is stripped off right by the security devices, it still retains the original source and destination like it never even knew that it was actually subverted and sent to be inspected. So let's slide it, add this into our architecture. First I have to slide us back over to our front door. We've kind of outgrown the security groups. We've got a lot more traffic. We wanna take advantage of some of these partner firewalls that we know that you can find in our marketplace that work with our gateway load bouncer, and we're gonna go ahead and we're gonna slot that in right at our, uh, public subnets right in our front door. And in order for that bump in the wire to work, we're gonna have to add some routes we call it ingress routing to our IGW. We're gonna basically say if you're gonna talk to any of our targets, make sure you go through the gateway load bouncer first. That fleet, remember, again, as Matt had mentioned, it has the ability to expand and contract with auto scaling, so it'll work well with our workload. But once again, our database folks are ever paranoid and they want to actually have some inspection from any traffic going from our back end VPC into their databases. So if you make a little bit of room over here, we can accommodate this again by using those endpoints. Now the gateway load balancer endpoints are just like Private link in the ways that you can put them all over your, your workloads, and they will be the main way that you gain entry into your gateway load balancer into the traffic. And I also caution you to make sure that when you're doing something like this, if you're centralizing this VPC, which again I want to point out is not connected to anything else, so my infosec group is very happy. We have a lot to uh to work with and we can go ahead and expand, make sure those rules are understanding of the traffic pattern that things come and go in, right? So where does, where you load balancer shine? Well, it's a security device, so it should shine everywhere, right? So all of those places that I mentioned, a lot of customers in all those fields, they take advantage of the gateway load balance. So now we've kind of built out our architecture. Let's take a look at the bigger picture of what we built. So if we slide in here, first, we've got our ALB that we've used for the front door. We're using ATW, right, automatic target waiting to make sure we're getting the most out of all of our targets. Let's say those targets are very big and expensive machines, right, that, uh, cost a lot hourly. We want to make sure that there's nothing wrong, nothing's going idle, so we're using ATW there. And then next, we used ALB for our ingress controller for our containers, and we're using NLB for our direct depod for any of the things in our containers that require low latent, fast connections. We also used NLB as a connection piece, as I promised, I didn't use anything else to connect up our uh our, our VPCs. I used the NLB. And then lastly we used our gateway load balancer. It's our gateway, our our load balancer of choice for security, and our database VPC as well as our front door are both protected by this device. Again, we don't need to rebuy those firewalls over and over and over again. We can leverage them in the same way, and we can kind of ring fence the VPC that those firewalls live in so that has no other connectivity other than the gateway load balancer endpoints that are coming in. So now, as promised, I'm gonna hand it over to Melyn, so he's gonna tell you about all the cool things that are coming for ALB and NLB. Thank you, Jamie. So before I start, I just wanna do a quick checkpoint. So we started with Matt talking about the cool technology that goes underneath in building all the systems, the underlying hyperplane architecture, the nitro systems, uh, all of the cool parts. Then Jamie talked about how is the bigger architecture, where do these pieces fit? How are the load balancers that we offer actually serve the needs that you have. So now, having looked at the bigger picture, now I'm gonna talk about specific things on ALB and NLB. So we have launched a lot of capabilities in the last few weeks. I'm going to talk about two key capabilities we launched on the network load balancer or NLB. And then after that, I'll also talk about two key features that we launched on the application load balancer. So let's get started. On NLB we launched something called as a quick pass-through support. That's the feature we recently launched, but before we start there, let's understand, uh, the basics first. So the basics is I, some of you raised your hands when, uh, Jamie asked the question, how many of you use ELBs? Um, NLB uses an algorithm called 5-tuple has to select a target. But what is a hash? Hash is essentially a mathematical function that takes a number of inputs and produces a single output. For example, this is a mathematical function and you have 5 different feeds and incoming packet, incoming request, for example. The protocol is UDP. That's the first of all. The second is source IP. There is a source IP address there. There's a source port. Destination IP is that of the NLB. And lastly, the destination port. So these are the 5 couples or 5 entities. So you feed these 5 entities to this mathematical function, and then you get one single output. And let's say the output is 2. So it says NLB says, I'm gonna pick up target number 2. So whatever incoming traffic comes in for this combination of 5 to, I'm going to send it to target number 2. So that's how NLB routes traffic today. So this is one key construct that, uh, that we'll use later on. The key points here is the number of targets as they change, the answer might change because as the number of targets are different, the output might come up differently. And then NLB also caches this entry for 120 seconds for UDP, uh, and for 350 seconds for TCP, OK, but that's not variable. So now the second construct or 2nd concept we want to learn about is I said we launched a function or a feature called Quick pass through, but what is quick? How many of you heard of Quick? OK, that's good. Um, so we'll, we'll do a little bit of overview of Quick. Quick is a newer protocol. It essentially, it's a UDP-based protocol RFC 9000. Um, it is written fundamentally for mobile nodes. All the protocol that we have known so far for TCP UDP stack has been written for static node, but now, as we all know, with the world has changed. So the benefits of a quick protocol is reduces the connection latency, and I'll show you how, as we talk about. It has a built-in security. It has built-in TLS 1.3 encryption and as well as it has support for migration and multiplexing. What that means is if the mobile load moves, uh, and we use our, our cell phones everywhere, everywhere we go. So if we're moving from Wi Fi to cellular or vice versa, our IP address underneath is gonna change, so the connections are still going to be persistent. Your connection will not drop. I mentioned then Quick provide some benefits so let's look at the benefits. So there is a TCP stack. Let's understand what a TCP stack looks like that we all know on the network layer we have an Internet protocol and then on top of that we usually either on a TCP or UDP, but on a TCP stack, let's say the TCP stack provides us with data reliability. As well as congestion control elements. On top of that, then we build, uh, security with using TLS 1.2 or 1.3. And then on top of that, uh, actual application messages go on using HTTP 12. Now, compare that with Qu, the underlying, uh, network protocol is still internet IP. On top of that, it runs UDP and UDPort 443 is quick. On top of UDP then you have a quick plus TLS 1.3 combination and it's an interesting protocol in the sense that even though it runs on UDP, it has the elements of guaranteed connection, uh, condition control, and encrypted payload. So it has, it is a hybrid and best of both and on top of that it has, uh, HTTP 1.3, uh, for, for the application workloads. Now let's compare the handshakes that happen in the current TCP with TLS. So when in a current TCP environment there's a three-way handshake, the TCP 3-way handshake that goes on Cincinac AC, that's a 3-way handshake. And then on top of that we will do, uh, clients typically will do a TLS handshake, and after that there will be actual data transfer. So there are a number of handshake messages that are involved and after that the data connection actually happens. Compare that with quick, it's just one round trip, and that's it, and data starts flowing. So that is the advantage because this this protocol has been optimized and we have lessons learned from decades of use of TCP with TLS um quick has been optimized and you can see why the connection establishment is so quick and and why the data uh can move with. Uh, low latency. Now I, I said we launch a feature called NLB quick pass through, and I highlighted the word pass through. What is it? It is essentially based on an IETF draft, uh, that's mentioned here, and pass through means NLB does not terminate, terminate the quick session. It will pass through the NL the traffic that's coming from client straight to your targets, and I will walk through the signal so you'll know exactly what is happening. And what are the benefits of NLB with quick, uh, pass-through feature? There are 4 benefits. Number 1, it reduces the connection latency. Number 2, and we looked at why that it reduces connection latency. Number 2, it maintains target stickiness even when the IP address of the client changes or the IP address or port changes. Number 3, it ensures backward compatibility. We have added a feature called quick and a TCP listener. So by default you can start a quick connection, but if your targets are not capable of supporting quick, it'll fall back to TCP. So you have a fallback option. And last but not the least, it gives a complete control to the application developers all the way from client to the end host or end targets that are running in your containers so you can manage your own, uh, certificates, you can manage your own encryption, you can upgrade your application versions and your infrastructure doesn't have to change. So now having learned all the basics, let's now dive deep. So, Quick has two types of packet formats. There is something called a long header, and a long header is typically used when the connection is established. In the connection establishment, you will say, uh, there are a bunch of fields there, uh, and we'll highlight some of those later but we don't have to remember all of those. And once the connection established. Client and the and the server or client and the target, they will switch to a short header after the connection has been established, but the commonality between the two is what is called as the destination connection ID or DCID. It's a field that's 160 bytes, which is, uh, bits, which is 20 bytes. And why is this field important? This field is important because it maintains session stickiness. It is outside the encrypted payload of the quick packet, so you, the load balance load balances can see it, um, then it maintains consistent, essentially it remains constant even when the source IP and destination IP or source, source IP des uh, so port changes. It remains consistent. And then it's this field is used and for load are used by load balancers to maintain a connection stickiness. So that's the use that was the intent of writing this protocol. So now having learned why this connection is important, now let's look at actually how the connection establishment works in Quick. So on here you will see 3 different signals on the left hand side you have a client. And on the right hand side you have a target and in between sits a load balancer or network load balancer. The client will typically initiate a connection and it sends a quick initial packet and it sends out a random DC ID, the field that we've highlighted before. It's, it sends a random destination connection ID and then we calculates a 5 hash and we looked at what is the hash value that gets calculated in one of our earlier slides and it selects a target and sends the packet to the target. Now, the target responds with a handshake packet. And responds to that with an SC ID or a source connection ID. It is a 17 byte connection ID and this might be too much into it, but this is a 300 level presentation, so I know some of you may appreciate the details. So there's a 17 byte source connection ID. Within those 17 bytes, you also have something called as a server ID that is 8 byte. So the server essentially tells the client that my ID is this 8 byte and. Use this going forward. So even though when the connection changes, the client will keep using the same server ID. That means the connection will always go back to the same server. That's the, that's the point of that, uh, connection ID. Then NLB receives the packet. It simply forwards that back to the client. Then client looks at that source connection ID and says, Ah, I understand now the server actually gave me this ID, and I'm gonna use that from that point on. So from that point onwards, the client will use the same destination connection ID as 17 bytes going forward regardless of the IP address changes. Now, the client, in that case, will respond with a quick handshake packet. Of its own, uh, and then NLB will now create a connection ID to a server ID mapping. And I'll send the packet to that target. The same now because of the source ID lookup, the source connection ID lookup and the server ID lookup, the packet now gets back, sent back to the same server. This at this point, the target handshake is complete and then the target responds back to the client andLB simply looks at the packet and sends it back to the client and at this point the client handshake is complete. This is what is happening in the meantime and at that point, the client will start sending the data and this is where the data transfer continues at this point onwards. Now this is where the connection establishment and data transfer progresses, but now we said this protocol is fundamentally written for connection migration or connection or a client that's roaming. So how does this actually work? So client essentially roams, it changes it's either IP address or I or port or both, but keeps the same 17 byte ID. NLB will then look up the connection ID. It knows then based on the lookup, which server ID to go to and simply forward the packet to the target. Target receives it, then the connection is stickiness is still maintained with the same target. And then the packet goes back to the client. So this is how the connection is maintained even when the client has roamed to a new IP address or a new source port. So now having established this, let's understand how to enable this feature. Your question might be, OK, this is great. How do I enable NLB on quick? Fairly simple 3 step process. Step number 1, just create a quick or a TCP quick listener. Jamie showed you on, on the previous, uh, on this section, how do you create. Listeners, uh, on, uh, NLB, so you select, I've highlighted there on the bottom left corner on the red bracket you'll see either you select a quick or a quick TCP listener. We recommend using quick uh TCP TCP quick listener so you can start with quick by default and if it doesn't, your client, your, uh, endpoints are not capable, it will fall back automatically to TCP. That's step number 1. Step number 2, you essentially create a target group. That and enable quick on that as well. And step number 3, this is needed only if you're not using. AWS load balancer controller. Jamie showed, the load balancer controller, it's our orchestration that helps orchestrate load balancer. So if you're using load balancer controller, then this tape is not needed. But in this case, this is where you assign server IDs if you're not using a load balancer controller. That that was a simple process of establishing enabling Quick Feature. Now, some of the key considerations, as you, as you enable quick, there are 4 things I would like you to be aware of. We can consider them as sort of things to note. Number one, this implementation is on the IETF draft. The ITF draft has remained stable for a number of years, so we feel confident that it will. Become RFC in its current state but things might change. I mean, life changes things might happen, so just keep an eye in case there's changes. We'll obviously if it changes we'll, we'll update our implementation. Uh, unlikely to, uh, the draft will unlikely to change, but just wanted to give you the caveat. The specification IETF specification also states that there is no UDP fragmentation allowed in quick. Uh, this is by definition of the protocol, so there's no UDP fragments allowed, uh, and because it's not allowed, we'll also drop it when NLB sees it. Number 3, This protocol relies on a server ID that is sent by a server. So a server essentially sends its own ID and says, please use me going forward. So the server ID essentially. I make sure that the connection always goes to the server. But remember, because that, because of that, if the server at, at some point, if the server goes down or something, you have to make sure your software is capable of handling that, that error condition. And last but not, not the least. Quick, allows you to. Uh, uh, take Internet facing traffic and therefore you're to disable, uh, your access control. So if you're enabling access control, you'll have to implement them on somehow on your, um, on your endpoint. That's, that's again the definition of the protocol. I just want you to be aware of it. All of those things are documented in the blog that we wrote a few weeks ago, so feel free to scan the QR code. what you just saw here is is exactly documented here. Now, in the next, so that concludes our quick feature. I'll do a quick overview of other three features. So the second feature we launched on weighted target group. This feature is now available on NLB. Uh, Jamie did a great job on, on, you can, you, you saw that on the ALB. NLB also supports, uh, weighted target group now which allows you to distribute traffic based on configuration of weights on different targets. What are the use cases? Where do you, where would you use this? is blue-green canary type deployments when you want to download, uh, minimize the downtime during updates and patching. You want to do AB testing for different versions of software, for example, or you want to try different, uh, user experiences. And third is migrate applications when you're migrating from one version of application to another version of application. That's when you use weighted target group again. Similar works in a very similar manner as that you're familiar with ALB. Imagine this is a network load balancer that has two target groups. There's a blue target group and a green target group. You can assign weights and based on the definition assigned weights, the traffic will go to those target groups. Again, this, how to use this feature, the details of this feature are documented in on this launch block. Feel free to scan it. We looked at two features that we launched, key features on, uh, that we launched on NLB. Now let's switch the gear a little bit and talk about the two new features we have launched on ALB in the recent weeks. Number one is an interesting and a very exciting feature called target optimizer. Uh, target optimizer and I'll talk about, I'll talk about what it is first and what benefits the feature provides, and I'll talk about how it works. So essentially it allows you to enforce maximum number of connections. Uh, you can, you can specify, say I want to have only one connection going to one target and that improves your. Success rate as well as uh it gives you low concurrency. Why is this useful? It also increases target efficiency. This is meant for your AI workload, maybe training, maybe inference workloads where your target wants to dedicate it all the time only for one single task. For example, with today's ALB algorithm or load uh algorithms are on, uh, ALB, let's imagine this is a round robin algorithm you choose, or at least connections you choose. ALB will, if it's a round robin algorithm, the incoming traffic will still go to a, a target that may still be busy and it may be doing multiple tasks from a previous request. So in that case, your concurrency remains high and your error rates may go up in this case. But with target optimizer what you can say is you can say I want only one concurrent task going on on these targets and that will ALB will ensure that it'll send the traffic to the target that is available to access only one task. And how do we achieve that? There is an agent that runs on each target now, so the agent is actually monitoring how many tasks are running. On the target and it'll communicate that back to the ALB and based on that ALB will send so what does it do? It gives you higher success rate it improves your. Uh, throughput of your target themselves and it improves efficiency of your target. So excellent for your AI workloads. Um, you can use in one of the architectures that Jamie was mentioning again, uh, very exciting feature if you want more details they are in this blog again, whatever we do anytime we launch a major feature like this, we'll always write a very detailed, uh, blogs with it with configuration examples, topology, etc. so feel free to take a picture. And the last feature we're going to talk about. Now is the rewrite a URL and host header rewrite. So application load load balancer now has exciting capability. You can not only rewrite a host header, but you can also rewrite, uh, part of URL and the technology that's used essentially is reg reg regex or regular expressions. Imagine there's an original request that's, that's a get request that's coming in. It's user slash hello.htm and then there's a host header that comes with it, example, uh, ALB.com. You can specify a rule that says match the path and, and you're specifying it and I wish I could do annotations here is a look for strings called user and hello. HTM. And on that transform that with by adding dollar 1 essentially refers to the users and dollar 2 refers to the the purple part, the hello. HTM, but in between insert EN. So as you can see when ALB will transform the request and it'll insert that EN again that you can configure this and it'll send to the target. So that allows the targets to do, um, interesting things. You can, you can manage your fleets appropriately. The original request doesn't even have to know the client doesn't even have to change. So this is how it works on uh request level on the URL, but now let's look at how it works on a host header. So on a host header, you can also say there is host header says example ALB.com. You can say in the match host header you can say look for anything that starts with example.star and in there transform that with dollar M or or M with it. So essentially when the request goes in the host header will have an M.LB.com. So this could be your fleet that's that's uh. The servicing only the mobile nodes. So again, this very, very, very powerful capability to change your request and your host status on the fly. Similar to all the features, this also has, uh, a launch block, so feel free to take a picture of this. So that essentially I gave you an overview of the four features that we launched, and I'll invite Jamie to, to close it for us. Interesting. OK. Almost lost my mic here, so I'll hold it on this hand. So, Matt had taken the, the covers off and pulled back the curtain to show us how ALB, NLB, and gateway load balancer are actually created. I walked you through where they apply and some architectures and then Melinde went through and added the new features that we have today. Now I know I've seen a bunch of you using your phones to take pictures of some of the QR codes, so I would like you to pick up your phones as well and type in the routing loop.net. So every Wednesday, Matt, myself, and a couple other hosts do things just like this on Twitch, YouTube, and Twitter. If you go to the routing loop.net, it'll give you all of our previous episodes that cover things such as this, as well as what we're going to be coming out in the future. We'll be covering some of those chalk talks that some of you might have missed. We'll invite some of those folks on stage, so, or on the, on the show, so please tune in for that. And also please don't forget to fill out the session survey. This helps us calibrate to make sure that we're doing the right thing for you and you're learning what you want to learn. So thank you very much and enjoy the rest of your day at Reinvent.