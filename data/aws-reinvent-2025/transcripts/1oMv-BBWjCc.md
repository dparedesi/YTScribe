---
video_id: 1oMv-BBWjCc
video_url: https://www.youtube.com/watch?v=1oMv-BBWjCc
is_generated: False
is_translatable: True
---

What's going on everyone? So, uh, thanks for taking 20 minutes out of a crazy conference schedule to uh talk with me about, uh, AI initiatives and building solid data foundations on open lakehouse technologies. Uh, so we'll introduce you to Click if you haven't heard us. We'll spend the bulk of the time today talking about kind of where the current AI landscape is and most specifically how to build scalable but also cost-effective data platforms to drive really any data initiative that you might have, but specifically those focused around AI. Uh, Click as a company, we're a global software company, offices around the world, uh, many of you likely use us even if you don't know that you use us. Um, fundamentally, Click has a product portfolio that provides customers with an end to end data platform. Focused on both data integration and transformation as well as data analytics. So wherever you are in your data journey, we can likely help get data into digestible and consumable formats, or if your data formats are in a good state as we speak, we can help you analyze and provide analytics on top of your data. Uh, we are very proud of our AWS partnership. It's, it's, uh, it's, it's been long lasting and we hope it will continue to be, uh, into the future. Uh, we've been strategic partners with AWS for, uh, many, many years, and we rely on AWS to power much of the capabilities that we're going to talk through today. So we're both a partner of AWS. We're also a consumer of AWS services, and one of our primary. Objectives in partnering with AWS is to help customers deliver successful data projects on on AWS as fast as possible, right? We want to get you from data to outcome very quickly, very easily, and um really enable all of your, your data professionals to be successful, uh, driving data projects on AWS. So AI's happened, right? It's, it's, it's, I don't think I'm uh out of bounds by saying that. If you look at the maturity curve, we're kind of inside of it, right? Uh, most organizations are, are starting, continuing, starting to deliver AI projects within their organizations. So it's really up to us as data professionals and as data teams to figure out how we're gonna build strong data platforms and strong data foundations to deliver successful AI uh initiatives. And many organizations are starting to make progress, right? I think it's, um, it's good to see that almost 70% of organizations have started to build a formalized AI strategy, right? Uh, pretty rare these days to approach an organization and they have no strategy or no developed plans for accommodating uh uh AI. What has become really challenging though, and one of the factors that I think is starting to hold it back is that ROI can be really hard to predict and quantify on AI projects. And many organizations are struggling to kind of dip their toe all the way into the water because they're not sure whether they're going to get the perceived benefit out of those projects. So that has started to slow down the rollout across the across the industry though, you're seeing lots of successful AI initiatives, a lot of them around kind of chatbot type engagements, code writing and development, and we're starting to see more AI initiatives fall into the analytics perspective and realm. If you talk to organizations about what's holding them back though, we talked about kind of the difficulty in predicting ROI, uh, but a lot of the barriers are around that data foundation, right, you can't really do anything with your data if you can't trust your data. Right, so how do you build that strong data foundation and that strong data platform that you can then deliver successful AI projects on top of, and that is something that we're talking to a lot of customers about, and they're telling us that it's holding them back on achieving a lot of their vision around some of these projects that they want to implement. So how do we start to make progress, right? How can we start to build on these data foundations and these data platforms to deliver on our successful projects? We at Click believe that it all starts with an open data architecture, right, and we'll talk about what that means and how we can deliver on that through the remainder of today's presentation. So what does an architecture look, look like, uh, in terms of these open foundations. So if you look across the left of the screen, that's your data, uh, those are your data sources, right? This is where the data is gonna come from that you wanna build your projects on top of. We believe an open architecture should support as many sources as you might have, from the simple, maybe API endpoints or SAS type applications, to the more complicated, maybe database transactional systems that you may wanna bring into your data architectures, the, Oracles of the world, SQL Servers of the world, Postress SQLs, all the way down to the much more complicated streaming real-time data sources, right, those sources that deliver semi-structured data, those sources that deliver data in real time, those sources whose schema evolves over time, all of these sources need to be brought in to a particular data architecture. Now that then data architecture then needs to provide capabilities to help you move and transform that data into high into that that is of high quality. Right, so a data platform that is open needs to handle the data movement and that also needs to be able to analyze and act on that data in the most efficient way possible. Right, and we believe that an open data lake architecture is the, the best place to store and manage your data so that it is available to as many different consumers as might want to, uh, might want to consume it. These could be AI, uh, AI agents, these could be traditional data consumers like analytical and, and query engines, right? So this is what a proposed architecture looks like. And we think that Iceberg, an iceberg formatted Lake House is the best approach for storing your data, right? And there's two primary reasons for that. If you talk to customers about why they're considering adopting Iceberg or Lake House solutions in general, most of those conversations drive towards one or two particular drivers. The first is cost, right, when building out these architectures that have to accommodate. Data source that we might have right from the small scale to the large scale, from the batch oriented to the real-time cost is going to be a factor in bringing that data in, and Lake House Technologies, specifically those based on Iceberg, have proven to deliver the most flexible data storage at the lowest possible cost. The second value proposition is interoperability, right. We are trying to help organizations shift from what I would call platform oriented architectures, where all data sits inside of the platform consuming that data, to a more open architecture where the data can sit inside of an open architecture and then as many platforms as you want can connect to and consume that data, right, and Iceberg serves as a as a very powerful framework for that. As proven by what you see over on the right, right across the industry, most vendors, AWS, of course, leading the charge, have chosen Iceberg as a lake house format that they are putting a lot of eggs into that basket. AWS has services like S3 Tables, a variety of glue services, Sagemaker, EMR, Glue, Athena, all of them very well support Iceberg. Within the AWS ecosystem and across the industry, whether the data warehouse providers, other hyperscalers, variety of consumption engines, they all have chosen Iceberg as a format and a technology that they will support, right? So this interoperability across the data ecosystem is a huge value add of any lake house technology, specifically those based on Iceberg. And Click, of course, is now in that world, right? We released uh earlier this year, the Click Open Lake House that has introduced a managed iceberg offering within the Click product portfolio that we can help customers ingest data into, transform data within, and then present out to their consumption engines. If I layer this on top of the architecture diagram that you saw. Earlier you can see the click open lake house powered by Apache Iceberg that runs on AWS provides three primary capabilities, right? So first you'll see high throughput ingestion, that's ingestion directly into iceberg tables from all of the various sources that I mentioned, right, from the simple API type endpoints to the more complicated database and CDC type endpoints. All the way through, we're happy to announce at this show new support for streaming data ingestions from streaming providers like Amazon Kinesis, uh, various versions of Kafka, even micro micro batch ingestions for many files that might exist on object stores. All of that data can be very simply and cost effectively ingested into iceberg tables. Now, beyond that iceberg ingestion though, any customer who's tried to use Iceberg at scale will know that Iceberg, if it's not optimized, will not deliver on its core value, especially real-time ingestion. Iceberg tables need to be optimized, you have to compact files for efficient file level access, you have to expire snapshots so your metadata files remain reasonably sized. You have to delete orphan files so that you're not consuming terabytes on disk to process a 100 gigabyte data set. And the click adaptive optimizer will do all of that for you, right, as you ingest data into Iceberg, our optimizer kicks in, and we continuously optimize Iceberg for low cost and uh great performance. And then finally we have warehouse mirroring. And this is an important component of the interoperability function that I mentioned, right? It's not good enough to store your data in iceberg and just query it from a single engine, right? Iceberg tables need to be consumable by as many possible consumption engines as possible in order to deliver on that open uh perspective. So what we offer is we ingest data into Iceberg, we optimize Iceberg, and then we can present that data to a variety of consumption engines from your data warehouses to your query engines, so that they can reach into the click open lake house and run optimized queries against iceberg tables. And all of this runs on AWS, right? Uh, as an example of the cost efficiency that we're helping customers achieve, we actually ran a benchmark, we just published this benchmark a few weeks ago. Uh, we took the exact same data, it was a real-time streaming ingestion into Iceberg, relatively I, I'll call it moderate volume, right? So not low scale, not high scale, kind of right in the middle, probably a volume similar to what many of you are working with today. We ingested that data in real time, right, this is a real-time use case into a click open lake house iceberg table. And then conversely, we ingested the same data directly into a data warehouse, right, to just showcase the cost efficiency that you can see with open lake house ingestion into iceberg. The conclusion of the benchmark was that we were able to deliver data freshness at a magnitude of 5x faster, right, so data latency you can see in the open lake house was in the 1 to 3 minute range, and the data warehouse was in the 5 to 15 minute range. Uh, so again, 5x faster data, and we could do that at a lower cost in the 75 to 90% range. We actually ran the test on a few different warehouse sizes, from the very small to the slightly larger. Um, and we were able to showcase not only fresher data into Iceberg, but at a much lower cost. And then again, you get to take advantage of the interoperability of those iceberg tables that are now consumable across the widest range of query engines. So if you just, if you look online, we published this benchmark a few weeks ago, there's a lot of detail, information on how you can replicate it yourself, but again, delivering on that cost efficiency at scale objective of getting data into Iceberg in an efficient manner. So again, open lake house benefits, uh very fresh, high speed, real-time data ingestion into iceberg at a very low cost, and with the adaptive optimizer layered on top so that you get great query performance out of your iceberg tables, again, while benefiting from the cost savings that we see by ingesting data into an open lake house. Right. So that's what I wanted to talk about in slides. I did prepare a quick demo if you wanna see how this works, but I will say, if you wanna see an even better demo than the one I'm gonna show you, come to the click booth. It is literally as far that way as you can walk in the expo hall. So if you go way that way, you can listen for the cowbells or see people huffing and puffing on bikes, then then you'll know you're in the right. Space. What this demo is, this is a real streaming data, IOT style use case where you can generate your own data, right? So you get on the bikes, you pedal as fast as you want. During your ride, you're going to generate about 300 to 500 m readings that we're pulling off of the bike, real-time sensor data that we then load into Amazon Kinesis as a real-time streaming. Platform, we ingest that data into iceberg tables in real time, we optimize those tables so that we get great query performance, and then we're providing analytics capabilities on top of the data that you've generated while riding the bike. Right, so it's both kind of a fun user experience, but also a real world example of real-time streaming data ingestion into Iceberg, right? And if you wanna see how this works, I'm going to keep my fingers crossed that I can switch to a demo. Perfect. Now, those of you who know the Wi Fi in here is not great, so I am gonna take advantage of a recording. I normally don't do this, I like to do demos live, but so that I'm not uh suffering from Wi Fi issues, I'm just gonna run the recording and I'll kinda talk you through what we're seeing here, right? So this is a, uh, this is our Clicktown cloud platform, specifically our Open Lake House uh solution. And we're gonna build a streaming ingestion project, right, we'll give our product a name, and we're gonna define the iceberg details of how we want to ingest this data. So the catalog we're gonna use, AWS Glue, the storage engine, A AWS S 3, and then a compute cluster that runs in AWS to process that data into Iceberg and optimize that data continuously as it's being ingested. We step through a data onboarding wizard. This is where we select our Amazon Kinesis connection. This is where those real-time events are going to get loaded to. And we have two Kinesa streams in this account. This is like an e-commerce type data set, and we're gonna pull those two different Kinesis streams into Open Lake House. Our data is in JSON format. We do support a variety of formats, so any type of data, whether it's highly structured or less structured, we can pull in and ingest as a part of this process. And we have options to determine how far back do we want to read, how do we want to handle nested data, if you're using like JSON and you've got structs or arrays. We can either keep it nested or flatten it out, we can append or merge, and then we can also define the partitioning of the table that we're ingesting into, right? So we build out the project, here's a picture of an end to end pipeline from kinesis into landing for raw data and then processed into iceberg. And from here now we've got a running pipeline that in real time is taking data from source event in kinesis into a queriable iceberg table at very low latency, right? Over the course of the event, we've done almost 2 million bike sensor readings, so and growing, so we'll probably be uh much higher than that after the event finishes. But this is just an example of a running pipeline processing data into Iceberg, and here's an example of AWS Athena then querying that iceberg table with near real-time freshness. Right, so whether you're using Athena or any other query engine that you might have access to, you can uh run those queries against the iceberg tables. Lastly, how do you trust that data, right? The Clicktown cloud platform has this concept of data quality. Uh, every data set that we ingest data into is assigned a trust score that's based on accuracy, validity, uh, semantic quality of that data, completeness, so that you can also know not only are we ingesting this data into iceberg, but we're delivering data that you can trust. It's of the highest quality, uh, and you can measure that. Monitor that over time, right? So that's the ClickT Cloud platform, and that's the example of a real-time streaming ingestion into Iceberg, but very simple, right? You set it up once, you set it and forget it, everything runs continuously, everything is optimized for you, cost is managed and kept low, but that data is accessible to the widest possible variety of query engines, right? So again, if you'd like to try out this use case for yourself, definitely come by the booth. You can either just ride the bikes and work up a little sweat or it's only about a 32nd ride, you probably won't work up too much of a sweat, uh, but you can kind of try out the bikes for yourself, generate some data, and then we can walk you through the end to end data pipeline that you yourself have participated in and kind of see how that data's ingested, processed, uh, and then finally stored in iceberg and served up to a variety, uh, of analytics use cases, right? So I do want to thank everyone for your time today. I'll peel off over there if anyone has any questions that you wanna ask uh before the next session starts. Again, if you wanna learn more about Click, please come find us in the booth, walk that way as far as you can. Listen for the cowboys, look for the cowboys cowbells, uh, listen for the er or watch for the. Spinning click logo, uh, come over, ride the bike, see a demo, uh, we'd love to take talk further. Thank you all for your time today. I know it's, uh, uh, it's, it's a, it's a jam-packed agenda and I'm very, uh, thankful that you were able to spend 20 minutes with me here today. You can come find me right over there or in the click booth over the rest of the conference. So thanks everyone.