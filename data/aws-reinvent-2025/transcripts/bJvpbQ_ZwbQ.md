---
video_id: bJvpbQ_ZwbQ
video_url: https://www.youtube.com/watch?v=bJvpbQ_ZwbQ
is_generated: False
is_translatable: True
summary: "This session explores Alight’s strategic migration to the Cloudera Data Platform (CDP) on AWS, detailing how the leading employee benefits provider modernized its massive data estate to become AI-ready. Vishnu Papitti, Senior Director of Data at Alight, shares the journey of migrating over 8,000 workloads and 124 applications within a tight six-month window. Facing challenges like scalability bottlenecks, missed SLAs, and significant technical debt, Alight leveraged Cloudera’s hybrid data platform to achieve a 100% user adoption rate, meet all SLAs, and reduce cloud object counts by 40-70% through the use of Apache Iceberg and the Lakehouse Optimizer.

A subsequent panel discussion featuring Kartik Krishnamurthy, SVP of Product at Cloudera, delves into the broader landscape of AI in the enterprise. The conversation highlights the critical shift from experimental AI to scalable, secure execution, emphasizing that 95% of AI projects fail due to poor data foundations. Key topics include the rise of **Agentic AI** to automate complex decision-making workflows, the necessity of **Lakehouse architectures** for interoperability, and the paramount importance of data security and governance in preventing model leakage. The session concludes with Alight’s future roadmap, which includes deploying AI agents for benefits enrollment and claims adjudication to enhance the experience for its 35 million users."
keywords:
  - Cloudera on AWS
  - Data Lakehouse
  - Apache Iceberg
  - Alight
  - Migration Strategy
  - Enterprise AI
  - Agentic Workflows
  - Data Governance
  - Legacy Modernization
  - Lakehouse Optimizer
---

Hi everyone, and welcome. It's so, it's so great to have you here for this session. Over the past year, AI has moved from experimentation to execution, where customers used to come to us and ask, how do we use AI. Now they're asking us how do we scale it, how do we govern it, and more importantly, how do we extract the most value from it. At its core though, AI success really depends on data. Trusted, secure, and accessible data, and that's where really Cloudera comes into play. Cloudera brings forward a hybrid unified data platform that allows you to manage, analyze, and serve AI ready data regardless of where you are. You could be on hybrid cloud, you could be multi-cloud, or more importantly, you could be native on AWS. We bring the AI to your data to help your AI model scale in production, to help you run AI privately, and for you to have the best success with your AI projects. But today, You don't have to take my word for it. We'll start off the session with Vishnu Papitti, who is the senior director of data at Elite, which is a leading employment benefits management. Vishnu will talk about how he and his team successfully migrated to Cloudera Data Lake House, completely modernizing their data estate and migrating it to Cloudera on AWS, the challenges that they encountered and the success stories that they have gone through along the way. Following that, we'll have an industry. Leadership panel discussion with Vishnu Papiethi. We'll also be joined by Kartik Krishnamurthy, who is the SVP of product here at Cloudera. We'll talk about what it takes to make data AI ready. What are some of those considerations and challenges that are still holding the AI back from getting productionized successfully, and of course, how do we differentiate in this world of AI? We have a packed session for you. So without further ado, please join me in welcoming Vishnu Papietti to the stage. Thank you, Anu for the amazing introduction. I'm Vishnu Papasetti. I lead uh data platforms and shared services at Eli. I'm going to walk you all through Alite's journey to the cloud and how we are leveraging cloudera as well as AWS services for our AI journey. To start with Elite. Elite is a leading human capital, cloud-based, uh, technology-enabled company that Manages thousands of clients and millions of employees' health benefits, wealth benefits, and their leaves and payroll data. So what we, why companies choose Eli is because of their unique advantages with the services that we offer. The first one is on the benefits. So we take care of benefits right from pre-hiring to retiree stage. And then we have processes that are AI driven that enable faster servicing of the employees. So the next advantage is on the optimized cost and the overall employee benefits experience. And to add that with years of experience handling with large clients, so we have 70% of the top S&P 100 companies are allied clients. And 55% of the top S&P 500 companies are allied customers. Now, I'll move on to our CDP migration journey. What the the challenges that we had faced, the approach that we took to mitigate those challenges, and there are success stories after migration to AWS Cloud. Again, these are all the common challenges that we had with on-prem as well as our initial migration. First one is about scalability, right? How do we scale the workloads to kind of being AI ready, right? And then what are the hurdles that we typically have with respect to the infrastructure, with respect to the people supporting the infrastructure, then. And then the financial issues corresponding to that, so we had several SLA delays, SLA missed because of that we had financial implications too. And then we accumulated over the years a huge technical debt. We had 8000 plus services running that are very legacy, and we had to automate all of that. And overall user experience also took a dip because of all of the above. Right, so we had to kind of ensure we had to move to a platform that is stable enough, that is scalable enough, uh, and then we kind of give the service level and SLAs that our users and our clients were expecting. So the approach that we took was to kind of narrow on what is the service, right, and we were previously on CDH and CDP CDH, so it was no brainer for us to kind of look at CDP as our motive destination, but we wanted to evaluate what are the benefits that we have and with our experience evaluating a lot of vendors, we narrowed down on CDP. For many reasons. One with respect to its performance, with respect to the ROI that we get, with the cost that we are spending. And with all of that we did a 4 week POC just to evaluate a workflow just to understand the product capabilities as well as the migration from a migration planning wise what we can learn from that exercise. So 4 week exercise was very successful and we were able to identify all the scenarios and from a planning perspective we were able to identify everything that we could line up for the migration. The next and the most important thing is onboarding of the stakeholders and To kind of notify our clients on possible outages in the future, so we made sure that we plan our exercise in a way that it is managed in phases and it is done in an effective way wherein we communicate all the planned deliverables and the expectation on specific changes and outages up front so that we are all aligned on shared objective. And one of the major things was on the legacy workloads. We had to kind of decide on an approach on what we would and how we would approach handling legacy workloads. So we're basically moving on with respect to the approach. So we kind of took a three phased approach, basically 3 hours either to rehost, retire, or rewrite certain applications. So with that approach, what we were able to identify was to kind of look at But I, I think I went two steps ahead. Anyway, so, um, the approach was to kind of look at um The applications that are not compatible with CDP and how we could migrate it out. And certain things we are able to kind of move it out of CDP and run it via shell scripts and other applications, and certain things, the core applications we made sure those are running in CDP and then for that what we had to do was rewrite the existing scripts to be able to to be able to kind of cope up with the CDP latest. Tool set, right, so for an example, again we had to move away from SQL-based coding to Spark-based coding and we had to write a lot of automated scripts to do the conversion. And um And the next one was again to look at the technical debt, and we wanted to make sure if there are things that we could retire, we are able to retire. So with respect to the migration planning, we kind of, we had a 6 month target to migrate our entire cloud infrastructure over to the cloud, and we split that into 9 waves. Each wave being 3 weeks each and we ensured we have clarity and deliverables for those 3 weeks and and ensured from a from a development wise, from an automation wise, from a user expectation wise we're all aligned and working together with partnership with Cloudera we ensured we had the right right talent with us achieving those goals, so. So the next one is with respect to automation. So in collaboration with again with Cloudera, we ensured we have the scripts that needed to be automated, things that we received from Cloudera, as well as we build certain things from in-house, and we were able to automate things that we had to automate because 6 months is a very short window and without automation there is no way it is possible. And overall with all of those steps and meticulous planning and execution with us and with partners and and collaborating with AWS with respect to the infrastructure and everything, we're able to hit the 6 month migration window successfully and we're able to also. Have a 100% user adoption as well with all of our reporting connected to Cloudera, we made sure we were successfully transitioned our reporting as well. And the most important two things are with respect to the challenges that we had in terms of missing SLAs and in terms of our scalability during enrollment period. So from a stability point of view, we, we ensured we hit all of our SLAs and the processes that we had defined and the streamlined way that we had our batch processes running. We're able to hit all of our SLAs. And from a scalability point of view, from infrastructure wise, Cloudera enabled us to kind of have auto scaling enabled where we needed whenever we needed infrastructure, we scale up. So at a peak time we scaled up to at least 80 to 90 clusters and we were able to scale down when the infrastructure was not really required and we were able to optimize. Our workloads with respect to when the user activity was high we were able to kind of separate out the performance and the and the pipelines that for batch execution we were able to separate it out and for user using the same list of clusters so for user activities we are able to kind of execute in a parallel. Clusters so that the execution for the reporting is also clean and successful. Just to highlight some of our key metrics and accomplishments, overall, as I said, it was a 6 month migration window, had a massive 8,000+ workloads and a team size of 150+, and the overall 124 applications that we had migrated. And again I have listed some of our cloud costs and professional services costs overall and and from a revenue wise, in fact this was when when we had successfully delivered we are well below 3 million. Target that we have and I would say this is one of the cleanest and very successful migration with Eli and was a highlight for Elite when we did migrate to CDP as well as to AWS. Moving on again, here are some of the products that we use, Data hub, data lake, primarily um uh on the lake house, and all of our infrastructure right now is on AWS. And what this has enabled us to do, um. is to kind of we will move on to the next steps on how this has facilitated our faster adoption of G AI and AI capabilities. So traditionally we have been using AI and ML models, leveraging Sagemaker and All of that, and we always had a challenge with data and data being available for the users to use, and what this has done was to fast track all of that and then make the data available for the consumers and when they need it at the right time. And the speed at which they would need the data. So moving forward within Cloudera, what are we looking forward to? As you all know, um, Iceberg is the de facto kind of opal table format that um. That that is kind of being very efficient with respect to data reads. So what we typically see with respect to our batch data loads and our user consumption, since all of our data is in AWS under the hood, obviously data resides in S3. And reading data from S3 when we have large tables where we have millions of small batch files, the read time and the execution time of the queries would obviously take some time. But how do we mitigate that? That was a key challenge that we had. And to address that is the Apache iceberg and the OpenTable format. So iceberg table. Basically is helping us to kind of move data consumption and add a layer on top of S3 to have a faster consumption of data. So it's again Open table. Some of you may know, many of you may know that all this iceberg table, it's not a database, it's not a data store. It's a metadata management and efficient management of metadata where we can faster retrieve the data, right? So, so that kind of is enabling us to kind of think forward on how we enable services for our users. So again, um, so we were early adopters of Lake House Optimizer, and I'll go through some of the success stories, what we are seeing with our POCs. Um, again, there are two major things that we see with large files in our data, which is basically on compaction and table cleanup. So, um, and if, uh, If any of you are clouded our customers or you would have seen performing table statistics updates post your batch loads and even before your batch loads, right, so all of that would be taken care of with respect to the lake house A measure. So it does table management efficiently and you'll be, you don't have to worry about performing explicit table management activities. So the early POC results that I'm seeing is with respect to the overall object count, and again this is the object count anything less than I'm taking off anything less than 50, any objects over than 50 that you store within your S3 right under the hood of your tables where your your object count is anything greater than 50, this is, this is the statistics that we have. And what we are seeing is the larger the tables are, the lake house optimizer is performing way better. So as you can see, our object counts overall has significantly reduced. Overall I'm seeing from object. Reduction wise from a 40 to 70% reduction in object count and the same thing once the object count reduces, obviously the size of the data would also reduce. So I'm seeing 48% overall at a maximum reduction in the overall object size as well. And object size, the snapshots, manifest files, again, all of these are related to the data and the metadata corresponding to that data, right? So again, manifest is nothing but where your data. within our spectrum of S3s, right, so it has a log of every single data that it has, and the smaller the files, the lesser the manifest file. So what we are seeing is around 60 to 80% of reduction in the manifest files as well. So what this would do for us again, so this is not in production and this is still in POC. What we are hoping that this would definitely. Improve our table access faster and the performance we are looking at having an improved performance. Obviously there are cost aspects too that will reduce the cost and all of that, but more than anything else we are looking at faster table access and better user experience. With that, I'll conclude and hand it over to. Thank you so much. Great, thank you so much, Vishnu. Don't go too far. Uh, we'll now get into our panel discussion. For that, I'd love to reinvite Vishnu Papietti and uh Karti Krishnamurthy. Please come on stage. I Thanks. Vishnu, phenomenal presentation. Thank you. Thank you so much. Especially I'm so excited about the numbers that you shared about the lake house and the optimization on the lake house that Claudera is doing. But before we get into that, I really wanted to ask you about some of the driving forces that made you think about Lake House as the decision to modernize towards. There are so many paradigms out there. How did you guys align on the lake house? Sure, first thing was, I would say 3 things. First one, is with respect to our batch loads and our SLAs, right, right, so when we are leading, when we are reading data from large files and processing the data, our batch execution. We were hitting some SLA issues and then and then we had to implement certain things where we retry those processes to make sure it executes within that SLA window. So there was a manual intervention and the next thing is on the query performance, right? So when we look at the G AI evolution and all of that data is required for everyone within within our organization. So how faster can we provide the data, right, right? So when we have millions of objects within S3. Our response rate to those data requests were slow, so we wanted to have some kind of an ability to kind of provide them faster data, and it is seamless for the users and with the unified metadata, the way we don't have to replicate the data multiple times, right? Exactly. ETL pipelining across proprietary formats is one of the banes of. The data. The third one is the obvious one is the cost, of course, obviously the reduced object count would definitely bring the cost down. Perfect. Yes, absolutely. Standardizing on an open table format with different engines cooperating is a fantastic way to go about making that data AI ready. Kartik. We are investing so heavily in the data lake house at Cloudera. Uh, so can you tell us a little bit about what are some differentiations that you would like to highlight for the audience here? Absolutely. So Claudera's journey with Lake House started 4 years ago. Our first, um, time we released a product with Lake House was in 2021, and since then till now, um, It has progressed rapidly, right, both in adoption, customer interest, number of workloads, product has also evolved. Iceberg as a table format itself has evolved as well, and it's become more and more standardized across the industry as well, right? So. With that, what we did is we continue to invest in the adoption evolution of Iceberg both from an engineering perspective as well as supporting it with a bunch of product capabilities. We were the first to support replication with Iceberg. Why is that important? Enterprises like Allied and others who get onto this data journey and serve mission critical data. They want that to be highly available right in the event my system goes down. I want to be able to have a secondary system to continue to serve my data, right? And beyond that, it's also in in most cases a compliance requirement and audit requirement, right? Second, um, so replication was one. Second is uh capabilities around it. We centralize our our whole AI strategy around Lake House. Right. And, and I believe Allied is also using Lacos as the, as the base for their AI data. Why? um. As the AI paradigm is. Like exploded in the last two years. The data associated with that has also become important, right? The trusted data, which is what Clauda's mantra has been, is to provide, ensure that your data is trusted because trusted data drives better AI and trusted AI. So building that ecosystem around lake cows and supporting that with AI. Uh, Capabilities, right? Third is features like Lake House optimizer. Why is it important? Because inherently Lake House comes with a lot of requirements for enterprises that require data to be compacted. We see, we saw Vishnu talk about compaction, why it was essential for availability of data, right? So we want to make all of that automated. Want to remove the manual process associated with that, right? Standardize the automation so you can set policies on it and you can set how long you want your snapshots to be and all these different features and capabilities that that we won't automate it, right? And last is rest catalog that is an important part of Lake House because why iceberg is becoming the open table standard for the industry. Um, and loudera is an open ecosystem. We believe in an open paradigm and the ability to, uh, have open exchange of data. So what the rest catalog does is allows the data in the clouded ecosystem to also be sort of queried by other engines. Why is this important? Because our customers tell us they want to use the best tool for the best service, right? And. We have 6 different engines over the time. We also have 6 different engines that support Iceberg and Lake House, but customers may have made other choices, and they have ecosystems, right? Other compute infrastructure, so to be able to do query from other engines so that you can have a single table format and a common access to data. So all of these capabilities we continue to. To add into the product and last but not the least, we made it available everywhere, right? So our bread and butter, right, our hybrid cloud story and the total cost of ownership story, especially with things like Lake House Optimizer with the compaction with us trying to give you the best PCO, if you will, right, in terms of operations. I think it's an important conversation because We've also heard a lot of the budget now is moving towards more AI-driven initiatives. Now at the core of it, we do know that AI needs to be rooted in data. However, with the budget shifts, it's even more important that your data platform gives you far more for less, right? And that's always been our mantra from the beginning. So shifting gears a little bit to AI. Vishnu, uh, I wanna start with, um, really, you know, where do you see a light going, right? Uh, what's the next frontier for you guys in terms of, uh, AI? What are some things that you're already in discussions about, uh, for next year? Sure, sure, yeah, so, um, here at Light, um. We had traditional AIML models for a very long time, so we, we had, we always had that, um, with the with the exponential growth in G AI and AI features, um, one of our internal statistics showed that, uh, 75% of the uh um HR leaders are expecting Gen AI results from, from companies, right? So that being said. We had to be ready, and we are ready in that space. So that being said, so with our traditional AIM malpractices, we also added G AI capabilities, partnering with obviously from data with Cloudera and with services from AWS leveraging Bedrock and Sage Maker AI. We're able to bring in new cases to. Users and again it's not something that for us for adoption's sake we are doing it but we touch 35 million employees and we want to make that experience a lot better with the technology that is available and to start the journey we started off looking at Enhancing our search-based processes, so we use G AI to kind of and make our search engines very robust, and we embarked on a reporting technology consolidation which is more advanced and with GII features enabled with authoring capabilities. So again, all of this where possible this year because of all the advancements. That we had done years before with cloud data and data being available and data being structured in a way we could use that in Gen AI and looking forward in 2025 and again for during this year's enrollment people again employees who are using Ali you would have seen a better experience with respect to AI assistant during enrollment. So basically you could just Put in a question to the AI engine to auto enroll. It does everything. It will compare plans this year to last year and then give an optimized result. All of that we were able to enable with all the back end data, with all the the setup that we had done in the back end. And looking forward to, we have a lot of opportunities, as I said, since we are touching 35 million employees plus we have opportunities to kind of we have use cases planned for benefit stream, wealth stream, claims adjudication, contact center efficiencies. There are a lot of use cases. 2626 is going to be an amazing, exciting year for. For data people and data teams, that's great. Sounds really exciting. You said you touched on something very important there, right? You basically said, look, we knew that we had to respond from a technology perspective to deliver G AI-driven capabilities, but you still grounded it in terms of starting with the business cases, starting with the use cases, and figuring out what is that outcome you want to. Kartik, we have seen like AI adoption, especially in the top enterprise space that we deal with, has still been a challenge, right? Moving models into production has been a challenge. You know, what are some of the trends that, uh, you know, you're a business leader, you speak to so many executives. Can you summarize some trends that you see in terms of why it is still continuing to be a challenge? It's very um interesting in terms of the studies that's going on, right? So, uh, as Vishnu pointed out, we have also done surveys, um, but there are two papers that came out recently, one by Forrester, I believe, and one by MIT. The MIT papers said. 95% of AI projects fail. The Forrester paper was more the Forbes paper, I think, um, was more positive, saying 7, 74% of the project's success, uh, succeed, or I have made progress in the AI paradigm. Our surveys point out that 46% of customers, our executives are worried about security. 50% of them have seen data leakages as their training models, as they're doing other things, right? So that's a large number of percentage in terms of afraid of data leakage. So security is the number one paradigm that what constantly we hear our customers are worried about security, right? And then the life cycle of data, right? One of the things Clouda does is it helps with the life cycle of data all the way from ingestion to processing to operationalizing it and finally getting it ready for AI, so. Integration of that data, right, secure securing that data, supporting that ecosystem fully end to end, these are all paradigms and processes, technologies, products that we have invested in over time and this is what. Today's AI leaders in terms of business leaders tell us that. Strategic decisions around AI, right? Outcomes of AI is also a challenge, right? What outcomes do they want? Because when the explosion started, uh, with chat GPT releasing 3.5 GPT in 2023. The push from enterprises pivoted towards junior high, but AI has several other pieces that is actual. Traditional AI that is used in uh business logic and in compliance and a lot of other use cases risk and fraud detection and etc. but Gen AI has changed usability of that platform, right? So which means that it has opened it up to more and more business users than before, right? So it's made AI accessible to. Beyond your data scientists, your everyday analysts, right now, the challenge is OK now that I've opened up, I have, I have a chat GPD. I type chatbot available, but how do I take advantage of this? How do I measure KPIs out of it? How do I make it productive? So that is something that still many business leaders are, are, are struggling with. And on the other hand, AI is just. Marching forward on right, we saw an AI. It is now it's agentic AI. You see, I mean, the last 3 days, how many times have we heard agent everywhere, right? So. Agent code, agent policies, so it's become a world that is now fascinated with what AI can bring us, and it is changing how we live our lives and how we work and how we consume from that work and it's become a different life cycle of work, right? Our hiring, I mean, human capital hiring, I think you may have noticed this, right? How you're hiring, your hiring processes have now changed where they. Yeah, so I mean the whole sort of information exchange is changing. So that is what business leaders are struggling with is how do I optimize this? How do I grow? How do I, my employees are worried about losing their job, right? I mean there's one end you're talking about technology evolution of the AI. On the other end, am I hiring the right people? Do I have the right skills? So all of these things are today's challenges. And from a cloud's perspective, we want to simplify this, right? So that is our focus. Yes, absolutely. In fact, one of the, one of my favorite things about this is that we actually provide a plethora, in fact, a spectrum of capabilities. We cater to the builders, the hardcore data scientists with your high code notebook experience. We have apps which are recipe books, if you will, that you. Come and take a quick start to get started on your AI project and then we also have AI studios, including agent studios that help you build simple AI applications in a matter of hours. So we are really trying to, to Kartik's point, really democratize how AI is delivered and consumed across the broader spectrum. So it's actually one of my favorite things in this whole equation. Now, naturally the push for AI is very rapid, right, and one of the things that I've heard certainly technology leaders talk about is That they don't want to commit to a certain model or they don't want to commit to a certain paradigm because the space is evolving so quickly. So talk to us a little bit about the openness and the extensibility of an AI platform and how important is that. Yeah, so from my point of view, um, it goes hand in hand with the experience that we want to provide to the end users, right, and technology being there, how do we enable that and even to kind of Just before, even before the user experience come into the picture, what is more important is how all of our applications are integrated, right, is the data technologies, the governance aspects, like even with AI, the model governance is also equally important, security, right? So we have to kind of first get that. Into a well architecture framework for everything to work right and then from a from an end user wise, right, so again there are multiple ways to look at it. One, again, how do we personalize to every employee right in their their lifetime, right? So how do we customize to their needs right at the right time when they need. Them the most, right, and these tools enable us to do that, right, and we could, we could think in the future now we could roll it out in that point of view and then how do we guide them to use that, right, right? So this is a bright future and from a user. And the consumption wise, there's a lot of work from companies like us, but at the end of the day it is to make employees' experience better, right? That makes perfect sense. Now let me, I want to ask, since we have two industry leaders here, both of whom are talking to You know, analysts and customer executives, what do you think the next frontier is going to be? What should enterprises really be preparing for from the next frontier in AI and gente innovation? Kartik, we'll start with you. The market, the industry, the consumption is evolving rapidly now. There are 2. Pillars I see one is the evolution of gen AI in the practices of daily application right in enterprises, um, whether it is uh CRM, uh, call customer call handling, call management, etc. right? Uh, remember the days when you had press 1 for this, press 2 for this, and now that's becoming more, um, intelligent, right? We see a number of our customers use the initial first step was LLMs. The second and the 3rd step, we're seeing vision models and voice models like whisper and everything get integrated into our ecosystem. What does that mean is like now I can analyze call recordings. Now I can make, I can feed them back into the response system, right? Um, vision models have become a huge, uh. Part of how businesses now incorporate like compliance. Either forms or compliance photos, all of that get added into the ecosystem, right? Second is agentic workflows. Why are agentic workflows important? Um, there are a number of workflows that used to exist that are that were manual, they were error prone, right? So some of, a lot of these in the, in the past. Several years we've always tried to automate that, right, but beyond automation, it is about making Intelligent decisions based on thoughts and evaluations and etc. so I think that is where it is important for businesses to become efficient, why? Because today's. Economy is all about efficiency. On one hand we want to grow more and more and more. We want to consume more. We want to grow our revenues more. On the other hand, we also want to be efficient about it, right? Remember there were times when you run. We can uh production system upgrades that'll be 48 hour upgrades because we're, we're just there, right, um, say bye to your family on a Friday night and see you on a Monday morning, right, till everything is back up because you don't want business interruption, right? So now I think that is where um agentic solutions are coming into play is to help automate a lot of these manual work that can be done. So I think those are the two things that I see and the investment in AI is continually growing, right? So nowadays we see uh predominantly 60-70% of the investment in AI projects, but here is the important piece AI does not make sense without data, right? So still the challenge of getting the right data to the right AI ecosystem. And just like how we have data silos now we're creating AI silos, right? So we are the AI system don't necessarily, and that is what I think we're trying to bridge we're understanding that. The farther we go into this model like how we've created data silos, if we create AI silos, then it's harder to bring those systems together together, right? So that is the hope in terms of these agentic solutions, etc. is to prevent that divergence from systems, right? Makes sense. No, that's fantastic, and I completely agree. I think having that right data foundation in terms of interoperable, open, and having an ecosys. System or the ability to extend to an ecosystem is so key for us to drive those AI projects. Vishnu, what about you? What do you think? Where are we going next? What is the next frontier? I mean, I totally agree with what Kartik was talking about. And even in our use cases, he was rightly pointing out on customer center, right? So that's a huge area of opportunity where there is human interaction that is happening, right? We could. Make their experience so much better. The audio qualities and with the tools we can enrich that information a lot better and have the information available, a 360 view of the customer that was always there, but their experiences put in together with all of these with GI based solutions integrated into one application, right? So it is very important that we integrate all of these applications to. make the experience really better by taking information from the past and their current experience and their voice call, putting everything together and then giving details for the call center employee who's addressing a specific problem to address that faster, right? So previously the things that used to take minutes to hours, we can solve that fairly quickly, right? And then this is what we really believe, right? It's not only on the The example that Kartik was talking about and customer center, it's across the spectrum of where we have human interaction that happening with the clients. We want to enrich that experience, right, right? Yes, absolutely. No, couldn't agree with you more. Like you said, lots more and lots of excitement coming. We're all, we all feel like we're just around the corner from the next innovation in AI and agentic workload. So yes, let me tell you a story about. A few years ago when um I was speaking to customers and I'm saying the story because we just crossed Thanksgiving right uh and during Thanksgiving a lot of the company's businesses start. Um, sending out coupons and etc. to, to customers. So at that time, about 7-8 years ago, sending coupons out was a process by itself. You start that effort much earlier, months to figure out what, what kind of coupons, how do I harness customer information, what products to send coupons for, what merchants to partner with, and all of these things, right, um, now. With Asiante Solutions and with one of our larger media partners, and I'm sure you are also seeing similar things is I have a business and a business analysis comes in and goes to the agentic system with uh a natural language. Tell me which tables and which data that I need to use to build a campaign around it, right? In a matter of 30 minutes it gives you these are the tables. This is how you join. He is the low code solution here is the high court solution, right. So I mean we've progressed to the point of efficiency where we're making our jobs and our employees and engineers and analysts work a little bit more efficient because at one point this used to be tribal knowledge right you have to go who has worked in the data ecosystem before to say have you done this in the past what. At the tables that you're familiar with, right, or go try to find the data dictionary and search through your lineage and search through your, um, data governance tool. Now this is how it's evolving, so it's making us far more efficient. So, uh, I just wanna in the, in the context of. Just post Thanksgiving, I just remembered, yes, no, that's a great story, and it really, yes, we should, did you have anything to add? Yes, just on the same topic, right, integration is going to be key this is talking about with all the data, it's corresponding metadata, where to retrieve what and how the data is structured for faster retrieval of that information for the agentic workflows to consume the data, right, and Integration of other technologies, governance, all of these things have to be architected in a way that we could efficiently support this, right? So again, we can move faster, but if the underlying engine is not well architected, we will run into issues. But still, again, this will not stop anything. So we will continue to build solutions parallel and The infrastructure and the governance that is required around it. Absolutely, and you know it's not just data governance, it's also model governance. It's AI governance. There's so much ongoing. That's why I love that Cloudera pretty much takes on the entire life cycle of AI right from developing the model to productionizing it to monitoring it, doing model ops like we've got the entire portfolio and suite of products. There's so much more innovation that. Are focused on in delivering over the next few years. So really appreciate everyone's time here. Thank you, Vishnu, for that fantastic presentation. Thank you for all your thought leadership, Kartik, as always, great stories, wonderful anecdotes, and a lot of great insights. Thank you so much. We are available for questions if you have any, please come and meet us. Thank you again for all your time today. Have a great rest of the day. Thank you. Thank you.