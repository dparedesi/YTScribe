---
video_id: OXNTzhPnJU8
video_url: https://www.youtube.com/watch?v=OXNTzhPnJU8
is_generated: False
is_translatable: True
summary: Redis has evolved from a simple in-memory cache into a comprehensive modern data platform addressing critical challenges in real-time application development, including hot key problems, cache invalidation complexity, and the need for sophisticated data structures that support contemporary use cases such as vector search, semantic caching, and real-time feature serving for machine learning applications. The presentation begins by addressing fundamental caching patterns and the inherent limitations of traditional cache-aside approaches, including stale data concerns when time-to-live settings require guesswork, thundering herd problems where expired keys cause all requests to overwhelm the primary database, and cold start scenarios where empty caches provide no protection, all while maintaining that the source database must still handle significant read traffic despite the cache's presence. Redis introduces client-side caching to solve hot key problems for frequently accessed data like influencer profiles, keeping local copies within client libraries and sending invalidation messages when data changes, and the Redis Query Engine addresses secondary indexing challenges by allowing complex queries combining full-text, numeric, geo, and vector fields with MapReduce coordination across cluster nodes, unlocking use cases like querying shopping basket contents across all users or selecting restaurants within specific radii filtered by cuisine tags. The Redis Query Engine powers blazing-fast vector similarity search capabilities that consistently outperform competitors across all benchmarked permutations of datasets, embedding models, vector sizes, and workloads, enabling rack implementations and semantic caching through LangCache, which achieved 70% cache hit rates at Asurion, cutting customer service portal response times in half and quadrupling engagement by caching LLM responses and returning results for semantically similar subsequent queries without expensive model calls. Redis Data Integration solves the cache invalidation problem that Phil Carlton famously identified as one of computer science's two hard problems by implementing refresh-ahead caching patterns using proven technologies including Apache Flink and Debezium to capture database changes via CDC collectors, apply transformation logic for denormalization, and keep Redis continuously synchronized with primary data sources, eliminating the operational nightmare of thousands of cron jobs and complex Kafka architectures while enabling CQRS patterns where writes go to optimized relational models and reads serve from Redis data structures. Access Bank's implementation demonstrates the dramatic performance improvements achievable through this architecture, reducing peak-time response times from 173 milliseconds to 29 milliseconds and scaling from a maximum 500 requests per second on Oracle to tens of thousands on Redis with simple cluster expansion, while Redis Flex addresses total cost of ownership concerns by auto-tiering data across memory and NVMe storage, keeping hot data in RAM while cold keys reside on slower storage, supporting databases up to 50 terabytes with 90% disk allocation and reducing costs by up to 75% compared to pure in-memory deployments, making it particularly suitable for feature stores that maintain complete datasets synchronized via RDI. The native JSON data structure implementation using binary trees enables atomic updates on document sub-paths through JSONPath expressions without retrieve-serialize-modify-serialize-write cycles that risk race conditions, supports partial document retrieval for network efficiency, and achieves up to 92% memory reduction for homogeneous vector arrays through compression techniques that make hierarchical data modeling with vector similarity search practical where it previously consumed seven times more memory than hash-based storage, while Redis 8.4 delivers 40% faster scaling, up to 90% latency improvements for common operations including 55% for hashes and 70% for sets and sorted sets, crosses 1.25 million operations per second on a four-core single-process deployment with mixed workloads, provides 2x TLS performance compared to ElastiCache on equivalent vCPUs, and adds quantization for vector search indexes alongside numerous developer experience improvements that eliminate paper cuts and transaction requirements for common multi-command patterns. iFood's real-world implementation across Latin America's largest food delivery ecosystem processing over 106 million monthly orders demonstrates Redis powering three critical platforms: the GenPlat generative AI platform handling 1 trillion tokens and 400 million requests monthly from 200 services relies on Redis for consistent rate limiting across multiple LLM providers including OpenAI, Gemini, and internal models serving 500 users; the internal RAG solution uses Redis vector store for embedding-based document retrieval that provides context to generative models through configurable ingestion and retrieval pipelines; and the feature store platform delivers over 5 million requests per second with sub-10-millisecond latency by storing nearly 2,000 features in Redis for real-time classical machine learning model inference serving personalized recommendations to millions of users, collectively showcasing how Redis has transformed from a simple acceleration layer into an essential component of modern data architecture supporting generative AI, real-time analytics, and mission-critical application workflows at massive scale while maintaining the simplicity, performance obsession, and developer-friendly experience that characterized its original design philosophy.
keywords: Redis, caching, vector search, semantic caching, data integration
---

Hello. Hi everybody. Welcome, um, to my talk, or to our talk. Uh, we're doing a joint talk together today, um. About powering real-time applications um with a modernized cache. Um, my name is Peter Kylo. I'm with Reddi now for 8 years, and I lead the product management team responsible for the core engine of Reddis. But before we start, I wanted to ask like, who, who knows what Reddis is? Can you pull? Oh, almost literally everybody, so I was going to explain what it is. So, I'll give you the very, very brief what what this is. Um, oh, so sorry about that. Thank you. Um, Reddi is an in-memory data structure store, um, that is most often used as a cache, as a session store, as, uh, a message broker, as a database, as a vector search, uh, store, or full text search store. And we have nowadays about 2.25 million Docker pools per day, and we actually just crossed 10 billion Docker pools, which is amazing on the, uh, open source, uh, version of Reddit. Um, we're voted again by Stack Overflow as the most loved non-relational database, um, by the developer community. Um Within my team we focus upon three things. There is we focus on the core Reddit, Reddit is open source, and the data structures that you've all been using. Um, we should focus upon developer experience, which means client libraries, visual developer tools, co-pilots, uh, Rdi Insight, maybe some of you have used it. Um, I see some people nodding. Um, and we also focus upon moving data into Reddis and make it easy for you to move data into Reddis. I'll talk about that later on. The way I structured the, the talk is um is a talk of an evolution. Um, I think, um, everything that we did like in the product is, is a consequence of how users were using Reddit. Uh, it's not, um, most often it's users that were pushing the boundaries of Reddis that, uh, we're hitting limitations of what it could do, and we try to find a better solution for that. Um, of course, there will still be some product announcements in there. So sometimes it will be like a QR code, which you can use to, to scan and look into the product later on if you would love to. Um Because the, the talk is low on AI, I've used or asked Chachi PT to create some images. They're quite funny, so I hope it's a bit entertaining, so at least I can say AI as many times as all the other talks that are going on today. Um, and then lastly, um, later on I'll be, uh, joined by Eber Scachecci, which I just learned is, uh, an Italian last name, who's a machine, machine learning engineer at iFood, and he will talk you through how some of the, uh, capabilities that I will show you in Reddis, and how they're using that at iFood. But I want to start with the opportunity of Reddis. Um, according to Microsoft, 80% of the LTPLTP workloads on uh Microsoft SQL Server are pure read queries. Oracle claims that for typical LTB workloads, it ranges between 70 and 90%. And 90% means that for every right you make into a relational database, on average, you read the same unchanged data 9 times. Or in other words, if you order food on iFoods, uh, you check on average on your phone, 9 times that the food is going to arrive, right? Um, these RIs are expensive as they run on, uh, um, relational databases that require lots of compute requirements. And this is exactly why Reddit was created by Salvatore. It's about 16 years ago. It was created to accelerate applications. As a side effect, however, um, some users, they thought they could also reduce the cost of that relational database. Um, I'll come back to that later on. This is the first chat GPT generated image. Not my favorite. There's one that comes later on, but, uh, um, so the prompt here is very, very straightforward. Um, so it's a, it's a common known problem, uh, the whole keys is a common known problem in Reddit. Everybody knows what it is, or? Uh, um, so hot keys happen when, uh, keys are accessed very frequently or written into very frequently. Now there are more than these two use cases that I uh explained here on the slides that, that, that can happen, but let's focus on those two today. Um, Reddis, uh, when, when Salvatore created, uh, Reddit, he was thinking about throwing away as many other things in relational databases that were not needed, and he wanted to make it very simple. So Reddis has a main, main threat where all the commands are executed sequentially. And by minimizing, for example, the time complexity of all these operations and by being obsessed about that, you can actually do many, many operations on a single thread. Now, if you want to scale it, if you want to scale your Reds cluster, you can have a key space, you can actually create a Reddice cluster, so you can have many uh nodes within a cluster. But a single key can only reside in a single shard. You can do some replication, etc. but still, there are some limitations of you, what you can do. There are two common patterns here. 11 is the frequently accessed, uh, keys, which could be, for example, in the case of a social media, there is an influencer, not me, um, whose, uh, um, profile is being accessed very frequently. So there could be a hot key, and that is uh where a high throughput is going to go into a single Rd process. But what we saw when we look into our data in the cloud, how, how Reddit is being used, we actually saw that there was like lots of usage of sets and sorted sets, and we thought like, well, why is that? Like are there, which use cases are they using that for? And I know if some of the users here are using Spring data redis and Spring data Reddis, you can add simple annotations that say indexed or add index on top of some of the fields you do. And then actually that's translating that into sets and sorted sets. So it's creating second indexing use cases in Reddis. Um, and what happened then is that if you want to do, for example, you want to say this field and that field, it will do an intersection in between those two sets, right? But also becomes like a, a heavily uh hot key, um. Uh, in your key space. And hotkeys, last thing about that is actually that it makes your, your keyspace imbalanced. Now I've been working upon um These two problems, and the, the first problem, like the, the influencer problem, um, we've created for that client-side caching. And the client-side caching, we'll actually keep a copy of the hotkey within the client's library. Or in the client application. So on a repeated consecutive reads, you will just access the data locally from local memory and avoiding the network round trip. And this is of course for heavy for great reads, heavy workloads. We have some customers that sometimes do one right and read the same key 50 times. That's a perfect sweet spot use case for that. Um. Reddi will track each client library, which, for each client, which keys are being cached in a local cache. And then we can, when the, when there's a write in the Reddit database, we can actually send an invalidation to those client uh those client libraries. Um, it's been around for a while, but it was not supported by any of the client libraries. So we added the support within all the major client libraries, and it's available since Reddit 704 onwards. So this solves the, the, the influencer problem. For the 2nd reindexing, we created something that initially was called Reddi Search, but we now call the the Reddik query engine. And with the red squaring engine, you can define a schema, and the schema defines which field in either hashes or JSON, we will index, given also a prefix. The index doesn't reside in the key space, and whenever you do a write into the hash or into the JSO, we will just atomically update the index, and then you can access that index. Now it comes with a coordinator, so whenever you make a query, or read query, you will hit one of the charts, sort of, I say charts, but the nodes in, in your cluster, and the coordinator will like do MapReduce operation to all the other processes and will gather the results back. And we've got support for different field types, tag for full text, text, tag for numeric, for exact uh matching numeric, geo, and also vector. And you can combine in the query, you can actually combine the different fields that you're, you've indexed within an index to create complex queries. Now, this actually changes the, the, this is a significant element in in cache modernization because it unlocks like a huge variety of new use cases that you can do. Um, one of them could be, for example, common use case at a session store, you put stuff in, in items in your shopping baskets, but the shopping basket is a single hash for, for all your users. But now you can ask a question like, how many users have this particular item in a shopping basket right now. This is something you can now ask to Reddis. Just give me as an example, but there are many more. Um, one thing is that Reddit is commonly being used for is a, is a feature store, an online feature store, where you, you, you, you, you retrieve features to serve them to an AI, and machine learning model. We'll talk about later on more. But now with this query engine you can actually do the selection of those queries. So imagine you're a, a food delivery company, um, and you would like to retrieve all the restaurants that are in a specific radius from a specific uh viewpoint, you wanna say all the restaurants that are having a tag for, for example, Italian restaurants. And you wanna retrieve all these restaurants and feed them into a recommendation engine. Before that, you have to do like a two phase approach with some other solution. Now you can do that in Reddit directly. And it makes, this makes sense because you probably want to serve those recommendations very fast and retrieve them uh from Reddit. The Reddit query engine is also powering our vector search uh technology. Um, it enables, enables you to do like blazing fast, uh, vector similarity search. And just for you to know how fast it is, we actually, uh, have a benchmark. There is a, a blog post that we have written about that, and you can also find the codes that we use for, for, for benchmarking this on GitHub. Um, we, we, we tried all permutations. We tried different data sets, different embedding models, different factor sizes, different workloads, different queries. Whatever we tried compared to our competitors, we came out faster. We serve the vectors from memory, so that's the obvious reason why we can do that. One of those use cases that you might be using this factor search technology is is for is rack and Aber. We'll talk about that later on. But it also powers um a new product that we're building, we've been building which is called Lancache. And again, it's kind of like a, a sweet spot use case for Reddit. It's called semantic caching. So semantic caching allows you to cache responses from an LLM and then on subsequent similar or semantically similar questions, you can return the response rather than calling your LLM. And as such, you can of course increase the performance or the latency to your application up to 15 times, but you can also reduce the cost. You can avoid calling the expensive LL. Um, we've got a solution there, um, that is, uh, served from our clouds. Um, it has a RAST API that you can call and you can also use, um, several embedding models. We have, uh, support for OpenAI and Anthropic. You can, we also have our own model that is, uh, specifically fine-tuned for, for semantic caching. Um, Asurion is one of our customers that is using semantic caching, and they were able to do a cash hit rate of 70%. Um, and with that, they would actually reduce the, the response time in their customer service portal, uh, in half. They would cut that in half and they will get 4 times more engagement on their customer service portal. Um I, I personally think that this is a great use case, uh, a technology fit for Reddit, right? Reddit was used for caching. You can put a time to live on top of this response, so you can recalculate it after a specific time. This is a sweet spot use case. So it's now available in public preview. If you want to get access to it, it's actually, you can directly get access by on reddit.io/ Lancache, and you can get started with the free database. I'll see the phone coming up, so I'll, I'll leave that on for a while. Um I'm going to switch gears and talk a bit more about caching. So, there are several known caching patterns. You've all heard about cache aside right behind read-through and all these caching patterns, but the cache aside is like the the most common one. So in a cache aside, I know you all know Reddit, but I'll still explain it. You first go to your relation, you first go to Reddit when there's a read, you check in Reddit, it's not there. You've got a cache miss. You will go to your relational database, execute the query. Put it in red as, return to your application. On a consecutive read, there will be a cache hit, and you, you will return a point faster, and as such, you accelerate your application. So this is great when part of your data set is hot, higher P95 latencies are allowed because in some cases, you will still have to go to that relational database and data is changing slowly. And notice that um Your relational database is serving all the rights, but it's or your primary database, but it's still serving some of the Reds, right? Remember that total cost of ownership, uh, um. note that I made in the beginning that Reddit was supposed to kind of like reduce the total cost of ownership and and this is really true because you still need to serve the reads from that relational database. A common problem with this, uh, um, yeah. Well, I love this one. but there's still a better one, hang on, um. Uh, so, a, a common problem with the cache aside is stale data. And, uh, stale data can occur, of course, when you, you did it right in that relation database and you've put some, some keys in your cache. So then at that moment in time, they're out of sync. So the way you can solve that in Reddit is by putting a time to live on that key, and it will expire after a certain time, and then of course, there will be a cache miss and you will call that data again. You will, sorry, we, we'll call your primary database again. And the problem with this is it's a bit of a guesswork. If you put your time to the very fine grains, you will have lots of cash cash misses. And what was the point of having your cash in the first place? If you put it too long, you will get cumbers cumbersome customer experience. Uh, Imagine, for example, you would put all your order list or your recently bought orders into, into a cash. As a customer, I buy something, I go and check the order list. It's not there. I buy it again. That's quite a cumbersome experience. So that might not be an ideal fit. Now this is my favorite. Um, another problem with uh Acache sites is anybody wants to guess what the uh image on the left is? So uh thundering hurts. Uh, which, uh, chat GPT took very little, um, so a thundering her problem, uh, can occur when, um, due to some in a distributed system, due to some, uh, situation, all requests will go into a single resource and as such you will overload that single resource. Um, it can happen, uh, in our social media, and the influencer, when we put an expiry on the social media and the, the, the, and the key will expire. All requests into Reddit will be cache misses. They will go to your relational database. It can also happen when you put like a TTL like expire at a specific moment in time. All the keys with that specific moment in time, they will expire, and they will go there. It can also occur as a cold start, because if you start with the cache aside, initially the cache is empty and all your requests will go to your relational database. So the source database still needs to be filed according to uh serving all the potential reads from that uh um database. Now this is a quote from a uh uh a seasoned developer. Uh, it, it helped me ease great in this, uh, in this talk, right, uh, so Phil Carlton used to work on, on Netscape over a decade ago, um, and he stated that there are only two hard things in computer science, caching validation, and naming things. Um, now that alums are basically writing our codes, we can consider the naming things solved. Um, so if Reddits could solve the cache invalidation problem, that we basically solved all the, uh, computer science problems. Um, so, why is cache invalidation complex? The first reason is that Reddit is a non-transactional database, so you don't have the concept of doing something like a, a two-phase commit in between your rights and to, to those two data source. But that's not the biggest problem. The problem is that the application doesn't know. Which data points are actually being cached within Reddit. You might have a single row in a specific column that resulted into 20 keys, cache keys in Reddis. So there's no mapping in between that. Um The way we see our customers solving this is by using instead of a cache aside, they use a refresher hash, a refresh hat cache or a prefetched cache is also a common name for that. So in that case, you move the data from your primary data store into Reddit. There's some magic there happening on in the middle, and then. You do have some way to capture all the new changes in the database and to rep replicate them to Reddis. In this case, all your rights going to Reddis, and all your reads are being sourced from, uh sorry, all your rights going to your private database and all your reads are being sourced from Reddis. Um, you can also see this for the, uh, for the architects or the, uh, the computer science room as CQRS. It's a proper, a common use case of uh CQRS where you will, you will model your data, uh, for rights in a specific, uh, domain-specific model, and then, then you will surf or translate your data into Reddit data structures to serving your reads faster. Um, of course, this is great when, um, data is accessed randomly. You can't, uh, um, have high P90 file latencies. Data is in flux or higher data consistency is, um, required. Now we, the way we see our customers doing that is uh there are 3 different ways. Um. One of our customers is actually, um, they have like thousands of Chrome jobs that that move data from from Oracle towards Reddis. It's a, it's a, it's a literally an operational nightmare. Other ways could be by using complex architectures, using Kafka, moving data, and of course it makes add more components in your, in your architecture. Of course, ETL tools could be handy to do that as well. They do can do change data capture, but ETL tools are typically made for relation to relational workloads, and they don't know about the variety of data structures that we have in Reddis. And for that, we created Reddi data integration. Reddit are short RDI. It makes it uh synchronizing REDDS to from any relational database and Zoom like any data source. Very straightforward, simple, and fast. And we've built RDI on proven technologies, so we're using Apache Flink and the beesium under the hood. And we're adding our expertise on top of that on how to transform data from a relation or any data source towards the variety of data searches in Reddis. So, RDI actually is a low code solution. You can upload a transformational logical, uh, logic towards uh uh uh RDI. It can allow you to do some, uh, dennormalization. What happens here is that we will take first, uh, we will capture the changes from our initial load from your data source via CDC collector. We'll have an intermediate Reddit database. We're using Reddit streams for that. And then there's a second process which is, uh, um, asynchronously. Retrieving those items from those streams is applying your transformation logic and then writing it into Reddis. In the transformational logic, we might also do lookups into Reddit, for example, for denormalization. Um, we've got many customers of the, using RDI already on, on premise. Um, but Access Bank is one of those that is actually, um, using it, uh, um, for their mobile banking application. They were originally Doing all their core banking logic and and writing it into Oracle and then their their mobile banking app was actually serving their REIs directly from the same Oracle database. They have 6 million daily active mobile banking users and they're trying to scale, of course, the REITs as an application modernization. They were adding Oracle Red replicas, um. But it didn't hit the throughput that they needed. Um, so what we did is we added, um, um, RDI and together with our Reddit query engine that I spoke before, and now the, the results speak for themselves, right? Before they had like 173 milliseconds, uh, uh, response time at peak time, and now it's 29 milliseconds. They were maxed out at 500 requests per second that they could do on their original, um, um, Oracle database, and now they can do tens of thousands and they can actually just scale Reddit by adding more notes into the cluster. So I have a demo for that, um, I did do the recording, so, uh, I didn't know I could do a live demo, but, uh, um, in this, in this demo, it's a, it's a recording. Um, I will, um, show you some data in a Postress database. We'll spin up RDI in Reds Cloud, and we'll show you how it results into Reddis, and then afterwards, we'll make a change, of course, and hope that the change, uh, ends up being in Reddis. Um, so let's go ahead. So, I'm connected to the Postgrad database. Um, I'm going to show you the tables that are there. It's the Chinook data set. Um, I'm gonna show you one table, the Jara table. Uh, so there are two columns. There is Ja ID and name, and the first ID is rock, which we'll use later on in the talk. So I've already created like a database, um, and I will go ahead and connect to that database. So, um, this is Reddit Insight, the visual developer tool for Reddits. You can see there's no data there, but I'm sort of live recording. So I doping and pong, and Reddit is always friendly and answers me. There's a data pipeline section. I already set up the, the, the, the, the initial setup there with private link. I can go ahead. I can select some um tables. I will do artist, album, genre. And right now in the UI I can only choose to write into Hah or in JSON, but you can write in old data structures and you can also upload your transformation file. It gives me a summary and then I can go ahead and I can deploy the pipeline. So we, we made RDDI like secure from the, the ground up. So this will run in your dedicated environment, your dedicated RDI cluster together with uh uh our, your dedicated database. Um, what it first will do, now we deployed this, it will do the initial sync of like hydration, so it will retrieve all the data from the postcard database. It's not that much, but the, the time it takes mostly here is to deploy this pipeline actually. So there we go now it transferred this uh um. Those records, there are 647 records, uh, in total. And afterwards, once it finalized that, it will go into streaming mode. So we got the notification, it's now in streaming mode, so now it's capturing all the changes from the database. We can go ahead and look again into Reddit Insight. The tables are now there. You've got genre ID, genre ID one. OK, there's a key rock that is there. So that doesn't prove still that we can do changes that capture and we can keep on your cache fresh with this, uh, with, with any change that comes. So we'll now do an update in, uh, uh, in the, uh, Postress database. And for that we're going to then use, of course, the BSM uses the right headlock from Postress. So instead of showing you within RedS Insight again, what the changed value would be, um, I, I thought to be creative and I use our RedS MCP server just to talk a bit about AI and, uh, access the data within, uh, Reddis. So I first asked it, of course, how many keys there are in the database, and it knows there's a tool DB size. OK, we've got 647 brackets. So, so far so good. Now then I was doing this recording and I really struggled, um, typing the, the right question. I wanted to ask it like, hey, can you retrieve this genre ID with ID one and can you tell me what the value is, uh, but I was struggling a bit, but I, I thought like, do I do an auto recording or I just leave it it is because it's actually interesting. it still figures out correctly, uh, what the value is. Um, so the, the first thing it will do like, well, I, I don't know, it's a key value story, it's schemala, uh, redis. How do I know which key I need to retrieve? So it does a bit of scanning. Um, it's going to scan a couple of keys, and in those scans, um, you can actually see that it figures out what genres, and, and, and with that, it can figure out what is the, um, the, the key pattern you use, like in redis you typically do use a column in the middle like to separate the specific, uh, um, uh, fields within a key or to make a higher key or like a nesting like a 3 view. Um, so it figured out, OK, there's uh what the structure is, it figured out there are 25 keys, great, and this is the structure, so that's the key that is there. I guess and ask me friendly of course if I can retrieve it and um it still doesn't know which command to to execute because it could be a hash it could be whatever it is so it first it's going to figure out the type and then retrieve that from. Reiss. Um, I hope we're going to see rock and roll in the end. I know we will. Um, so there we go. Um I'm, I'm happy to say, uh, uh, we're working upon that in our team that RS data integration is now in public preview. Um, you can go ahead and get access to it by scanning this QR code or just go to reds.io/data integration. Um For those who've been paying attention, they might have thought like, well, in a cache aside, I have like a portion of my data set, like 20%. But with, with this new Reddit data integration, you will move all my data, and we got a couple of sales reps in the, in the audience, they will like be very happy about that, um, because it might be that you think that it's going to cost me a lot more. So we thought about that and um. When, when Salvatore created the Reddit, um. Like the, the, the, the bandwidth of memory was about 10,000 megabytes per second, and nowadays, uh, and VME storage is 16,000 megabytes per second. Like, so that it's the, the, the architectural principles, design principles that, that he used for creating Reddits like 1516 years ago, they're no longer valid, um, so what we created is we created Reddit Flex. And Flex is auto tiering for higher scale at lower cost. So you extend your keyspace with a slower storage, either SSD or NVME. And so key and values, they either reside in memory or they reside on the slower storage. Of course, we will keep the keys that you access more frequently into RAM and the ones that are cold values, we will, our cold keys and cold values will keep them on the slower storage. So, within Reddit Cloud, you can actually choose how much RAM you will have in your database. You can configure up to 90% uh to be on disk. You can create databases up to 50 terabytes that starts at 250 gigabytes minimum. And, um, of course, the new pricing is, is in place, and it reduces the cost compared to an in-memory data by up to 75%. We've got customers like iFoods. I will talk about that later on, of course, using this as well. Um, and the, the top use case for using this is actually um a feature store use case. And if you combine it again with RDI we can actually move and keep that feature store always in sync, the latest feature that you have, uh, um, in your, in your primary databases. One more QR codes, there's one more coming. Uh, Reds Flex is, uh, since, uh, this week actually generally available on, uh, Reddi cloud. So One more, uh, evolution, uh, story is, is that, um. Um, as, as, as users were leveraging Reddis, they were actually using it for more rich use cases, and this is a beautiful prompt as well, by the way, but um. What we saw is that lots of users, they were actually serializing. Java objects and writing them into, into strings in the strings data structure. Um. Now that's interesting because what they needed was they need some level of nesting. They needed like a hierarchical data structure, but the, the, the, the, the only hierarchical data structure available in Reddit is a hash, where you have like one level of nesting. Um, so the, the prompt here was, um, draw me an image of uh Jason serialized into a string. Um, there are many things wrong there, but, uh, um. So, what is the problem with the, that, that we see that our, our, our users, uh, our community actually doing with this is that they would like to um do, for example, small updates within a NASA document. But to do that, they have to retrieve the entire CLized object. Visualize it, make a, make an update, serialize it and write it back. Meanwhile, somebody else or some other client might have updated that key, so you need to watch that key and you might need to retry that entire operation. Another problem is is that if you would like to retrieve some part of your Nest document, you still have to retrieve the entire document. And that's not the reddest way. The reddest way is about simplicity and about performance, right? You only want to retrieve and send over the network exactly what you need. So we created a native Jason data structure. Um, there's lots of text on there, but the, the main, the main point is that under the hood, it's a binary tree. And as such with JSN path, you can actually do two things. There are many things you can do, but the two important ones to take away from it is that you can do atomic updates on a sub path of your documents. So for example, you can say in this JSON, um, there's a MacBook with a quantity of 15, you can do JO.inorby, you can specify the path of your document and you say, incremented by 3. This is the reddest way, right? Rather than having to retrieve that document, client site, make the change, and write it back. And you can of course also still retrieve subparts of that document. Now, We've built this Reddit query engine, and this also works of course with our Reddit query engine, so that makes it very interesting because we can now index fields based upon this adjacent path expression. Users also wanted to use our vector search capabilities with um uh with this radi query engine. The way to mold a vector in into into JSON is just by using an array, and you have all the values of your vector in that array. Um, the problem that with, with this uh uh initial. Data structure that we've created was that if you would model the same. Vector in adjacent data structure it would consume 7 times more memory than it would do in a in adjacent sorry into into a hash which makes it like very impractical because we can be way too expensive to use that and so it's great that we have this capability but it was. Not useful for vector search capabilities. So what we did is we actually build a a feature that allows us to compress vectors, and we call it like for homogeneous arrays. So when all the elements in your ad Jason are of the same type or the same elements, we can actually compress that rather than having type, value, type, value, we'll not just like compress that. And for uh vector specific vector types, we can actually reduce the memory up to 92%. And that really makes it like practical because before that it was like 7 times more, and now it's actually even less than hash. I think this is a, uh, it's really interesting because now we can actually model uh hierarchical data in Reddit and also add a vector uh similarity search on top of that. Now, that's not the only thing that we've been improving. Um, we've also, uh, focused upon that open source and making it like incrementally faster. Um, this is a benchmark that we did on a mixed workload, and we try to focus upon that. We try to focus upon like real-life workload, not like workloads that are like labs, lab setups, uh, tests. So in this case, it's like 10% rights, 90% reads, um, we're using 4 IO threads, but each uh key is 1 kilobyte in size. And every minor version that we make, there is actually an increment. Um, and the latest version, 8.4, we actually crossed to 1 million. So on a four-core box, right, on a single main thread, Reddi process, you can now do more than 1.25 million operations. Um Mixed workload. And we also wanted to make it faster, of course, for our enterprise customers. So what we've done is we invested a lot in our TLS solution. So when you have like a a secure connection with TLS towards our Red cloud database, we can easily um deliver 2 times the performance compared to Elasticash on the same number of VCPUs. In general, Reddit. 8.4, which is the latest release, is the, the fastest Reddit ever. Um, it's up to 40% faster in scaling up and scaling down, which is particularly important for our, uh, our customers, for example, in events like Black Friday. We, we, we focus a lot upon improving the latency of each command. Um, we can now just by upgrading to 7.4, which was the latest, the, the previous, uh, minor to, to 8.4, you look, you can get up to 90% latency improvements just by upgrading. Um, I know that the, the top two there are Hyperloglock and Bitmap, which might, you might not be using. However, um, for hashes, it's up to 55%, and for sets and sort of sets it's up to 70%. And we also added uh quantization into our vector search solution, which obviously drastically reduces the uh the size of the index, but also higher, enables a higher throughput. And we spoke about the memory savings before. There are many more things that we did in Reddit 8.4, and it would just be like a long enumeration of small things and actually I think they're all like paper cut fixes. Um, sometimes you need to use two commands to, for example, to add an element into a specific, uh, key and then do an expiry, which will require you to do a transaction, etc. etc. So there are small things that we keep on improving and actually make a better, uh, experience for, for our developers. They're made there. Um, I don't wanna go over the long list, but we wrote a long blog post about that, with all those changes, and I'm happy to say that the Reddit 8.4 is now also GA generally available for Reddit open source, and it will be available early, uh, next year in our, uh, in our cloud and also for our enterprise, uh, on-premise customers. Been talking a lot. I'm going to summarize. I'm going to head over finally to ever give me 11 moment. So the the the modern cash, like how this all evolved, like, uh, from, from, from, let's say 10 years ago, we believe that the red query engine is now really an essential ingredient in our modern cash tack. It doesn't only solve like the hot key problem for the sets and sort of sets, but it also powers new use cases such as rack, semantic caching. It transforms existing use cases as session store, but also feature store use cases. Client-side caching, of course, solves the other, the other hot key problem. Whereas data integration, which is now hosted, um, allows you to cache data and keep it always for uh in sync with uh your primary data source. It allows you for for greater scalability, and it's false the first time. And the last thing you need to know is that we're obsessed by latency. It's our fastest release now with RADS 804. We're the fastest secure cache, and also the fastest effective database on the planet. And I highlighted three red things, and these 3 red things are um things that Aber from iFood is going to talk about. So I would welcome him to the stage right now. Thank you. Good luck. Thank you, Peter. So, I'm, I want to share with you, first of all, my three goals here. My first goal is to share part of my journey and some applications that use REDD is inside iFood. And my 2nd goal is to connect these applications to what Peter showed you, and my 3rd goal is to. Uh, make my English clear to you, so I hope to, to achieve the first two goals with my English, right? Um, first of all, I'm from Brazil. I, I worked with tech for more than 20 years. Probably just shows part of my age. Um, I'm at iFood for 5 years. I work today. I work, uh, inside, uh, DNAI platform team which is part of the MAL platform inside iFood, and part of the examples that I will show you is inside this platform team. Um, before iFood I worked with industrial automation for almost 15 years. Um, and what connected me to the data and AI area was my master's degree in computer science. So, um, I know as I am, you will probably forget part of these numbers in a few seconds. This is what I expect, but what I'm trying to show you here is that iFood has. More than 106 million orders per month and this basically shows which is the amount of sessions that we have per day, which is the amount of users that are connected using iFood. Uh, iFood is known, um, who here knows iFood? Well, great. Who, who here knows I read, I read, uh, went to Brazil? Great, great. So iFood is known to be the biggest food delivery company in Latin America, but it's also a company with a whole ecosystem. We don't deliver just, um, food, we deliver, um. Groceries we also have um logistic services to deliver things to to to companies, restaurants and other types of uh merchas um we have, um, when we think about partner establishments we have more than 4. 450 establishments, uh, we have, we are working or running inside more than 1500 cities, different cities in Brazil. We, we have, uh, connected to the logistic ecosystem we have more than 450 delivery people. Uh, we have probably more than 60. Different users using monthly iFood app and we, we call employees from iFood Food Lovers. We have more than 8000 food lovers at this moment and to connect to, to the applications that we'll show you and this is the message that I, I want you to, to record from, from this slide so we have. Thousands of different services running when I think when I say different services are microservices, softwares that are running and supporting all these numbers we have agents, internal agents or agents that interacts with external uh partners running inside our. Our infrastructure, so this is what i food um represents in in Brazil. And the first, uh, application that I want to, to show you or, uh, connect to Regis is what we call internally as Genplat. Basically it's, uh, GAI platform which connects users and different services to GAI models. It's basically a way. To centralize the access to the models, but it also solves governance and a lot of other uh challenges. So uh the numbers of this platform is basically 1 trillion tokens per month, more than 400 million requests per month, and basically we have at this moment more than. 200 different services using our platform to access GAI models. Part of these models are inside AWS. Part of these models are outside. For example, we have OpenAI, Gemini, and we also have some open source models that we trained by ourselves and are running and making G AI inferences there internally, I would. Um, besides these services numbers, we, we also have, uh, more than 500 different users that use DNAI models to create code and use them to make their life easier and better. And where's Red is here. Red is, is basically um the system used to control the rate limiting. Um, I think you already know. That, but when you think about the GAI model, you have limits. You have requests per minute limits. You have tokens per minute limit, and Red is used inside this platform to guarantee consistence of this number. Oh, I'm a user or I'm a service. I'm using this specific model and I need to control this rate limit, and Red is, is an excellent. Solution to do that. So, um, the second example that I want to show here is, uh, the rug that we have internally. This is basically I, I know that we are in a 10th moment, but a few months ago we were talking about 2 months or probably 2 years ago we were, we're starting to talk about red. It's basically a solution that solves the. Missing context from the G AI models. So I'm if you're using an open AI model, if you're using a Google uh model or other provider model, you already know that it doesn't have your data there. It wasn't trained using your data. So the right solution basically solves this problem. It gives context to the models to answer things and to. Um, give you the answer, you know, giving this context. So our internal reg API basically is, um, split it in two important pipelines. The first pipeline it's the ingestion pipeline which gives, gives, uh, given a, um, a group of documents. You split them in chunks. You store them at Ries vector store and then. This, this process is configurable by services or by projects there inside iFood. And then there's a second pipeline which is the retrieval pipeline. During this, these two pipelines we are using different models through Genlat API. The first type of model. Is the embedding model which basically transforms the text or the content in a vector of numbers and stores inside the vector star. The second pipeline is the retrieve and answer thing. So if you receive a text, basically first you transform this text in an embedding. And then you retrieve from the vector star which is inside rags the contexts that are near that text and with this nearest context you deliver it to the model to the GNAI model to answer your initial question so um. The 3rd application example that I want to share with you is the FeatureStar. I don't know if. Everybody knows here what's the, the solution or the pain that features star solves, but it's basically, uh, delivering content to make inference inside of food before our GNAI model was, uh, were, were used, we developed a lot of classical machine learning and part of this classical machine learning are making inferences to and. things for example give recommendations of dishes or other type of products to users that are connected to our application so when these models are uh making inference they need information they need inputs and these inputs are stored inside feature store and connected to those sessions, users establishments. Partners that I that I showed you, we have this type of solution. Basically you need to make a recommendation, for example, to a user, sometimes thousands or millions of users, and you retrieve some features from that user from the feature star and you give it to a classical machine learning model and the output is basically a. Recommendation that can be used there. So with the featuret we basically have 22 different types of um storage solutions. One is for offline inference which basically is inside our uh data lake, and the second one is stored inside regis. So when we want low latency, when we need to deliver things to the model and make inferences below. Let me see here, 10 milliseconds, uh, we use REEI. This is basically one of the, the best solutions or best points that REDs solves there inside iFood. So this is it. This platform also delivered more than 5 million requests per second. It's, and this is basically because we use Redis, um, and we have more than 1, almost 2000 different. Features that are available there. Um, this is it, I think, um, we, we are ready.