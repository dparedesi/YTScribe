---
video_id: ffy7G4lK94M
video_url: https://www.youtube.com/watch?v=ffy7G4lK94M
is_generated: False
is_translatable: True
---

Yeah, thanks for taking the time out. And uh first of all, I've got to apologize for being fluffy. It's because it's cold in here and I was wearing that AWS jacket and I'm all covered in fluff, so that's not my normal appearance, but anyway, I'm, I'm Chris Hillman and I'm the global AI lead for Teradata and I'm joined by. The main event from a small company you might have heard of called Nvidia, yeah, a little startup called Nvidia. So I lead strategic product partnerships and really excited to work with the Terry Data team. So we're very excited to show you what we built together and Chris von Tree kick it off and we'll go from there. Yeah, sure, yeah, so um. I've got, I've got a, a, a, a big team and we work in Americas, Zimia and APJ. Um, and we've been doing some great work recently with some of the biggest companies, you know, because of the sort of nature of our platform, like huge data sets as well. So, um, I've picked out one case to talk to you about today. Um, it's actually to do with, uh, automotive manufacturing, but hopefully you'll see this is a kind of pattern that is really applicable across, across all industries, and you'll be able to see why that's kind of gonna be relevant to you, so. Um, the, uh, we're going to go through what the problem was, what we did, uh, Nina's going to talk a bit about how we did it, um, and then some ideas of how we're going to build this in the future. So, um, the case is actually, um, as I say, for an automotive manufacturer. Now the, the issue they have is that, um. The development cycles are really long, you know, so traditional, uh, manufacturers are stuck in like maybe a 4 to 5 year cycle, um. And yeah, I mean, there's so much going on in that, so many different parts to, to put together, um, literally and figuratively. Um, so, so one of the issues they're trying to fix here is, uh, R&D efficiency because newer manufacturers, um, if you take some of the Chinese manufacturers now, they've gone down to like a two-year development cycle. So they're like bringing new products on the market so much quicker, um. Now the R&D side of this, uh, is, is, is hugely complicated because there's the engineering side, there's things like, you know, the efficiency and uh the safety, um, but also one of the issues they have is the level of expertise needed to do this work. Um, so like an automotive engineer, it takes a long time to get to the point where they can actually, uh, make decisions, uh, because everything's got to be super safe, obviously, um. And that is particularly what they wanted to focus on. Can we, can we, um, speed up that R&D process using AI? Particularly wanted to make this a gen AI, uh, uh, project, um, and then could those junior engineers still be as productive as the, the, the more senior ones? So the starting point for all of this was uh the specification documents, so a huge amount of documentation is created when they're creating these new models, um. And that would normally be traditionally searched manually, um, and so this has got everything in it, you know, tolerances and um benchmarks and all these kind of things, expected levels of um of uh expected levels of performance. So by taking those uh documents and turning them into vector embeddings, um, which is where we use the Nemotron services that uh Nima's gonna talk more about in a minute, um. Uh, I'm not gonna go into vector embeddings cos I guess you kind of already know that what that is, that there's plenty of other information about that, but basically it's taking a complex data type and turning it into a vector of numbers, and once we've got that, uh, once everything's been converted into vectors of numbers, we can store them because terradata has a native vector store in it. Um, we can store them and then they can be searched semantically, that's the key thing. So semantic search is you're searching on the meaning, not on older stuff like we used to do like TFIDF, you know, um, some of my data scientists call that brutal methods, but, you know, it used to work, but nothing like as good as what works now. So, so take the documentation, put it in the vector store, and then at massive scale you can search this stuff, uh, for meaning and similarity. So that's, that's half of the story. The other half of the story is that they've got a very sophisticated test track that they run these cars around when they're in the testing phase, um, and that is measuring hundreds of things on sensors that it, uh, uh, you know, that data streaming off the car. So, um, you've got things like, you know, uh, brake temperature, um, battery drain, battery load, all these kind of things, many vibration. Hundreds of data points coming in and actually in real time, you're talking about terabytes worth of data for every run these cars do round the track. Um, now that needs to be processed and obviously time series, uh, signal processing is, is an established thing that's been around for a while, um, and that kind of process of smoothing and resampling and all these kind of things, again, we can do that now directly in the database. So at the end of the day, you end up with a, uh, a system that has. The unstructured data and the structured data in exactly the same place, and then that's where, you know, the real magic happens here because using a large language model, you can query that in real time and get answers that combine both of these sets of data, OK, um, now interestingly this is where our partnership with AWS was kind of key, because if you think about this um. Research and development data that's in there, it's super sensitive, it's about as sensitive as you can get because that cannot get out to competitors. So, um, we were actually using Cloud here and, and there's no way you could use the public service for Cloud, um, because that would be sending this data backwards and forwards, you know, so using uh Cloud in AWS in Bedrock, um. I like the term, I, it wasn't me that came up with it, but somebody's got this term uh private gen AI. So everything is within your network and you're not sending anything out elsewhere, and the kind of AWS were key for that. Um, so given that, you can now ask questions like what anomalies were there in this run, and uh the system can look and say, you know. The temperature on the brake disc was too high at this particular point, you can then say why was that, and it can look at all the other sensors and the specifications and maybe say, I don't know, you were going too fast or the brake pads are worn, or the disc is worn because it's looking at the vibration at the same time. So, so given all of that together. We basically can answer questions like this, so, you know, which parameter is responsible for the worst fuel consumption, simulated results with this spec compared to the previous spec, um. And it can actually answer this is because the vehicle speed threshold for the 7th to 8th, this is a quick test this 7th to 8th gear up shift in the gear shift map has been increased by 5 kilometers. Um, and now given that, it means these junior engineers can actually access the same information that the senior ones knew, and they can begin to actually work on this, this, this, these problems and actually be far more efficient with the way they work. So looking a bit more closely about how we did that, I'm gonna hand over to Neima to talk to you a bit about the Nematron service. All right, thank you, Chris. So very quickly, the Teradata Enterprise Vector store is powered by Nvidia's Nematron models, and this one in particular is composed of two specific workflows. The workflow on top is the end to end rag solution. It's retrieval, augmented generation, so the ability to generate embedding, generate, uh, vectorized embeddings, store them, retrieve them from a vector database, re-rank the results, and run them through a reasoning engine if you want, before you output to a, a customer via an API, uh, or whatever the interface you are. And you can also put like guardrails in front of it. The lower workflow is the extraction engine. So this is the ability to take unstructured data in any format and parse it through and then basically decouple it and, and uh split the text-based embeddings from the image-based embedding. So in this particular workflow, for example, we have a PDF parser and you can. These are all composable as microservices. You can swap out any of these components for whatever which component that you'd like to use. So if you have a better PDF parser, if you have a PDF parser, it's multilingual or multimodal, or a newer version comes out, you can swap these out in situ and the overall pipeline will continue to operate. So in this example, the way they've set it up is that, um, documents as they come through the PDF parser will get split up between. Uh, infographic on the top versus textface only on the bottom. And different modules, different retrievers will be able to deconstruct the images, charts, graphs, uh, bar charts into a text-based format so that you can store those directly into the embeddings and, uh, and therefore every bit of metadata or document data is in, uh, transcribed and available in a text-based format. What's really powerful about this is that this one, this particular configuration on the bottom. Uh, the current Nematron models and OCR, uh, Nematron OCR will generate about 50% more accurate results compared to other open source, uh, options. And in its current configuration, it'll roughly parse through about 30 pages per second per GPU. So you can transcribe about 1.3 million pages of documents per week per GPU, and that will scale linearly. Um, that's the general consensus. All these are completely configurable. So over time, as we build better versions of the Neotron, Nano Super and Ultra models and the retriever, rear ranker, and guard rails, they can be swapped out in situ by the customer directly by the end users and. Able to generate better responses. So over time, just the quality, the throughput, the speed will all improve uh on a from the current baseline. So with that, let me turn it back to Chris and we'll go on the next one. Yeah, thanks. Thanks, Neima. So yeah, and what I haven't shown on here is the um. The the time series processing of the sensor data, but that's all, that's all happening in the database, so that's not part of this final system, that's kind of pre-processing that goes along with that, OK, so, Given that that's what we've we've done, um, obviously we've been looking and talking a lot about what's coming up and what's in the future, um, and, and, and of course the answer has to be agents, doesn't it? I mean that's the answer for the future for a lot of us. Um, so, so looking at how this might, um, how you, cos this is what I would call augmented intelligence, OK, so although this is like. Something you couldn't have done 3 years ago, you know, pretty much. Yeah, you, you could have done it in different ways and it wouldn't be anywhere near as accurate, but, um, moving on and you start to look more at automation, um, how would that work? So, so building on top of that, you know, I like, I like that definition of an agent, um, a wrapper for an LLM that provides a frictionless automated experience. You've probably heard lots of different, uh, variations on what agents actually are. But for us, there has to be. To go from that kind of chatbot to something automated, there has to be this kind of bill of materials around it, there's a lot of other things you have to get involved in er to go for a full automation. So obviously you need the agents themselves, you need some kind of uh agent framework, so uh crew, lang chain, whatever, there's loads of them. But that will typically be running in a Python server. Um, you, a lot of people are looking at low code applications, um, we are obviously ourselves, um, and uh that's a nice way of bringing in maybe the engineers themselves into this kind of process, cos they're not gonna be writing the Python. Um, you need the large language model itself, um, and you know, that's what we're talking about with, with Bedrock in this instance. Um, guardrails models are just gonna be so important, and if you're looking at anything to do with agents at the moment, you really need to research guardrails models. Everything that is. Input to that, sorry, every, every prompt that agent outputs and every result it gets back needs to be checked because there's a lots of scary things that can happen, um, and hallucinations are the least of the problem, but so yeah, guard rails model is super important. Um, the comms protocols, I mean we've gone for MCP uh we've got our own um uh uh open source version of an MCP server which connects to terradata. Um, you need the knowledge platform itself, which is, um, the, the terraated system, and then other tools because, um. You know, there's always other things that you're gonna need like web search, um, uh, file search, you might have uh an application that needs controlling, so you need these other tools. So if you look at the diagram on the right, the bits that are colored in are the bits that that we do, um, but there's a whole ecosystem around it that needs to be built out if um if you're gonna turn from augmented to, um, automated. So another way of looking at that um here along the bottom you've got the tool, the tools that you might need, cos if you, I always like to think um and I think it was actually crew AI came up with this agents, tools and tasks. The agents have a job to do. They've got tasks they need to carry out and they need tools to carry to complete those tasks. So the kind of tools you've got on the bottom, you need access to the data source, you need um access to the external data in the data fabric, um, you need the analytics themselves, uh, so either for us we have stuff built straight into the database because um. A lot of these functions that we would use in an AIML world aren't set-based SQL, so we need uh these parallel functions built in that you call from SQL. Um, BYOM is a key, a key part of this, stands to bring your own model, so if you, if you create a model in, in pretty much any system. If it can be exported to the format called ONNX, the Open Neural Network Exchange, then we can import it and we can run the inference side of that in parallel along with everything else. Um, and then, uh, you know, all the other kind of functions you might use like the, the, the, uh, time series processing. So when you look at that middle layer, the MCP, the reason there's multiple ones of them, um, and again there's probably other sessions on this around here, but um, there's a problem with something called tool overload. So as you add more tools to that MCP server to prevent the agent from hallucinating, so it uses the right tool for the right job. You risk confusing it because then the, when the LLM creates the query, it might pick up the wrong tool if there's too many of them. I know ours has got a lot because we've got such a huge capability. um, so one way round that is to actually create. Uh, tasks or like specific MCPs for the things they're gonna do. So if you think, you know, you might have a load of DBA tasks, things like reindexing. Uh, archiving tables, things like that, you could create a specific MCP server and restrict it to just those tools, so it it takes away some of that tool overload problem. If you're just querying, you might just have a query tool again and not have all the other stuff around it, so it's a one way around it, um, and as I say, the agents themselves sit on a separate layer in, in the Python server. Um, so that's, that's where we're going with this, and, uh, you know, it may be that. It'll carry on being augmented, it may be that the agents are finding all the anomalies, suggesting solutions, and then somebody else is making the final decision, or it might be that they end up being completely autonomous. Um, OK, so last slide, so. Unstructured data, untapped gold mine, um, you need the vector stores your way around that and using services like Neumatron, um, and getting that, I mean that throughput that you're talking about now is quite incredible to get the, the, the, the, the shred the PDF shredded, um. Don't forget hybrid environments, you know, a lot of companies we work with are still using on-premise as well as in cloud, um, and that is, that doesn't matter to us because it's the same software running everywhere, so we often have this, this, because a lot of the innovation happening in the cloud, so, you know, you still need that kind of service, um. And then, yeah, if you're looking, focus on building the expert agents. So if you scan that QR code, um you'll get more information or come and see us on the terradata booth. Thanks very much.