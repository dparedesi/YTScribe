---
video_id: 6Udbsg4uPeU
video_url: https://www.youtube.com/watch?v=6Udbsg4uPeU
is_generated: False
is_translatable: True
summary: Emphasis AI's agentic framework revolutionizes legacy modernization by extracting intelligence from decades-old code and converting business logic into living data rather than simply translating programming languages, addressing the fundamental problem where every CIO struggles with innovation velocity because touching legacy platforms built on COBOL mainframes, Java, Natural Adabas, or assembler carries catastrophic risk across interconnected systems requiring scarce specialist engineers no longer in the workforce, leading enterprises to accumulate crushing technical debt through endless workarounds and layers that make even basic changes prohibitively expensive while rendering modern agentic AI initiatives impossible since deeply monolithic systems prevent agents from delivering meaningful productivity. Traditional modernization follows a predictable five-step cycle of relearning legacy systems, reimagining functionality, rearchitecting components, recoding implementations, and running operations, essentially translating COBOL to Java only to watch the new system become legacy within three years, perpetuating the modernization treadmill without fundamentally changing the underlying problem of intelligence trapped inside code requiring specialized knowledge to access, modify, or extend, whereas Emphasis's approach extracts business logic from code and documents into human-understandable knowledge using domain-specific ontologies developed across financial services, insurance, and engineering domains, creating enterprise knowledge graphs that capture not just data relationships but semantic meaning aligned with business concepts, enabling continuous evolution without creating new legacy systems. The NeoZeta agent performs autonomous and semi-autonomous reverse engineering with humans in the loop, reading documents and code to extract business rules, data dictionaries, and processing logic from complex capabilities like post-trade processing where millions of trades must complete processing by 4 PM cutoffs for regulatory reporting, parsing COBOL copybooks and programs to generate comprehensive documentation including summaries, business rule logic, input specifications, data element impacts, and processing steps, with LLM-as-judge verification providing confidence scores for each reverse-engineered rule where 95% accuracy qualifies as excellent and 85% triggers human review, typically requiring manual corrections for the first 100,000 lines but achieving 95% automation for subsequent millions of lines, converting everything into knowledge graph representations where SMEs can explore relationships between domain models, functions, attributes, and business processes for verification and understanding. The NeoSaba semi-autonomous business analyst agent transforms extracted knowledge into user stories organized as epics, features, and acceptance criteria following agile methodologies, presenting information as business process workflows where analysts can view individual business rules, combine multiple rules into new capabilities, and completely reimagine applications rather than performing lift-and-shift migrations, generating Behavior-Driven Development specifications with Gherkin syntax establishing quality gates from the beginning, assessing user story quality through INVEST criteria scoring, and providing prompt mechanisms to refine stories, break down overly complex requirements, and ensure acceptance criteria meet enterprise standards for testability and clarity, fundamentally allowing business analysts without deep technical expertise to drive modernization based on business logic understanding rather than code comprehension. The NeoRena agent architects target state designs customized for each client's enterprise standards, consuming playbooks defining reverse engineering steps and standards specifying architectural patterns, generating logical models, physical models, sequence diagrams, observability patterns, and infrastructure specifications aligned with chosen technology stacks such as Java Flink architectures for stream processing, building comprehensive context that feeds downstream development agents with complete architectural specifications eliminating guesswork and ensuring consistency across modernized components, with the system adapting to various target architectures whether microservices, serverless, event-driven, or batch processing based on enterprise requirements and standards encoded in knowledge graphs. The NeoCrux code generation agent translates architectural specifications into executable implementations, integrating with existing coding agents deployed in enterprise environments, generating complete applications from context-rich prompts that include business logic, architectural patterns, data models, and quality requirements, demonstrating dramatic performance differences between GPU and CPU execution for trade processing workloads where scaling from 20,000 to 100,000 trades reveals cost-performance trade-offs enabling job-by-job optimization decisions, with generated code including observability instrumentation, error handling, and compliance with enterprise coding standards automatically derived from knowledge graph metadata, while continuous testing through behavior-driven development specifications created during user story generation ensures generated code meets functional requirements without extensive manual testing cycles. The Ontosphere enterprise knowledge graph serves as connective tissue orchestrating all agents, functioning not merely as a database but as a semantic layer encoding domain knowledge, business processes, technical architectures, and organizational standards in interconnected representations that support reasoning, validation, and automated decision-making, accumulating institutional knowledge that persists beyond individual modernization projects, enabling future initiatives to build on previous learnings, supporting impact analysis showing downstream effects of proposed changes, and providing queryable intelligence for business users exploring system capabilities without reading code, fundamentally transforming enterprise knowledge from implicit expertise locked in veteran developers' minds into explicit, accessible, evolvable data assets. The transformational results demonstrate that typical 50-million-line modernization programs requiring seven years under traditional approaches achieve dramatically compressed timelines through agentic automation, but more importantly, the infinity symbol at the end of the timeline represents the paradigm shift where extracted intelligence living as data ensures enterprises never face legacy challenges again, as business logic remains accessible, modifiable, and extendable without specialized programming language expertise, new requirements integrate seamlessly through knowledge graph updates automatically propagating to affected components, and continuous evolution becomes natural rather than requiring major re-platforming projects every few years. The framework's distinctiveness lies not in translating between programming languages, a commodity capability many vendors offer, but in extracting and preserving intelligence that survives technological transitions, enabling business-driven innovation where domain experts directly shape system behavior through natural language interactions with knowledge graphs, eliminating the translation barrier where business requirements filter through technical specialists who may misunderstand nuances, accelerating time-to-market for new features from months to days, and positioning enterprises to adopt emerging technologies like quantum computing or neuromorphic processors by regenerating implementations from persistent business logic rather than rewriting from scratch, fundamentally decoupling business capabilities from implementation technologies and ensuring organizational knowledge assets remain valuable regardless of underlying technical evolution.
keywords: legacy modernization, agentic AI, knowledge graph, COBOL migration, enterprise ontology
---

So my name is Anno Nair. I'm the CTO for Emphasis AI. And I'm, I'm Bharra, senior partner in Emphasis. AI. So we are gonna talk about legacy modernization and emphasis framework agentic AI framework for legacy modernization. That's what we're gonna talk about. We have, uh, some exciting demos as well as part of this thing. So I'm hoping you'll like it and if you have any questions, please feel free to get us offline. I don't think they allow us questions here, but yeah, why not, right? So, um, to start off with, let me give you a little background here, right? Let's discuss the problem statement first. I've in the last 25 years. Every CIO I have met, this is the problem. I can't innovate fast enough. Because it's too risky to touch any legacy platform. Are you guys on the same page when it comes to this? In fact, one of the CIOs actually told me that I have 49 core systems that are built on cobalt mainframes, and if I touch one of them, I have to touch all the 48, remaining 48 as well. So this is the problem we decided to solve. Emphasis. AI. Has built many different agentic AI solutions to solve these problems, so emphasis has uh has an experience building and modernizing legacy application for many years. So we're gonna talk about this, but more importantly, I think I wanna double click on the problem a little bit more to really understand the genesis of our solution, really describe the solution itself. So, you know, the problem with the legacy system is that enterprises are Uh, you know, are anchored on these systems for years. And these, these systems have code, business logic embedded inside code for years, right? And every time you want to do something, you need engineers. Right, you had business logic, you need, this is in Cobalt, this is in Java, or it could be natural or natural Adabash, or it could be assembler for that matter, right? It could be all of those kind of systems, but you need specialist engineers to handle this. These engineers are not easily available. They're not in the market. They're out of the workforce at this point in time. Therefore, you start reinventing the wheel, keep reinventing the wheel on an ongoing basis, and you start creating a lot of technical debt. Now, any change is too costly because you're doing it at 10 different places and you're connecting, creating layers and layers and layers to solve the problem because you don't know what's actually going on. So now you think about this and you decide to put AI on top of it, agentic AI, right? You're driving innovation, you need to put agents, right? Whether it is agents around claims or agents around underwriting or whichever agent you want to put agents on top of this. Think how it is going to be, because all of these legacy systems are so deeply monolithic. It is extremely hard for any agent to give you any productivity. So all your agentic AI objectives and goals go for a toss. Just because you have this, this is the reason why modernization, this is, this is the reason why any innovation is so slow, because you have to touch everything out there. So we, we at emphasis, we thought about this and we said, OK, you know, it is really not just about technical modernization. It's not just modernizing cobalt to Java or natural to Java or C or C++ to something new. That is not the point. It's about taking the intelligence out of the legacy systems and converting that into data. That should be the goal, as opposed to just modernizing code from A to B. That is exactly the approach we have taken. So let me talk a little bit more on how we are able to solve this thing, but for that, how do you typically modernize? Are you aligned to this, everyone? You, first you'll relearn from your legacy systems, then you reimagine, then you rearchitect. And then you recode, and then you start running it. This is how you do. You take cobalt and you convert it into Java, and you start managing it again. In about 3 years' time, that becomes legacy again. This is how you typically do it, right? So what we have done is a slightly different approach here. We have taken, we, we've built agentic AI agents, AI agents that are autonomous and semi-autonomous with humans in the loop, because anybody who says that everything is 100% autonomous is joking. It's not. OK, so we take, we took documents and code, we built an agent called Neozeta, and this agent actually reads everything, converts everything into human understandable knowledge. How do you do that? We use domain knowledge, emphasis domain knowledge to really make that happen because we've encoded them. We created, we create a knowledge graph. And then we thought, OK, now we have a knowledge graph. How do you take this and Create a new system. What do you do? You generate user stories out of it. So we created another agent we call it Neo Saba. This agent takes everything that you've learned, converts into user stories, and focuses on governance, focuses on compliance, co-focus on processes, business processes. SABA stands for semi-autonomous business analyst. We take that and we allow a business analysts to work on it. We then convert whatever we create out of this, we put it back. Into Uh, the knowledge graph. Now, what's the next step? You reimagine how a rearchitect. We've created another agent we called it Neo Rena. This is an agent that will take everything you've learned and helps you define the target state architecture. This target state architecture is customizable for you, for the client. And everything you create out of this aligns to enterprise standards. The reason why it's customizable is because you need to make sure that it aligns to enterprise standards. OK, so once you create that, then all the architecture is done. You take that and you rewrite. We created an agent called NeoCrux which takes everything, allows you to prompt, it uses whatever coding agent is available. In your enterprise and starts writing code out of it, continuation to ops as well, right? Why not? You've built everything you put into production, you've tested it through crux and all. Now you're putting into production, you're gonna run operations as well. But all through this we've built a connective tissue. We've extracted the intelligence out of the out of the whole thing and created an enterprise knowledge graph. That enterprise knowledge graph is not just a database. Knowledge graphs a database, but this is not just a database. It, it, it has meaning. It has enterprise meaning because it is connected to your domain. We call it Ontosphere, and then we used a layer on top of it to orchestrate the whole thing, so that we can get it as autonomous as possible. So this is emphasis approach to modernization. Now, you will see some of them are autonomous, some of them are semi-autonomous. As AI progresses, as context abilities grow, this will become more and more autonomous, and essentially you create a live intelligence of the enterprise. So what you are going to see today as a demo, you're going to see Neo Zeta. Extracting code out of extracting intelligence out of code, you're going to see Neo Saba allowing the business user to create user stories. You're going to see Neo Rena, you're going to see Neo Crux, and you're not going to see AI ops because we didn't have enough time, but you're going to see a glimpse of ontosphere as well. So you're gonna see 5 things, Zeta, Saba, Rena, Crux, and Odon ontosphere. OK, so I'm going to switch to. Uh, a demo and let Bharat walk you through a whole suite of things. But, so, uh, let me start with first talking about Ontosphere because this is where the heart of the information lies, right? Every knowledge in the system, whether it is code or document. Is going to be brought into his data on the ontosphere now we're using a couple of uh domain ontologies uh that uh we're using a couple of domain ontologies which have been built up which is, uh, you know, on the financial industry, insurance, and other industries. We have also kind of really put in a couple of engineering ontologies which is through our agents. OK, this is the basis on which a lot of modeling will be done into the system. Let me go to the first agent. So this is Neo Zeta. As you see, Neozeta, I've selected a, I've selected a program. I've selected a capability, and we took a very complex capability in terms of the post-trade processing because this is where you have a process which stops at 4 o'clock. You have maybe millions of trades which have to be processed and files have to be sent to the federal authorities. So we took this problem, we said that the only way that we will be able to solve this is pick up a lot of intelligence from each of these processes which have been coded in legacy cobalt. Uh, this is an example of cobalt that we're showing you and then reverse engineer it and post that help to kind of really modernize it using our agents onto, uh, onto a fli-based architecture, and I'll show you a glimpse of how did we execute it on a CPU as well as on a GPU, how the performance varies. So, continuing with our demo, I have selected the, I've selected the capability. I have capability to upload my files which may be documents which may be uh different assets. I've uploaded cobal copybooks and I've also uploaded, as you see, a, a domain model out here. So these are the programs that you see and this is the domain model that I've uploaded using the relearn agent. I'm actually relearning each and every program or an entire capability. So as you see, I've actually selected a program which is quantiles along with copybooks, and I'm doing a reverse engineering of it, right? So there is uh so what it does is it, it has capabilities to generate the data dictionary. Now, we haven't fed any information, but you see that there's a good amount of information that it has brought out from the system and it's starting to map it with the actual functional attributes. And I'm also showing you the document that it produces. The document has a certain format. It produces summary, then for each and every business rule, it will start providing me a lot of data elements, as you would see, right? Uh, it'll give me the logic. It'll give me the input. It'll give me the data affected. It'll give me a lot of other processing elements that I would need as part of relearning a certain code. Now, the most important piece is how do you verify that the system has done it very accurately. So we've introduced what is said LLM as a judge, and it gives me a confidence score for every rule that has been reverse engineered. As you see, It tells me that one of the rules has got an average coverage which is 85% for us, 85% is average. Anything above 95% is something which we'll kind of counted as a better one, but uh. Uh, it, it tells me so that I can, there is a human in the loop interface over here. I can go and make, you know, corrections over here. Uh, the 1st 100,000 lines of code typically need such corrections, but then for the next million lines of code, it's an automated way where you get 95% accuracy from the relearn agent. So we've produced data dictionary, we've produced business rules, and you've seen the business rules verification as well. The third part is to convert everything onto the ontosphere, which is the knowledge graph, and which is what, let me demonstrate that to you. Uh It's just showing you some of the attributes from LLM as a judge, uh. So I'll go to the knowledge graph right now. Uh, actually, before I go to the knowledge graph, it has produced information also for the entire capability. So you have a model to take program by program or for the entire capability. Typically if you have a big job which is running and multiple programs in it, multiple processes, you would like to see such a view. In terms of bringing the entire intelligence out. And finally, the third step in converting it to a knowledge graph. So if you look at it, it's, I've picked up one of the programs which are reverse engineered and it is showing me which are the domain models that has been associated with it. I can double click on functions, on the, on the attributes, on the information, and this is something that uh SME can use it for our verification purposes, right? So essentially, uh. Everything that you saw earlier was the first agent. This is where the information comes to the second agent. All information is put up as part of the business process. This was exactly the graph that you saw earlier. It's coming now in a, in a, in a business as a as a as a workflow item, and I can leverage this, look at it, and start reimagining my application, which is in the top section that you would see. I can even pick up business rule wise, combine multiple business rules and create a new rule. So we've given those capabilities out there. On the top, what you're seeing is essentially uh nothing but uh. And and overall agile remodeling of the application. So I'm completely reimagining it. It's not a lift and shift model that we have. I'm defining, I'm, I'm helping the BSA define the epics, features, user stories, and for every user story there is an assessment done on the quality of the user story through the Invest score, as well as you have a prompt mechanism by which you can generate acceptance criteria. Generate a lot more information. So you see the gherkin output over here, we actually create BDD at this point in time, so quality essentially starts for us at this point in time. Uh, this is the prompt where I can generate information in the form of, uh, if, if, uh, if, uh, if it doesn't meet my invest criteria I can further break it down into many different rules. So I'm going to the 3 agent in the interest of time, uh, from a third agent perspective now my important part is to rearchitect or redesign it. So I've created. So, so over here what is needed is 3 things, right? I actually 2 things. One is a playbook about the steps which we have to do in terms of reverse engineering, and third is, and second is the standards that I will use. We've given both as an input, and I'll just show you how do we do this, but it started to give me all information whether it's a logical model or it's a, uh, it's a, it's a physical model. Or it's a sequence diagram or it's an observability pattern that has to be leveraged. Everything which has been fed as a standard is being brought up by the agent, which is Raina, the third agent that Anoop spoke about, right? Uh. So, uh The the job of this particular agent is to build the entire context and make a prompt ready for my next agent from a development perspective. So if you look at it, I'm generating right now, I've fed the information that I need to generate it using a Java Fli model. OK, I've just shown the step how fast you can actually do it. In the interest of time. So, I'm trying to create a new design, and I've given to it. I'm giving to it uh the playbook as well as the standards which have to be used. I'm using data engineering architecture. I'm giving the playbook for the standards which is the Flink one. And I'm generating the entire context. So all information pretty much gets generated from that perspective. Now I'm going to my 4th agent, which is the code generation agent. Now it's a, uh, you know, this gene this gets the entire information and generates code from a Java Flink architecture perspective. What you essentially see is. An example of how this ran on the GPU as well as it ran on the CPU. So there's a big distinct difference in terms of the timing that you would see if you run we modeled it for multiple sets of traits. So for example, if I go from 20,000 trades to 100,000 traits, what will be the cost from a CPU perspective versus the GPU perspective, and we help them to make a, make a call from a, from a job by job perspective. So that was one I had to show. Let's go back to the, the deck. So guys, uh, this is what we have seen so far. From a results perspective, we've achieved this. As a typical modernization program with about 50 million lines of code takes around 7 years. OK. We've achieved this, uh, kind of progress that infinity at the end, guess what it is? It is the fact that your entire intelligence moves into data and lives with you forever. You never have a legacy, right? That's the thing, and this is kind of, we kind of put together, how are we different from anybody else. There are a lot of people you'll get who move A to B. But very few who will extract intelligence and and give you a state where you'll never have a legacy anymore. So that's what we wanted to cover our agents have actually run on db stab. So you know they, they, they scale up pretty high on that. So we've constantly measuring it, monitoring it from that perspective. Thank you guys. Thank you guys. That was all we had. The time is up. Thank you. Thank you so much.