---
video_id: RQfW7eQsXqk
video_url: https://www.youtube.com/watch?v=RQfW7eQsXqk
is_generated: False
is_translatable: True
summary: "In this technical deep dive, titled \"Using Strands Agents to build autonomous, self-improving AI agents\" (AIM426), Aaron and Shaotai from the Agentic AI organization demonstrate the practical implementation of \"Strands,\" a framework for building autonomous AI agents that can generate their own tools, modify their own system prompts, and orchestrate complex workflows through meta-agent patterns. The session is structured around a live coding demonstration available via a GitHub repository, guiding the audience through six pivotal commits that progressively increase the agent's autonomy. They begin with a basic agent equipped with a shell tool and then introduce the concept of \"self-extending agents\" that can write Python scripts to a local directory—such as a calculator or weather tool—and immediately load them for use without restarting the process. The demonstration advances to \"self-modification,\" where the agent updates its own system prompt stored in an environment variable, effectively rewriting its instructions and persona in real-time based on interactions. This establishes a critical three-step loop for autonomy: dynamic retrieval of the prompt, self-modification by the agent, and persistence of the changes. This loop also enables a form of memory, which is further enhanced by integrating vector stores like Amazon Bedrock Knowledge Bases to retrieve and store conversation history dynamically. A significant portion of the talk focuses on \"meta-agents\" and the \"Swarm\" pattern, where a main agent can dynamically spawn sub-agents using the \"Use Agent\" tool to handle specific tasks in parallel. The speakers explain how agents share context through a \"Swarm\" tool or a file-based \"Journal\" system, allowing for \"fire-and-forget\" asynchronous workflows or recursive \"thinking\" loops where agents refine their outputs multiple times using the \"Think\" tool—mimicking a human researcher revisiting and refining their work. They visualize this as a paradigm shift away from static, pre-defined graphs towards model-driven orchestration where the AI determines the necessary team structure. The session concludes by showing how to productionize these agents using \"Agent Core,\" an AWS managed service. By adding a simple `@app.entry_point` decorator to the Python code and running a CLI command (`agent-core launch`), developers can instantly deploy their local agent to a scalable, serverless cloud environment. The speakers also highlight the \"Agent Core Dev\" command for local containerized testing that matches the production runtime. Finally, the Q&A segment addresses the stochastic nature of these systems, introducing \"Agent Core Policies\"—natural language guardrails that use CEDA and automated reasoning to generate formally proven constraints—as a way to bound the agent's autonomous behavior, and citing real-world examples like a personal research agent that scours Arxiv daily and the automated generation of the Strands TypeScript SDK itself."
keywords: Strands Agents, Autonomous AI, Self-improving Agents, Agent Core, Meta-agents, Swarm Pattern, Agent Policies, Self-modification

We're Going to be learning about strands agents and building autonomous, self-improving AI agents. So, my name's Aaron. I'm a principal engineer in the Agentic AIOg. I'm Shaotai from the same organization as a research engineer. Yeah. So today, um, by the end of this session, we will go through watching an AI agent read its own source code, create new capabilities while it's running, and then we will deploy it to production, a self-evolving agent. And this is not theoretical. We have um a GitHub repository we'll share with you as well, it's all working code. Feel free to follow along, or after the session as well. So we can jump in. So we had this um this polyp, just as everybody was, was coming in here. Uh we got some of the results here. So we have a such a mixed results, very mixed results. I vote for I'm new to agents, so don't mind me. Thanks for this voting. Looks like we have people build simple workflows, use frameworks like stranded agents, new to agents, and production agent systems heavily, right? A couple of people even building autonomous agents already. We love that. Perfect. Yes, so thank you for those. So, so what are we building today? So we have a GitHub repository. You can scan the QR code, open in your GitH, and follow along with us over the comments. We have 6 commits to share with you. Each individual slide has its own code, so you can also follow us while we're explaining the ideas, and you can inspect the code and run. Yeah, so in this, in this repository, um. And you can, you can talk to us outside afterwards as well. We'll, we'll be around, um, but in here this is, this is the code that we're going through today. You'll see several commits in there as Shagatai mentioned. Each of those commits builds up from the last, um, towards our final. Self-evolving agent. So feel free, feel free to, to follow along there. The agenda for today So we'll start with a very basic agent, um, looking at the poll results. Shouldn't be anything revolutionary to most folks here. We'll start with a very basic agent, and we'll, we'll start gradually introducing more and more autonomy. First of all, we'll have the agent build its own tools. So, you know, folks who've built agents before, agents are a couple of things, one is the model, one is the tools, and these tools. Often we write them ourselves, you know, we, we build Python code or we build an MCP server. What we're doing here is having the agent actually build that tool and test that tool itself. And if the tool fails, it can correct the tool's implementation and continue from there. Once we have that, we will have the agent update its own system prompts. So again, um, with the work that Shagatai and I have done in, in Quiro and, and Q Developer, for example, um, we have, you know, engineering, science teams. Evaluating, refining, writing these prompts, that kind of thing. In this case, we're actually having the agent do that itself. We'll continue the autonomy, uh, increasing the autonomy from there, we'll talk about learning from interactions. So, you know, memory kind of concepts with agents, we'll dig into that. And some more topics on further increasing autonomy, uh, we've, we've coined them meta agents, agents that orchestrate themselves, so we don't have to define, you know, a graph, a workflow, they do all of that for us. We'll dig into that as well. And finally, we will, we will productionize, we will deploy to agent core, and show you how simple that is as well. We will save 5 or 10 minutes at the end for Q&A, so if you have questions, please write them down. Um, we'll save some time at the end for those. And like I mentioned, we'll be available outside afterwards as well, for further discussion. OK, So, over to Shagatai. Let's jump. So starting with the basic agent is simple by design. We designed SDK for developers to develop their agents faster. For this case, we have a shell tool attached to the agent, and we are asking, what can you do? In, in this video, you can see I'm asking the question, and agent starts streaming to the my terminal. This is the default behavioral strands. But it only has a shell tool in it. With Shell, agents can do access the environments they are running on, but, I have a question. Can agents build their own tools? So we will be explaining that in a flash. We call them self-extending agents. So strengths how to load the tools from directory. Imagining like you are building a front-end service, you put a file and framework allows you to load that file as an API. Same applies for agents. So, I have a small code here, I wanna highlight some points. I place a system prompt to the agent, which says, create a tool for yourself. That's the goal. Start using them directly. Because the trend allows hot tool reloading from directory, it can store a file under tools directory, in this case is weather calculator, whatever you want, and to define the way the tool is a tool decorator, which we love and use. We attach the system prompt to our agents, but this is not enough by itself. We enable load tools from directory goes through, because with this way, agents will be watching the directory while you are developing your agent, or agents can write the file and test the tool for us. Which we, we are just gonna watch together. So, let's see. How agents going to create their tools and start immediately using it. One thing to notice here, agent will not going to stop after writing the files. You can see agent created a tools directory which is empty. And using the shell tool, which is the only tool it have to create some files, you can see the calculator handed path password generator, text analyzer, unit converter. This keeps going, but interesting thing is after agent created these tools, it started using immediately. You didn't stop the process. An agent created these files 5 seconds ago. And now process finished. With this example, we showed that agents can create tools during runtime and start using them. This is so critic for us and for the agents, so they can build their abilities on the fly. So, can agents update their own system prompt, Aaron? Yes, so. We just saw an agent that builds its own tools. Now we're going to look at an agent that updates its own system prompt, as well. And we start getting into memory concepts a little bit now. Um, but your agent being able to learn over multiple iterations, it's learning over time. So What we have here is very similar to what, what Shagatai just showed us. Um, we have a system prompt, this time it's, this time it's a little bit different. The system prompt's a little bit different here, right? You see the system prompt is actually coming from an environment variable named system prompt. And what this allows is, um, it's, it's a persistent storage for our system prompt essentially. It's just an environment variable. Um, but our agent, you might notice, not only does it have a shell tool now, it also has an environment tool. This allows the agent to read and write to environment variables. So we can say to our agent here in our system prompt, update your system prompt in every turn. Because it has the environment tool, it will go ahead and do that. And there are a few, few important steps here. So, 3 important steps, we have. Defining our system prompt, which comes from somewhere that can be changed, like an environment variable. If you're deploying um to production, perhaps, this could be any storage, right, it could be um an S3 object, it could be a Dynamo DB record, um, so that it's not tied to the compute environment, right, like an environment variable. So you can store that somewhere else, load it, read it at run time, um, and then you have. Persistent storage for your system prompt for the agent to modify. The second key piece here. Is we're reconstructing the system prompt on every invocation. So we, we first define our agent here, right? It does not have any system prompts. There's a system prompt parameter in strands, there's not one defined here, it has no system prompt, just does whatever the model has been trained to do. We're adding this dynamic prompt here to our environment variable, and that is what we set as the agent prompt. And you'll notice every single time we're in this wild loop here, we're, we're. Reprovisioning the system prompt, we're resetting it basically. So if that environment variable has changed, that will now go through into our agent here. And then finally we, we invoke the agent. So 33 important steps. One is we define somewhere that the agent can read and write to, um, and then we reconstruct the system prompt on every invocation. We can see a small example of this here. So here's the environment tool we give to our agent. Reading from some kind of persistent storage, reconstructing the prompts. Here we're resetting the messages I'll show you in a second, so Shagotai wrote this. Um, you can see here, my name is Shagotai Kelly. He's telling it that we're presenting session aim 426. That's this session here that you're all in. We send that prompt to the agent, so there we can see, it's checking, it's called the environment tool. System prompt was not set. The agent has set the system prompt for us. So now we can go back to the agent here, we see it set the system prompts, we can ask a follow-up question, what is my name? And now it will say. My name is Shagotai, my name is Aaron, but this is Shagggotai. I'm impersonating. So here it's a, it's a very kind of rudimentary form of, of memory, in a sense, right? And we'll, we'll talk about different mechanisms for memory. This is a kind of rudimentary form to, to introduce the concept, so. So now we have an agent that can change its system prompt as well. And what we did here is you see how we cleared the agent messages. This is the conversation history of the agent, all the interactions we've had with the agent, all of, um, you know, the decisions the agent has made, the tools it's called, the results of those tools. Those are all in that agent.messages uh property. We're cleaning them up in this example to show that, um, the knowledge is not coming from the previous message I just sent, right, I said, my name is Shagatai Kelly. That goes into the conversation history, we're wiping that out to prove that it's actually getting that information from the system prompt, not the messages, which are now empty. So just an example of how that can apply to, to, to system prompts as well. So now we have Very basic agent, shell, environment, tool. It's created some of its own tools in that tool directory. We've shown that it can modify its own system prompts. And I'll see to Shagatai can talk about what's going on. Behind the scenes. So Now we started with agent with one tool, shell tool, and we gave additional environment tool. Shell tool can also update environment variables, but, Let's talk about 3 steps Aaron mentioned. Dynamic prompt construction is the first step. So your agent system prompt needs to be allowing a dynamic part. Not everything in your system prompt needs to be dynamic, but there should be a portion needs to be allowing your agents to get the latest version of that system prompt. It could be a Dynamo DB, it could be additional external resource you are getting your system prompt, but you need that portion. Second, self-modification. Agents need agency to be able to update that portion for the future users. The third piece is persistence. You need to persist that version of system prompt in somewhere. So this is crucial for the agents and the sub-agents your agent will create so they can keep the context going. We will be talking about agents will create their subagents. Unless you store this version of system prompt, your sub-agents will not going to be aware of that changes. So what if we ask agent a question today and expect that agent, that piece of context available for the agent tomorrow? I'm leaving to Aaron to talk about it. Yeah. Here you go. Yeah, that's good. Um, so yeah, if I'm just gonna jump back a second as well. So this loop that we have here for the, this is again about the prompt, but it, it applies to everything really. Everything to do with these self-evolving agents. Um, it's this 3 step loop, right? You have the, the retrieval of some information. Right, this first step, the dynamic prompt construction. We have the second step where the, the autonomy kind of comes in, the agent is able to modify its behavior in real time, maybe generating tools, changing system prompts, some other mechanisms that we'll talk about in a second. But that self-modification is, is what really enables the autonomy, right? It's not a, it's not a static definition. Our engineers, our scientists have not defined, you know, a big heavy graph, with all these edge case handling, all this error handling. You know, 50,000 different conditions that we, we have no hope of writing and covering all of them in a, in a good way. So we're offloading some of the, the engineering and science burden to some of the models that are now, uh, capable of this orchestration, especially some of the more state of the art models, um, you know, Claude 4.5, GPT 5, those kind of models are able to orchestrate themselves, essentially. And then finally we, we persist that, right? So, for example, when it's writing tools, it's writing to a file system. Could also be, I don't know, pushing them to GitHub maybe, um, for the modification of the system prompt here for memory. Um, it's again persisting that, right? We saw the example with an environment variable. As Shagtai mentioned, it could be, you know, Dynamo DB S3, somewhere to persist that. But these three steps are critical. So dynamic retrieval, modification, then persisting those modifications. And that's a loop, right, on the next invocation. The modifications that we just persisted are used, so it's continually modifying and evolving itself through every interaction. Those interactions don't have to come from humans. Um, you could have an agent that is triggering self-modification on schedule. Um, it could be running, triggered by events, perhaps. Maybe you even have a actor-critic kind of pattern where you kind of have an agent with uh another agent, kind of battling it out with each other, correcting each other in an actor-critic kind of pattern. So to take that a step further. I mentioned towards the beginning. Meta agent concepts, as we've called it, so, um. We spoke about memory through system prompts. A few more methods for memory as well, perhaps this is a, a more familiar method of memory using a vector store. So in this sample, we have a 3 step process. We have First of all, Searching for relevant conversations and past contexts, so if, if, you know, that previous slide we saw. Dynamic construction, dynamic retrieval. That's what's going on here, in that first step. We're retrieving information from a knowledge base, in this case, Bedrock knowledge base, could easily be any vector store, Mem zero, agent core memory, S3 vectors, whatever. Uh, we're retrieving that information and putting it into the agent's context. So that's that first step on the loop that we saw, the, the retrieval of, uh, modified information, things that the agent has modified, memories perhaps. Step 2, the agent responds, so we invoke the agent, you can see here we have that result line. We're just invoking the agent. Because we previously called the retrieve line there. The agent has those memories from the Bedrock knowledge base, pre-filled into its memory. We're doing a search on the knowledge base there, you can see with the user input. So if I type, you know, what is my name? Our agent will call the retrieve tool that we've added here, which links to Bedrock knowledge bases. It will pass through my query, what is my name? Retrieve some documents that we've stored in our vector store, our memories, what my name is. Um, and those are pre-filled into the agent's context, which, as we saw earlier, is that agent.messages, property. So by doing agent.tool. retrieve, I'm pre-filling the agent.messages, context, essentially, I'm, I'm pre-filling the context window, pretty much. With strands, when I, when I call that retrieve tool in this manner, with a direct tool call, agent.tool, I can just, you know, programmatically call the tool there. It's pre-filling the agent's context with the results from that tool. So I can now have a workflow, as we see here, that's pre-filling the agent's context with my memories, just with that simple agent.tool. retrieve line. We invoked the agent on the resort line, and then the final step, as we spoke about on the behind the scenes. Is the persistence The last, last, uh, last line here again, we have a direct tool call. This time it's not the retrieve tool, it's a store in KB tool. You can guess what that does, it stores in a knowledge base. And you can see the example documents we have here at the bottom. The title of that document is Conversation. History, and you see the content here is what the user has asked our agent, and what the agent responded with. So, for every interaction with this agent, we are retrieving information from Bedrock knowledge base, pre-filling the agent's context, running the agent, and then storing the results in that same knowledge base. So when it runs again, the agent knows what we just did in the previous interaction. It's all stored in that knowledge base. There are, you know, many different options for what you want to store in the database, uh, in the vector store, right? Maybe you want to store every interaction like we have here. You can also give the agent those tools, right? You see here, we have, um, the shell, the retrieve, the storing KB tool. So not only are we calling them directly in this workflow to prefill our agent context and enable continuous learning through, through memory, with the knowledge base. We're also able to have our agent call it those dynamically, so we do not have to program it even. We could, um, you know, you could ask the agent a question, er, what did I do last Sunday? Right, and it will, it will dynamically call the retrieve tool, um, and respond with that information. So that the agent is also able to, at run time, dynamically call that tool, as well as me manually calling it. So yeah, here you have that retrieve, and here we have the store at the very end. Can we ask questions? Sure, yeah, sure, please which permissions are being used for the tool at the end, right? So you need IM credentials or something like that to access the knowledge bases. Where does it take that from? So retrieve tool using bedrack knowledge bases and your agent runtime needs to have that IM role. If you have that, that's just gonna work. But if you don't have that bedrock knowledge base ready, I will suggest to check Bedrock agent core memory, which is going to provision the memory for you. But it's, yeah, it's essentially, this is one of strands comes with a suite of tools, we have that strands tools at the top there where you see we're importing shell and retrieve. It comes with, you know, a, a suite of tools um that you can use. The retrieve tool here is for retrieving information from bedrock knowledge base. We have other. Tools as well, right, um, agent core memory, as, as Shagtai mentioned, you could use S3 vectors. You could use some of our partners like Mem Zero. We provide tools for that as well. There are MCP servers for memory, and each of those tools, um, has their own authentication requirements, right, for, for Bedrock here it's, it's. AWS credentials, right, so as Shekattai mentioned, it could be an IA roll, it could be, um, you know, you type AWS configure in your terminal, set up your access key, secret key, that, that all works. Can I ask a question here? Yes, uh, so you say it has history vectors, so. agent can retrieve or look into money. Yes. Yeah, a couple of ways you could implement that. We could have, you know, we could have multiple retrieve tools. 11 that talks to our S3 vectors, one that talks to bedrock knowledge base as we see here. We maybe we have two bedrock knowledge bases. Um, so a couple of ways you could do that, you could provide, you could either provide multiple tools, right? One per knowledge base, and your agent can dynamically select which to retrieve from. Another alternative you could do is you could actually bundle all of those, I think, into one tool, just like a retrieve memory tool, right? We have the retrieve tool here. In that retrieve tool, it's, it's very simple. It's all open source, you can go look at the code on, on our GitHub repo. Um, it's a very simple tool. You're able to modify that however you like, right? Um, you could have it retrieved from S3, then retrieve from Bedrock as well, and combine the results into one. So a couple of ways to do that, it depends how you want to, um, surface it to your model. It could be multiple tools or one that combines. Yeah. OK. So we can see a short example of this running now. So again, Shagotai made this one. Do you know Shagatai Kelly? And it's responding information about things that Shagggotai has worked on, because Shagatai uses this agent when he's working, so we, we, we know what he's working on here, and it's given us a nice little uh bio about our good friend Shagotai here. And this is your typical memory kind of integration, right? These are documents in a knowledge base, our agent is able to use them. Simple as that, really. OK, so I'll hand over to Shakarai again to talk about further increasing autonomy and agency with meta agents. So, so far, if you are following along, this is the place we will be discussing about how we can increase agency. Because we gave an agency so far to update their system prompt, their own tools, learn from every turn, but we didn't talk about. The steps, which we will be explaining here, the building their own tools is one level of agency we gave them. The second level is creating agents dynamically. The main agent you are running can create their own sub-agents with custom system prompt, custom set of tools, and they can be a sync. Your main agent can trigger one, leave it running, collect the results later. It could be a syn, trigger the agent, wait for the result, and get the results. But we have additional agency we're not going to touch today. Agents can update their own Python code and continue running, which is possible. That that second point I think is particularly interesting, the creating agents dynamically. We've, we've seen, you know, research papers from, from Amazon and, and Anthropic where, um, we have a research agent that Anthropic have published, for example. Um, there are predefined agents in there, right? So we're, we're taking that a step further. We're, we're saying to our orchestrator agent, our main agent that we're talking to. Um, we give it a tool that allows it to create another agent, a sub-agent, dynamically. And none of our engineers, none of our scientists have defined what that agent is. We haven't given it a system prompt, we haven't given it tools. We're able to dynamically create them, so our model itself is deciding how it needs to break down the task. Um, what sub agents might be ideal, right? It's like if you're managing a project, right, you might know what kind of skills you need to work on that project. That's what the model is doing here, it's figuring out what skills it needs. It knows what tools are available for the agents, and it can dynamically create them. And again, each of those sub agents could also be um self evolving, perhaps. Maybe it's a great time to give an example for this, to imagine your agent having a tool named Use agent. Right? And that agent tool will accept system prompt, tools, model provider, and model settings as parameter. So let's say that. In this example, I attached a couple of tools to the agent shell, us agent, think, swarm, graph, and journal. So let's talk about it before we start running the video. Shell tool we all know, it's perfect, it's access to the environment. Use agents have ability to create a new sub-agent, which is another agent interface. You invoke a new brand new agent, which accepts system prompt tools, model settings that can use a different model provider, could be Anthropic, could be Google, could be Bedrock, any model provider you would love. And accept additional tools because your main agent in this case shall use agent thinks swarm graph maybe your sub agent don't need that in the first place so. You can see there's a tink to all. In other agents you can see ultrathin, deeper tinker, whatever you can, you can count that. The idea behind the think tool is we are calling one agent and the result of that agent passed through the second agent and then the other agent until a number of times, like recursive thinking exactly. So we are allowing agents to create their instances programmatically a number of times to deeper that idea throughout the time. We have a swarm tool which is a shared context between agent. Aaron have a great example for this. We collaborated on this presentation. Do you want to take some time to explain? Yeah, um, so yeah, Shakartai's just talking about some of the meta tools that we have, right? So, so use agent, think, swarm, graph. Um, swarm you can think of as like a self-organizing team of agents, basically. So, uh the, the example Shaggattai was giving, him and I have paired, uh, brainstormed, sat together, you know, pens, paper, PowerPoint, ID code to make this presentation. Um, I wasn't telling Shagggotai what to do. He wasn't telling me what to do. We were just two pair collaborating. It's kind of what a swarm is, right? There's no lead agent, that's just a collaboration of agents, and they figure it out among themselves. Super interesting concepts, um, you see similar things in frameworks like crew AI as well. So thank you for that, uh, example because today we were talking about that and we have a great diagram in a couple seconds. So we have a graph tool as well. This is for conditional edging. You can define a group of agents are directed to each other. And the last pieces of this is journal. Journal is using a file system to purchase data so our shared agents can have a backbone of context. It's more of an agent scratch pad. It could be like a to do list. Exactly, exactly. If your agent starts working on something, they are not losing the context about each other. They will be all aware on the same context. So let's see in this example, I'm saying you use agent tool, that's it. I'm not prompting. I'm not doing a lot of prompting. It's asking me what to do. And I'm saying, testing usage and tool, I'm not giving any hint to the model. It says, OK, I'm gonna use us agent a couple times. I'm gonna calculate 2 + 2. This is the best thing to do, right? And main agent invokes two different subagents to do two different things. But so far what we achieved with this ability, we expanded our context length because our main agent had 1 million context length in this example. But every sub-agent agent created have a brand new context that can spend 1 more million, 1 more million, and they can call each other to spend more. So here is the diagram we have been all trying to put together. In this diagram we have re sub agent tools called 4 times in parallel. Agent invokes those tools in parallel and not waits to see the results. It is fire and forget pattern. The main agent is going to be idle after this step. On the first row, we have one agent uses another agent. That you don't have 1 million contacts thanked again. And that creates two more doors in parallel, but it's going to wait for the results. And this result is going to be get back propagated this agent, so this agent is going to store the result in a journal. So this row going to spend 1234 million tokens in the worst scenario if you keep everything running for a long time, you expect at least at at max 4 million tokens you are looking at. So the second row is the thing we just mentioned like recursively calling the agents and share the context across the time. Which is, which is this? I, I liken the think tool to. Um, you know, if I'm doing research on a topic, um, you know, I'll, I'll do my initial research, write it all up, and then I kind of do the same again, right? I like refine my understanding maybe a month later, right? I'm revisiting what I just did. It's kind of what the think tool sort of does. It's, you invoke the agent, it does something. And you kind of get it to do it again, right? It's like think deeper, think deeper. And in this example, we have it thinking 10 times to show the case, right? Um, but, you know, maybe you only want it to revise 23 times. So yeah, go ahead. Yes. You asked if they're the journal tool. What is the difference between, yeah, how is Jo and Journal is using a file system here, so there is no extra dependency. Let me go back one second for that. I think I'm unable to go back. I got you. Thank you so much. Can you go one more back? Yes, so maybe, OK, I'm just handling it. So this journal tool only using the file system, so there is no memory or any other thing happens here. If you plug the Wi Fi off, this agent's still gonna work because of that. Um, yeah, but they will be eventually consistent. I think that your nice difference as well, I guess, is, um, and, and it depends, right? There are many ways to implement memory. Um, what we saw previously with Bedrock knowledge bases, S3 vector, agent core memory, Mem o, those are, those are vector stores, right? It's, it's, they're based on embeddings, you can do semantic retrieval. Um, the journal tool here is markdown file based. It's another way to store memories, right? You can have markdown files of memories. Um, it, it limits your ability to do semantic search, right? You can't, uh, it's not embedded at that point. So yeah, a couple of different ways to implement sort of a a a a a a mechanism for your agent to kind of track progress, right, which is, which is a form of memory, yeah. And in semantic search, there's a delay once you start that context and how that's going to be propagated back to you. In this case, time is crucial for us because one another sub isn't going to be triggered immediately. You may not going to be able to wait that ingestion finishes. Yeah, we've um we've experimented a lot as well with things like, you know, Chroma DB um to run like a local vector store as well, super, super fast, like a couple of milliseconds to run. Um, which, which helps with some of the latency stuff that Chagatai mentioned, but then you have the trade-off of, well, now it's not distributed, right? If the compute that, that is running on disappears, that's gone, right? So it's, yeah, a lot of different trade-offs. So we were mentioning the swarm by context sharing each other in the swarm pattern, we have a small, small, small circles have intersection between. That's the context we are trying to explain. They have a shared context about each other and while they are working, they will be keeping their contacts in their boundaries in in case of in this this shared context. Can be implemented in many ways. In, in our example Swarm tool we've implemented it in memory. So there's this context is being, you know, agent one updates the context, agent 2 updates the context. Well now agent 1 and 2 are able to both get both of those contexts, right? So they, they're able to almost communicate with the shared context. It's as Shagatai and I will. Working on this presentation, right? We, um, we wrote down a bunch of notes, right? This is our, this is our shared context in memory. Same, same idea. It doesn't have to be in memory either, right? The shared context could be any sort of storage, right? It could be, could be Dynamo DB whatever you want as well there. So after swarm, we have a graph on the last row. You can see these are the directed systems. You can actually make them loop again. They can stop when whenever the task is finished, but there's an interesting thing we have below here. The graph node, one of the graph nodes using use agent. What is that? Yeah, so you're able to compose these like primitives, these orchestration primitives, a graph, a swarm, use agent, they're all able to be mixed and matched. You can have a graph of graphs, you can have a swarm of graphs, you name it. They, they, they're all able to mix and match and compose with one another as well. When agent is running, it can change the way it's running. We are leaning towards the model driven approach. The models get better. They are getting better to create this kind of such systems never been visible or explainable for us. This is something beyond static than our workflows. We have been building all. Shall we? Yes. Intelligence is ability to adapt to change. The reason we are building software to help people, it's not supposed to be static. And the way to do that might be agents, might be robots. We will all see it together in the future, I believe. Do you want something to? No, OK, OK. Let's deploy. Yes, so. We have a video here of our code. We changed the way we code previously. We let the agent update their system prompt, build their tools, right? But we need to deploy this to somewhere else on cloud, because I don't want my MacBook keep open for forever, right? So you can deploy your agent to any runtime with strength, but in this case we are going to deploy the agent car. To deployed Agent CR, 3 different CLI commands, and 1 decorator you would remember. The decorator comes from Bedrock agent Core Python package, and I am importing the memory primitives from Bedrock, so my agent will be able to learn from every interaction after we deploy. In our local, we were using Bedrock knowledge bases. When we are deploying the Agent Core, I switched to Bedrack Agent core memory. And I attach a session manager to my agent, so every session, every turn is going to be part of my context of my agent. So I can ask a question as a blocking, fire and forget, and streaming way. These are the three different options when you're invoking an agent, as we mentioned, like a creative sub agent, that's fire and forget. So I'm using Agent Core launch CLI command to deploy my agent to agent core. This is provisioning the memory and knowledge base for me. It's immediately useful after this line of common finishes. And I'm running Agent core status to make sure my agent is deployed, and I'm using Agent Core invoke to invoke the agent. Because we see how fast that provision does well, so I guess the, the point we're trying to get across here. We've built this great, you know, self-evolving agent, built its own tools, can modify its own system prompt, it has memory, it learns from our interactions. Now we need to ship it somewhere so that, you know, we can turn it into a business maybe, start making some money perhaps, um. With a simple 1 decorator, you just do at app. entry point. At the top of a function, and now it's an Agent Core agent. Right, now you can deploy it to Agent Corp. And as Shaggartai showed us, we can run Agent Corp launch. And that does everything for us, and as we saw, that was a real-time video, not sped up, that was provisioned everything. And now it's infinitely scalable, running on AWS um in a servous manner, paid per request. So, so far we deployed our agent while I was talking. That might be fast from the vision, but let's, let's try and to, I think we have some resources. If I, if we jump back just one second, yeah. Um, I wanted to show here, let me play this. Do you want the shower code? I'm going the wrong way. So if I play this again. OK. So yeah, 2 SDKs, we have strands. Bedrock Agent Core has another SDK, um, Python, TypeScript, you name it. We just launched Strands and TypeScript today, if you're interested in that kind of thing. But here you see that entry point, right, it's a decorator, we just have app. entry point. Once we have that, we can just run those CLI commands. Agent. agent core launch, agent core invoke. We've also just upgraded the Agent Core CLI this week. Uh, encourage you to go and play with it. You can also run Agent Core Dev. Does the same thing as Agent Core launch, but locally, so it, it puts your agent into a container that matches the same input output interfaces, as when you have had it deployed, um, but just running locally. If you, if you've built lambda and you're using SAM CLI, similar kind of concept where we just containerize your application here, um, with like a local dummy version of Agent Core runtime essentially, but it, it works exactly the same way as when you deploy it as well. So so far we have been explaining advanced topics. We expected agents to build their system prompts, their tools, and maybe it's hard to keep up with everything we explained today. I would suggest to check it out our GitH resources on our GitH organization named Strend Agents.com as well. So let us know what we can do better and let's build together. Yeah, so I'm leaving a couple more seconds to take some photos. Yeah, and you see there's a, there's a few links here like I mentioned, we just, we just launched TypeScript as well for strands. We just launched an evaluation SDK, uh, an evaluation SDK for strands. Yeah, we launched a bunch of features. There's um bidirectional voice agents we just launched today as well, so you can now talk to an agent in real time. It can understand intonation, rhythm. Um, you know, if you're, if you're stressed or happy, that kind of stuff. Um, and we also introduced a new concept called agent steering. So if you go to the strands agents.com website, um, that third QR code there on the right, you'll see on the navigation menu in our user guide, um, a section called experimental. If you go into there, you'll see that some of the new stuff we've just launched, like the bidirectional voice agents, um, agent steering as well, which is super interesting. It's more of that actor critic kind of pattern. Um, we're applying a lot of that stuff into Quiro as well, uh, if you're familiar, um, Quiro is built on top of strands, and everything we're showing you here today powers Quiro as well. So the best way to predict the future. I invented. Here we are at Rainvent. I hope you are enjoying this, uh, uh, meeting so far, and thank you for joining us. We have a Q&A session for 15 minutes. We have a great time to chat and explain all the questions you have, and yeah, we have some resources as well. We will be keeping this open and we are here to answer your questions. Yeah, go ahead. So this, this is all like. A bit abstract. Do you, do you have like a practical use case that you think is exemplified by these tools? Yes, we have beyond that. We have beyond examples then you can just install right now on your computer and test these all features. And beyond, we have an agent. It can update their own messages. It can remove the tools from the list of tools and everything. We can talk after this. I can share with you directly, or you can visit the strand agent agent builder. You can give it a try by yourself. Some of the, some, some use cases we've seen, um, research agents, very beneficial here. I think we see where, where you're looking for. I personally find it useful for two things. One is where you're looking for emergent behavior. Um, and the second is where the problem space is so big that it's infeasible for me to engineer it, right, as an engineer. Um, say for example, developing something like Quiro, right, uh, or, or, or QCLI or something, or clawed code, right? Um, those tools being able to extend themselves with, you know, um, LLM intelligence, we found to be very useful. For example, um, now Kiera is able to write its own tools, right, if I say. Uh, you know, where is the space station, right, how far away is the space station from me? There's no tool for that, right, but the agent is able to build its own now. And, you know, I work at, um, maybe I work at NASA, right? Um, now Kiro is, is able to, to use, you know, my internal APIs or whatever to find where the space station is. So we, we've, I, I personally find it useful for two things, emergent behaviors, and one where the, the scope is so broad that it's like I'd need a team, a big expensive team of people to engineer that. That's where we found it useful. We have 2 more questions, 3 more. It seems to talk about. Um, Where we not want. that Seems like a huge lot of space, so do you have any? That's a great question. Leaving to my principal engineer. Yeah, so we, um, I just mentioned, we, we just released this, um, steering concept. Um, and you'll find as well we just launched agent core policies, right? And as you mentioned, um, you know, these models are stochastic, right? They're, they're not deterministic, which is beneficial actually. Um, but what we've built with, um, agent core policies, I encourage you to go try it, is it's adding some more of that determinism to these agents. So we have these very powerful dynamic agents. We kind of want to draw a box around them and say this is your boundary. Right, you can evolve and self-modify, but only within these constraints. And those constraints, um, can be defined in many different ways, right? You can try to write a bunch of if if statements. But then we're back to the same problem where I have to engineer this huge scope, right, I can build a state machine to validate everything the agent's doing, but it's unwieldy. So with, with agent core policies, um, encourage you to try them. You're able to type in natural language, and it will generate um a policy for you. Very similar to an IA policy. It's built on the same language, it uses CEDA, our open source library, for this kind of thing, for policy language. And you're able to define, deterministically, what the bounds are of that agent, right, it, it, it, those bounds can be literally anything. And the policy is able to be formally proven. Um, using some of our automated reasoning techniques, um, you, you don't need to deeply understand automated reasoning, right? We have a natural language input there, you write natural language, it will generate the policy for you. That policy becomes kind of the source of truth for the bounds of the agent. Um, but yeah, I definitely encourage you to try that. The critic part is policies are outside of the agents. They are not inside of the code base, so even they evolve, there is nothing to do with that. Please, I was at a session before talking about the design of, uh, agent, and one of the things they mentioned was that. Container let's call it um was for the session. So if I write these tools they're not persistent and I thought about how to That's a perfect question. You can store your tools in an S3 bucket and let other agents to download and use it, but there are many ways to do that. I'm taking, is there just like a check box to an agent court to say share this. That's a perfect idea. I, I will be working with the team to make it possible, like a persistent environment. We will be, we will be, we will be taking that feedback very care, yeah, there's a couple of things on Agent Core, um. The run time, it's, it's a long running container, right? You can, you can have a long running autonomous agent there, but you're right, it's a single piece of compute, right? It's a container essentially under the hood, it's a container. Yeah, so it's ephemeral, right? It's a container. It might be long lived, but it's, it is ephemeral ultimately. Um, so as Shakartai mentioned, it's, it's. It's back to that topic, right? When, when these agents are evolving themselves or modifying themselves, we need to persist it somewhere. So it could be S3, it could be wherever, right? And in the example we gave, if you, um, I'll, I'll share the link again in a moment to that repository. If you clone that repository, if you change the system prompt to um write your tools to S3, and if you give it the strands use AWS tool, it will write those tools to S3. As an example, I think the Docker container needs to be persistent in a way we can just reuse the same environment as is, but that's definitely we will reinitializing like the whole container state, like all of the memory is something we're looking into, yeah. There are ways to export the Python on time, but we can talk later. So an agents to be an agent. I We are pitching the model-driven approach, so we are not determining what to do in when in when so we are letting the agent have those tools and models are deciding to use it or not. I hope this answers right now. So what are the criteria someone has to decide whether should I be with a new agent or should I be. It's completely based on the request you're asking. My, my manager was asking, Should I buy an AWS stock today? Do we need to create a new tool for that? Definitely not, but agent created 5 different subagents to make the market research, check the tag of documentation, right? So it's, it's basically depends on the question. I hope this answers. I have a gentleman here. Can you talk a little bit about, uh, production deployment of these agents because I would not use the agent CLI. Yes, yeah, yes, very good point. Um, so we built the Agent Core CLI for developers, right? Uh, same reason we, we built strands actually. Developer experience first. People are not going to use Agent Core if it's not easy to use. We understand that. Um, so we've, we've built Agent Core, the CLI for that purpose, right? I'm a developer. I want to quickly deploy it, um, and we've actually been surprised, um, there are a number of companies that do deploy to production that way, which is interesting. Um, but, you know, on the, on the more, um, perhaps traditional side of deploying this to production, Agent Core has support for Terraform, CDK, right, you deploy it through your typical CICD kind of thing, uh, to production as well. Uh, gentlemen gentlemen, I will thank you. Yeah, I, I was just wondering like, I mean, have you tried using like a self-evolving system on real world. Can, can I repeat the question again? Oh yeah, have you tried using the self-evolving. Have we tried using the self-evolving agents on real world? Yes, I can give you one example. Uh, as a research engineer, I built a research agent for myself, running on GitHub every day, on schedule, keeps evolving on system prompt and searching in archive. For finding new ideas for me because a lot of archive paper are releasing every day and I cannot keep up with what I did is for myself around 2 years ago deploying my strength agents took it up, but we were internally using the strength back in the time so it keeps searching Dasiv. Using the cross ref to compare the papers and sends me a Slack message every day. I just read the post every day and itself of agent build those archive tools, cross ref tools, and whatever you can imagine from that point. I have another repository similar to the research agent, but it's for building tools. It's building the tools for protocols like a TCP WebSocket. And more, I didn't want to spend my time to keep building tools. I just built an autonomous agent to, to build that for me. I, I've found it useful for, like, for example, we have Kiro CLI, right, formerly known as QCLI. Um, we have an internal version of that that implements, um, a lot of this kind of style of, of agents, right, a self-evolving agent, and it's, it's just my personal assistant, essentially. I don't want to have to code it and prompt it on how to do research and coding. And testing, and X, and planning my weekend, right, every, it's basically my personal assistant, and I don't want to define all of those because I, there's like a 1000 things that I do. By enabling it to evolve itself, I don't have to program everything. I'm relying on the model to define that. Um, obviously, I'm doing human in the loop there, right? I'm reviewing what it's doing, I'm, I'm looking at the tools it's generating, that kind of thing. Um, and obviously, you know, you can evaluate these agents as well, right? Every time they make a modification, uh, you're able to run evaluations. Like I say, we just launched strands evals, agent core evals. That's 11 method there as well. That was a gentleman. First of all, I think super cool in the novel, so congratulations to you guys who told this. Uh, it's more of a common slash question. The entire immutability artifact deployment paradigm goes out the window, right? Like whatever you tested or not doesn't matter because whatever it gets deployed, who knows what that is, right? So I, I, I kind of wanna get your thoughts on that. Like testing this becomes effectively impossible, right? You don't know. No, I wouldn't say it's impossible. I would say, um. I think the, the points that you run those tests changes, right? Like instead of, instead of everything happening, and the scope of those tests, test changes, right, like my deployment tests here would essentially make sure it deploys. Um, and you know, whatever functionality it had previously still works. As it's self evolving, I would then test it again, for example. Like every modification it's making, test it again, pretty much. And that's the, that's I think the human in the loop portion. I have a great answer for how we can actually evaluate these such systems. When we gave an agency to the people, not the agents, just skip, skip the agents for one second. We gave an agency to a people. We need another people, a same agency to compare. Why we are not thinking for the agents the same idea, you build one autonomous agent to do something, build a similar one to test it, because these are the systems we use to actually improve one of the other, right? That that's the place that such systems can evolve together instead of we trying to predict what it might happen. We will see what will happen. There are, there are definitely, um, certain use cases it's, it's appropriate for and some that it's not for sure. I think it's inherently stoic, you know, like to your point about humans, but we don't always test the humans. I think the humans build trust by time period of time, right? So presumably that's the approach you're taking more philosophical, I suppose, right? Like has done things for me. Past 9 times. Mhm. Yeah, it's kind of like these agents are, um, almost like learning by interacting with the world, right? They're, they're living in the world with us doing things and, and like they, they build a tool that pulls an API, they test it, it fails, they modify the tool, now it works, right? At some point in that chain, you probably want a human to review. But where you do that, it could be, you know, immediately at every step. At the end, after a month, right, you, you have a lot of options there, but definitely human in the loop still for certain use cases for some of the other use cases like these personal assistant tools that we've built, um, it, I actually want it to just go and do everything, right, so. But you could have it right specs and say, you know, I wanna call another agent that builds this, another agent that checks another agent does security checks, right? Have a chain of stuff that all of them have to sign off before the tools added to this shared direct. Absolutely, definitely, right? You can do, you can do a lot of the automation of what we do today, yeah. Definitely. Really simple questions are the type script and Python SDKs are they? Python SDK helped us to build Type typeScript SDK and we will be building the rest of the SDKs altogether. We initially built the Python SDK for internal reasons, like 3 years ago, and it's became something publicly available today. And lots of our customers ask us TypeScript to have, and we experimental version, we launched the experimental version today of TypeScript. And that SDK built by the Python SDK running on GitHub. Yeah, if you go check out the type script repo on uh GitHuborg, look at the pull request, look at the issues. The agent built that SDK, and you can see we didn't just YOLO, right? Um, we, it's human in the loop. So if you, if you look through there, you can see our engineers working with the agent to build it, and it saved us a significant amount of time. Um, but the, the idea with, with the different languages and we're also talking about more languages, um. The idea is that they are at feature parity today. TypeScript is uh V0, kind of preview release. Python has been V1 for, for a couple of months now, but the idea is we have feature parity. Yeah, they use JSI. Today they're developed separately. Yeah, so that we can make the most use out of um each language features and the ecosystem, yeah, but it's something we have spoken about at length, yeah. So before we left, I will take one more question, right, and we would love to give you, uh, give you some feedback after this session. We would love to see your ideas, and this is critical for us to evaluate, are we done in a good way this session. Please check it out in your AWS events app. So give us some votes. We have one more question. I, I was kind of wondering if you best practices regarding the memory because it seems like collisions duplication. I mean, it just So semantic indexes are basically collapsing the data in a 3D environment, right? And that environment might be having the same context over and over and over again in that regard. So the way to store the context and how to retrieve it is basically the engineering problem we are all going to be solving. If you are storing the conversation stor in the same place, you may need another knowledge base to attach additional context so they're not never going to be overlapping each other, right? This kind of engineering decisions we are all going to make from further as an agent builder persona. OK. Thank you everybody. Thank you so much.