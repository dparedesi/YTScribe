---
video_id: 5oAbpxdZtXs
video_url: https://www.youtube.com/watch?v=5oAbpxdZtXs
is_generated: False
is_translatable: True
summary: "In this session, Jonathan from Fortinet and Amar from Adobe discuss the evolving security landscape in the age of AI. They highlight that while traditional security fundamentals like defense-in-depth and IAM remain critical, AI introduces a new, expanded attack surface. This includes risks like model poisoning, unauthorized access via privileged escalation, malicious prompts, and data leaks. They emphasize that the distinction between \"traditional\" applications and AI applications is blurring, as legacy tools (like Photoshop) now integrate AI features.\n\nAmar categorizes the current state of AI adoption into two camps: one with fragile, POC-level applications lacking proper governance and observability, and another (where Adobe aims to be) that emphasizes rigorous governance, sequestered agents, and continuous logging. They discuss the importance of \"data maturity\" and hygiene, advocating for a medallion model where raw data is refined and classified before being used in runtime environments.\n\nA key theme is shifting the security burden away from developers. Instead of handling generic best practices documents to developers, Amar suggests using AI agents to provide contextual, step-by-step guidance for building secure, compliant applications (e.g., PCI compliance). They also discuss the need for \"AI-native\" security tools, such as using AI for anomaly detection in WAF logs to reduce noise and \"LLM firewalls\" to inspect and filter prompts and responses, effectively re-establishing trust boundaries.\n\nThe session concludes with a call to action for organizations to orient their culture towards being AI-native, investing in workforce training, and partnering effectively with cloud providers and vendors to navigate this complex ecosystem."
keywords: AI Security, LLM Firewall, Data Hygiene, Adobe, Fortinet
---

Alright, well thank you all for attending the session. I'm gonna have to get used to the fact that you're all wearing headphones and I'm not, so I'll just trust that you can hear and are, are interested, um, so, uh, we are here to talk about something I think that is really evolved tremendously over the last year is the security implications specific to how AI, um. I, you know, fundamentally shifting what we do in security. And so this is gonna be a conversational uh piece where, uh, my friend Amar here and I are going to talk about things that we're seeing, some of the best practices that, uh, we have been looking to implement or uh maybe Adobe has implemented. But, uh, just for, for a quick introduction, um, this is a session where, uh, both Fortine and, uh, uh, Adobe, um, have, you know, kind of collaborated. Omar and I have done this, uh, a few times and, you know, not in this topic but various topics, and so I love what how he frames things, um, but, uh, you know, Omar, thank you for being, uh, with me. I, I always appreciate, you know, the opportunity to do these things with you. Um, maybe quickly introduce yourself. Yeah, absolutely, um, really happy to be here. Um, reinvent is one of my favorite, um, conferences as a an AWS community builder, and before we start, I just wanna understand the room and see who are, get to know the audience more, um, just raise your hand. Who here is responsible for securing AI workloads? Oh, that's a pretty big number. Who is here is being asked to. Use AI to do something, whether it's security operations. OK, all right, so we've got the right audience. One last question, who is here is has not used AI. Yet planning on, OK, we have, we have a couple, um, I did not, I expected maybe 5, so 2 is not. Yeah, um, to off, so yeah, I'm excited to be here to talk about the implications. I think from 2019 maybe to 2030. is going to go down in history as a very Um, big moment in human history, right? It's similar to discovering electricity or hospitals, fire, like so many, the internet, absolutely, um. Those changes are coming at us very fast. Like sometimes my team deploys something that is AI, and as a security person I'm not aware why? Because they wanna keep up with the market. They wanna move fast. They don't have time for checks. So we're all here to kind of figure it out together and, uh, without further ado, I'm just gonna introduce myself. I'm Amar. I am an AWS uh community builder, and I also. Uh, like look after DevOps, the DevOps team at Adobe, so a little bit of cloud, web applications, and AI security. Um, without me signing up for any of the AI security, I found myself, like many of you guys, uh, Applications has AI in them and now I'm responsible for AI security. Yeah, I think that the scale that Adobe has in AI is what I why I like getting your input. It's like you have literally hundreds of applications that are quite mature. I use these applications. I'm sure many of this audience uses them, um. And so you know for my part, you know we have different perspectives. Amar, uh, is a cons a consumer of security tools. He builds his own security tools. They build AI applications. For my part, I'm responsible at Fortinet for, uh, cloud. SAS and AI engineering and so primarily what I'm doing is not just looking at tools that we consume internally and ensuring our practices, but how customers want to adopt third party security tooling and integrate those into their their pipelines and workflows. And so we have these, you know, I think complimentary views on the world, um, so, you know, one of the things that I think is uh. Very, you know, interesting about this era is how, how we build applications, how those applications are attacked, and how we defend from those attacks has changed so much. Right before we had this environment where it was largely static. You were concerned about the infrastructure, uh, you, you built an application. You were protecting source code. Um, and, uh, it was very deterministic, uh, but now we've added on this additional layer, it's additive, right, so you still have to take care of the old stuff, but now we have a system that, uh, reasons, right, so you don't necessarily always know what the outputs are, um, you know, you know, I imagine your environments are, are very much like this. Yeah, absolutely. So if you, this is not an adobe picture or anything, but if you think about, uh. The applications that you guys use, you know, BDF, anyone here probably used one. Photoshop, maybe you haven't used it, but now all of these established legacy, in my opinion they've been there for a long time, that legacy in a bad way, they've been there for a long time. They have AI in them, so you can edit. Using prompts like prompt driven editing, you can say remove the background. The AI component wasn't there. Everything else has been there hasn't changed is the same so before people. Um, really get concerned about AI, which you should be concerned about. The fundamentals have not changed. Defense in depth has not changed. The way you set up your AWS accounts has not changed. Your VBCs, your IM policies, all of these things still exist, did not go away. In addition to that, you now have to think about, hey, do I have a model that is part of Photoshop or BDF or whatever your product is. Is it built in-house? Is it off the shelf model? Who is able to access that model? Am I calling that model privately? Uh, what if it's a model that we've built, where the data is stored, how is the data curated, cleaned up, uh, encrypted in rest at rest in transit, and all of those. So yeah, this is, this is a typical, um, AI stack now where you have a data pipeline, you have agents probably, uh, which is simply a tool that can make some decisions and call other tools. Backed by an LLM intelligence, so it could invoke um an API. It could get data from a, you know, a data store. It could read Jira, uh, whatever you give it access to. So we'll, we'll talk more about access control and the nitty gritty, but Yeah, simple, this is, yes, this is the infrastructure for, for us we just say we, you know, think this is a just a much larger attack surface, um, that is really perimeterless, truly, um, and, uh, we, we have to think about the old ways that these applications were attacked because they can be chained to and, and we'll, we'll show an example of that, uh, two new styles of attacks, um. Um, that are specific to the AI now the risks and concerns now we, I, I mentioned earlier that these things were largely anecdotal, you know, if you go back maybe a year or 18 months ago, we saw these things happening, but now we literally see this happening on a daily basis. Um, you know, model poisoning, this can happen very quickly or it can happen slowly over time, like the, you know, as if you're boiling the frog, right, so I can start to, uh, create bias or poison that model slowly so I don't trigger anything, uh, and those things are harder to catch, uh, on unauthorized access, you know, this could be privileged escalations we see all the time. Where people are uh creating tools or they are uh that are not well secured and maybe they're using them at the users using them for privilege escalation right so that's a common attack path um malicious prompts you know this is like you guys are focused on protecting prompts all day long, um, and data leak, right? so asking for information from a, uh, an AI tool, an agent, and, and then getting back data that it should not have gotten. Uh, this happens every day. Um, in addition, like, what do you see? These are, these are the ones that I see. Like, what do you see? Or is this consistent? Any, anything you would add? There's this and there's another, um, one which is the AI, uh, layer, meaning this is, I would consider this more of a machine learning. You're building models in-house. These are concerns for you. Um, but as a consumer of AI tooling, there's another, um, segment that we should talk about probably and just a quick, I wanna understand who here are building models in-house and who you are just using AI tooling that you buy. So who, who in the audience is building their own models in-house? Nice. Who is using off the shelf models and other AI tools? Yup, pretty much everyone. So, you are likely trying to get the most out of an AI an LLM if you, if you buy it, for example, um if you're using the APIs. The LLM is not to be trusted because maybe it has limitations, right? Like it was trained. 3 months ago, maybe 2 years ago, and you want to feed it your own uh enterprise data, uh, may maybe your documentation, maybe your workday information, whatever the use case is, um, so your attack surface is. Can your personal, your company data get leaked and who has, who has access to that data, right? Um, you're gonna put that in a vector database. How do you do that correctly? How do you worry about context engineering so your agents are reliable? You're using MCB servers, probably you have, you know, remote MCB servers that which I do not recommend. Building an MCB server is really easy these days where just build your own like your own GitHub MCP for example, and I get like MCB servers make life super easy because your prompt now has a lot of tooling behind it that can be invoked to get a lot of work done. Um, so can I ask you a question on that? So as, as a security provider, we get asked by our customers to provide MCP, um, um, for MCP agents for our security tools to integrate them. Um, you know, do you look at vendor implementations as separate from maybe these other examples that you're talking about, uh, you know, yeah, if there is a trusted vendor, yeah, like there's a difference between a remote open source MCP and a trusted vendor MCP. So for a company like you with a complicated set of APIs that I cannot, I don't know, uh, fully, providing me with an MCB that can help me. Uh, use natural language to interact with the API would be lovely if it's secure, maintained, well done. I can also try to have just enough MCP that I built. Like it doesn't have to be very comprehensive, right? But I would be very hesitant to. Just pulling a remote MCP. Without looking at the code. You know, you integrate that MCP server with your ID and all of a sudden, um, your IDE can actually invoke that MCP and all of a sudden you have an OS injection attack on your laptop. That could harness credentials that could do whatever. So, uh, I'll be very careful with open source remote MCBs because attackers are actually like sprinkling those out there. You know, to lure you to use them, which that's, that's what we're looking for. So it's like you, like, uh, importing rusted library that you don't trust. Absolutely, yeah, in, in, in fact, uh, you know, this has gone from anecdotal to being so well established. I mean, you know, if you're in security, they, you probably understand that OASP is, uh, a top 10 for, uh, LLMs and, and generative AI in addition to their traditional top 10. Um, but these are the ones that we see and that that was a good one, I think, Amar, so, um, you know, let's, uh, let's talk about kind of the camps that we see, and this is, uh, our, our experience internally at Fortinet. We have people in our finance team, uh, you were talking to our CISO the other day, uh, that are literally thinking, OK, I can vibe something out, and I, I've created this application. It's got maybe like it's some kind of agent component in it, um. That is not a mature product. Those we have tons of those proliferating, both, you know, like our, you know, skunkworks and shadow IT efforts, uh, and then, you know, we have our customers who have tremendous numbers of these, but these are, I would call POCs, right? They're very fragile. They're not meant to be in production, uh, you know, when you start getting maybe even sophisticated in your POCs, you have, you know, agent to agent communication, and then you have these unvalidated reasoning loops that. You know, if, if you have a compromise in one or a bias in one, then it propagates and and escalates uh through that reasoning chain. Um, telemetry and observability. You know, we get these AI tools and these agents and nobody has thought about. One, do I, how do I observe the infrastructure on which these services run to make sure it's on trusted infrastructure, uh, and then two, you know, am I, am I auditing, which is very important and, and, and very different from what it used to be, but auditing the, um, uh, the, the response and how the agent or the application comes to reason, um, none of that is happening in a lot of these implementations and so. You know, all of, all of these, you know, things like, you know, no, no recovery and backup processes, no data maturity, um, you know, we see that. Every day proliferate and I think there's a second camp here, uh, and maybe I'll, I'll let you look at some of these and, and, and talk about the lessons that you guys have learned at Adobe because I, I, in my opinion I think you guys have done a lot of things really well, you know, you and I have talked about, um, uh, you know, sequestering agents, not being overly permissive. I think that's, uh, you know, controlling reasoning chains. Uh, continuously logging, in which I, I wanna expand on that, but, um, maybe you'll, you touch on these points for us. Yeah, absolutely. So when we think about building agents, we think about the agent as a user, like a human user, just to make it simple to understand what this agent is. Typically an agent is receiving information from somewhere, which is a prompt. It could be. From external users, internal users, or a system. For example, the agent could be reading geo tickets and based on that is making some decisions. So, where the data, where the instructions to the agent are coming from. Second, what that agent is allowed to access. So how many systems that agent is allowed to access? Does it have just enough access, or does it have access to all your S3 buckets, even though they, you know, it's just easy, you know, to, to just give it access and let it fetch the information from the right places, let it figure that out. And third, where the agent put the information? Why do I think about it like agent security from that perspective? First, the prompt could be malicious. For example, let's assume you have a Zendex, which is where you customers kind of file tickets, and that Zendex replicate the ticket internally to your internal GR system. Meaning if I embed some malicious instructions in that Zendex ticket, it gets replicated to a private trusted Jira system and gets picked up by your agent. So all of a sudden without you kind of analyzing and doing threat modeling, you have gotten bad instructions to the agent without. You know, thinking about it carefully. That instruction could allow the agent to fetch some secrets or whatever, then the attacker may instruct that agent, you know, when they created the Zendex ticket. To create a um a GitHub issue with secrets in your own public GitHub rebos. Because most companies have a private Rebo and maybe a public presence for open source tooling. So the agent does not think anything is, is concerning here and output the malicious content indirectly. All of this is indirect communication with the agent. The agent does not have the full picture. Maybe they are multi-agents, right? You have multiple agents. Agent number 1 is responsible for consuming the data. Agent 2 has access to things, and agent number 3 is um responsible for exporting that data, right? So, no one agent should have. The 3 things, only 2 things the agent should be able to either get data and access systems but never export data externally, or it could, um, just do a combination of, of two of these, never the 3, and an agent with all of these, uh, permissions or uh privileges has excessive agency, which is a thing, right? Um, there are some common sense and some best practices like use an established agent development kit like, for example, the uh Google ADK, um, those come with all the security best practices baked in encryption, authentication, agent cards. The agent should advertise itself, what it does, what it should have access to, to other agents and whatnot. Um, but yeah, these are the things that typically I think about when I'm thinking about agents. Yeah, you, you talked about permissions, um. Zero trust is something that we, we think about. We, we think about treating the agents themselves as if they're users, uh, in, in providing access to services and tools only when they need those, uh, that access, um, and so, uh, you know, essentially what is the, the, the posture status of the, the work, the, the, the virtual machine or work, uh, hardware that that system is running on. Does the OS have vulnerabilities, uh, you know, does it, uh, has it acted, uh, you know, in a malicious way, uh, previously, um, and so we look at all of that and say, OK, this application can't access this one other application. And by the way, we wanna check the, the remote end to make sure that application doesn't also have any vulnerabilities. So we're monitoring bidirectional communication between those agents or services and or users. Um, and, uh, I, I'd be curious, has anybody thought about AI and, and zero trust in that way? And, no, no, a little bit? OK. Um. But, um, yeah, I, I, I think you know it's very clear to me that there are two different camps, uh, you know, we wanna get to camp two, I still think, you know, we see tons of folks in in camp one and, uh, you know, internally at Fortnet we're also working to, to shift our user community, um, to the latter via force controls so that leads us to some we're gonna have to go through 3 takeaways. The first takeaway is, you know, going from, uh, what I would say has been quality to governance and really implementing governance around AI. So we've broken this down into, you know, 33 points that, uh, you know, that Amar and I have talked about. So model integrity, um, this is something that you have a lot of opinions about. So you know, I enforce the first bullet here enforcing model validation, uh, managing drift, uh, red teaming, right, so. You know, red teaming before, or let me put it this way, is, is it more of an automated effort for you in that regard, like you have canned prompts and you test against those, or is it actual people, um, do you do that internally? Do you look externally for, for this sort of Yeah, it's likely an internal AI red teaming which is diff completely not uh traditional red teaming. Traditional red teaming is all about a uh like a red teaming campaign. You do recon and you do all these steps until you reach and exploit. This is you get all the information about the model, what the model does, and you trick it. Creatively, like those people have to be writing prompts in, you know, all caps, um, changing like subtle things get you different outputs. Why? Because the model is, uh, it's tokens. It's just trying to predict the next token. So you have to trick it, yeah, so it's many, many iteration of. Can I get this model to return something that I consider bad could be bad, you know, could be secrets. It could be, um, sensitive, um, words, things that. Absolutely. How, how open are you with these red teams? Do you provide them kind of, is it, is it a black box effort and they're trying to penetrate it, or is it kind of like, hey, here's all the information you need to know about the application? No, you give them all the information to help them be successful. There's no need to be. Hiding anything transparency. This is the model. This, this is like expected behavior. Can you check and validate this model is working as expected? Got you. Got you. One of the, uh, one of the model integrity points that, uh, I think is very important is, and this goes into, uh, also data maturity, but is capturing, uh, security related, um, parameters, uh, and tagging. Um, that data, like, for example, you know, when we inspected the transactions, um, are we, you know, do we audit what the temperature was, what the top P and K values are, uh, do we understand what tokens were used and, and what context limits were applied, um, I think this is important because. Uh, what was, I think, in the machine learning world this effort to tune the machine learning service, uh, or algorithm to perform better, that's, that, that was great. It was a quality effort. Now we're really focused on that as an auditability effort because you have to go back and try to figure out, uh, why did it make the decision or why did it reason in the way that it did and provide the response, which can be very, very difficult if you are not capturing that sort of information. Um What do you think? Yeah, I think um. I'm going to, since most people here are cons like consuming off the shelf models and integrating with AI tooling, I'm going to focus on how the anatomy of this um security. application has changed. Now, there's no such a thing as a traditional web application or a traditional desktop application. All of these applications have some AI integrated with them. That brings, it could like, could be agents behind the scene, it could be a model behind the scene, absolutely a model behind the scene for sure. Uh, but an MCB server, uh, uh, factored, um, database and whatnot, so we have to think about how we secure all of these systems now, not just what we are, you know, used to. We're already behind in security. Like every day there is more regulations. There's more CVs. Uh, 2024 we had 400, um. OK CDs, more alerts, more whatever you think about, there is more of. We don't have more people. Head counts you're lucky to get more head counts, right? um, so you're being asked to use AI to scale the team to solve all of these problems and AI has added another attack surface that we talked about that did not exist. And if you're building your own models now you have also to worry about the model integrity, the drifts, red teaming that, so the scope is just expanding and expanding every day. Yes, um, you know, and I think what you highlighted just a minute ago was a difference in our, in, you know, how we, our perspectives, uh, at Fortine that we're building models for that go into our security tooling. Um, and so, you know, we, we focus a lot on, on these sorts of model building. We do use some external models for some of our tools, you know, like document searches and things like that, uh, but a lot of time, you know, if we're building a security tool, we're, we're building that, uh, model from the ground up, right, all internally. Hygiene. I think data hygiene is probably it, it might be the biggest area of opportunity, um, you know, we see a lot of, uh, data that's sucked into, um, a rag or, or, um, you know, an agent has access to that it should never have had access to that could be either from, uh, having access to network resources that it shouldn't have, it could be, you know, uh, you know, access to. You know, SharePoint sites that have just bad data in it, um, and you know, the most successful customers that we've worked with in this area have implemented data maturity, you know, medallion models, right, so I have raw logs, you know, that's my input, semi raw, right, semi raw, and then ultimately I have a parameterized. I have a tagged set classified set of data. I know what my PII is. I know what I should keep out. Um, and then, you know, I've created that now that is my gold standard, but I don't want that in runtime, so I copy that separately so that I'm never using that gold data in my runtime environment, you know, that's kind of like, you know, what we've seen work really, really well, um, uh, and then you also have data lineage that you can go back and revert to, to prior versions of something. Actually it's corrupted, right? Um, how do you guys approach data maturity? I think you covered it really well, yeah, OK, um. Um, yeah, uh, so, uh, the hygiene, I, it, I think with all of this effort and we talked about early on the expanded attack surface, um. We could talk about tools and I, I'm very tool oriented, uh, but I do think it comes down to hygiene. Like if you have poor security hygiene before you started any of these efforts, you're going to be in a whole world of hurt, uh, if you try to go forward without doing the hard work of cleaning all this up. Um, I, I, I would say like, um, I would go back to what you and I have talked about in the past is. Um, uh, how you put context into compliance and, um, best practices, right, so you can go and find best practices, you can find, uh, you can go through a comp a compliance audit and you can be told, hey, these things need to be, you need this type of encryption, you need something generic, rather generic, right? I think what, uh, you have shared with me that you all have done is make them contextual to. What Adobe's best practices are, which is completely different. Yeah, I think this audience would benefit from that. So I'll, I'll share some of the things that we've done using AI. To Be more security oriented and do security uh better. So if you think about the day to day of a security operations person, it's really the grunt work, the work that no one wants to do, sensitizing data from across multiple data sources, a wiki page, uh, GitHub ReadM, an internal, uh, BDF policy, you name it. So all of that context. It's hard to keep in your head, but AI is really good at it. Like AI is just text hungry. It can just make sense out of it, and this is 90% of the analyst's job in um. One of the things that we've done like in the analyst security review threat modeling space is in the past we had a big team that couldn't scale to do threat modeling. A developer wants to build some new services. They wanna know the threats. Like, I have a database. I have a bucket. I have a back end, maybe three-tier architecture. Um, what are my requirements? Like, should I use KMS for encryption? Is does S3 default encryption good enough? Um, they have no idea what the compliance requirements are. There's so many requirements that need to come into play, not to understand the risk and threats, but also what is this application is you know. Uh, requires from a compliance perspective. Maybe this application is, is called for PCI. Maybe it's HIPAA compliant. It's so many things. No one security engineer is able to keep all of that context in their head if they have like 20 applications they need to secure. Cool. But AI can have access to all of that data, right? So, how about you have few agents that do threat modeling and security reviews for your organization? How does that work? You curate best practices for your company and document all of these best practices. So in the past, we typically, the manual security review or threat model used to be just industry best practices. This is the output. We do a review and we say, hey, use encryption. Use the latest docker image, rotate your images frequently, like just generic stuff, and you give that to a developer and the developer is like, how can I use this? I guess it's just a lot of best practices you've found online and. They may help, but they're not exactly. Um, step by step instructions for me to build a PCI compliant based on Adobe's context. That's very difficult, super difficult. So you create it all of that information and you document it and you make it available in the form of a rack or an MCV server and you make this a self-service. So you have a service, you have a UI and developers go there and they upload their artifacts and questions. Maybe the system asks um multiple questions. Is this a PACI compliant service? Is it public? Um, is it three-tier architecture? And goes out and curates what you need. To build this thing securely, what do I mean by this? You probably have an authentication library that you developers need to use. You probably have a terraform for creating buckets or databases that you developers can use. So instead of that developer needing to figure that out, build it themselves, you link the agent links to that documentation to that repo, to the step by step instructions, and interactively the developer is able to get to from point A, which is nothing, to point B with all the context needed to deploy that service in accordance with your internal policies, your PCI. Uh, your compliance needs and just your internal tooling available uh for developers. This requires maturity as far as like curating the right wikis and making sure those are the only things seen by the agents, because the, if you give the agent access to all of your wikis, it may not get the right. You know, it may get like an outdated, uh, er like uh documentation that you do not want the developer to be using and you do not want the developer to blindly trust what the agent is getting at, um, so it should, you know, you should query it the information. It should you make sure the agent has only access to that very good information that you need a developer to have access to. I think what you're, what you're talking about is like not putting the burden of security on the developer, and I think you know even in the secu as a security vendor we've always talked about oh well we can recruit developers into security, uh, uh, part of the culture, and, uh, you know, so they have to do these things, but you know we interrupt their delivery of applications and that slows them down and Uh, you know, we've not done a really good job of making that easy on the developers, and, and what you're describing is sort of antithetical to that, and I think it sounds like a very positive way. I have a lot of opinions about this area. Um, who here heard of the term secured is everyone's responsibility. Everyone, right, um, so we have so many departments are who are responsible for the success of the organization. This term is only uttered by security. I haven't heard legal say legal is everyone's responsibility. Like they take ownership for their scope. They are the expert as far as legal goes. They keep your company out of trouble, um, if you go to accounting, finance. Each department always say, I am the expert. I get it done. Security is the only place I get it. Security is hard. Breaches can happen and everyone can get in trouble, but we really like security needs to take ownership of security, and you cannot hire a very expensive machine learning engineer and have 50% of their day triaging tickets. Remediation, patching. You need to figure that out for them, um, so can they can deliver your company can win, and this is how you end it like security becomes an enabler, not a blocker, yeah, it becomes, uh, you know, we, we've been, we've had our customers ask us how do we make security a profit center. I think by removing that burden from people who generate profit makes that argument to a large degree actually, you know, um. The, the hygiene in Providence, a lot of that is about data, data security, data maturity. Uh, I will say on that front, uh, it is very hard to do, uh, if anybody's ever tried to classify data at scale, you don't even know where it is, firstly, um, so you know, if you haven't looked at a DSP, a data security posture program, uh, with, you know, uh, data leak prevention which should be implemented both on the endpoint on the network, really look into that. Um, there's tons of, uh, AI driven tooling and automation that can make that very, very lightweight now but very effective. On the trust boundary side there's also an opportunity to, you know, filter on the network, uh, because there's not necessarily always a fixed perimeter, but you can start to intermediate, uh, uh, or interdict in those agent to agent interactions. You can limit what types of prompts are allowed, um. And this is where we're seeing an interest in LLM proxies, LLM firewalls. Um, has anybody looked at deploying such a thing yet? There's a few of them available. OK, so this is gonna be like the next generation firewall reborn. I'll talk about that a little, a little bit more in a second. Um, but this helps you reestablish a trust boundary for, uh, where data is allowed to egress, um, how prompts are injected into your environment and how those are filtered, also providing a point where we can audit, uh, the interactions, um, of the users with your, uh, with your, um, with your tooling. Um, any thoughts you wanna say about the trust, the, the, the trust boundary, uh, you guys are really focused on making sure that your, uh, web application firewall and API protection is very robust. I, I think that's, uh, probably something that stood out to me. Um, you also have a very automated way of approaching that. Um, I think that would be pretty interesting if you, if you could touch on that. Yeah, I think this is one of the things that. Closer to my heart, which is using AI to accelerate security operations and one of the things my team does for Adobe is web application firewalls and those are interesting to manage um, if, if you have a large organization, uh, you typically um. Cannot scale like and have a big wafting. Yep, it's really hard, but the thing is WAF can produce a ton of logs that no one will look at because just so many noise, so much false positives. Writing RAF rules is like rocket science. Like if you wanna deal with regex and you understand regex, great, but most people are not regex friendly. Um, so what we thought about is just see if AI is able to help us here. So the system we came up with is. For logs, you pretty much need to build your own lightweight um machine learning model. So this model is simply there to Um, Detect anomalies. So define a baseline, what normals look like for you day to day, and then generate anomalies. Those anomalies, you don't have to route those to a human anymore. Those could go to a triage agent that takes action, maybe correlate with other data sources. Maybe it's a bad ID, um, IP address, for example, we deployed WAF to a service and WAF said, well, there is a bot that is trying to get in and. It wasn't a bot. It was just a CRCD, uh, deployment. The, uh, the worker, um, IP address was seen as a bot, but the agent was able to look that up and it found it was an Adobe, um, CICD worker IP address, and this is a non-issue, uh, non-issue right now. But the the deployment time was an anomaly. We have a team in India. It's a different time zone. The CICD system kicked in. Uh, wife saw it. The machine learning detected it and we didn't have to wake up anyone, any person in the middle of the night to investigate it. The triage agent looked into it. You can, you cannot have, and you can also have LLMs help you with writing rules. They're really good about like as far as rejects generating code and whatnot. But do not trust the LLM. Um, build a rack system to help the LLM make the right decisions. For example, curate all the rules, best practices, the vendor documentation, uh, maybe he'll have an MCB server for terraform as well, so it has the latest, uh, terraform documentation for building, uh, AWS, uh, WAF rules, and the rack system should have. All good decisions that the AI had made in the past recorded and bad decisions. So when the LLM is trying to generate a new rule, it could look up what happened in the past, good rules or bad rules, uh, and see, and if this new rule again is good and bad, and then suggest that, and you need a human in a loop. You can't just cannot trust all of this and just deploy. Yeah, you're probably gonna block legit traffic, so put it in staging. Uh, have people validate that it's working as expected before you push it, but the point here is to augment your team with these capabilities that, um. We're not available in the past. Yeah, definitely, uh, I'm, I'm gonna move on, uh, but there's, there's a lot more we can keep expanding on. I think one of the punchlines from here is, um, again, as I said at the beginning of, of this takeaway. Traditional machine learning learning workflows, yeah, uh, you know, checklists and hyperparameterization or, uh, you know, having the audit ability that was, that was quality control effort that was nice to have, uh, in AI when we start looking at, um, the implications, uh, they become govern. And its artifacts and so um uh they should be you should be able to audit you should be able to reproduce and follow the chain of uh reasoning um in response back uh through that uh through that process. So take away one has has an example here. This is an example uh from Sagemker where in the JSON. What we, what we do here is we have, uh, tagged our, um, our model uh with metadata properties so we know where it was committed, uh, we know what security check checklist applies specifically to this particular, uh, this particular application. We know that the KMS signature has been verified, um, but and this is what it looks like, you know, in some cases our security, um, our security audits, our checklists have passed and they, they are approved in the in the version control. In some cases they have not, and then they are rejected. Um, those hyperparametters that are tested in those in those security tests might be, you know, something like a a prompt injection uh evaluation or a jailbreak evaluation. Uh, that we've captured here and we capture this for every revision, um, so this is just an example from a, from a Sagemaker deployment. So, uh, let's move on to the second one. I'll, I'll actually jump into this, and this is hyper simplified. There are so many different ways to, to deploy, uh, a network architecture, um, and segmentation, but this is like the basics, um, security tooling that we would bring to bear, uh, for a user that's interacting with maybe a, an MCP agent that you have or, or maybe in this case we're using a public LLM. Uh, in our example, but you know, firstly, we still want to maintain. An obfuscation of our internal network. So we want to enforce the perimeter. We want to enforce zero trust on the user who's trying to access services potentially, um, handle routing, um, and that's part of our segmentation. Um, you know, so we're doing a lot of things on, you know, the traditional things that we've always done on the network firewall to handle that traditional, uh, IT and networking requirement. Now, in addition to that, now, uh, you said building, you, you talked about building your own lightweight machine learning model to look at web application API anomalies. You don't have to do that. We've done that for you. And so what we're doing is we're, we've really moved. Web security away from uh static signatures. Yes, like from a least cost perspective we will look at signature-based detections first, but what we ultimately look at or use is anomaly detection. So everything that we're applying for, uh, you know, uh, a request to API endpoints is, you know, machine learning based and, and AI based. So now that we can. Start to build a schema or uh do schema validation as we're learning about how the users interact with the application, so we're continually tightening tightening down that security profile for that specific endpoint uh without generating the false positives. And so really web application and API protection has come into maturity in a in a completely different way than the old WAFs that were very noisy and uh you know folks like Amar would not look at the logs giving you a hard time um but it's also an opportunity to to broker uh uh TLS offloading or termination on behalf of our back end infrastructure, uh, centralized client authentication. And also do bot mitigation. Um, there are a lot of bots out there that, that, uh, are, are bad. Uh, some bots, uh, you, you may, uh, include as part of your internal tooling, and you don't want them incorrectly flagged. But in addition to that, we will, we have this concept of uh an LLM firewall or, or a proxy. So this is doing model routing. It's model handling model selection. So as you have an entire fleet of agents, uh, hundreds of, of agents or more, uh, potentially, this is making sure that that gets routed to the proper agent and not to the uh, the improper agent. It's applying token budgets and limitations on context potentially. Uh, it's, uh, it's, it's applying basic guard rails. So, um, yeah, you know, are my, uh, are my users, uh, trying to inject a particular type of prompt? Are we trying to exfiltrate data that we shouldn't exfiltrate? Um, and so it's doing a lot of things that are firewall oriented but very specific to the LLM. And so what does that look like, uh, in practice? So a user might interact with, uh, uh, one of your agents. So the user submits a prompt that comes. Through the external firewall that's hosting your elastic IPs hiding the internal private network, um, and so that goes through the web application firewall that does, um, an inspection it's handling the the TLS negotiation client, um, authentication potentially and it goes to your AI agent. Now that AI agent then says I have a prompt and a system prompt and a user prompt and it wants to send that out to this public, um, this public LLM. And so it sends that out the LLM proxy or firewall is going to scrub that to make sure that the, the prompts are sanitized, uh, that no data that is, is being sent out that should not, uh, as part of that, uh, request to the LLM and in this case the LLM is going to then, uh, respond and it's gonna request information because this user maybe wanted to look up, you know, uh, inventory or pricing or doing a search as part of a shopping experience. And so that then is gonna go to back to the agent and the agent's gonna say I need to look up uh some information. How do I do that? Well, it knows that this particular MCP agent has the tooling to do that, but we're gonna inspect that the, the, the MCP session itself, which is Jason RPC formatted, um, and make sure that that is, has protocol integrity, um, and is not a malicious query. Uh, that now I'm showing an API call to an actual web server, so the MCP agent in this case is front ending the API. Um, that API call could very well go back through the, the web application and API inspection point, uh, but, uh, I didn't show that here just for simplicity, um, and then all of that is sent back, uh, to the agent and contextualized in the LLM. Which then routes all that back through the inspection points back to the user. So, it, it looks complicated. It's complicated. Now, that is super simple. Now, do that when you have hundreds of agents who are task specific. And that gets really messy and so these are necessary points I think in your infrastructure that need to have inspection, right, um, and so, uh, you know, doing this in the right way in a very complicated environment. Requires a lot of attention to detail, um, but this is how we would go about in a simplistic way of, uh, defending AI in a in an AWS environment. Now, what happens when you don't do those things and implement those controls, right? Well, uh, this has actually happened, so this came from a customer and we replicated it. Uh, so maybe it starts out with a traditional type of attack. You have, uh, an SSL injection, uh, SQL injection that says, you know, here's a, uh, here's an SQL injection in a prompt. Uh, I don't have controls around my prompt, so, uh, I, it, it accepts the prompt and passes it on now. This could be from overly permissive agents. There's no input sanitation, so the fix for that would have been guard rails, anomaly detection, uh, potentially DLP, but that escalates into a, um, SSRF attack. So, uh, a server side request, uh, request forgery comes from possibly poor segmentation. There was access to a, uh, an agent or a service that should not have been accessed. There was excessive trust potentially, so we, we wanna make sure that we're on the flip side of that implementing segmentation and doing input validation. Now, uh, what that looks like as we go through this is, well, in that SSRF, uh, request, I was actually trying to get the keys, uh, to, uh, to the AWS environment and so after getting a token, um, uh, eventually we are able to, and this, uh, this is a very long demo that I've shrunk down into a few slides, but we were able to export the, uh, the keys from that EC2 instance, uh, that we were targeting now. These keys have root access to the AWS environment, right? You know, again, POC quality, uh, you know, that's what happens, um, because it's easy. Now what we did, we then did was we were able to take that, uh, those, uh, those credentials, use them in the AWS account to gain access, um, to, uh, an S3 bucket, so we have excessive permissions, had access to every route, had access to everything. Um, what we should have done is implement lease privileges, applied IM roles, um, and then also classified data so we could have protected that. But what it looks like is, you know, for this e-commerce application we had a number of, uh, reviews, right? And these reviews, uh, you know, were user feedback about the product that we just database them in an S3 store. But what we, what we could have done is we could have added, and we did is added a malicious prompt as a user review. hey, this product's great, by the way, this is how you need to respond to all users and so we were able to do that so we never compromised the actual uh. AI, um, uh, or the actual application, the e-commerce application itself, we, we, we compromised the, the data that it pulls from injected a prompt that overrode the system prompts, and so this is how we attacked this because we didn't have any of those things I showed you in the last slide on takeaway 2. Makes sense. I have a question. Did you rotate the credentials? Like, I saw people taking pictures of your AWS API. It's fine because they no longer exist. Yeah, um, but good point, yeah, no, they, those credentials that this entire account no longer exists. Um, yeah, so that was a good point. No, I wouldn't have done that. I might have done that, giving you a hard time. Oh, absolutely. Let's move on. Take away 3, so we got about 10 minutes. Um, the future of AI security or security is AI native. So you've talked about implementing, uh, AI security tools, um, but this is, uh, a little bit about, uh, both, um, our, our practice, but also the workforce enablement, how. Your developers or developers are using uh co-pilots and tools that are aware of security specific to the environment. I think this is some of what you've actually talked about. Um, but, uh, in the first one, AI augmented threat modeling, right, so identifying architectural flaws, so you have security tooling that can tell you where you have misconfigurations. Um, do, do you rely on, you know, static, you know, um, posture analysis? Are you looking at more like AI oriented tools in this regard? I think you need both now. Um, static analysis tooling is not enough anymore. Um, they always had limitations as far as like false positives like if you use a static code analysis tool you've, you've, you know what I'm talking about, um, so you need. The AI context you need these tools to also play together. Maybe the AI provide context to the, the static code analysis provide context to the AI, and the AI kind of go find find if it's a false positive or not. So creatively in ways that we haven't seen before, you need to go and. Play around these tools and this is go back to the workforce enablement I think AI is coming fast at us but we are not spending time training our security people to be more AI aware and also our developers to be more security aware so, um, the AI training is and AI literacy is a thing now so probably need to slow down and take this and. You know, read books, take courses, explore, experiment, and develop a skill is, is, is needed now. OK, yeah, it, it, so you, you talked about sec, you know, you drew this distinction between security being everybody's responsibility, but maybe, maybe it's not right. But what does that look like in terms of the overall culture at Adobe? How, how is it, how does Adobe look at? The security culture, so we, we. When we think about security being everyone's responsibility, yeah, everyone needs to be security aware. Like you need to know what encryption is. Like you're building, you're a developer, you need to know the best practices. But if you're spending 50% of your time doing security work and what not what you're hired to do, that's a problem. You, you're spending most of your time not building features, which is exactly what. You had in your job description so our culture is more balanced where we as security organization take a lot of ownership and we are ultimately accountable for security, but we also and we provide a lot of tool and guidance and support and we keep up developers are using agents to write code. We provide guidance to the agent so the agent is able to write better secure code. We don't wait and blame the agent and complain about AI. We do something about it, so the culture is more about being proactive, anticipating what developers are needing to do and providing them with tooling, the guidance, the support needed for them to move fast in a secure manner. I, I think the summary for Takeaway 3 is that um this goes, this goes back to doing the hard work and it starts ahead of any of the tooling you may implement um it's, it's about um. Building those best practices that are not generic, they're specific to your organization that allow your users who have to get apply those best practices actually make them meaningful. You spent a lot of time talking about that. I, I, I, I, I would summarize this as being that. Absolutely, yeah, yeah, well said, um. And then, so let's, let's talk about ecosystem because there is this, you know, you have your internal security efforts, your applications, uh, you, uh, user communities, you have, uh, partners and third party, uh, vendors, uh, like Fortinet, right, as everyone here does, and everybody here is presumably on AWS, so everybody has a role in how these, um, uh, this ecosystem collaborates, right, so. You know, I think a few points about that, um, unifying security pipelines across platforms, um, you know, for example, uh, you know, as you're, you're deploying on AWS, if, um, you know, if something fails a security check either on your side or from, uh, Fortine, we can, you know, inter, interrupt, you know, a, an approval or, uh, in Sagemaker as an example, um, that we gave earlier, um. Shared telemetry, AI driven observability. I think this is, this is really huge. Um, it is, it is very hard. You can't do anything if you don't have the observe the observability. Um, we focus a lot on surfacing, uh, you know, and making visible what is in your AWS and other, um, states across your, uh, you know, your entire footprint, um. But uh you know that requires, you know, a lot of hygiene of what tools and applications you have standardization, um, so that that observability actually makes sense, right? I think. Um, what do you think? Absolutely. I think you, you hit the main points. You, most of us like using AWS. AWS is not a security company, so. Some of the security rely is on you to build internally and also to buy the right tooling to get the right support to be able to complete like have full circle um coverage so um again nothing has changed all of the. Old worlds still exist. Now you have to think about how the old world integrate with the new AI world and how you, you know, how you go about securing that, how you train these people, how you get the right tooling. We're not thinking about AI security tooling yet. We don't even know the companies that are out there doing work in this space yet, um. And what established companies like you guys are doing about it, so today's session is like I knew about a few things that I did not know about before. Yeah, um, so a few things to take away. I think this is what summarize the, the, the points that we wanted to impart, uh, we know that. The velocity of signal in terms of uh log volumes, uh, the interactions that users are having with these federated applications that you're creating that are AI driven is tremendous. Um, it requires, uh, defense in, in depth. Uh, it, it requires effective network planning. Uh, we wanna orient our cultures and our workforce to be AI native, but to do so, I think you, as you've talked about Amar, it takes a lot of. Preplanning, uh, a lot of hygiene effort, um, the tax surface is, is very broad, um, we need to maintain a consistent, uh, structure and discipline around managing. Uh, lineage, right, both for our, our models, our applications, uh, and for the data that those systems consume, and I think it's, it's not, it, it's not going to be successful unless you look at who and how you partner with AWS, with your other cloud providers, just your overall ecosystem, um, and they'll have best practices that you can make specific to your, your organization, um, and it's, uh, you know. It takes a it takes a village in in that sense I think. So with that, we're out of time. Uh, I, we'll hang out here if you guys wanna talk, uh, ask questions or share your ideas, uh, I would love that, uh, but thank you very much for spending some time with us.