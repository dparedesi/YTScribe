---
video_id: gy59STBBsX0
video_url: https://www.youtube.com/watch?v=gy59STBBsX0
title: AWS re:Invent 2025 - Ops in the AI age: Innovating together for faster, more efficient operations
author: AWS Events
published_date: 2025-12-02
length_minutes: 64.45
views: 2305
description: "The rise of generative and agentic AI is transforming how organizations build, operate, and secure applications. As systems become increasingly autonomous and complex, intelligent operations are essential. Join us to discover how AWS Cloud Operations continues to innovateâ€”helping you monitor AI agents wherever deployed, operate smarter across environments, and unlock insights through accelerated analytics. Explore AWS-native and managed open-source observability providing visibility into distrib..."
keywords: AWS, Amazon Web Services, AWS Cloud, Amazon Cloud, AWS re:Invent, AWS Summit, AWS re:Inforce, AWS reInforce, AWS reInvent, AWS Events
is_generated: False
is_translatable: True
---

Please welcome to the stage, Vice President of Search, observability and Cloud Ops at AWS Nandini Romani. Hello everyone and welcome to Reinvent 2025. I'm excited to kick off the very first innovation talk of 2025, and every year I look forward to this session myself, my team, because it motivates us to look back on the year that we've spent on innovating and new adding new features so that we can address cloud operations for all of you. And we have been innovating with you not just this past year but we have been doing this since the birth of the cloud and in the last two decades we've seen some major paradigm shifts in technology ranging from virtualization to containers to serverless and so on. We've been on this journey with us. And now we are on to the next tectonic shift, agentic AI. We are living through the next big transformation and just like compute databases and storage, AI will be a foundational part of everything you do. AI has disrupted customer experiences to such a point where our end customers now expect all of their end user experiences to be faster and smarter leveraging AI, and I know many of our customers, many of you here have already deployed AI or you are at least using some of it to meet this demand. Having said that, we do know that generative AI adoption varies by industries, by verticals, and other, you know, specifications based on what you are working on and according to one report from McKinsey, 88% of organizations are already using AI in at least one business function. More than a third of you have either already fully scaled or are scaling and deploying AI. Another 1/3 of you are running pilots, and the rest of you are still experimenting. And many of you, I think, especially for those of you in highly regulated industries, you might be moving with more caution and understandably so. And this is why compliance and security is super critical when it comes to Gen AI and this is where AWS can really help you. We have done this for you when it comes to infrastructure for decades, and we will do the same for you when it comes to AI. So no matter where you are on this adoption curve, we want to meet you exactly where you are, whether it comes to cloud or AI. As many of you know, we at AWS love telling our stories through the voice of our customers, and this time around, I wanna kick off this talk by introducing one of our customers who uses AI to enhance their fan experiences. What is the PGA Tour? It's a traveling technological masterpiece. Spread across 200 acres with over 120 cameras and 36 radar trackers collecting data on more than 32,000 shots, every player, every swing, every week. Using the power of AI to give fans what they want. Anytime, anywhere. Amazing. So please welcome David Proven, VP of digital architecture at PGA Tour, to the stage. David, welcome. Thank you. Uh,
good morning guys. Thanks so much for coming out here today. Excited to talk to you about what we do at the PGA Tour. If you haven't heard of the PGA Tour, the video hopefully has fixed that for you. Please go and download the app immediately and rate it 5 stars, we'd really appreciate that. Um, golf is unlike any other sport you're going to consume on a week by week basis. We don't play inside a set amount of white lines. We don't play in the same stadiums. We play in these amazing natural venues throughout the world that change, that deal with weather. And instead of like one field, we have 150 acres of entirely playable area. Each week, 156 players all playing asynchronously against each other. Which leads to up to 14 golf balls at any one point in the air being tracked in real time. And like we said in the video, over 31,000 shots captured each week on the PGA Tour. So what do we do with this is what we're looking at with UTA. How do we actually take AI solutions we'll talk about, and I want to first talk about how we think about that operationally, because at its core, the PGA Tour is an operationalization that runs golf tournaments. That's what we do. Week in, week out, we are running golf tournaments throughout North America, some other parts of the world, and we are building, installing, running, tearing up, putting back down again. We are an operations company at our core. So we approach AI, we think operationally first. How do we score and validate things that happen in real time? If we're not first, we're last in golf. How do we keep a human in the loop at the right point in time? And everyone talks about like secop ship left, DevOps ship left. We took operational thinking and shifted that left. From the beginning, how can we approach operational thinking and what we do? And what we built. Was this, which is our AI shot commentary system. For those of you that don't know, we provide a product each week on our digital platforms called Torcast. Torcast is a full 3D rendering of a shot tracked golf course. People spend between 30 to 60 minutes in this experience is what our most engaged platforms that we build upon. And for us, we really wanted to build a commentary system for me in particular, that didn't just tell you what was happening, but told you why it was relevant, why it mattered. It's really great to say Isaac scored a goal, but not say any cost 125 million, which is the interesting fact that goes with it, right? So our commentary is generated in two elements. We generate a fact. Scotty Scheffler hit the ball 175 yards, he is 4 ft from the pin, and then we add context into that system. He has a 98% chance of making this putt for a birdie, and will end up in 3rd position and leadable result, which will move him above the FedEx cut line. And that is really taking a point in time and making it relevant to our fans. And all of this is powered by our cloud operation and how we build it. We're going to walk you through how that system works. Firstly, we talked about at the beginning, there's those radars. There's a golf course collection system where we're literally gathering. Tons of data, like terabytes of data each week off the golf course. All of that folds into our cloud-based scoring system, which is doing coordinate realization. It's working out which player hit it, where did it go, it's updating statistics in real time, what's their new strokes gained value, what's their new statistical values. We then built on top of that, our AI shot commentary system. And what this does is every shot on the golf court is ingested in real time. We analyze 72 million statistical records to find the most irrelevant one. And then present that back over to the short commentary system in Torcast. And while that left to right flow is super exciting and we're really proud that it works, what really underpins it is the operational dashboard underneath. How do we know it works? How are we sure that it's operating fine each week on the PGA Tour? So we're using synthetics, we're using insights, we're using alarms, log analysis to give our operations team, which are really the beating heart of what we do at the tour, confidence that it works. There is not a better feeling in the world than when the golfer hits that first tee shot at some ungodly time in the morning cos we start golf at first light to see everything flow through the system and work. It's a really powerful feeling when it works and a mildly terrifying feeling when it doesn't work. So let's talk about the cloud Watch dashboard that powers the AI shock commentary system, and we're gonna zoom into some elements here to help you understand how we're taking these AI problems, scoring validation, human in the loop, and production first thinking and actually realizing those. First one I'll show you is our validation score. So for those of you who grew up in England, which is me, we have this concept in driving tests of like minor faults and major faults. Our scoring is analyzed against this, when the commentary's generated. Is this a minor issue or a major issue? If it's just a minor issue, we'll push it through, maybe we'll regenerate it. The blue line is showing you here are things with zero errors, they went straight through the UI, no human involvement. The orange is one error, it's usually a stylistic language thing, so it still goes through. Anything more than that, it goes into this review, we don't put it in the UI, we hold it. About 98% of our commentaries go through without further involvement. When this doesn't work, we can then see what those look like. In a dashboard video again, the top one is showing you commentaries that for some reason failed validation. The score was wrong, the par was wrong, the yardage is wrong, something like that. Underneath my favorite one cos it sits there and ticks through when they work, the commentaries that are going through that you can see on the platform. And now I wanna show you what happens when it doesn't go well, which does happen sometimes too. So the first chart on the left is really just a standard LLM's timing, we like flat lines, things should look consistent timing. The SQSS one is uh Bank of Utah a month ago. We had not had an event on the PGA Tour for about 1 month, we had the Ryder Cup. No real PGA Tour events after Procore. Scoring starts up and that moment in your pit of your stomach when someone doesn't work, no shot commentary. And this is a sponsored feature for us, it's high profile. At that point in our traditional model, you go into debug logging, how do I analyze this, how do I pull this out? How do I get the developer around to go look at this? Because we'd shifted that thinking far left, we're able to pick up our SQSS queue and realize very quickly, we're getting commentaries, we're not generating them, so our queue is not being processed. So as opposed to traditionally what's taken is like maybe a full day to kind of dig into it, get the right person, get them online, get it fixed. The first line support team just restarted the cluster. And then my favorite line in cloud watch is when it goes straight up, cos that means you've fixed it. You see everything kind of sync back together and going in parallel. So again, we then learn from this, we put in new metrics, we put in synthetics, we're now sending control messages through SQS so we can keep things operating, we learn and we get better. We're not perfect. But it's one of those things of like real life happens. How do you respond to it? In our sport, if we don't fix it immediately, the backup just builds. It just builds there, those golf shots keep coming. So the quicker we can resolve it, the better for us. So at the results of this, like we talked about 31,000 shots, and each of those shots costs us a penny to generate for shot commentary. That's the bedrock fees, that's all the stuff that goes with it, the sky watch fees, 1 penny per shot. It's really important for us that we can do this thing cost effectively. We've also added zero extra humans to monitor this system. It's full. It's folded in with our traditional operational support team. No extra bodies, we just added dashboards, alerts,
and reporting into our existing setup. It's all in CloudWatch. We use that already. Again,
apart from that issue a month ago, the system's been highly available. My favorite thing on a Thursday morning is to watch everything just work. It's the greatest feeling in the world. And when it doesn't, it sucks. But fortunately, pretty good. And then like I said, with that problem resolution, a much quicker response time for us when we dashboard. Constructively, thoughtfully, and we involve the developers in it, in that process right at the beginning, kind of move that decision tree down. But we're not done, um, we are relentless in trying to do better. And we're currently working with AWS on piloting how we can use AI investigations to really help us change how we do log analysis. So for those of you doing this today, probably a traditional thing, you're logging out Jason objects, logging out error codes. Quite often that has to go to developer, like,
hey Steve, we've got this error code, what does this mean? Or it's a giant knowledge-based document I've got to fill in and understand. We're actually playing now with, what if we log the error message as a human readable string. Hey,
I failed the ingest because the scorecard isn't up to date. And we actually log out at the developer level. The developer knows what the issue is because they wrote the code right out as a log statement, the actual English transaction, because it turns out LLM's really, really good at summarizing that and telling you what the issue is. So what that's allowing us to do as opposed to when an issue comes up, we've got to bubble up through level 1, level 2, level 3, and you get to the guy at level 3 that fixes it in 5 minutes, but took you an hour to get there. Able to push that decision making way down. And help those guys in the first line, the guys that have really taken the bullets on the front line and let them move it quickly. So as excited as I am with the stuff we've done to date with AWS, I'm actually more excited for where we're gonna go next in this operational journey that we're, we're doing on the PGA Tour. And with that, I will hand you back to Nandini. Thank you David. It's truly inspiring to see PGA Tour, just like many of you approach AI with the same customer obsession that we have here at AWS now.
2025 is the year when agentic AI moves from experimentation into production. These agents no longer just respond, they think they act and they even make decisions on your behalf. So The now while agents certainly help you with creating efficiency and making things a lot simpler, it has also created certain operational challenges. Let's think about it. First is trust and safety. Visibility into your AI agent behavior is super critical. Think about, let's say a customer service agent that you've created and a customer reaches out regarding some billing issue. Now the agent in a matter of seconds makes several decisions. Does it handle the matter on its own? Does it escalate to a human specialist? Now the question when it comes to observability is no longer did the agent work. We all know the agent works so great, but the question becomes how and why did the agent choose to handle the situation the way it did. Second, operational complexity is only increasing. You are now managing microservices. In addition, now you have distributed agents. You have event driven architectures, and on top of that you are managing all of this across dozens, hundreds of accounts and multiple regions. You want AI to be able to help you not just with applications but also making operations more efficient. Third data explosion we have seen this over the last several years and now your AI agents work 24/7 and they handle thousands of customer requests every hour and each of these requests is generating an immense amount of telemetry throughout the day. That's not just more data. That is, it's exponentially more surface area that you now have to both monitor and secure. So we have been working to make these things simpler for you so that you we handle the heavy lifting of cloud operations. Now as complexity grows our mission at AWS stays the same. We simplify cloud operations for each and every one of you at AWS we pioneered cloud computing, but we are not stopping there. Through every shift in technology we have always been there for you, removing the undifferentiated heavy lifting and building new capabilities that work well together. And we will continue to do that whether you are all in on AWS or on in hybrid environments or even multi-cloud. Whether you're using autonomous agents or still deploying traditional microservices, we handle operations for every one of you. This is our commitment. So today I want to show you exactly how we are delivering across these three pillars. First, monitoring and trusting your agentic AI applications. When an AI acts. It does so autonomously. You need visibility into what the agent is doing and the why behind it. Second, operating smarter with AI. As complexity grows, we want AI to help you with your operations. Third, simplifying overall operations, and I'm talking about traditional observability for databases, containers, and so on and so forth, not just as it relates to AI. So let's dive into the first pillar. Like I mentioned earlier, you are now have agents that are smarter and taking actions on their own on your behalf. So picture, let's go back to the same chatbot who's handling customer support issues. Your agent now is making decisions in a matter of seconds which you may or may not agree with. So agents are right a lot we know this however, they are not always right. What if an agent makes the wrong call, wrong decision, and turns away your customer? Agents, this will impact not just your end user experience but also your brand reputation. This is why observability is now the control plane for your trust, safety, and accountability. Next, traditional monitoring tools can tell you that the chatbot responded to your customer request in let's say 2 seconds, and it did so with a status code of like 200 maybe. That's great so you know the agent is working, it's running, but it doesn't tell you the why behind what decisions the agent is taking. AI agents can create hidden issues that legacy tools cannot see. Now don't get me wrong, performance metrics are always important. You saw them in, uh,
David's dashboards. You're gonna see them every day, and they're super critical. However, they do not give you the entire picture when it comes to AI. So in October we announced the general availability of generative AI observability in CloudWatch. This feature helps you monitor, troubleshoot, and optimize your AI applications and workloads. It simplifies a few things for you. First, you need to make zero code changes. This is because we leverage open telemetry, which allows instrumentation and makes everything super simple if you haven't tried it yet. It also allows you to use any agent development framework of your choice. You can use strands agents. You can use lang chain, land graph, or Crew AI, and many more. Or you can also choose to instrument manually by yourself using the Open telemetry SDK. Second, this works no matter where you choose to host your agents. You could choose, for example, Amazon Bedrock Agent Core. Or self-managed Kubernetes hybrid environments, multi-cloud, it just works. You get complete visibility out of the box into latency, token usage, and performance across all of your AI workloads. Third, you can pinpoint issues anywhere in your AI stack. So let's go back to that billing issue we were talking about, right? with end to end prompt tracing, you can now trace that single customer from the initial complaint that they made through every model invocation, every tool. Tool execution every knowledge based query all the way to the final response from the agent to your end customer. Think about it. That's a series of events that you can track no matter where you choose to host your agent. Now you can see exactly how the deci agent made the decisions it did, so you have entire visibility into the decision tree that the agent is taking. So, however, we know that agents do not operate in a silo. They integrate with other services, applications, and other dependencies across your entire estate which would be across multiple accounts and regions. So when it comes to troubleshooting, now it's challenging to understand where all of these resources are and how are they interacting with each other. That's why we launched CloudWatch Application map. So this requires no manual configuration. Application map will automatically discover and organize all of your services, including uninstrumented services. I think this is key. Uh, and the map organizes it based on exactly how you operate, based on your application, your teams, and even your business units. It correlates and analyzes metrics, logs, and traces and offers audit findings that simplify your root cause analysis. Now when your customer service agent that we talked about, when it's responding slowly, you will not only know which service is degrading, but you will also know why it's doing so. So with that we are now at an exciting part of this presentation where you actually get to see this in action. I'd like to invite to the stage someone, someone you have all known for like over 20 years. Please put your hands together for Jeff Barr, VP at AWS and chief evangelist. Jeff, come on stage and show us how it works. Thanks. Alright, great to be here. I almost broke my leg coming in, but I'm alive and well, so happy to report that. Um, it's been super fun to work with this team to prepare this content, and to me I'm thinking about this a little bit, and it's really kind of this idea from agents as kind of a cool thing that we could think about to now we're building them, now they're an essential part of our, our business. And now they're so important we actually need to monitor them and maintain them and make sure that they're working the way we expect them. So we, we see these agents going from, from proof of concept to production. They connect with your customers 24/7. So you have to actually monitor them, maintain, make sure you're actually maintaining your customer trust. And what I want to do today is show you a bit about how that actually works. So let's take an example of a pet clinic. The pet clinic is growing, it's expanding its digital footprint, they've got medical appointments, nutrition recommendations, and more. And these customers are pretty demanding. They want a near real-time experience. They want instant confirmation when they're booking an appointment at 2:00 a.m. for their sick pet. So deliver, to deliver on this, these plans or these expectations, they've got 3 agents. They put all these agents into use this year. And they run on completely different hosts serving different functions. They've got a customer facing orchestration agent. It's running on Bedrock Agent Corp using the crew AI SDK. This is the front door. It hits the customer inquiries, routes requests, and coordinates with the other agents. And second, the appointment scheduling agent. This one is built with lang chain. It runs on EKS and it manages real-time availability. It handles cancellations, it sends reminders, and it integrates with the clinic's existing scheduling systems. Then there's a third one. This one's a nutrition recommendation agent. It analyzes pet health data and it recommends supplements, and it's hosted on Bedrock Agent Corp using the Strands SDK. This one reviews medical history, it looks at, looks at and considers breed specific needs, and it makes personalized product recommendations. So, we've got our 3 separate agents, and they're here to help this pet clinic scale. They handle thousands of simultaneous customer interactions. They make real-time adjustments based on appointment availability and health data. But there's a real challenge. The operations and the AI teams, they need visibility across all three agents, and we saw how they're built on, on different environments and platforms to know not just that the agent is running. They need to know that this agent is actually making good decisions and delivering a consistent customer experience. So we want to help to get you observability access across your agents, across any framework, and anywhere you might want to host. And of course we want to make it easy for you to start. And so we set this up, so there's the implementation actually takes just two steps. So first, you auto instrument your application, you install the AWS Distro for Open Telemetry SDK, which we call ADOT for short. And second, you enable transaction search in the CloudWatch console, just one click, and this lets you receive telemetry from that A.SDK and it gets sent to the CloudWatch OTLP endpoints. So real easy. So once instrumented, you get unified visibility that goes across your complete AI fleet, no matter where you build and no no matter where you host them. You can see your entire fleet at a glance, you can see runtime sessions, runtime invocations, CPU consumption, what we can think of as the, the, the curated set of golden signals to know that your Gen AI is working as you expect. But we don't stop at just the high-level dashboards. You can dive deep into the individual agents to see sessions, see traces, and aerometrics. So for each of these agents, you get again these golden curated metrics that are specifically designed for your G AI workloads. For example, things like throttling rates and error rates, and the idea is these golden rates, golden metrics will help you to understand if the agent is performing as you expect. Now beyond the metrics in the dashboard, you can also drill down to view all sessions for any agent. Lots of great detail here. Here's an example. You click into a session, you see all the traces, you double click on any trace to look at the full trajectory map. Your nutrition service, calling medical history processing, checking pet types and inventory, so you, you see exactly how the requests flow through your system. So let's say you see an error in the trajectory map. Well,
you just click on it to see exactly what went wrong. So in this case, the nutrition service can't find the pet type, and the agent expected a rabbit, but the data for the rabbit wasn't actually there. And so we can actually see the failure point right away, right there in situ in the code. That's not all. You can also drill into the chat span and you can see the entire exchange. You can see the system prompts, the user prompts, and the tool messages, all the details you really want. You see exactly how the customer experienced your agent, and you can validate the error message, so you get full visibility into how your AI agent is actually doing the reasoning. Now one really important aspect of this, if,
if your users share some sensitive information like names and emails, you can mask that and you can protect that PAI by simply turning on the Cloudwatch data protection policies, and you'll see that that's masked out right there in the in the log info. Alright, so maybe you're thinking, this is great for my agents, but these agents don't work in isolation, they're part of a bigger picture. So they're calling microservices, they're querying databases, they're maybe pulling real-time data from across all of your infrastructure. And so for example, your appointment agent needs some payment processing, your nutrition agent needs inventory systems, your orchestration agent needs all of them working together. So you need visibility into the entire operational health of your application, and because when one of these things fails, your whole agent fails, and when one of those agents fails, your customer experience fails. So there's a, there's a cascading reaction there. So this is where the CloudWatch Application Performance monitoring or APM comes in. We help you troubleshoot the operational health across your entire application stack. So a lot of cool stuff happens here. We automatically discover your application topology, both the instrumented and the uninstrumented applications, and you can also just progressively, as time permits, add instrumentation where it matters the most, all with just a few clicks. So let's go back to our pet clinic example. Let's imagine customers are experiencing some issues with the nutrition agent where the agent fails to recommend supplements for a pet rabbit. The agent appears to be running, but something in the underlying infrastructure is broken, and you are on call. You need to find it fast. So you kickstart your troubleshooting directly from the online pet care business unit, or you can go with any other grouping based on how you organize your services into production, development environments and and and so forth. So you dive in and you,
you drill down to all the applications within that business unit and you filter by the failing services to highlight exactly what's broken. You select the, the,
the failing pet clinic front end, and you see what we call the complete blast radius. Sounds kind of dangerous, but that's the phrase we often use, the blast radius. And we trace the failure from the front-end service to the visit service and finally down to the payment processing. So,
so each hop, and hops, these are hops not related to the rabbits we talked about earlier, but each hop shows you exactly where the request failed and why. So you get all, all that flow within your, your topology. So next step, the fixing issues is even easier with automatically generated operational audits for your applications, and the this built-in view actually correlates monitoring, monitoring data across your infrastructure. You get a head start on root cause analysis before you even begin your manual investigation. So we think about this, your application needs more than just operational health monitoring. You need to actually understand, and what I've shown you today, the actual um end user impact. So that's where the real user monitoring and the synthetics come in. We connect the the real user monitoring, which we call RM, and the synthetics to your application performance data, and you get a complete view across all the layers, from the infrastructure health to the agent performance, to the actual customer experience. So as you as innovate with these agents, and as I can, as I'm talking to a lot of you, realizing you're making them totally essential to the way you are innovating and moving your business forward, we're going to innovate right there alongside you. And what we've shown you today, just the beginning, we've got a lot more queued up to show you the rest of this hour and throughout Reinvent, we're going to continue to work with you and build the great tools that you're going to need to succeed. Thanks so much, and I'll be back in a bit. Thank you Jeff here we go. OK. So that was incredible. I love watching demos and I cannot wait for all of you to try it. They're already launched today. I saw them come in this morning, so I'm, I'm waiting for you all to try it now whether you are building agentic applications or traditional microservices, how can AI help you operate smarter? Here's the reality our environments are only growing more and more complex as a result, you are managing more alerts, more, uh, incidents than ever before. How do you discern which alerts are important, urgent? How do you. Discern which incidents are like sub 2 sub-0, and so on. Troublesshooting is always like finding a needle in the haystack. We use this phrase every year, but now this haystack is getting more and more bigger, more and more complex every single day, um, so you need AI not just to help you observe better but also operate smarter. When AI does the heavy lifting of correlation analysis root cause investigations, it frees you up so you can innovate on behalf of your customers. So Cloud Watch Investigations launched earlier this year in GA. It is a generative AI powered tool that has a capability to automate root cause analysis and gives you a hypothesis of what went wrong. It also guides you through troubleshooting as well as remediation. But what's more, um, I think like what would be better than being reactive and even if you solve the problem quickly, it's not having the problem in the first place and being in a more proactive mode. So you wanna go to a preventative mode instead of always being reactive. Now how do you do that? So one of the things that we have launched recently to help you with that is interactive incident report generation in CloudWatch. So I wanna explain why this is so critical in my conversations with many of you. I hear all the time how does A AWS handle post incident analysis and how do you use that to improve your own operational excellence internally? How does AWS do it? So we've had this process we call correction of error and if you Google COE it shows you center of excellence, but this is correction of error, uh, and we've been using this for over two decades. So what we do, we document everything that happens during an incident. Identify root causes, take actions. How do we prevent reoccurrence? All of this knowledge is then captured in a database. And with just one click you can now excuse me, take the same information that we have done for two decades at AWS and now using Cloud Watch investigations report generation feature you can now generate your own incident summaries and we have also added AI powered Fivewise workflow. It's a process we use internally on answering the five whys behind every incident. And we have the same process out for you with this feature. Now this feature automatically gathers and correlates all of your telemetry, your deployment, and every text and action that you take during your own incidents and investigation, and it produces a detailed actionable incident report. Now here's why this is so powerful. This is not a generic template. This is tailored to your change history, your environment, and your architecture. It's customized for you so that you can create a similar, uh, knowledge base from which you can then have insights and have AI powered, uh, preventative actions that you can take. We also know that not all of your teams are in the AWS console. Developers usually are in their favorite IDs or in AI productivity tools like Hero, so we are bringing observability exactly to where the developers are, to their workflows. We're doing this with CloudWatch MCP servers. Developers stay in their preferred tool of choice. They connect their AI agents to CloudWatch, and they have access immediately to application performance and other health information so that you can troubleshoot issues, operational issues in particular, way faster. But what about other workflows like pull requests, code reviews, or deployment workflows? So we are introducing application observability for AWS GitHub action. This meets developers exactly where they are right in GitHub. Now you can bring your production telemetry to your code. You can diagnose your production issues. Use,
you can use live telemetry to, uh,
diagnose it. You can auto generate pull requests with the exact code changes that you will use in production, and you can instrument applications directly in GitHub repos. So now that we've seen these two features, I'd like to bring Jeff back on stage to show us how it works. Watch out on the steps, Jeff. There you go. All right. All right, so,
um, the,
the beauty of modern operations is having some choice. Well,
you can actually maintain the speed of troubleshooting. And there's a lot of different ways you might be doing that. You might be troubleshooting natively in AWS, you might be building your custom agents, or you're working at GitHub, and If you take away one important thing from my talk today, we really want to meet you where you are and how you work. So cloudwatch investigations is going to complement what you're already doing. It's gonna leverage AI to answer questions that you might say, why did this alarm fire? And it's gonna hopefully do it for you faster than manually connecting the dots. So what do you do, you actually configure an alarm to automatically trigger this investigation. At no additional cost, so every alert turns into an instant AI powered analysis. By the time you actually get that alert, you log on, the investigation is ready, ready there for you to review, so you get to the root cause in minutes instead of hours. So here's what happens when an investigation gets triggered. The AI builds an internal topology of your services and your resources, and then it analyses and correlates the metrics, the logs, the traces, and the deployments. All those things that you might have to go to several different places and look here, there and, and elsewhere to get all the right information. It pulls it all together, within seconds, it surfaces some hypotheses and even suggests some actions to accelerate your troubleshooting. And what you do next, you review these key insights, and that that they're going to show you the actual reasoning behind each of the hypotheses, and you get full transparency into how the AI actually arrived at the conclusions that it did. Alright, so from here, so inside Amazon, we,
we do this process, you heard we call the five whys. So we, we, we literally dive deep to make sure we fully understand the deepest of the root causes for any problem. So we've built that same thing into this feature. So automated incident report generation, building on our own processes that would have served us incredibly well for over two decades of uh of running AWS and all of Amazon actually. So this is not simply just like a generic template, it pulls from your telemetry, your deployment, and your health events, all the things that tell the actual real data-driven story of what happened in your actual environment. So the timeline events are automatically pulled from the logs and the metrics, speeding up the process of actually documenting the the investigation, so you can focus on the learnings and then improving your own team's operations. And these reports, they also include the, the root cause analysis that was generated using the, the answers to these five whys. And again, the same framework that we use inside of Amazon, we call it the COEs or the correction of errors, and these COEs actually become somewhat legendary. Over time because they're they're effectively the repository of all the ways things have gone wrong over years and decades, and we refer to these often, and making sure that as we design and architect new systems, make sure that we fully understand all the lessons learned from the from the now historic COEs. But we also extend these troubleshooting capabilities just beyond the AWS environments. We're,
we're here for developers, for SREs and DevOps engineers, making sure that we meet you where and how you like to work. So we see that teams are building custom agents using their own LLMs on popular LLMs, and they're also using IDEs such as Kira. So let's, let's kind of take the, the, the role. We're, we're an on-call SRE and we need to fix some code, but we're not an operations expert on this particular application. The, the CloudWatch MCP server connects the AI models directly to all of the tools inside of CloudWatch. And so the SRE can actually ask questions in natural language, but I,
I think there's actually a pretty cool and amazing breakthrough. So you don't have any, you don't need any specialized operations knowledge. So I go ahead and I ask Kira, and I, I literally just ask a question. Can you investigate why payments are taking too long in the billing service? So Kira does a couple of things. It analyzes the alarms that are firing, it gets the, the business SLOs, it takes a look at some historical patterns. Then it investigates the log anomalies and it correlates the metrics with the deployments. And then it points to the exact method call, the one that actually caused the SLI breach, and then it goes to the next step. I think this is actually super, super amazing. It suggests actual fixes and optimizations. Now all this happens through natural language interaction, so there's no dashboard navigation, there's no query languages, no switching between pages and tools. So everybody on your team is now empowered to troubleshoot like an operational expert. This year I've spoken to developers in, I just counted yesterday, 14 different countries, and I actually have been able to get some really interesting senses of, of what some of the more, more um, Interesting challenges are. And one of the things that they tell me, and they tell me in a lot of different ways, is,
but they want to get the make sure that the operational metrics and the production telemetry, they somehow want to get that side by side, the source code, so they can actually see in a very live sense how their code is affecting operational metrics. And so we're bringing this intelligence. Directly into GitHub, this AI powered intelligence. So we know that as developers, we often spend a good part of our day in, in GitHub. So the, the new AWS observability GitHub action, it plugs in seamlessly into, into your developer workflows. And it, as,
as I'll show you, it triggers analysis with CloudWatch and with the application Signals MCP servers. So here's how this works. So as a developer, I,
I can open up an issue. And all I have to do is I mention the, the AWSS APM GitHub user, and maybe I and again in natural language, help me investigate availability issues in my appointment service. So, the AI analyzes the, the live production telemetry, it's got access to the source code, and it jumps in and it identifies the actual root cause. So here it's, we,
we, we. It comes, it says remove this line from from the function function 3 different version lambda function. Go ahead to line 27, and we actually, you can see there that it's actually raising an exception on a particular ID. So it gives you a very specific, very detailed fix. And from there, if you look at that fix and you say, OK, and here you have to make sure you, of course, as the developer understand that this is the right fix, you then ask the AI to implement the solution. So from there, it's going to automatically submit the pull request with precise fixes based on both the telemetry and the code analysis. So at this point, you're now in this intelligent developer partnership where observability lives directly inside of your native environment. Again, the modern operations that we're talking about today, we need to meet you as a developer and operator where you work. So we want to support you with your tool and tools of choice. We want to make troubleshooting fundamentally easier. So again, as we often like to say, focus on what matters, building exceptional applications for your customers. Thanks again for watching. Thank you, Jeff. All right, so Jeff showed us how AI can help with troubleshooting and it accelerates troubleshooting for you, but let's think about an instance where you're overwhelmed with data. So AI helps you scale, but it also generates exponentially more data. This creates new operational challenges. While AI helps you solve some of the old ones, it's automatically also creating more challenges for all of us so think about it during a time sensitive event either let's say you're, uh, live streaming or you're in a product launch or for some of us getting ready on the run to reinvent. And you need to scale your capacity at near real time, but your data is fragmented and you don't have visibility into one single place where you can go and look at your entire estate. Now that's exactly why we are focused on simplifying operations in the cloud. It's not all just about gentic. You need to continue to improve how you operate, how do you manage your infrastructure, and so on. So first we wanna give you faster and easier insights and this begins with log analytics. We are enhancing pipe processing language or PPL to make log analytics faster and more accessible throughout. Amazon Open Search Service now with NLP support and more than 35 analytical commands, you can pull insights from any log format using simple intelligent filter, extract, and parse. For example, with just one query, you can parse your cloud cloud trail logs, filter by specific APIs. You can correlate that with VPC flow logs. You don't need any custom parser or regex patterns, and you can also start. Analyzing immediately even if you're not familiar with PPL syntax by simply asking it in natural language, show me the number of lambda errors over the past one hour and open search will do the work for you to get the right query and the right insights. Next we have also expanded CloudWatch real user monitoring to both iOS as well as Android so that we can help troubleshoot mobile end user applications for you. So now you can understand screen loads you. You can see crashes any API latency and most importantly you can see how it impacts your end users while they're viewing whatever it is on their uh mobile phones so now you have visibility into both web applications as well as mobile applications. Next, you can also correlate user behavior with logs and traces in cloud watch application signals which we launched last year in order to give you end to end visibility. So with mobile support you can also narrow down the root cause to a particular set of cohorts of users. You can do this by particular device type of operating system versions or even by location. But insights alone are not enough. For example, during a security event you are correlating cloud trail API calls. You're doing this with VPC flow logs, IAM policy changes with resource configurations, and you're doing this across dozens of EWS accounts and possibly across regions. This is no longer just a log analytics problem. It is a security investigation problem. This is why we have simplified security monitoring by launching cloud trail aggregated events. For example, let's say an S3 bucket gets 10,000 get object requests, and it does this in 5 minutes. Let's say you need to understand whether this is just normal behavior or is this data exfiltration. Now with the aggregated events, you don't need to no longer scroll through thousands of JSON events. Aggregated events lets you easily spot the pattern and answer critical security questions so you know right away you don't have to like you don't have to write custom queries you don't need to spin up lambda functions in order to parse the high volume of logs. It will tell you that 10,000 get object calls were made to your S3 bucket. And it did so with an error rate of 2%, let's say, and more importantly, it had a spike of 400%, uh, during this access. So in seconds you now know who accessed your pipelines and you know which of the resources were accessed. So Cloud Trail now does the heavy lifting for you so you don't have to create all of that pre uh by pre-aggregating for you. Next, So a lot of your data lives in silos. For example, you might have application data in one account, mobile data in another, database metrics in yet another. So when you're investigating an incident, you need to connect the dots for all of these things. This is where centralized observability really, really matters. So let's say you have logs that are across development, QA, production, and even across business units. Now it can be a challenge for you to monitor all of this across accounts and regions, so many of you have come up with your own DIY solutions to handle this with ETL pipelines to help remove this heavy lifting for you. We have launched cross account cross region. And log centralization and cloud watch, you can now consolidate log data from all of those things I was telling you, whether it's QA QA proud Dev, etc. across all of your AWS accounts and regions into one single destination account. And we're also expanding this cross-count cross region capability to support database insights with these enhancements now DevOps engineers, DBAs can monitor and troubleshoot across both RDS as well as Aurora databases. And now many of you are operating at global scale, serving millions of viewers, let's say across multiple regions. Centralized observability is super critical like we said earlier. And that's exactly what is the situation at Warner Brothers Discovery. They do this every single day, running a massive ad platform across both HBO Max as well as Discovery Plus and other properties. And to show us how they use AWS to simplify their operations and drive performance in real time for their personalization of ads, please welcome Anand Narajan, VP of engineering from Warner Brothers Discovery to the stage. Welcome, Anan. Thank you,
Nanhani. Uh,
good morning, everyone.
My name is, uh,
Anand Narajan. I'm an engineer at Warner Brothers Discovery. If you're like me, you probably turned on CNN today and checked up on the news, or maybe you caught a rerun of The Big Bang Theory yesterday, or maybe Friends, admit it, Friends again. Uh, maybe over the, uh, Thanksgiving weekend you binge watched on White Lotus or you caught up with Harry Potter with the kids. If you did any of that, or if you're going to catch the Wizard of Oz or the sphere, if you did any of that, if you're going to do any of that, you're watching content from my company. We are Warner Brothers Discovery. We are one of the largest media and entertainment companies in the world. And we work with AWS because they are a global partner. They too have a vast global footprint just like us. I lead AgTech at WBD and today, I want to talk about some of the exciting things that we are doing with AWS. We delight about 128 million+ customers in 100+ countries. Uh, on our flagship streaming services like HBO Max and Discovery Plus, you can watch the Tour de France or you can watch the Olympics in Europe, or you can watch cricket in the UK or the NHL in the US. You can also watch premium scripted content like, uh, Game of Thrones or the Superman movie. Have you guys watched the Superman movie? If you haven't, I'm not going to spoil it for you. You'll have to find out whether uh Lois Lynn picks Superman or Cluck and this time you'll have to find out whether Superman gets defeated by Lex Luthor or an AI. I can't talk to you about any of that, or what I can talk to you about is how we do streaming on AWS. You see,
we take some content, like the Superman movie, and we break it into thousands of little segments. Each of these segments is about 3 to 5 seconds long. We construct a list of those segments that's called a manifest. When a user, uh, say Nandini, presses play on the Superman movie in the HBO Max app, we deliver that manifesto in real time, within a few seconds. The app on her device, parcels that manifest, pulls in those segments and plays them in sequence, and that in a nutshell is how streaming works. What happens if Nandini is an ad supported user? Well, AdTech is unique in the streaming world because ATech has two sets of customers. One, the subscriber users, people like Nanhai. And the second is the advertiser. Advertisers care about these ads being effective, whereas users, they care about their viewing, uh, viewing experience, not being overly disrupted by ads. What that means is that these ads are personalized. If Nanhai or Jeff or David watches a Superman movie, they're going to see different ads this time around when Nanhai presses play. We will fetch those ads for her in real time, splice those into the manifest, and then deliver to her again within a few seconds. Delivering the right ad to the right user at the right, on the right content at the right time is the heart of what AdTech does. This is how we do it. When the user hits play, her request will wend its way through our back ends, which includes my ad tech stack, and all these back ends are built on top of a bunch of AWS services. That request will wend its way eventually to a third party ad decisioning server on the far right there. This ad decisioning server is the one that actually picks those ads, and it's going to log its decisions, billions of decisions in, in a, in a month. Those decisions, those logs will then wend their way back to us as a feedback loop. That's the pink arrow labeled one there. Now that feedback loop takes on the order of hours, which is way too slow for us. So we worked with Amazon. We built our own feedback loop. This time around, we take the same signals that fire when Nanhai sees the ad, and we fed that into Amazon Kinesis, Flink and OpenSearch, and constructed our own feedback loop. That's the blue arrow marked 2 there. This feedback loop takes on the order of a few minutes. To see why this is important, let me show you an example. This is one of our open search dashboards. This is our eyes on Glass. This is what we look at when we want to check whether we're delivering ads correctly. Let me just drill down into one of those charts there. What this chart is showing you is how we're delivering those ads. Those green bars represent proper ad delivery, as you can see, as the event progresses, as more and more viewers watch this content, we will deliver more and more ads, and as the event ends, ad delivery drops, which is expected. The red and blue tips at the top of these green bars, those represent problems. Those represent cases where we are either misdelivering ads or not delivering ads at all. Either way, that's a problem for us because if we don't deliver ads or we deliver the wrong ads, it's lost revenue for us. You can see now why that feedback loop matters. If it takes on the order of hours for that feedback to come back to us, the event or program may have ended by the time we even know that there's a problem. A feedback loop on the order of a few minutes is crucial for us to be able to course correct, um, and,
and, and collect revenue. All of this is, uh, holds doubly true for live events. With live events, there's all the challenge of showing personalized ads that I talked about earlier, but in in addition, with live events we are building that content manifest as the event is progressing, as users are watching it. And often we don't even know well ahead of time when the next ad break will appear. Delivering personalized ads in live events is one of the hardest problems in streaming. And as if this is not enough, when you have 100,000 viewers watching this event, they're all going to hit the ad break at the exact same time. Recall, we're showing personal ads, different ads to different viewers. So this means that we end up having to do hundreds of thousands of these decisions at the exact same time. That imposes a huge burden on our back ends, and they have to be scaled to absorb that kind of load. If we don't scale or if we underscale our infrastructure, We want your ads and we want to collect revenue. And if you overscale, we'll incur a huge cost. Our first attempt at dealing with this situation was to auto-scale based on traditional metrics like CPU and memory. But those are not quite as good for us. In this chart, what you see is the, the green line represents our traffic increasing and dropping. And the yellow line is our infra-scaling, struggling to keep up. So we worked with Amazon on an alternative approach. In our alternative approach, we developed a custom metric we call that permits, and permits bake in much more of those business factors. They take into account content duration, DVR window, number of ads. We took those permits and we fed it to Amazon, Prometheus, and Kira, and we auto scale based on that. Additionally, my teams build a predictive approach whereby we look at the rate of change of permits over the past few minutes. And use that to predict the rate of change from us for the next few minutes. Now you can see our outcomes are much better. The yellow line stays above the green line. Our infrastructure is scaling. We are confident that our infrastructure is going to scale as the traffic changes. There's still work to be done here. Ideally, what we want is that that yellow line should closely hug the green line without ever going under. So there's a little bit more work for us to do here. But all in all, I'm very proud of what we have done here. We now have a 90% fill rate, which means that when there's an opportunity to show you an ad, 90% of the time we will show you an ad. And we do this at very low latency. Trust me, if there's one thing streaming users hate, it's when you interrupt their content with ads that buffer. So we are now able to meet the requirements of high concurrency events with personalized ads at low latency with barely a dent in our uptime. And our costs are going down and in utilization is going up. This is great news for us. Our successes depend a lot on our partnership with Amazon. We've used several Amazon services Prometheus, OpenSearch, Link. Uh, and I'm very excited to see where we go, where we take this further with our infra scaling, where we take this further with traceability with open search, and in general, I'm, I'm excited about our partnership with Amazon. Thank you. All right, can't believe where did the time go? I wanted to do a long winded wrap up and everything, but my clock's telling me we're already at time, so I'm gonna rush through this. So our commitment to you is we are improving our cloud operations support for all of you, especially along your agentic journey for sure, all the while we're supporting hybrid and multi-cloud environments for you. Now we have a lot more coming, so watch this spaces and don't wait till reinvent next year. Watch for us doing more regular launches, uh, especially in this age of AI. So let's quickly recap everything we covered today. We showed you CloudWatch generative AI application map. We also Jeff showed you how to operate and use MCP servers and CloudWatch, GitHub actions, um. And then finally we showed you how to simplify operations and you saw Anan show you some of the features in uh open search as well as mobile RM. Please try that for your end users um and then I wanna especially thank David Proven and from PGA Tour and Anandan Rajan from, uh, Warner Brothers Discovery as well as Jeff. Bar for joining me here on stage showing us innovation can take many, many forms, and we are here to support you whether you are running and testing agentic AI or running battle tested mission critical applications. We are here to support your journey. Take a look at this. Take a picture of this QR code. There's some, uh,
talks here, but there's a lot more to see. There's so much opportunity here. I wanna give you one sneak peek. Please attend Matt Garman's keynote tomorrow where he has several other launches from my teams on, uh,
cloud operations in particular. I guarantee you you're gonna love it among all the other, um. Announcements that he has. So with that, I hope to run into many of you during like the Chuck talks, breakouts, etc. Most importantly, connect with the brilliant minds here. It's a rare opportunity that we all get together. And our partners, customers, the demo booths, check out the Quiro lab and most importantly have fun and I hope to run into some of you out there this week. Thanks again for spending the hour here both here as well as online. Bye.