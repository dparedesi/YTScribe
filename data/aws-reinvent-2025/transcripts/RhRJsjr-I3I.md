---
video_id: RhRJsjr-I3I
video_url: https://www.youtube.com/watch?v=RhRJsjr-I3I
is_generated: False
is_translatable: True
---

Hello, hello everybody, uh, good afternoon and welcome to Autonomous Agents you can Trust. Uh, my name is Neil LeBlanc and I am with IBM, uh, and I lead our go to market for Watson X. governance, and my name is Diana Griffin and I am with KPMG and I am focused on AI governance and I am part of the alliance with IBM. So hopefully you all didn't get too stuffed at lunch, uh, but if you did, then we're here to keep you awake and burn off some calories. Look, um, you know, I think as we look back over the course of the last 2 or 3 years, I don't think too many people would have predicted just how powerful or where we would have gotten, right? How AI would have evolved, and especially as we kind of look at agents and you look around the, the expo floor, you look through the thousands of sessions that were in the catalog. There's a lot about agents and as we kind of look at how they've evolved over the course of the last year or so, they have certainly become more powerful and it's expected that they are going to become even more powerful and they're going to become even more sophisticated, but it comes with some concerns, and it comes with some concerns around what their potential impact is going to be, what some of the ethical ramifications are going to be, and even what some of the impacts on society are potentially going to be. And so as we kind of look at the, the promise of, of, of agentic AI and and and agents, um, you know, there's been a number of talks that I've listened to, but they could be used in a number of different ways. Certainly they could be used to help uh effectively uh help humans perform tasks and, and achieve business outcomes, and they could be used to augment human intelligence, right? We think we're smart, they're smarter, but they can help us, um, if they're used in the right way. Uh, they can help automate a lot of some of the, the mundane and the, the time consuming tasks that we're, that we typically have done, uh, over the past of the course, over the course of the last little while. Um, they can help improve efficiency and productivity. Um, and they can also help. Us improve our decision making as well as the quality of the responses that were given. You know, there are studies that come out all the time, and you know this is an example of 3, and I think if we probably wait a week or 2 weeks or 3 weeks, we're gonna get more studies, but you kind of look, and the first one was uh something that McKinsey had come out with that said, you know, Agentic AI and agents are going to help contribute to an added value of about $4.4 trillion of, of, of profit annually. And if you look at something that the BCG had come out with, you know, they recognize that agents are here to stay and that they're going to lead to about an expansion of about 45% growth by over the course of the next 5 years. And Gartner has said that basically by the, by 2028, 1 3rd of all of our interactions with generative AI with G AI is gonna be uh through the use of action, uh action models or through autonomous agents. Agents are everywhere again you walk around, walk around the, the show floor and everyone's talking about agents. It's not that they're only everywhere, it's that they, they come from everywhere and they don't know where, you know, organizations are faced with a, a dilemma where they're struggling to get a handle on. Where did they originate? Who built them? How were they tested, what data were they tested on? And that's a, that's a, that's a concern, because you don't know, they don't know what is being used within a, within an organization. And so, you know, when we look at when we talk to our customers and, and Diana and I will talk about this in a little bit, but as we look at our customers and talk to them about their AI strategy. Obviously they want to get as much of a return out of their AI as possible, but that's only going to be possible. If there's good governance, if there's good AI governance also put in place, and so when we look at some of the reasons that why, you know, governance is, is critical, you know, one of the things that, you know, if we look probably 2 or 3 years ago, uh, it was probably a lot easier, maybe 3 to 5 years, it was probably a lot easier to understand, you know, when we're looking at traditional and predictive ML, you know, how many models were being used, what were the use cases that they were being used for. Then we saw, you know, Gen AI, um, and the rag use cases, and we started to lose track. We started to figure out, OK, well, we know potentially what LLMs, what foundation models we have, but we don't know what the use cases are. We don't know what all the prompt templates are, and so we started to lose sight. Now with Agenic, it's becoming even more apparent, right? We're losing. Uh, the transparency into who's building them, what, how they're being built, uh, where they're being built, and how they're being trained, and that's cause for some concern as well. The inability to explain behavior again when it was uh. You know, traditional predictive ML, you kind of knew that the models were going to run, they're going to spit some outcome or output out even with Gen AI, there was kind of some human in the loop, right? There was somebody who would put in a prompt, would get a response back and go, no, that doesn't look right, and was able to make a decision. With agents, especially with multi uh uh uh agentic systems, you know, as they start to talk to each other and as they become autonomous, the inability to understand the decisions that were made, the lack of explainability is a major concern, um, and there's still some challenges around manual error prone testing. You know, there's, there's even more concern around bias around drift, just around the general health and the quality of agents, and then there's a number of disparate tools and, and of course, not to mention sort of the lack of those guardrails, right, those safety guardrails that are put in place to ensure that they're behaving the way that they're supposed to behave and that they're being used the way that they're supposed to behave. So as we kind of look at at Watson X. governance, we kinda, not kind of, but we, you know, look at how to, how to address some of these, uh, these concerns and, and some of these risks. So the first thing that we kind of look at is, you know, how do we start the whole observability? How do we monitor the agents for their performance? How do we know that they are performing the way that they're supposed to perform? Right, how do we know that um that they're, they're producing the output that they're supposed to produce, uh, and this is a continuous monitor, uh, continuously monitoring and, and raising those alarm bells if something should happen. There's the evaluation capabilities, right, evaluating at build time, at at testing time, and, and continuously evaluating even when they're in production, uh, and ensuring that the same quality is adhered to uh while uh in production that was met when uh prior to them being deployed into production. And then the last is it's not just good enough to. Understand the behavior it's not good enough to just understand the score and the results that are coming out, but it's about the optimization. How do you know what is the right agent to do for the job, right? How do you know if something is starting to fail, if the quality, if the is starting to hallucinate? How do I go back and fix it? What is the, what are the steps that I need to take? So these are some. Capabilities that you know we we look at with with Watson X. governance and I'm just gonna walk through kind of a a quick uh a quick demo here. It's only about a minute and a half maybe I shouldn't say quick, maybe I should say lightning since this is a lightning round, um, but let me just set up the, the demo real quick we could we could remove that placeholder. Uh, Mark did confirm with me, so it is the latest one, but what we're gonna, what, what we're about to see is a scenario where. You know, somebody wants to onboard a new use case for a banking, a banking tooling. Um, and as part of that use case they want to onboard a new, uh, a new agent and so we're gonna go through and we're gonna identify the use case and identify some risks. So again, I'm just setting this up, it is gonna go a little bit quick, but basically what we're looking at here, this is WatsonX. governance and this is the, the, the what we call the governance console. This is kind of the, the landing page and this gives you that sort of that landscape of AI and how it's being deployed across the organization. So this is gonna go quick, but basically from this, from this area we're able to start drilling down and getting into the details. Now we can start seeing the inventory of the agents that have been on boarded within my organization, within my within my, you know, department, within my enterprise. We can drill down even further to start looking at some of the metadata. We can see who who built it, when was it built, uh, what is the purpose, uh, so on and so forth. The other area is around being somewhat of risk averse, right? So within what's the next governance, we have the ability through what we call a questionnaire to ask a series of questions. Now this is a bit of a cooking show, so we're not going to go through and answer all 60 questions, but we're going to get to the outcome real quick. So based off of the responses to the to the questions, it'll automatically identify what are the risks that we need to be concerned with, how do we. Go and mitigate those risks. What are the controls that we need to put in place and how do we have an audit that we know that we've actually taken the necessary steps and the appropriate actions to mitigate some of those risks and document that so when auditors come in, they'll know what steps have been taken. So all of this could happen before we either on board a new agent or do it at the at the time of the use case to understand here's the use case here. The desire. Here's what we actually want to do. Here are the agents that we or the assets. It doesn't necessarily need to be, you know, a single agent. It could be multiple. Here are the agents that we want to use, the assets we want to use, um, and then we combine the two so we understand, uh, what, what the use case is, what are the, the agents, and it might be one or multiple, and that same agent might be used on, on multiple use cases. So understanding that the relationship between them. And then last, as we're going through this and understanding the, you know, the sort of the risk posture and then as we get into the approval it's now saying yes I approve this, let's go ahead and approve this use case and on board the, the, the, the agents so a bit of a a shameless plug, uh, you know, analysts agree that, you know, Watson X. governance has been recognized as, as one of the leaders, um, with, uh, as part of, uh, AI governance and so with that I think, you know, let's have a conversation about what we just saw, what, what I was just talking about. Um, so quickly, Diana, I think, you know, one of the things that, you know, we've, we've heard that governance kind of has this sort of. Notion that it's more of a of a hindrance, more of a hurdle. I'm curious to get what is your take on on that. So I think that we should look at AI governance as guardrails and not as a blocker. Traditionally governance is seen as a blocker, but when we think of it as how do we adopt, a lot of organizations don't know how to get going and they're using. The the lack of a plan or the lack of guardrails as an inhibitor and so when you actually put governance in place and you put thoughtful governance in place and what you're going to accept, it allows your innovation to accelerate. Organizations and your your innovative people now know what they need to do in order to get those use cases passed. A tool such as Watson X allows you to build it in, have use cases already approved, so an inner, so a developer can go in and say, I can start from this use case. I can use this type of code. I know that will get approved. And go through the work flow and get it approved quicker. I have clients who have really great ideas and they're really excited for them, and they take 9 months to be even able to get the AI agent idea approved before they. Can even start to build because they don't know what the governance is by having strong and clear governance guardrails, it accelerates innovation, so we need to change our paradigm on how we look at governance and by using a tooling. It allows us to share that information and create a streamlined approach to getting it approved and monitoring it. Yes, I think that you're 100% right that so many people were nervous and saw governance as being sort of that blocker, but really it can help accelerate and help generate the return on investment on the AI investment that customers have made. When you don't know the governance and you don't know how to adopt it, it is, it, it actually prohibits people from innovating and also encourages them to build it on the side of their desk. You don't know what's going on in your environment and you're increasing your risk profile, so you really need, you don't want. Shadow AI you want people to create this. You don't want to actually stop them. People think, oh, if I don't tell them how to do it or if I don't give them governance, then they're not going to do it. They're doing it anyway, so you want to provide those guardrails and provide people with an instruction manual of sorts in order to know how to accelerate their innovation. But um what are you seeing as some of the the challenges that that customers are seeing in terms of being able to adopt or implement governance, implement those processes, implement those the the the practices, and, and so, and by the way, we only have 7.5 minutes, so some of the things that I personally see um is they don't know the they don't even know what they're looking at. This is all happening fast. No one necessarily is an expert in this space. People are just have maybe a few more at bats and. Others, so they don't know what to do, so they don't do it. They hide their head in the sand. That's one thing we don't know who is the AI governance committee. We don't know who the right people are, so they can't make a decision because there's no consensus. The old ways of working no longer work and so they don't, they don't wanna trust it. They think that they, they, they, they don't have the risks assessment and they don't have all these things, they're just gonna be able to stop it. You're not stopping it. So you're not governing it, you're not, you don't know what the words are and you don't know how to get going, so you're just not doing it, but that's exposing your organization to so much risk. Yeah, I think you know if you, there was funny stories where if you asked the, you're in a boardroom and you ask a group of people whose responsibility is it who who for AI governance, everybody would look around and go, well, maybe that person, but at the end it's everybody's everybody and and you're right that they don't know how to to get started absolutely um. What, what are you seeing? How do you see sort of the agentic and agents sort of increasing the need for uh for agentic? I think that with the increase and the promise of agentic and agents, it makes AI governance even more important because you need to know that that agent is now autonomous potentially is actually doing what it says it's supposed to do. And through a tool like Watson X you can monitor it for drift, you can monitor it for bites, you can actually set these thresholds and understand, is my agent performing? Is it doing what I said it's supposed to do? Is it acting in the use case that I've designed it to? And is it continuing to perform? It allows you to get out of pencils and spreadsheets and allows you to have a better understanding that it is actually doing what it says it's going to do. I think that's so important with the invent with agents and the adoption of agents, especially agents doing agents upon agents. If you don't have a governance tooling, how do you know what's happening? And with a dashboard you can see if it pops up if there's a problem. Yeah, I, I just think that the whole autonomous nature of things and, and not knowing, as you said, sort of the agents on top of agents and you know I I I like I said before. If you put a prompt in, if you ask a question, sort of the Gen AI, the Raguse case, you get a response back, as a human you can make a decision about whether or not it's, it's the right response. With agents, if you make a small mistake on that initial agent that's calling the second one, it's just going to continue to get amplified. Um, is there, is there anything, um, in specific that you or could, could, uh, guide or advise, uh, the audience on when assessing agents and, and looking for, for different kinds of, of, of agents to, to be used? So when I look across the landscape, I start with the process and I identify where the pain points are in your process and I. Focus on outcomes. I know probably a lot of people talk about this, but you should not automate your tasks and your the specific things you're doing now. You wanna look across your landscape of your process and identify where your opportunities are and then automate that that outcome level and then you can have agents automating on top of monitoring that to make sure it's performing as planned, but you don't want to just. Automate your bad process. Like I, when I used to do software development, people would say I really want this red button. They didn't really want the red button. That was their pain point. Don't automate that red button. Look at what your outcome needs to be, and that's how you can design and build your agents, OK, um, any. Any sort of last thoughts that you wanted to share um that I didn't ask you about or I think to me one of the things I'm most excited about as adopting and looking across this is how you you can get so much more with this this is no longer prompt engineering. Or machine learning those were ways that you could have done those things using your traditional means with the evolution of where we sit today, you have to think about an AI governance tool. If you don't have an AI governance tool in place, it's going to make it really difficult for you to know that the agents you're installing in your building are actually performing you're getting the value from them, and you don't have a bunch of people. Building things on their desk, um, their desk that are now in your system that expose you to penetration, expose you to bias or other things that could cause a lot of risk into your environment and I think that a tooling helps you understand what you're building, what you're what you're deploying, um. IBM Watson X is hybrid for purpose open, so it's not telling you how to build your agents. It's not telling you what tooling it can do. It can monitor your entire environment. I think it's a really strong platform, uh, that really opens you up to accelerate your innovation. Great, perfect, thank you. Um, so I think with that, you know, there's a couple of things. So obviously we're working on, uh, what's the next on governance on, on delivering out a, uh, uh, uh, a development kit around Agent ops, um, and we have a booth back there with a big IBM sign and there's a big bumblebee outside. So with that, thank you very much for your time today and enjoy the rest of the conference. Thank you.