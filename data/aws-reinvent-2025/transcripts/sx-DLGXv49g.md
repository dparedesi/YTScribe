---
video_id: sx-DLGXv49g
video_url: https://www.youtube.com/watch?v=sx-DLGXv49g
is_generated: False
is_translatable: True
summary: |
  Fin Nguyen outlines a four-stage LLM journey to cut cost, improve performance, and gain control, then Amwe Shiv shares practical patterns from the open-source AI on EKS project. Stage 1 (foundation): put an AI gateway in front of model providers (OpenAI/Bedrock/etc.) to centralize cost visibility, policy enforcement (PII filters, safety), audit, and key management. Options include Envoy AI Gateway (K8s), LiteLLM (developer/Python), or managed OpenRouter. Stage 2 (optimize inference): naïve Hugging Face + Flask yields ~40–50% GPU utilization. Understand transformer pipeline (tokenization CPU, prefill GPU, decode memory-bound) and context-window memory growth; use inference engines like vLLM (60k+ stars, 100+ architectures, supports Nvidia/Trainium/Inferentia/AMD) with paged attention (GPU virtual memory), continuous batching, and quantization (FP16/FP8) to reach ~80% utilization, ~5x throughput, and ~80% cost-per-token reduction. Other engines: Triton, SGLang. Stage 3 (cut latency with cache): output-heavy tasks (reasoning, tool use) reprocess prompts; strategies include KV-cache offloading to multi-tier storage (NVMe/FSx/S3 via GPUDirect), prefix/semantic caching, and capacity/model-aware routing to maximize cache hits. Benchmarks (NVIDIA Nemo/Dynamo) show 3x faster TTFB and 2x end-to-end latency, or 2–12x TTFB gains for chat. Open tools: LM Cache, Mooncake, NVIDIA Nemo/Dynamo. Stage 4 (scale/disaggregate): distributed inference when traffic is high or models don’t fit a node. Parallelism dimensions: data/tensor/pipeline and expert (MoE like DeepSeek, Qwen). Expert parallelism uses all-to-all token exchange (more complex comms). Disaggregated prefill/decode clusters sized separately; routed KV cache (e.g., with Nemo) improves throughput (1.3–2x on 1–2 nodes; scales with more). Frameworks: AI Bricks, LLMD, Dynamo; Amazon Rufus used vLLM to scale to 80k Inferentia/Trainium chips on Prime Day.
  
  AI on EKS (open source) provides modular, cost-effective architectures (training/inference/ML ops/agentic), optimized for x86, Graviton, Inferentia, Trainium, with Karpenter-driven elasticity; swap components (vector DBs, engines). Blueprints/charts for inference (vLLM, Triton), training, observability; guidance on perf/cost. Practical patterns: deploy an inference-ready cluster; a Helm chart starts a vLLM pod, Karpenter spins GPU nodes on demand. Benchmark with your own data (inference-perf, vLLM’s guide-llm) to meet SLOs. Ray-based autoscaling adds replicas when queues grow; scale down when idle. Startup optimizations: seekable OCI (Sochi) parallel image pulls and model streaming (RunAI) from S3 cut startup from ~8m to ~3m on cost-efficient nodes (63% faster). Distributed inference leader/worker template sets pipeline/tensor parallel sizes; pin instance types to avoid slow shards. LLM gateways centralize routing, retries, guardrails, and key management (e.g., route large requests to frontier models, retry on failure). Advanced: AI Bricks controller + Envoy for load-aware routing, LoRA adapter mgmt, mixed accelerators, distributed KV cache. Resources: generative AI on EKS workshop, AIML observability stack, GitHub repo, AWS Skill Builder.
keywords: AI on EKS, vLLM, Karpenter, inference optimization, KV cache
---

Welcome to OPN 414 VRM on 80 B testing to production and everything in between. My name is Fin Nguyen. I'm a senior Gen AI specialist and I'm joined by Am Wei Shiv, who's a senior open source ML engineer. I'll be covering the LLM journey and some of the findings, working with many customers. And AMW will be covering uh the AI on EKS project and some pragmatic uh LLM deployment patterns. OK, let's get started. I think we can all relate to this story. At the beginning of the LLM era, it was common to prototype using OpenAI, build your first Q&A chatbot, uh, and then when you move from prototyping to production, uh, often, uh, you might get surprised by the real cost of running this application in production. The good news is uh open source has really caught up to frontier model, promising better cost, more model choices, and provide better control, opening up LLM to more use cases and applications. So, uh, today, I'm gonna walk you through a four-level journey that uh organizations of all sizes, enterprise and startup are using to deploy open source model in order to reduce costs, improve performance, and a better control for your LLM applications. OK, the first stage is foundation. So, you know, similar to the previous story, it's very common for many teams swipe their credit card and start using, you know, model providers such as OpenAI or cloud or even Bedrock. However, this creates a number of challenges. This does not provide a centralized uh cost visibility, very difficult to enforce different policies, uh, and does not provide uh any compliance. So the solution to this is pretty easy, is to put an AI gateway in front of all the different users and all the different requests. An AI gateway provides the ability to track cost by user, team, or project. Allow you to enforce different policies such as content filtering for PI data, uh, as well as different safety mechanisms. Allow you to audit everything and control access, uh, you know, by storing different API keys, allowing, uh, you know, team to only access uh what they are allowed to access across model provider or model uh names itself. OK, so now you have visibility and control, um, you know, we can move on to the next phase. I should mention also, there's a number of frameworks out there that allow you to implement those uh type of gateway. If you're a Kubernetes user, Envoy AI gateway might be a popular choice. If you, you know, more uh Pythonic or more developer-oriented, Light LLM is another great option. Uh, and then finally, if you're, you know, looking for a managed gateway, open router uh could be a great option. OK, now we have control and visibility. Let's move on to the next level or to the next stage. OK, so maybe you've gone to a hugging face um and uh picked a model, you download the model and you use hugging face transformer library, uh and maybe you put it behind a web server, such as FAST API or Flask. By doing that, uh, you can only achieve a 40 to 50% utilization of your GPU, right? And so you need some kind of optimizations in order to maximize the resources that you are using. So, you know, just by taking a model and deploying it, your infrastructure is underutilized, your costs are high and your user might be left frustrated. So before we cover some uh inference optimization, it might be worthwhile to take a small detour and understand the transformer architecture. Um, also, one thing to note is that the whole industry has converged on the transformer architecture, and many models are using this type of architecture or, you know, small variances of it. OK, so there's 3 different stages, the tokenization, which is the process of breaking text into uh tokens, and this is CPU bound and uh very often, not the bottleneck. And then you have the prefill stage, which is the process of processing your prompt to create a KV cache representation of your query based on the model that you've picked. And this is uh GPU bound or accelerator bound. And then finally, the decode is the process of using that KB cache and predicting the next token. And that process is memory bound and not using a lot of GPU. Another factor that might impact the resources of your application is the context window. So, you know, with the latest model uh allowing you to uh stuff a very large uh input sequence, sequence length, um, you may need to understand that um you also need a large memory in order to uh process and store the KB cache. And so, as you double the sequence length of your input, uh, you also need to accommodate and double the memory resources for your application. So while a model can really accommodate a very large input text, it might be worthwhile to understand what are the upper bounds of what you're allowing in your applications so that you can plan your resources accordingly and mitigate some other memory errors. So, you know, VLLM is an infants engine. It is open source. It is part of the Linux AI and Data Foundation, as well as the Pytorch Foundation. It is very popular. It has over 60,000 GitHub stars, very active as well, uh, over 800 PRs per month um by 1000, over 1700 contributors. So VLNM provide a single platform to run over 100 model architecture across different accelerators. It's not just Nvidia GPUs, it's ranium and influentia, as well as AMD and other accelerators as well. already. So going back into our use case, if we now use an infants engine such as VLLM. Out of the box, it provides a number of optimization. So page attention, uh, you know, think of it like a virtual memory for GPUs. So instead of loading the entire model weight, you can page through those weights, allowing you to have more resources to process your batches. Continuous batching is another technique. So, you know, if you were to consider a batch size of one, you will never, never be able to really optimize the GPU utilization. Um, but however, you know, you'll get a better latency. If you have a batch size limit very high, you will maximize the throughput, but you will compromise on the latency of your application. So continuous batching is the best of both worlds. Dynamically, it will resize the batch size so that you can get a good throughput without compromising on your latency. And finally, quantization, if you were to download a model from hugging phase, it is by default in FP 32, precision data type. And so there are some techniques that allow you to reduce the precision type to FP 16 or FP 8, therefore reducing the memory footprint of your model and allowing for more resources for processing your batches. So by using an infant's engine, Out of the box with the existing optimization, you can increase your GPU utilization to 80%. You can expect a 5x better throughput and an 80% savings per token. VLLM is not the only inference engine out there. There are others such as SG Lang and NVDR Triton Infant Server. And they are all open source as well. OK, great. Let's move on to the next level. So you've optimized your infrastructure, you increased the GPU usization, but you know users are still complaining about the latency, especially in certain scenarios. So maybe you build the rag applications or maybe you, you know, via MCP build tool calling. And so part of building those capabilities, you need to provide some instructions and system prompt that need to be sent to the model as you are calling those different tools. So, you know, this is to say that every time you are calling your VA application or you know searching for your Python documentation, you'll be processing your system templates over and over and over again. So, you know, one could argue that not all tokens are created equal and some tokens could be, you know, maybe cash or you know reuse so that you can really focus on the new token or the new text being sent to the application. Another um trend or phenomenon that is compounding the problem is the shift from input-heavy token to output-heavy token. Uh, this is, uh, I call it the grade token ratio inversion. So, at the beginning of the LLM era, remember, summarization or translation, uh, you will stuff your input, um, context window uh using VAG, uh, and then the output will be dramatically smaller. And then as we move to the optimization phase, uh for uh multi-turn chatbot and Q&A, uh, you know, code generation, you'll see a more balanced, um, you know, ratio between input and output to finally, to today, reasoning models, right, where uh they are creating a lot of intermediate token that it needs to, uh, you know, take and iterate in order to create a final output. Right? For use cases such as deep research, uh, coding agent, uh, and multi-agent systems, um, you can, you know, guess that a lot of those tokens are context being carried over across the thinking of those models and therefore uh not very optimal uh if you need to reprocess those tokens all over again. So caching is needed in this case. So, in order to work around those memory management challenges, there's um a few techniques. Uh one of them is called KV cache offloading, right? So if you see the GPU memory is quite finite, right? And so, how can you have a system that offload the KV cache across different memory or storage data is, you know, what um you know, new frameworks are coming up with. Nvidia and NIO is a new framework from Nvidia that allows you to transfer the KV cache across two different GPU across two different nodes, but as well across different storage and memory tier as well. NVME, FSX, and F3 are all storage options that allow you to load the data directly into GPU and bypassing the CPU and the operating system. Another um architecture evolution is, um, if you consider, uh, I have processed the beginning of a conversation in a worker note and I've processed the KV cash somewhere, if you consider a traditional load balancer. Um, they don't really have any awareness of whether Kiwi cash has been processed, right? It is maybe using a round robin algorithm and so therefore, you know, it doesn't give you the ability to reuse that KV cash that was processed previously. And so, you know, with KB cash offloading, it's pretty common to see those AI routing mechanism, uh those AI uh KB cash model and capacity aware so that you can route the request to a worker that, you know, has processed the KB cash previously and therefore optimize cash hit. So, you know, to put it together, maybe you want to use different strategies such as prefixed caching, KV cash offloading, or maybe semantic caching coupled with an AI router and you can accelerate or you can get a better time to first token because you can skip the prefill and reuse the KV cache across different requests, across different users. Some benchmark. These are benchmarked from Nvidia Dynamo, which is a platform for running those types of solutions, and they tested with 100,000 requests using LAA 70B for an average input of 4000, and average output of 800, and they found a 3X better performance for time to first token and a 2X better latency overall. Another benchmark they've conducted for KB cash offloading on 20 multi-tre conversations with 15 users um for Queen 38 billion model, and they've found a 2 to 12x better improvement or better performance for your time to first token. So, you know, when we talk about TV casual floating and different frameworks out there, uh LM Cash, Mooncake, and NVdDia Dynamo are all open-source framework for you to use uh to implement those types of solutions. OK, the last step in our journey is called scale or distributed inference. So this is not a journey or a path I would recommend to everyone. If you can stick to a single node and keep it simple, that might be better, but you know if you have use cases or needs such as a high volume of traffic, we're talking, you know, 1000 requests per second or a minute, or maybe you need to accommodate a model that cannot be contained within one node, these are, you know, good examples for you to consider how to do distributed inferences. Um, one thing to know is, uh, the three dimensions of parallelism, right? So when you're going distributed, there are usually different strategies on how to do that. Data tensor and pipeline. So data is when you duplicate the model across 2 different nodes, shard the data, and pass the data across 2 different nodes. There's tensor parism is when you shard the weights of a model and you have the data passing through the different weights. And finally, pipeline parism is when you shard different layers of a model and you pipeline the data across different layers across different nodes. And now there's a new dimension called expert Paism. If you consider all the latest state of the art open weight model, such as Quan, Deepsik, or Kimi, they are all using a mixture of expert model, right? The big difference is that they are a lot more efficient, right? So Deep seek has 256 experts, and when you perform infants, you only activate 8 experts, so a fraction of the weights in order to, you know, perform inferts and get, you know, better performance overall. So when you do expert parallelism, you're sharing the different experts across different nodes. When you do expert paraism, um, they have a different communication pattern, um, so increasing the complexity on how you architect, uh, those different paraisms. For example, expert Parisson use an all to all token exchange or peer to peer. Uh, they have 3 to 6X more communications. Um, the communication is asymmetric. It's between different experts and, and, and so therefore very hard to predict between which nodes the communication is gonna occur. Contrasting this to data tensor and pipeline, where, you know, communication patterns are all, uh, you know, very symmetrics and uh very uh fixed volume uh in nature. Another pattern in inference engine is called disaggregated architecture. Remember, a transformer model has two different stages, the pre-fill and the decode. So when you are running inference on one node and you have completed, you have completed the prefill and performing the decode very often because decode is memory bound. Remember, your GPU will sit idle during that process. And so now, many inference engines are adopting a disaggregated architecture. So naturally, you want to decouple those two different stages in their respective clusters so that you can allocate more adequate resources for each of the stages. So maybe more GPU or more flops for your prefill clusters and then maybe more memory optimized for your decode clusters. And note here that uh Nvidia Nixo is being used to transfer the KB cache across those different clusters. So now you can allocate better resources, decouple, and scale independently those two different stages. OK, so now, if you put everything together, disaggregated architecture with prefill, and decode, and maybe you're using those four different levels of paraism, right? Data, tensor, pipeline, and expert for, you know, maybe serving deep seek, uh, you can really scale and increase the throughput as you add more nodes to your clusters. So this is another benchmark from Nvidia and they've benchmarked this aggregated architecture on one node versus two nodes. So on one node, you can see a 1.3x better throughput, on two nodes, a 2X better throughput, and this suggests that as you add more nodes to your cluster, you can even increase even more the throughput for your applications. So I, I talked and mentioned about Nvidia Dynamo quite a bit, but there's other frameworks that allow you to implement and run and architect all those different capabilities. AI bricks from ByteDance, LLMD from Red Hat, and a number of other companies are all also open source options for running uh advanced um you know, infants workload. OK, to recap, we talked about, you know, setting up the foundation. How can I set up an AI gateway so that I have more control and more visibility across, you know, different model providers. By using an inference engine, I can increase the utilization and optimize my GPU consumption and increase the throughput. By implementing a number of memory or KB cache uh strategies, I can reduce the latency and reuse the KB cache. And finally, by using this higher grade architecture or different palism strategy, I can perform distributed inference and increase the throughput even more by um you know, uh adding more nodes to our clusters. And there's a great blog post, and this is how the Amazon team or the Rufus team more specifically, have used VLLM to scale Wufus, the Amazon shopping assistant, to over 80,000 influential and triennium chips during Prime Day. OK, with that, I'm gonna hand it over to Henri. Henry is gonna talk about AI on EKS project and some um practical uh deployment patterns. Thank you. Hi everyone. So we are going to talk about the practical side of everything Fi just mentioned, and to do that we need a couple of things. We need infrastructure under which we can actually deploy these patterns. We need deployments, so blueprints or charts or something to actually implement these patterns, and then we need guidance to actually know how to use these. And so we have this open source repository called AI on EKS. Uh, please feel free to visit it. Uh, basically it has um kind of a three-tiered strategy of what we offer. It's exactly those layers. And so on the infrastructure side what we have is purpose-built architectures that are able to give you training or inference or ML ops, Agentic, basically all the components you need to run whatever workload it is you want to put on top of it. The infrastructure is very modular, so if you want to run, you know, like a vector database, for instance, and we provide one tool, but you want to run a different tool, well, you can swap them in and out really easily. It's optimized for a bunch of different hardwares, so we start with just traditional X8664, but of course we also offer AWS accelerators like Neuron and Inferentia and then Graviton as well, and it's very cost effective, so we use this hardware optimization as well as the fact that we keep the infrastructure really lean and then we use Carpenter to scale it out when we need to grow the environment that we have. On top of that, we have the deployments. So depending on what it is you want to do, we have blueprints for inference, for ML ops for training, Agentic again just to keep aiding you in your journey, whatever that might be. We of course are flexible. We know that today maybe you know our talk today is on VLLM, but of course there are other inference engines. We talked about AI bricks versus LLMD, other sort of productionization stacks, so we want to make sure that we're flexible. Again, just like the infrastructure is scalable, so are our deployments. We want to make sure that we're cost effective there and then that we support the appropriate hardware. And then finally on the guidance front, we want to make sure that we provide you with all the information you need to make effective decisions in how you want to approach things. So we try and optimize for performance and optimize for costs, of course we want to give you practical guidance so we talk about why we chose the tools that we did and some alternatives if you want to consider them. Everyone here is, uh, you know, we're talking about the LLM journey. Everyone here is at a different step. We know some people might have never deployed an LLM, might have never used one. We want to make sure that we support you at that point as well as anyone who's going to a global production. And then of course just like with everything else we do, we have a whole host of different topics that we're going to talk about there. Uh, I apologize for the very dense slide here. Um, this is to show you what we're going to use to kind of put into practice some of the things we talked about today. So the inference ready cluster is one of our infrastructures. It's the core one that we use to highlight inference. And just to highlight a few things here, you'll see we use 4 availability zones. In this case we're in US West too, so that gives us that flexibility. The reason we do that is we want to give you the best success at actually getting a GPU instance. We know they're not always available in all availability zones. But we also take into account the fact that going across availability zones adds latency, it adds cost, so we make sure we handle that in the deployment by making them topology aware. We also provide all of the controllers that you're going to need to go through this journey of deploying your first LLM, benchmarking it, and so on and so forth. And then I just want to show you how easy this is. 3 lines you clone our repo, you CD into the infrastructure, and then you run this install script. And what this does, if you look on the right there, this is a file that's available in every one of our infrastructures. This is our customization. So basically you can see. We have these enable lines. That's the modularity we talked about. You don't want to use the observability stack, turn it off. Actually, everything's off by default. You can just delete it. You don't want to use AI bricks for some reason or you want to use something else instead. You change that line. You want to use fewer availability zones. You want to use a different region. We try to make it really easy. So that's the infrastructure. Let's talk about the deployment component. So we also offer a set of charts to go with this infrastructure, and so you know we're talking today a lot about LLMs, of course that's a very popular topic, but we recognize there's other things you might want to do on this infrastructure or just in general. So we also provide diffusion support if you want to do text to image. We've got a few models there that you can use. Uh, like I said, we're all about choice, so, uh, VLLM diffusers, LA CPP, if you want to do something on graviton or, or arm-based processors, and of course a whole host of inference servers and more. Uh, we just published some benchmarking that we're going to talk about, um, and then we have some of the other controllers that are available right there. And again, of course, I want to show how easy it is. To use the charts on the left there is the chart. Most of that is actually optional, but you can see for instance we have a model up here. You want a different model, just change the model name. This is the exact model name that's from hugging Face, and then over here is the deployment. You know, once you add and update the repo, you don't have to do that again unless you want to pull a new version, but you can see here this this is this file here. And uh if you go to the repo, you can see we have a whole host of other templates you can use if you want to do something else. So with that in mind, let's talk about the actual journey. We know that everyone here, uh, whether you're just starting out, um, or sort of on the end or somewhere in between, we're, we've got your back and we're gonna help you out. But let's start at the beginning. So you've never deployed an LLM before, you just kind of want to check it out. We call this model testing and so uh in a model testing scenario, you just want to see, does this model work? Uh, does it give me some sort of semblance of reasonable output, how do I use it, um, and so on and so we're going to use uh Queen 3 for that. We also want to keep the infrastructure small so like I said, by default, very slim infrastructure at at steady state, and we're going to see how we actually scale up to bring in a model server. It's consistent, so you saw that helm file a second ago. Everything you're going to see is going to use a very similar helmM file, and all the endpoints are going to stay the same. So if you're testing you know VLLM with Queen 3 and Triton with some really big model, for instance, everything is as consistent as we can, so you can swap in and out. And it's customizable, so we have all the model parameters exposed. So if you want to override things or change things, that's all possible to do. Uh, our, our, uh, helm chart there. So if we zoom in on our inference ready cluster, we're going to throw out all of the sort of controllers and all of the pods that just keep our infrastructure running, and now we're just going to focus on model serving. So we have our inference cluster and we want to deploy a model. Well right now we don't have any nodes that are going to support it because we're not going to bring up a GPU node when you don't have any GPU requests. So you ran that install file on the helm chart and now you've got a pending VLLM pod. What's going to happen? Well, Carpenter is going to realize that there's an unscheduled pod, and it's going to bring in a node. And then to actually get the model started, we need two things. We need to pull and start the container, and then we need to bring in the weights. So you'll see here if you're just starting out, probably using Docker, the Docker image. So we're going to pull that in and then we'll pull the weights, and now we've got a running model server, we can actually start consuming it. So whether it's a pod that's in your infrastructure or if you're port forwarding or using an ingress and connecting to it externally. Uh, now you have a model that you can actually send and receive requests from. So you'll see, um, you know, in this case we talk about VLLM. Of course, we talked about Triton a little bit, uh, there's SG Lang, there's, there's other, uh, model servers, but we just wanna show you how easy it is to prove to yourself that VLLM works, uh, and, and it gives you this performance, and Triton can also work and gives you, uh, you know, uh, what it does. So if we look at our charts right there, um, we go from VLLM to Triton. And we change our image, same exact configuration file otherwise, and you can deploy Triton and get a very similar endpoint other than the Triton versions, but Very similar, very consistent, and then just like before, we skip a few steps, but now we have a Triton model server running up and we're able to send and receive requests just like that. So we have a model server running we're able to see, OK, it works, um, and now we actually kind of want to understand how well does it work and that's where benchmarking comes in. So we use benchmarking because we want to see, does this model actually give me usable output. Does this model server give me good results because there's a few different components and so we're able to switch and test different things. We want to optimize our parameters to make sure that they meet our SLOs. We want to test those components and then really importantly, what you know if there's maybe one or a couple of things I want everyone here to leave with is test it with your own data. Synthetic data is great. But because of the way tokens skew the performance, if you have you know a different distribution than what you're actually going to see in production, your model server will perform differently. So that's really important to bring your own data and be able to test with your own data. Again, we're gonna, we're going to look at what that looks like, so we have benchmarking here and instead of, you know, our own model consumer pod, now we have a benchmarking framework that's going to run a test and actually give us results as far as how this model server is performing. In this case, uh, if you go to the QR code, we have a very deep guide on using inference perf, but because we're talking a lot about VLLM and the VLLM ecosystem. There's another tool called Guide LLM that's part of the VLLM project that actually does um you know, complementary but also does a few other things that are really nice. We don't yet have the chart for that, but I do want to show you really quickly, um, it can run tests and and give you this output. The really nice thing it does, which I really like is being able to subjectively at first kind of understand how well is the model actually performing. So if you look, you know, I'm not going to claim that this is our data set, but this is a data bricks data set, but it is a real data set rather than just being synthetic data. So you can see in the data set over on the instruction side, these are the prompts, and then over here is sort of what the response should be. And if you look, I took a small excerpt of what I actually got from it. And you can see maybe the output, you know, here it's summarized as being 2000 versus giving you the full date. If that's close enough for you, great, but if you need maybe more, maybe you need to tweak your prompt, maybe you need to tweak, get a different model, this gives you a subjective sort of view of is this doing what I expect it to do. The next step, of course, is to go into deeper evals and of course unfortunately we don't have time for that today, but happy to happy to talk about that in a different setting. With benchmarking sort of done, you've now optimized a single instance of a model server to meet your SLOs. You have to start thinking now about what are we going to do with this model server once it leaves our sort of small environment. Ultimately we're going to send requests to it, and we need it to be reactive to those requests. Are we going to scale this up and have 200 instances running all the time and we won't know whether those requests are needed? Um, or, or whether all those replicas are needed, or are we going to start really low and cause some issues because things are timing out or the latencies just aren't good enough. So that's why we do model scaling in this case, uh, we're gonna look at Ray uh as part of the VLLM ecosystem and so what we wanna make sure is that as we scale up. We scale up because traffic is increasing. We're not just doing it just to, you know, just in case, because if you scale up your replicas, you are consuming resources whether or not they're actually in use, and that means you're not able to run other workloads or your costs are going to increase for no reason. On the other hand, we also want to make sure we can scale down so as that traffic drops, maybe you're very regional as far as sort of the traffic patterns and you find that uh overnight maybe you're not getting as many requests you want to make sure that you can scale down for that and aren't doing it just rules based. Of course it's more cost effective, and we want to make sure that just like we provide the configuration for the model server itself, we also make auto scaling configurable. So of course we have a chart for it. Um, if you look, uh, now instead of uh just VLLM we are asking for ray VLLM and then we expose some options that are specific for auto scaling. And so let's, let's look at what that looks like in practice. In this case you can see we have a consumer and it's sending a lot of requests way faster than our responses are coming back and so actually Ray, the head pod is actually going to be able to see that there's an issue that these requests are queuing up and what it's going to do is it's going to automatically create another replica. And of course we're not having a server just sitting there for it, we're being reactive and you know being cost conscious, so we'll have to scale up our replica or in our instance, then pull all the container, pull the weights, and now we're actually able to to adjust and meet those requests and response times. And then of course as as those drop, then we scale out or scale our model servers back. The problem with this, if you've ever been in the scenario where you're waiting for a container to come up, is that these containers are very large, probably 10s of gigabytes, and the model weights are very large, also in the 1s plus of gigabytes. So if you think about what you have to do to be able to do that is you have to create a node. Then you have to wait for the container to pull, and only then can you pull your weights. This is a time consuming process, and if your users are waiting for you to do that, they might just give up and go somewhere else. So there's a few strategies to kind of address this. On the top there we talk about caching, um, the actual container itself, so you can do that with an EBS snapshot. So when your node comes up, your container images are already there and you'll pull them instantly because it'll come out of the cache. And likewise you can do the same thing by either putting your model into the container image or putting it somewhere where it's more accessible. Problem there, if you've ever done this, is you have to maintain this cache. Uh, images get updated, models get updated. It's kind of a pain. Um, so what we, uh, what we do is we try to optimize this process in, in two ways. We, we optimize the container pole using something called Sochi, which is seekable OCI. It comes by default on the inference ready cluster, and that allows you to parallelally pull all of the layers, so they come onto the node much faster. The other side, once we've got the container running, is to pull the model weights, and we do something very similar with model streaming. So VLLM has this option to do model streaming. It uses a Run AI model streamer, and so we're actually able to pull the model weights onto the container faster. So let's go through what that looks like. So we wrote a template here, uh, very simple. What this basically does is you give it a model name and it's going to put it on S3 for you. Very simple. So, uh, when you run this, it'll just pull and push over to S3. So now we have, we're not having to pull directly from hugging face, we can go through S3 because the model is already there for us. Now what happens is um you can see kind of looks similar it's fortunately it's very hard to show a parallel poll, uh, but this does happen much faster, uh, I promise you, and uh we're able to meet these demands, uh, demands, uh, a lot better but you know you can believe me or uh hopefully you believe my my benchmarks that I have here. So what we did was we ran two different benchmarks or the same benchmark on two different instances. We took one that kind of just was good enough to fit the model, you know, it's like the baseline sort of instance that would fit the model that we did, and one that was way overkill, uh, just to kind of benchmark and see like how much better is this if we have a way more performant node. Uh, we're also using a 15 gigabyte model, so just to give you sort of uh a little more context around what we're doing. So if you look at the top left there, that's the image pull. So this is just using using Docker Hub. How long does it take to get this container image pulled? On the cost effective node, 3 minutes 46 seconds. On the overkill node, 1 minute 41 seconds. That has to do with enhanced networking, faster processing, and faster storage. And then um Then we looked at Sochi. So Sochi is how we address the first part of this problem, which is the container pole. So with our Sochi parallel pull, we dropped the cost effective container pull from 3 minutes 46 to 1.5 minutes, and then on the performance from 1 minute 41 to 36 seconds. So really big difference there. So then we looked at the second part of the problem is how do we get those weights on the node faster. So with our cost effective 14 minutes 18 seconds, with our performance 11 minute 18 seconds. Now with model streaming we're able to get that the container weights pulled or the model weights pulled in 1 minute and 29 seconds, and for the performing one, you know, 1 minute 14, not, not much of an improvement there. Still a few things we can tweak, but this was kind of like, let's just see how well this works. So just to summarize, cost effective 8 minutes and 5 seconds total container start time to 2 minutes and 58 seconds, 63% improvement, and then on the performance side, 3 minutes down to 150. So if you're kind of comparing. With these optimizations that took very little to do, we got the cost effective node to start faster than the performance node without the optimizations. So not a whole lot of work on anyone's part to actually implement this, but way faster start up time on a more cost effective node. So just, just something to keep in mind as you're sort of trying to figure out how do we improve this process. So we'll take a quick detour here and talk about distributed inference. We've so far been talking about models that either fit on one GPU or up to 8 GPUs, but basically on a single node. But what if you want to use a really large model or you want to use a model that's maybe a little big, but you want to spread it across smaller nodes, maybe a little more cost effectively. So this is an option here. Um, and, and, uh, yeah, sorry, uh, should have switched sooner, but, uh, the, the thing that we, we leverage here is, you know, as, as you mentioned, those three degrees of parallelism, so we use tensor and pipeline parallelism, and of course, uh, we want to make sure that we know about where we are uh availability zone wise so that we're not spreading these nodes across availability zones which is going to increase our latency and it's going to increase our costs. So we have a chart for that, of course, you can see here we switched from just the VLLM framework to leader worker set VLLM. Here we set a pipeline size, a pipeline parallel size, so that's the amount of nodes we're going to spread this on. And then tensor parallelism decides how many GPUs we're going to use from each node. The other thing I kind of want to point out that's really important, you'll see this instance type here. So this tells us which instance we're going to use to actually deploy this model. If you're familiar with Carpenter or just auto scaling in node groups. You might have a node group that says, I just want G5 instances or G6 instances or maybe just GPU instances, and if you deploy this without selecting an instance type, you might get instances that are smaller than others. So as you bring up your pods, some of them might start while others. Might crash because they're on smaller GPUs or you're going to have weird sort of networking issues because you've got nodes that are really fast but they're bottlenecked by GPUs that are slower. So I, I would highly recommend if you're going to use this, make sure you're using the same instance type. And again what this looks like, you know, we skip to the actual sort of uh inferencing part, but we have a request going to our leader and then the leader coordinating amongst the workers how the the processing happens comes back to the leader and then back to the consumer. Now, uh, we'll, we'll take, uh, we'll talk about LLM gateways a little bit. So LLM gateways are, uh, really great for a lot of different reasons, um, routing, they're really great because you are able to do things like, um, uh, error handling. So if you have multiple model servers, uh, if one of them fails for whatever reason, you can retry that same request without sending a failure back to your client. It lets you centralize on observability. So if you have tracing that you want to aggregate rather than force all of your ML teams, you can centralize that through the gateway rather than having everyone do their own. Lets you centralize guard rails, so if you're managing uh a lot of inferencing, um, you're able to say, hey, don't return a response back that has PII in it, could be a good idea. And then of course if you're a platform team and you know we're kind of talking a lot about self-managed inference, but you can use these with external providers if you don't want to give API keys to everyone in your organization, you can centralize them in the gateway and then have all of that accounting over there and then of course teams consume them and you can track that and attribute them to teams. In this example we're gonna show that you know we have this request. One of the really nice things about the gateway being able to anticipate the best model to send a request to. So in this case, large request maybe it goes to a frontier model on a model provider uh versus one of your smaller, uh, inference, uh, self-managed, um, inference servers, uh, I'll run that again, just to show it, um, so that's when we kind of run out to Bedrock. And then the other scenario, uh, as I mentioned, is this retry. So it's going to try the small, smaller, cheaper one, maybe there's a failure before sending it back, it's going to go to the other server and then send the request back. And then finally, um, advanced optimization. So you know we've talked about how to identify, how to benchmark, maybe you want big ones, maybe you want small ones, how to to do scaling. Now we're sort of at that kind of advanced stage where we're looking at some more advanced techniques. AI bricks being a member of the VLLM ecosystem, so we chose to highlight that here. As he said, there's a lot of other, uh, a lot of other tools you can use. We also have, um, Dynamo, uh, in our, um, in our repo. So if you want to swap this out with Dynamo, that's a possibility, but the idea is that you've got more, um, context to where load balancing. So if you think about the multiple model servers and the stochasticity that comes with running these LLMs, you might have a few instances that are processing requests a lot longer than other ones. And so you don't want to round robin where you're going to jam up certain replicas, but you want to make sure you always send to the model server that has the most free processing. So that's that context aware load balancing. Um, they do, uh, LA adapter management. So if you have, um, LA adapters that, uh, it can know based on the request whether to unload or load and, and where to route them to. We talked a lot about this distributed KV cache and of course, um, you know, AWS has a lot of different accelerators. This gives you the ability to mix and match those and still maintain your SLOs. Only change is changing the framework. So if you were using this with Queen before and now you want to try using this more advanced thing, um, switch from VLLM to AI bricks and you've got this AI bricks deployment. And so here we'll show you an example of going to a model server that has a LA adapter that you need or maybe one that has less, less load on it, and based on the envoy gateway that's running and the AI bricks controller, it'll know to which model server to route this request. So like I, like we mentioned before, we know everyone is at a different point in their LLM journey. We want to be here to support you. We wanna be here to enable you. We know that for a lot of companies, a lot of the people here, running infrastructure is not, you know, what they, what you wanna do. Deploying models is not what you wanna do. You want to build and enable on top of that. And so we're here for you, uh, whether you're just starting out or sort of towards the end there. And of course with that we have more things to show, um. I'll slow down on this slide because I know there are a few QR codes. We have some workshops, so please, if you're interested in learning more, uh, we have the generative AI on EKS workshop which goes through a lot of this material through a more guided manner, as well as introducing agentic AI and observability. We have this other repository, uh, so if you may have noticed the AIML observability line on, uh, one of the deployments, uh, we have an open source observability stack for AIML so if you're interested in that, please check it out and then of course there's way more in the repository than we could talk about in, in an hour and so please, um, you know, if you, if you can check out the repository. Is entirely an open source project. We love getting issues. We love getting con uh contributions. Please feel free to open issues for future requests or bugs or anything like that. Uh, we, we are really hoping to um to engage everyone here and and make this a great experience. Finally we have, we have another resource, um, this is our AWS skill builder. So, uh, if you are just starting out on this journey or just wanna kinda level up a little bit, we have some, uh, material on AI here that you can use and, and, uh, kind of go more uh more deep into. And then, you know, finally, again, you know, thank you everyone for, for joining us today. um.
