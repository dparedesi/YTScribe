---
video_id: m6V5DsoU7X4
video_url: https://www.youtube.com/watch?v=m6V5DsoU7X4
is_generated: False
is_translatable: True
---

Good afternoon everybody. Um. Welcome. Uh, this is my first silent session, so if my voice modulation is slightly off, please bear with me. Uh, the talk is going to be strongly consistent, and I'm hoping you guys will eventually hear it. Uh, so that's good. Um, so, uh, my name is Suma Pri Nayam. I'm an engineer at AWS. I have Amrit Kumar with me, who is a senior principal in, uh, Amazon by Dynamo DB. We both are here to talk to you guys about the two replication modes of Dynamo DB Global Tables. Um, the guarantees they offer and what to choose for what, what kind of workload. Customers today are building systems that need their data close to their end users. Um, and their end users are all over the globe, are multi-regions, and this can be for a variety of reasons. This can be for reasons for low latency, um, or, uh, uh, compliance reasons, uh, geo-compliance reasons. Uh, but a lot of customers are building, uh, applications and systems, where data has to be close to their end users. They also want these data to be highly available. What do you, what do I mean by that? They want the rights which happen in any region to be available in other regions as well, simultaneously, right? Um. And for business, and they need high business continuity in case of um uh regional service disruptions, uh, if one region goes down, they, they might want to kind of have high business continuity for uh high business continuity for their applications. These requirements are making customers and a lot of applications multi-region by default. Uh, and this means the database also needs to now start growing to be multi-region by default. Danamoribi Global Tables was created exactly to kind of support this kind of multi-regional architectures. But multi-region, unlike single region, is hard just because regions are very far apart, they're across oceans, their latencies are very variable. That means when you write a data in another region, it's going to take some time to propagate to a region far away. For example, a ride from Virginia to Tokyo may take a couple of 100 milliseconds, while a ride from Virginia to Ohio may take 20 milliseconds. And this is a very different latency profile than a ride, which is replicated within a region. And if your architecture now allows multiple rights across multiple different copies of this data, then you have to reason about how are these rights ordered, how are these rights propagated, what is the What is the order in which the rights will be visible to end customers? These fundamental realities mean that there's no one single replication model that would fit every every workload. Hence, Dynamo DB Global Tables offers two different kinds of replication models for um different workloads. As you build these multi-region architectures, you have to kind of have some trade-offs. And one of the, one of the biggest trade-offs is, do you want your application to be fast and have predictable performance as it would when it writes to a single region. Uh, or do you want rights to be visible across multiple regions? Uh, if a right is done in one region, do you want it to be visible in other regions? Because this communication of rights going across regions is going to have a latency penalty, which means that you will have to lose sacrifice of the latency or you have to sacrifice consistency. And Dynamodi global tables has two modes. One is the asynchronous replication mode, which helps you have the high, low, high performance, low latency, predictable performance, um, suited for predictable performance workload, while, uh, multi-region strong consistency global tables, which is a whole mouthful to say, so we're going to call it Mercy Global Tables, uh, which is suited for workloads which requires strong consistency where right in any region needs to be visible in other regions, where if it's strong, if it's red. The other big trade-off surrounds around. What is your recovery point objective and what is your recovery time objective. Um, the recovery time objective is a objective where how seamlessly can your application fail over from one region to another region or how quickly can your application recover from a failure. Um, Dynamodibi Global Tables, um, from the ground up, from first principles was designed as an active, active architecture so that you don't have to worry about this, and you can have an application architecture which is, which has a recovery time objective of zero. The recovery point objective is an interesting one. It is the object, it's the objective that says how much data can I recreate or afford to lose um if I have a service disruption or if I have my service fails in a single region. And this is going to drive how fast you want to replicate rights or how strongly rights have to be replicated if you want a low RPO. A Dynamo DB global tables, asynchronous and Mercy Global tables have different, uh, different RPO characteristics. And this, and your workloads can now pick and choose, um, which trade-off they want to make. With this, I'm gonna hand it over to Amrit to dive deep into the consistency models and asynchronous global table replication. Thanks Ari. Thank you so much. Uh, so, I need a quick show of hands from you if you're able to hear me or not. OK, thank you. Um, My name is Amrith. I, as someone said, I work on the Dynamo DB team. I'm a senior principal engineer. I spend much of my time working with customers and The thing I like the most about that part of my job is I get to see what it is you're building with Dynamo DB. So I wanna talk to you a little bit about, uh, strong about eventually consistent global tables, and I'll hand it back to Somo who's gonna talk to you about uh the strongly consistent global tables. Um, what we're gonna cover consistency models, use cases for multi-region, uh, global tables, and, uh. Sumo actually, I think, is the brave one here. He's gonna show you a live demo, so that's gonna be fun. All right, so let's talk about consistency. When you're building applications in a database, consistency is about whether your data read in multiple places is the same or different. In a single instance database, consistency is relatively easy. You write your data, you can read your data. Things are, things are good. Dynamo DB on the other hand, is a distributed database. And your data is stored durably on multiple copies. We'll talk about uh how many copies and how we do it and so on. In order for us to scale, it is not possible for us to always give you strong consistency. Applications sometimes need strong consistency. What is strong consistency? You write the data. Everybody should be able to see the same data instantly. Eventual consistency is you write the data, everybody will get to see that data eventually. In other words, if you do a bunch of writes, and then stop, all copies of the data will converge to one place, that's eventual consistency. If you're not able to guarantee strong consistency for everybody, it makes it easier to build applications if you're at least able to say, when I write some data, I can read it back. That's read after write consistency. And the last one which I want to point out is something called monotonic reads. With eventual consistency, I said data converges to one place. But suppose you were to do multiple rights at T 1234. Monotonic um reeds basically say time will always move forward. There are database architectures where you can read 1324. We do not give you that. We always give you monotonic reeds, OK. To understand global tables, you need to understand first something about Dynamo DB. So this is a quick overview of Dynamo DB. You, the customer over here, this is your application. You connect to Dynamo DB over a network. Your request goes through a whole networking infrastructure, load balancers, and all of that stuff, but eventually makes its way to a request route. Your data is stored over here on the right. And this is shared infrastructure used by all of our customers. So, we have millions of customers, and they all share the same infrastructure. When your request makes its way to a request router, its job is to figure out where your data is. I want to read an item on which storage node is your data actually stored. That's its job. In order to do that, it uses metadata. Now, the most important thing for us is to make sure that we only show you the data which you're allowed to see. So we have to verify who you are, authenticate you, authorize you. Then we have rate limiting to make sure that you don't overdrive your database. And then we send it over uh to a storage node. Now, For availability and durability, we store 3 copies of your data on 3 of it, on 3 storage nodes in 3 different availability zones. When you want to do a right. Your right always goes to a storage node leader. I just depicted leader here uh with the crone. So, with that in mind, let's talk about how an eventually consistent read works. You send down a request. It comes to a request router. Since it's an eventually consistent read, I can read any copy. That's one of the, uh, one of the reasons why you have to understand eventual consistency in your application because when you make a get item call, You have to realize that you may not be getting the most recent version of data. Your application has to understand that. However, if you do want the uh most recent version of the data, you will do a strongly consistent read, which is also something which we support. Then we will send your request to the leader. The leader is always part of a right, therefore, it always has your most recent data. Everybody OK with this concept so far? Yes, OK. So, when you want to go from a single region set up, now this was already 3 availability zones, but if you go from 3 availability zones in one region to multi-region, you have to think about a couple more things. First, there's propagation delay between regions. Propagation delay between AZ is always much shorter than the propagation delay between regions, so this is something which you have to consider. When you write data to a table, If it's a global table, it's our responsibility to get it over to the other region. But in what quarter are your rights sent across? The guarantee we provide is that on a per item basis, Data will be transferred in the right order. In other words, it is monotonic on, it's, you're never going to go back in time. It's monotonic on the other direction, on the other region as well. Global tables are also active, active. So, your application could legitimately be running in multiple regions. We have global tables which are in 30 regions. We're responsible for the replication. Your application can be writing anywhere. But if multiple parts, multiple locations were to modify the same item, What is the item finally gonna converge to? We use timestamps for conflict resolution. So, we'll talk about that as well. And finally, we're gonna talk about failure scenarios, various different failure scenarios and how we deal with them. So, Let's talk about actually the way in which replication works. I have here a picture of a um. Global table. I'm showing two copies, we'll get to multiple copies as well. So in US East one, I have a table copy, and in another region, whichever region it is, I have a replica. Whenever you make a change to a table, Dynamo DB will first perform the rite locally. And to perform a rite locally, it means it has to be written in two availability zones. Once the right is written durably in this region, it goes to a replicator. And the replicator's job is replicator is going to read from streams. This is normally the symbol for streams. The replicator's job is to read from streams and propagate to the other region. OK. So, the sequences you perform a write in region one. Once the right is durably written, it goes to streams, the replicator pulls the stream and writes it to the other region. Fairly straightforward, correct? This is two regions. What do you do if you have multiple regions? One possible architecture is to have one replicator for multiple regions. We don't. We have one replicator per region. And the reason we do that is because we want each path to be its own individual failure domain. Let's assume that US East one has a problem writing to this region, whatever it is, that should not have any impact on this path. Therefore, your right comes in to US East One where you perform the right, it goes through streams. Now you have multiple replicators, one per target region, performing the replication into the other. The exact same scenario can be played in the opposite direction as well. US East One becomes whatever the remote region is, and it has a replicator in that direction. Replicators are unidirectional, so from US East One to destination region, replicator in the opposite direction, another replicator. And this is the reason why these are eventually consistent. You perform the right here. The right is durably recorded. If you do a strongly consistent read in this region, you're gonna see that data. There is a propagation delay before it makes its way over there, but it will eventually get there. That's the eventually consistent part of global tables. So, let's assume you have a table, it has N regions. Like I said, each region to another region is a pairwise uh replication mechanism. This is, this arrow just shows that there's a replicator in one direction, a replicator in the other direction. So what happens if you have various failure? Oh, let's first talk about uh timing diagrams. OK. Um, Time progresses forward. We have 4 regions here in this picture. I perform a rite in region one. That right is durably recorded in the region 1 and it's then replicated to region 2. Everybody good with this representation so far? Yes? OK. That replication makes its way over to Region 3. That replication makes its way over to region 2, but at a different time. Each of these is an independent replication stream. Of course, there's one more here. The important thing to realize is all these replications happen at different times. OK. They're all independent. Now, let's assume that I do a right, the right is successful, I got a 200 on the right. I perform a consistent read. Whatever I wrote will be visible in that consistent reader. OK. What happens here? I did the right The replicator has got the data. The right only happens here. A consistent read here will not show you the data. Remember that strongly consistent reads in eventually consistent global tables. Strongly consistent reads in eventually consistent global tables only reflect those rights which are in that region. A strongly consistent read in this region will reflect this right. A strongly consistent read here may not reflect that rate. Again, the right was replicated. It will see it. When you're building your application, realize that just because you do a strongly consistent read on an eventually consistent global table, it does not guarantee that you've got the latest data, OK? Um What happens about ordering? I told you that ordering is on a per key basis. Let's assume that these are two different items. On the first item, the initial state is 10. On another item, the initial state is 200. I perform a right. A equals 11. This item has been modified. At a later time, I write B equals 201. So these are two rights which occur in this order. This replication comes over here. This replication happens at a completely different time there. On a per item basis, A, or another item basis B, we will guarantee that the replications are going to be in order, but across items, we cannot guarantee the same thing. The replication streams are independent. Therefore, this replication happens here, the other replication happens here. Notice this right happens after this right. On a per item basis, we can guarantee monotonic reads. Across items, we cannot, across keys, we cannot. OK. And the last thing we'll talk about is conflict resolution. If you have an active, active system and in multiple regions you modify the same item. Which one are we going to survive? It's the last rider. So all of our regions share atomic clocks. We know exactly what time you're performing the ride. You do it right in two regions, no matter how close you think they are, one is gonna be before the other. The one which is later will always win. So how do we go about doing that? Let's assume that this is an item which we're accepting a right for. I'm going to record as part of the right that it came in the IAD region at a time stamp of 10. I'm going to propagate it over along with the time stamp. If there is a right in the other region at timestamp 11, This right is going to be ignored because this is at an earlier time stamp. Last rider wins. And of course if there's a writer timestamp 11, it'll propagate in the opposite direction. Therefore, both regions will conserve will converge to timestamp 11. OK. So, conflict resolution using a timing diagram at approximately the same time, the right is done in two regions. Both of them are going to replicate forward. This time here, 20 and 30, uh indicates the time at which the riot happened. Since this right says 20. A subsequent right, which says timestamp 30, will overwrite. Therefore, in region one, You do the right at time 20, a subsequent replication of the right at 30 is going to overwrite it. Similarly, replication into this region where there were no rights, R1 happens first, this right comes in first, this one comes in second, it is going to converge here. Take a look at what happens here. You performed a right at time 30. The replicator sent down something at time 20. We dropped it. Same thing here. The replicator came down here with 20 it dropped it. Important consequence of this is that in region one, You will see a streams notification for this and a streams notification for this, if you're building an application using Dynamo DB streams. In this region, you will see a notification for 20 and then a notification for 30. In this region, you will only see a notification for 30. In this region you will see only a notification for 30. You will never see this notification. So if you're building a streams application, realize the consequence of having eventual consistency in your global table. And the last thing we'll talk about is failure modes. A 3 region global table, US East 1, US East 2, and US West 2. For whatever reason, USE 2 is completely offline. Nothing happens to the replication between US East 1 and US West 2. That continues just fine. You can write data here, it just doesn't get propagated over there. You can write data here, it gets over to USES 1, it doesn't go over to US East 2, that's all. At some point, when USC 2 comes back online, we do the catch-up and the data in all three regions will again converge, last rider wins. OK. Slightly different scenario. US East 2 is perfectly running here. There's a network partition. Any right in US East one will make its way over to US West too. Any right in US East I can't make it over. So in this condition, all 3 tables are active. This table can serve reads and writes. It is just going to be serving data which is local. When the network links are re-established, data will come over, again, we will converge to uh event to the last writer wins. And the last one I'll talk about is this particular failure mode where one network breaks. I want you to listen to this one carefully because this is something where eventually consistent global tables differ from strongly consistent global tables when someone gets to it. Any right in US East I will be propagated over to US West 2. Any right in US East too will be propagated to US West too. Any right in US West too will be propagated to both. A right in US East 1 will not go to US East 2. We do not relay. A right in US East 2 will not go to US East 1. We don't relate it right. Therefore, a right here goes to both, a right here does not make its way over there. When the network link is re-established, that will be re-established, OK? Keep that in mind because it's different in a strongly consistent global tables. So I mentioned streams earlier on the timing diagram. Remember that your stream will only reflect those rates which actually happen. If you get a replicated right which is stale. You will not get a streams notification. You will only get all rights which are actually written. So you will get any right in the local region, and replicated rights may not happen. OK. If you were to perform a transaction in a region. We guarantee asset transactions in the region. Remember that replication is on a per item basis. Therefore, it's possible for you to see a torn transaction in the remote region. If you're building an application which needs, which needs transactions, remember that eventually consistent global tables may show you a torn transaction in a remote region. OK. With that, I'm gonna hand it back to Sumo, and he'll talk to you about strongly consistent global tables. Thanks, Amrit. Thanks for the deep dive into asynchronous global tables and how the application works. Uh, some of the slides were eventually consistent, but that's OK. Um, So The, we built asynchrons global tables, a lot of customers loved it for the workload, uh, and gained a great, great lot of adoption. But customers still wanted to read their rights from any region. Some applications got really smart about this. They said, let's take asynchronous global tables, let's assign a single region as a designated leader and writer, and we're gonna read and write from that region. This is like pretty much like my household. We have my wife as a designated leader. Everything goes through my wife in the house, right? Uh, it works fine. Um, the latency profile of the reads and writes depends on how far the regions are, right? Um, and it's very different from every region. So strongly consistent reads will have to go back to the designated region and it's, it's. Um, and, but if my wife is not at home, now my kids and I have to kind of reconcile the stuff and see who's, who's right, and that becomes a problem because once the designated leader region is not available because of a network partition or something else, then we have to figure out whether the latest right from that in that region is available to the other regions or not. Other regions have to reconcile this stuff, and you'll have to have a failover protocol in the application to take care of this new building, getting a new designated region. Which will be the leader, uh, writer and reader for all the rights, uh, strongly reads and writes. Similarly, some other applications said, you know what, we're gonna get around this whole right problem and having higher latencies, we're gonna write locally, and then we have, then we have to do strongly consistent reads because it's a small part of our application, we're gonna do a, a read from all the regions and see which region has the latest right. Um, this works fine, um, but again, the strongly consistent reads is going to be very slow, as slow as the region which is uh farthest away from you. Because you have to read all the rights from that region. And if that region is not available, you don't know how to reconcile all the reeds you have, so this becomes problematic. To take away the differentiated heavyweight lifting from customers, we built this feature into Dynamoribi global tables. We build multi-region strong consistency Dynamo DB global tables. Which solves read rights problem, makes rights highly available even in case of region failures, and makes strongly consistent reads seriallizable at an item level with all the rights that have happened to the item. So how do my wife and I solve this problem? We have a joint calendar, we kind of share that and take notes. And we build a basic primitive block, a journal, which spans multiple regions. As the replicas of the global table. This journal is an up and only log, and the journal would successfully append a log if 2 out of, it's able to kind of durably come in and store the log in 2 out of the 3 regions. And on top of that, All the logs from the journal are delivered to the clients in the order the journal opens them. So, the journal can have multiple clients. You have 3 different regions writing to the journal, then you, you want the clients listening to the journal to receive the logs in the order. The journal sees it, not in the order the clients are upending it. So the journal actually. Provides an interface where all the clients are, are getting logs delivered in the order the journal has appended the logs. Now, How does the right work in case of uh multi-region strongly consistent global table, right request. Um, if you see, As soon as the right request comes, it is forwarded to a GT replication engine, but this replication engine is different from the asynchronous global tables. This one is inside TaramodiB as opposed to the other one, which is waiting for the streams and asynchronously replicating it. This replication engine has 222 key functionalities. One is generating the replication log entries for generating replication log entries and opening it to a multi-region journal. And second is to listen to the multi-region journal and applying all the log entries in the order, the journal is it's upended to the journal. So the replicator usually reads from the local Dynamo DB table, says, hey, what is the state of the item, you're writing to this item, generates a log entry and opens it to the multi-region journal. Once it's successfully appended to the journal, You get a call back from the journal saying, hey, I have a new log entry for you guys. And it sends it to all the replication engines in all the three regions. And then all the 3 regions apply the, uh, apply the log entry. They can apply it any, anytime they want to apply, but the region in which the right happens, the application is done immediately and a success or failure is returned back to the customer. Now, this is a very, this is the same uh request flow diagram, but it's just in a timeline to just kind of set pace along with the slides. Um, the ride happens, the log entry is appended, and then you get a call back in each of the regions, and you can see the application of the log entry can happen at various points, uh, various times, timelines in different regions. But the region in which the ride originates, returns the 200 once it applies that log entry. Now what happens if there are concurrent rights to different items um from different regions? In this example, we have two regions having two rights to two different items. Um, it might seem that the right A arrives at R1 before right B. But sinceri B is appended in the journal before right A, Write B is applied by all the regions before right A is applied. So there is a journal allows you to kind of serialize these two rights uh uh journal serializes the rights for you, uh, in the order in which it opens the log entries. Now, how do strongly consistent leads work in case of uh mul uh multi-region strongly consistent global tables? The key thing here is to know that when a strongly consistent read arrives in a particular region, that that particular region has all the, all the rights. It is not, it's not going to miss any rights from any of the regions. And the only way for that right, read to know that there are no rights, it's not applied is by ensuring that it is applied everything from the journal. And how does this, uh, how does, uh, how do we kind of ensure that everything from the journal is uh applied before the reader is served? We use a reed fence, or tracer bullet or heartbeat, whatever you call, it's another log entry. This log entry is appended to the multi-region journal, and then we listen to all the log entries, we'll try to listen to the heartbeat, but as a part of listening to the heartbeat, we'll also listen to all the other log entries which we have not applied so far. So we'll start applying all those log entries which we've not applied so far until we reach the heartbeat. And once the heartbeat arrives, we can be sure that one, we've seen all the rights in all the regions because the heartbeat is serialized along with the other rights in the journal, right? And you've seen all the rights before the heartbeat, so this acts as a reed fence, and now we can successfully you can correctly say that yes, this strongly consistent reed is going to return the latest version of the item, no matter which region the item is written in. Now, the protocol is the same, even if it's the right, if you, even if you have a single region right, um, and your reading strongly consists from that region because it ensures correctness and there's no bimodal behavior. But the, the um thing to keep in mind and remember is that since the heartbeat also goes through the journal. And it's happens to the journal, you pay the penalty of the right. The latency of the strongly consistent reads is almost equal to the latency of rights. Now, uh, last year we did, uh, we actually had multi-region strongly consistent tables in preview, and, um, at that point in time, we, it was launched as a 3-region replica, global table replica. Uh, we got some amazing feedback from you guys saying that, hey, 2 regions is more than sufficient for us. A third region is not necessary because we incur operational cost and the cost of having a Dynamo DB table in a 3rd region. So, well, before we went to GA we launched a witness region. Um, and what a witness region allows you to, uh, what a witness region allows us to do is that for multi-region journal, we need at least 3 regions because the journal has to open to 2 regions to successfully kind of say, yes, the log is durably committed. But if there's a region region disruption, then you don't have 2 regions. So we need 3 regions at least for a multi-region journal. So the wit the witness region is just saying that, hey, your multi-region journal spans 3 regions, but you don't have to have a Dynamo DV table in the witness region. So you have no resource, no cost, no operational costs to to kind of maintaining a witness region. So, Mercy Global Table supports 22 full replicas and a witness region. Uh, like, um, uh, we did talk about asynchronous global tables and how it resolves, uh, uh, concurrent rights. Um, and for mostly global tables, it's very easy on how to resolve concurrent rights because you're writing to the multi-region journal, right? So if there are two rights, it's going to write to the same journal, they're going to get serialized. But the important point here is, what is it that we write to the journal? Um, what is the replication log entry we write to the journal and why do we do this? So we write a very I don't put a replication log into the journal. The reason for this is that um we want it to be applied at most once, no matter what, because a GD replicator can read the replication log entry, apply and fail, fail after that or crash after that, and then It might restart from the previous checkpoint of the rep from the journal to start listening and try to reapply the logs, so we make the log entries item potent. And how do we do that? Like everything Dynamo, Dynamo supports optimistic and currency control, so we just take any request that comes in. Read the current item and convert it to a conditional, um, conditional insert or conditional delete. For example, here I'm incrementing the counter of the number of attendees for Reinvent 2025, right? When this request comes into a region, It will read the current state of the item. And generate an insert log entry. And this insert log entry is conditioned based on the timestamp which we store our system metadata for every global table item we have, right? Um, and says, OK, fine. Update the item to, to the value 7 from 6. If the time stamp matches whatever the previous time stamp on the item is, and then we also increment the system money data to show the new timestamp. Now let's see how this works. Now I have 2 increment reinvent calls from 2 different regions. Both of them are going to generate replication log entries which look almost identical, except that one of them may have a different timestamp, new timestamp on the item versus the other one, right? Now each of these regions are going to get a callback for each the log entries in the order. So region R2's log entry will be applied first. And you'll get a success back in region art too. It's great. What happens when both the regions read the 2nd log entry? They will not be able to do this insert again because this is a put item with a condition and the condition doesn't match anymore. So R2 is going to say I can't apply it, I'm just gonna skip this log entry. R1 will return a failure to the cus will return a failure to the application saying, hey, there was a replicated right conflict, please retry this um uh retry this right again. Usually, the SDK is smart enough to retry this because it's a retryable exception, so SDK will retry this exception for most applications. Multi-region is strongly consistent. Mercy Global tables and failures, like I said, we went through strongly consistent reads, how do they work? We talked about right conflicts, how do we handle. The other key thing is understanding the failure characteristics, what happens in case of failures, how do how do these tables behave. Since the regional latencies are different, if a, if there is a region is completely isolated, that region is not going to get, be able to serve first any rights or strongly consistent uh reads because it can't write to the journal, right? Uh, and it can't serve any rights. But interestingly enough, Before the partition, US East 1 would have had a very good latency in terms of rights and strongly consistent reads because its closest neighbor is US East 2. So it only needs US 2 to acknowledge US East 2 and itself to acknowledge any right quickly. But once USE 2 is isolated, it has to pair up with the US West 2 for all the journal comments and all the journals, which means the latency of the application would increase if you have one region, uh, if, if you have your closest region completely isolated. Now, if there is a network partition between US East 1 and US East 2, this is the most interesting use case. Uh this is the most interesting failure scenario. Um, in case of uh Asynchronous global tables, if there is a network partition between US East 1 and New US East 2, the replication stops because it's a point to point replication, uh, bi directional replication. But in case of Mercy Global tables, it actually works because in steady state, US East 1 is replicating logs to US East 2 all the time and US West 2. But if there is a network partition between US East 1 and US East 2, those replication logs just take a longer route. They just travel through US West 2 to US East 2. So any replication logs, even from USES2 to US East 1 will travel via US West 2. So your application latencies will go up because there's a network partition and the replication logs will have to take a longer path. But all regions will be available for strongly consistent reads and rights. This is a key difference between asynchronous global tables and Mercy Global tables. Now, um, Amrit was talking about streams and transactions. Um, all rights to a key are visible in the same order in all regions for Mercy Global tables. Uh, for a given key is uh visible in the same order, that doesn't change with streams. Um, Mercy Global tables don't support transactions and a time to expire still. Um. So these are 2 key things to keep in mind when you're kind of using Mercy Global Tables for any of your workload. Now this is just a recap what are the regional failures and how, how do these two tables differ. In case of a regional failure, again, asynchronous global tables remain both of them remain very highly available in both of the regions, uh, in the remaining healthy regions. In case of a network isolation, again, they remain asynchronous global tables is highly available in all the regions. Your reads are going to be very eventual, uh, will be stale, but rights will be still available. But in case of Mercy Global tables in the isolated region, regions and writes won't be available, but other, other regions will be highly available. Network partition, like I said, they behave slight slightly different across both these global tables. Now, one of the key things I get asked everywhere I go is like, hey, how do you guys gain confidence in this protocol? This is this complicated system. All distributed systems are very hard. Um, and in Dynamo, one of the things we kind of take very seriously is distributed systems, uh, human mind can't reason about it. Um, so we use a lot of, um, uh, tools to help us kind of reason and check our system invariance all the time. So for global tables, we use this new tool called P model checker. Which allows you to model the states and transitions of an item and the protocol, and Validate it during runtime, during application runtime as the Global Table's protocol is running, we can generate application logs and validate that using P Observe. And since P uses a language which is very close to C, it's kind of easy to keep the code and the model in sync. So anytime you want to change the code, the practice is usually to change the model first, verify the protocol still works, and then kind of check, uh, modify the code. A big part, like my friends across the stage were talking about how do you build resiliency. A big part of it is also chaos and scale testing. There's a lot of testing which goes into all the failure modes and how global tables behave in failure mode. So we do a lot of chaos testing and scale testing to ensure the protocols still work and validate the invariants of the system. Last but not the least, anti-entropy. For both all global table replicas, we are performing anti-entropy always to ensure there is no divergence and items rights converge in case of asynchroous global tables. Eventually, and in the case of Mercy Global tables, they are converged. So we keep performing anti-entropy, and this is a property we do even for regional dynamo DB where we have 3 copies of the data across different availability zones. We have a system which continuously performs anti-entropy to verify the replicas have the same data. We also have introduced an experiment in AWS fault injection service, which customers can use to kind of test themselves how their applications will behave if there is a failure with Global tables, and I think, I think this is very important because we've tested a lot of this stuff, and it is customers who need to kind of test their failover, their, their, how their application behaves when there's a failure. So we do support. paused replication for Dynamo DB global tables, which will help you test what happens if replication is paused for asynchronous global tables. Couple of things which Suu talked about. First, we currently don't support transactions with strongly consistent global tables. We are working on that. If you use TTL Time to Live, it is supported with eventually consistent global tables. It will also be supported with strongly consistent global tables before long. Um, so keep those two things in mind. When you're building your application, one of the choices you're gonna have to make is, should you use eventually consistent global tables or strongly consistent global tables. These are some of the things you have to keep in mind. With asynchronous global tables, there is no rate cost across regions. Your rights are always in the local region. Your reads are always in the local region as well. With strongly consistent global tables, even your strongly consistent reads have to effectively go across regions. That's the heartbeat which someone was talking about. So remember that a strongly consistent read has a latency of approximately the same as right. Uh, availability. Both of them are going to be highly available, but your data is instantly available in the case of a strongly consistent global table. It's eventually consistent in the case of a. Asynchronous global table. Also, remember the network isolation failure modes are a little bit different. This is the one which is most important for your application to deal with. And I strongly urge you, when you are building an application with either form of global tables to use the fault injection mechanism and see how your application deals with the failure of replication. This is not something you should discover in the middle of an event. You should test this on a regular basis. The other thing which you should do if you are building an application which is active, active, it's really good that you're driving traffic to both the regions, but if you are building an application which is active at a standby region. Always, please try to drive traffic to the other region as well. 10%, 90%, or do failovers on a regular basis. Do not make those things uh only exercise during a failure. Test continuously. And again, the last one talks about the kind of things that you would expect to use these different global tables for. You want to try again? Yeah, OK. Uh, I think that's fine. I'm sorry about that. Sorry for the interruption. Hope this time it works, so. So I've reset the counters. I'm choosing IAD as the region where I'm gonna do the rights. I'm gonna increment the counter 5 times they're in a loop. While I'm incrementing the counter, I'm gonna do strongly consistent reads from the 2 other replicas I have, which is one is the newest East 2 and one is the newest West 2. And see what the experiment provides us. So as you can see, um, in IAD the values went from 0123 45, which is local rights, which is fine, um, and in the US West too, it's very interesting that when it started reading strongly consistent reads for quite some time, um, we're giving the same value as eventually consistent reads, and I ran 10 iterations of it and. It, it reads 0 and then skips directly to 5. It just sees 5. So you can see that it's an asynchronous replication. The eventual value kind of comes over to to US West 2. Likewise, since USC 2 is nearby, it does see 0s, 3s, 3s, and this one. But one of the key things to notice here is the latency. The latency down at the strongly consistent and eventually consistent reads is very low. I mean, I'm talking for, I'm, this, this is running on my laptop, but from Vegas, so it's like about 80 milliseconds to all the regions, but both reads are really, really fast because they're talking locally to the region and they're not talking uh to the they're not using strongly consistent reads for Mercy Global tables. I'm going to run the same experiment with Mostly global tables and let's see what we get. So first you can see that ID rights were almost the same time still, writing to one region is fine, but You can see that strongly consistent reads and eventually consistent reads now start seeing the same values as they kind of are incremented in IAD, for example. And also, a key thing to notice here is now your latencies for the strongly consistent reads have gone up from 70 milliseconds to about 200 milliseconds because, like I said, strongly consistent reads for multi-read and strongly consistent tables do write to the journal. So that latency does add up when you're doing strongly consistent reads, right? Um. Likewise, you can see that they, they finally see, they see most of the values throughout. Um, in some cases, you can see, for example, here, the strongly consistent read in US West 2 does see the value of 4, but the eventually consistent value sees 3 because it is reading the local copy of the data, which may be stale, but the strongly consistent read is reading whatever is the latest right, uh, which has happened. The second example I'm going to show you is Um, a multi-writer example. Here is where we're going to increment the counter across all the regions simultaneously, right? I'm gonna have 3 different, uh, threads, um, incrementing the counter across 3 different replicas. So we'll see how this works with asynchronous global tables first. Run this experiment and you can see that. They all incremented the counter 5 times, but they just landed up with the same value eventually, because they're not talking to each other, the riots are not coordinated, the rights are not getting propagated before the riots happen locally in the region. So all three replicas land up being the same value. This works for applications where you really don't need the rights propagated across regions. You can maintain separate copies of the values. Um, so this is, uh, and you can see the latencies are also very fast for the rights. The latencies are almost as, as good as how long it takes from going from Vegas to each of these regions. We're going to run the same experiment again with Mercy Global Tables. And see what happens. Since each of the regions is incrementing the counter 5 times, you'll end up with the final value of 14 and 15, but you can see that the values get incremented 011234. But there are some, some rights where you can see the value, the time taken is very high. For example, in US West to the first right, where it's right is 5 and value is 6, you can see it's taken about like half a second to write. And if, if I were to go debug this stuff, it might, it, in all probably this is because it got a replicated right conflict the first time it tried to increment the counter, so it had to retry that increment the counter again, um, in the region again. So, the retries do add up and you can kind of see this latency kind of go up, but you can see that all three regions eventually converge to the same value of 15 after incrementing the value of 5 times in each of the regions. So that was the part of the demo. Amrit, do you want to kind of Do you wanna hit that switch for me again. All right. So we talked about how you make a choice between using strongly consistent global tables and asynchronous global tables. And let me just quickly recap the things which we talked about. Customers building applications sometimes need the ability to have data available in multiple regions, distributed applications if you want to have uh redundancy in the event that there's a regional failure. Some customers are able to tolerate a non-zero value of RPO. They are willing to uh replay that data themselves. Others who are not will probably want to use strongly consistent global tables. There are multiple trade-offs when you go from one region to multiple regions, and when you go from multiple regions asynchronous to multiple regions synchronous. One of the things which we've realized is that building applications with scale is typically much easier if you're able to embrace eventual consistency. If you require everything to be done synchronously, You are at the mercy of having the long network lags. If you're able to tolerate eventual consistency, you can do everything with the inter AZ or intra-region costs. Um, how does Dynamo DB behave in single region versus multiple regions? And the most important thing is how to choose the right model for your application. But no matter which one you choose, the one thing which I will hope you take away is this test your application in all the possible failure states. We do a lot of that. We use P model checkers. For those of you who are not familiar with P, it's, it's a much more recent version of what you probably use, which is TLA plus. It is extremely useful to verify your algorithms are actually going to work the way you expect them to when there is a failure. Thank you very much for your time. We enjoy doing these presentations. We'd love to get your feedback. So please let us know what it is you wanna hear next time and we'll try and do that. Please take the feedback on your mobile app. Thank you.