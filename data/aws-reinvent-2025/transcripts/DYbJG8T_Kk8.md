---
video_id: DYbJG8T_Kk8
video_url: https://www.youtube.com/watch?v=DYbJG8T_Kk8
is_generated: False
is_translatable: True
summary: This AWS re:Invent session, Architecture 313, focuses on helping software developers and builders transition their existing skills into the world of agentic AI by demonstrating how service-oriented architecture (SOA) principles still apply to building agent-based systems. Andrew Baird, a senior principal solutions architect at AWS, begins by demystifying agents, explaining that beneath the seemingly magical capabilities, agents are fundamentally Docker containers using SDKs like Strands to interact with large language models (LLMs), with Model Context Protocol (MCP) servers providing standardized integration mechanisms for dependencies, creating a familiar service-oriented architecture pattern. He emphasizes that traditional SOA design principles such as loose coupling, modularity, reusability, and discoverability remain essential when building agentic systems, though some principles evolve to accommodate new requirements like maintaining contextual memory across multi-turn conversations, allowing autonomous coordination to emerge at runtime rather than being deterministically defined, and managing capability emergence as tools change and become available to agents dynamically. Baird introduces entirely new design considerations including thinking in terms of goals rather than CRUD operations, ensuring reasoning transparency through comprehensive observability that captures the agent's thought processes, understanding agents' inherent tendency toward self-correction, and embracing nondeterminism as a fundamental characteristic requiring different testing and validation approaches such as AWS Agent Core evaluations announced during the keynote. Dan DeLauro from Wex then shares a practical case study of their journey building "Chat GTS," a virtual engineering platform that evolved from a simple chatbot into an operational support system serving over 2,000 internal users within three months of production deployment. He describes two real-world implementations: an automated network troubleshooting agent that handles connectivity issues across Wex's complex multi-cloud and on-premises infrastructure by orchestrating reachability analyzer, flow log analysis, and change detection to provide natural language diagnostics, and an event-driven EBS volume expansion system where agents respond to CloudWatch alerts, perform triage by analyzing system state and history, execute pre-existing SSM runbook automations, update Jira tickets, notify stakeholders, and even create pull requests to remediate infrastructure drift, effectively eliminating 2 AM pages for routine operational issues. DeLauro emphasizes that success came from applying existing architectural patterns to AI, building specialized agents with clear boundaries and responsibilities, using AWS Step Functions to provide discipline and control over autonomous systems, implementing defense-in-depth through guardrails at multiple layers, creating shared knowledge bases using Amazon Kendra with Gen AI indexing that treats documentation as infrastructure deployed via CI/CD, and establishing comprehensive observability that tracks not just metrics but reasoning chains through distributed tracing patterns adapted for AI workflows, ultimately demonstrating that builders don't need data science backgrounds to create production-ready agentic systems when they leverage familiar architectural principles alongside new AWS services like Bedrock, Agent Core Runtime, and MCP.
keywords: Agentic AI, Service-Oriented Architecture, AWS Bedrock, Agent Core, Model Context Protocol, Event-Driven Architecture, Operational Automation, Distributed Systems, Step Functions, Amazon Kendra
---

Alright, well, uh, good, good morning slash afternoon to everybody. This is Architecture 313. Maybe, maybe your first breakout session so far that you've attended at Reinvent. How many people this is like other than the keynote, this is the first thing you've been able to attend as part of, OK, a couple people, great. Well, thank you for making it all the way to the other side of the, uh, the conference. If you attended the keynote this morning, you're the expeditious ones or if you are like staying close by on this end of the strip and knew that you wanted to attend this and you did not attend the keynote in person, you're the, the ones that have forethought and foresight to, to make your time efficient. So anyway, thank you for being here. Uh, my name is Andrew Baird. I'm a, I'm a senior principal solutions architect with AWS. I work out of our Atlanta, Georgia office. Um, I've been with AWS for about 13.5 years and. Um, there has been no stretch of time in my entire career in technology where I have felt as motivated, as, um, you know, disrupted, as excited as now for sure. And uh my my past time at AWS has been mostly dedicated to um developer oriented things, building systems, DevOpser list, um, our API oriented services, CICD, um, and the amount of impact and the amount of like capability that's been delivered to folks like me and the, the customers I talked to in the last 18 months is, is mind blowing and hopefully everybody in the room or watching on, on YouTube. that resonates with and um so we're here to talk to you a little bit about um for all the folks that uh maybe have like career histories and trajectories like myself that are software builders, developers and are like coming into uh the the fold of generative AI as it's um kind of evolved into quite a bit of focus around agents, um, what are some, what are some ways you can take the skills you've got already and the, the things you understand and bring it into this new world. Um, so we're gonna have the agenda broken out, um, as follows. I'm gonna spend the 1st 3rd of the presentation talking from a perspective of, of from AWS talking about, um, how to, how to think about and understand agents in the context of service oriented architectures, um, as a builder. I'm gonna talk about how, um, some of the design principles and, um, you know, the, the, you know, important dimensions of building good. Distributed systems that are service oriented really lend themselves well, um, or may have evolved in the in the world of agentic AI. Um, I'll drop a couple ideas about some services that have existed even before today's keynote announcements, um, that could help you as a builder, um, getting into generative AI, and then I'm joined, uh, by Dan DeLauro from, from Wex who's gonna go, uh, much deeper into a a particular use case and architecture, um, and approach that they took when building out. Um, their first, uh, set of agenttic systems, um, inside of, inside of the organization, so you, you can hopefully walk away with, um, at the end of the session with a lot of, um, like deeper understanding and a new level of comfort as a software builder on, um, on how, how you might think about approaching these topics and building systems, building agentic systems using a lot of the knowledge you have already and getting a lot of like tangible credible advice from, um, a team and a person who's who's walked that walk over the last year or so, um, so let's jump in. So agents for builders when when we've talked about agents and um maybe maybe others have talked about agents they're often like displayed in a way like this there's a there's an agent that's doing things that sound really like personified they're observing they're taking actions they're learning um and and when somebody describes agents to me in this term in in these kind of terms for me as like a as a technologist as a as a practitioner of technology. It feels fuzzy to me and it makes me feel like does that mean the the capabilities have become so advanced that there's like this there's this software, these models that are like living inside of the systems we deploy and they're, they're, they have the ability to take actions and. And literally learn in a way that um makes you know everything that I'm used to and the way that we build software feel moot now and and like technology has evolved past the point where um all of the skills that um feel comfortable to me and the knowledge that I've that I've earned until this point. Um, is, is, you know, being like put at risk in some way and so it it gives me this premise of like does that mean like service oriented architectures are dead in some way that we now have, we now have software elements that have like superseded the context of, of like how we design distributed systems and the, the way in which they deliver capabilities, um, is this, is this the message now? Is this where we've reached and. So I, I've, as a, as a technologist what I like to do in those moments where you feel like something has like is just outside your grasp of understanding and you wanna dig in a little deeper, you wanna understand it from like a technical perspective and you and you dive, dive a layer deeper and you get hands on. So what is it like observing and learning all of those things they have, they have like technical meaning and um why those descriptors are often used for agents but really like from a technical perspective talking to builders in the room. And, and watching the live, the, the, the YouTube stream, um, what is an agent really um in in in the center of the story there's these LLMs which I don't need to describe too much of but. Um, all of the capability that, um, a model brings to the table to do things like reason, to have, you know, varying levels of expertise across different domains and, and industries, there's, uh, a way in which we wanna like tap that knowledge and reasoning capabilities within the context of some like some, you know, business scenario, um, so that's still part of like that's a central part that. I'm comfortable as a as a software builder. I'm not a data scientist. I'm certainly not an AI researcher. The, the amount of like, uh, you know, uh, advanced technology capability that's embedded inside that model, I don't understand it to a, to, to a deep enough context to like uh extract it and describe it at a builder perspective, but that's living at the heart of the systems that I'm gonna build with agents, um, talking to that LLM, we have an agent application. It's usually a container, a docker container. Um, there's ways in which, uh, building that software, and this is maybe what I believe is like one of the biggest advancements over the last 12 months is having, um, technology companies and developers get a much better understanding of how to, uh, how to integrate with models in a way that let them have a deeper, uh, meaningful impact on. Uh, different application context and so the the SDKs like strands have made it really easy to, to build agent applications, those docker containers that are gonna be deployed that, uh, define how the software is gonna interact with the LLM so like. How, how the prompts are structured, how the different turns in conversation are gonna occur, how they manage things like interaction with memory and past conversations and, uh, integration with tools which we'll talk about in a second, but all of those, uh, those like new elements of how to, how to. Well integrate with an LLM so that you can get application level capability out of that model um is is packaged for you in a lot of like neat and very abstracted ways that developers are are much more comfortable with and SDKs like strands um so I'm gonna call, like I'm gonna draw a box around those two things this, this docker container, um, based deployed application and the LLM. I'm gonna call that an agent and you build those agents, um, and you know, pretty abstracted ways using these these SDKs that make it easy. Um, for how like the request and response may it into that agent we've got a use case whether it's, um, you know, an agent that's sitting inside some chatbot architecture, um, more commonly folks are kind of recognizing that, um, the ability to build like autonomous systems that sit in part of business workflows, um, and data architectures, uh, those inputs and outputs might be, um, the same types of messages and events that you prepackage from other upstream applications and services you have, but. There's some notion of like a request, uh, making it into the docker container you built and how that request is gonna relate to the prompts that are sent to the LLM that's sitting at the center. Um, that agent application and the SDK used to build it has some mechanism for that agent software to interact with dependencies. Um, maybe the next biggest, uh, advancement that's made building, uh, agent applications easier is the, the emergence of MCP just, you know, about, about 12 months ago this month I think when that. Um, that standard was, was first released and really like matured to the point of being adoptable in an enterprise production landscape just maybe 7 months, 67 months ago, um, as the authentication story matured a little bit, um. So, so now we have like a docker application, a bunch of like new software capabilities on how to interact with models, but then a pretty standardized way to make requests out to what our dependencies might be, and those dependencies can be, um, software applications, um, they can be databases, data resources, document repositories, um, all exposed via like a standard integration protocol and MCP, um, that uses service oriented mechanisms like, like folks are folks are used to. Um, there may be other agents that it's interacting with, um, via protocols like A2A, um, those MCP servers that you're writing may be the integration mechanisms for, uh, integrating with those agents too, so we got a couple different options for integrating with agents. Um, and like collectively you have a, you have a story here that that feels like a service oriented architecture hopefully, um, we've got DACA containers, we've got integration mechanisms, we've got the ability for, um, like defining dependencies and how they, how they interact with each other and sit in the context of some business use case to me it feels like that feels like service oriented architecture, um, so everybody here should be. Encouraged, I believe that um the way in which agentic applications are truly being built and developed has tons of symmetry to the same types of systems we've been building for for decades and so I'll talk in a minute about different uh service oriented architecture principles that still largely apply. There's a lot of net new patterns that are emerging in technologies. It seems like every. Every week, um, there's new like libraries that are going viral and new capabilities that are being announced and launched and just know that like as much as it feels like our heads can spin at the speed of innovation that's occurring right now, um, software engineers, um, technology builders have been building muscles at adapting and remaining flexible in the in the patterns that you're, you're, um. Embedding into your designs for for years and years and years and all of those muscles for for you all and for the builders I think sets sets us up as an audience to um to take advantage of these technologies rather than necessarily just be disrupted by them um it sets us up nicely so so take confidence in in that in that premise and then lastly um don't forget nonfunctional requirements and we've seen somebody that talks to. You know, hundreds of customers throughout the year and what they wanna do with agents and how they've, how they've been building so far, um, the amount of like technical magic that feels like is occurring as folks build these systems lends itself to being, it's, it's very easy to forget the things like, you know, operational excellence and security and resilience are, are things that don't come for free that they're, they need to be intentionally built into the designs that you have and, um, there are a lot of folks that, uh, I think would attend a session like this have probably been. That voice in the room in the past while building systems that um know things like you know scalability are are important to include in the design up front um in the in the design thinking so all of that all of those tendencies you have um as a builder are gonna lend themselves to making you really valuable as as agent applications are being, being built and developed. Um, OK, so let's talk about some design principles. So, uh, on this next set of slides there's gonna be kind of an evolution of. Some principles and terminology that hopefully folks are, you know, largely familiar with and how they would apply in like uh an API based distributed system and the design principles that apply and then we're gonna talk about some, some new ones that um we'll we'll try and explain in a set of like terminology that maybe we'll like feel more familiar um like in the lexicon of building distributed systems but um are new things you need to consider inside of building building agentic systems. So here's a, here's a set of equivalent design principles I'm, I'm gonna say when you're building agentic systems versus um all the distributed systems you've built historically. So loose coupling still totally applies um how you deploy those docker containers, how you think about like the um the benefits of things like asynchronous processing of, of like, uh, having no shared concerns between systems, being able to scale various aspects of a distributed system independently. Um, the, the way in which, uh, an agentic use case is gonna sit inside the architecture, all of those principles still apply in very, very, very much the same way, um, for modularity, I think this is one where we've seen, um, a tendency to build anti patterns in the space inside of agents compared to service oriented architecture. So, um, rather than having. Uh, agents that are expected to handle, um, a lot of, a lot of various different tasks and, um, contexts within the same use case all be built as part of the same set of prompts, um, systems instructions, um, that you're gonna deliver to a particular use case, the, the ability to think of agent granularity much in the. Same way as you think about um service granularity as you're building um microservices and service oriented architectures um the same types of tradeoffs exist um so if you have uh a use case where it's like very logical how you might break down those tasks along different um like business logic lines or ownership lines or security segmentation, um, the same types of benefits you'd have gotten when building services in that way you'll get from building, uh, multi-agent systems in that same way. On this, on the flip side of having modular agents, making the tools that you build reusable, um, so reusability and having, you know, the ability for, uh, a rest service satisfy the requirements of many different clients that are integrating with it, you wanna build your, your, um, components of tooling, your MCP servers with the, with that same kind of mindset in mind where if you have, uh, like a, a retail context and you've got, um, an MCP server that's talking to your, you know, your order history systems and. Um, you know, exposing information about past transactions and orders that have been placed, if you have, um, various different agents within like a customer support context and like the e-commerce buying context and the, the fulfillment context all having their own MCP servers that talk to the same set of eventual back end sources of truth, you're gonna find yourself in the same types of, of, uh, you know, lack of consistency and, um, and problems that would have emerged in a service oriented. Um, you know, landscape before, um, so having your tools and MCP servers be built in a reusable way and, and being built by like the appropriate business line domains that own those topics today we've, we've seen like a more of a pattern emerge where, um, the agent, the agents that are being built are often very use case oriented and sit close to the edge of an organization and they're building MCP tools for themselves that their own agents need and they're building the proxies between their own agents and. Um, the back end architectures that, uh, they need to integrate with and we find it's much more, um, efficient and scalable over the long term to, um, to instead think of the MCP server as like an extension of the service integration mechanism for those sources of truth that different agents are gonna integrate with, um, OK, discoverability, uh. Just like you'd have um service registries and um you know uh copious documentation about what the different capabilities are of a particular distributed system and service or API, you want agents to have the same type of capability. It should be obvious to anybody who's, uh, meant to interact with an agent or part of a. Um, part of it, you know, a technology team that is meant to, um, integrate with, uh, an agenttic system that you've built or a developer that's being onboarded into your organization to have a very clear understanding of what the intended boundaries are for an agent that you've built, right? And the good news is a lot of these things are a little more self-evident when building agents to humans than. Um, building APIs in the past, there's a lot of stuff we do in natural language, um, when building agents that, um, make it, make it a little easier to be, to be discoverable, but, um, the same general concept applies that as you're building agents, the, um, the need to maintain like team-wide, department-wide, company-wide, um, catalogs of where those agents are and like what their capabilities are meant to do, um, how to integrate with them, documentation for them. Um, where the observability points are for them, all of those things apply to agent systems just like you would have when you're building service oriented architectures. Now let's talk about a couple of principles, a few principles that, um, I'm gonna say have evolved a little bit that, um, still like, um, like at their core there's, uh. There's ways in which the knowledge you have about a prior principle and service oriented design is gonna like transition um naturally into the new world but there's a little there's a little nuance or a way that some contradiction might might have emerged that you need to take into account um so uh the benefits of statelessness and service oriented design have been have been well understood for a really long time the benefits that gives you to resilience and scalability like deployment safety. Um, having, uh, as much statelessness embedded in an architecture as possible is, uh, was a, was always a practical thing to strive for, um, but now, but now we know in agent applications, um, having persistent contextual memory, um, across multi-turn conversations potentially or. Um, or use cases within workflows that require multiple turns, having contextual memory be embedded as part of the context window for, um, for the agent that's performing the tasks or the, you know, achieving the goals that you define this is an important part of getting the most capability as you can out of, out of an agent. So there's elements of the data architecture, um, the, the memory in particular that is gonna feel, um, it's gonna, it might feel a little unnatural as like a distributed systems builder coming from the past to be comfortable with the idea that. Uh, the, the runtime application of an agent is gonna, is gonna have a lot of like you can think of it as session state, um, but conversation state and memory, um, be building within, um, the same use case, uh, that the same run time like infrastructure that's deployed, and that, and that adds a couple different dimensions that you need to be conscious of. It means that, um, as, uh, memory gets embedded in these multi-t conversations that, um, things like scalability and. Um, and deployment safety across like when users are in the middle of having a conversation, work is in the midst of being done, um, how, how, what that means for your use case, the idea if you were gonna, um, disrupt a multi-ter conversation in the midst of, of, in the midst of it happening and what the user experience is meant to look like in the, um, and, and, and when while that's happening in real time, um, next, let's talk about, um, orchestration versus, uh, autonomous coordination. Um, so still like the idea of building like logical graphs and the types of of work that should occur, um, and understanding what the dependency flow and structure looks like for the architecture you build, um, those things were very deterministic and able to be documented and understand at design time in the world of distributed systems and. Using capabilities like um state uh step functions um from us and other like graph based uh workflow um platforms um but with agents you have the ability to let the coordination emerge at run time um as the the type of like non nondeterministic work that you might wanna take advantage of it's possible for there to be multiple use cases that a single agent system is gonna satisfy and depending on. The type of request that comes in and the context for it, you may have different specialized agents downstream that handle different components of it so understanding like how coordination is meant to relate to each other and how that coordination is described um. And how, how to allow it to emerge and at run time versus um defining it in a very discreet way and you have trade-offs here you can make and and Dan's uh portion of the, the conversation there may be use cases where you want more determinism in the workflow that you're building and you don't want um a lot of like emerged coordination that's that's more autonomous and, and like nondeterministic at a macro level and, and you can, you can combine um some of the, the principles here with the principles you, you know and the patterns you have from. Um, from service oriented architecture, uh, like patterns of the past that will still apply. Um, so next service contracts and capability emergence, uh, it's, it's like has been fundamentally important and like good API design to, uh, understand like the concepts of backward compatibility and, um, like client client server relationship at the contract level and what the different data elements were and like the allowed values, um, and like ranges of those values, um, was a was a key part of what allowed, um, service oriented architectures to proliferate. And and be stable, but in the world of of agents capability can emerge at runtime and that's a that's a it's a benefit of it and the idea that as like tools change and capabilities are deployed, you, you don't have to update layers of your agenttic stack in order to expose those things and expect the agent to. Um, like evolve its behavior through coding changes you've made as those tools, descriptions and capabilities emerge over time, the agent will discover them at runtime and it will allow new capability to emerge as tools are built and as they change and as capabilities are released, those things can emerge, um, can can emerge in real time. Um, some, some, I'm gonna call it totally new design principles, um, that folks need to kind of get comfortable with. Um, software engineers often joke that a lot of what our career has boiled down to has been building crud, you know, building create, read, update, delete, and a million different business contexts, but in the end if you're talking about business logic, there's some way in which the code you're working on on any given day is gonna boil down to building some form of crud related to some type of business domain. Or or a technical domain object and and that meant that that's that like helps ground your design thinking and the patterns that you're gonna build and things are very different now and and the way in which you think about um like how to prioritize what use cases are a good fit and what models are gonna be like practically applied to um you need to you need to like pivot your design thinking to this idea of goals that. Um, goals are, uh, like the new unit of, of technical work being done rather than crud, and if you can distill the type of, of, uh, value you want your software system to, to deliver. To your business or your customers in the context of a very succinctly describable goal, um, there's a good chance that a model will understand, um, the work that you're trying to achieve and, and, and you'll get a better idea of like what what data and what context needs to be delivered into that, um, into that model to achieve the goal, um, reasoning transparency so the whole idea of reasoning as like a deployable piece of technology. Is fundamentally new on its own, but it means that observability is, is fundamentally different too. The, the logs that we used to gather, um, needed to only include a lot of like metadata about the actions that were taken, metadata about those actions, the events that occurred, um, within the sequence of code. Um, so time stamps and like where you know elements of code were executed and now there's this nondeterministic ability for software to, to think inside of your, um, inside of your architecture and being able to, to think about how you wanna ingest that observability into an architecture and, and use it thoughtfully when you have operational reasons to review it or analyze it, um, or debug it, um, so having a, having a good set of ideas of how, um. How reasoning transparency and the evidence of it is um is part of your operational pipelines and um and architectures is a is a key new thing that you need to to think about you know buil building other distributed systems to ingest in the process and things like that. Self-correction, um, there's tons of patterns like circuit breaker and, and others where error handling and fault handling has been a, a large portion of what, um, can make distributed systems reliable. Uh, agents have like a general tendency to want to, um, self-correct and find ways to achieve their goal via multiple paths, and there may be times where this is a big benefit for you and there's other times where you may not, you may not want an agent to take advantage of this. So having um a very explicit understanding that at build time at like the definition of the agentic system you're you're designing and deploying that this tendency exists inside of the agent you're building and what it might mean. The different error scenarios are that we'll encounter whether it be like a tool is unavailable um or like access changes occur that like limit its capability and how an agent may react to those things and what you'd what would you want its behavior to be in those scenarios and to take those things into account as you're building your instructions, um, designing your, your downstream like dependency architectures, um, and all of those things is, is net new work that you should be thinking about as a as a service oriented builder. And then lastly, like maybe most fundamentally is this idea of nondeterminism in general, um, understanding that, uh, the, the type of testing that you're able to do, we just announced this morning agent core evaluations can play a huge role here, um, there's, uh, ways in which. The like the that fundamental premise that the software work being done is gonna be nondeterministic is is gonna change a lot of the um the typical ways in which you've achieved some operationally related goals um for for agentic systems compared to how you how you worked before. OK, um, now I'm gonna just gonna run through things that hopefully if you're in the session you understand um these services are available already but um I've mentioned strands a little bit, um, a couple times already but. Uh, AWS's mindset here is like adopt an agent framework in general. Don't, don't feel like you need to write software that's gonna integrate with all your models out of the box, um, and, in some sophisticated way without taking advantage of the SDKs in the market and strands. We think it was released at a very neat time where, um, the model capabilities and their ability to reason really advanced to the point where, um, it led us. Bu il t an SDK that hit a really, a really nice sweet spot on making the ability to, to develop quickly and reach production quickly, um, like at, at the, at the like the appropriate level of attraction that leans into the models, um, capabilities to, to reason without a lot of like design time complexity but still a ton of robust operational and security benefits that you get from using it too, so, um, choosing the right agent SCK. Um, for like building and authoring the code related to everything I've described so far, um, we have, uh, the Quiro CLI and the Quiro IDE. You have the ability to adopt cloud code. You can have your model inference for the cloud models behind cloud code get deployed on Bedrock. If you're a team that manages like model governance and access inside your enterprise and you like the Bedrock model of that, you can have cloud code be, um, backed by by a Bedrock model, um. And so many of the things like as you as software engineers are are learning these things there's new capabilities that are gonna um help you author the software and and produce it for you without you having to um get down to that level of detail yourself and then and then obviously Bedrock Agent Core run time as as you more and more are gonna be responsible for um understanding like the high level translation between business. Requirements into like the prompting and um co-development you're gonna do with gentic systems having an infrastructure platform like uh Agent Core Runtime that um abstracts you know as much as it can on the infrastructure side in a serverless way and give you a lot of confidence that like the AWS reputation from like operational excellence and security are gonna be embedded in that story um is gonna is gonna like help you accelerate and and get the production more quickly. And then I just wanna call out here that the Agent Corps run time um where the Docker containers get deployed can also be your MCP servers that you build as well. So all the different tools that you might build for your agents deploying to Agent Corps runtime is a place those MCP servers can be deployed as well. OK, so that's all the, the general kind of overview that I'm gonna provide you today. I'm gonna, um, pivot over to Dan and Dan from Wexo is gonna come up and, and talk to you about their experience building agents as a company. Thank you. Am I on? All right, good afternoon everybody. My name is Dan DeLauro. I'm a solutions architect on the cloud engineering team at Wex, and let me change the slide here. There we go, it's the right button. Sorry about that. So last year I was sitting out there, right, and I was in a session just like this and I was hearing about Bedrock and Agentic and all these new tools that make it look really easy to build with AI, but it always kind of felt a little bit out of reach, right? I'm like Andrew, I'm no data scientist. I don't have a background in machine learning and I'm not building neural networks, but I didn't need any of that because honestly. As builders, all of us, right, as people who understand systems and patterns and architecture, we've actually got an advantage and that's kind of what prepared me for all that like the building part though, like this speaking thing is new, that's a new thing for me, but I'm here so thanks for coming out today. I'd like to talk about how we've been using Bedrock agents and now Agent Core to enrich our operational support at Wex and I think you'll see a lot of the principles and a lot of the things that Andrew covered. They're there and they're kind of what made that possible for us and they're really what helped us go from pilot to production in under 3 months and now we've got well over 2000 users internally, so it's been a good year. But first we've got the company slide in case you don't know who we are. Wex is a global commerce platform. We power mobility, benefits and payment solutions for organizations in more than 200 countries. We operate one of the world's largest proprietary fleet network. Works and we help consumers manage their benefits accounts things like HSAs and FSAs and LSAs and COBRA and we handle everything from corporate travel to expense management all with the goal of simplifying the business of running a business. Last year we processed over $230 billion in transactions in more than 20 different currencies, so it kind of goes without saying, right? Our platforms need to be reliable. They need to be secure, and they have to run at scale. And that's where global technology services comes in now we're sort of the team behind the teams we design the shared services that drive platform engineering standards, reliability, governance, cost optimization, you know, all the paved roads that let everybody move really fast without getting into trouble. My team's job at Wex, I'm sorry, is to make cloud feel simple even when it isn't. And last year global Technologies saw more than 40,000 support requests. Now that's a lot, right? But if you think about it, every last one of them is critical to somebody, even if they are repetitive and time consuming for us. And we've got operations and we've got SRE and support in all of those tiers, but we're always looking for ways to reduce that number. Without reducing quality, so like everyone else, we started looking at AI but we didn't wanna do anything flashy or complicated, we just wanted to start small and build something simple that would just quietly make our lives easier. And that's what inspired us to build Chat GTS and in case you don't see what we did there, GTS is just short for Global Technology Services. It's kinda got a ring to it, right? Uh, but anyways, I promise there's more to this than a chatbot story. Sure, it can chat, it lives in our chat. It can read our documentation, and it'll do Q&A all day, but underneath it's evolving into more of a virtual engineer, one that understands cloud and network and security and operations. But we're not replacing people, we're just doing what we do in operations we're automating the repetitive stuff and we're expanding our self-service capabilities really we're just trying to free people up so they can focus on the problems that matter, the ones that actually move the business forward. But we had to start somewhere, right? We didn't want to just build agents because we could. Or get stuck in that cycle of chasing shiny objects whenever something new came out. So we looked at our data, our support history, and those 40,000 tickets, and then we asked ourselves, what are we seeing the most, right? Which are the most complex or just flat out painful? Really, where are we spending the most time? And then we looked at the places where we had existing automation or run books or some kind of a process that people were already executing. And we realized that was the sweet spot, that high volume, high friction, well understood work, that's where we knew we could make a dent with AI. I brought two examples to share today, but keep in mind we did start with chat and we focused on Q&A so that we could build up our knowledge base and we saw immediately now instead of opening a ticket just to find information, people were able to come and find it on their own, and we were building agents that could leverage that knowledge base to make their own decisions, so that was sort of like setting the foundation and laying the the platform for all of this. But now, um, this first example is honestly my least favorite ticket and hopefully some of you will understand why when we get to it, but the second one is a little more exciting because we're moving beyond chat and we're embracing event driven design with Agent Core, but together I think they show how AI can really become a part of operations, not a side project, but more like a teammate who's always on call. So can anybody relate to this? Has anyone ever had a network issue they've had to troubleshoot? Well, I know you do, right? If, well, consider yourself lucky because at Wex we operate hundreds of AWS accounts and we've got Azure and Google all spanning 8 regions and multiple on-prem data centers and it's all interconnected. Now you probably know there's a lot of tech that goes into making that possible, so when something inevitably fails, it can be challenging even with the right tools it's hard. So picture this, it's almost noon, you're getting ready to go to lunch and you get a ping in your support chat and it's Jared from the PAS engineering team and he's like, dude, we're blocked, we're trying to deploy this cluster and we're in this new VPC and we're expecting it to reach all these things. Now Jared doesn't have access to the transit gateways or the firewalls or the VPNs and even if he did, maybe it's a little out of his wheelhouse, but that's not his job, right? You know that feeling you're like Jared, you don't wanna leave him hanging, but you wanna go to lunch now you almost, it's like you don't have a choice you have to respond. Well now you don't because we built an agent that can respond for you and now what used to require all this tribal knowledge across all these different domains that can happen in minutes, and anyone can use it now, even Jared. Now in this example it's an EKS cluster and the agent knows it's an AWS and it could go into our core network account and it can use reachability analyzer so it provisions a network analysis path and we know that takes a while, so while that's running it fans out and it checks flow logs it looks at any recent changes in the network and then it looks to see if there's any known issues, right? There could be something already happening. But by the time it's done, it's collected all of this information from all of the things it's looked into and it breaks it down in natural language and presents it to you as the end user in chat, and it shows you exactly where that traffic dropped and why. So now when Jared needs to escalate, he can do it with the right team and all of the right information. And since we're logging all of these investigations, we're able to spot recurring issues and maybe we identify opportunities for some tighter guardrails in the network, but at the end of the day, this is a perfect example of how an agentic system can scale maybe where humans can't. Now the second one is, is my current favorite because you know like I said this was more than a chatbot. Now we're building agents that are responding to alerts and anomalies and they're understanding the state of a system before they're deciding how to react. They're not just hanging out waiting for a ping and honestly this is the best kind of AI because you don't even realize you're using it. And we all love EKS, right? Containers, they're gonna solve all our problems and you know it's gonna save the world, but they're not always the answer. I don't judge, but at Wex we've still got some critical workloads that just make more sense running on EC2, but we still have to support them. So picture this. You've got an EBS spike out of nowhere. Your cloud watch lights up and you can almost smell the smoke right now this could be a warning shot, but why chance it if we can get in front of it? But if that workload matters, someone's getting a page. Somebody's gotta wake up. They gotta log in, then they gotta figure out what's happening, where is it happening. They're checking logs, maybe they're running playbooks to expand the volume, or maybe they're just hopping on and clearing some space, whatever they're doing, that takes time and let's be honest. Nobody wants to do that at 2 a.m. I certainly don't, and that's one of the reasons we built this, but it's the reality today. So now we can flip the script a little bit. Let's take a sip of this real quick. All right, so instead of paging an engineer we can send these alerts to an agent with all of those metrics in context so now we're not calling anybody, we're not waking anyone up that's no longer the first line of defense. The agent can see what's happening, where it's happening, and it can find an agent or a team of agents to help with the issue. Now our first agent does some discovery. This is like triage. It looks at the operating system, it looks at the version it looks to see which platform it belongs to it looks to see how critical it is. And then it looks at history. It looks to see has this happened before? How many times have we expanded the volume here? You know, we maintain policies to cap expansion, right? You don't wanna just keep adding disk, you're kicking the can down the road and that's never gonna work. So at this point, if anything looks off, the agent steps out of the way. And we escalate back to a human and then we're back to the way we do it now. But if not, our agent can connect into Jira through agent court gateway and it can open a ticket and start logging the incident. Now think about it, this analysis is huge because you're collecting all of the inputs you're going to need a week later. If it evolves to an incident, you need to craft an RCA document. So now that we know what's happening, it passes on to our maintenance agent, and this agent can choose from a library of pre-existing SSM documents. Now keep in mind these are the same documents that our ops engineers are using at 2 a.m. We've got playbooks to run diagnostics and backup and clean up and expansion. It's all the usual runbooks, but we've exposed them as tools to the agent on an MCP server. So now, whatever the agent decides to do, it's using the same automations that we already trust and we're not waking somebody up to push that run button. We've eliminated the chats, the texts, the pages, the cross team escalations. And now these systems are, they're starting to learn how to take care of each other. And as they learn they're building memory and over time they can start to recognize patterns. Now we can learn where we're starting to, we're cleaning up on occasion, where we have to escalate, and maybe they start to spot issues at the application layer. Now that's what makes this feel less like automation and more like a team that gets smarter over time. And once the issue is resolved, we can update Jira through Agent Core Gateway and then we can publish status to SNS through an MCP server and from here we can notify engineers on call systems, dashboards, whatever else is downstream from there. But we're not done yet. This same agent is gonna follow that resource all the way upstream back to the terraform or the cloud formation or whoever or whatever built in in the first place, and it's gonna open a pull request or it's gonna create a Jira issue or send an email. It's gonna do something that closes that loop between operations and infrastructure so we can remediate that drift we introduced in the incident. Now, I'm gonna be honest, I've been doing this a long time and I've seen some cool stuff, but when I see stuff like this work. For real, like the first couple of times I still kind of feel like a little kid. It feels like magic, right? And I just hope it sparks some ideas and inspires, you know, to see what's possible when you think about applying AI to operations. I know this is a basic example we're just expanding the volume here, but we're just getting started. This is us learning how to use what we already know and we're reusing what already works. It's not magic, it's just engineering and architecture, and that's kind of the point. So I'm gonna zoom out. I'm gonna talk a little bit about how we got there and then we can look at some diagrams and we'll see what it looks like under the hood. So when we started this we knew we wanted to build more than a chatbot and a rag pipeline, right? We wanted to create a platform we wanted something sustainable, something extensible, something that would inspire all of the other teams to come and collaborate and help us expand this because operations takes a village. But we didn't have to change the way we built things, we just had to let these old patterns breathe a little bit. It was all the same stuff. We built agents with boundaries. We gave them clear responsibilities, and we let them all work independently. And we saw guardrails become the contracts for what those agents could and couldn't do. And events, it became less about something happened and more about here's what happened, here's what it means, here's what we can do and here's what we did the last time it happened and of course observability, right? We still need to see everything, but there's more to this than 200s and 500s now we're looking at behavior and reasoning. There's nothing new though it all translates and it all still makes sense. So here's what it looks like. Out of the gate, I'll be honest, it was a little challenging because at Wex we don't use Slack, we don't use Teams, we use Google Chat and there's no native integration between Google and AWS for collaboration. So on the left Left, right, we have our users chatting with us in our workspace domain. Now those requests come over the internet. So in the middle we have our WAF with Imerva that it secures our inbound traffic. And on the right we have our AWS environment fronted by API gateway, and from there we use a lambda to route and acknowledge messages and then we use step functions to orchestrate all of our agents. We store state and conversations in dynamo. Reasoning traces land in S3, and of course Bedrock hosts our agents and knowledge. So on paper it looks pretty simple. It's neat. It's easy. It's serverless. It's exactly what we wanted. Now our chat application is only assigned to the users who are allowed to use it, and all of those messages come with a signed token that we can validate and then it hits our router and here we can filter out noise oversized prompts, and we can send a quick response to that user basically saying hey we got it, we're working on it, and that helps sort of absorb the model latency because agents still take a while to do their work. But really at the front, at the edge, this is just a clean, well defined contract for everything else downstream and it keeps our front door predictable. Now I'll admit before this I never really did much with step functions. I know they're, they're on all the exams and everybody has to learn them, but I always felt like they were just for people who've built too many lambdas. But it turns out they're actually perfect for AI, you know, Bedrock gave us the intelligence, the parts that can think and create and make decisions. Step functions gave us the discipline we needed to keep it all in check. We use the retries and fallbacks and state transitions, and that's where it really clicked. You know, like Andrew said, these agents, they can be autonomous, but they don't have to be. You can give them as much freedom or as much control as you need to, and that's what made them fit so well in these operational workflows. So Google gives us a trusted identity, but it has no concept of permissions. That token really only tells us that you're allowed to talk to us. So we take that identity. And we reach out to Active Directory and we fetch your entitlements and cash them in Dynamo. Kept it pretty simple. It's all based on group memberships and OUs and that way we're not beating up on systems that were never really meant for real-time traffic. So basically Google tells us who you are. This is how we figure out what you're allowed to do and how far you can go before you ever reach an agent. And when we invoke that agent, we wrap the entire prompt in context tags and we include your identity and your entitlements and send that downstream to the agent. So now whatever the user claims, the agent only trusts that context because it's immutable. And here if something goes sideways, say the agent fails or times out or can't produce a response, we log the error and we can send a safe response back to the user without blowing up the whole workflow. And once the agent responds, we want to capture everything. We store messages in dynamo and trace traces in S3, and from there we format the response so it looks good in chat. You know, Google has their own markdown language, and that allows us to include citations and reference links and any attachments that came back and then we update that temporary message we sent from the router. And that really that takes what feels like a request response and it turns it into more of a transaction. It's more like an actual conversation and that's kind of the point. So really that's how step functions helped us. It gave us this structure where there was so much potential for chaos and it's all the same patterns, right? There's retries and fallbacks and circuit breakers, and it's baked into every state transition that helped us hold all of these creative parts accountable and now we can observe it and we can measure it just like any other workflow. So when it came to agents, I mean we were inspired by SOA, right? Think about what it taught us to break up the monolith, right? We took these big systems and we turned them into smaller pieces with a clear purpose. That same discipline applies to agents and we built specialized agents and gave them one job that they have to do really well and that keeps the focus really clean and the reasoning sharp and then the handoffs between agents is even cleaner. But instead of having one giant orchestrator pulling all the strings, ours acts more like a conductor. It sits in the middle. It can interpret intent, and then it could figure out what's happening, where is it happening, and then it connects that problem to the right expert or the right team of experts. But even experts need boundaries with all this autonomy, we needed to have guardrails. We needed some way that we could enforce policy and compliance, so we apply guardrails at the edge with the orchestrator, and this gives us defense and depth for every decision that happens on the platform. Now we're sanitizing text. We're blocking topics and redacting PII both on the way in and on the way out, so now the guard rails, they're not just protecting our data, they're actually protecting the agents from themselves so they can wander, right, but they still can't color outside of the lines. But none of this works unless they're all operating on this same source of truth, and that's where shared knowledge comes in. Now all of our agents tap into the same knowledge base, so that means when we have a Q&A agent answering a question about connectivity, it's pulling from the same material an ops agent would if it were troubleshooting. They're separate services, but it's consistent understanding. It's like giving every application in your service layer this unified data plan only now it's made up of run books and reference architectures and living documentation. But thinking isn't enough, right? At some point we got to let the agents do something. But they act just like any other service on the network. They're calling APIs, MCP servers. They're executing the tools we've given them to do their jobs. So with Bedrock Action Groups we can run these lambdas inside of our VPCs with tightly scoped permissions, and then we can control what they can reach and what they can't reach. And at the end of the day it's just services talking to services just like any other application layer on the network. So how do we define that truth, right? Documentation, of course, and ironically that was the hardest part of all this because at the enterprise, you know it can live anywhere and even when people can find it, they don't like to read it, but they'll chat with it, that's for sure. So we wanted to build something that could grow and evolve with the organization and we needed something more than just a a dumping ground for files in S3 so we chose Kendra with the Gen AI index because it gives us this hybrid. We get keyword and vector search with multimodal embeddings. And with all of the built-in connectors for things like Confluence and GitHub and Google Drive, we're able to keep all of our information in sync automatically with cron schedules. Now it includes source code and diagrams and policies and runbooks. It's like all of our domain expertise finally lives in this one searchable layer. And the best part about that is it actually comes to us we don't have to chase it down anymore we don't have to find the documentation. But the real breakthrough wasn't just building a knowledge base, it was figuring out how we're gonna manage this thing, right? So those data sources for Terraform for uh Kendra, they're configured in terraform and they live in GitHub and we deploy them with CICD. So with self-service we allow the people who own that content, the subject matter experts, to maintain it themselves. They can open a pull request and we can deploy their changes through GitHub's pipelines. Now it's still enterprise knowledge. We're just treating it like infrastructure now. I'm not gonna lie, observability was a bit of a puzzle. Like we have this third party chat up on the front end, this hybrid identity between Google and Active Directory, and we had lambdas and step functions and bedrock agents. It started to feel like we had metrics coming at us from every direction, so we had to find a way to stitch it all together so we could actually see what was happening. So we built this persistence layer in Dynamo and this is where we store the things we care about long term. This is all of our users, their chat spaces, their sessions, all of their messages, but it's not a transcript. This is relationships because every item here keys back to a session and a trace ID. And those traces land in S3 and this became our black box. This is like the flight recorder that captures every decision that's made across the platform. I mean, it's basically distributed tracing, right, but instead of following a request we're following a train of thought. And of course we needed transparency, so we pushed all of our logs into Splunk through Kinesis and now even info security and compliance and risk and legal, they've all got a real-time view of what's happening on the platform. They can see what people are saying. They can see what the agents are saying. They can see all of the redactions, all of the policies being enforced, and at the end of the day. Everyone's happy. Now I'm gonna say this like it was 10 years ago because it feels like it, but it was only 4 or 5 weeks. But back when we started there was no agent cops there was no built in observability, but we still needed a way to understand what was happening. So we got together and we built this dashboard that produces some practical insights and of course when I say we I mean me and my team and cursor and Claude and co-pilot, but you know all my smartest friends helped out. But literally with this dashboard we can replay a conversation step by step and then we can see the story sort of unfold. We can see what people are asking for we can see where the agents are struggling and we can see where maybe our knowledge base needs some work, but this kind of visibility doesn't just measure quality, it shapes it. This is what drives our road map. This is what tells us what we need to build next. I'm almost done I promise it's my last one then everybody can go. This was a big year for AI. I mean all the new models, all the new services, the tools, all the new acronyms that we're secretly Googling on the side, and I hope I'm not the only one who has to do that. But honestly I've never had more fun learning and building at the same time, and I feel really lucky that I get to work with this stuff because it's awesome. It just is. I've learned a lot this year, but there's 3 lessons I'm gonna carry into next year that I wanna share with you. Number one, the architecture still matters, right? It's all the same diagrams. There's new services and there's new icons. But you don't need to be a data scientist to piece it all together. I feel like I'm living proof of that. And second, you don't have to build a platform. You can start small, but think big. Build something simple that will teach you what to build next. And of course please breathe right go outside touch some grass, maybe wait till you get home there's not a lot of grass in Vegas, but seriously it's way too easy to get wrapped up in all this tech, right? We still have a say in this don't let it overtake you because you could blink. And the next thing you know you're in Vegas on the stage and you're explaining what you did last year because that could happen, trust me. Thank you. This is awesome. Thanks Dan thank you everybody. Hope you enjoy the rest of the conference. Remember, uh, this is your first one we've got surveys that come out really helps us and gives us the ability to speak in the future, especially if you enjoyed the, the, uh, the session here. So have fun this week, um, stay safe, enjoy the, the party on Thursday, and, uh, thanks again. Thank you.