---
video_id: hyyb4-0KVaw
video_url: https://www.youtube.com/watch?v=hyyb4-0KVaw
is_generated: False
is_translatable: True
---

Quick show of hands, who here has tried to scale AI adoption across a large organization and found it much harder than the blog posts make it sound? Exactly, Prime Video and AWS have been right there with you and today James and I are here to share lessons, challenges, and breakthroughs that led us from isolated AI experiments towards AI native transformation. I'm Lilia Abaiborova, a technical product manager with Prime Video. Unlike most traditional PM roles that focus on customer facing features, my role is on developer experience. All of Prime Video developers are my customers, and my team obsesses over metrics like internal tool satisfaction and average revisions per code review. Last year I started an AI enablement program in Prime Video, and I'm excited to share the journey of how we went from. AI system adoption towards AI native transformation. So today we will start with what we mean by AI native. Then James will take you on a journey of how he accidentally sparked. A company-wide movement. Of AI adoption. I will cover organizational transformation and measuring outcomes, and we will end with a really fun demo. But before, just to set the stage a bit about Prime Video. Here on the screen, you see some of our global reach numbers as well as fun highlights from recent Prime Video Studios original content. We have thousands of other titles in our catalog from Prime Video Studios as well as our channel partners. And of course we're big into live sports. Last Friday, we streamed an NFL football game, 2 NBA games, all running smoothly during one of the biggest retail days of the year. What's behind all of this? Hundreds of developers innovating constantly on behalf of our customers and making sure everything is working smoothly. That scale and complexity is where AI native transformation can really bring massive impact. Last year I was here at Reinvent and I shared a journey of our early AI coding assistant adoption in 2024 as it became. As it evolved from an apprentice assistant towards an expert assistant. This year we're here to talk about the next step in that evolution. AI native development transformation. But what do we mean when we say native? It is fundamentally different from the AI assisted development process that most organizations practice today. Think about a typical workflow with AI. You might get requirements for a feature and paste them into AI chat to brainstorm a potential architecture approach. Once you land on something, you paste more context into an ID. And write code some of it is generated with autocomplete some of it is written by hand, eventually once the feature is ready, you might paste more context into another tool. To create documentation or a launch announcement. So we use tools like autocomplete and AI chatbots constantly, but we use them in ad hoc and isolated manner, pasting context between them all along the way. The impact of productivity is on individual productivity and isolated tasks. AI Native is a fundamentally different approach where we reimagine how we build software. AI tooling is embedded throughout each step of the process of software development and every role PMs, developers, designers are enabled with AI. Instead of autocomplete, we rely on agents that can own multi-step workflows end to end. And context is embedded and shared throughout. The tooling and infrastructure and goes throughout teams as well. And that's what leads to organizational productivity transformation. But like all transformations, it didn't start with a top down mandate. It started grassroots. With builders experimenting, sharing. And James is here to tell us about that journey. Thanks. Hey there. My name is James Hood. I'm a principal software engineer at Amazon. I've been with the company for 16 years. Most of that time spent in AWS. I did do about a 5-year stint in retail fulfillment software in the middle, but again, vast majority of my time has been at AWS. Now, I am a principal engineer, and I'm also a former AI skeptic. I don't know what your experience has been like with AI, but my previous experience looked something like this. I would see some flashy AI demo. It would look mind-blowing and exciting. I would then Try to use it myself on real, real software problems, and I would find that the tools just fell over, really struggled, um, and it left me feeling very skeptical. Now, memes are my love language, so I have this slide up here. But if you wanna look at it another way, I was in a cycle where this demo would spark curiosity and excitement, which would lead me to experiment. I would end up with failure or disappointment, and then I would, I would be in a skeptical state after that. Then maybe another demo came up, which took me around that loop one more time. But it doesn't take many times around the loop before it's really difficult to get somebody from skepticism back to that curiosity phase. So what changed? Well, a number of things changed. Reasoning models got better, introduction of MCP. So different technical, uh, innovations happened. But for me, my big aha moment came when I realized that, you know, these large language models that back these systems, they are, they're kind of these mysterious black boxes, and depending on how you prompt them and use them, they have these emergent behaviors that were not intentionally designed in. And what I realized, I, I read a lot of tech articles constantly, and I came across some articles that described some techniques that I was able to reproduce. And when you take the tools and you combine them with the techniques, that is this journey toward becoming what I call a Gen AI power user. My first major success story was using these techniques to build a feature at the time into Amazon Que developer CLI. I built a major feature into that tool. Um, I'm not on their team. I'm, uh, not familiar with their codebase. And it's written in a language that I didn't even know at the time. And I went from concept to pull request in 2 days. And it went from concept to production launch in 7 days. And I'm gonna show you exactly how I did it. So, one of the, one of the, uh, constant narratives that I hear around AI is, I tried to use this AI tool, and yeah, it generated a lot of code very, very quickly. But when I looked at it, I didn't like the code. I ended up having to change it a lot. And by the time I'd done all of that work, maybe it was just faster to do it myself, manually. Now the fallacy in this argument is that essentially you gave it a rough, vague idea, and you had to go to a very precise implementation. And I will tell you, if you remove AI from the, that scenario and you substitute a human in, and you do the exact same thing to them, you're gonna get the exact same result. That's because software development is a process. We have decades of experience that to go from a rough idea to implementation, you have to go through a process that requires research and requirements clarification. You need to design a solution. You need to create an implementation plan, and then you can implement. And that's going to, that's going to increase your chances of success. So I was able to, leveraging these techniques, use AI as a way to speed run me through this process. So the first step is to research the code base. I'd, I'd come up with this rough concept for what I wanted. I'd written a little, a short doc. I'd even shared it with the, the QCLI team. And they said, yeah, I think you're onto something with that. It was a Friday. My wife was out of town for the weekend. I went home, and it wasn't just some like idea I had. Like this feature was really blocking me. Like I really wanted to have this feature, and I thought, why not just create a proof of concept. So first step is to research the codebase. So I downloaded the codebase, and at the time opened up QCLI and I just said, what is this code base? And it went through and file system, searched through, and it told me a little bit about the code base. It said it has these main components, and one of them was the Q chat component. And I knew that was the one I wanted to change. So I said, do more research on the QChat implementation and give me a summary. And it did. And I said, OK, great, uh, create a directory called planning and write this summary information to a file called Codebase. MD. And it did. And it did a really good job. Now, I knew I wanted to add a new slash command. So I said, write a file in the planning folder called slashcommand.md covering how slash commands are implemented. Include enough details so a developer could implement a new slash command using this information. And it did. And I looked through and did a pretty good job. I said, great. Researching the code base, done. That was 5 minutes. Next step, clarify requirements. Now, this is something I got from an article, but I had put my rough idea doc into this markdown file, and I said, read roughdea. MD. Ask me one question at a time so we can develop a thorough step by step spec for this idea, kind of, and the prompt had a little bit more to it. If you think I'm hiding some magic from you, or you're eager to take pictures of these prompts so you can copy and paste them, I have a demo that is coming later where you don't even need to do that anymore. But what this did was it turned the agent into this partner that would ask me questions. Like it asked me, you know, what specific problem are you trying to solve with this feature? And I would answer it. And it would ask me just clarifying questions. Sometimes I didn't always have the answer, or I didn't always need to have the answer. Sometimes, for example, it said, how do you want the sub-command structure to work, to look with this? Do you want to use flags? Do you want to use Git style? And Well, first it asked me the question, and I just said, give me some options. And so then it said, OK, you could use get style subcommands, you could use parameters. It gave me some options, and I said, I like that one, but tweak it this way, and that's the answer that went into this file. So after about 24 questions, I realized we were getting to the end of requirements clarification because it started asking me questions like, should I implement it like this? And it just starts spitting code out at me. And I said, OK, I think we're done with requirements clarification. So at this point, I'm like 40 minutes in. Again, I thought I was doing a proof of concept at the time, so I was just kinda like quickly trying to get through this. So the next step is design. And I said, based on all the planning docs so far, create a detailed design document so a developer can immediately begin implementation. And it did. And I read through that design doc. Uh, again, I thought it was a proof of concept, so I kind of quickly skimmed. I went back and forth a little bit with it and had to correct some things, and maybe made some manual edits to it myself. But at that point, we're ready for implementation plan. I'm about 1 hour into this process. So again, this is something I took from an article, and it says convert the implementation plan. So the design doc had an implementation plan section into a series of prompts for code generation LLM. They'll implement each step in a test-driven manner. And there was more to it. It was, you know, make sure each step takes a meaningful step and is kind of self-contained, and there's more to it. But again, if you're stressing that I'm not sharing the prompts, I'm not trying to be secretive, I have a much better solution for you coming later. But it followed what I said, and it came up with this like 14 prompt plan to implement step by step this feature. I also had to generate a to do list so that I could keep track of where I was. And finally we're ready for implementation. So my next step was to say, read all the documents in the planning folder and implement prompt one from the prompt plan and update the to do with progress. And it did. And then I said, great, read the planning folder again and implement prompt 2. And it did it. Uh, after prompt 2, I had to figure out how to build the package because I actually didn't even, like, I really didn't know this, this code base. Um, so it took me a little bit, figured out how to build it. Um, actually had the agent help me a little bit with how to build it. And once I got it building, I was able to kind of start it up and, and test a little bit. I think I got to prompt 5 or so, and, and, uh, the reason this context feature was even needed was because I need, I had to keep saying like reread the planning folder, and I wanted a way to pin it, uh, pin certain files, and that's what the context feature was. So by the time I got to prompt 5, I was able to like start it up with my feature, and then I used my feature to build the rest of it, which was pretty fun. So, this, this was a pretty mind-blowing experience for me to go through. And this success, this initial success, and these techniques got me very excited. And what I'll say now is, as compared to that vicious cycle earlier, is I'm now in what I call this AI native virtuous cycle, where I hear about something new, or I try some new technique, and I have a lot of curiosity around these tools. I experiment with them. Successes and failures, regardless, I share my learnings with others. And then one thing I always like to point out is that I think both the healthy virtuous cycle, as well as the vicious cycle involves skepticism. I'm, I don't have to tell you all that, uh, you know, AI is, uh, there's, there's a lot of noise around AI externally. And so I do think that having a healthy skepticism is an important part of, of getting in this, uh, AI native virtuous cycle and making sure you set your expectations and be curious, test things out, but don't assume just because some things don't work that the whole thing is worthless. There's really big value to be used in these tools. So after this experience, Uh, I felt this tremendous sense of urgency to spread this knowledge, uh, across the company. And what I did was I created a Slack channel internally, and it's called Amazon Builder Gen AI Power Users. And that channel has grown beyond my wildest imagination. This was just in March that I created this. I can't share exact numbers, but there are literally tens of thousands of members of this channel with people around the company who, uh, it started with my story that I'd shared, but now we have this massive grassroots movement of people who are actively working out different techniques and sharing their learnings and sharing different tools, and it's been an incredible experience. Um, one last note, uh, that I wanna say about the story that I told is, you know, uh, one differentiation I like to make between vibe coding versus what I, what I was doing, which I consider AI native software development, is that If all I did was say, implement prompt 1 through 14, and that was it, then it would have taken me 2 hours, right? But I said it took me 2 days to put up this pull request. And as true as it is that I don't know the language that, of this repository that I was working in, I've also been programming for over 2 decades. So while I can't fluently write that code, I could read what was going on roughly. I was following. Along as it was implementing, I was making, I was going back and forth with the agent, like questioning aspects of the code, and using, leveraging my human judgment and experience to make sure that we were headed in the right direction the whole time. So I think that's a really important distinction to make. Now, this movement that was, that happened internally in the company and is still ongoing, is, again, very exciting. I don't think that you can succeed in AI native transformation without this kind of grassroots energy and movement and excitement from your builder community. However, transformation has to come both from, uh, you can't rely purely on bottoms up, and you can't rely purely on top down. It's a mix of both. And so I'm gonna hand it back to Lilia to talk about some of the organizational support that was given to help continue that transformation based off of this energy. Thank you James. Um, so James shared how he's sparked a grassroots movement movement. But grassroots movement needs organizational support to scale. We found that to support a large scale adoption program like this you need 3 pillars. Access and infrastructure to make sure all teams and roles are getting access to best in class AI tools and there's no friction with access to context, etc. Second is culture and learning. We learned that becoming AI native. Requires this type of um. Learning Flybe that James showed to become part of the culture. And finally, trust and rigor. So this is where we make sure that um. Sorry, um, there are controls and, uh, governance built in at every layer of the stack and every process. These three pillars reinforce each other and we'll go deep into each one. So let's start with AI access and infrastructure. Software development involves cross functional teams of product managers, designers, developers to take product through phases from ideas to coding. Through deployment operating. Learning, which is where new ideas come in. This is the sequence we commonly refer to as software development life cycle or as DLC. Up till now, AI assisted processes and tools have accelerated primarily the coding and testing part of this process. Offering code suggestions, generating unit tests, speeding up individual programming tasks. AI native methodology goes beyond coding. It fundamentally reimagines how we build software. James walked us through how he used QCLI to clarify requirements, design solution. And we have scaled it from individual workflow to team productivity. Every phase of the software development life cycle and every role has embedded AI support with some shared tooling some customized tooling. And let me just walk you through a couple of the tools here as some of the terminology might be new as of recent. Kiiro is an AI native. Development agent, it has both ID and CLI interfaces. And what we've been excited about is that it's been gaining traction not just with developers but also PMs and designers. We use it for prototyping ideas and that way we can accelerate some of the early phases of the software development life cycle so we're not blocked on engineering teams to help us with the initial prototypes. We also are able to use QuickS Suite, which is the business intelligence agent, and it helps us with researching requirements, learning from past experiments, analyzing data, and quickly creating, uh, automating agentic work flows for business. But beyond that, many teams have been building tailored agents and solutions that are more customized for their domain code base and problems. So for example, um, we've built test agents to be able to automate testing and quality validation across hundreds of devices that Prime Video supports. Um, and similarly we have operational agents that use AI and. Hundreds of data sources that we have to be able to. Diagnose troubleshoot, and sometimes even suggest solutions for operational issues and incidents. And what connects all of this is shared context and capability layer where we use knowledge basis and model context protocol to make sure that all roles can share the same context without needing to paste it between different steps and tools. So for example when a product manager creates a spec with an AI assistant tool for that's best for spec requirement development that becomes available to our development team in Kiro VI and MCP integration. And then as they build the code and merge it, that code becomes available to operational agents to use as part of the context along with metrics logs and other data sources. So how did we build. The stuck and enabled the. Context sharing all the way through. At the foundation we of course built everything on AWS, so building blocks like identity and access management, compute observability. For the model and capability layer. It helps that AWS Bedrock has the best selection of models, the largest selection of models of any cloud provider. And we also leverage some of the some of the managed solutions like for example Agent Core Sagemaker um for our workloads. But context matters as much as the model quality as we talked earlier, so MCP servers and hooks have become our context backbone. Connecting AI tools to up to-date business and technical content. What has been key in Prime Video is balancing the central investments. So for example, access to a code repository or issue tracking system is funded centrally. Whereas Teams are building out more specific knowledge bases NMCP servers, for example, access to all the Prime Video catalog metadata that is essential for the Prime Video teams but are not necessarily needed for other Amazon teams. And at even more local level teams are building. A sharing prompts or um steering files to enable development in the manner that you saw James shared earlier. And finally at the top is the experience and tool layer where again it's a mix of all the shelf tools like Kiro as well as customized agents that I mentioned earlier that teams built for their needs. So you heard James share his workflow reimagination with a development workflow. I wanted to see if the same infrastructure that we built for enabling our development teams can also help other roles like product managers and reinvent our workflows. As part of our AI program, I write a regular newsletter with some of the new releases both internally and externally as well as fun success stories that builders share, and that process of searching the external sources, the internal Slack channels, curating and editing the manual tools used to take me 2 to 3 hours to put out good content out there. I started leveraging AI for some parts of the individual tasks like searching the latest stories um related to AI and development tooling. And eventually evolved it into an agentic workflow where I can pitch data from internal and external sources, build agents that can pull things from different Slack channels and curate them, and then publish everything as a using a kiro template. So now this helped me change this process down to about 30 minutes of polish and review. But this is just 1 p.m. workflow. And we're seeing the same transformation across teams and roles, um, so things like operational review documents, data analysis, um, we're seeing PMs TPMs, um, sorry, uh, and, uh, managers, not just developers really engage with the, um, agenttic capabilities that we have. When every role starts reimagining their workflows, that's when we see transformation happen. And what we need for that is for learning and experimentation to become part of everyday culture. So how did we enable that? We leaned in into the AI native learning flywheel that James started earlier. Teams learned new techniques, experimented with workflows, shared results widely, and it created curiosity and healthy skepticism. That led to the next wave of experimentation. That loop And not a top down mandate was what drove adoption that lasts. So how did we build that flywheel? We started with safe sandboxes, demo days and hackathons where teams were encouraged to use AI to automate parts of the process that weren't AI native yet, maybe building test workflows for different devices or automating some of the on-call operations. And as that started gaining speed we've formalized some of the mechanisms so some of the early adopters became our official AI champions. Our principal engineering community started an AI bar raising program at Amazon. We use the concept of bar raising for calibrating interviews and for up leveling the quality of code and follow-ups after incidents. And so we applied the same concept to AI maturity. Finally we codified best practices into a quick start guide for teams who were ready to move from AI assisted work flows more towards AI native. And it wasn't just the recommended tools but importantly techniques, for example, maintaining live architecture specs or the spec driven development that you saw demonstrated earlier. We also embedded AI reflections as part of. Rituals at all levels, so be it uh. A team retro or an organizational quarterly review we have an AI learning section or time allocated where we share both successes and challenges and that's what becomes that's what makes experimentation. Truly normal. But mechanisms alone weren't enough, we also had to be intentional about. Who we reached and what they needed to learn. If you accelerate only one part of the process of one role on one team, then you're just moving the bottleneck somewhere else so if our development team became twice as efficient and fast but decisions and requirements we're still following the old processes. Then the overall we were not really getting more efficient as an as an organization. So we needed to design a learning path for every role. And not just teach them new tools and prompting techniques but also how the new ways of working and collaborating with each other and with AI. So for engineering um we've ran workshops on spec driven development technique that James shared earlier um and then we've also had training on agent work flow creation. For product managers and designers we were already leaning heavily on QI for um writing and creating prototypes sorry and creating aspects but uh with learning how to use coding techniques or white coding techniques in that case for PMs it was OK we started doing prototypes um as well just recently I ran a workshop for about 50 product managers, um. And we, many of them coded for the first time and it was truly transformative for them to realize how much more power they had to explore some of the ideas and interactions um that they had with AI tools. Our data teams have created interfaces for natural language queries from their data so that they didn't have to be uh bogged down in creating simple queries and instead focusing on more complex problems. And we even trained our leadership to use MCP um to do things like streamline goal reporting or acquiring strategic document knowledge base. Um, speaking of MCP model context protocol, you, you, you'll notice that that's highlighted in every quadrant, um. So every role had to learn how to use access and use um relevant knowledge bases and tools via model context protocol. And as people adopted new tools and experimented more and shared, we realized that in some cases we were moving faster than our processes and guidelines were ready for. So that's where trust and rigor comes in. And I wanna start with sharing a couple stories from like early pitfalls on our way to IN needs transformation. First was the code review overload. As AI got good enough to generate multi file changes. We found ourselves in a situation where tech leads and senior engineers were bogged down by a large volume of large code reviews. So even though the code generation accelerated, what we found is that the, the, this created a bottleneck in the code reviews. And to address that We did a few things. We leaned in into the Amazon leadership principle of ownership and as James was describing earlier, it's not by coding it's really important to truly review every line that you produce, of course. Um, and then we've also built some code review agents that we incorporated into the workflow. There's always a human review that's required, but in some cases we realized that we can leverage agenda code review for some part of the process. And so with those changes in place, the code of your velocity started flowing again. Second was policy and process friction. As teams started building more agents and automating workflow, sometimes fully autonomously without even human in the loop, agents needed access to data. But our review processes were built for human approvals, not agentic approvals. And so the innovation will slowed down uh by the by this friction. What we did was partnered with our security, privacy, and, and other stakeholders to modernize some of the processes and even automate some of the governance steps. Having learned from these lessons early on, we realized that rigor and mechanisms for governance need to be built in at every layer of our system. So you'll remember the I enablement stack that I shared a little while ago. Um, and so at each layer we've built in the right guard rails and controls. Um, at our infrastructure we had to build in things like capacity controls because of course AI usage can can spread so quickly without realizing it. Um, we've also of course enabled IM policies and other mechanisms that we would use for any production workload and so we applied it for our developer tooling and infrastructure. At the model and capability layer we've. Created prompt templates, fallback mechanisms, and rigorous evaluation sets so that every time a new model came out or something else changed we could quickly. Move on and adopt it and have confidence. At the context layer, um, that was probably one of the more challenging ones to crack, but like, um, enabling the right set of authentication controls for remote and local MCP servers and codify the right approach organization wide. Has really helped us unlock um safe experimentation there. And finally at the experience layer. Quiro and workflow specific agents enforce the right permissions based on a combination of single sign on authentication within Amazon as well as the um. Service to service authentication that we use for agents. So with this controls in place, builders didn't have to worry about compliance or be as tentative or uncertain about what they can use and what they couldn't. And the safe path became the easy path to take. So those were the three pillars access and infrastructure, culture and learning. And trust and rigor. The natural question. Is did it actually work? Well I can't share the exact numbers, I wanna be transparent about how we measured the impact because you can't transform what you can't measure. Early on we were focusing on AI adoption, um, and probably the whole first year we were obsessed over that number but eventually it got saturated of course as um everyone was using AI for those kind of isolated tasks so we started focusing in our adoption metrics on more tools specific to different phases always DLC, uh, adoption per roles so we still monitor it very closely, but it's not just pure adoption. And in terms of velocity and efficiency outcomes we use um DOA metrics, the industry standard metrics for velocity, um, and so we've been seeing meaningful increases in our development deployment velocity as well as code review velocity, but of course it's really important to control um for incident rates and rollback rates. So this is something that we always have to keep hand in hand attention metrics. But beyond those organizational wide adoption and velocity metrics we've also dove in deeper into specific areas that we were trying to change with those agents that we built for specific use cases and so that's where we track things like efficiency gains and time gains and um other metrics that are best for the use case. Finally, um, probably the best source of insight is really listening to our builders so regular surveys, interviews, and importantly leadership listening sessions have been a good source, um, for us to learn where we can where where we can make changes and what's working well and what needs an improvement. Here are a couple of quotes from our recent engineering round of interviews. Um, I'm particularly excited about the top one, where this engineer used to spend 3 weeks at a minimum on something, uh, on something that wasn't even coding, so it was not the part of her job that she was most excited about, but it was about designing. Write, uh, writing down the design documentation iterating with the team aligning. With different stakeholders and so with AI tooling this the writing of the document has become done in under 3 days and then she really could focus her time on discussing with stakeholders for integration use cases. And with that it's back to James for an exciting demo. All right, I think I'm contractually obligated to give a demo if it's an AI talk. I don't know. Uh, great. So thanks so much for taking us through all of that again. It's so important like grassroots energy is great, but then that structure of organizationally is how you really make it scale. Now, that story I told you earlier, where I was saying, don't worry about copying prompts, we have something better, um, that story was from way back, like early March time frame or so, and it was a story that I shared internally, and it sparked a lot of excitement, and I got a lot of feedback that many people found it very inspiring. And so I thought it was important to share that story with you all as well. That said, March was a long time ago, especially in AI time. It was a very long time ago. Uh, and we've come a long way since there, I'd posted an internal blog with that whole breakdown, and there were people like copying and pasting these prompts. Well, we've found a much better way to do this, and we just open sourced it less than 2 weeks ago, and it's called Strands Agent SOPs. And rather than read a blog post to you, I'm just going to show them to you. Let me log in here. All right, so this is Give it a minute. There we go. This is the GitHub repository. Um, SOP stands for Standard operating procedure. What is a standard operating procedure? Well, it's a document with a set of detailed step by step instructions that help usually a person to do some routine task or process. In this case, these are agent SOPs, and they help agents to repeatedly, repeatedly follow a set of steps and tasks. So I have Cro CLI pulled up here. I'm actually in, uh, I'm actually in the code base, um, of the strands agent SOP. And I'm going to run this, uh, if we look at our prompts, we have a strands agent SOP, uh, MCP server that we vend. And do you remember back when I said, what is this code base and explore my code base? Well, now I have a Agent SOP that I can run. That generates this prompt. And now the agent says, great, I'll help you run this code-base summary SOP. There's some parameters required, codebase path. Uh, otherwise, I'll default to your current directory. Well, that is what I want. In this case, I'm going to tell it to go ahead and consolidate. So it'll generate a number of documents, but I want to create a consolidated agents. MD, which is The default And that's all it needs. So now it starts running this standard operating procedure. It starts reading through the code base. Now I want to show you what this actually looks like. So a standard operating, uh, agent SOP is just a markdown file, but it's a very specific format. It has a title, it has an overview saying what it does. It has this parameter section that talks about different parameters that this SOP can take. And then it has a list of steps. You can see step one is set up and directory structure, initialize the analysis environment and create necessary directory structure. Now, if I just stop there, the agent might do different things every time that you run it. The magic happens with this constraint section. There's a constraint section that includes these RFC 2119 style keywords like must tell in caps, or should or should not or must not or may. And this adds constraints around how it should execute this step. So in this case, you must validate code path, uh, codebase path exists. Or in this case, analyzing the codebase structure, you must use mermaid diagrams for all visual representations. It, it seems really simple. But with this structure, um, also with these steps, you end up, uh, we also have an example section because LLMs work very well with examples. And then there's a troubleshooting section as well. That helps it with troubleshooting. This format, we have found internally, the agents respond incredibly well. Like our results with just kind of prompting versus uh free-form prompting versus following these, these constraints. The agent, uh, we, we internally, lovingly call it deterministic, that their behavior is kind of the, it strikes the perfect balance between you want it to. Uh, follow the same procedure every time, but you wanna leverage the flexibility of agents. And what we can see is that the summary has been running in the background, or this codebase summary has been running in the background and it's generating these files. So it generates this codebase information file that talks about the technology stack. Has the directory structure. Right. And programming languages used, it talks about the architecture that found in the code base and it uses mermaid diagrams, which is very concise text-based format, um, that Markdown can render. And it just generated a new one, components. And it's going to continue following that SOP. And every time you run this, it's going to generate the same set of documents with the same sections in it. Sometimes the text is a little bit different each time, but it's the same critical information because it's following the standard operating procedure. Now, you may have been looking at this saying, OK, this is great, but This seems like a big pain to write this document. This, this is very long. There's a lot to it. And SOPs would be a pain to write if you had to write them by hand, but you don't have to. So the SOP. the SOP, um, program that we've ended open source, it offers STP server. It also comes with this rule embedded. So you can say, Rule, it outputs this rule. That you can use with. Whatever gentic system you use. In this case, I'm using Qiro. And so I import it as a steering file. You don't have to worry about reading this format. This is just something that tells the LLM that exact format. And with this as a steering file, I can then author these very easily. So in this case, um, I'm going to use this context feature. To add my steering file. OK. So you can see now this agent SOP format is there. And just as an example, I'm in another folder right now. I'm going to say create an agent SOP, uh, that given a person's name. Um, Uh, outfits. A short, fun poem incorporating Their name So all I have to do is chat with it, and then it's going to create this SOP. It knows the format because I put that into its steering files, and there you go. So now we have this SOP that's called Name poem generator. You can see it follows that format. And since I have the MCP server loaded, I'm gonna go ahead and reload Kro CLI and I should see my name poem SOP pop up. So if I run the prompts command, there's now this name poem. I can run it Says, OK, needs to know my name. So I put in my name. And then it wants to confirm the spelling of my name. Y LLM. OK. So, generated some poem. Um, but I can also So I can say Um, At an optional hometown. OK, so now it's updating this agent SOP to add this hometown parameter. OK, so now I'm going to Run this again. Another thing about agent SOPs is if you know the parameters ahead of time. First off, this. So you notice this parameter is called person name, and this is called hometown. Well, if I run this name poem, and I pass it some arguments, I don't, I can, it's an LLM that's interpreting this. So I can just say, my name is James, and I'm from Tucson, Arizona. And it'll parse the parameters out of that. And look, there's my poem. Now it's desert themed because I'm from Arizona. OK, so cute, cute demo, but. This is, we want something more real world, right? I more just wanted to show you what the agent SOP format looks like. However, Note that I have the GitHub MCP server loaded. And I have access to these different GitHub tools that it provides. And so how about I say create an Agent SOP that triages GitHub issues given a repo URL. A little more real world. Anyone have a GitHub repo? No? I mean, I love poems, but. All right, so there's my SOP. Let's go ahead and take a quick look at it. So it creates an SOP, takes the repo URL, it grabs the repository info, fetch open issues, says all open issues from the repository, analyze each issue. And then generate a triage report, and it says to save it to this file, and then present findings. So I'm gonna say, um, update this. I don't. Search for unlabeled issues. Triaging means applying relevant labels and adding an issue comment or or a triage comment to the issue and Uh, don't write the report to a file, just output it. And add a dry run flag. Uh, that Won't actually. Make the updates, but reports what would have happened. For demo purposes. So I'm just able to converse with this agent, and you can see it added the dry run flag. It updates the SOP with what triaging means. And then you can see it's modifying these constraints under the covers. Now, because I have the GitHub repo, or excuse me, because I have the GitHub MCP server, you'll notice that some of these constraints actually name tools. And that's because MCP puts all the tools in context of the agent. So when you're authoring this, you want to make sure you have the MCP servers available that it may need to use, and that'll help it create a more effective, uh, more effective agent SOP. All right. So now I'm gonna run it. OK, it's made the updates. I'm actually gonna run it on the strands agent SOP repo. It's relatively new repo, so not too many issues. And now what you'll see under prompts is I have this GitHub issue triage. OK, it says great, give me your repo URL. And then the dry run flag's optional, and it defaults to, uh, false, but I'm gonna set it to true. OK, so now it's triaging unlabeled issues. So it's calling the GitHub MCP server search issues tool, and you can see that it's applying the no label feature. It's in dry run mode. It outputted that here. And it called out a couple issues. It said, OK, here's a high priority issue. This is, uh, labels to apply, enhancement, feature request, MCP, comment to add, and this is the comment it was going to add. And then medium priority, these other two issues, and. It calls out labels that it's going to add. Now, if I didn't run this in dry run mode, it could, it would actually make those updates to the issue. Uh, I could also go back and forth with it. If I don't like how that's, uh, you know, don't like this comment or want to use a diff different style, I can just go back and forth with the chat. Now this issue number 22 is kind of standing out to me. How about we actually implement it? So Again, I'm in the agent SOP, uh, Repo I just used my codebase summary that generated this super nice agents. MD based off of scanning the entire code base. And, and now there's this agents. MD that gets my agent up to speed with this code base and understanding this codebase. And now I'm gonna run this thing called PDD. This was prompt-driven development, which was an internal predecessor to Quiro's spec-driven development. And this is that issue that I was looking at, I issue 22. I didn't create this. This is not my user. Um, this is an actual feature. Oh, a little bit of latency going on. All right, let's try this again. OK, so this is an agent SOP that takes you through that entire process that I walked you through earlier. So this asks for my rough idea. So I can say rough idea is this linked to this issue. And then as this project directory, this SOP slash planning. So I just gave it this rough idea, uh oh. Yeah, for live demos. Let's see if I get this again, I might have to restart. Oh, OK, great. So now I read my issue. OK. So now it knows It starts doing some setup. It creates this directory structure. So it creates this planning folder that has uh design, implementation and research. It took, it took that issue and it copied it into this rough idea. And then it starts asking, do you want to do requirements clarification? Do you want to do some preliminary research first? Um, any additional context you want to provide. It also tells me to run this command. That way, it pins anything, any documents that are created as part of this process. So I can say, sure, requirements clarification. So we'll start there. And again, I didn't have to share some paste, like copy paste some prompts. This is something that you can download and you can use. So it starts writing this idea honing file, and it says, OK, question one, what's the scope of the tool? This feature request was asking for MCP tools in our MCP server. So it says this issue mentions adding list SOPs, use SOP. Um, tools to the MCP server. Should these tools A, replace the current prompt-based approach entirely or complement the existing prompts, um, or be configurable, uh, I want both of them. And then it'll continue. It'll just ask me questions, and then once we get to a logical stopping point, I can tell it to move ahead to design, I can tell it to go to an implementation plan. Um, and I would absolutely do that for you, but we are out of time. Uh, just a second, but if you're interested in seeing me implement that entire feature, you should come to our code talk or you should come to my code talk later today, which is happening, which is talking about how Amazon teams, uh, use AI assistance to accelerate development, and it's ambitious, but I'm gonna try to implement the whole feature on stage, um, so it'll be exciting either way, come see that this afternoon. Uh, we also have, uh, a session being given by, uh, Ava Knight and Bethany Otto, and they have done a lot of work on kind of how they measure the impact of AI across the company. So that's another good one to catch. Uh, we also have a QR code for the agent SOP GitHub repo, so you can download it, you can try it for yourself, you can give us a star would be fantastic, um, but. With that we're at time. I really appreciate your time today and thanks so much.