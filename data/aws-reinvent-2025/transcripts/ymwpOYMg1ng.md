---
video_id: ymwpOYMg1ng
video_url: https://www.youtube.com/watch?v=ymwpOYMg1ng
is_generated: False
is_translatable: True
---

It's great to see you all here. My name is Sartha Khanda. I'm a principal product manager with Bedrock Agent Core team, and I'm joined by Vivek Bahaduria who's my principal engineer counterpart. And today we are going to talk about something that every team building AI systems deeply care about, how to keep your agents out of trouble. We are at an exciting point in the industry where how software is getting built and used is rapidly evolving. Startups and enterprise teams are building completely new agentic applications that can understand users' goals and then work across tools to get the job done. But the moment you start giving agents the ability to take real world actions, like calling APIs, updating records, or making transactions, every misstep can have real world consequences. So in today's session, our goal is to explore how we can ensure that the agents act safely within their intended boundaries so that we can deploy them in our most valuable workflows and give it increasing level of autonomy. We'll first start with why this matters, the promise of AI agents and what can go wrong if the right control mechanisms are not in place. Then we'll walk through different layers of agent safety controls that are needed to put agents in production. And finally, we'll go deeper on how you can implement these controls using Agent Core and walk through a deep dive example. So let's jump right in AI agents are a new, uniquely powerful class of software system that are able to understand users' intent. Plan actions and execute multi-step workflows in order to achieve a goal. It's a fundamental shift in what software is capable of doing. Unlike traditional programs which follows a fixed set of rules, agents figure out how to achieve a goal dynamically, which makes them a lot more adaptive, context aware, and flexible. This adaptability is already transforming how enterprises are using software in their daily workflows. We're seeing customers across every single industry vertical gaining immense value from agents in automating, in automating complex workflows. Think of automating uh IT ticketing process where you triage and resolve certain tickets. Agents are also being used to generate structured content like knowledge artifacts, design assets, or even software code in ways that were completely unthinkable just a year back. And finally we are seeing a lot of adoption of agents for decision support, where agents assist humans by gathering the right context, surfacing key insights, and taking actions on their behalf. But what makes agents powerful. Also makes them high stakes. AI agents Can take unsafe actions without the right boundaries and controls, and this is a big problem for organizations who are trying to scale their agentic workloads. In the real world, using AI agents can produce several risks and challenges. First, agents have unpredictable runtime behavior. Even when you design your agents carefully. The agent's decisions at runtime often depends on the evolving context of user's input and tool responses, all of which cannot be predicted completely during the design phase. Second, agents may attend to take actions that are outside their intended scope. Think of agents trying to trigger workflows that are only meant for admins, or agents trying to kickstart a high value transaction without the right authorization. Agents may try to access and modify sensitive data, or they may completely misinterpret business rules, for example, taking actions that are aligned with users' intent but not aligned with companies' internal policies. So the real question is, how do we give agents the freedom to act while making sure that they do follow the intended boundaries. Let's try to understand that with an example. The right mental model to think about agent safety is to treat it as layers of protection. You want to control how the agents run. Control what it's allowed to do and also verify what it actually did. Agent safety starts at the runtime layer. In this particular place, the key goal is to ensure that the agents execute in a fully isolated environment so that one user's workload does not interfere with that of another. This is your first line of defense where you establish a safe boundary that contains what can actually happen during execution. Now, once your agent is safely executing, the next step is to give it access to tools so that it can take meaningful actions in runtime. You want to give your agents access to tools, APIs and services via a gateway that acts as a controlled boundary through which every single tool call must pass. It is exactly at this boundary that you want to implement policies which allows you to define and enforce fine-grained rules on what your agent can do and under what conditions. So for example, in this particular case you can define a policy which says don't approve a claim above a certain threshold. Now, let's say your agent has executed the next obvious step is to know what it actually did and how did it perform, and that's precisely where observability and evaluations come into play. In this particular layer, the key goal is to be able to trace every single agent decision that was being made during execution so that you can see which tools did the agent try to invoke, whether those tools were allowed or denied based on policies, and what was the final outcome. Now this brings together the entire safety framework where different layers help you contain, control, and verify agents' behavior so that you are sure that they do actually follow the intended boundaries that you plan for. Now, to dive further into how you can implement some of these controls with Agent Core, I would like to welcome Vivek on stage. Thanks, Aar. Um, I'm going to dive deeper into how specifically Agent Core helps here. Um, And I'm going to start with maybe a quick raise of hands. How many of you have built an AI agent? Cool. Quite a few. Um, how many of you have actually put into production? OK. One, how many of you who have put in production actually feel safe about the security controls that you have in production for AI agents? Yeah, and that's expected. That's the current state, right? Um, and that's there because as an agent developer we have to constantly make this, uh, uh, trade-off between how much autonomy to give to an agent and how much safety controls we want to put. The more agenttic we make an agent, the more autonomous it becomes. We feel that and because of indeterminism, because of the lack of safety controls in the agent development frameworks, we don't have the full safety guarantees that we have. And this way with Asian core runtime we are trying to solve a few of them with a layered approach. Let's dive deep into it. And this is where, how to contain what, what can happen within your agent. Now, my definition of an agent is a simple one where it's an LLM in a loop. So you have a, you have a task at hand, you have given an LL you're using an LLM and that LLM kind of runs in a loop to achieve the purpose and use the tools that it provides. And that's what an agent code really is, like you can use any agent development framework, sometimes you have some controls, sometimes you have less controls, but effectively using an LLM for reasoning and then you're acting through tools. And that's the code that runs in agent code runtime, and we provide a strong isolation boundary across every user session. Think of it in a way where every user session has its own memory, has its own file system and CPU resources that are not shared by anyone else. Two user sessions cannot cross boundaries, cannot access each other's data, and there's no cross firing that can happen inadvertently or even if there's an adversarial attacks that's going on, and that provides a very strong guarantee for you. It's as if there's a clean room that is created for every user session and better yet, once that user session is done, everything is removed, so there's no after the fact user session can come in and use any residual data or context from that. Now that's a great guarantee for. Agent sessions, imagine that if there was a concurrent session that is running in typical workloads like microservices, um, there could be a possibility if agent has access to the state it can store state on behalf of one user and may access that same state on behalf of another user. That would be very, that would not be possible with a strong security and isolation boundaries with agent core run time. We are using a lot of our technology that we have built over the years in AWS like Firecracker, Micro VM that we use for a lot, driving a lot of our serverless architectures to within this. Post that there is now we are containing what an agent scaffold or agent code can do, but agent interact through their external environment via tools, right? That's, that's how they can do so even if we are containing what it can do within that runtime, there is still a possibility that through the tools that we have provided it can do a lot more. It can access state. It can affect environment. It can exfiltrate data, or it can access inadvertent data that you don't want or it's not trusted. And that's where it's important to curate the tool that you're providing to your agents. It's the same as uh lease privileges that we try to do with every other microservices tool. What actions it cannot do, do not provide it. Uh, an example, a simplistic example would be that if you're building a chatbot that should not give refunds to your customers, don't provide a refund tool. So curation of tool does help. And with agent core gateway, it makes it simpler because you can provide your API resources, your lambda targets, or even your MCP targets and sort of create this abstraction where you are curating the tools for an agent, um, and, uh, it works totally with MCP compliance and maybe quick raise of hands, how many of you already know about MCP or model context protocol? So quite a few, but for those who do not know, like there's a very, it's a protocol that was launched last year in November, sort of, and what it does is that it provides two functionalities or a simplistic way two functionalities to an agent where it, where an agent can discover tools through. Tools so it can know what tools are available to it and then it can invoke tools in a very uniform manner and that's what MC protocol dictates. So any agent can use those MCP servers to discover what tools are available and then use those tool definitions when it's required and invoke them. Agent code gateway is full, fully compliant MCP server that you get out of the box, but what's more important is, even if you have curated that tool. With lambda interceptors that we have launched, you can do fine grained access control. So these are the interceptors that work before every tool call and even after every tool call. You can now enforce policies which are much more fine-grained. A certain identity may have access. A certain IT may not. Have access a user that has booked a ticket should only be processed, should not only be eligible for a refund versus a user who is not. You can, you can actually implement your own business logic and business rules there. It's a nice abstraction because it gives you those controls which is outside the boundary of an agent code. So, um, uh, and again this helps because this is more enforceable and everything goes through it. The next one There is still a problem here, right? Now that we have contained what an agent can do, we've also given curated set of tools. There is still stuff where what if I work and often agents work on behalf of users, so they obtain an identity from a user and they work on behalf of it. Now if there are multiple users that an agent is working on behalf of, how should it not crosswire? How does it implement this on behalf of flows in the right way where the identities are propagated the. The authorization authorization controls are implemented, violates accessing external resources, and this is where we developed agent core identity to do 3 purposes. It gives an identity to your agent, that's one. It does inbound authentication, so you're end user, you can bring your own IDP you can authenticate much like any other services that we have we authenticate who is coming in, but more importantly, it also does outbound authentication. Agents are built to interact with their environment. We are giving tools to interact with their environment. It can do on behalf of flow and agent core identity managers credentials for you, token management and credential exchange for you. agent code itself doesn't have access to those credentials. It only gets token for the identity that it's working on behalf of, and then it can use it to access the tools. Now that we are also ensuring that the traditions are right, we are using the right tools and it's contained, there is also still stuff to know what an agent is doing now. What we define as a simplistic definition of an agent, it's an LLM in a loop calling tools. It's incredibly hard to debug this. There are no pretty workflows. There are no code to go back to and see what's going on and why it's happening. Uh, every request can take a different path in that whole flow. Um, elements are incredibly resourceful too. If a path fails, they can take another undefined path that you don't know ahead of time, and that's the feature with observability. You can see all the session traces, um, in like across all the components including tool calls, LLM calls, agent code, and so on and so forth, and create, uh, over the period of time what your agent is doing, you can get a view of that. You can query cloud watch logs or you can have readymade ready-made dashboards that you can see. You can also see over the uh over the period, ready-made dashboards that says how much token usage is there, which tools are failing often, which tools are not, what are you, 4 XX errors, 5XX errors, and all of that is available. With this, there is observability, but There is still a problem If I know what is happening but I don't know what is right versus what is wrong, making all the right, making all the tool calls successful doesn't still mean that the agent was helpful. It achieved the goal success or what I'm trying to do or it's relevant to my end user. This is where the next thing comes agent core evaluation that we have launched only today. Um, and this is how it looks like. So there's a user making an agent call. And then we already know with agent code observability all of those traces whether from agent code or from related agent code services like gateway runtime identity, all of them get into observability all of them track through one session ID that you can see all the traces. And now what you can do on top of it is that as an agent owner you can define what your criteria criteria are for saying that what is right versus what is wrong for your agent. You can define your auto verifiers, auto graders, or we called evaluators for your agent. There are 13 pre-built evaluators that we offer, but you can write on your own. So once you write, and the typical way to write them is LLM as a judge, so you define based on the input is the session trace, and you define to you what is right, what is wrong, and you can evaluate them. And the good thing is all of this also comes back to agent observability. So again you can come to observability and session and you can see based on your evaluation whether it was helpful, accurate, and whatever the dimension that you're interested in. You can also see a dashboard where you are seeing over the period of time which sessions are the ones that are falling short of your expectation, which sessions are exceeding. That's the monitoring traces. The good thing about this you can do it online, so you can continuously evaluate. You can set up your pipeline with samples, certain percentage of requests, and continuous and whatever evaluators have configured, it can keep evaluating them. You can do use predefined evaluators. There's a dozen of them, or you can define your own, and you can use it at testing phases too, which doesn't have a dependency on cloudwatch. You can just call when I'm developing an agent to test my evaluators or test my agents as I'm developing them. Now this is great. Now I can know what my agents are doing, verify continuously what they are, how they are performing, but with safety, I don't want to take chances of like after the fact. I know that after the fact that maybe it didn't do the right thing. I need more deterministic controls right before uh an agent is acting or if I know what is going bad, I want to affect it so it doesn't do bad. And that's where policy comes in and policies is works on top of gateway. So let's take a flow that happens without the policy. There's agent, there's an inbound tool call an agent calls the gateway for the for the tool invocation, and the tool invocation happens from the gateway. Now with agent core gateway as that interception layer, you can assign policies. You can write policies, deterministic policies. That apply at the every tool invocation and that get deterministically enforced now the nice little nice stuff is that you're not relying on agent code to enforce it. You're not relying on tools to have that business logic as an agent developer you can deterministically put it in the agent code gateway. Similar to evaluators, you can define it ahead of time, so there's a policy admin which comes, defines those policies, and once they are defined and associated with the agent or gateway, they get evaluated for every tool call that you make through that gateway. And with everything else, all the policy allow and deny decisions gets locked in the agent core observability again against the same session trace so you can see the full trace of what's happening. We kind of discussed some of this already. So agent core policy provides this unified protection for you. It intercepts and governs all agent to tool interactions. There's an operational simplicity to it. No developer integration is required for each tool because it's applied at the gateway level. It's not per tool. That you can do. You can also associate the same policies to multiple gateways too for agents. There is auditability. All the enforcement decisions are logged and traced into the agent core observability. And performance is a key metric here because imagine it's happening on your on your transactional path in your online path and you want to have millisecond latencies overhead there. If it takes a lot of time or it is, it is very resource intensive, it cannot be applied for every tool call without the performance trade-offs. Let's dive deep into a use case and understand how overall policy will work. And we are taking an insurance agent as an example. Uh, and a typical agent, uh, user sends the prompt. Um, there's an agent is running an agent called runtime, so that's the container unit. It has its instructions. It has its tools. Some of the tools are local, that's implemented within the agent scaffold, and then there's an agent context that's available. It calls, it makes model calls which are external to the runtime, and it makes tool calls which are also external via gateway, right? That's the general 4. There are 3 tools that we have provided it. There's an application tool which helps it create an application. Um, there's an invoke risk model tool which helps assess the risk model for that application that is created, and then there's an approval tool where somebody can approve that I approve this insurance application. Now that's a very simplistic insurance application. If I were to put policies on it, these are the three steps that looks like. Step one, policy authoring, and in here I want to enforce a policy which says. Uh, in natural language, um, let's start with that, that allow the user to call the application tool with the coverage amount is greater than is under $5 million so no policy coverage, no policy application where coverage amount is more than $5 million and the second one is much more interesting for me, the second statement, if you're not able to read, I can just repeat it. If a user has a principal tag called department, so if an identity that is calling has a. Additional attribute that department is finance, then only they should be able to do risk modeling and this is sort of uh something where if there's a different user that is calling not from the uh finance department, they should not see the risk modeling uh and this is an identity based kind of a a validation, not a simplistic uh validation on just the tool inputs. How many of you are aware of cedar? OK, good to find one, and for this you don't have to know CEA though, but I'm just showcasing it here is we are enforcing them through CEAR and that's very important because CEA is this policy authorization language that has been developed before, uh, Gen AI. It's already supported in API gateways, lambdas, Amazon verified permission is developed and open sourced by Amazon. And it's sort of a very formal policy for policy authorization. It's much more like Rigo if anybody is aware of our Cubanities and stuff, very similar to CEDA. And what what what is important here is that even though there's a natural language definition, there is a formalization of it through CEDA, so you can actually see and enforce it through that that also gives it much more performance and determinism. Um, the natural language really helps in ensuring that if you're not good at authoring these, you can just start with natural language. There's a lot of verifications that we do that it's actually right, um, and in here it already shows that it's valid policies, then you can apply them. Second one, once we have created these policies and associated with the gateway, now you can just start calling your tools or or start running your agents. Same flow looks like this. We send a prompt. uh, agent reasons through the model, decides on a tool, but before invoking the tool, now there's an interception for the policy. The policy gets enforced whether it's an approved or a denied decision. In this case it's an approved, so it goes ahead and makes the tool call. The tool result comes in and goes back as a response to the end user, right? If it was a deny call, it would have returned directly from there and would not, would, would have moved forward. The third step is the observability, whether it's allow or deny. You can come to your observability traces and see what is going on, what policies are approved, deny the decisions of it. You can see the payload information. You can see which resources, and you can also see, uh, which is not captured in here, we'll show it in the demo. You can also see over the period of time how your policies are working, how many sessions or traces they're denying, how many they're allowing, and you can see the distribution over a period of time. With this, we can Actually see it in action too through a demo. OK. So this is Agent Gore console. Let's start with gateway. I've already created a gateway in here. The gateway name is what we selected, and you can see inside this gateway. Um, gateway associated with an identity provider cognito. So there's a cognito identity that I'm using for inbound. There are 3 tools configured going inside one tool, and these are tools that we already talked about. There are 2 properties it has, uh, it needs coverage amount and region as an input. These are the definitions that goes into the model. This is how the model knows how to call it. Now with these tools and with this gateway, there is also log delivery. This is where the tracing comes about. There are vended locks that you can configure where this lock should go and the tracing is enabled as well. So all the observability will be enabled for this. Let's quickly run it. There are no policies yet, right? It should just run, uh, it's a very bare bones agent. Forget about custom claims now. We did not add any claims. There are 4 tools here that are created, and, but we only added 3 tools. Um, and does anybody know where this 4th tool came from? OK, I'm not surprised because it's very custom for the gateway that we have added, and the way to think about it is that. In this case it's not useful, but there's a search tool that we have added. Now it's not there in MCPSpe yet, the search functionality, but you can actually do contextual search based on what your, what your agent is processing and just get the list of tools from there versus all the list of tools. There are just 3 tools here, so it doesn't matter. We can just list all the tools and provide it in the context, but imagine there are 30 or 300 tools in your that you're providing all those tools and you don't want to provide all of this in your LLM definition. Then search can come in and help. Let's move forward with the demo. So in here, now I'll just try to make an application request providing the US region. We wanted a region and a dollar amount, $10 million. And it's a dummy implementation, so it will approve, it will just create an application for me for now. Create an application. This works, uh, right now without any policy enforcement so far. Now we can go back to the console and try to see how to enforce, how to create our policies. So here we are. And this is the policy preview. Um, you can come, you can go to create policy engine. Policy engine, imagine it is a container of policies so you can create many policies, all of them go into one policy. We'll name it in this case, underwriting use case. And now we will select a resource. Our resource will be our gateway. That's where we are applying these policies. We'll select the resource. This is the same natural language that we talked about and went through. So again, the same natural language that we're saying don't allow if it's more than $5 million and don't approve, um, and only risk model access should be enabled for a for a principal or a user which has attribute finance, right? In real time you can see CEA policies being created. You can even iterate through that, uh, as you're authoring these policies. Uh, in this case I'm showcasing I got it right in one flow. You may take a few iterations. You can see your policies, see the valid if they're valid or not. You can, and, and even not going through a lot into Cedar. There are 3 sections or 4 sections to it. There's principal action resource, which identifies who is accessing the action is the tool, the resource is the gateway, and the when is the important part here. At what conditions are you allowing it. We validated both. Both looks fine. One is on the principal hashtag department name and the other one is the right dollar amount that we wanted to do. All of this looks fine now we can just create these policies. There's a validation mode I'm just choosing for test purposes, uh, permissive. We created these policies. Now they're still not associated with gateway. So the next step is to associate with a gateway. So I'll just associate with my existing gateways. We already created a gateway that we had before. I'll just go select it, and there are two modes for test. You can just do emits log only, but it doesn't enforce, but it says what the decision would have been. It's very useful for you to see, like, do a dry run on it before enforcing it, but in here I want to show it in a demo where it actually prevents, uh, um, from calling that tool so it's enforced. Cool. We can go back to our agent now, now that policy is enforced, and run the same agent, right? Again, there are no custom claims yet, so it all shows none. And in here it's again very interesting right now and if anybody is noticing um. We only see in available tools one tool right now. Um, instead of like 4 tools that we had before. And can anybody guess the reason why there's only 1 tool now? Um, well, what it means is that with CEA, it's the deny all is the default. So by default, unless you have a permit policy, it's deny all. Now we only configured permit for two tools. So the first two, the search tool, there was no permit, so that's denied. The third tool that was there for approval, there was no permit, that's denied. This user doesn't have the right department. There's no department set, so the risk model tool is denied. The only one that could be. If the dollar amount is less than $105 million is the create application and that's what agent sees. So this is a very good way where your agent never even sees the tools that it cannot use, uh, and that's pretty important because then it cannot hallucinate. There is no loops where agent tries something and then it fails because policy denies and then it comes back. OK, we'll move forward. And I'm going to try the same thing. I want to create an insurance application. US region and for coverage. Amount of $1 million. This should be allowed uh based on our policy and it should go forward. OK, so this work. Now we can try, but it doesn't showcase that our policies is really working. So we can try a number which is greater than $5 million. Let's say in this case, uh, $10 million is what we can try. Application creation. In the US region And amount is $10 million I'm just specifying additional instruction in here to make the tool call again because it's in the same context and agent sees that it already created an application. It generally sometimes says I already created an application, so just I'm providing additional information in here and if you see it actually failed to call it because the intercept happened with the policy and it failed. We can go back and check. And now you can see there's a few traces, right, that was there before. We can go there and see if observability is helping us. You can come here This is an auto filter that gets configured here because you come through the traces based on the policy engine. There is one interaction that we have had so far, one deny, and you can see that one trace, that's why you can come here and find the information. For that. Yep. So it says policies enforced, uh, and it says the interaction that is out there. Now these are the metadata that is there for the system to see, and you can actually filter and configure metrics on all of these. So these are pretty useful if you want to have these dimensions of deny, access, policy engine, or metadata around agent or gateway, where you're applying these. Now we can go back and try the other policy too, which to me is a far more interesting one, which combines the principal attributes and the context that we are providing. OK. We'll go back and Try this again. Good And this time, I will add custom claims to my identity. So by custom claims, I, I'll add those tags that The department of the user will be uh finance and there'll be other things like cost center and stuff in that and we can see here and what this means is that in your job token, the Java web like the JSON web tokens that gets to to here, those claims will be present, those attributes will be present, and then you can run your policies against that in this case, if you see now. It is their department name is finance, employee level is senior, and cost center is XYZ. These are three attributes that are provided by the IDP that gets all the way, uh, propagated to the gateway, and now you can implement policies on this. Now in this case, and this is something that is important, previously we did see that there was only one tool that was listed. Now you can see there are two tools listed because the other two we didn't have any permit policy. They'll never come unless we add any permit policy. But for the other two, the both conditions are satisfied right now. So there's a department in finance. So that's why the risk invoke risk model tool is available to the agent and it can use it now. We can go forward and try the same. Approach now where Now we'll try to just trigger the risk model and see how, if it makes the call or not. And if you cannot see, yeah, I'm typing in. Uh, I first typed in, I want to do the risk model, but I didn't provide all the information and it said like it needs more information, and that's what, uh, I'm providing in here. Once I provided, it did the risk modeling and it provided me the status as success. So the call, the call did go through. The counterexample to that would be I can add a department. And said the department. Other than finance, say, engineering, which people do not maybe trust us enough for engineers to make this finance decision, um, and you will see that the list tools itself will not show. And in here you can see the list tool itself is now showing just the create application and not the risk model. Um And if you make the invoke call, the same interception will happen, and it will not go through. Um, I'm going to move forward from this. So that's the full policy in action that you can see both the policies got applied, and the nice thing about it is that it gets applied to both the tools available to the agent and at the invoked time and across all the tools that are available via the gateway. Now, coming back to the agent safety framework and the layered architecture that we have, runtime helps to contain what can happen. Identity helps to control who is allowed to, to act, to act. And what actions they can take, gateway controls what tools you are providing to an agent, what an agent can do, and with observability, you know what happened. We're enhancing both gateway and observability now with policy to now have fine grid access control over this and imagine this lambda interceptor that we talked about is another way to implement fine grid access control, much more do it yourself, and policy is much more managed for you. Um, then observability helps you, beyond observability, evaluation helps you verify if your agents are working or are doing, are working right for you. With with all these features where you want to get to is that this trade-off between agent performance and agent safety should not be pushed to the application developer. There should be foundational controls that are provided through agent platform infrastructure that helps you build safe agents without trading off autonomy. For the AI agent applications. If you need more resources, here is a launch block for the policy, and there are more GitHub code samples that are ready to try for you. Thank you and open for any questions.