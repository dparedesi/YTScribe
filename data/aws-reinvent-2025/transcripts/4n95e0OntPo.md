---
video_id: 4n95e0OntPo
video_url: https://www.youtube.com/watch?v=4n95e0OntPo
title: AWS re:Invent 2025 - Accelerate Developer Productivity with Amazon's Generative AI Approach (AMZ309)
author: AWS Events
published_date: 2025-12-03
length_minutes: 51.72
views: 488
description: "Get an inside look at how Amazon harnesses generative AI to transform developer productivity. Learn how the combination of AWS Bedrock and agentic AI frameworks powers intelligent developer tools that streamline workflows and automate routine tasks. See real-world examples of spec-driven development automation and AI agent integration. Walk away understanding how to implement similar AI-powered development tools to boost productivity and accelerate delivery in your organization.  Learn more: AWS..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

Over the last year, there is one statistic that actually kind of. Made me think a bit differently about the challenge of developer productivity. Uh, and that is that about Most developers just spend 30% of their time really writing code. The rest, they do. Documentation Ticket management Meetings, more meetings, right? I mean, if you think about how AI is working with, with developers, right? About 2 years ago, AI would allow you to autocomplete a line of code, kind of helping you go faster when you're building. Today AI can help you build an entire feature from A requirement stock. And that not only, it's not, it's not a small improvement if you think about that, it's more of a shift on how building software actually is gonna look forward in the last, in the next few years. And about how software developers really reclaimed that 70% of their time that was spent elsewhere. Welcome to Reinvent, uh, whether you came in yesterday, Sunday, or today. Uh, thank you so much for joining us. Um, Today, we're happy to share how Amazon is using generative AI to enhance developer productivity. And I just wanna say a quick note, right? What we're gonna be sharing today, it's not just a product pitch. We're gonna be talking about what we have built, what we have learned. And how we've been measuring what we've been putting in production. Um My name is Alex Torres. I'm a solutions architect and I'm joined here, uh, by Steve. Uh, he leads our, uh, the, uh, the team that is driving AI native development within Amazon stores, and he's gonna be talking more, a little more about what they've built, their team's built, and what they're driving. Um, So, if you're ready, um, let's get started. I'm gonna be talking a little bit about the journey of AI. I will be level setting where we are, how far we have come since AI was released, how AWS enables builders internally and externally, and then we'll dive into how Amazon is taking what's available today, what we have to build to complement what was available when we started building things, and we'll leave with some takeaways that you can take back and maybe run by your teams and implement. So let's take a look at two years ago, right? If you think about 2023, AI launched. And everybody kind of was asking themselves, what is this? What is a prompt? Do I need to become a prompt engineer? A lot of people started kind of working through building POCs, understanding the technology. AWS at this point launched Amazon Party Rock. I don't know if you all remember. And we launched Amazon Bedrock just to enable experimentation, to allow people to get familiar and make the technology available. In 2024, as we were building internally and all, a lot of our customers were building with AI. We started seeing more of the year of production, like the POC started becoming a reality. We started deploying to production. We, we released the initial version, version of Rufus, if you've heard about it this week or if you used it on the app, um, it's our AI shopping assistant. We released Q Business. We released Code Whisper and Q Developer, right? So, we started launching production applications. And the questions kind of shift, right? Like, how do I keep my costs low? How do I start thinking about security a little bit? What is the project that I should prioritize? And as team started building and deploying to production, we got to this year, 2025. And in 2025, which is what I've been calling the year of proven business value, uh, The question shifted a little bit too, right? Like, hey, I have all these AI tools, they're awesome. They can really increase productivity. But how do I make sure that people use them? What, how do I make sure that if people are using them, they're using them the right way? How do I make sure it is secure and compliant, right? Things that while you're experimenting that kind of fall back, but now that they're production, you need to keep those things secure. So I wanna talk a little bit through this journey, how we have seen customers actually starting building with AI. And where that has taken us, right? Most customers start their journey when they're building applications um with AI or using generative AI or anything of that sort like Enhancing some of their role-based processes that they have in place. On the right. You can see The left, sorry. You can see that, right? Like think about, hey, I have this process that has 5 rules and I need to do them, so now AI is gonna run them for me instead of me. The problem that we see when in this stage, which is more human, it's needed, is that anything that is not expected happens, the workflow fails and then you have to restart and things kind of don't work. As models got smarter and as companies kind of move forward with their AI journey, you go to that assistant kind of uh application. Think about that chatbot that people have that have knowledge bases how you allow democratize a little bit more AI access. And it helps you. It can help you maybe summarize a document. It can help you um on the context that it has. It can help you find a wiki, it can help you do some of those things. It's better. But it still needs quite a bit of human oversight. This year with Agentic guy, we have seen that shift of. Maybe being an assistant, maybe enhancing some of the things that you do to become a collaborative part of your day to day workflows. Think about moving from that, summarize my document to help me prepare for my customer meeting tomorrow, or write me a spec to build a new feature, or these are my customer requirements, what can I do with them? And the agent by itself start kind of thinking, figuring it out. You don't have to give more instructions. They become smarter. And that's kind of what that collaborative. Step S, that's where most advanced companies are today. And then we have the pioneer, like that's the people that are really ahead of the curve. This stage is actually rare. It's where the AI is autonomously working on its own. It spins up, it assigns task tasks to itself, it runs them, it checks with the human, right? But they just required like that the governance high-level overview. So as you think through that, most, I think most of you, I don't know, I'm gonna assume are in between assist and collaborators you're using or building agents or in the developer space, and that will bring it back to what we're talking here today, right? We moved from Q developer, really helping you. Understand a code base or a function to Kiro that is able to understand your code base and help you build features and that's that collaborate collaborate stage of AI developer agents, right? So, when it comes to gentic guy in the development world, right, we have to talk where most of the time goes when it comes for uh to, to developers, right? And the reality is what I mentioned earlier, 30% of the time typically goes on the right side, coding and, and development. The rest of it, the 70% of the time is planning, documenting, going through reviews, uh, optimizing things break that is escalations, etc. right? So I wanna split in two and I wanna talk about a little bit that journey that we've taken for coding and then I'm gonna come back to that 70% and then I will hand it over to Steve to talk about what we're building. So, the way that AI is changing software, and that's coming back to what I was saying, right? Think about how we like 2023 Code whisperer, if anybody use that, um, help you out a complete code faster, that 30% of your time just was more effective. We moved to being able to understand a a code or documenting a larger file, generating a more complex, maybe function or something like that. To fully completing a task, end to end, right? Like, build my UI, um, figure it out, and that's where we are today. The problem is that if there is no standardization and every developer does whatever they feel works, you end up with inconsistent results that really don't allow you to deploy that code to production sometimes, right? Uh, has anybody deployed any AI generated code without reviewing it and Production came down. I know of a couple of things. OK. So, that's what we started coming with that idea of spec-driven development. I'm pretty sure most of you recognize our friend, Kira, right? So, the idea of spec-driven development is just helping teams standardize the way that they think. A feature description, a feature request, a GitHub issue can be anything that anybody's writing. But how does our team that is building a complex application take that? Validate the user requirements. From those user requirements, from those specifications, build the storage that you need to have a consistent technical spec. That builds those or specifies how those things are gonna be built, right? Like, consistently for every team, for every product that you do. And once you have that information, how do you make sure that it's built in order that it doesn't break, that there's no orphan code that you use test-driven development? That's where those implementation plans and that consolidation happens. So, that's how we started thinking, moving from vibe coding to a more structured way that can allow you push to production. All of this has been possible because today we're actually at a tipping point, but today, this year, this quarter. Think about how far the models have gotten, right? This year, we actually got to calling working. Really well, right? Like the agents are able to recognize what they're doing, what tool to pick, and they can access external systems. We, there is a lot of MCP servers out there tooling that wasn't available before that enhance how our models and our agents actually interact with the things we do. Opening GitHub requests. Uh, pull request, reviewing your code base, etc. And as people build, there is a lot of frameworks that are open-source that allow you to not have to start from scratch. So this allows faster experimentation, it allows failing fast. You don't have to come up with how am I gonna build my agents, right? You can reuse some of the code that is already out there, some of the frameworks, the tools, etc. And this is really great, really great for coding, right? However, going back to that 70%. There is a lot of overlooked productivity drains in a developer's day, and we've been doing a lot of uh. Studying how our developers do, right? And if you think about that 30% that developers spend writing, you can think about ticket management, status updates, go to Jira, go to Slack, come back, send, like, get everything going. That can easily take 20 to 30% of your time. Think about meetings. How many meetings do you have a week that could have been an email? How many meetings to really drive value of what you're actually building or pushing forward. And if to Providing status updates, managing your ticket queue, going to meetings, you add the process complexity of architectural approvals, security reviews. Security approvals. Finance, etc. You are already at that 70%. Let alone the developer that knew how something works left and the code is not documented. You're using a new API. There's no documentation. Now, you have to figure out how those things work, right? And how much time goes into actually learning something you've never used before. Maybe you inherited a package that is written in a language you've never seen before. How much time are you gonna spend learning how to do those things? And this is where AI actually shines, like reducing the friction of anything else that is not just Building code, writing code. So, from that perspective, how can actually agentic AI help developers, right? Software developer agents, yes, we've talked about them, but you can specialize them. And you have an agent that actually just writes Java and he's amazing at writing Java or doing. Unit tests. Imagine automating if you're, maybe you don't use AI for generating code, but every time you finish building a feature, you have an agent that automatically snaps and writes all your unit tests for you. And the sister agent. Writes all your documentation. Who here likes writing documentation? Uh, thought so. Right? The same thing. Imagine that now you submit your security review and you have an agent that actually does everything, tells you all the vulnerabilities, and you are able to remediate them before you go through the security review. So you start skipping steps. You start reducing the amount of time that you spend in the queue. Like there is always steps that you can't skip, right? Please do not avoid upset, but if you can just have them something that they can quickly approve, that reduces the amount of back and forth, unnecessary meetings, etc. In the same way, transformation agents, right? Think about migrating something or maybe you're refactoring a code base from one language to another. With that being said, building agents, what I was saying is not simple if you haven't built one before, if you haven't really worked with agents. It can, it can be intimidating. That's why I wanna talk about strands. I'm pretty sure you all have heard about strands, and if not, it's our SDK to build repeatable agents or reusable agents on, on AWS or anywhere for any for, it's just open-source, it's in GitHub, you can check it out after the session, but it can let you get started within minutes. You can try the agents locally, you can plug any LLM you want. Um You can integrate with MCP servers, native tools, custom tools. And it allows you to quickly throw things out. If it doesn't work, start again. If it works, you trade over. Like, you have these templates that you can use. If it works, tune it up. Make it work better. Make sure that it works with your tools. And when it comes to, no, you have your agents, you have, you have them build, you have all of these things. I wanna think about, I want you to think about how you could leverage Agent core. If you haven't thought about like, I know there's a lot of build your agents to place them on agent core, etc. but think from that developer perspective. If you start building agents into your pipelines, you could actually leverage agent core for these agents to run and help you automate a lot of the things that you're doing, whether it is communication, whether it is code reviews. If you're not familiar with Agent Core, Agent Core allows you to just run your agents in any framework. You can use and graph, you can use strands, and you just run it. It allows you to connect to your MCPs through, uh, Agent Core gateway, so you can have as many tools and it, it manages that semantic discovery for you, so you don't have to overload your context window. Um, we offer tools by default with Agent Core browser and the code interpreters. You can run your functions. If you have functions that you want your agent to just run and doesn't have to be an MCP server, you can also do that. If you are implementing memory, short term or long term, to improve your users or your customers' experience. You can, it's fully managed and it discovers that for you. If you need to authenticate, we have that layer for agent core identity. And Of course, we wanna make sure that you're able to trace and audit everything that's happening, so we also build that layer of observability. With the idea that as you're building your agents, whether they're developer or not, you know what's happening. So if you have eventually on that pioneer stage that I was talking earlier. You have an agent that automatically runs and does code reviews for you or finds bugs or security scams and fixes them. And you have cuts the PR and somebody in your team reviews it. Right, this is that that's one of the use cases that you can think of, for example, on how Agent Core could help you automatically build that fully autonomous agent that, hey, I need to do these things. Thank you. With that being said, right? I wanna recap really quick, um. Over the last 3 years, the landscape has changed a lot. Every 3 weeks, there is a new update. Tools changed. And we have been adapting. We, we started building things when we're not ready. So, I'm gonna hand it over to Steve to talk about how Amazon has built tools, how Amazon is looking at improving developer productivity, and how have we've been measuring that. So, thank you. Thanks, Alex. As Alec mentioned, I'm Steve Tarza. Uh, I've had the pleasure for the last 6 to 8 months to come to work every day and think about generative AI and how we can accelerate developers at Amazon. When March came around this year, someone came to me and said, Hey Steve, do you think we can make Amazon developers 3 times faster at getting features to customers? And I thought, wow, 33 times. OK, I've seen, I've seen percentage-wise increases, but I haven't seen these step-wise changes. And so at the beginning of the year, as I say in March, we formed this team called Storageen. This is the team that I lead inside Amazon stores, and we were tasked with pioneering AI native development inside Amazon stores. You can think of Storagegen a little bit like an internal AI startup. Uh, we support thousands of developers spanning folks that, that build the website, uh, and the mobile application, folks that support Amazon sellers, and those that help fulfill Amazon orders. So we're supporting all developers that, uh, that work on Amazon stores. Before I get into it, let me talk a little bit about Amazon's development culture because this played a major role in the solutions that we ended up building. First, uh, maybe if you heard some of the talks earlier today, Amazon has a two pizza team environment. These are teams of 8 to 10. There are thousands of them that help make the store happen. Developers in these teams innovate on behalf of customers. They're thinking long term. They're thinking big. They're thinking about bold solutions, solving complex problems, and handling millions of customers. They're also continuously learning. They're exploring new technologies. They're staying at the forefront of development technology. And last, they have full end to end ownership over the software that they build. They're responsible for the quality, the stability, the maintainability, and the operational support for what they, uh, what they build. That meant that any AI solutions that we built with Storen needed to take all this into account. One of the biggest problems that we face is constraint on time and resources. We want to develop as much as we can for customers as fast as we can, but on one hand, that constraint will sometimes drive innovation. On the other hand, sometimes it can constrain that innovation. We see increased friction from keeping the systems running through operational support, increasing technical debt, and then of course the system complexity grows over time as well. We also see issues trying to keep the store consistent. This is increasingly difficult with thousands of teams, and it forces us at times to run campaigns across thousands of 2 pizza teams to try to achieve that consistency at a point in time. But even with that, we see inconsistencies. Alex mentioned earlier some of the challenges that developers face. We see the same thing in Amazon and in the store's organization. We want to get developers more time to write code. We want to get them out of the business of doing non-development activities like gathering context, reading documentation, searching for information. Teams spend substantial effort on coordination activities, things like status updates, stakeholder management, uh, and cross-team alignment. Of course, the more teams there are, the more this becomes a problem. Finally, engineers spend double digit percentages, uh, keeping their system running, triaging issues, and managing tickets. Let me talk about how we set out to solve some of these problems and how we, how we set out to build the solutions that we did in Storageen. We knew we had to change the way we work, not just build some AI solutions to have folks use. So we borrowed some practices from startups. We said, hey, we're gonna experiment quickly, we'll file fast, and we'll, we'll find the solutions that really, really resonate with developers. So in our experiments phase, we started with just a small number of teams, 1 or 2, to test our AI solutions. If something failed, we stopped, we stopped here. For things that succeeded, we moved on to the scale phase. We gathered feedback from customers. This is maybe on the order of 10s of teams validating that the solutions were applicable and helpful for what there's, uh, what they face. Solutions that moved past that moved into this mature stage, and this is where we started to scale these out for broader usage. This is sort of the phase that we're in right now. Some of the solutions that I'm gonna talk about here in a few minutes, they're in this mature stage, and we're gathering feedback on how they work at scale. And finally, as we move into 2026, we're moving into the impact stage where we drive adoption of the tools that we've built and we uh we demonstrate the scalable acceleration. As I mentioned, we started in March. We set an aggressive timeline which doesn't really look that way on this chart because it sort of says Q1 to Q4, but the reality was we said from March to November, we had, so roughly 6 to 7 months to develop a set of tools that could help developers move 3 times as fast. We picked a diverse set of teams to work with so that we could make sure that what we were building did not only work for a small set. We set this timeline up, we got started and in Q1, so March basically, we started with some of our initial experiments. Some of them were successful and moved on, and in Q2 and 3 we went into that scaling phase and as I mentioned, we're sort of in Q4 right now in that maturity phase, uh, looking to scale that impact in 2026. Like any good plan though, it didn't really survive contact with reality. Uh, it got way messier than it looks like on this graph. There was a ton of demand for the products that the team was building. One of them this morning, Dave Treadwell talked about called Spec Studio. I'll talk a little bit more about that here in a few minutes. Uh, and the products, some of the products we were building were quite successful, and so there was, uh, again the significant demand which forced us to accelerate our timeline. So probably throughout the day, you've heard people talk about AI native, AI native solutions, AI native development, AI native this, uh, and we, we got uh questions very early on. What does it mean to be AII to be AI native? And so we did, we created a definition for ourselves so that we could, we could say this is what an AI native solution looks like. Uh, this is continuously evolving, but this is sort of where we're at right now. So AI native solutions showcase three particular attributes. First, they're proactive, so they can independently drive activity. This is things that don't require human prompting. They can just take action on their own. Uh, in this case, we reserve human intervention for places where there's critical judgment and nuanced decisions needed. The second aspect is the idea that they're intent driven. You can define a goal or an outcome that you'd like the AI to achieve, and it can then go and translate that into coordinated action across tools, agents, and services to accomplish that outcome. Finally, we think AI native solutions are deeply contextual. They can reason over persistent organizational knowledge. They understand what a team does. They understand the historical decisions of the team, and they understand the business objectives of the teams. Unfortunately I have not seen a solution yet that is fully AI native. Uh, I'll talk about one today that I think it's pretty close. It's called AI Teammate, and I'll talk about that in a second, but the reality is AI solutions exist on the spectrum from AI enhanced to AI native, and this, this calls back to something Alex was mentioning about AI assisted and AI augmented, uh, improvements. Uh, AI enhances when folks use AI to accomplish tasks that you're already doing, uh, but just faster. In AI Native, we seek to do things that we couldn't do previously with AI. Remove steps from the SDLC and potentially scale processes that were previously out of reach due to cost, time, or effort constraints. For many AI solutions, the focus is on the development bubble. Alex mentioned this earlier. This is not where engineers spend most of their time. In fact, uh, the, the studies that we did show an even smaller percentage than Alex quoted, uh, and so we as our storage end team focused on activities outside of that bubble. We focused on the things that folks are spending their time on that stop them from being able to code. In addition, we said, hey, there are all these bubbles here. There may be AI solutions for some of them, but it's also really tough to connect them together. And so how can we actually make it so developers don't have to spend time moving things from step 1 to step 2 to 3 and on. We knew we had to move quickly. Uh, Agent Core didn't exist when we started. Uh, I wish it had. Uh, it would have been great. Uh, so we had to use things like lambda, Bedrock, and Dynamo DB to build some of the solutions that you're going to see today. Uh, we've been able to provide feedback to the Agent Core team, and now we're shifting many of the solutions we have to that solution, uh, uh, to help sort of streamline our approach. We also recognize very quickly that the fact that Amazon was built on AWS gave us, gave us an accelerant. Uh, the LLMs that we use, uh, typically Cloud Sonnet, uh, understand AWS. That means that we don't have to teach it and it lets us move faster with many of our solutions. So the first of the two solutions I'll talk about today is spec-driven development. Alex mentioned a little bit about this, uh, earlier today, Dave Treadwell mentioned Spec Studio in one of the innovation talks. I'll dive a little deeper into it to you, uh, today. So the idea with spec driven development is that we shift from writing code, maybe abstract one layer further, and we start writing specifications. Uh, AI can then automatically generate portions of the code from that specification, and I like to think about this as the 80% 1st draft version. It's not something that I've launched a production, but it's something that gets me going faster and then I can use it to, uh, to finish the piece of code that I need. Specifications are also interesting because they provide context for AI systems as well. So not just for coding, but for other tasks, document writing or, uh, status updates. Specifications actually provide a very interesting, uh, piece for that, and it does it without disrupting developers because specifications exist in the abstract. You don't need a developer to go look at the code to tell you how it works. One thing that specs do as well is give a unified source of truth, uh, stakeholders, be it the team itself, other teams, other stakeholders like legal or accessibility, for example, in the store's case, can provide the requirements once. And then they're incorporated into the specifications. Now that they're in the specifications, they can folks that have contributed them have confidence that they can be enforced using AI in the development process. They can also be validated by AI to make sure that they were, uh, that they were consistent. There are 2 additional things that we think are very exciting about spec-driven development. Matt talked a little bit about some of this in his keynote, but the ability to eliminate tech data through specifications. Because if you abstract all of the sort of core pieces of the software into a specification, you can then re-implement it in another language, in another, uh, without unused code, uh, with new programming patterns. We can also enable consistent cross-platform implementations. So if you have a specification for how a piece of software should work in the store's case, we could implement a web and mobile solution without having to do the work twice. So how does spec driven development work? What does it look like? And how do we handle the fact that we have tens of thousands, if not hundreds of thousands of packages of existing code at Amazon already? We knew we couldn't ask folks to go write specifications, that was never going to work. So we developed what we call code tope, where we actually take a piece of code and generate a specification from that code, and then humans can modify that specification, can augment it, add things to it, and then that code can be translated, or sorry, that specification can be translated into code using your favorite AI tool, Quiro, for example. The thing we also recognized at this point was it's not just code that you can generate with specifications. You can generate the documentation, you can generate the tests, you can generate validations. So you can kind of imagine a case where you generate a piece of code using the specification, but then you use the specification to validate that the code meets the specification. And so you have this nice closed loops cycle for spec driven development. This is a view of Spec Studio as Alex mentioned, this is something we use internally and so I'm showing here uh to sort of walk through the solution, but uh this is the product we use to generate specifications. Uh, when we put it out, as I mentioned a little bit, we recognize that there are lots of artifacts that could be generated from a specification things like system overviews, developer documentation, usage guides, diagrams, yeah, the spec too, but, uh, the other things turned out to be super valuable. Throughout the rest of my talk today, I'm gonna talk about a package called storage and gent timers. This is a really simple package for everybody listening. This is basically, uh, effectively like Cron, but as an agent, uh, and it allows someone to basically say, hey, I'd like this activity to happen every 30 minutes or or something of the sort. Uh, so in this case, this is a specification that was created of the agent timer package. You'll notice here with no human intervention, Spec Studio generated the system boundaries, what the software does, what it does not do, and sort of hidden a little bit on the right hand side there, it also calls out the AWS service dependencies. Scrolling a little bit further down in the system overview, Spec Studio pulled out three capabilities, uh, from the code the natural language processing, time or state management, and notification delivery. There's quite a bit of detail if you, if you look at these closely and how these are handled, and this was all pulled out of code again, no human intervention needed. One of the things that's great is for developers that are onboarding to a team. They don't have to read every line of code to understand what the software does. They can read something like a system overview to get a quick understanding, and it's a lot easier to parse through. The last thing I'll point out on this slide is the little blue links. Those are citations. Uh, and I'll dig deeper into what a what a citation is from the specification now. So here's one around click what we call clarifications. This is when someone requests a timer using natural language, but they didn't fully specify it. So this is what an actual specification looks like. It's structured, it has a description, it has constraints, acceptance criteria, business rules, dependencies, the things that you would expect to see in a specification written by a human, and something that then you can test against. Have these things been met. But maybe one of the coolest parts is that it's linked back to the code. So here you'll see the little section that says related code. If I click on that, it actually takes me to the lines of code that were used to generate this part of the specification. This is sort of lines 6 through 46 if you can see them, uh, and what you'll notice is there's a lot of detail that the spec was able to pull out of the code. On the previous slide, one of the pieces that was pulled out was a truncation length. If you look at line 26 here, it's not in the, it's not in the function description. It's not detailed in any comments. It's just in the code, and Spec Studio was able to pull that out and recognize that's a requirement for this particular piece of code. Finally, uh, for Spec Studio, uh, detailed diagrams can be generated. Uh, you'll notice here again callouts for services. This helps with things like security reviews. We can quickly get, uh, an understanding of how a particular piece of code works. Uh, one thing that does happen on occasion though is these auto generated specifications can be wrong. Uh, the AI can just kind of get it wrong sometimes. Uh, and so you can within the Spec Studio tool, you can actually add feedback and say that's actually not right. It's not just plain English, it's any language an LLM will understand. What's nice about this is this can then be incorporated into future spec generations or code generations. So you can dynamically uh modify and adjust these specifications uh in life. The last really cool thing, or I guess the last two really cool things, is if you have specifications for all of your packages, which we're headed toward, uh, we can do things like semantically search our specifications. So if I wanted to say find all of the code for Amazon that shows Amazon search results, I can just say show me all of that code, and it'll return all the packages that have to do with showing Amazon search results. It's really powerful in the environment that we work in with thousands of teams. The last thing shown here on the screen is deep Q&A on a particular code package. You can dive deep into particular aspects of capabilities and have this sort of Q&A with uh with the AI about the particular package. So we talked about specifications. I'm now gonna talk about our second product. We we hypothesized that uh folks typically use AI in sort of this one on one scenario. You, you chat with a, you chat with a chat agent, you invoke an agent, you work in your IDE. We hypothesize that it may be something interesting and unique if we brought AI into a team. And so AI Teammate is different in that it doesn't work on any one person's behalf, but it instead works as part of a team. So what does AIT mean? AI Team A is a proactive team member. It joins your team and uh gets connections to all of the systems that a normal person on your team would. So things like Slack or Quip or ticketing systems that you may have. It continuously learns from those systems and it starts to suggest and take action based on what it's seeing happen within a team. It does this throughout the development life cycle just like a developer would, and in the systems that developers are already using. AI Teammate can handle routine tasks automatically. Things like answering questions for other teams, drafting documents, executing tool-backed actions. These things can all be done automatically. AI Teammate, and this is one of maybe the coolest parts of it, creates this persistent team memory. It maintains context across systems and communications, which allows us to accelerate future work. It brings that context and memory to everything it does. So when it when it invokes another AI system, it has all of the history of the team. When it invokes another agent or responds to Q&A or does a task, it understands the context in which it's doing back to the deeply contextual part that I mentioned earlier. And finally, AI Team Ma allows developers to focus on high judgment activities. Things like architecture, design decisions, strategy, and problem solving. I'm gonna walk through some examples of AIT made in action. Uh, this is showing in Slack because it's sort of the easiest thing to show, uh, and you'll notice some names are blanked out. You can kind of ignore that. But here's an example where a team member asked, how does the storage and agent timers package work? within just a few seconds, AI teammate could go and query Spec studio, get the specification, understand it, and give an answer to the developer, uh, something that may have taken a developer. A couple of minutes to do on their own, but maybe worse, interrupted another developer to go get the answer for them. That's cool, but you know, maybe the, maybe the specification doesn't have all of the context and so in the same sort of sort of Slack thread you can dig deeper and say, well, what has the team talked about with respect to the agent timers and you can. Understand the the sort of cross between the specification and the team's dialogue so you get the combination of the knowledge and the specification and so here's an example and it cites back to the specific Slack message that it's using to reference in this example. So, OK, great, it can answer questions. It's a chatbot, right? No, it, I don't think so. Uh, now I wanna add a feature to the smart, uh, to the notifications. I wanna add priority levels to it. AITA can help here too. Uh, we can give it a document, that's what you see happening on the right-hand side here. It can read the document and then generate tasks based on that feature request. In this case, it says, hey, there are 5 tasks that I'd like to generate, uh, in order to get these priority levels in place. At this point, developers can now discuss and adjust whatever AI Teammate came up with. Once they're satisfied with it, they can ask AI Teammate to actually create the tasks in their project management system. Sometimes, and this is really cool when you get to see it work, AI Teammate will recognize that the conversation sort of reached this natural conclusion and will proactively ask, Hey, should I go create these tasks for you at this point? In this case, we asked it to create them. It created them, and the tasks have now been created in our in our task system. This would have been time spent the time spent for developers or managers to create these tasks, but maybe more importantly, the quality of the tasks is much higher. It's very detailed. It has the context of the team's conversations, the context of the rest of the software that they own and operate. OK, so we've got the tasks. Now, now what? Well, AI teammate can also help us with implementation. So we can point AI teammate at a particular task and say, hey, can you make a first draft implementation of this? And uh this is one of the cool things where AI teammate, because of the proactive nature of it, can also do this while the team is asleep. It doesn't actually have to be invoked. And so you can, again, you can think of these as first draft implementations and as models improve, we'll be able to do more complex code generations and we'll also be able to improve the quality of it. Now AI Teammate has completed the code review. It publishes it back to the Slack channel, uh, and also just automatically gets an email to the team saying, hey, I've completed this code review. Uh, can you go ahead and take a look at it? In this case, it's a very simple example, but you can see here's, it created the definition of priority in the code. And then it created 4 priority levels. When the developer went to code review this, they said, hey, actually I'd like to have a critical priority level as well. And so they provide the CR comments just like they normally would to any other developer in their team. A teammate sees this and says oh I should update my code and so it updates the code based on the comment and does a revision and then shares back the revision with the team all with no people needing to do anything in in the process other than the code review. Here's a copy of the change. You can see I added the critical aspect of the, uh, the critical priority level, and the code change is ready to move forward now. As I mentioned, I chose a simple example for today, just to sort of uh make it easy to understand, but you can imagine more complex examples. Uh, AATmate has shipped hundreds of code revisions to date. Great, Steve. OK. We've heard all about AI Teammates. It sounds really great, but how does it work? Like what's actually happening there? Uh, at the core of it, it's really pretty simple. Uh, it's a lot of traditional scaffolding and engineering happening in the background. Uh, we hear about AI and it sounds magical, but the reality is it's magical because of what we build around it, in my view. Uh, so in this case, you'll see on the left-hand side event sources. These event sources are any system that a developer may work in and can generate information flowing into the team. Uh, we collect those up in our lambda connectors, push those into a stream. And then AI teammate batches these up, and you may say, well, why are we batching them? The batching allows us to process lots and lots of information coming in all at once, in sort of a, a rational way. So you can imagine someone saying, hey, I'm going to be out of the office tomorrow, and then one second later saying, oh, I meant today. Uh, it allows AI teammate to sort of compress those and not have to process them quite as often. Uh, anyway, they get into the queue here and the first thing we do is say, let's capture some memories, and this is one of the really cool parts of uh of the way this works is it captures these running memories related to what the team is doing. This allows us to compress very disparate events into one context window so when we're processing an event in the future, we can go and pull these together in one place. Uh, we then go to our next step, which is effectively another lambda, uh, sort of underneath the, the bedrock symbol there. Uh, at this point, AI Teammate does sort of its first pass and says, OK, this is the context of the team. This is what's just happened. Do I think I can add any value here? This is a proactive part. It decides if it thinks it should do something, and in many cases it says no, actually I don't have anything interesting to do here, and it'll pass and not do anything. It just records the memories and moves on. But it can decide to invoke a tool, and the tool can be anything your imagination can can sort of come up with. We even model things like Slack messaging as a tool, so it's the AI deciding it should send a Slack message or deciding it should respond in a ticket, not us actually hard coding that. On the very far right hand side of this diagram, you'll see MCP tools and agents. This is one of the ways we can scale things like AI Teammate, which is this orchestrator to handle thousands of teams at the company that are specialized in different domains. You'll notice in here too, just I just want to reiterate, the LLM is only here a couple of times. There's a lot of just traditional software engineering going on to make this happen. Uh, we recognized with AI Teammate we couldn't, my team couldn't build everything and so we had to build something that would allow us to scale. This is the agent to agent connection that I mentioned earlier. It allows us to scale the expertise by embedding organizational knowledge into reusable agent capabilities that can then be invoked by AI teammate. It allows AI Teammate to coordinate complex task tasks autonomously, breaking down work and then orchestrating across specialized tools. This reduces human handoffs and interruptions. Uh, agents communicate directly with one another to complete multi-step processes. I alluded to this earlier when I said uh no longer do developers have to carry around context from tool to tool. AI Teammate will do that for them. Last, it maintains a consistent context across all activities from planning to execution to documentation. It understands how, how we got there. It uses the team memory and historical decisions to do that. OK, so I set out what we were, uh, what we were sort of tasked with in March. We talked about a couple of the solutions. Where did we end up? Uh, well, we ended up seeing 4 times the number of features shipped to customers for a subset of our teams, uh, by, I think it was November. Uh, we saw a reduction in operations and routine tasks. We saw an improvement in quality, and we saw implementations compressed from weeks to hours in many cases. We did all of that, but we learned along the way, that's for sure. Uh, at the beginning, we sort of focused on efficiency. We said, how can we make developers more efficient? And when, uh, sort of as time went on, we said, wait a minute, why are we focusing on efficiency? Let's focus on reimagining how we do the process all together. Let's figure out how we can do things we never could do before and so it allows us to tackle previously impossible challenges. We started with what we thought were clear measurable experiments, uh, but we recognized we had to stay flexible in approach. It turns out, as I mentioned, our plan didn't go as planned. We had to stay flexible and progress was more continuous than we had initially anticipated. The last thing I'll say uh is that AI AI native teams are a collaboration between AI and humans. They work together, they work in parallel, they unblock one another, and they help reduce operational overhead as a unit. Where are we going from here? What's next? Well, we've already started work to scale our AI native solutions to thousands of teams inside Amazon. Uh, Dave mentioned this morning, our plan for 2026 is to have 75% of the teams in stores using these solutions and reaching those productivity gains that we talked about. We've also started to utilize Quiro for accelerating code development. With that, I'm gonna hand it back to Alex to talk about some of our the key takeaways. Thank you. Uh, so, Steve shared a lot of good insights. Uh, so I just wanted to like take a second to just do a recap, right? Like software development is, we all know it is evolving and it's changing fast. So it's important to understand how the technology can help you, right? We're going from what we were mentioning, right? AI enhancing tools to building, maybe not writing a, write a, a line of code in, in a, in a feature or a package that you're building, right? And how adopting AI development techniques like native development techniques can help you build more towards that, but Steve was saying that 0 to 100 towards that native development applications and things like that. So, Evaluating the new capabilities as new things come up will push the. Frontier forward, right? But the most important across these two things is that if the results are not consistent, right? If you don't have a standardization across teams building that memory that persists, developers come and go, the package stays, right? So, how will you make sure that those things stay and you're able to deliver faster or dream bigger, right? Those are the three things that I just wanted to leave you with, um. Thank you so much for spending the afternoon with us. I know it's happy hour time. So, uh, we'll be happy to let you go. Well, we'll stay here for a minute if you have any questions, if you'd like to talk about any of these things, we're happy to answer, uh, questions. So, thank you so much.