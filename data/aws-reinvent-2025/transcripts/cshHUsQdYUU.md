---
video_id: cshHUsQdYUU
video_url: https://www.youtube.com/watch?v=cshHUsQdYUU
summary: "In this strategy-focused session, \"Next-Generation Data Management — Insights at Scale with Agentic AI in Pharma,\" a senior consultant from ZS addresses the critical inflection point facing life sciences organizations today: the urgent need to transition from isolated AI pilots to scalable, transformative \"Agentic AI\" implementations. Opening with insights from a survey of pharmaceutical CIOs, the speaker reveals a decisive shift in leadership mindset—9 out of 10 CIOs are now prioritizing deep operational transformation over mere technical experimentation. They recognize that to unlock the true potential of generative AI, organizations must fundamentally reimagine their business processes rather than simply automating existing, often inefficient, workflows. The presentation frames this transformation through two essential paradigms: \"Data for AI\" and \"AI for Data.\" For \"Data for AI,\" the speaker argues that traditional data lakes are insufficient for powering high-accuracy agents. Instead, organizations must build a \"metadata lake\" that creates a rich semantic layer around raw data. By meticulously codifying business rules, data lineage, and subtle domain-specific nuances—such as the differing definitions of \"sales\" across marketing and finance—companies can boost the accuracy of AI-generated insights from a baseline of ~70% to near 98%. This high-fidelity context is the prerequisite for widespread adoption. Conversely, \"AI for Data\" involves deploying agents to autonomous manage the data lifecycle itself—from \"self-healing\" data quality checks to automated governance and metadata curation—liberating human engineers from repetitive maintenance tasks. The session delves into three high-impact use cases driving this shift. In the realm of Analytics, ZS demonstrates how codifying the \"human workflow\" of analysis enables agents to execute complex, multi-step tasks like commercial forecasting with minimal human intervention, reducing time-to-insight and delivering 40% efficiency gains. In Clinical Operations, the focus is on massive-scale Document Generation; generative AI is utilized to draft thousands of regulatory documents, protocols, and informed consent forms, ensuring strict compliance while slashing drafting time. Finally, within the Software Engineering Lifecycle (SDLC), agents are revolutionizing legacy system modernization and testing, with the speaker citing up to 75% efficiency improvements in code validation and error detection. Concluding with a strategic roadmap, the speaker emphasizes that the \"context is king\" in the agentic era. Success depends less on the technology itself and more on a \"cross-functional\" operating model where IT and business domain experts collaborate intensely to feed agents the deep operational context they require. Leaders are urged to rigorously define their value metrics upfront—balancing efficiency gains with new value creation—and to ensure their infrastructure is robust enough to handle the computational intensity of a fully agentic workforce. The key takeaway is clear: do not just pave the cow paths. Rethink the process for a hybrid human-machine future before automating it."
keywords: Agentic AI, Life Sciences, Data Management, Metadata Lake, Data Governance, Clinical Document Generation, SDLC Automation, Pharmaceutical Analytics
is_generated: False
is_translatable: True
---

Hey everyone, nice to meet you all. All right, as we discussed, what we're gonna be talking about today is data management and use cases for agentic in data management as well as in life sciences. So before we begin and dive into the data management specifics, I wanna talk about what are our CIOs saying in life sciences. What do they want to know about and what are their priorities when it comes to gentic? We'll be probably not surprised at all. to hear their number one thing that they want to do is see value from Eugenic. Historically last year, last 2 years we've been doing a lot of pilots, a lot of POCs, but now how do we do and enable transformation? How do we use these new capabilities that we've discovered to get value and insight at the end of the agency transformation? So you'll see like two paths. I can have a lot of quick wins or I can really. Rethink my entire transformation across all of our CIOs that we've asked, 9 out of 10 are now shifting to the mindset of it is time for change. How do I re-envision and reimagine my processes in this dawn of agentic? I can take what I have now and just automate it as it exists, or I can start to rethink my processes so that they can really be optimized by agents. And AI is demanding this change so these are the themes that our CIOs across Pharma are starting to ask now how do I start bridging the gap between experimentation and actual execution and implementation? That's where the crux of what life sciences CIOs want to see. So how do we do that? How do we start thinking about that change before I go there just again. This is a survey from some research that ZS had done with life science CIOs. We have a council where we get a lot of insight of where do you wanna go, where do you want to see? As you can see, 9 out of 10, they want the pace of digital AI tech innovation to grow and scale. What does that mean to us as technologists, as people focusing on insights in gente? That means we cannot continue to think about technology transformations or AI transformations the way that we have beforehand. We have to think about them. As what is the end value that I want? What are the people process changes that I need so that we can get to that true transformation, and you can see 9 out of 10, 9 out of 10 are asking us to shift that mindset. A lot of the times at the senior levels we've seen this mindset shift, but how do we permeate that throughout the entire organization? And what does that mean for us in data management? There are two core paradigms from a data management standpoint that we like to think about. So we at CS define it. There's AI for data and data for AI. What is our goal? What is the goal of data management? The goal of data management is to enable. Accurate high quality insights right at a short level there might be process optimization down the line that you use for your data. There may be, um, you know, other operational capabilities that we need to enable, but at its core, the reason we are doing data management is to enable insights when we talk about data for AI, one thing that we've all probably learned very quickly is that it's easy to get inaccurate results. Using generative AI, I think when we first started doing a lot of this work, the best accuracy we could get by using just an off the shelf LLM for a just conversation was around 70%. Nobody wants to go to their CIO, to their executive leaders with 70% accuracy, so I'll talk a bit about how do we prepare data for AI in order to ensure accurate insights and high quality results. The second paradigm is AI for data and even within that there's two components AI for data engineering. So how do I optimize my software engineering life cycle to be more efficient and effective? We've seen you can get up to 40% efficiency on that. Think about it from requirements all the way through to deployment. How do I maximize my efficiency in data management? The next piece is once I've created that data, once I've created my data products, how do I then operate this? Think about data quality, data governance, access management policies, security, all this data and metadata that I've created, how do I sustain that in an operating model? We've all talked about metadata governance discoverability to date. Why hasn't it worked? Because it takes a lot of effort to curate and create that metadata. So how do we sustain that? We believe we can use AI not only to create the data but also to maintain and sustain the data, and I'll talk through a bit more of these paradigms and how we've seen it permeating across the industry. Alright, I spoke a bit about this, but 3 areas where we can really transform. Number 1, reimagining data consumption. What does that mean? I've now curated and created my data such that it's effective for AI. What Agentic is allowing us to do is rethink that human agent human machine interaction so that we can get and consume insights in a different way. How do I transform my analytic work flows so that I no longer just have to receive an insight? How do I interact with my insights? How do I interact and prioritize those insights to get the most valuable one? So Egenic is really allowing us to completely transform the way we consume insights today and produce insights today. I spoke a bit about redefining data engineering. Why can we do this now? Most people started their agentic transformations in other use cases about a year ago. But for data management and data engineering, it wasn't really working. We were writing SQL. It might not have been the best. We were writing code. We weren't producing the right things and it was because we lacked the right context. Agents AI require high level of context to be accurate, even for code and engineering development, but this is one area that now where we are today and the context we can. Provide we can truly accelerate the data engineering life cycle by using I mean yesterday even um Amazon launched a few of their frontier agents where they were talking about from a DevOps perspective from an ops perspective and development that can be used for redefining this engineering life cycle. So how do we start looking at some of those frontier agents, creating our own frontier agents to maximize the scaling of the engineering life cycle. And then next data governance as I was mentioning, how do we curate metadata that we need to drive this accuracy by leveraging AI? How do we have adaptive governance, self-healing data quality? How do I automate that, uh, you know, retrieval of information, enhance transparency? We've all talked about discoverability for years, verification. But it hasn't been successful. We all are striving to learn this verification, but how do we now use agents to ensure that we can maximize that? We finally have that motivation and incentive to create the metadata we needed for that verification because we can now use it not only for discovery, not only for quality, but for insights. So we've got the incentives lined up and we have the tools and capabilities to do it now. So the time is now for us to really focus on data management innovation. I spoke about the first paradigm, data for AI. What does that mean? We've all probably heard about data products, analytic ready data sets, but to drive accuracy for my insights, I actually need to create a much more broad set of metadata. We've coined this in DS as our metadata link, so we have a data lake, but we need to now create a metadata link. When we think of metadata, most people will look at the top and say table and column level metadata. Perfect. I have all the metadata I need. I can give you a field description in English. I can give you a table level description in English. That is not going to take us from that 70% accuracy to the 100% that we need to really ensure people are using and adopting our insight models. We need to explain lineage and joint conditions. We need to explain business rules and have those codified so that an entire organization can consistently use those business rules. Then you need to go one level deeper. This is at a high level across all different domains. How do I now start codifying different domains differently? There may be a nuance where, you know, and sales means something in market access sales might mean something differently for a field rep. Sales means something completely different for someone in finance. So as I go deeper in my domains, I also need to create. The right metadata so that my AI and different agents know the nomenclature I am using correctly and adequately because even those small nuances are that are different at a domain level will drive the accuracy and change the accuracy that we have. So now not only do I have to maintain a data lake, I now need to create a metadata lake that is comprehensive, accurate at a domain level. The other important paradigm change here is your operating model. We may have worked in these centralized operating models. A lot of firms have started to move to your data products operating model, but it becomes even more relevant here. You now need deep business involvement working with IT to bring this to life. The subdomains that you're seeing here can only the context can only come from having that domain knowledge. Knowledge and that deep experience with your area of business, only a finance person will be able to help us articulate, are we providing the right context for finance? Only someone that works deeply in market access can help us codify that. Can we use agents to create this? Absolutely, but they still need to be fed that context at some level so that we are helping them refine the accuracy of this metadata leak. And by implementing things like this we've truly seen it go from that 70% to about 98%. We're still working towards the 100%. That is our goal. ZS is really focusing next year on getting it to that 100% accuracy across insights, but just by creating something like this, you jump to 98% in your insight accuracy, not to be underestimated. Now let's talk how does this come to life? What are use cases? This slide is overwhelming. Don't read all the details necessarily, but this is a client that we've been working with to bring the entire insights and analytics engine to life. In pharma, as you all may be familiar, we have a lot of times large analytics teams that are executing and turning analysis of all different kinds, and it is an army of humans that is producing these analyses. They may be in forecasting, they may be in performance insights, but we have hundreds and hundreds of people executing analytics. There are different types of analytics. They might be simple descriptions. analytics, you know, how much did my sales drop today? They may be deterministic analytics. They may be, you know, predictive analytics, but all of them have these different human workflows enabled to bring them to life. At the end of it, they also have that last mile insight that needs to be provided. So, how have we been using Agentic to make this real, to make this automated? Number one, what we were discussing earlier, create your analytic ready data sets and create them by those different domains. Then ensure you have the deep context layer and the context layer expands even further than what I was showing before. Now I'm not only creating the context for my data in all of that metadata lake, but I also have to create context of what those human workflows are like. Your knowledge base now grows with. When I am asked to do a driver analysis as a human, the five steps that I traditionally follow could be this, but I also might follow a different step in a different circumstance. All of that knowledge of the human workflow has to be codified, but we can really bring this to life by saying I create an intelligence layer with APIs accessing those traditional classical AI models. I now have agents for these different use cases that then go execute that model following the human workflow that I would have had before and always of course keeping different humans in the loop as you grow in complexity of the type of analytics you'll have more humans in the loop and that knowledge base will continue growing, but at your descriptive analytic levels you probably can remove the human in the loop because it's very simple questions that I might be asking natural language. But this is one of the pretty big transformative use cases we've seen in pharma where we gain 40% efficiency and also the time to insight completely decreases because I've now removed a lot of those manual steps that I had in the process because I've codified not only my algorithms, my models, but I've also codified a lot of the human intelligence that my agents can help execute. A different use case. This is more on your clinical side, and my colleagues will be speaking about this in depth later around 11:30, but one of the other big use cases in pharma is document generation, and there's many use cases for document generation on the commercial side. You'll create content for marketing on the supply chain. There will also be document generation. On the clinical side, there are hundreds and thousands of documents that need to be generated. And traditionally we would have followed a very complex process for generating that we can now use generative AI as well to power a knowledge hub of your protocols, your informed consent, and all of the different steps that need to be followed to create these documents more efficiently, saving tons of money, time, and also helping us ensure that we're following validated procedures to create these documents because compliance can also be embedded across this entire ecosystem as we. Create the files so this is, I won't go too in depth like I said, my colleagues are speaking about this in about 30 minutes so hopefully everyone if you're interested in document generation go there but this is one of the bigger use cases that we've seen take off not just on the clinical side but also on supply chain and commercial. So it's a fantastic use of generative AI because it's really great at, as we all know that great at creating narratives, great at creating content, but how do we constrain it to help us be more effective in these massive areas of document generation. Now my favorite topic where I'm closest to is what I was speaking to earlier, the software engineering life cycle. This is an area where we've started doing a lot of work and starting to drive efficiency. One of the biggest areas of efficiency driver in the software development life cycle, yes, as you can see is that build and test phase. It's very easy to build net new code using generative AI. If you have legacy software though, it, it's a little bit more difficult. I think we found that generative AI is much better at the newer, you know, Python-based languages. If you're doing something pretty old, it might not be as effective, but for testing, we've seen up to 75% efficiency gains for testing. It is amazing at looking at your code, finding errors, really helping us execute that entire test life cycle. But across the entire SDLC, across the entire. Software engineering life cycle, there is a ton of efficiency to be gained, up to 40% in our entire pipeline development just today by implementing agents across the board. We've started doing this, and this is our initial measure of what we've seen from an efficiency perspective. One of the focus areas for ZS next year is really how do we continue to drive this to scale and ensure that entire organizations can do it consistently so that their whole data management ecosystem can leverage and be effective from this perspective. So what do we want you to take away? Number one, I, I spoke about this. It is time for CIOs, for farm organizations, and for all of us to start thinking about how to take your insights to scale. We're not doing this anymore in a POC mode. We really want to transform our organizations, whether it be how our organizations create and manage their data, whether it be how organizations do analytics at scale, whether it be how we create. Documents for a clinical organization, there are so many use cases, but if we look at it with a narrow mind, we are not going to extract the value because our operating model has to change. We are now in this human agent interaction era and we all have to be aligned on the goal at hand, the transformation that we want to achieve, and how do we operate together, um, how do we provide sustained ROI. Before you start this transformation, think about the ROI that you want to yield. Is the ROI purely on efficiency? A lot of the times we, we think that's the only gain we can get from genative AI. We think that's the only gain we can get from agentic. But there's also a lot of value that can be created. But up front, think about that value creation as well as the efficiency gains that you want before taking on the endeavor of the transformation. Uh, the middle, and I, I'm saying this again, but. We should not transform our processes as they are today. We can. We might not gain the entire efficiency that we want. We need to rethink our processes and then automate those. Nobody likes to start with process flow mapping VSM, but that's really where a lot of these transformations need to go first before we start automating the existing, we need to think about what is the future process look like. The paradigms are different now. It is not the same for a bunch of humans to work on a process as humans and agents to work on it. So what does that new paradigm look like and what is that new objective that we are trying to achieve? Infrastructure infrastructure resiliency and the operating model around your infrastructure becomes more important I think in the keynote we were hearing a lot about the changes in infrastructure. How do we manage infrastructure? It is very as we start these agentic transformations, it's very easy for cost to blow up. You can imagine all of these calls, all these GPUs being consumed nonstop. We have to ensure that we're starting with robust infrastructure or. Thinking how we are doing our infrastructure before we start these transformations, especially these large scale ones. Imagine we're producing those gigantic herds of clinical documents I was mentioning before, and we're doing things in the way we used to, not rethinking our infrastructure that could get very costly for everyone. And did we even end up saving anything where we wanted to go at first? We have to ensure robust infrastructure and robust processes thought through up front. Um, and now the most important cross functional thinking, digital and AI teams have to work together. People are going to have to collaborate in a way that they might not have had to before. There is only so much an IT team can do alone in these types of transformations. We are all gonna have to have a new. Digital workforce with agents involved in the mix to make these transformations successful that cross functional thinking we, we've all talked about it before. I mean I've been in pharma now for 13 years prior to that different you know area but in pharma there's been this divide that I've always seen, you know, the business. IT that will not be sustainable in these types of transformations. They are all end to end cross functional transformations that will not come to life without either of those one, to set up this infrastructure I was just talking about it's critical, and two, the other to set the context, the business vision. They have to come together. The differentiator with. Aic is that business context and setting it up effectively to ensure we are successful. So hopefully as you all go back, you take one thing out of this is business process transformation is what we're going for. Context is going to be king in the age of gentic, and we all need to work together to bring that transformation to life. So thank you, and we'd love to see you at booth 1720 if you have any questions.