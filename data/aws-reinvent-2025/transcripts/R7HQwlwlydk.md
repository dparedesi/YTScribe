---
video_id: R7HQwlwlydk
video_url: https://www.youtube.com/watch?v=R7HQwlwlydk
is_generated: False
is_translatable: True
---

Hello everybody. How's everybody doing today? Welcome to Reinvent. Eyes, I'm on fire. Can't you tell? All right. So what are we talking about here? You've got ideas, lots of ideas. Your organization has ideas, big ones, small ones, great ones, not so great ones, but how many of them are just sitting on your backlog because you can't seem to get it approved, to get enough resources, to get enough time, people to test these things out. So how do we get there? What's the vision? How do we build all these things at speed and at scale? And I'm gonna talk to you today about a couple of different processes. First, we're gonna talk about AI DLC, which is using AI as. A partner in your development instead of as a tool. Second, how do you build agents that build agents? So you wanna build agents at scale to talk to your customers, make it faster to build chat, video, whatever you need to do with customers? Great. How do you do that? And how do you make this model your own? So your backlog is waiting. It's big. There's lots of stuff in there. It's great, but how do you get it done quickly? You know, you've got great ideas, but what happens to great ideas when they sit on the shelf for six months? That guy does it or that guy does it. Or somebody else figures out how to do it for you. So what are you gonna do? So first we're gonna start with the process. And the process is AI development life cycle. So take all the best bits of agile. And when you're talking about agile, you talk about all of the pieces that go with it. You've got your user stories, you've got the visioning documents, you've got stories, you've got code, you've got domain models, all of those artifacts that go into this process. And so what's the biggest problem today with vibe coding? It's that your AI doesn't know everything that's going on. So how do you inform the AI of your full context without running out of tokens every two minutes, without having to just circle back through the same problems over and over again as you've all seen with vibe coding. So this process goes through from the beginning and you teach the AI through every step of this process how to understand your problems. What are your goals? What is your understanding, and those days down there, they're not hyperbole, they're real. We do it with customers. 5 days to production ready code. What does that take? During discovery, you have to teach the AI about your environment. What does your environment consist of? CICD tools, your, um, development environment, the libraries you use, your, um, preferred languages, all of those pieces go together, what services are whitelisted, blacklisted, all that stuff. And you start with a vision. So your executive has a vision for a product or a product owner or whatever it is. In this case we had agents building agents to go faster to produce customer facing um agents. I'm gonna say the word agents a lot. I apologize. So. Once you have your environment and everything established, you move into requirements analysis and you take all that vision and you turn it into user stories, but you do user stories with enough technical content, so now you could build a domain. And the domain models are separated, so they're solvable as a coding step, and then you use those domains and those stories to actually build your code and your test harness and your CICD deployments. And then finally you deliver it. And look, it's not without its hiccups, it's AI. We all know they're not perfect. They're trained on humans and they're as imperfect as humans. So you're gonna have to do some work, especially 34, and 5. But on day 5, you should be looking at your backlog and saying what is the next set of features? What is new things that we've discovered during this process? It's heavy duty. It's hard. It takes a lot of thinking and time. It takes a lot of tokens and context, and you know we're not going to get too much deeper into it, but you might say, how does this align to my AI ideals or my. Existing agile software development life cycle, and it's a clear map. And you get outputs into your current tools, so tools like Jira will have all of the stories in it and as you're doing development, those stories are advanced through the process backlog to in progress to testing to validate it to deploy. So as you're working through your life cycle and you define your agile life cycle, you inform the tool on how you like to do it and the tool helps you manage. The whole stack, it's not magic. These are text files that have very rich prompts in them. And so if there's something about the process, and yes, I'll put up a link at the end for the process repo or you can go get it from GitHub, make it your own. They're just text files. You can fix it to make it work in your environment with your restrictions, your regulators, whatever else is necessary, so. What did we, you know, in advertising, ad tech, that's the space we're working in here today, what are some of the key challenges? Brand voice, your advertising has to speak on behalf of a brand. So when we're talking about this, let me give you an example of a tire company. So you're selling tires and you sell tires for minivans. What are you going to emphasize during that process? You're going to say they're safe. They're. Durable. They'll take your family home in comfort and style. You also sell big lug tires for pickup trucks that go in the mud and spray mud everywhere if people are driving through the dirt. Well, then you're going to say these are tough. They've got big lugs. They're American made, and you have a very different voice for each of those types of products. So you need to be able to standardize and simplify the creation of that voice. It has to be safe. If somebody tries to get your AI to give them a recipe to build something nefarious, you don't want the AI to do it, but you also don't want it to indulge in comparisons. So if somebody asks you about another brand of tire, say I don't have information to speak about this other brand. Don't try to just pull. Whatever is trained from knowledge set, so you've got brand safety and agent safety asset management, what are the taglines? What is the copy you would like it to use as a part of it? Are there videos? Are there diagrams? You have all this content that goes in an advertising context. And finally, it's got to be cheap. You can't afford to spend a billion dollars every time somebody comes and talks to your chatbot about your product, so you have to be able to build all of these capabilities at scale when you're doing this in a customer facing context, so. This is what Nativo did. They had something in their backlog for 2 years, and their vision was how to enable their advertisers to quickly create agents that could chat with their customers about the advertisers' products quickly and efficiently and at scale. I don't need to read you the slide. They succeeded. So like I said before, that 5 day process is real. You'll probably have to spend a couple of weeks after that with production polish and release, you know, it's a real software code stack. We're not building kitty software. It's not just some example that you use to show that the stuff works. This is real production grade code, so. We got to build a brand. So we start with an agent that understands the basic things about how to build an agent. We start with brand voice analysis. So you've got all the content from your advertiser, things like, um, we've got their copy, their goals, their statements of intent, all of the other things that go into building an advertising campaign is available. Well, why not use another AI to analyze all of that content and produce the first version of a voice. A voice is just a prompt. It's not magic. It's just text. So then you use it to analyze it, have it produce a voice with keywords and tenor and all of those other aspects you want in your voice, and you do that quickly. You know, from your existing assets and your resources, knowledge-based content, you may be advertising for 150 different advertisers. How do you need to be able to select from your content repository which assets go with which agent. You have to build the guardrails, so there's two levels of guardrails. Bedrock has guardrails built in. Those will provide you those basic things so you don't get unhappy content coming out of your bot and posted all over the internet. You get that level of safety, you know, prompt injection, all of that other kind of good stuff, but you also have to build the guidelines about are you allowed to talk about competitors. If so, what's the comparison chart? What facts are you allowed to do, you know, what data are you pulling from? And finally, available resources. So if you've got relational data stores that have a lot of metadata that you want to make available, make it available. If you have open search databases with semantic search capabilities, what filters do you need to apply to limit it to just this? So you use an agent to build all of this. Well, that's great, but then what do you do now you've got all the makings of an agent, but you don't have an agent. So, you consolidate all of this information into configuration. So rather than trying to code a solution at this point, we need to manage the data for the solution. So what is your voice specification? What are the content guard rails? You can build all of that dynamically and store it in a data store, you know, just a relational database, JSON data stores, document databases. There's any number of solutions to this where you can store all of this information. So that's great. How do I deploy it? There's nothing magic about AI and agents. Now. We would recommend you use Agent Core as a way of managing this layer of complexity. In this case, the customer chose not to. They wanted to absolutely guarantee minimum cost operations, so they use lambdas for everything. You can deploy an agentic framework within a lambda just as easy as you can in a container as anything else, as long as you give it the right tools. So, standard 3 level, API gateway for safety, React web application at your front end. Lamb doesn't provide all the capabilities, so a dedicated brand analysis. Knowledge base, the actual agent itself that you're going to use to show to your customers, safety services, S3 for static resources and natural language to text because that's how agents think. They want to be able to say I want this type of data from the database and be able to supply. Then what do you do with all this stuff? In this case, um, data has gravity. We talk, we talk about that a lot at Amazon. So they already had a bunch of their data in a post-grade data source in RDS. So don't reinvent the wheel. Just make sure that your natural language text to SQL service is read only and has the appropriate safety guidelines coded into it. You can talk to existing data sources. You don't need to split it off and be special about it. We had an S3 bucket that had all their stuff in it. Well, you can build knowledge bases based on S3 content. You can store video. You can store images and diagrams and anything else, even, you know, playback files, whatever you need, you can store in S3. And then you need guard, you know, you need the basic services. What model are you gonna use? Well, Bedrock, knowledge base, um, open search knowledge base, but it's a built-in feature, Bedrock to make it enabled and easier. So open search could encapsulate about 80%, not open search, I'm sorry, Agent core could encapsulate much of this, but not all of it, and that's an accelerator that we would recommend that you use to encapsulate a lot of this stuff to make the heavy lifting even easier. So, let's go down another level. When you're have a customer coming in, what do you need to do at the startup of every time you're gonna have a conversation? Well, Tell it about the prompt. That's all your voice and all the other stuff. Initialize bedrock. Create a set of tools that go out to those lambdas that we already had to create for the other version of the service where we were building these agents. And finally, it's initialized. In this case, they use Landgraph. You can use strands. That's one of our favorites. Um, at the time, strands didn't have TypeScript support, so they used Land graph, which does have TypeScript support. The framework is an automation tool that helps you build out all of this. So when it starts up and right down here, we are using Agent core memory because that was the fastest way to build reliable memory at scale. So what does it mean to actually run it? It's a very simple loop. If any of you have done land graft before, it, this is about as simple as the loops get. We come in, we have the brand ID and the agent config. That's all you needed to initialize everything. So now you know exactly what context you're working in. The message ID gives you enough for the session, so now you can pull back from memory any conversation you've had with the customer already. You get the message, you use the AI for what it's really good at, which is reading all that prompting all that other stuff, and it's going to make a set of decisions. What more information do I need to help answer this question? How do I get to it? And that's where it's going to call out to those tools, those lambdas we initialized in the last step. And finally, it's going to compose a final response. But like we talked about safety. How many times can it loop? That's a darn good question. It all depends on how responsive you need the service to be. You can ameliorate some of that by having live responses. So as it's looping, have it send out, say, look, I'm looking in the database. I'm searching in the knowledge base. Tell the customer what's going on under the covers. You don't have to give them the technical details, but showing somebody something on the screen while it's running is a huge. Benefit to your customers because they know it's still alive. They know it's still ticking, and it prevents woodpeckers, so it's people who just can't help but click that button over and over again until they get something. And finally you get the response and since they wanted to audit all the responses, they created another repository that had all the messages. They didn't need the full context of the thing they needed the full response, so they just logged that in parallel with sending it back out to the client. So in context, this would be used as a chat agent. So you've got, you're familiar with the advertising bar on the right-hand side of a lot of websites. Well, what if it was the website was doing a story about tires, and now you can place a chat on the side from a tire retailer that says, would you like to learn more about Bob's tires? And if they say yes, they can go down a script and say what type of tires are you looking for or what's your vehicle type, what you're in the market for, what's your timeline, all those questions that you want to ask to generate those sales leads, which is the part of the sales process we're really getting to how do we get back to the customer to get them to engage with the advertiser and build it out. But all of this It's simple, straightforward, and it's generic. I can plug any configuration as long as it has those basic values of voice or other things into it that will help you deliver this to your customer. It doesn't have to be bespoke every time. A lot of the time everybody thinks about an agent. It has to be unique. It doesn't. As long as the problem space is solvable, you can do with standard tools and just make it a repeatable function. So why did it work? We started with a clear direction. In 5 days you don't have time to go around and waffle around with. What do, what do we want it to do? We build the domains from the requirements, from the original vision, from the, the discovery that you did in your environment. It's a partnership, so the AI is doing a lot of the junk that you don't want to do. Who wants to sit around and write user stories for hours and hours and hours? What if you're gonna have the AI and all the human has to do is validate the stories? You know, if you want all of these types of aspects, you could do it at speed and let the AI do the part that humans find boring. Who wants to write unit tests? I don't know of a developer who likes to document or write unit tests. Make the AI do it because that's what it's good at. I'm not great at writing documentations. I've been writing code for 45 years. How many documents, how much documentation have I produced? Bare minimum. You know, that's just the truth. Everything is on that minimum lovable product, or as most people know, minimum viable product. And it's production grade because you told it upfront what you needed to be to be production. So, did it work? Yes. Have I done it more than once? Yes, this is customer number 3 running down the pipeline. All of them are headed to production. This one is currently in alpha with their advertisers right now where they're testing it out, making sure it meets all the business needs for the customers who are going to be utilizing this production. We have the guardrails. We've got it, and it's repeatable and scalable. It has all those bowls that you need and the safety guard rails from both cost, time, and energy. You've built into the system and you did it in 5 days with a couple of weeks downstream of what you have to do with anything when you're running it out into production. It has to pass all those same guard rails. There's no magic that's going to solve the production pipe. Finally, what's next? There's the repo. Have fun with it. Go out there and remember it's just text. You can go out there and you can download these things and you can change them. Make them work for your organization. Work at speed, and scale. I would say don't start with a little kitty app that you just want to kick the tires on. Make the investment with the time and teens necessary to really give it a real shot. So, Nativo, they're polishing it for production, making, you know, fixing anything the customer feedback gives them. They're expanding. They're already working on phase two scope. What can you do? Try it. What do you got to lose? It's a little bit of time. It takes a lot of work because the first time through it you're learning the process and you're uh running it. Sorry, I'm gonna cut you off. We're gonna play the music and cues for to get you off stage so we can mic up the next person, but thank you so much for your presentation. That was awesome.