---
video_id: YrdtOwMzJCU
video_url: https://www.youtube.com/watch?v=YrdtOwMzJCU
is_generated: False
is_translatable: True
---

Hello everybody, uh, my name is Syros Santos, and, uh, together with, uh, Josh and Angelo, we'll tell you a bit about, uh, you know, how we build and use an AISRE to help with, uh, to help troubleshoot incidents around production. Um I'm with the founder and CEO of a company called Resolve AI. You have probably seen us on your lanyards this week. We build agents. That can Use and run tools like a software engineer, let's say all production tools from code to observability to infrastructure, with the goal, like I said, of accelerating incident troubleshooting and remediation, but also helping run production more broadly. And today we're gonna share uh information of how we, what are the challenges in building a product like this, and how can it be used in production. Uh, a bit about myself, uh, I worked in observability all of my career. I previously Uh, built two companies, a log analytics product called Loginsight. I dropped out, I dropped out of my PhD to build that in 2008. Uh, we ended up getting acquired by VMware in, uh, 2012. And then, uh, together with Mayan, who is my co-founder at the Resolve as well, we co-created Open telemetry in 2018 and built an observability platform called Omniscient around that. At the time, our thesis was that, uh, to build more powerful observability solutions. We had to essentially move away from proprietary standards and to open telemetry in many ways both provided an open standard, but also an implementation that we could use. Uh, that company got acquired by Slack, and then I had the L. To run a large production system with uh very strict SLAs, and I saw firsthand how hard that is, right? And uh we had times where uh we would pause production pushes for like a month, around this time actually, because many of our customers were e-commerce. And uh also there was a period of time where 90% of our SRE team resigned uh because of burnout, and uh that was partially the motivation for me and Mayan to to to work on resolve, to not create yet another tool that humans can use, but rather create agents that work alongside humans and uh make the life of, let's say SREs and uh software engineers better while also they improve the outcomes. Now, Software engineering, in our view, uh, is like two parts, right? It's the coding and coding and building, and then it's the running of the production. And, uh, when you're writing code or when you're creating software, you have to have a lot of context about production. And vice versa. When you're running production, you need to understand the application logic, the infrastructure, and all of that. And what we have seen so far is that, uh, you know, uh, AI AI has had already a lot of impact in the in the coding part, right, in the building of software. I'm pretty sure many of you actually, how many of you are using, let's say, an agent to to write code in production? You know, uh, we started obviously like probably 2 or 3 years ago after GPT 3.5 with uh GitHub co-pilot, and that was more like an assistant to a human, more of like auto-completion. Humans were still in the driver's seat completely. But, and of course it was like the first wave in some sense, and we got some acceleration, but still, mostly we're moving at the speed of, let's say, human execution. Uh, the second wave, which I would say probably landed about a year ago, is when we started having agents that, uh, you know, did more autonomous work in creating code, right? Essentially, we moved from humans being, let's say, the operator of an ID almost, now to an agent being the operator, and the human now managing, let's say, the agent. But still, you know, this is For only a small part of what the work of a software engineer is in many ways, right? Uh, we have some data statistics, and I'm curious for those of you who work in bigger companies, what is your take on this. But really, the, the, the time software engineers spend in writing code is usually like 10 to 20 to 30%, especially for brownfield applications. Of course, if you're starting new and building a new application, maybe you spend 80% of your time. Uh, writing code, but that's usually not the case when you're dealing with existing systems that have been around for a while. Where, where does the rest of the time go? It obviously goes into maintaining, troubleshooting, running, optimizing production. And uh That's a very hard problem. It's a very hard problem for humans and it's a very hard problem for models. Our view in some sense is that models have solved coding to some extent, and it's not uncommon to see that 80% of code might be generated. Uh, by model, but That doesn't mean we're moving, let's say 5 times faster, right, because that's not enough to accelerate the whole life cycle of software development and production. Now why is this hard? This is hard because If you are dealing with the entire software stack, the whole production system, you are dealing not just with code, which is obviously an important part of it, but now you are dealing with a lot of tools. Uh, think of like observability tools, CICD pipelines. Many times we have multiple of these tools, and they were not designed to work together. Um, the data is siloed. Oftentimes we even have like two tools for the same type of data, right? It's not uncommon that your infrastructure metrics might be living in one tool and then your application metrics in another tool. And of course, then you have also infrastructure that by itself is very dynamic and complex. And again, it's not uncommon that somebody might be running across multiple clouds. Now, on top of all this complexity, let's say, of tooling. It takes multiple, let's say different types of expertise to run a production system. It takes, you know, there is, and, you know, oftentimes, let's say this expertise might be combined in a single human, right? We have oftentimes application developers who are on call and manage infrastructure as well, right? But in reality, it's kind of a different skill to write, let's say an application versus to manage a platform, or to even like debug an infrastructure problem. And then on top of that, Unlike, let's say code that is self-documenting for the most part, production is never fully documented. Usually, the knowledge of how to run production lives across tools, let's say your dashboards, your alerts, across documents, run books, SOPs, prior incidents, but oftentimes just in human minds, and that's what makes it very, very difficult for any human as well for any human in reality, but also for for AI as well. And what is the effect of all that? Uh, you know, reliability obviously is a priority for anyone who delivers, uh, you know, the business through software, and, but it usually comes at a very high cost of humans, tools, and, uh, oftentimes it's not even achieved despite this very high cost. It's not going to be uncommon if you ask a CTO what to rank their expenses. They will tell you, of course, engineers is the top expense, you know, maybe the cloud is the 2nd, observability is my 3rd. But despite all that, you know, war rooms and, you know, escalations are a very, very common occurrence and um oftentimes involve tens of people. And even in scenarios where, let's say you have maybe a network operations center, right? Or like, L1 SRE versus, you know, software engineers who are on call, typically escalations happen all the time. The other thing we see often, and I have some very interesting stories to share, uh, maybe afterwards if you have time, is that infrastructure is extremely infrastructural observability comes at a huge cost because they are very overprovisioned. We are actually in our effort to ensure when something goes wrong, we have the data to troubleshoot the problem. Obviously, we collect as much data as possible, and then also an infrastructure itself is overprovision to ensure like if something goes wrong, or, you know, if you have a spike, you can handle it. And, uh, you know, brownfield development in particular is a very, very huge challenge because of the complexity and, you know, the the the large system and the lack of, let's say, it's difficult for any human to know the entire system. So We see very commonly, you know, when we talk to our customers, or even from our own experience, that there's usually a small set of humans that are experts, and, you know, instead of maybe building or architecting, they end up firefighting all the time, because obviously when something goes wrong, it takes precedence over everything else. And, you know, usually when a new engineer or somebody joins an organization, it takes them probably 1 week to submit the first PR, but it takes them up to 6 months or longer to be primary on-call. And why is that? Because there is so much context that you need to absorb. You might have all the expertise as a human, right? You you are probably an expert SRE, but then you join a new organization and, you know, you have to learn all the things about that that production system that is quite unique compared to what you've seen before. Um, and of course when an expert resigns, you don't just lose their skills, you lose all this accumulated experience about the system. And of course, AI generated code just makes this all worse. Just focusing on maybe the most painful part of running a production system, which is being on call and dealing with incidents, it is quite common to see situations where, first of all, maybe you're if you're the person that gets called to troubleshoot the problem, usually you have a cold start, right? It's a very stressful situation to be in as an individual. It's probably one of the hardest things when at 30 in the morning, let's say you get paged and you have to go figure out what's going on with all the pressure of what if I don't do right, all the downside effects of that, right? We also see quite common to have like huge uh internet tunnels with like sorry. With 20 plus people, or it's not even uncommon to have, like, especially in larger enterprises, situations where maybe one team gets paid, they don't solve the problem. Another team gets paid, they don't solve the problem, a third team gets paid, they don't solve the problem until you end up like in the networking team because it's the DNS I suppose. And you have all these people now in a slack tunnel, maybe 100 of them, trying to figure out what's going on, right? And you often even see that maybe people remediate the problem, but then the actual root cause is not found for another few days. And I think, especially now that we started generating a lot of code, but not only because of that, right? Like, we're creating a lot more software, a lot more technology, and that's very desirable in my view, right? Like, that's good for the world. But, you know, managing all that software becomes a lot harder. And, uh, you know, we have, like, fragmented tools. We have, uh, you know, very large systems that it's impossible for any human to know by themselves, and to some degree, maybe like the silo kind of effect we have is desirable, right? Uh, for anyone who tried to go through a transformation and maybe, you know, implement observability, you know how hard it is to move from one system to another, right? And oftentimes, maybe one tool is very good for infrastructure, another tool is very good for applications, and maybe it's desirable to have these, you know, data separated and used by different tools. But, OK, what is, I guess, the solution to all of these problems? I I I, I talked about So, we do believe that We are about to move, or we're in the process of moving to this 3rd wave of of AI, let's say in software engineering more broadly, that applies to software engineering, which is Let's say humans, up until, let's say, maybe 2 years ago, were both the operator of all the tools they had, you know, in in a in a in a development environment. And also the glue across them, right? For most tasks you probably have to look into code, you have to look into logs, you have to understand your infrastructure and combine the information to get to some task completed. Um, I think now we have moved to maybe oftentimes agents being the operators of these tools individually, and then a human is the glue, right? So you might have an agent write code for you. You might even have an agent that can query a system better than you can do it, but then you still are the person who's a human is stitching together the information. But I think we're about to move to this new world, let's say, where agents are both the operators and the glue, and the humans will be operating at a higher level of abstraction. And we think that's the path forward, but this path forward involves essentially a few things, right, that are very, very hard, but I think we're getting there. And if I have an insight to share from my experience working day and night with AI for the last couple of years, is that I think agents are going to work in production as well as they work for code today. And we're actually almost there, and the impact is going to be even bigger because these are harder and more stressful and more painful problems. But of course, to do that, we need to be able essentially to take the expertise of humans across all these different disciplines and kind of encode it into agents or give the agents the tools. That humans possess We need to understand, make the agents understand and operate all the human tools, because Obviously, all the software environments in the world today were built for humans to operate. So agents have to essentially learn how to operate and understand these environments the way humans would. And maybe the hardest or the most important is that each environment is unique. So we have to be able to build agents that kind of can both discover knowledge the way humans would, but also learn from every interaction, right? So over time they become better and better at working alongside humans in getting results, and it's not a frustrating experience. And of course, as you may all know, models don't have memory. So this is a very, very hard part of building an agentic system because all of that has to happen outside of the weights of the model. And that's kind of the solution we built at Resolve. Basically, we are a multi-agent system, and maybe a simple way to think about it, like your typical coding agent is one agentic loop with probably 10 to 20 tools that the agent can use to produce code, understand and produce code. There's always About 10 of those for code, logs, metrics, infrastructure, knowledge, documents, but they all have to work together in accomplishing tasks. The most common tasks humans use resolve for is to be on call, troubleshoot and immediate alerts and incidents, and typically any alert involves Going, let's say, from code to metrics to logs back and forth a few times, as you probably know, those of you who have done this type of work, until it gets an answer. But also, if you think about it, most tasks humans have to do in production have similar characteristics, and that's why we believe that by building a multi-agenttic system that can operate the tools as a human, that can learn from humans and allows humans to now essentially instruct this system, almost like a junior engineer. It can be very, very effective in accelerating and how we run production systems, but also making our lives easier at an individual level, right? Like, it's not as painful, hopefully, if you get paid at 3 in the morning, or maybe you don't even get paid anymore at 3 in the morning, right? An agent gets paid, and if you can solve not solve the problem, then, you know, you're at the escalation point. And uh one thing that maybe I would say that surprised me in building Resolve is how quickly uh when we started the company, we didn't even know people would trust an AI in production. And now, you know, we are working with some of the largest, you know, technology and finance services companies in the world, and I'm very surprised by how quickly people adopted or the companies adopted this. And, you know, Angelo, who's here from Coinbase, can explain that part in a bit, but with that, I'm going to pass it on to Josh, who's going to give some more details about the technology and give a demo. I get the exciting part, so I get to do the demo, but I'm also standing in the way of Angelo. So I'm, I know you're all here to to hear from Coinbase. So I wanna give you a little glimpse into how the product works, um, how it's used, and, uh, let's jump in. So we're here to talk about AI for incidents, uh, AISRE, that's what's commonly referred to today. However, when you think about an AISRE, there's a lot it can do beyond just resolving incidents. Resolving incidents is kind of starting with the pinnacle, the hardest part of the equation, because of these complex systems. Everything is changing. Knowledge is locked up in each individual's heads as well as unstructured and structured data across systems. And interestingly when Spiros and I were doing a demo, uh, early on in the days, probably about a year and a half ago to the head of technology, uh, I think the world's largest music streaming company, the thing that he saw when he saw us do an incident walkthrough, so an alert fires, we triage it within minutes you have a root cause, he said, hey, this is amazing. He's like, but what I really see here is giving us those incremental incremental gains and keeping my engineers in their flow state, because what an SRE does is not just incidents. They're kind of that catch-all for everything about your operations, whether it's platform engineering, how does CICD work, um, let's optimize our costs. Let's do capacity planning, and so forth. But by starting. With incidents, if you do that really well, it unlocks everything else. Um, there's a new book out, uh, by Nicole Forsren called Frictionless, and it talks a lot about developer experience and friction created by these systems, the adoption of AI in the new world plus the old world, and what she latches on to is the importance of keeping individuals in their flow state, so giving them the autonomy. to actually go and get context when they need it to unblock themselves. She also talks about, um, cognitive load and reducing that cognitive load through abstractions and new modern tools. And so what I wanna show you in the demo is not only the incidents because that's what you're here to see, but I also wanna show you what the possibilities are in using tools like this. So, uh, I've worked with Speros and Monk, uh, many times, uh, previously Omniscient. What we learn in building observability tools or any developer tool is you kind of wanna embed yourself into the workflow already. And so a common way to use resolve is directly through Slack. Many of you, if you've been in incidents before, you're part of these alert channels or when an incident is declared, a channel is, is spun up, and this could be in Team Slack, wherever. Um, and what ends up happening is you're given something like this, an alert you're probably not that familiar with. It doesn't have a lot more than probably a dashboard that you're gonna follow. You're gonna look at a bunch of panels, and you're gonna go and drum up a log query. You're gonna start to page folks. Let's look how Resolved works. So there's kind of two different ways that resolve gets triggered. One is autonomously, so an alert fires, a web hook shoots out, or an API call, and Resolve goes to work. The other is through natural language. I'm gonna take you through that in a little bit. So when this alert fires, it says the front end air logs, uh, it looks like there's really just front end air logs, some labels, um, and so forth. That's the alert rule. Now what's interesting about this alert is if you were to investigate this, the first thing you want to do is see that air spike and you might wanna look at what those logs show you on airs. But this gets a little bit more complex. This is a nested investigation here you see the findings that it actually was a a dependency service that was cascading errors to the front end, OK? We checked the infrastructure and we also see that there was a deployment. What do you know? Uh, change caused this incident. Now the hard part about Slack is there's context in here. We wanna make it actionable so that you can remediate immediately if you want, you can create a PR right from here. However, when you're in an incident, you actually want all that knowledge at your fingertips, all the data, and so let's look at the UI so that you can start to see the work that Resolve is doing. And where I wanna take you first is I wanna expose you to how it works. The reason being is because then you're gonna start to get those wheels turning to see exactly what you can do with resolve and all the entry points for context to help us understand your systems. So here's that same alert that fired. This is just the payload coming directly from Grafana, but you're gonna see, start to see resolve go to work. So it's not just like a zero star RCA. Like give us an error and we're gonna go and troubleshoot it. What we're doing is we're relying on explicit and implicit data, so we call that knowledge. We're, uh, understanding your system. So we're building a topology or an understanding what we call a knowledge graph. I'll show you that in a second, and we're even looking through history. We're accessing tools. And so what the Resolve does is it generates a plan. This plan is multi-step by nature. It's, it's gathering evidence. It knows how to drum up the queries. It understands rate limits. It understands the tools. It knows where the data lives. And as you can see through here, it's going through each individual step we're looking at logs here we generate 8 different log queries. Um, these are very precise log queries understanding exactly what we're looking for. They don't have to be generated by the alert itself we can generate them ourselves. Then we start to look at some of the historicals. So one big part about AI, especially in this use case, is we can have an investigation that looks beautiful that we're like, you know what, we checked all the right things. It checks out from our experience, but it still takes humans, um, to resolving incidents and providing that feedback or to be in a part of those channels where we see the remediation take place for us to understand if it's good. So we look at historical investigations, we create memories from those. Then we start to look at some of your traces again. We want to see where those spans, especially in a complex microsurfaces environment, uh, where those air spans look. We're gonna look at your logs, your infrastructure events again. Change exists everywhere. If you just plug this into your code, you don't actually know when that PR goes into production or not. You may know when it's merged. We look at charts and dashboards, so a lot of what we have to do is we have to understand your observability systems, not just what's within there, but how they're used just like a human would be. So we're understanding what are the dashboards that are commonly used, what do these dashboards mean? How are the panels used, and we allow you to embed information as well within there. And now we start to update the investigation report. We're looking at the code again like we can have some great theories with high confidence, but without checking the code to see the intent behind what happened, we're not gonna get to causality. We're still gonna be at that correlation phase and finally we end up here. So here we've got a working theory. This working theory, it does have high confidence, so this was a cascading event. Now one important part about AI, especially where when you think about the folks that are doing this type of work incidents themselves, they pride themselves on understanding the data, knowing where to look, what it looks like, seeing those anomalies firsthand, and so you have to provide that same type of information directly. Um, so we're providing the logs that we've queried. We're providing charts. These are all made on the fly, so we're not ingesting all of your observability data and then acting on this. We're actually querying it in real time. We're generating these queries on ourselves or using what exists within dashboards now as you go through, this is where it gets really interesting if you're escalated too. Say you were part of this incident, you are gonna see a bunch of different theories. You're gonna see a bunch of people that are posting charts in that Slack channel. But the reality is what you want is you want to understand the timeline of the events. You want to see how big the impact is. And you wanna understand what those ears were telling you because this is where your intuition is gonna kick in, OK? And then finally, any recommendations? And any supporting information and so as you can imagine this seems like a simple incident. The amount of work that takes place in people's heads and the tools that you're using to get here, um, is overloaded. So seeing that that is an incident we do that all day. So an alert fires or an incident channel is spun up resolve goes to work. Now let's see what we can actually do with this information as well. So we've got um two different interfaces. So one, of course, Slack, your collaboration tool. The other one is UI. Resolve can be triggered via natural language because when you think about it, if you have a question you want to understand the health of the system or maybe one of these incidents that actually starts in customer support where someone's complaining about errors on the front end, you might wanna chat directly with Resolve so you can do this directly out of Slack or within Resolve, but you know what, let's take a new use case. So I'm new here. Tell me about our AWS infrastructure. Sorry, it helps me to read out loud while I'm typing. And services Um, Format it in a table because I'm a visual person. So when you think about when you've joined your last engineering role or when you hire somebody, chances are you expect them to ship their first PR in week one, but that's because you've created this surface area of understanding that they need that's this small. However, what, how long does it take for them to go on call? It's what, 3 to 4 months. And the reason being is one, it's context, it's understanding the complexity of the systems. But it's also having the the confidence to know that what to do because you know those run books, those SOPs, they're not kept up to date. And so here just within 30 seconds or so we kind of created a wrapper here. So uh we'll talk a little bit about the integrations, but here what we're doing is we're essentially using the AWSCLI to generate all of our AWS information here. So we've got EC2 instances, we've got RDS, MSK, and so forth. Now if I'm new here and I'm curious, I might wanna say, hey, tell me more about RDS. Um, what services, read and write to it, um, maybe draw a mermaid diagram. Because those are really in OK, and so here what we're gonna do now is when we need to understand RDS and we need to know the dependencies, let's show you how we get there. So I'm gonna flip over to the integrations view. So interestingly enough, um, one of the biggest challenges that we had early on was, hey, this looks like magic, but we all know it's not magic. You still need to connect to all the tools that a human might use to troubleshoot. And that's absolutely true. We had to put guardrails in place just so we wouldn't overstep initially when we knew we didn't have the right data that we needed access to. And so we've got dozens of integrations, everything from your integrations, uh, I'm sorry, your infrastructure yourself, um, observability tools, uh, unstructured data, even where tickets are created. Now, right now you're looking at this in read only mode, but you can imagine if we can get to an RCA, why can't we create a JIR ticket? Why can't we actually daisy chain or create a PR ourselves, OK. So those are the those are the integrations themselves. Now what we do with those integrations is we build what's called a knowledge graph. Now this is my least favorite part of the product because I think it's hideous and no one ever uses these, but everybody wants one. here this is a simple application, 19 services, but it's a microservices application. It's got a lot of dependencies as well. Now what's interesting about this is we maintain this because when you think about understanding anomalies or spikes or anything like that, you actually need a temporal view of your entire system. And so we create this on the fly using these integrations as well as other contexts that we create. And that's what we use for these investigations as well as to know where to go next, OK? If we go back into chat, I'm actually gonna take you into one that's completed just because I know I'm gonna get kicked off stage. Um, I did this just, uh, just right before the session, but this is the same exact chat. So here we see the database architecture, the connected services. There's our beautiful mermaid diagram, um, and fun fact is we never create, uh, any diagrams ourselves by hand anymore because of this. Um, key insights, limited database usage, so only two services are running. I followed up with another question though. I said, you know what? I want to know about the health of RDS because every company I've gone to, they run into issues with Postgress. Tell me if you see any anomalies or spikes. So now let's look at what it did. So it took a look at all these charts and dashboards. You can see the queries that it's running, so it needs to know that, hey, we're going to Performance insights to find this. We need to look at these dashboards. And again, uh, every time I go into an incident and I click into Grafauna, there are about 14 different variables that I have to select before I get to what I need, um, even if that dashboard is attached. This does it all for you and in parallel, and here's the report that was published. So you see key metrics, CPU utilization. There were some spikes detected. There's a pattern. Here are two different spikes that occurred. So in the early morning. We see that this is unusual, so a CPU spiked in the evening, we're seeing a reoccurring daily pattern, the same pattern observed in previous day's data. Here what we found was the CPU spiked to 343%. And the right latency was there. In fact, it actually automatically went and root caused itself. And so here we see that there was a resource intensive SQL query. It show it builds a chart telling you exactly what that query resulted in, why this is problematic. So again using the code as well. And here are the positive findings making recommendations. And so one of the hard parts about this is not everything is a root cause. Sometimes you just want us to do a task. That's what SREs do. You go into ask engineering at work because you wanna just understand why am I getting errors here for this one, it decided to do an investigation because of these anomalies. When I ask which service is making the query and can you generate it, of course it can. So here what it's gonna do is it's gonna investigate the code to understand where that query came from. It's gonna find the root cause and now it's gonna generate that PR. Creating a PR is as easy as this, OK? And this is kind of the difference between Cogen tools and what Resolve offers is we know what production looks like. It's not theoretical anymore. Here are the changes that were made, the expected impact. Now, using knowledge that's created within here. If you want to, you can actually just enrich with our resolve markdown file how you wanna interface with the resolve. So if you wanna add any run books, any nomenclature, anything that we can't pick up from, say, your notion, your Atlassian. And you can do that as well. Um, last night I created a slash command. Here what it's doing is it's formatting a postmortem, OK. So this is the power. This is what an AISRE should be. It's really an AI for productions. It's not about just incidents. It's what that enables you to do because what we see is, it's more than just engineers that are on call using this. It's really every engineer that has to create, that has to innovate, that has to write code, and they wanna know the impact of their code. So, let's jump back into the slides and we'll talk a little bit more about how we built this. So there are really 3 critical aspects for what we've built. Um, the first one is that it understands. so that knowledge graph, that idea, so when you think about like what is a human, when you look at those principal engineers and you're like wow, like we want them on the most critical project, what is it about them? It's they know the history, they know how the system works, they've seen all the fault patterns, they've read all the postmortems, they probably wrote 60% of them. They also have that experience, so they've learned through the process. They've seen all the re factors and been a part of those and then they've developed that intuition, that ability to reason when they've given a piece of data they kind of know the sniff test. Resolve has to do the same thing, so we have to build that understanding of the system, but we don't have the luxury of time. We have to do it through integrations and what's already available, OK? And so that's the knowledge graph. Now learning is where it gets really interesting. Because we're, I'm sure if you're starting to or already involved in AI, you understand the concepts of memory and even these markdown files if you're using cursor and so forth, um, for us it's a little bit different because there's paper trails everywhere in terms of how people use tools. Meaning like we know the most common dashboards that are used. Why wouldn't we use that information to understand how we should navigate your systems? Or if people are, say, uh, everything has a P1 label within your alerts, why wouldn't we investigate those or at least put that as a rubric, um, during an incident itself, if you're going to have a channel where everybody's working through it and you're getting all the way to causality and remediation, why wouldn't we learn from that? Um, these are all system interactions. Additionally, we provide a layer where you can add your own context, and this context can be added at a team level, an organizational level, and so forth. And that's additional knowledge that we use to inform how we answer questions and the tools that we use. There's always, say, proprietary information that you might have that you want to share so that we can be even more effective. Now the reasoning piece is really interesting because what happens is every single prompt that you see, it's really sending a message to us to say, hey, go do something with that. And so we have this planner. This planner is essentially the brains. It's saying, OK, given this prompt, what should we do? And that prompt can be a payload from an alert. It can also be natural language. Um, what it does then is it goes and it calls these. Sub agents that are specialized agents to go and gather evidence. Now they go out, so we have to respect rate limits, understand the systems, how they operate, understand the payloads that we're going to return. We have to process those as well. Once it received that information, it needs to create additional plans. OK, so what other evidence based on what I saw. Um, all of that comes out into that response, but it doesn't typically stop, stop there because of the feedback loops. So a lot of the time there's a human in the loop. That's what makes it so successful. And this is part of the UX that's so difficult is how do we make sure that a human knows that this information you can guide. You can say, hey, last time we heard there was something over here, can you go check? A human doesn't want to make that check, but we can do it for you. And so that's where the response comes out and then we go back through another loop. Now when we were building resolves, so back historically when we got started, the context windows weren't even large enough for logs. So a lot of the air logs where the root cause lived, we couldn't even process with foundation bottoms at that time. And so you can imagine how effective it could be in getting to root cause. Now a lot of companies though, they're starting to dabble with this whether it's in a hackathon or in production because. This is ripe for disruption and this is a pain point that every engineer lives with, which is understanding the system, um, having the agency to access the tools and get access to knowledge that they need. And so what a lot of folks do, and these were those first like glimmers of hope, is you just take an air that spits out, you throw it into a foundation model and you see what comes out. Now a lot of times those recommendations are pretty logical. Sometimes it nails it. But the reality is that doesn't work in complex systems and pretty much in any system with any level of reliability because you need the context, you need to understand the code, you need to understand all of these other layers beyond that. And so what a lot of folks moved to is now with MCP interestingly enough, being an AI, you're always like, is this an opportunity? Is this a threat? What does it mean? With MCP servers, what we found is now we have access to systems and motivated customers to build MCP servers to access data that wasn't readily available. Maybe it's legacy systems. Now, the challenge though with just using LLMs and MCP is say you run a query to your observability, maybe it's natural language or you say, you know what, we're getting errors from here. It can go and process that request, but what's it gonna do with it? You kind of need state. Um, you need context. There's also additional prompting you need around, say, handling times, um, like, do I check one hour or a week? Um, what happened historically? How do I know this is an anomaly, anomalous. However, this can get you a little bit of value right out of the gate is just using the MCCP servers that are available, but it's not gonna get you to. Complex problems because you still have to understand um both the historical but also the context as you daisy change these systems together and so what a lot of folks are doing, especially as foundation models get even stronger is they're going to agents. Now you can build an agent that understands a lot of context. It connects to a lot of, it can make a lot of different MCP calls. Ideally there's security and governance within there that you're taking care of as well. You also have to build the user experience for that, but what ends up happening is you're putting so much context. To these events that have happened historically, meaning like you're learning from past events and so you're adding run books to it and you're trying to give all these directives, what ends up happening is one, it becomes bloated, it becomes slow, but two, it's not prepared to happen, to handle novel incidents, but also novel questions, prompts it hasn't seen before. And that's why the single agent approach really starts to break down. And then finally multi-agent systems. That's where we've ended up. Um, the challenge here though is orchestration. So not only do you need domain expertise for every single one of these agents that you produce, whether it's processing logs, metrics, code for the specific use cases, but you also need to understand rate limits. You also need to understand, uh, failure patterns. You need to create an eval system. Those failure patterns, if we don't get results from, say, the trace agent. How do we reconcile that? Um, these are all things that you have to consider when you're building this. And so the complexity, just like moving to microsurfaces did, yes, it created a lot of like agility, smaller teams, smaller surface areas. Everything's API driven, but then you have to orchestrate it all, and then here we are with Kubernetes. And so we're seeing them all. The good news is most companies have something homegrown, and usually it's complementary. It's just an input into the systems like what we've built. So now let's hear about Coinbase's journey into AISRE and what Angelo has been working on. Thank you, Josh. Uh, I imagine it's a Friday afternoon and uh you're trying to unwind for the week, uh, when all of a sudden you hear that dreaded sound coming from your phone, right? You really don't want to hear that, but you're on call, you just got paid. There's a problem in production, you have to do something. You have to get online and resolve the problem. You still don't know exactly what is wrong. You don't know how long it will take to restore the service. Uh, it's not great. Your boss is even maybe asking you questions on, on, on chat. Uh, anyone, uh, uh, familiar, is anyone familiar with the experience? Yeah? Raise your hand. We have a few, yeah. OK. I, I've, I've seen many of, uh, of, uh, of these, um, uh, events, unfortunately. Uh, and, uh, I'm gonna try to, in the next 10 minutes to, uh, show how we can, we have improved or we plan to improve this, uh, in the future. So, my name is Angelo. I'm a, a software engineer at Coinbase uh in the infrastructure team uh where we build the foundational components uh on top of which, uh, the products are, are developed. And in my time, I spent a lot of time. investigating incidents also preventing them from happening, you know, leading the response when they inevitably do happen. My relationship with the AI initially was kind of skeptic, right? Um, until a few years ago, I, I thought like many other people that these were just a fancy auto completion tools. They may be helpful to generate some scripts that you would throw away the next day, uh, but I, I was proven wrong actually. Sometime in 2023, I realized this, this is actually much bigger than, than, than that. Um, I realized it, uh, it can actually, uh, help professionals, uh, build software much faster at a, at a way uh, greater speed. And it can enable companies to build products, um, and iterate really, uh, much more quickly. So this has an impact on the, on the tech industry as a whole, on the economy, and I believe in, even on society, uh, eventually. So, I kind of had uh this, this moment. Uh, for those who are not familiar with the, with Coinbase, it's a, it's a cryptocurrency exchange founded in 2012 that allows uh more than 120 million people worldwide to trade uh over 40,000 crypto assets. And we have a total of uh almost uh like, like over, over half a trillion dollars in uh, in assets that we have on our on our platform. So, Uh, our, our system has, has grown a lot, uh, scaled a lot, and in order to sustain, uh, this, this business, um, um, our infrastructure are also, uh, has also developed and grown, uh, uh, a lot. Uh, we went from a Rubyion rails monolith that was running just on EC2, uh, to a complex network of thousands of services, um, deployed on Kubernetis uh, clusters. And on hundreds of them. Um And we have all sorts of data stores for for specific use cases. Uh, now, we have solved one problem, but there's nothing for free, of course. And now, now it's so much more complicated to understand. No one really understands the full system and when things go wrong, who can, uh, really fix it in a, in a, uh, in a, in, in a, in a good time, in a, in a short time. Um So, Coombs has been a very uh positive in uh in uh enabling uh in uh adopting AI tools since the beginning, and that's good. So now we have uh um all engineers use uh some uh AI tools to write code, to review, um, to summarize, to get familiar with new code base. There are all sorts of, uh, use cases that uh help accelerate the, um. You know, the, the act of writing software. Um, and, and this, uh, has been, uh, uh, our CEO has uh encouraged, uh, internally and externally, uh, to, to, uh, adopt them. But their use has been limited to code only. We got to the point that writing code is no longer a bottleneck. It's, it's fast. It's not it's not an issue anymore. And as it happens with software, you optimize one function that is the bottleneck. Now you're going to see something else that becomes the bottleneck. Now it's. Uh, you're gonna see that you spend more time maybe waiting for the code review or uh um waiting for some approval, um, or maybe for the build uh to, to finish, to deploy or something like that. Uh, but there's also another thing that maybe it's a bit overlooked and it's the operations of, of the production environment which is now more complex, uh, the operations and the debugging when it doesn't go as it, uh, as expected. So, that's uh, the, the, the, the topic that we wanted to, uh the kind of question we ask ourselves, is it possible to use AI for uh uh production similarly to how um we all use AI tools to generate code. And uh to be honest, I didn't know at the beginning and uh you you, now I can, I can say that I'm, I'm, I'm a Optimistic about this, but you have the right to be skeptic, as I was a skeptic too in the past. Uh, so we decided to start with the incidents because incidents are very time consuming and stressful events that no one really likes, but you know, they happen. We have to, we have to deal with them. Uh, lots of people, lots of engineers get, uh, uh, together in a room, trying to figure out what's the problem. Uh, there's a cost, there's an engineering cost. There is a, a context switch, uh, and, uh, and, and also an impact on the business, uh, because users are probably not happy about what, what happened during the incident. Now, can we make this faster? Uh, can we resolve this faster? And so we decided to, to, to try to resolve as a potential uh tool to help us uh make this a bit, a bit less painful. And uh it, it turns out that uh it is possible. Uh, we had some uh criteria that we wanted to, to follow some, some principles and say, OK, if we manage to get all these uh all these uh um check boxes, then we can uh uh actually adopt it in production. Uh, so we, we, we trained the agent on real life incidents that happened in the recent, uh, past, uh, with the real metrics and, and everything, and then compared what is the answer of the agent, uh, versus what happened in reality when engineers were there like trying to debug. Uh, and then we, uh, we also stimulated the, um. Like kind of vague uh uh prompts. We simulated an engineer who doesn't have deep domain knowledge on whatever was broken. Um, so that, uh, kind of this level level levels up, the engineers, and even if you are not the expert on service X, you can still uh make some progress towards the root cause, uh, by using these tools. Uh, we found in some cases it wasn't able to get like deep to the real root cause, and, uh, almost always, uh, it was because of, uh, uh, missing data. It didn't have enough uh context, enough knowledge to get to the, um, to the root cause. So in practice, we, we use the only two data sources, Kubernetes and DataDog, to resolve can access, uh, can, can see what the pods are up to, what, uh, how they evolve over time, and they can read metrics, logs and traces from, uh, from Dataog. It can do a lot more, but these are the only two that we have integrated so far. Uh, we also, uh, gave you some general knowledge and you can see, you can imagine this as um um. The, the knowledge that a new engineer acquires in their first week at a company. Uh, you're gonna learn like all the things that are, um, how to deploy uh code, um, what are the best practices, what are the conventions we use in this company. They are like different from anyone else, right? And then, I mean the following weeks as a new engineer, you learn about the team specific knowledge and this and this is also what we taught uh to, to the, to the agent. So we combined data sources, general knowledge and team specific knowledge or Ram books, right? And um and it really improved the, the performance uh at the end. The performance in terms of speed was pretty good. It was able in most cases to find the cause of a real incident a lot faster. And uh uh have a, a greater than 50% accuracy, which meant that it was able to get to the real root cause. Um And as I said before, this can be improved uh by um by adding more data sources. One of the most important, uh, would be the source code, as, as Spiros was mentioning before, if you, if you give the agent access to the source code, it can do, um, much deeper analysis. It can, uh, understand what, what line of code is responsible for the problem and it maybe even suggest a, a fixed potential PR. I have two example incidents. We're almost out of time but uh this is an example of uh of incidents, just a screenshot from, from the result where, where um this was caused by a um Badly tuned load test. We we constantly low test our services in production to make sure that they didn't scale anytime. To respond to volatility events and In this case, one endpoint was hit really badly by the slow test. The resolve was able to traverse the the dependency graph of uh uh of our services and get to um to the service that was suffering, had like very high CPU utilization. It reached, it reached all the, all the HPA limits, um, and also was aware that there was a low test. Because we taught uh the agent the concept of low test at Coinbase, which is different from other uh organizations, like very specific to our organization. A second one, Was caused by an out of memory uh error in a sidecar container. Again, this was like deep into the um service dependency graph. From outside, it looked like there were like tens of different endpoints that were affected were failing with 500s, but it was kind of hard to get to the, um, to get to the root calls and all the services that were failing, they all had in common this sidecar container. That was running out of memory. Resolve was able to find that in 12 minutes. However, due to the limitation because it cannot access the source code, it cannot know why, right? And that has to be done, that has to be done manually right now. OK. So, uh, how do we use resolve today? Uh, we basically do, uh, Uh, invoke it manually. On a on a chatbot interface or from Slack as you saw before, uh, but in the future we are gonna use it, uh, and all other investigation also whenever there is an alert, uh, but in the future, I, I hope we can, we can integrate it so that it can propose remediation actually. Uh, and, uh, you know, in the future, in the next few years, I, I expect that these, these agents who will become more kind of more autonomous, who will give them a bit more freedom to operate and to suggest and even make changes to our environments, and, and we will see it as a kind of natural thing. Uh, that now seems a bit something that we wouldn't trust. Like, I wasn't trusting in, in the past, I wasn't trusting an AI tool to write code, but then I realized it was a good idea to trust it. I think we're probably gonna see something similar, a similar shift in 2 or 3 years from now. And uh with that, uh I am uh uh done. Uh, if you are interested uh in uh knowing about Coinbase, you can uh check out this link, uh, or if you wanna chat with me, you can, uh, you can find me, uh, you know, here or, or, on LinkedIn. And uh spirits. So anyway, uh, Angelo is one of my favorite people to work with because he's obviously very thorough, very extremely high bar, and, uh, you know, getting past that bar, I mean his bar, of course, in the Coinbase bar makes me extremely happy and you know what we've managed to achieve, of course, with Coinbase both pass the bar let's say of like very high quality engineers obviously that, you know, wouldn't like use a tool unless it's very effective but also very high compliance and security environment at the same time. Um, I have two questions. A year ago, actually, it was the first time I gave a demo. It was, I think, maybe around the in event to Coinbase, and the reaction was, Oh, this looks very useful, but we'll never use AI in production, I guess, or an AI tool like this. What changed over the past 1 year? Uh, I guess never say never, right, because, uh, that, that, that's, that's the same thing. I, I would think, uh, you know, 3 years ago I wouldn't let, uh, an AI agent to write, to write code for me, they didn't know how to write code, but they actually do. Uh, so what, what changed, uh. We maybe we had that red pill red pill moment uh that we realized this has a very high uh potential uh and uh security is obviously paramount, it's very important. Uh, we cannot, uh, compromise on that. uh, but once you understand the system, you understand what it can do, you can put guard rates, right on, um, on the agents, so you know it's not gonna do anything that is not allowed. Uh, yeah, so I, I, I guess we, we've spent a lot of time, correct, yeah, by the way, and uh, the other question I have is you made the prediction, right? You said you were a skeptic, you know, you started believing in code, you start believing now in agents for production. If your prediction goes wrong, how is it going to go wrong? I think it will go wrong probably in many ways. It's very hard to predict what in what direction it's going to be wrong. But again, if it's wrong, my prediction, my meta prediction is also going to be wrong too. I don't know. Uh, I, I, I think it will be maybe a slow adoption, so not, not everyone will get on board, uh, uh, you know, quickly. So maybe a few in in companies or sectors of of uh will take a little longer to adopt uh but uh may maybe this is the way it will be wrong, um but uh in general I think they will take the stage and free up some time from SREs our own happiness. Thanks for trusting us, by the way and uh thanks for the sharing all this data. uh I have my own predictions by the way, and uh I I've now worked with agents for so long uh that I'll I'll give them to you in a minute but maybe to kind of summarize in a way. What we believe, uh, or what may be the key takeaways. I think that I truly believe the impact of AI in production is gonna be more than code, because yes, we can generate maybe 80% of code, but that hasn't allowed us to move 5 times faster. Once we manage to apply the same concepts and the same reasoning abilities, let's say across all production, and that's what the result does, we'll see that actually our lives are gonna become easier and we're gonna be able to move a lot faster. Now, that's not an easy problem, right? Like Josh showed and like Angelo described about, you cannot build agents that, you know, maybe like an LLM on top of logs, right, or on top of metrics. These agents have to operate these tools as a human and they have to understand the context of the entire system the way a human would to be effective. And Finally, the issuing abilities of the agents, and we're getting there, right? The models are getting better, the agents we're building are becoming more effective, have to be almost on par with humans, right? And sometimes they have to be more effective, right? I always compare the adoption of agents in software or production, similar to maybe self-driving cars, you know, we don't trust self-driving cars if they're as good as humans, right? They have to be safer than humans to allow them to to drive on the streets, and I think the same is true for for agents, but, and I think we're getting there. And, uh, you know, if you ask me what are my predictions about the future, uh, I think we're on an exponential curve of improvement, and the reason, by the way, I asked Angelo about his prediction, because I think most of us get it wrong in the direction of uh we we kind of extrapolate based on what we've seen in the past, and we're actually. Maybe not we don't see we're not as aggressive in our prediction because it's hard for humans to see what happens in an exponential curve. I do think that productivity gains from AI will exceed any other technology that happened so far and actually will be in many ways, maybe a solution out of many problems we have as humanity even. I do think that, uh, you know, more specifically when it comes to software and technology, I believe agents will be actually doing more and more work. Today, you know, the way Angelo was describing, maybe agents respond to humans or get triggered by an alert, but I think they will start working, like, you know, 24/7 without actually complaining, taking longer and longer tasks, using more and more reasoning, and technically, let's say a lot more like this. tedious, laborious kind of effort into tokens and you know delivering outcomes and you know getting essentially humans to operate and I think we're getting there at a higher level of abstraction. So in my view of all this is that it's not like agents are going to take the work of software engineers, it's just that we're going to be producing like 10, 100 times more technology, and I think that's a great thing for the world. And uh with that, uh, you know, thanks for attending. You can actually go and use the results today. So if you scan this QR code here, it takes you to, uh, you know, a sign-up page. You can connect your tools if you want, but there is also a sandbox that you can play with right away. And you can go into the sandbox, play with it, and go back and connect your own tools, right? And this is like our preview of our, you know, fully self-service product. You can use it for free, and uh if you like it, we're happy to talk to you more.