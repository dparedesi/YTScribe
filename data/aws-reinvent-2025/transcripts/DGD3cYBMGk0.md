---
video_id: DGD3cYBMGk0
video_url: https://www.youtube.com/watch?v=DGD3cYBMGk0
is_generated: False
is_translatable: True
---

Hello, everyone. Um, I'm Ambuj Gupta from uh PWC, uh, director in AI engineering. Um, with me today we have, uh, Kevin as well who's, uh, uh, also from, uh, PWC director in AI engineering, uh, Nafi and, uh, Nafi and Wale who are from the automated, uh, uh, reasoning team who actually helped with this overall solution. So today we are gonna talk about how we are, uh. Utilizing automated reasoning in addition to bedrock, uh, to bring a more accurate and verifiable answers, uh, to different inquiries, be it be, uh, any sector. So, uh, let me get into that. So let's talk about that, uh, why did we Why did we come up with automated reasoning? Uh, what is automated reason and why do we need it when we already have guard rails, right? So, We all know that in the last many years, a couple of years, we have seen tremendous progress with respect to LLM's Gen AI, and we have seen it is spanning in all the dimensions and horizons with the greatest speed, but one consistent problem we have seen all the time is that the accuracy of the answers and ha hallucination, as you can see here uh. The hallucination part is the toughest uh uh outcome of that where it is creating a lot of problem if we are dealing with customer service industry, if you're dealing with the industry where accurate answers are more important, then it becomes a problem when we don't have a solution for that um in a timely manner, right? So to do that, uh. Uh, we have, uh, Amazon, uh, automated reasoning which works automatically with the, uh, in the guardrail space where it works to ensure that the answers are accurate, which means that it identifies and suggests why this answer is correct and based on what factual knowledge. Secondly, that it checks for the soundness, which means that whether this answer is derived logically based on verifiable information or is it something else because it always comes back with three types of responses number 1, whether the answer is valid or not. Number 2. Um, if it's not valid, why it's not valid? And number 3, it never pretends. It will say that, hey, I don't know if the answer is, uh, valid or invalid. And, in addition to that, it just doesn't come back with a couple of, uh, responses. It provides recommendation that should we ask more questions to, uh, the customer, maybe information is missing or, uh, we should pass this information for another scenario where we can, uh, present a different answers to the customers, right? So let me get to the next one, right? Lastly, that with the responsible AI um implementation, we need to make sure that any answer which is provided is, uh. Explainable and uh there is transparency in that and it consistent with respect to any channel if customer is reaching out to us with automated reasoning all this happened because uh as I was saying, guardless, they have uh limitations where you can do content filtering, you can do topic filtering, but after that what we cannot do at the scale is that implementing the business rules and this is where automated reasoning comes into the picture to ensure that we are able to utilize these business rules or a standard operating procedure, be it any industry. Uh, pharmaceutical or, um, uh, utility or, uh, financial, anything where we can have these business rules that would change in uh they would vary with different subject matter but with this automated reason we are able to scale it, uh, without a lot of modifications. Let me get to the next one. Yeah. So as you can see that automated reasoning is the algorithm search for new mathematical proof. The way it works is that it derives the mathematical proof of the rules and applies them on the answers which are coming from GAI or LLM and then provide the answers based on that. And for that it uses a logical model and software to explore the states because it is able to search across a different or vast state of information as well. Yeah, let me, let me also specify that, uh, automated reasoning is not a new service. It has been used already in Amazon for many, many years, right? So if you go to the next page, you would see that it has been used for, uh, VPC, uh, reachability to make sure that if a connection should be going out or going in, uh, IMS analyzer, that, that's how it's able to quickly change the permission whenever we make the changes in the policy or a role for a group or anything like that, right? Uh, ensuring that S3 block public access, how can we make sure that, uh, in service, which is our data, which is not supposed to be exposed publicly, uh, is, uh, is, um, is managed accordingly and then utilizing the, uh, inspector service as well, which is utilizing the same automated reasoning to identify different, uh, vulnerability and anomalies happening in the AC2 instances, right? Now let us, uh, uh, let me present you, uh, a one, diagram where you would be able to see that how we have been utilizing automated reasoning with the generative AI. So on the top you would see that generative AI is providing answer for uh, all type of uh questions, right? But as we all know that there would always be an answer, but accuracy of it is not guaranteed, right? Uh, and that's where automated reasoning comes like a lawyer who never pretends to know the information, whether it will say that the information is valid or it's not valid, or it will say that, hey, we have to take one of these systems, maybe ask more questions to the customer, maybe some information is missing, or provide some additional annotations because utilizing those, uh, downstream lender processes can take corrective actions or maybe transfer the chat or the call to a human agent. Right? And um, uh, in addition to that, it also provides recommendations what needs to be done. So, let me hand it over to Kevin, who will, uh, look into that. Yeah. Hello everyone. So just quickly dialing back to everything that Ambuj was mentioning here. So working across different use cases, building air solutions, different industries, all regulated. Now these are probabilistic systems, so we're very aware that we're producing outcomes that need to be verified. So essentially we're not flying blind. We have metrics such as faithfulness. We build retrieval augmented generation. Uh, metrics that assess quality that falls into the evaluation, uh, dimension of the AI solution development life cycle. So when AWS came to us and told us they had this specific service that gave us an entirely different dimension which could be brought in, which could give us auditability and traceability of what we're building, so. Primarily what I'm referring to is that, you know, there is a different branch of mathematics that could be applied and effectively verify the solutions that are being. Produced and the language model outcomes that are being derived for any solution that you're building, you're building all of these wrappers on top of language models, but there is a layer that actually mathematically verifies this. Now when PWC started doing this, we approached it from the lens of having two different concrete portions. So whatever Ambuja was describing in terms of that logical design and then the answer verification piece is kind of what we did in these two parts that you see. You call it policy design and then verification and testing. So staying with that for a second here. The policy design is a key concept which hasn't necessarily been approached in this way before while we were dealing with probabilistic systems, so you have a bunch of business rules. Language models are good at understanding those business rules. You ask models to look at those business rules, follow them, and ensure, you know, they're producing a result in line with those business rules. The most we used to do was, you know, absolutely follow these business rules do not spell something out if you're unsure about the solution. That's something that we've always produced, but how do you ensure that those results are consistent, mathematically verifiable and accurate? So this was, you know, ringing good news to the ears of all governance committees that review models and then pass them through to production, right? So. They came up with a solution which first and foremost lets us define those policies as logic statements. So logic statements are basically statements that use your uh and operations or operations, all of your boolean logic that we traditionally learned in high school, right? So once we're able to derive them technically what is supposed to happen, and again this is where you need full certification of it from an actual logician or somebody who has a PhD in formal methods looking through those statements and ensuring that they actually reflect the business rules. Now what Nafi and Wali and team have created is a system that's close to 99% accurate in terms of deriving those policies as well as verifying them because verification is a step where you populate these logic statements with variables coming in from your language model outputs. Say you, if you have X and Y and then R with like a bunch of other variables, you fill them in and then you have a true or false outcome. That effectively is, you know, a standardized deterministic outcome for you to verify if the model is performing as it's expected based on some policy that you've laid out and this becomes paramount in regulated industries, right? So that was the key thing that we were able to derive from the service and we're starting to sort of uh. Extrapolate and apply this to as many use cases as possible, so the verify and test bucket that's being described here is effectively testing it at training time. So you derived a bunch of statements you're trying to see if that applies to your use case and if it works well enough, you're effectively progressing it to inference time, which is the deployment and monitoring solution. And this is where Ambuch was referring to, you know, this being applied to security protocols already within AWS. So in some sense, AWS is the leader in this field. They've already developed formal verification methods for security protocols, say within S3 or, you know, judging IM policies through, you know, converting. Network topology into these formal statements but now given the fact that we have language models this could broadly be applied to any given use case that you have. So that's what we found value in and we've tested this out in a few industry applications so one being pharma content review. So you're generating content for, uh, pharma copy in some sense and you're making sure that that is aligned with the label, a bunch of business rules that are set out by, you know. Central organizations like the FDA for example, and then Ambuj will be talking about utilities outage, which is the other use case that we're focusing on here. Uh, there are other use cases like the EU AI Compliance Act where we're broadly applying every single use case and linking it back to the Compliance Act itself. And then there's some sense of doing this on cloud security where we're ensuring that cloud infrastructure is compliant against a bunch of rules like HIPPAA or FINRA or something of that sort, right, so. With that, uh. Moving back to utilities outage, and I'll let Ambuch go back. Thank you. Uh, thank you, Kevin. So, uh, let's talk about the use case, um, uh, of the utility outage. We all know that, uh, during outage situation or a storm situation, uh, Utility companies, they prepare, uh, uh, uh really well uh for situations where uh if customer is uh losing a power they are able to respond to that how soon our power will be back, how soon they'll be getting the services back, and, uh, what is the, uh, what are the like uh uh situations around it, right? So the way we have been working with other utilities is that utility they work across the states, right? And every state has different, uh, state laws, and they have to follow that. Now, uh, so far it has been that he, it is pretty much determined a way to design these processes that for one state we have. A process which says that a low, low level or maybe a low priority ticket will be responded or would be fixed and maybe in 10 to 12 hours, but other places the uh the ETAs is different, right? Not to manage all these uh scenarios, uh, the system. Have been tightly coupled and built in a manner that they are doing the job, but when it comes, uh, for the scalability, we hit a roadblock, right? So the problem that we are seeing here is that, uh, customer who's experiencing a power outage, they demand that, um, they are provided response in a quick manner, um. Inaccurate inconsistency should not be there at all, right? And there should be useful self-service options, right? So they, they know that when the power is coming back or there's a truck rollout would happen and then there would be crew at their location, right? So to do that, uh, we created, uh, utilizing Amazon services. We created this particular, uh, utility outage chatbot. So what you would see on the top is that, um. Outage of standard operating procedures, these documents, which which can change based on the states, they are loaded to automated reasoning, right? And what automated reasoning does is that it, it extract all that information and create mathematical model, derive different variables, creates rules, and index those rules, uh, in an efficient manner, right? And uh at the bottom you would see that we have a chatbot which is created using Amazon services such as Amazon Lex, uh, Amazon, uh, uh, Amazon generative VI application, Bedrock. We also have Lambda, and also you would see uh Dynamo DB which is storing the real-time information about uh the outage situation. Different, uh, across different zip code there are many companies, they offer different version of these databases, but this is a NoSQL highly scalable inform uh information database which is gonna store all the insights related to those, uh, those, uh, uh, locations. So you would see that customer is asking that, hey, when my, uh, power will be back or what is the status of the outage, right? So generative AI or the AI would provide an answer, but it that answer has to be verified, and that's where, uh, automated reasoning comes into the picture, where it looks into the answer and before it let that answer go to customer, it checks whether the answer is valid or not, because as I was saying before, that a low priority ticket should not be answered. Answered in 2 hours versus a high priority ticket or critical situation where someone is dealing with a life and that situation that should be handled quickly, right? So we want to make sure that those, uh, those scenarios are scalable and you would see that on the subject matter experts box that you would see that they are able to make these changes live, right? Because during a storm situation we also have to make validations and verification that what policy should or should not exist, right? Uh Uh, so, any question at this point, uh, with respect to this diagram? OK, so this makes sense. Uh, seems like everyone got it. Uh, with that, let me hand it over to Kevin. He would, uh, speak about the, uh, design for pharmaceutical content review. Yeah, yeah, uh. So getting back to this particular use case, uh, we were. Dealing with the pharmaceutical content review use case where we looked at pharmaceutical promotional content and Judging or ensuring that they were actually sticking to the rules that were laid out by the FDA and companies have their own internal rules that we need to align with purely for regulatory purposes first and foremost, so the flow that you have on the top is what we develop basically by instructing the language model, uh, giving it access to the rules so that it can produce judgments on what needs to be updated within a piece of content. Now what we found with the flow below is that it's complementary to what's on top, right? So language models are great at language understanding, so they're producing your outcomes, whereas formal verification or symbolic logic gives us a means to take those statements, actually convert them into logical statements, and effectively serve as a second line of defense for the outcomes that are being produced, right? So in this case if you ask us what did we necessarily use as these rules, certain times you don't have these rules documented in that case you could take a perfect example that you think is already aligned with all of these rules and you're able to derive rules out of them as well, those examples, so. Just to sort of close what we're talking about here, this is again gaining a lot of mainstream momentum as of last week, well, again, like I'm describing a fundamental shift in how we're using probabilistic systems and then grouping them with deterministic systems just last week, Deep Seek Math We do came out and. They were using in order to verify mathematical theorems a verifier which was effectively using a theorem prover or a solver that is used within the automated reasoning service, right? So I mean it's only probably going to get more ubiquitous as we, as we're building more and more use cases here. Um, happy to take more questions, but I do want to pass it along to Nafi and Wale to give us a sense of the roadmap and what they're using this for, so. Thank you Kevin. So I just wanted to reinforce uh about what both Kevin and Ambush just talked about to highlight that even though we're talking about automated reasoning and about math, it's still like really we're talking about applying this and the service is generally available to the through the Applied guard rail API and really what we wanna do is basically um. Work in terms of common sense. Like there are a lot of small, uh, actions or rules that we want to make sure that they apply and the idea is that you can take these rules and turn them into automated reasoning policy and what you're really gaining is that now you can move with confidence, right? You can take the advantages of uh of generative AI and be able to move this confidence so um we're available to help, you know, feel free to reach out to your account teams if you have use cases that you would like to, um, you know, look into the team is available to help. I'll now pass it over to Wale. Yeah, thanks Nafi. Um, I think my colleagues already said everything. I'm a scientist from AWS. Um, we've been doing automated reasoning for about a decade, and the idea now is to bring deterministic guarantees to probabilistic systems because we know LLMs hallucinates by design and so with automated reasoning. You can take policies, you can auto formalize them into logical statement, build models, and then use that as a verification layout for your generative AI applications. And so yeah, I think if anyone wants to talk to us about automated reasoning and how to use it as part of Bedro guard drills, um, we'll probably wait behind. Thank you everyone. Thank you.