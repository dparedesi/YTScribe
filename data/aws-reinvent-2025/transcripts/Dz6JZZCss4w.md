---
video_id: Dz6JZZCss4w
video_url: https://www.youtube.com/watch?v=Dz6JZZCss4w
is_generated: False
is_translatable: True
---

Hello everyone. Welcome to Reinvent. A quick show of hands, who here is running a generative AI workload in production? Majority of the. Keep your hands up if You are spending more on your AI infrastructure cost than you originally budgeted. Half, half of the room, I guess, and leave it up in case uh you are running it on multiple clouds. So you are the reason why we are here and for those who have put down their hands, you will see how one company solved the challenge of cost and scaling of running their Gen AI workloads. Whether you're running it on one cloud or multiple clouds, you will hear some design principles and the approach that will help you build and scale the best way. I'm Weber Saberwal, senior solution architect at AWS. I'm joined with Vishal Saxena, who is Chief Technology Officer at Optus, and Lavanya Bhandari, who is also a senior solutions architect at AWS. This is our agenda for the talk. I'll quickly go over through retrieval augmented generation RAG, then we'll be spending majority of our time on credit AI migration. What were some of the lessons learned? What was our approach, and then we'll conclude with AWS programs to help you accelerate with the migrations. Let's get started. So our focus is on Optus's flagship product credit AI migration. So it is very important for us to do some level setting here. So what is retrieval, augmented generation, or RAG? So it enables your LLMs to query on your proprietary data. So instead of asking an LLM a question, you are asking it to answer. On your data, your documents, and your context, so it has 3 stages. First, it will retrieve the relevant documents or chunks. Second, it will augment the prompt with that context. And third, the model will generate the response. Here are some of the common use cases, so RAG is a fundamental concept that is empowering a lot of enterprise use cases, whether it is improving the content delivery because now your responses are grounded to the verified sources or it is building contextual chatbots on your business data. Sorry, or it is on doing personalized search which you can do beyond your keyword searches or doing a real-time summarization of on massive amount of documents. And on AWS. We have knowledge base for Amazon Bedrock, which is a managed rag capability on AWS where you can build the entire end to end workflow, whether it is ingesting documents, retrieval, prompt augmentation, or response generation. It has built-in session management and it gives you automatic citations to all the responses you get. Let's quickly see what's what happens under the hood. So for the first data ingestion workflow, you have your data sources, you divide it or chunk these documents and you send it to an embedding model to get those vector embedding to keep the semantic meaning of those chunks. You store it in a vector store, so when the user goes and asks a query, the same embedding model is used to embed those queries. Hit the vector store, get the relevant context, and augment the prompt to give it to your large language model. It could be anthropic cloud, meta, on Amazon Titan models and get you the response. Fairly simple, right? You have retrieve, augment generation. But what most people don't talk about is productionizing G AI is hard. So you see 90% of the Gen AI POCs don't make it to production. Once you go to production with the scale, latency comes in, and that gives a bad user experience. 74% of the Gen AI projects hit performance or reliability issues, and my favorite, 52% of the companies report fragmented tooling and have vice-based evaluation, meaning. They are duct taping different tools, application, cloud providers. So if someone asks how's the application doing, the answer is, yeah, it looks fine. It's because they don't have end to end observability. So we have already discussed productionizing Gen AI is hard, which is a bigger problem. You have a company who is running their Gen AI workload in production, serving real customers, and migrating them is even harder. These are some of the common concerns, and these were the exact concerns which Optus had it in mind when they started their migration to AWS. What about the security and compliance? What about the architecture we are building? How is it gonna scale for our growth and most importantly. What is the return of investment? What is the ROI? Let me invite Vishal on stage who will give us what are those non-negotiable requirements and the approach how they started the migration. Thank you, Robo. I must say that I can certainly relate with all the concerns that Weber mentioned. My name is, uh, Vishal Saakana. I'm the chief technology officer at Optus. For those who don't know what Octus is, Optus is the leading provider of data, news, information, analytics, and workflow products on credit market. Think about us as a Bloomberg of credit. Uh, we started more than 10 years ago. We were primarily focused on bankruptcy, which is where you find distressed assets, but over the last 10 years and so on and so on, we have pretty much expanded our product offering all the way from distressed assets to non-performing loans, performing loans, high performing loans, private credit, and pretty much the entire life cycle of credit market today is, uh, supported by the Optus platform. In terms of who uses our platform, if you are focused on trade, market and financial service industry, you are today most likely using Optus. Uh, our clients could be investment banks, investment managers, uh, law firms, advisory firms. Now, we're here today to talk about the migration of grid AI. But before we talk about the migration, it's important for all of us to have some context on what grid AI is. So that's why I want to play this two-minute quick video that's gonna give everybody some sense as to what the product actually is. Looking for the edge in credit investing? Information overload is your enemy. Actionable insight is your weapon. Meet Credit AI Vault by Optus, designed to break down barriers in private credit data discovery. Instantly access and analyze millions of FIDOCSteel documents, transcripts, and Optus intelligence data. Ask natural language questions, get real answers, not search results. Credit AI Vault securely accesses your public and private FIDOC documents as well as deep Optus intelligence. Vault delivers insight with speed and precision. Source citations keep you audit ready, and responses link directly to the original document. Real-time sync means as soon as a deal doc lands in your data room, Vault is ready to extract, analyze, and act. With advanced permissioning, your private data stays secure. Each firm's knowledge base is protected, fully SOC2 compliant. Drill deeper, pivot instantly and gain context. No guesswork, no wasted minutes. Vault integrates seamlessly into your workflow, empowering you to outpace the market and turn intelligence into outperformance. Experience the power of Credit AI Vault. Activate Vault on your FinDoc subscription today. So that is the Cre AI, uh, what you just saw is the next generation version of Cre AI called Crid AI Vault. Uh, but when we launched Create AI as the first gen AI product on trade market in the industry, uh, that was back in 2023. And when we launched Crede AI, one of the challenges that we had is that even though AI space was constantly evolving, the models that we actually wanted to. Use the ones that were giving us the best performance in terms of accuracy and so on was the open AI models which were not available on AWS back then maybe not even today, but that's beside the point. Uh, we had pretty much all the product offerings, uh, hosted in AWS so as you can imagine, great AI is just one out of more than 1 dozen products we offer to our customers. So now we have, uh, partly. Uh, product hosted in AR, uh, which is the open AI side of the create AI, but then everything else is in the, is in the AWS that led us to a couple of different challenges. One is the scalability in terms of like if you think about what Webhub talked about in the rack architecture. There are different components of the credit AI architecture whether it's the embedding, whether it's chunking your knowledge base, your vector database, uh, even your GPU instances we basically had all of those managed by ourselves. We pretty much built a component driven architecture because we realized even back then. That the AI space will constantly evolve and we wanna be able to swipe in and out these components as and when the new technology is available, but the biggest challenge we had that we had to still manage the complexity of that architecture. So that was a 11 challenge, as you can imagine, the data and the services that are backing create I are also many of those services and data also backing many of our other products. Which basically means we have to shuttle data back and forth between AWS and AJR, which led to its own set of operational challenges and complexity. We wanted to simplify that architecture by pushing the expertise to the provider as much as we can so that we can actually focus on building high value features for our customers and not worry about the managing the complexity of the rack architecture. And then also wanted to unify our cloud architecture. So those were the kind of challenges that we wanted to overcome. When we started the journey of great migration. Now, As we are thinking about the migration back then, we just didn't want to do something that's only one step or two step further away. We wanted to really think more holistically, more broadly, so that we get it right for, for the next, uh, several years, and that's where we came up with this set of non-negotiable requirement. You, as I kind of show you that you will see that these requirements will be applicable to pretty much any software architecture, uh, but sometime, you know, when we are looking at AI products, we often. Uh, overlook how, uh, all the other architecture requirement that you would typically see in a software will also apply to AI products. So what are those non-negotiable requirements? Scalability, as we, as I mentioned before, uh, we wanna be able to scale the offering, whether it's the increase in the development speed or spending less time on managing the complexity or just giving better response time to our customers as we get more and more customers on our platform. Obviously you wanna minimize the cost. Uh, multi-cloud operator has its own set of challenges and costs, not to say that, you know, you cannot achieve it, but if you can simplify, why not? Why not simplify and bring down the cost and optimize our infrastructure. Response, uh, is very important from an end user pers perspective because you can have the best LLM model being used but if your, your, uh, response time to customer is not subsecond, uh, then you're not giving them a great experience so we wanted to make sure that we're able to also optimize and kind of improve our latency. We are a SOC2 compliant platform. That means we have obligations to meet our own SLAs, uh, that let us things like zero downtime is, is basically, uh, is a very hardcore requirement for us and then. We are constantly evolving. AI space is constantly evolving. Our customers want more and more products, more and more features, which basically means we have to be able to double and iterate our features faster than ever before. So these were our non-negotiable requirements and obviously, as I mentioned before, multi multi-cloud gives you, you know, some sort of complexity that you can certainly manage, but we wanted to consolidate and kind of improve the reliability and operation maintenance of our, uh, infrastructure. So why did we decide to go ahead with Amazon Amazon Bedrock as Weber mentioned before, rag architecture has a number of steps that we were managing on our own and if we can simplify those steps, why not simplify so that we can spend more time focused on, uh, building future development for our customers. When we were using OpenAI, we knew that there will be other models out, you know, at some point who might be better, better use for us. And when anthropic cloud came, we knew that's just the beginning, right? There will be more and more models out there. So why don't we just have, uh, a mechanism in place where we can swap these models in and out, and we wanted to have access to the best LLMs out there. Obviously we want to. And reduce the cost, but as I mentioned before, we have a lot of services and data in AWS already backing our other products. So if we, there's an economy of scale between those services and services that are powering create AI, so if we can simplify, uh, those services or bring them together, uh, we can actually. Scale further, uh, I also wanted to mention that we were using, we're managing our own GPU, uh, instances, so we want also wanted to kind of scale them because with the, with the credit AI you're having the real-time question and answer coming in you have to, uh, scale the embedding in real time. Uh, with AWS because we're already using it, we kind of knew what is the expected, uh, service level offering. We had, you know, certain SLA already being accomplished for other products we wanted to bring them to create AI. I already mentioned AWS consolidate consolidation and the most important thing is when you start a journey you wanna make sure that you are, you have the right tools to be able to, you know, kind of de-risk your, uh, unknowns as fast as you can. You don't want those unknowns to be kind of floating out in ether for too long because eventually you're gonna run into them anyway, so you wanted to, uh, you want to de-risk those, those concerns earlier in the process and that led us to do POC with. Amazon Bedrock, our solution experts from AWS were very kind to spend a lot of time with us, and we were able to finish POC, uh, quite fast, uh, pretty much in two weeks they were able to have something working end to end, uh, and that had a very promising results. Uh, obviously there's nothing to do in terms of infrastructure automation yet because you're still in the POC stage, uh, but as you all can imagine, POC is not the same as production, right? You can, you know, address some of the concerns, but there's still a lot that you have to kind of unpack and kind of solve. So what's the difference between POC and production? Let's do the reality check. scale you probably have hundreds of documents in a POC to test your accuracy, to test some of those concerns that you want to kind of alleviate early on, but in production you're talking about millions of documents. Latency in POC you will typically be OK with whatever latency you are getting because your, your answer to that will be, well, we can tune it later on, yeah, but then later on is not possible in production. In production you're looking at subsecond latency and you need to have that so that you have, you know, great, uh, end user experience for your customers. In POC stage, you probably will have, let's say, up to 10 users playing around. In production, you're talking about thousands of concurrent users. That you know infrastructure quality is not that not that of a concern POC, but in production you're looking at high availability, disaster recovery, all those good things that you will have typically for a SOC2 compliant, uh, platform. Quality in POC again could be good enough because you know you can improve that accuracy, but us serving the sophisticated financial services customers we have to have 99% plus accuracy because that's that's very important for us to have a uh a a solid product in the market. Obviously you have to think through SOC2 compliance, uh, all of your legal compliance, governance type of concerns, uh, security is a big part of that. You have to kind of have that available in production and even in lower environments, to be honest, but at the POC stage you generally don't think about a lot of those things up front. Uh, again, uh, on the operation side, uh, you can drop a bunch of documents in S3, set up your, uh, knowledge base, get the POC going in 2 weeks, but in production, uh, you wanna have a fully automated CICD pipeline. You wanna have all those monitorings, all the reliability tools, uh, in place so that you can monitor at scale. So that's the difference between POC and production, um. So how do we, how do we achieve, uh, that migration from PUC to production and for that, um, I'll hand it over back to Weba who's gonna help us understand the architecture that we use to get from point A to point B. Thanks Michelle. So what we have covered so far are the challenges and the non-negotiable requirements. So anything we do now we have to keep those non-negotiable requirements at the center of our discussion. So let's see what was our final architecture and then we'll drill it down what were some of the design decisions we made, uh, before we arrive at this architecture. So we'll go with the data ingestion workflow. The first step is the data extraction. So we built an even driven architecture. As soon as the new documents, new data come in, a lambda gets triggered to do that initial validation first on the document types, document size, and extract the relevant metadata from each document. And then a call is sent to Amazon Textract. Amazon Text Track is responsible to extract the text. And the structure and information from the document while preserving the document layout and relationship between different content elements. And then it gets stored in Amazon S3. Now Amazon S3. Has a direct integration with Amazon Bedrock, and that's where All the chunks or the content what we have extracted from the documents are stored on Amazon Estuary in a separate prefix because you do not want to mix it with your source document, but there is always a data lineage between the extracted content and the source document along with when was this extraction happened, the time stamp, the metadata related to it. Now with Bedrock, if you recall the initial. Rag pipeline which I explained. The first step is to chunk these extracted documents and we'll be diving deep into what are some of the chunking strategies there and then make a call to a coherent embedding model to get the semantics or the vectors for those chunks and store it in a vector store here Octus chose Amazon open source service for efficient retrieval. This is very similar to the rack pipeline I shared earlier. Now let's go through the Q&A flow. First, the web app is hosted on AWS Fargate with scales as per the traffic and going back to the non-negotiable requirement on scaling and less maintenance or overhead, it falls right, it fits right in there. So the initial validation on the user is done at this layer, and then we are using Amazon MSK as a streaming service to handle the inter-service communication. While maintaining high throughput. Efficiency for the query outputs. Now this is the layer where the entire query cycle orchestration happens. So it makes calls to different services and it has a direct integration with the rag pipeline. The first, when the user is asking a query, it makes a call to the coherent embedding model again hosted on AWS, and this happens under the hood. Get the relevant chunks from Amazon Open Source Service. But this is the additional advantage. It has out of the box integration with Amazon bedrock guardrails so that you can have quality checks and security controls right within your pipeline. And then the model generates the response and sends it back to the user. So you see this slide is very busy. Now let's go and drill it down what were some of the design decisions we made before we come to this architecture. This is one of the critical design decisions you'll make on the chunking strategy, that is how you go and divide or chunk your documents into logical or smaller pieces. The first one with any POLC we tried was fixed. It works well as long as you have small documents, but actors are long, complex documents, and The flip side is it might chunk the document mid sentence, mid-paragraph, or if you have a long financial table, it can chunk mid-table. So, for large, complex documents, the results were not promising. The next one is hierarchical chunking. When Octus tried that, it gave really promising results. So the idea here is to have two layer structure. One, a child, uh, chunk, which is for. Precise matching, and a larger parent chunk for the context. So when user asks a query, child chunks and the parent chunks are sent to the LLM. So now it has both precise matching and context. But here there is a catch. The embedding model which they chose had a token limit of 512 tokens, so the larger parent chunks were not able, were not being handled by the coherent embedding model. So the 3rd chunking strategy, Octus tried was semantic, where you use a large language model to identify. The natural breakpoints within your document, it analyze the documents, understand the meaning semantics between different sections, paragraph, and divide it. Across those logical boundaries, it could be the end of the section or topic switch. But here, the catch was, there is a cost associated with it. But Optus was OK to absorb the cost because it gave really good results. So here you see there is no one size fits all. You have to use the chunking strategy based on your document types. A document, a chunking strategy for a support document cannot be the same for a 200 page financial document. Now we have the chunks. The next logical step is to embed those, and as I mentioned earlier, they chose the coherent embedding model. But before that, Octas was managing these embedding models on dedicated GPUs. Which was giving them challenges to scale. And it was very costly. So using coherent embedding models when they tested it gave them superior retrieval results. It was able to understand the complex financial documents, the relationship between different entities, and they got the right results that they were looking for. Their customer base is across Americas, Europe, Asia, and it handled the multilingual cap the model has multilingual capabilities which they needed. And last, manage service benefits. Now, the embedding model or the instance where you are running the embedding model is no more a bottleneck. So previously, once that instance goes down the entire pipeline. Is done. Now because of the managed service. They don't need to worry about the model upgrades or the coda errors. They simply need to call the service or the model and get the response. So implementing this increased their throughput by 10x, and from a bottleneck, it became a non-issue. So once we have these embeddings, it also helps them to scale to multi-tenant architecture. And now let's see how did they manage the multi-tenant isolation. So Optus does provide an option to their clients to bring in some private data. So hence it was a deliberate decision. To keep client A data in a separate knowledge base and client B data in a separate knowledge base because they wanted to have these physical isolation or boundaries between these two knowledge bases and don't want to rely on the client ID on the documents. This also helps you Get your security and compliance. Easier because your customers demand and auditors require you to have physical boundaries between the data. Previously, because of the embedding self-hosted embedding models, it was not possible for them to scale. Keeping the same GPU instance or dedicated GPU instances for their clients. But kudos to Octa's engineering team. They had to bring in some innovation to support it. The first AuthZ access management. So as soon as the user logs in. It does a real-time access management on what all knowledge bases the user has access to. Second. What is the fine grain excess user has on the documents, so that help them to manage that excess as soon as the user logged in. And then the global identifier service. So in financial services like you know there can be a company ABC Corp which is mentioned as ABC Corporation or with their ticker symbol ABC so this service manage the cross references so that any questions related to ABC. It fetches the relevant information across all the documents, wherever that reference is made. Then moving on, how we further strengthen the security is bringing in bedrock guard rails, which provides the content safety and performance optimization. So, Bedrock does have inbuilt content filtering based on the rules. You can filter out topics, inappropriate content, or even you can prevent users to ask some specific queries. So for financial compliance, there is no PII leakage and there is no inappropriate content which is given back to the user. It does have built-in hallucination controls and relevancy checks. Earlier, Optus was using a third party application TrueLens to make the call, and that added latency and some more cost to their infrastructure. But with this, since Guardrails is built within the pipeline. There is no need to make these external calls, the token usage is low, and the response and performance is better. So instead of having multiple calls and bringing an external service complexity, it's one call and there is built-in safety as part of Amazon guardrails. So once we decide on some of these design decisions, there has to be an iterative process to improve the performance. And these are some of the decisions we made as we matured the rag pipeline. The first on the search type. So one option is to use the semantic search where based on the meaning of the query it fetches the real, uh, it fetches the relevant context, but then there is an option to do hybrid where both semantic and text-based searches are made and then it merge the results and give it back to the user. Because of the extensive metadata they have put on the documents, it was easy for them to put metadata filtering, and that improved the quality results what they were getting based on the query. You know, the system was identifying the metadata to be put in so that only the results relevant with that metadata are given back to the user. Re-ranking. Initially Octos was hosting a re-ranking model which again bring that complexity of managing your infrastructure cost, but Amazon Bedrock Knowledge Bas has out of the box integration with the re-ranking models so once the content is fetched. These contents are ranked or there is a score given to it based on the relevancy to the prompt and then it is sorted and given it back to the LLM for the content generation. So it might result into less number of. Uh, content. And improving the performance. And the last is on query reformulation. Optus is dealing with complex queries. So if you give a complex query and try to fetch relevant details, sometimes the content dilution happens. So the idea here is to break down the complex query into simpler queries. Run it in parlor, fetch all the results. Pull it and give it back to the LLM so that the responses are relevant to that complex query. But one of the main decisions they have to build was how strong is their data strategy, and I'll invite Vishal again to explain the data strategy and how did they do the migration and the results. Thank you. So you may be wondering, in a, in a Ja AI application, why are we're talking about data strategy. And my answer to that is your AI is as good as the data behind it, right? If you don't have a unified data strategy, you're not gonna have a unified experience for your GN AI application. So I wanna spend a minute to talk through what all investment we made in our data architecture. And by the way, that was not just for the G AI product that was across the board for all products, but it played a big role, uh, in bringing that unified experience for our great AI application. Typically in an organization you have some sort of data generation application, data collection application. It could be third party data. It could be data that you're preparing, uh, within your organization. I call it like an as reported data, right? Uh, but then you have to figure out a way to kind of standardize your data collection so that you can actually unify data set, uh, across multiple data sets. And one of the main thing that you need, uh, it depends on the indu industry to industry, but in financial services as. Above mentioned, uh, they're different identifiers. They could be QSIP ISense it all, and so on, and you need to have some sort of mapping across different identifier types so that you can join different data set across same identifiers. We built something called master data management that allowed us to kind of centralize the data collection and unify, identify lookup through a reference data service. We did a similar unification on the document side, making sure that all the documents are ingested in a centralized way and then you basically. Standardize the data from as reported into some sort of standardized data that your application can consume and in order to do that, we build a set of data pipelines. We primarily use AWS Glue, but we also use some other technologies depending on the complexity of the data set you are transferring from point A to point B. But the goal is here to basically ETL, right? Extraction transform transformation and loading. You're basically simplifying your reported data structure into a a normalized data structure which is more suitable to your application. On top of that, then we have basically sort of APIs and those APIs allow all our applications to consume data in a very consistent way. We don't want every single engineer to figure it out those queries again and again. There's also an economy of scale. There's also a consistency and there's also, uh, more reliability and better monitoring because we know that there is a single data access layer and you're seeing on the right hand side create AI as one of the application consuming those unified data services, uh, through this architecture. It's an investment we made early on across the board, and that certainly played a big role uh in bringing that unified experience for great AI. All right, so we spend all this time thinking about non-negotiable requirements, um, as well as all the other things that are that needed to be in place for you to migrate, but what actual migration look like, right? So, infrastructure of code is very important to us. We use Terrafon across the board. Uh, we wanna make sure that, uh, we're able to scale infrastructure automation. As I mentioned before, at the time of PUC it's very important to have a unified, uh, integrated CICD pipeline so we make sure that is in place as part of, part of the migration. Obviously you have CDI already running in production and we're migrating into a different simplified and unified cloud cloud architecture. You need to have it running in parallel to make sure that you're doing your AB testing and making sure that you're cutting it over in a, in a way that is, um, uh, least, um, interruption to your to your users. So you're doing those parallel runs and validations, uh, behind the scene. Obviously you wanna make sure that you have security and compliance in place. uh, AWS, uh, guardrail is there, but then we also use a lot of other, uh, technology like Wiz, uh, as a cloud security posture management to make sure that we're constantly monitoring our cloud. Workload and again that's not just specific to create AI that's across the board but it's very important to call it out because when you look at a gen AI product uh focus is always on the LLM and the coolest technology but not necessarily on all other things that you have to have in place for it to kind of scale in production. Uh, we use Dataoc for monitoring and pager duty for alerting, so we made sure that those things are in place. And at the end of this migration, after all this hard work, let's look at what are we gaining in terms of results. Right? So the cost went down by 78%. Uh, we are not managing anymore our own GPUs. We are able to gain economy of scale by combining and leveraging, uh, data services that are already out in production for other workload, and that also means the cost per question went down by 87%. It also led to a better user experience. Uh, rabo mentioned about how we were able to bring the private document, uh, feature in credit AI thanks to the simplified architecture and odds the investment that we made as part of data architecture. Uh, that also basically means we're able to deliver features faster than ever before. That also leads to, uh, more customer satisfaction. Before we were able to sync documents, uh, in hours, and now we're able to sync documents in minutes just because we can scale those embeddings, uh, with Amazon Bedrock. Uh, I mentioned the, the increase in the development velocity just because of the simplified architecture and the fact that we're not managing all those steps on our own anymore. We're letting Amazon Bedrock manage all those things behind the scene for us. By unifying the cloud, uh, architecture between AWS and AGR 2 just now everything is in AWS. We're also able to, uh, reduce the maintenance overhead and, uh, just have, uh, spend more time on monitoring and alerting one infrastructure versus trying to do that on two places. By the way, you can certainly manage multiple cloud. I'm not against multiple cloud by any means, but if you can simplify, definitely simplify. If you can't simplify your business reasons to have multi-cloud, for sure, go for it. So here is the results in a nutshell, uh, I talked about each of them, uh, but as you can see, uh, these were the kind of gains that we achieved as part of the migration. All right. So now, let's look at what all we've achieved. We talked about scalability as a, as a core uh non-negotiable requirement. We were, we were able to achieve that. Uh, we talked about reducing cost. We were able to achieve that as well. Uh, response performance got better because now we're using the best model available and as I said before we were using best model available before too, but now we can I trade we can we can match up that evolving needs of AI just because we have access to the all the LLMs that are out there available on Bedrock. Obviously SLAs we were able to meet those SLAs uh and then. Simplified architecture allowed us to just focus a lot more on what our customer wants, which is basically the features that they're interested in versus us trying to manage the complexity of, of, uh, the rack architecture or multiple cloud infrastructure. Which basically means our DevOps teams now focus on more higher-end, uh, uh, value task. All right, let's talk about lesson learned from this entire migration. I think one thing is for sure, we had a very good view on what kind of sim simplification we wanted. We knew those non-negotiable requirements from day one, so having clear cut requirements on day one certainly help us stay focused on this migration. We also had a strong support from AWS solution experts whether it's PUC or pretty much solving complexity at every step of the migration. They were there right next to us and that helped us a lot, um. In terms of what was really hard, I would say that it's a constantly evolving space, right? You're always trying to get ahead, you know, 10 steps ahead, and then you realize that the entire, you know, uh, industry is suddenly 10 steps ahead. So even though you're ahead, you're constantly catching up. There's not a lot you can do about it other than just thinking about how do you simplify your architecture in a way where you can make changes easier, right? If you can iterate quickly, you can keep up with that pace, uh, there's no, no, no like single bullet answer, uh, other than that, in my opinion. In terms of what are we focusing on next, uh, here's a roadmap. There's a lot of different things on the slide. I'm not gonna bore you with all the details, but two themes that I want to highlight here is we're focusing a lot these days on agentic workflows, agentic AI. Uh, we actually launched something called covenant AI, which is a deep research on covenant agreements, uh, available in production for our customers. Uh, we're also seeing a lot of demand from our customers to interact with great AI through APIs, and that means you're gonna focus a lot more on the MCP server setting it up and kind of designing it the right way. That's a, that's a new space for, for all of, all of us. So those are the two main, main themes that we're gonna be focusing on in terms of the roadmap. And now I'm gonna hand it over to Weber for closure. So we talked about a lot of things, but if there is one slide I want you to take away is Dancing coconuts. Who doesn't like dancing coconuts? No, but let's see, what are some of the best practices for rag applications? Chunking strategy. We spend a lot of time on this, so break your documents into meaningful. And have the right chunking strategy. So test it and see which one is right for you. Next is optimize. Look for the parsing or the hybrid search, uh, re-ranking strategies, what we talked about. Cache results for the common queries instead of hitting the vector database again, see if you can have a cache database for the faster retrieval. Observability, log everything, user experience, the model response, the quality hits. Make sure you observe all the different metrics or parameters. Never go blind. Build in a rag evaluation. Make sure your responses are grounded to the test data you have, to the documents you have, and there is a relevance of all the responses what you are getting. Update. Do increment updates to your vector store. If you have stale data, that means you have stale answers. So make sure your vector store is updated to the latest documents, information you have. Test, test, test. I cannot stress more on that. And last is on the scale. Increase the load slowly. Then make it repetitive and you should know what are your applications break points. Kudos to my uh colleague Man for building this mental model. So if anything you I want you to take away from this talk is coconuts. Now let me invite, uh, my colleague Labana Bhandari, who will also share what are some of the AWS programs available to support and accelerate your migration to AWS. Hi, everyone. Wow, that was a lot of information how Octus went through POC to production, uh, for their migration journey on credit AI. Uh, how many are in POC phase today with generative AI applications? Anything at all that you have your use cases today with in the POC and not yet in production? All right, I see some few raised their hands. Um, that's a good start. What we are what we have also learned is 90% of the POCs are not going into production. Um, I, it could be because they have, uh, limited understanding of ROI or how to take it forward to production. Um, believe me, you're not alone. We hear that from a lot of customers, and that's where we want to help. Um, so if you are not talking to your account manager, ask them about all these programs that can be an enabler in pushing your POCs to production. So bring your out, uh, outcomes. What outcomes are you looking for? So typically. Uh, most migrations go through all these phases from exploration to, uh, fully deployed production applications. Um, when it comes to exploration and POCs, we, uh, Yb and I are part of a demo squad program. We facilitate that for our customers. Um, we leverage your use case. You bring the use case to us. We build the custom demos for you. And we share um the results we demonstrate that to you um and then we can take it forward. We also have uh another uh program team that uh is on Gen AI uh in innovation center uh they also leverage your use case but then they have an advisory option as well. So if you are willing to do it yourselves, um, you have an option to engage them and they can, uh, provide advisory role going all the way into production. All right, um, in the advanced, uh, phases where, uh, piloting or deploying or going into production, we also have our professional services, uh, that can help you with your, uh, Gen AI journey, right, um, our partners play a very vital and important role here because. The most important thing, they bring a good funding element here so they have options for you, um, to be able to leverage them for various phases within your, uh, journey and also into production. They provide consulting approach obviously and give you that. So AWS brings the GEI maturity and you can build those capabilities, all right. Uh, with our AWS migration journey, um, and our most of you might have heard about migration acceleration program if you have not, that also is another enabler for you to look at, um, your entire, um, end to end migration plan, uh, of your workload, uh, we go through from discovery all the way up to deploying and continuing into production. But we don't want to give you a solution when you come to us and give us a use case. It is not a good practice for us to, you know, go through and say, oh, maybe this is a good idea. Let's push it to in bedrock and make your application built on bedrock with MAP, a migration acceleration program. We assess and validate each of these phases and, uh, understand the objectives going. From validating if this is a good customer fit if your use case is a good fit for the solution that we're thinking would apply right? and then going through, uh, preparing your data strategy, do you have data silos? How are we bringing your data? What kind of different data aspects we talked about rag knowledge base and all those components that might help, uh, but how are we preparing your solution towards that once we have a good understanding of your. Um, discovery and qualifying into that map program, that's where we dive deep into understanding the use case, the outcomes, figuring, figuring out a technology solution is this the right fit of putting even a full emphasis on your workload and what does it take? We talk about, uh, AWS solution. We talk about your, uh, execution path. Um, and take it to the production. Some of the objectives here that you are seeing are into execution where you can also see some of the validation and most customers who ask, who don't ask in POC fees, however. With with production how we shall mentioned in Octus we had to go through a security and compliance that's a key important factor to mention when you're actually talking about to those program owners uh either uh innovation center or even a partner talk about your. Compliance needs because a responsible AI plays a huge role in your production deliverable, right? Um, you don't want something to a chatbot deployed into production and you have a latency sitting there you ask a question, it says, wait, I'm thinking, and then it gives you a response after 2-3 minutes, which is not a great experience, right? So make sure there is enough and latency built in as well as proper responsible AI because you want the right answers to the right question and to the right customer that you're presenting. So, um, beyond your production, making sure your metrics, your observability is properly managed, making sure you're validating your production metrics we also support through our map program. You're, um, go to market. Options as well. How do we help you with scaling your, uh, deployments beyond production? So, with the understanding of a complexity around migrations, a common pattern that we often see is around your LLM. API endpoints where they are hosted in your external APIs, um, and migrations that our customers are looking for from that is an easy switch to Amazon Bedrock endpoint and you can have inferences built through that and we use our own influencing chips, uh, influentia as well for Bedrock but there's nothing to manage. You're not managing any infrastructure and that's what Optus went and showed you how they leveraged, uh, savings on their infrastructure because when they switched to Bedrock there's no infrastructure to manage and you are actually using a silverless option there so you can use Bedrock endpoints to be able to. Migrate and this is typically within 1 to 4 weeks you can validate, do a POC, do test, and then switch your LLM end points to Amazon Bedrock. Another next level uh of complexity could be your advanced workloads where you have either a fine tuned model that is built in or you have an existing model that is already built for, uh, that is already working with your operationally uh set up. Uh, in your production and you're willing to migrate that because you want to either scale, you're looking for latency, or you're looking for more complex opportunities, um, with that Bedrock allows you to do with custom import model option. Um, with that you can either bring your own model, um, that is built outside or within another source or even with SageMaker if you have a sage maker built in pipeline that you have built your models, you can also do custom import into Bedrock. This also has an option of model distillation. I'm not sure how many of you heard about that. So if you have a complex workload, uh, built into your generative AI application, you could leverage a larger model LLM and distill that to a smaller model. For obviously cost reasons, so you get a very cost performant option within Bedrock, and this can typically take anywhere from 1 to 3 months, right? Uh, depending upon your use case, your workload, uh, and we are here to help on that, right? The last, um, migration complexity could be around full stack where you know that LLM is acting as a brain, but actually you have much more beyond that. You need to think about how your customers are engaging, what are your inference options you might need multi-region depending upon where you're hosting your application or you might also need a couple of, uh, interaction points or agents that you may want to take. Uh, have the action taken by agents in certain cases, so you have multiple, um, outside factors other than just the, just your code generative AI application so for that. Um, you could start leveraging, uh, Bedrock advanced features such as guard rails for security, um, model distillation as I talked about, um, and prompt flows or prompt techniques, templates, uh, that you can enable within Amazon Bedrock to build that full end to end application for your advance full stack. Um, the last one is about, uh, understanding how the model evaluation works. Typically before, even when you think about, uh, um, some of you might have had an open AI or when it started first launched, uh, their application you started thinking and asking about and build their quick application in there. But then not every application requires your same level or same model for all your use cases, right? You may want to look at which model fits better for your use case as Vishal talked about Cohere was, uh, a best option after validating some of the models that Bedrock provides. Bedrock has a feature of multi-modal access that you can leverage and see which one fits better. Right, for your use case, so in your, uh, evaluation path when you build your generative AI application, you collect the data, you validate, you select a model, and then you go test. When you're doing that, either you have a third party or you have, uh, an AWS or an internal. set up when you're going through and optimizing that model, Bedrock gives you an option to do model evaluation so you can evaluate for quality, you can evaluate for speed, you can evaluate for cost, the, the three things that Optus actually did, uh, and then, uh, obviously you can look for other, uh, metrics around your rag or your agents as well. Then you go into finalizing your model selection, building that into your bedrock pipeline for production. Alright, um, so that should leave us 5 minutes. If there's any, anything that you have questions for us, we are happy to take in the, uh, audience beyond this time, uh, I think we can take some questions outside, uh, so we can enable other teams to come in. Um, thank you, uh, for listening. I know that was a lot of information, um, um, from Waib, Vishal, and Lavania here. Thank you so much.