---
video_id: UfGW7RoaNhc
video_url: https://www.youtube.com/watch?v=UfGW7RoaNhc
is_generated: False
is_translatable: True
---

Hello everyone. Welcome out to our session at Amazon S3 Security and Access Control best practices. We are super excited to talk to you about this stuff today. Uh, we've given smaller chalk talks that reinvented previous years, uh, about our best practices and what you can do in S3 to make sure your data stays secure, uh, but we're glad to be able to talk to a broader audience and hopefully give you some really good tips today. Uh, my co-presenter here is Akshat. Uh, he's one of the best product managers I've worked with at AWS, but he's also, uh, maybe even more importantly, got the skill of catching the shrimp in his mouth at a Japanese hibachi place. It was this Sunday. It was great. I caught a shrimp, uh, and this is Bryant, uh, principal engineer at Amazon S3. The running joke is that between the two of us we have 22 years of experience. 17 of that is Bryant. Um, so if anybody has any questions after the session we'll be outside, um, happy to answer any questions, but I also likes smooth jazz, so that's extremely smooth jazz. That's, that's my jam, yeah, uh, we're gonna be talking today about, uh, some of the ways that we think about security at S3, some of our tenets that guide us when we're trying to make decisions. Uh, we also have some best practices for you. And just to be to warn you in advance, this presentation goes to 11 because we've got 3 tenants and 8 best practices. Uh, we'll end this session with, uh, some actionable next steps, things that you can do today as leaders in your organizations or as developers to make, uh, your data industry more secure. Cool. Um, so start off with how does AWS think about security, um, and we went back and forth in this part of the session and essentially we boiled it down to this is security is one part of an integrated whole. Um, it's not that, you know, security is unto itself for us at S3, uh, and when we say one part of a whole, well, the question is what are the other parts of this whole. Um, and this is what we talk about internally, right? The real value is how security works with the other three pillars, um, durability, availability, performance, and how they enable customers to meet their needs. Um, and then moving on to like what these needs are, working backwards from customer requirements, these are the things that we think about, right? Customers function on AWS on S3. They are focused on ensuring continuous business access to data. Um, they, they need, you know, their users or their end customers to have access to data at all times. Um, we're looking at ways in which we minimize and mitigate risks to the business and then we think about delivering business value efficiently, um, so when we think about security it's all about how does security contribute to these things, right? It's not we want it to be secure just because that's a cool thing to be, although it is very cool to be secure, but, uh, we wanna help serve these business needs. Um, and so. Which is what it comes down to at S3 we think about security as being everybody's job. Uh, this is the whole AWS shared security model. AWS is responsible for security of the cloud. Customers are responsible for security in the cloud. And so at S3, as designers, uh, it's, I'm a product manager. I work on designing products. It's my job to think about security even on features that might not be necessarily security related. We think about security as a lateral function that functions across all of the products that we work and launch. Um, operators at S3, um, each team owns its own security model, and then there's a central security team, uh, that partners with them. Brand, do you wanna talk about that a little bit more? Yeah, so when we talk about like, uh, it being everybody's job, I might be working on a performance optimization in S3 or I'm working on a brand new feature with new APIs, but as I think about those things, I need to also be thinking about security. And so we've got a centralized security team in S3 that helps those teams to to focus on the right things and to concentrate on those things, uh, because there's that pressure to think, you know, I'm gonna stay in my lane, I'm gonna do performance or I'm gonna work on availability improvements and not to be thinking about how that impacts security. Uh, we really need to think of it as like one integrated whole, like the whole product is trying to solve a problem for customers, right? And then the last one is customers. All of you guys here, uh, we think security is your job as well. Uh, we build the tools, we build tools essentially for any job. We think about S3 as the bottom total. If it's turtles all the way down, the only thing lower is like desk, I guess, like, but. Uh, and so the hardware, it's just the hardware, right? Like it's hardware S3, and then everybody builds on top, uh, and so we think about that and it's part of like our best practices today we'll talk about 8 different things that we've seen customers do, uh, other customers do that's helped them stay safe in the cloud and secure in the cloud, uh, and also things we just generally recommend that customers do, um, yeah, and so. The last piece maybe I wanna talk about is continuous iterative improvement is a must. This is something that we take extremely seriously at S3. Um, so this continuous improvement essentially works in two different ways, right? One is how do we launch new features based on customer feedback, uh, to make S3 easier to use, uh, and help customers stay safe on the cloud. Second is how do we simplify security. Um, so some examples of that over the last couple of years, uh, the top two on the left, uh, S3 block public access, uh, enabled by default. ACL is disabled by default. Uh, this is actually the first launch that Bryant and I worked on, uh, when I came to S3. Uh, it's, it's a remarkable change. You create new buckets and block public access is turned on. Most customers don't use S3 for public access. Most people don't turn it off. They just stay secure by default. Um, all new objects are encrypted. Uh, you cannot land unencrypted objects into S3. Um, even if you send us plain text bytes, we encrypt the data as the first step before we store the objects in storage. Um, check sims are now enabled by default. That was one of the things I worked on last year. Uh, it helps you guarantee data durability end to end. Um, you know, since launch, S3's taken checksums at the front door and stored it all the way down to disk to storage. Now you can extend that all the way to your applications. Um, and if you upgrade your SDK, you get that for free. So everybody please upgrade your SDKs. Um, the SOAP interface, uh, is end of life, um. And then SECC, which is a niche encryption offering that very few customers use, is going to be uh disabled by default soon. Um, the announcements are out there and as you look at these iterative improvements, you see some of them are launching new things like always encrypting your data by default, right? So they're like, uh, that's one more thing you don't have to worry about. It becomes even easier. But others of the things are shutting things down, right? It's, uh, making sure that the surface area of estuary doesn't grow and grow and grow over time as we launch new features. We wanna keep it as small and focused, and, uh, effective for you to think about as possible, right? So if we, if we just always add features and never make them smaller at all, then you'll get lost in all the options. Think about, yeah, too many things to think about, right, yeah. Um And then We talk about this a lot as security isn't only about saying no, even though no is the probably the easiest word to say. Uh, You know, ships in harbor are safe, but that is not what ships are built for. It's the same thing with S3. Disconnected data is safe. That is not what S3 is built for. S3 originally launched as, uh, internet facing storage. It is meant to be used. It is meant for people to interact with their data, make use of the data, um, and so, uh, this is what we think about when we think about security. As well, it's not just about denying access to everybody or denying access to the wrong users. It's about building those paths to get the right users access to the right data industry, and this ties in with those earlier tenants as well. When we think about security being everybody's job, we don't want it to be security people saying no and other people trying to push past security as an obstacle, right, as a as a. Organization security needs to be about delivering new features and offering new capabilities to customers and providing it as efficiently as we can, right? Yeah, cool, so I'm gonna hand off to Brian, talk about some best practices. Yeah, we'll talk about like 8 specific things that we strongly recommend that our customers do that all of you take away from this today. Uh, and as we do this, we're gonna try to keep in mind those tenants, you know, we're gonna keep in mind that security is just part of a broader product. It's not about saying no, and that we need to iteratively improve all the time. So the first of the best practices is some really basic stuff that you should all be doing. You should all be blocking public access to your data. It's just table stakes. Uh, S3 launched Blocked Public Access more than 5 years ago now. Uh, Blocked Public access, uh, examines the policies that you write, uh, and ensures that you never write so broad a policy that your data is publicly accessible. Uh, that, uh, analysis is done by an automated reasoning, uh, library we use internally. Uh, and it, it protects you from inadvertently misconfiguring your data, right? When you've got a developer who's just trying to get something done, the easy button is to say allow all, right? Uh, and block public access prevents you from tripping over that, you know, not having those, uh, aligned incentives. Uh, blocked public access makes sure that it never sneaks in. BPA was turned on by default for all general purpose buckets a few years ago when you create a new bucket now. It is already enabled for blocked public access. Our new S3 resources that we've launched recently, whether that's uh tables or table buckets, vectors and vector indices, um, these new S3 resources have BPA intrinsically turned on. Like we just don't allow you to ever even turn that off. The one use case we see people trying to use public access for is for web hosting. Uh, Estuary was Internet facing storage when it launched, uh, but we strongly recommend that you go to Cloudfront for all of your web hosting needs. Now it's, uh, it's just a better option. It provides you with, uh, TLS support. Uh, it offers you, uh. You know, arbitrary domain names rather than, you know, trying to route everything get subdomain better caching and so, you know, your, your end users get better performance like the list goes on and on, right? So turn on block public access, use OAC with Cloudfront, uh, that's, that's the way to give only cloudfront access to your bucket, right, right? Yeah, yeah, you, you can use, uh, Amplify even right as the easy button for hosting a static website. On top of S3, your S3 bucket stays private, but it's the origin of a public cloud front distribution. Yeah, and a quick plug here if you're trying to make if you go to the S3 console and you go and, you know, try and start up like I set up a website, there's actually a one click button that takes you to amplify that shortens the process, right? So you don't even have to work through all of the steps required to set up a CDN distribution like all of these things you can just click the button and go. OK, best practice number 2 is turn on bucket level keys. Bucket level keys is a great cost saving feature that we've recently launched. And again, we're all about delivering business value efficiently. Um, we wanna make sure that this doesn't compromise security in any way, but that it helps you do things with less cost and easier to use. And so, uh, when you want to use your own keys with S3's encryption options, you do that with KMS with our key management service, and that means that every time that you put or get an object, S3 needs to on your behalf, go talk to KMS and uh use the key that you have managed there with your objects. If you have millions or billions or trillions of objects in your S3 storage, that request volume can really add up. And so bucket level keys allows you to tell S3 that we're allowed to use an intermediate key, a key that will be used to encrypt the data keys used in our envelope encryption on our buckets, um, but that is going to be decrypted using KMS and then cached securely temporarily, uh, so that we don't have to go to KMS every single time. Uh, it is optional on general purpose buckets. Uh, we do know that there are compliance use cases that require that you use that KMS access as a backstop and a second level of authorization, but for everybody else. This is an easy button to click. You just push that button in the console and you're gonna dramatically reduce your KMS volume. And since launch, customers have saved $400 million in AWS KMS costs just by turning on bucket case. So yeah, $400 million is not a small amount of money to save uh requests, and this is outside of their, their S3 request, uh, costs as well, so. That's a, that's a great way to save a lot of money. Uh, this is automatically turned on on Sary tables, uh, so there's, there's no setting required. It's already, already in place for you, yeah. So these first two that we talked about are everyone should do it, right? Everyone should turn on bucket keys, everyone should block public access. Uh, this next one is a little bit more nuanced and it's about how you scale. Uh, when we talk about a big problem like millions of users or thousands of applications or billions of end users, if you're, if you're a big, uh, application or social media property. Yeah, how, how do I solve that problem? How do I manage so many different, uh, pieces, uh, and we think that the right way to do this is to divide and conquer, to eat that, uh, problem a little bit at a time, um, that's not only trying to, uh, keep your applications separate, right, or your production stages separate. Um, architecturally, but it's also about making sure that organizationally you divide the responsibilities for management of your bucket efficiently between teams. You might have a, a centralized security team that's more important, uh. When they are enforcing the best practices or enforcing the guard rails as we talked about it in AWS, uh, for your whole enterprise, you know, they're, they're gonna be the gatekeepers, um, but then you're gonna have other teams who are just trying to get stuff done, you know, they're trying to launch an application they need to create a new S3 bucket. Who has to approve my policy, right? Uh, and we don't want that to be a bottleneck for your developers. We want you to be able to let your, your users do their thing, but also maintain the security of a centrally managed set of guardrails. And so we're gonna talk about a couple of different options here today, 4 different options for dividing the responsibilities between, you know, the guardrails and the enforcement of policy and, you know, more of the, uh, the enabling and the agile development. Uh, the first one we're going to talk about today is S3 access points. Access points are, uh, use case specific endpoints that you can provision in front of an S3 bucket. You can have many different access points all pointed at the same bucket. Each one of those endpoints gets its own access control policy. Uh, these are layered on on top of the S3 bucket policy, and so you would then divide the responsibilities here. We're talking about, you know, how do you divide and conquer the problem. You put the guardrails in your bucket policy. Nobody should ever access this bucket from outside of my VPC, right? And then your access points can individually enable every new application that comes along. Maybe I need one, read-only access point for my application and one read-write access point for my auditors, right? Uh, maybe I've got a development access point that can only talk to the partition of my bucket or the prefix within my bucket that's for ephemeral storage and not to the customer data. Access points, uh, allow you to modularize your bucket policy. And allow you to then deploy applications independently without touching a shared policy resource and they allow you to scale organizationally by splitting up the responsibility for the buckets and the storage and the and the primitives of how you're storing data from individual use cases where you're enabling access. Yes, and the scales really well, right? We see customers put tens of thousands of access points in front of single buckets and then lend them out specifically for different applications and teams. Access points scale up to 100,000 per, uh, customer account per region, uh, and that's a soft limit if you need to talk to us about a larger use case. But, uh, access points, well they can scale up really broadly or maybe not the best solution if you are thinking about individual users having access to individual files, right? If we're, if I'm a large enterprise with, you know, 1000 developers, 10 million customers, some specific PDF that they can see or some specific spreadsheet they need access to, uh, you don't wanna manage, you know, the cross product of all those files and all those users as access points. That's why we launched S3 Access grants. Uh, Access grants, uh, allow you to, uh, use either IM users or users federated in from a corporate directory to, uh, get access to specific S3 files or prefixes or buckets. Uh, they're stored in a, a list of grants. Uh, it's very easy to programmatically add and remove grants from that list without affecting anyone else's access. Uh, and, uh, the way this works is that your, uh, application is going to talk to access grants, request access for a specific S3 bucket or prefix, and then it gets a temporary credential that's beened by our AWS STS or Security Token Service. Uh, that temporary credential will then be used for the S3 access, and we've got. Uh, some magic sprinkled into our AWS SDK so it will juggle all of these, uh, credentials for you if you need to access hundreds of different prefixes or thousands of different files as it needs to, it'll, it'll on demand fetch the credentials it needs to make those requests so you can make that access like opaque to your application. Like your application doesn't have to think about it if you use the SDK. I think one key point to note here is like how powerful. The corporate directory is right, it just shows up as like one part of the design, but when, when we talk about building like bringing in federated users and groups, the you could have say a prefix called finance that's mapped to your in. IDP group called finance and so you know I joined the finance team I automatically now get access to the files I need right? I leave the finance team I lose access to the files I don't need to have and so it sort of goes back to the separation of responsibilities I think that you were talking about Brian, right, because the guardrails live on the bucket you've got your bucket policy that prevents any public access that prevents unencrypted access, right, prevents access maybe from outside your, your org's VPCs, um, but then. You don't have to stand in the way of individual users granting access to files or adding new grants. In fact, this is the foundation of our storage browser product which gives you a sort of a non-technical, you know, file browser kind of an interface on top of S3, and it's going to show you only the things that you can see because it has this list of grants to pull from. It doesn't have to evaluate your bucket policy every time. Uh, S3 access grants are a special case of a broader pattern. We call this pattern the token vending machine. Uh, token vending machine hands out on-demand temporary credentials that are tightly scoped to a specific S3 bucket and a specific S3 table or a specific vector store. Uh, these credentials are acquired from STS or a security token service. And we do that via a a scoping policy so you uh take a role as your foundation, the role that has access to those buckets, to those tables, uh, and then you add a scoping policy to that temporary session that restricts it to only a subset of that resource, um, you know, a specific object, a specific column, uh, these, uh, custom sessions that you create. Could be based on any kind of authorization you want, and this is the power of the token vending machine concept is that it's arbitrary logic there uh when I want to only allow Bob from accounting to have access to a file, I can do that with SEF access grants, but if I want him to only have access to a file on alternating Tuesdays if the sun is high, the sun is high, and if it's a certain time in the afternoon and only if I've recently like turned on, you know, an audit and he's in the office. Yeah, all, all of those data sources, whether they're directories, uh, corporate databases, policy files, uh, you're gonna use your own arbitrary logic that might be a lambda function or an EC2 instance or EKS container, um, you're gonna process all that, make a decision, and then just spend a temporary credential that embodies that decision, a temporary grant of access. So that's token vending machines as a concept. We've got one more way to, to divide these responsibilities, and that is attribute-based access control, AKA ABAC. Uh, you may have heard this also called tag-based access control. Uh, when you do tag-based access control rather than basing your access control decisions on the names of the users or the resources involved, you use tags or metadata that's been associated with those principles and resources. So rather than writing a policy, uh, that's, that says, uh, Bob, user has access to this S3 bucket, uh, I would say users like Bob who are on the accounting team have access to. Buckets that are tagged as accounting, right? It makes my policies more semantically meaningful and it means that if I am a uh centralized compliance team, so somebody who's writing these security policies, I can write those policies once. I can say accounting should have access to the accounting stuff and marketing should have access to the marketing stuff, and my mobile users should have access to their own data, right? And then never touch that policy again. Because I've devolved the responsibility for tagging individual users as developers or as marketing guys or as end users to another piece of the system, right? So one person owns the tagging, another person owns the policy authoring, um. So we're super pleased to announce, uh, that just a few days ago we added ABAC support to S3. Uh, this is a huge deal for us. We've been multiple years in the making. Uh, we are now supporting the standardized AWS tago API for bucket tags. So where previously you would have used the uh put bucket tagging API to uh replace all of the bucket's tags with a single API call, you now have granular tag and untag resource permissions on individual tags being added to buckets. These are the same tags that use today for cost allocation, so you don't have to to re-tag your buckets if you've already carefully tagged them, but they're now available for access control as you opt buckets in. Uh, tables and vectors now also support, uh, AEC. So, uh, if you want to start today with a policy on a user that has access to only resources tag development, you can go ahead and tag your tables, tag your, uh, table buckets, your vectors, uh, your, uh, S3 buckets now with that tag, and then write a policy that means something. To tie those things together to the organization, I think the key piece here is that this works across AWS services, right? So this like using tags to bring semantic information from your world into AWS and make it easy to reason about, um, you can do that with STS with all of our bucket types, uh, you can also do that with all of the other services and just have a simple model that everybody can think and reason about within your organizations and this is. Coming back to that tenant about making sure that we enable you to deliver things efficiently, like, this is yet another feature launching in S3, but because it's a standardized feature across other AWS services, it's actually a simplifier, right? It makes it easier to think about all your resources in the same way. Uh, this isn't an S3 specific thing at all. Uh, one S-a specific thing about this though is that when you, uh, opt into a bucket supporting ABAC, you're gonna be opting yourself out of that old bucket tagging API. Uh, it's just fundamentally incompatible with the tag resource permissions and so you really need to, uh, take this as a staged thing. You're gonna start using the new API. You're gonna check to make sure that all your tags are the way you want them to be, and then you will opt into ABAC, which opts you out of those older APIs. So Four different ways to divide and conquer. We talked about access points, access grants, token vending machines, and ABC. You know, this can sound like a lot, right? It's a lot of acronyms, uh, a lot of saying access over and over again. Access points and access grants and access control lists, uh, and people say, hey, isn't this supposed to be the simple storage service? Uh, and the answer to this is that the right tool for the job changes as you scale up. Uh, well, it might be great when you start with just a simple application, uh, you know, stereotypically you'd be in a garage hacking away on something, you know, you've got a bucket and a policy, and life is good, and you don't have to do more than that. If that's the scale of your application, we've got a simple answer for you, right? But if I'm talking about millions of users accessing trillions of objects stored in S3. A file that listed who has access to everything would be horrible, right? That'd be the worst possible experience trying to manage everything in one big policy like that. And so we are trying to provide you with the right tools, trying to meet you wherever you're at on that scaling curve. You might be a, a tiny little, you know. Baby's first application all the way up to, you know, a huge enterprise deployment and we're trying to provide you with tools to meet wherever you are. The analogy I used, I like to use is like S3 has pickaxes. S3 also has earth movers, right? And so like depending on the stones that you want to move, you either choose a pickaxe, use an earth mover, and there's everything in the middle. It's about finding the right one to use. It'd be hard to, to. To to hoe a garden with an earth earth mover, but it'd be awfully hard to build a, a giant building if you only had a pickaxe. All right, we're gonna move on to our next best practice, and this is that you should always test your security changes on a model. Uh, this is probably the number one thing I've seen as a development practice that customers struggle with. Um, so often, uh, somebody says, hey, I made a change to my bucket policy. And now my application doesn't work and my users don't have access. Everybody's getting page. Things are going wrong. Yes, so, uh, we strongly recommend that you do a model of your production system. This is easier than ever with infrastructure as code tools like AWS cloud formation. There are plenty of third party tools as well. You wanna just set up a copy of everything that's in your production environment, all the same buckets, all the same replication and life cycle rules, all of the, uh, same. EC2, you know, deployments doesn't have to be all the same data, right? Like it's not about duplicative copies, it's just having the same structure that you have. No, no production data touches this test stack, but it's set up with all the same controls, and then you want to make a tweak, you change your policy in that test stack. You run all your production work flows through it. Does it work, right? You also test the negative tape test cases. What shouldn't work. Uh, we've seen customers who say I changed my policy. Uh, and it broke everything, and so I quickly changed it back to an allow star and everything went back to working. So now I'm good, right? It's like, no, no, no, no, no, like you messed, you missed the part where you get it back to its original state where it didn't allow the things that shouldn't happen. So then once you've tested your positive and your negative test cases. The secret sauce here is that you keep those test cases and you run them every single time that you make a change. Uh, whenever I decide I'm going to add a new bucket, I rerun all the test cases whenever I decide my production environment is going to have a slightly different access grants set up. Run all the previous test cases, uh, and this corpus of, of test data is going to give you confidence that you are deploying into production with something that's going to work, right? It's, it's never going to be a matter of causing customer impact and then rolling that back quickly. Uh, you can stop holding your breath every time you deploy. Like, like for what it's worth, this is exactly how we do it in S3 in a service as large as S3. Uh, our developers write tens of thousands of test cases. There's a running bank of them, and every new code change has to go through all of the test cases before it's deployed. Uh, we also use, uh, some of our own internally built automated reasoning tools, uh, that. Continuously tests against the stack, um, and they have expected behavior. It doesn't always have to be positive. Sometimes you have negative test cases and it checks for the expected behavior and we've done both of these to run a service like S3. That's the best practice across the board. Everybody should be setting up a test stack for all their, their deployments to make sure that everything works and, and it's really. Like we think about it as a cost because now I have to go and do all that set up over again, but it's really not a cost it's an enabling factor it's a way to make things move faster uh it's a way for security to say yes we have confidence in your change rather than saying you know no we don't know if this is gonna work or not. All right. Best practice number 5. This is about our AOBS organization's product. Uh, AOS Organizations is a, uh, way to model your, uh, multi-account deployment with, uh, explicit organizational units and organizations. Uh, your organization as a whole is gonna contain multiple organizational units which can be nested. Uh, each of those organizational units contains one or more accounts. You've got an administrator account with special privileges that lives outside of any of those. And at each level in this tree of organizational units you have resource control policies which apply to every resource owned by any account in that organizational unit and service control policies which apply to every principle in every account within that organizational unit. Organizations are a great way to organize your accounts, uh, but they are also not just, uh, you know, for keeping things in tidy boxes they're for this separation of concerns when we talked about scaling previously you wanna divide and conquer. Well, we recommend that you put your. Uh, guardrails, your most important enterprise-wide guardrails into a resource control policy, right? Anyone in my enterprise, uh, that creates a new bucket is gonna have a resource control policy that prohibits unencrypted access to that bucket. Uh, it's gonna prohibit access from outside of my organization. Uh, maybe I've got a specific OU, uh, organizational unit that's for, you know, my web hosting, and it's got some special, you know, provisioning to allow it to still have a public cloud front distribution. But the whole rest of my organization lives in an OU where that's prevented. These guardrails then allow your developers the creative freedom to build whatever they need to, right? They can set up their accounts however they like. Doesn't matter what they do to their bucket policies, to their table policies, to their vector policies, you know that they're gonna be compliant with the org-wide, uh, guardrails that you've set of the resource control policies. It's also like a great ease of life. Use like enhancement, right? Imagine you have we now allow a million buckets, like general purpose buckets in an account. You have to go apply the exact same set of policy statements to each of these buckets. Just do it once at the organization level and then you know you can forget about it. We, we see customers with thousands of accounts. Who are, uh, running, you know, tens of thousands of buckets, and every one of those buckets has a boiler snippet that's been copied and pasted by their infrastructure as code, uh, and they hope that those policies never are mutated to remove that important piece uh with resource control policies you just don't have to worry about that. Another thing organizations can do is they can provide you with an easier debugging. S3 earlier this year launched the ability to get more informative error messages whenever you have an access denied problem, and the caller and the resource owner are within the same organization. So rather than just saying access denied, you know, opaque error message, I will leak no information about your S3 deployment. You get instead uh something more helpful it's gonna help you to pinpoint exactly which policy it was that you know failed to allow you access or that explicitly denied you access. This is the same implicit better error messages that you've been able to get within an account for a while now, but now within an organization you also get those more informative error messages. And we're also very pleased that just uh after Thanksgiving here we launched a new organizational policy for S3. Uh, organizational policy applies to all the resources in an OU. Uh, and it now allows you to set block public access controls at that OU level because this overrides any, uh, account level settings that you have in those accounts. So if I turn on block public access at the top of my org, it's going to apply to all of the buckets owned by all of the accounts and all of the OUs in all of my organization. So with one single API call you just never have to worry about public access being a thing in your enterprise ever again and you don't also have to worry about somebody mistakenly or out of frustration changing it at the account level because you have the administrator account. Those are the keys to the kingdom when it comes to BPA in this case only the administrator account who's ever gonna touch this setting. Cool. Go ahead. Just give us some more best practices. Uh, gonna move on to the next one, which is extend S3 security beyond S3. This might sound a little convoluted, uh, but, uh, we, we want customers to think about security the same way we do internally, uh, and so there's. Three things to think about that, right? Since launch when data lands at S3's front door, uh, we check some of the data down to storage. Uh, we use those text forms to, uh, validate the data every time we pull it out of storage. Our recommendation is extend that durability all the way to your applications, um. And it doesn't have to be only when you're talking to S3. Like a simple example would be, you know, you, you, you're a studio, you have cameras, recording equipment, so I can see that, uh, take checksums of the data as soon as the data is generated, uh, right, and send the checksums along at every step of the process. And on the way to S3 we can validate the checksums before accepting the data, uh, you. Packets with this data flowing all the way across the world. There's a bit flip in the server in the Bahamas, right? Your data could be affected, but using checksums is a way to guarantee that that doesn't happen during transit. Uh, quick plug here for the work we did last year. If you upgrade your SDKs and you use the AWS SDKs, they will calculate a checksum for your data automatically, so. My, uh, recommendation or suggestion is to extend that all the way to your applications to where the data is generated, but at the minimum use the upgraded SDKs that'll calculate the checksum so that at least when you start the upload process to S3, your data is protected. And I know our solutions architects at S3 have actually already built some tooling that will take those, those returned checksums if you keep hold of them and, and, and store them longer term on your client side. You can then provide those to tooling that will verify the authenticity and the integrity of the objects in your bucket. So without ever having to go grab all the objects or look at all the data, you're just checking to make sure that those checksums match, you know, years, decades later, uh, you're able to make sure that it's exactly the same data. It's because you, you, uh, are being a part of this, right? Like it's all one integrated whole and security is everybody's job. It can be partly your job as well, like as you build check sums into your application. Is going to uh make everything better, not just your S3 interface, but also your your local access to that data and we spent the last couple of years really expanding the list of algorithms we support. We went from supporting only MD5 to CRC 30. 2 32C1 Charter 56 we added CRC 64 last year. So there's a wide array of options based on wherever you are on your journey. If you need something really, really fast and performant, you can use CRC 64. Uh, if you need a secure algorithm, use Shadow 56, best of the line. Um, so whichever it is that you want, we validate for free and we store the checksums with your objects. Um, the second is on the encryption side. Uh, the same encryption that you want to do in S3, we recommend that you do at your own end, uh, and so that goes down to, you know, think of if you're using KMS, uh, key management service, uh, customer managed keys on S3, there is probably a compliance or a regulatory requirement that requires you to do so. Um, you can use the same keys to encrypt data in your local systems, right? It, I don't know, it doesn't make sense to me that you would have a data encrypted on S3, but not on the local client site systems and. So the AWS, uh, the S3 encryption client is a great way to, uh, use KMS keys to encrypt your, your data. You can do that client side and keep your encrypted copy locally, but then also have that same encrypted copy to S3, right? And the last one is. Always use encrypted communications when you talk to S3. Use TLS and if you use VPCs, don't transit the public internet, use private VPCs. Um, we use secure communications between all of the things within S3, uh, and so. We would hope we want, we recommend that everybody talks to S3 and your data is encrypted in transit. It's just a basic protection that you can, you know, have across the board, right? Just do the same thing externally that we do internally in S3 and keep your data safe, yeah, yeah. Next one, Turn on logging to enable reactor security. And the biggest ask that we hear from customers that we cannot meet is, can you give me logs when I haven't turned on logging? And it's really, really hard because we've tried to invent time travel, but it's not really working out, right? Yeah. We have on prime ships by yesterday, exactly, uh, and so turn on logging proactively so that you can react to security, uh, things that happen within your organization. Uh, two options you've got AWS Cloud Trail, uh, supports logging from across AWS, um, really easy to use event filters, cloud watch, uh, ingestion. You can do, uh, dashboarding there. There's a bunch of things options. On the other hand, you have S3 server access logs which launched when S3 launched, S3 specific activity logs, uh, can be useful when your buckets are really noisy, right? Because you only pay for the storage, uh, not for the cost of generating the logs themselves, um. And I, I cannot say this enough. Like reactive security is more than just logging. The typical use case we hear about is like a forensic audit. So, you know, I think something has gone wrong. I need to go figure out who did what action against which resource. That's the usual like why customers think about logging. But there's also like anomaly and drift detection. You get enough scale, uh, you have, you know, tens of thousands of buckets, trillions of objects, different, you know, hundreds of users, uh, all of these things can get really unwieldy to manage, um, and the only way to detect the anomaly in drift detection is to look at the logs and figure it out, right? That's one of the ways to do it, um. Yeah it's uh you know it's hard to to emphasize this enough like so many times when we think about security we think about you know access control and we think about encryption those are preventative controls, but you need that reactive uh control as well you need to treat security not just as an afterthought where oh something has happened but I didn't have logging turned on. It needs to be there, you know, from, from day one you've got an audit story, yeah, and then automated instant remediation is what we talked to a lot of customers about more recently, um, especially when your organizations get really large, um, you can set up rules in advance that act based on your logs, um, so a simple example is somebody turned on BPA this action, this thing can go turn it back on, right? So like. BPO is turned off. It's immediately turned back on. Um, we've linked a blog post that, uh, we really like. We worked with, uh, some of our solution architects to get it out there, uh, but like the logic is similar, which is you have some sort of an event lambda fires that takes some other action and goes fix it, uh, and that, that's something that you can do across a bunch of use cases. It doesn't have to be this is a lot broader than just S3 logging, right? You've got a bunch of different services that do monitoring of your configuration, monitoring of your traffic patterns, right? Yeah, we do, you know, we've got AWS config, AWS control tower, uh, config has rules that you can deploy and you can check against them. Uh, control tower helps you create safe environments, uh, by automatically creating resources with the right configurations, guard duty for detection monitoring security hub. Um, that you know, brings all of these findings together. So as we were talking about like tools for the job, um, lots of tools for the job, uh, it's about like choosing the right one depending on your skill, but ultimately all of these rely on turning on logging, right? Like you need the events there to, to have the guard duty findings, to have security hub alert you to the problems, to have config notice things you need to, to turn on logging on your buckets. OK, cool. And then the last best practice is plan in advance for durability and recovery. Uh, I don't want to make the same, uh, time machine joke again, but a similar concept applies here, which is you can only react if you've planned in advance, uh, and you've taken action up front. Um, I'm gonna talk through some of the options we offer. Uh, the way to think about this is, you know, on your extreme left, you have S3 conditional rights, which is like something we launched last year, right? Um. Very simple to use, uh, and simple to enforce, uh, it's essentially you can set it up in a way where you can only put the object if it's not present to prevent overwrites of your data, uh, and you can also enforce that using the bucket policy. You can use bucket policy to enforce that all rights on your bucket use those conditions and are, are never gonna accidentally overwrite something, right? So it's a simple way to ensure that your data is not overwritten, right? So if you have downstream. Applications that rely on this data in a bucket it's not ephemeral storage like it always needs to be used. Uh, that's one of the ways of thinking about it. Um, next we go 3 object versioning. Uh, essentially you can create a version stack so every right that comes in doesn't overwrite your data but just adds a version on top. All of the versions remain accessible. Uh, you use the version ID to make your request, so your applications continue to function while you accept new data. That lands on top of the same key, but Aksha, yeah, what if someone were to delete one of those object versions that I wanted to keep? OK, so that's the next one. so the S3 object lock. Uh, you've got compliance mode. Uh, you've got governance mode, lots of options there, uh, you know, you can set up rules for how much time an object cannot be deleted, uh, or you could say can't be deleted till this hold is taken off, uh, and you can set that up. The next one is S3 replication. Um, typical use cases of how people think about replication is, oh, I need my data in another region because, you know, my computer's there, all my users are there. I would rather pay for the move once rather than use it there, but it's also a good way to think about disaster recovery. Right, it's not, it's not just latency improvements or something. It's also having a second copy of your data, exactly. And then the last one is AWS backup, right? So this is like for your most business critical information, uh, things that you consider essential for your business to function. Back it up, right? Create a full copy, have that ready to go. Um, AWS backup is, uh, a native service that does a lot of these things, um, also provides you with RPO, like all of these things that, um, you would care about if you were to create a full second copy of your data. Yeah, they say one is none, right? So for anything that's gonna be business critical, anything that you can't, uh, replicate. Uh, you know, this is not for like every ephemeral bit of, of training data that comes out of your EC2 clusters, but it is super important for that long term, you know, financial data for that long term user data that needs to be retained indefinitely. Yep, cool. So these are, uh, 8 of best practices. Again, you know, the first two block public access to your data and turn on bucket keys are just things every single AWS developer should do. Uh, some of these others are more nuanced. You gotta think about exactly which of the approaches matches your scale when it comes to dividing and conquering scaling problems. Everyone should be testing. You can use AWS organizations to, uh, simplify things if you've got a multi-account set up. Uh, Aksha talked at length about the, the checksums that you can do and the, uh, local encryption and the local, you know, TLS connections that you can use to extend S3 security out into your application. Turn on logging, like you can't turn it on yesterday, but you can turn it on today, right? You can have that audit record from now and you need to have a plan for, for your durability and recovery. So let's talk about like the specific takeaways from today, uh. If I am a developer, right? I'm an individual contributor. I'm just, just a regular old engineer, which is what I am, by the way, I'm just a regular engineer. Uh, what I need to take away today is that I should be switching to the new tagging APIs right away. Uh, put bucket tagging still exists, right? We're not turning anything off today. Uh, but those new tag resource and untagged resource APIs are AWS standard, uh, and they work now on buckets. You should be doing tagging the right way and so start today and get ahead of that, uh, inevitable shift as we move to the new tagging APIs, uh. Can't emphasize it enough, we've said it already. Turn on logging on your critical buckets today, and you can, you can do that in a click, right, on the console, uh, logging. Doesn't feel like it's necessary until suddenly it is necessary and it's necessary that you already did it. So I turn that on today and then last of all, you know, really think about this concept of turning on checksums in your application, right, right where you uh create your data, having a check sum from the very beginning to ensure that object's uh integrity and durability, uh, and this is, you know, partly a mindset thing as well, uh, you know, check sums are a great technical way to ensure integrity. But making sure you always create checksums where you create your data is part of getting your whole organization to shift to a mindset like S3 where we think about the durability of data as we're creating it. If I've got a camera or an IOT device or a user application taking submissions, right, these are all things that I should be worried about from the beginning. What's the durability of this data? How am I gonna make sure that it is safe, uh, and part of the, the journey to thinking about that all the time. Is check something from the beginning, check some early, check some often, uh, and make sure that those match system-wide at the minimum upgraded SDKs, at least let the SDKs do it automatically for you, right? Yeah. Uh, and then leaders in the audience, uh, if you're a CISO or you're part of a team that manages security for the organization, first is please enforce BPA at the organization level. Can't recommend this enough. Uh, it's not that people in the organization want to do the right, the wrong thing, but, uh, you know, you want to set up guardrails in a way that it's very, very hard, if not impossible to do the wrong thing, right? So. Enforce BPA at the organization level. Second is push teams towards using ABAC. Um, attribute based access control is great, uh, because it works across AWS. Um, you could also take all of the more, uh, you know, the other options of using the token vending machine, etc. but, uh, ABAC is a great way to bring context from your organization into AWS. and use that for access control and ABAC is great because it, it scales, right? Like, uh, tagging and like just making sure the tags match can be a useful access control paradigm when you are, you know, at 2 buckets and 3 developers, right, but it scales all the way up to millions of users and trillions of objects. So, uh, it's one of those things you can, you can learn those skills early on and then keep with you as your organization grows. And then the last one is allocate resources for tax stacks and audits. Uh, this might seem like an expense and everybody's short on headcount, but it's not an expense, it's an investment. Uh, it's about, you know, starting early as you're building your platform because then as you scale, uh, it's an investment that keeps paying dividends with the problems that do not happen because you've invested in these, yeah. All right, that's the last of our best practices today, uh, as a, not a chalk talk or anything, but a breakout session. We don't have a Q&A section, but we're happy to meet with you outside the room here after we finish our presentation. Akshat and I have a long history of. Having wonderful hallway conversations that maybe are are better even than some of the sessions at Reinvent if I'm allowed to say that. So, uh, we'd love to talk to you. Uh, also we'd, uh, strongly recommend you check out some of the other sessions that are gonna be talking about S3 security here at Reinvent. Uh, I particularly like to recommend. Uh, I think it is, uh, STG 414 about our, uh, more of these best practices and sort of a hands-on, uh, on how you would implement some of these things, uh, as well as some of the durability thinking and the disaster recovery that's gonna be an STG 344. Yeah, cool. All right, thanks very much.