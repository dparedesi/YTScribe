---
video_id: GYlPFmrATjU
video_url: https://www.youtube.com/watch?v=GYlPFmrATjU
is_generated: False
is_translatable: True
---

Hello everyone, good morning. Can you hear me? Yes, it's great. That's a great start. So now that I gave you some stickers and you already like me, we can start the the our session. If you didn't receive a sticker there is a few over there, grab as many as you want please so. Over the past year, we noticed a major shift in how customers are working with LLMs, so they're no longer using these LLMs only to generate text, only to build simple solutions like a chatbot. They're expecting more. They are using the solutions to integrate with a whole ecosystem. With components that are outside the cloud, why not? Or even inside the cloud, they are working out with specialized data, so the real data that is part of their domain, so the data that will give you actual insights. So in this session we are going to talk about bedrock agents and a a sneak peek on agent score. My name is Marilia Brito. I'm from Sao Paulo, Brazil. I'm the senior technical leader of the instructors in Brazil and I hope you enjoy my session. So this is our agenda. First we are going to talk about different kinds of agents, so different kinds of configurations and talk about bedrock agents, inline agents, more agents collaboration, and I have a demo for you guys, um. Also, how to orchestrate a structured workflow using agents and components with bedrock flows, and after that, the agent core. So let's start. When I think about a genetic existence, we're not thinking about just a single component. We have a major ecosystem in some cases it's pretty similar to a microservices ecosystem. So We have here at least 4 components. These are the key components. First, the agent workflow. Your agent's going to receive an input and what the agent's going to perform after receive this input it will call an external tool, it will call um knowledge base, so it depends of the intent of our prompt. The components, so the modular parts that are component that compounds our ecosystem. Tools, models, browser, why not? To check external information using the web and also um a code interpreter for your Python functions. Sorry, we have also here the continuous evaluation framework because. Every operation that we perform, technical, not just in the cloud, but technical, we have the necessity to continue evaluate, so we have to collect metrics, which metrics? We need as longer, as long as these metrics are granular, we will be OK. And the feedback loop. To find continuous improvement, so let's check these components. First, this is pretty simple, right? I just have the user input, the exchange application, check the input and Provided any actions that are necessary and we have the response optionally we have in the right corner the human in the loop so someone to evaluate or approve the exit of the agent but this is too simple. Now the components. The first component, the foundation model that we are going to use for the reasoning loop. Which model are you going to use? It depends on your specific needs. We have more complex models that are more advanced in capabilities, and we have more simple models that are straight to the point for a specific reason. Knowledge bases. Oh, here we have the real value on Gen AI. So your own proprietary data that will be stored, why not on S3, maybe in a databases. So it's the real data that will provide you the value that you are expecting. Guard reos. We have to think about security in every single piece of our applications, so in this case we are going to to think about a feature that is capable to judge if something really worth to be uh handled by the model and if the model, how the model is going to answer the question. The APIs and tools that we're going, we are going to register so to perform the actions using maybe external APIs, memory management for context. And if you are looking to work in a collaborative way with MOC agents we can link them as well and perform different kinds of patterns so we can work with uh supervisor agents we can work with multiple agents performing different tasks in parallel so we can have the maximum throughputs um. We can have a workflow dictating to which agent we are going to send an action. There are plenty of possibilities here. And the flow. So how are we going to structure this process? Do we have a business need, a business need that needs to be uh followed so first to receive the input and check for the intent and classify and send to another agent that specializes or we have a conditional depending of a calculation. We're going to check a small demo about it. The continuous evaluation framework so the necessary metrics that we need to provide so we know how to improve even if even if it's necessary to improve our system we can use humans to check it or we can use also LLM as a judge. So human um. We have in the Bedrock console the ground truth, but the ground truth will take you to the decision maker console, OK. And Last we have the. We have to return the answer, right? That that's what we are expecting. So maybe we are using a vectorial databases, um, we are also collecting the metrics that we are the databases providing for us and then we are going to send the answer, evaluate this answer and continue the life in the continuity of the life cycle we are going to check what is what we need to improve. But This is too simple, right? So let's check how can we build this Algenic system. The first option, the bedrock agents. Well, here you can look for a single agent, a multi-agent collaboration. We can choose the model. We will provide the instructions, knowledge bases, and actions that are going to be performed using a little bit of function. It's very complete, a very complete ecosystem, and it's everything is managed, but. This is the case where we are looking for a built-in solution. We don't need a higher degree of customization. The next one, the in-line agents. Here the configuration is dynamic, so in every single invocation, we are going to change the parameters. Um, the model, the instructions, the knowledge bases of the data that we're going to use to solve a specific case, and the actions with the external APIs. That's a very nice approach when we are thinking about experimentation. Or we have a specific case where we really need to change this based on um based on something that will be provided in every invocation, so the body of the, the solicitation. And also the bedrock agent core. The agent score will provide you a modular ecosystem where you can think about um the the execution environment separately of the memory that it's separately of the gateway that is going to route the requisitions that is also separate of the control of the identities, but we can merge these elements to build our agenttic system. So in this case, we are going to work with different frameworks in, and whatever model it fits us best, even if the model is not inside the AWS. So let's check every single of these agent possibilities. So first, the manager, the manager solution, the bedrock agents. This is pretty similar to what we checked it before, but. Let's see. First, we have the customer prompt, right? So, the instructions that will be provided to the agent. The agent will check the instructions, decompose the task in several steps. And execute the logic. So how tools call along with the function, access data in a knowledge basis. After that, it will observe the results. Check if that information is enough to return as a valuable answer and after that, if it, if it finishes the loop, it will send the results for the customer. So, down here, we can check out the components that we are using in this ecosystem. So the foundation model, we are going to use a foundation model to work with gentic systems and the action group. The action group we are going to check next, so how to structure the the template and how it works. Also we have the knowledge base to perform the rag orchestration. I'm not going to detail this because I think that at this point everybody know, does everybody know about reg? Perfect. So retrieve augmentation and generate. So we are taking the data from a specific place that we determine. And after that, we will incorporating in the prompt that's going to be sent to the model. So we have the information that is truly valuable for us our specific needs. The code interpreter. We have sometimes we have to execute codes and the LLM itself won't provide this code with the quality that we expected. So when we have a different model to provide this kind of execution, so I for a specific need, it's better. And the memory uh configuration. We are going to talk about the memory. So here we have a banking assistant. Now it's better because we have the elements exactly where we expected in the architecture. But notice something. Before we can have an interaction with the agent, there's someone there that is not going to give you a free pass. So first we have the guard rail. In our banking assist app, we have um a generic assistant agent that we're going to reach, but before reach reach the guard rail, we'll check, we'll filter the prompt. So if it's OK. If it um if it's according our specific directions, then the agent will be. Call it, so the instructions will be ready and the actions are going to be performed. So internal actions using a Lambda function that can perform some kind of calculation. Why not? This is the example that we are going to check, but also we can use this Lambda function to call external APIs, OK. In the number 5 over there on the bottom, we have the knowledge bases and notice about the rag. So first we have the S3 bucket with the documents that we are going to provide for the model, but the model is not, the agent and the model are not going to search the documents in the bucket because we already established that we are going to use a victorial databases, and this is pretty much more efficient because. We will have the mathematical representation of each token. A token can be a word, a token can be um a word split in one or more or two tokens. It depends on the model that you are using. So this is populated in the open search services that we are using as a vectorial databases, and then we can provide the information using this kind of approach. But let's check the elements that we can see in this slide. First, the guard reyes. So the guard rails will be our security feature to work on bedrock. We have 3 kinds, but let's focus on 2. We have 2 kinds of configurations of the guard rails. The first one is deterministic control. So, the blue one. What is the determinisy control? We are providing words that we don't want to have in our interactions. So as a list or you can type in the console. I think a list, a list is a, a best approach, a better approach. So maybe some offensive words or maybe words that are not part of our compliance. So we will provide this. It can be directed to a necessary bucket. The probabilistic controls we are going to choose a determined threshold to say what is acceptable. So here we have categories like violence, misconduct, um. Something that is more um Then we have hate speech as well. So we don't want to receive inputs like that. We don't want to send this to our models, but why? Because there is a cost for each token on the input and the output as well, but if we can filter unnecessary solicitations in the beginning of the process, this is nice. In a banking app. There is no need to. They hate speech. Oh, sometimes the customer is not very happy with the solution that we are providing, so maybe we can set a medium threshold, right? So after that threshold, the, the acquisitions will be blocked, so the model won't receive these requisitions. There is also here, the control of the PII, so the personal identifiable information. In some case we need to receive this PII, so the name, um, the identification, the email of someone, but we don't want to retrieve this information. So we can we can choose two options over here. We can block the solicitation, so We, the customer will receive um a standard messages saying uh I'm so I'm a banking assistant and I'm here to help you. And nothing more, or we can mask this kind of data. So there is already a few categories that are ready to use for US documents in the UK, but if you are part of another community, another country as me, you can also custo customize this using rejects patterns, OK? There is one feature here that is my favorite in the probabilistic controls, the contextual grounding check. Well, we already talked about reg, right? In this case, it will control the quality of the rack, so we have a control to check if the information that was retrieved of the knowledge bases, so the chunks that were retrieved makes sense with the question. And we can establish also a threshold so if we need high accuracy, we will let the threshold in the maximum, so in high or if it's OK to have some, some information that are not very accurate, we can set to minimal, why not? It depends on your intention. And also, so, OK, the correct chunks were returned, but Basing on this, the answer that was provided makes sense. It solves the custom the customer needs. So we have these two kinds of checks in the contest or grounding check. So in this case, we are not thinking about misconduct, hate speech or something like this. We are thinking about quality of the response. Let's check now the action groups. So here we have the the agents um taking some approach to call the tools. Inside the agent, the bedrock agents, we are going to configure first the instructions of the agents, so what are we expecting of this agent. Um, in this case, uh, you're trained to provide weather information. Great. We can, we can also add here something like you will respond in a very polite, polite way. You won't answer questions about politics. It's better to control this kind of behavior in the guard reos, but you can also provide a few additional instructions in the instructions of the agent, OK. And after that, we are going to provide a little bit of functions with the logic that we are going to use for processing. So In this case, the function is already done. It's already ready. And we are going to use this template. This is an open API template and in this template, uh, there is the actions that the agent is going to perform. There is also the description of the two. And the Jason definition. OK. Even if you're working with an alum alumbida function that is going to provide the the whole execution without the need to call an external API, you have to to build your template, OK. Here is a very simple case. uh, it's a it's not about weather anymore, it's about time. I have a little bit of function that can check um the time in any place of the world, so I chose my city Sao Paulo and asked what day and time is now in Sao Paulo and it was 26th of November, so. It worked perfectly and this is in the interface. I, I made a test in the interface of the Bedrock agents, OK? OK, memory. Do you guys have a good memory? Or it is like a fish. 3 seconds after someone tells you something, you forget. Here in in this event I'm like a fish because the time zones are very different for me, but for our agents it's very important to have the memory. Why? First, because it's very annoying to repeat something every single time you're talking with someone, right? So Second, Because we need to retrieve some information to continue the conversation with quality. Well, When we are working with Bedrock agents, so in this case we have a travel assistant, so I was talking with this assistant. I want to travel to. Spain next year. Uh, can you provide me hotels up to â‚¬500 per day? Very expensive. But it's important for the agents to remember this in our next interactions so it can provide me a list with more hotels maybe uh some places that I can visit. We are going to configure the memory in 3 steps. The first step is to enable the memory. This is very important, so we are not taking every single word that was used in the past uh interactions. We are going to use a summarization, so the key points of the past interactions. After that, we have the memory duration. So it can be up to 365 days, so one year. And we're going to choose according our use case, OK? Minimum of one day. And in each session that our user is using our agent, we're going also to choose how many. Previous sessions will be loaded in that conversation. So. If this conversation changed a lot, so every time someone is speaking different items and different places, you have to understand the behavior of the interactions that you're going to have with your agent. So in this case, maybe a smaller number to load the sessions is better for a better contextualization. OK? So, 3 steps. Next we have the inline agents. So the inline agents are in the case that we need to provide the tools, provide the knowledge base, provide the model in our every single interaction. So using the invoke inline agent API we are going to configure what is necessary for this specific interaction. In this case, Even the instructions can change, so we have a helpful assistant for the Octank Inc. employees. Maybe HR assistant. We are going to choose the foundation model, in this case, the clause on the 4. Um, the action group that will provide the tools that we're going to use for processing also. We are, we are, we are saying that we're going to use the code interpreter for processing our code using Python, OK? And The input text that is going to be sent to these agents. So everything here is dynamically, OK? Now let's check the pattern of much agent collaboration. And this is pretty normal, right? This is not, not something that is, uh, new, but everything about uh Agente AI is new. But in many applications, just to work with a single agent is not enough. We need specialized agents to perform diverse tasks. In this case, I have a supervisor agent that is going to interact with 23 different agents. The first agent will perform some draft in contracts for us using a Lambda function based on information on the input. The 2nd 1 will check the existing contracts that we have. Probably in an external system. And the third one has the knowledge base maybe to to search for some juridical information, some legal information that we need for this agents. So let's check the agent's working. In my demo, I don't have the same example of the of the legal contract because I only remember I changed the demo after I submit the deck, so I'm sorry, but the logical is the same, OK. We still have the supervisor agent, but in this case, we have a customer support scenario. Our supervisor agent will receive the input, so the customer has an error on their browser. The customer is unable to use the terminal, something like that. The database is not working. And we have these specialized agents, so the intent classification agents will check the solicitation and classify. So is it urgent? Is it necessary to call the tech specialist agent, or is it a case that we have to check an internal policy to answer this customer? The tech specialist knows everything about technology, so it's able to check the error codes to provide guidance for the customer itself to fix the problem, or maybe to escalate for someone. And the policy. So, uh, we already have, we have two elements, right? So the policy oh you cannot have uh the new MacBook because of your current role. And also we have here the support case look up because maybe this support case is already open, is already active, someone is already taking care of this customer needs. The instructions for the agent, so. We can notice here that we have a flow established. We are not using bedrock flows, but we are providing the flow that we need in the instructions. So send first to classification agents, so the intent agent. After that, checking the category, you are going to er to call a specialized agent. We have another instructions over here, and they do not, so what I don't want you to perform here. There's the rest of the instruction. So how are you going to answer? Is it high? Is it like a step 5, step 4, step 3, step 1? It's dangerous. So it will check something like this, the policy guidance, if it's needed, so. So we are explaining to the agent what we are expecting in its behavior. After I configure this on my supervisor agent, I have to include the other agents, so the specialized agents. I have just an example of the intent classification agent and the configuration is pretty similar, so I have to provide the instructions. Invoke these agents when you need to determine the nature and category of a customer request and we have also in what this agent is specialized so in this case system technology issues, um eligibility questions policy and guidelines classification. Great. So I will attach. This agent inside the configuration of the supervisor agent. Um We are going to check now that we received a ticket, so there is a system. System timeout issues affecting closing pipeline. It started at 8:30 a.m. So out the description of the ticket. And now We have first the supervisor agent being invoked after the supervisor, the intent classification, so we have the exactly category that we need and we can see that it's a technical issue. So the tech specialty agent will be called to solve our problem. We can check the traces of the requisition as well. In the rationale, I have what the model thought. To provide an answer. So I'm going to call this agent. I'm going to classify the request based it on the requisition. OK. It was fast. Do you want that I repeat this slide or not? Is it OK? uh Because, uh, originally this slide had 7 minutes and I thought they will hate to look to a video for 7 minutes. Next we have the flow. We we have a small flow over there just within the instructions, but what if we need something that is more advanced, more customized, that we have conditions to send to a note or to an order, so to an action or a different action. In this case we have the Bedrock flows, so it's the fully managed feature inside Bedrock that we can interact with different building blocks. So in this case, Lambda functions we can use also a chatbot. Why not? Lex is here as well to integrate with previous configure knowledge bases, um, and also prompt templates. So sometimes we need to provide a a specific answer just replacing a few variables, that's why we have the prompt here as well, the prompt template. So These are the blocks, the notes that we are going to use. We have the logic, so it's going to coordinate to which node the flow will be delivered for the next step, the data that we are going to use with knowledge bases maybe to retrieve something directly from S3 or write something on S3. The code, so a lambda function can perform the execution logically or we can use the code interpreter here as well and of course the agents right that we are going to register in this flow there's the flow output with the answer and. Sorry, and in this case, we have two APIs. The Bedrock agent's APIs controls the life cycle of the flow. Um And the run time, the execution of the flow. And I have a very small demo for you as well. This is our scenario we have now. Uh, we are the owners of a company that is going to decide if it's, it's OK to provide a loan. So this is the body of the input that the the flow will receive, OK? So the income, the debt that this person has, um, in how many months it will be paid, the amount, the credit in the creditor store. Um, The first step here is to call a lumbar function. In this lambda function we are going to perform a calculation to check if this if this customer will be approved. So it depends on the results. So we have a condition over there if the loan amount is greater than the maximum affordable loan, so the maximum amount that this person is able to pay. We will send this to the prompt at the top, so the income to debt. And this is actually not a good news for this person because in this note we have the rejection letter. Because this person won't be able to pay for the loan. But If this condition is not met, we are going to send to the prompt in the bottle. And the prompt in the bottle will receive the information and call the agent to perform a few questions for this customer, so some more specific questions to help to to build the contract for the customer to have the law. We also have two different outputs because we have 2 different conditions. So I'm going to submit this input. OK. The input is being sublimated. The flow is burning. And we already have the answer, so the rejection letter, unfortunately. But in the trace we can check all the interactions that happen in the flow. So first we have the configuration of the notes. And after that, the action that was performed in this case by the Lambda function that performed the calculation. We can check the The invoked function, the Amazon research name of the function, and also the payload. There is the status code. Down there. Yeah, the alias of the function. And now we have the results. After we call the condition. Of improvement. Let's click, OK. According to the results. We are going to send to the next two notes, so the rejection letter. OK. And after the rejection letter is generated using the haiku 3, so a very simple use case. We have the instructions, instructions over there. The variables that are going to be replaced, so the applicant's name. And the output. And now we finish our flow and retrieve the information for our customer. So in this case, um, we really need to perform every single step to deliver the correct answer. So if the flow was not structured like that, maybe we are going to approve the loans for someone that doesn't fit our criteria because it all everything would depend only on the instructions of the agent. Here we have a more structured um approach. And now we have the agent corps. The agent corps is, uh, we are talking about a lot of agent corps, but it's a service that is pretty new, right? 6 months. That we are using, so I know that we had two launches yesterday about Agent Core. I hadn't time. I had enough time to check yet. Sorry, but if you, if you search for the blog post from Danilo Poncea, you will find, OK. So Agent Carr, Now we need a higher degree of customization. In this case, I'm working with different open source frameworks, and I want to add these frameworks to my gentic system. I know that every, every I know that every person in this audience has the, the preferred, uh, model, has a preferred um framework, and it's OK. I have mine as well, but for agent score it doesn't matter, OK? You can choose whatever suits better for you. The agent's car is built with a modular architecture, so it seems like um. I'm sorry, it seems like a microservices architecture. We have the execution environment. We have also, of course, the model that we're going to use. We have the gateway to expose the tools. There is also the browser to perform external actions using um the web, on website. The code interpreter that will be a little bit more robust than the other one that we're talking about. The identity to control the security in this flow, so for inbound and also for outbound. The memory for contents and the observability so we can check that every single component, every session ID is behaving like we expect. So we're going to We are going to check the modules. The first one is the run time. And to work with the run time first we have to build a recipe. In this recipe, we will choose the model. The framework, OK? Whatever you want. It's OK. And also the run-time decorator. So how the model and how the agent must be configured. The identity configuration, this is according to the security policies and the observability configurations. So which metrics are we going to use? This will be packaged into a Docker file. And the image of the Docker file will be a registry in the in the elastic container registry repository. When We make a requisition for the agent. The run time will be launched. And inside the runtime, we have the agent, so with the instructions, the logic, the framework, and the model, and we have an end point. And the end point will be reached by another module, so the gateway module. This environment, after it finishes everything that needs to process, will be staying in an idle state for 15 minutes. If in 15 minutes nothing happens, it will be terminated. You can prolong this. This runtime existence for up to 8 hours, OK? So this is a way to avoid a cold start initiation. Next we have the identity, and the identity is composed by two parts. The first part, the inbound, the second part, the outbound. On the inbound we are going to choose which provider is necessary to perform the call first to the agent. So this is in the way before we call the agent. We can choose IAM authentication. We can choose Cogninito, but we can also work with an OAuth provider. After we authenticate, we are going to be authorized to perform the actions that are provided in our security policy, OK? The token Will be stored in another component that we are going to check in the token. So, when the agent is going to call this external tool. Are we going to perform the authentication again? No, it's not necessary in this case there is a token swap, so like um. Like a trust Uh, action, so you have the token that authorizes to call the agent. So I'm going to swap for a new token for you so you can call an external tool. But what if I authenticate with IAM? And now to call the external tool. I need to use on off. I cannot just swap the token in this case. The agent will check that you already were approved it. For the, the inbound. And will generate a token more appropriate for the other authentication system, OK? But if we are working inside AWS we could use an IAM s signV4 to call the Alembbida function, for example. The gateway The gateway is the component that will restore the security token provided by the identity model and also we will expose the tools, so the servers that we are going to register. Um, The gateway also um can call uh internal tools or external tools, OK? And it there is a very interesting configuration in the gateway because. We can change the routing options. We can work with maybe just keywords, why not, but also with intent and semantic search. The memory. So it's important for our agents to remember the past interactions, but the interactions that the session is actively, it's important to remember as well. So in this case, we'll be able to to configure short term memory and long-term memory. In the long term memory, we can still have up to 36, 31 years. One year of context, right? And In this case we're going to choose what do you want to store? Do you want to start the summarization of the conversation. OK, it fits for most use cases, but you can also store the user preferences. So let's let's pretend someone is trying to buy a new phone. A new phone And this person already told that likes pink phones with the most um innovative technologies available with great sound with great camera already provided this kind of preferences so we are wor if you are we are working with something that is like a store, it will be nice to save the user preferences. The summarization fits for most use cases, but for specifics, we have the user preference as well. There is also the semantic memory. In this case, the similar context will be stored um next inside the Inside the storage and the storage is serviceless and you don't have to think about provision uh S3 book to provide a dynamo DB table. The agents car will take care of that for you. Browser We need to copy something from uh the Internet. We need to perform a screenshot of a specific browser, a specific website. So in this case we are going to work with a headless browser. Um, the playwright is pretty similar to Selenium. To perform these actions for us. It's serves as well. Everything here is serveless. So in this case it will start a session that is isolated, OK? And in this session we are able to check the actions that the system is performing if you if you want we can stream the web browser and check in our local server. So we can see the The agent typing, coping, screenshotting if it's, if you want, if it's necessary, using we can use the Nova Act for passing this information and we can use a DCV to check the streaming of the web browser. So here in this case the LLM will not manipulate the object. The object will be. Will be passed like a document, so in a computer tool call will be translated to CDP comments and after that it will execute in this headless browser. So here we have the action to click using the left button in this specific location in the web browser. After that, it will be sent to the agent's back when we open. This session in a browser, it will also generate a signed URL, a temporary URL for the actions that are going to be performed in this browser, and we can check this URL, um, using the API. The next one is the code interpreter. And the code interpreter we can, we can use in two different approaches. The first one is the system provider. So in this case we are going to work with uh default libraries from Python, JavaScript, and TypeScript. We have also the access to a terminal and this file system that you are checking here is ephemeral. So after the session is ended, so after every single interaction that was needed in this browser has ended, the file system will be deleted, OK? It won't persist between sessions. So we have this system in a micro VM. And uh it will execute the request and return the request to the agent. If you need to if you need to use more custom libraries, you can package your dependencies and work with this in the custom code interpreter. It will work fine as well. And last but not least, we have the observability. In the observability we have the GNAI dashboard inside the cloudwa service and in this dashboard we can check the agents that we are using. We can check also inside an agent, the activist session IDs. And also inside the session like this we can check more granular data and this is very important to understand if the agent is truly behaving in the way that we expect because we, we can see that it's not a very, it's not a small ecosystem we have several moving parts and only monitoring. We will see if this is behaving in the way that we expect. In this case, I have 168 sessions. We can check the traces as well, and we can check the telemetry in a different API if you want. So So that's all for this session. Uh, if you have any questions I can answer after I finish here, we have here also the the developer community pavilion. I was there yesterday, pretty cool place. The, the expo this year is amazing. If you want to play during your stay here in the event, you can also find the gamification area in the expo and try to win a cloud Quest tournament, a gen tournament, and if you want to know more about the cloud, there's more than 6, no more than 1000 free trainings inside the SkillBuilder platform. There is also free gamification on SkillBuilder. And I hope you enjoyed our section. Thank you so much.