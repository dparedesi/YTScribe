---
video_id: iUw1d4bUNJk
video_url: https://www.youtube.com/watch?v=iUw1d4bUNJk
is_generated: False
is_translatable: True
---

Uh, welcome. Uh, my name is Joseph Adoric. I am a director of product management at AWS. I've been with AWS for about 9 years building database services. Uh, one of the services that I get the pleasure of working on every day is, uh, Amazon Elastic Cash. Uh, during the creation of this presentation, I misspelled the word principal for probably the 17th time in my career. You always gotta remember it's your pal. Your principal engineers are your pals. They're your friends. So joining me is my friend Matt Olsen. We've been working together at AWS for over 9 years now. Uh, the other thing besides our passion of databases, uh, we both, uh, hail from Minnesota, so we're kind of the only two people on the team that can talk about winters, ice fishing, snowmobiling to school, casserole, hot dish, all the good stuff. So I welcome you today. I appreciate you taking the time, uh, to join us. Um, so is in my role as a product manager and, and I, I see a bunch, you know, I talked to a bunch of customers across a bunch of different databases, and the best conversations I have are ones where I get to ask you, what are you building, what's on your road map? How do we help you build? Sometimes I have to ask the question, what's not going so well for you? How can we make it better? What are the things that, that you're struggling with? And when I ask those questions rarely do I ever hear people say, you know what Joe, we don't need any features anymore, we're good, it's, it's full featured, we're good, we don't need anymore or it's fast enough, you know, or you know. It's pretty well priced. Even if you made the price lower, we wouldn't know what to do with it. We're happy to give you more money. Like none of those things ever come up. Customers are always asking us with every iteration, make it better, make it faster, make it cheaper. And that's really the premise for our talk today is with with In Valy and Elasticash, with every release we're making, measuring against ourselves, how we make it better, faster and cheaper. We're gonna talk. Um, about a number of those things, so we'll talk about what is Valy and, and how we got here today. Uh, I'm gonna talk about the multi-threading architecture that we just came out with, um, and, and how that benefits a lot of the workloads that you folks have, and what that means to you, and then Madeleine's gonna come up and talk about some of the, uh, reliability improvements that we have around dual channel replication, forward compatibility, uh, and cluster resilience, some, uh, reductions in memory overhead, and also, uh, talk about. You know, one of the one of my favorite features bloom filters and we'll talk about what's next after that. All right, so the origin of Valy, Valy is a really fascinating story, and I've been in the database world quite a while now. I'd say rarely do you get these inflection points or you get these opportunities in your career to work on something like Valky. Maybe it happens once a decade, maybe, maybe even once a career, um, but it's been a really, it's been a really cool experience, um, and, and for something to kind of have that, that, uh, level of gravita gravitas or excitement kinda has to start from some interesting origins. Um, and ValKy is kind of one of those projects that was really the necessity of invention. So what is Valy? Well, Valy is an open source BSD license data store for high performance key value workloads such as caching, session stores, leaderboards, semantic caching, and a number of workloads. Um, Valy is, uh, open source project backed by the Linux Foundation. Uh, it'll be BSD licensed and open source forever. So the way that we got here today is in 2000, so when Reddi came out in 2009, it was BSD licensed and open source uh up until March of 2024. Uh, and Reddis changed the license to a dual stack. Dual stack license. Uh, we're back online. All right, um, which, which really caused, uh, which caused, uh, some, some changes in the community and, in response to, uh, that licensing change, the community actually forked the last open source version of Reddis, created Valky, uh, Valky 72, uh, and we're off, uh, off to the races. So within the last year and a half we've had a launched the first major version of Valky 80 in September 2024, 81 in March 225, and then another major version release for 90 on October 21, 2025. So within a year and a half of the creation of the Valky project we've had two major releases um and quite a bit of momentum. And what comes next? Probably 10 to 0. But maybe the moon, but maybe 100, we'll see. Well, one of those two things is gonna happen, but I think that really the beauty of it is as an open source project you'll be able to see it coming a long, long, long ways away, um, and, and be able to see what the road map is and the stuff that we're working on Valky project. All right, so for, for Valy, you know, I think one of the really beauties about ValKy and one of the reasons for its success and in the, in, I would say the tremendous growth we've seen over the last year and a half is really is that multi-vendor governance model. It's, it's not any one vendor, uh, that's control of the license or the roadmap, but again a collaboration between multiple different vendors. Uh, it's a drop-in replacement for Reddi 7.2. Um, it'll be stewarded by the Linux Foundation forever and it'll be BSD licensed, permissible open source forever. Uh, there's 50+ organizations contributing to the VALKy project, 150+ code contributors. We have now 10+ managed service providers for VALKY. After a year and a half, which is, is tremendous, you know, over 1000 commits, um, and, you know, tremendous amount of container polls, so we, we're seeing a tremendous amount of, um, momentum on the project, uh, and growing. And I think the fun thing is, you know, having been in the database industry for a while, it's a small, like I'm sure in, in the industries in the, in the companies that you work with, it's a small group of people, so you might work with somebody 6 years later, they're working at another company, so I actually get to work with some of my former colleagues, uh, that now work at Google and collaborating on what's the roadmap, uh, for Valy and, and, you know, how that's taking form. So that's been really fun. Lots of friends, you know, contributing across Verizon and Hiroku and, and Snap. So it's, it's really, it's nice to. Have have all these minds working towards a common purpose. And a part of that too is, you know, the technical steering committee isn't, isn't a single company but it's six individuals, uh, that have a tremendous amount of experience working on Reddis, and now they're leading the Valky project, uh, and one of those steering members of the TSC is Madeleine who will be joining us today. So really an expert, uh, among experts in this space. And then with the creation of Valy, we also offer it in Amazon Elasticash and some of the things that we've, we've done there, added a serviceless version of it, um, and then I'll I'll talk a little bit about the pricing bit in a little while. Um, but with each new version of Valy, uh, we'll be offering it in Elastic Cash as well. So from Reddis 7.1, which is the last version of, of Reddis that we offer on Elasticash, we now offer Valky 7.2, which is a drop-in replacement for Reddis 7.2 minor version upgrade. Um, seamless zero downtime. We have 80 81 82, and then coming will be 90 and all the subsequent releases after that. So we'll track the open source project in Elastic Cash and we'll continue to shrink that time from open source release to when it's available in Elastic Cash as well. So from, from Elastic Cat team perspective and since we're in Vegas and we have to use all those analogies now it doesn't, don't get it doesn't have the great context later we're really all in on Valy, you know, it's really our future, uh, we're very excited about it. We're very excited about the Valky project, um, and all the, um, all the contributions in, in the broad, uh, momentum there. All right, so let's talk a little bit about uh some more of the substance of ALI. So, like, uh, like most of the physical world, you know, caching workloads, if they're, if they're steady state, they're super simple, right? You just pick an instance plus or minus a little bit of throughput, and you're good to go. The most interesting caching workloads. Maybe most interesting, most challenging customer uh workloads for caching are the ones that are spiky. These workloads are ride hailing service and a concert gets done. Maybe you're selling tickets for a concert, maybe it's a game that becomes popular. Maybe you do a food delivery service and you have to deal with, you know, morning, noon, and, you know, lunch rushes. Maybe you sell, you know, shoes and you get these big spikes. We typically see from customers, and it's kind of been predominantly the space is. You provision a certain amount of capacity. In this case, I'm using an M7G2X large for illustrative purposes. And you know you feel pretty good about it, but then you see that one spike at the end and then you start to feel a little bit nervous and you want to provision more capacity. Because you wanna be able to have it, you know, you wanna be able to, uh, you know, deal with any, you know, successes in your business. The last thing you wanna do is your cash to not be able to deliver the throughput it needs to in your business, uh, needs it the most. So you kind of go through this dance. Do I scale up or do I scale out? You kind of start to then decompose your application. Is this throughput bound? Is this memory bound, and you start to think about the complexity and, and how you're trying to do that. So what if I, you know, postured you a different outfit? What if we better utilize what we already have? This is kind of like the glass is half empty, glass is half full, it's twice as big as it needs to be. This is kind of interesting is how many of you know ValLKY 72, how many, the original version of Valky, how many people know that this is actually a single threaded architecture? Good raise of hands, I'd say 30% or so. Um, and you know one of the motivations for why this is single threaded architecture in Valy is we want to keep the execution of commands all in a single thread to be able to serialize the commands to prevent against complicated. Communication uh between multiple different threads, at least on the execution and, and I would say sometimes some challenging race conditions so architecturally we want to keep the execution on the main thread, um. But that doesn't mean we can't do other parts on other threats. So we look at bottlenecks for a system like Valky and this is for the This is for this this event loop we have right here folks aren't familiar with this we use this in the database world all the time. These are flame graphs uh they make beautiful art pieces too, but not, uh, but they're also functional. Um, and the way to read a flame graph, and this is for this call stack, is you, your, your horizontal axis is really how much CPU time does each one of these call stacks, uh, actually create, uh, the, the, the vertical depth is just how many function calls are within that particular call stack, right? So if we look at where we're spending time on this, and again for, for Valy, it's, it's a very simple, it's a very simple loop for how we process commands. We get a command from. Colonel, read the client or read the command, parse the command, execute the command, then send the response back uh to the client. So where do we spend most of that time? Well, a lot of it gets spent writing that that response back to the client. Little bit gets spent reading from the client. We spend a little bit more time getting that command and parsing that command as well, and then our e-pole time as well. So if we see the decomposition, we want to keep that execute command on the single thread, but there's a lot of work that doesn't actually necessarily have to be on that single thread. And if we go and we look at, you know, we go look at, you know, the rate of, I'd say CPU clock rates versus cores. Really we're looking at, you know, clock rates aren't improving as much as they used to. We're getting a lot more core density on individual machines, so really expecting to be able to scale up for higher throughput workloads with faster CPUs typically is not going to be the answer, at least for the workloads we're seeing and trying to have a hardware outpace our customer demand is is really not the answer. So we really asked ourselves well how do we take a single threaded architecture and then start to depose this into a multi-threaded architecture. So what we started to do is take those those CPU heavy aspects of the execution loop or the event loop and then be able to evolve the architecture to be able to spin those off into multiple IO threads. So on the main thread we'll look at the we'll look at the CPU usage and if the CPU usage starts to spike we can spin another IO thread and then start to offload parts of those commands, still keeping the execution on the same on the same main thread. And with that we can start to add in more work, right? We can start to add and get more throughput, more parallelism in the commands that come in, still keeping the execution on the main thread, um, even if we see the second IO thread starting to become more utilized, we can spin that off and create another IO thread and then saturate that one as well. So we have a lot more, so we get to use a lot more of the cores that are actually on the machines you're already using to do a lot more throughput while still preserving the same architectural. Um, uh, primitives that we want to do in Valy by keeping the execution on a single primary threat. So when the team came to me and they were working on this and they're like, Joe, we improved the performance by 230%, I was like, no you didn't, you know, like in the database world that you know if you get double digit improvements in performance, that's, that's pretty rare these days, you know, we've been after this stuff for quite a while. I was like, no you didn't tell me how and they're like. Valky 72 is single threaded. We made it multi-threaded. I'm like that's pretty cool, you know, like that, that even surprised me. So I mean that's, that's a significant gain, um, and, and I think that's, um, you know, we had a, we had a customer article published last week from Nextdoor, um, who I just got a notice from about something that's happening in my neighborhood, uh, and one of the things they said that it really spoke to me, um, about the Valy project is how. You know, the folks like Madeleine that have been maintaining the project have really gone back to core engineering work on just performance and scale and reliability and, and what that means to them and their business and, and how they've appreciated, you know, just creating multi-threaded IO is, is, is something we might have taken for granted or something we thought, you know, might have not have been, you know, prioritized, uh, but the tremendous gains that we're seeing are, are fantastic, um, and I know anybody sitting in the aisle, if you're, if you're a database person, you're like, yeah, but how did you benchmark it, you know, can you tell me more? Um, so here's some of the more information about a couple of blogs on, on how we benchmarked it. Um, these will be available in the slides afterwards or, uh, up on YouTube up on YouTube is, is the presentation is, is posted as well. Pause it for a second. So what's the result? So if you have a workload that looks like this and we add in this fancy new multi-threaded architecture, well, these are the things we're seeing from customers is they're able to say like, hey. We don't need that 2 X large anymore. We're getting significant more throughput. We can go down to an X large, cut your costs in half for that particular workload, and not only that, I didn't even make my axis is big enough on this, right? You have significantly more headroom to be able to scale those workloads. So again, this is in the spirit of making it better, but also cheaper as well to run more workloads on the stuff that you're already buying. But I hope the engineer in you is unsatisfied with this graph, and the unsatisfying part to me is you take the area under the curve of the purple line, subtract it from the area under the curve of the blue line, and you say we're wasting a lot of, we're wasting a lot of resources, we're not heavily utilized. Joe, wouldn't it look better if the graph looks something like this, and this is really what we've built with Serverless and Serverless allows us to be able to offer you a 100% utilized, uh, service. You don't have to go through this undifferentiated heavy lifting of trying to figure out how much capacity and spikes and all this other fun stuff, um, and we can offer it to you on a pay per request and, and pay per memory, uh, type usage type and this. Was really made possible by the multi-threading work that we did in Valy, um, because of that and the way that we build the serviceless architecture in elastic cash is kind of read this left to right from your client application or your application right, uh, kind of goes down more towards the caching nodes. As client application requests come in, they go through an NLB which resolves similar to a proxy fleet. That proxy fleet knows where your cash cluster and cache nodes are sitting and then goes in either fetches or writes to those particular caching nodes. What happens on those particular caching nodes is is data is really distributed in, in Valy across a number of different slots, and any one of those slots in any one of those given times can spike up. So the way that kind of the burden on us then is, is a serviceless provider then is to manage those spikes. This is where the multi-threaded IO stuff really comes in is allows us to much more effectively be able to handle these spikes on a given machine. Um, and we can spike multiple different slots at any one given time, and we can spike it pretty quick, um, you know, 30K, 240 km in 5 seconds, so that allows us to be able to be very efficient within an individual machine. And then we have to go through the same dance, right? Should we, should we scale up or should we scale out? Uh, and as we measure the heat on any individual machine, uh, we can also move slots to other machines. We can double the throughput for any given slot, um, every 2 to 3 minutes, uh, and that's our pleasure to do this work for folks so that you don't have to think about this anymore and we get pretty efficient at doing this. Uh, and then you get something that looks like this at the end, and then this line, this line is ours to manage. And a lot of the times I kind of get the question, you know, we'll kind of go back to. You know, when we introduced Valy, you know, we, we introduced it at a 20% price reduction, uh, for node-based and 33% reduction for serverless. A lot of times people ask me, Joe, how is that possible? Well, the more efficient that we can offer our, the more efficient that we can manage our resources through multi-threaded IO. We can do more work on one machine. We can lower our costs. These are the types of projects that. You know, we do all the time and then we're able to pass those savings back onto you as customers. So upgrading from 7.1 to 7.2, 20 to 33% cheaper, no work done, just happens magically. Going from 7.2 to 80 230% improvements for throughput bound workloads. So just going from 7.2 to 80 is a significant amount of price performance improvement without having to change anything in your application. It's just either pricing or the core system just gets better. With that, I'd like to welcome Madeleine up on stage, uh, to talk about some of the reliability improvements that we've been working on. Delightful. Thank you, Joe. So just to reintroduce myself, my name is Madeleine. I've been working on Amazon Elasticash, uh, for over a decade now. Uh, I started contributing originally to Res Open Source back in 2018. I was added to the Res core team, which is effectively the open source maintainer group in 2020. And so I've been working on this stuff a long time. So when Valky came around in 2024, um, that wasn't anything new. Like I've, we have a lot of history in building in open source and building a lot of really cool exciting engineering work. So, I'm gonna continue on and talk about some of the reliability improvements that we built into the Valky engine that improves both open source and our managed services. So Traditionally in caching, you want most of your workload to be sent to the cash, right? The cash is very efficient, it's highly performance, and when the cash goes down, more requests are going to be sent to the back end, which are usually higher latency costs. The back end might not be properly scaled to handle that increased load, and increasingly we're seeing stuff like semantic caching. Come into architectures which also has a lot of the same properties that we saw with traditional databases, so we want the cash to be as available as possible, significantly more available than the back end service that it's caching data from. So we focus on two main areas. The first is managed upgrades. So when you have a version of Valky, you want to be, you know, constantly patching it to get all these improvements we talked about, performance, security. As well as unmanaged. Changes such as when a node fails you want to be able to detect that quickly and do a failover to stand by replicas that are available. So let's talk about those two flows in a little bit more detail. So, I'm gonna start out with the steady state. So Valky has a primary and replica architecture that is, uh, sharded. So this is a single shard of a Valky cluster, and these can scale out horizontally to up to 1000 different, uh, shards. The clients are aware of the topology and will route requests to the specific shard that owns that set of data, and then it gets replicated logically to the replicas. Unlike some systems like Postress which do physical replication, this does, it actually sends the commands to the replicas. So let's look at a managed upgrade and see how that works. So within Last cache and the open source community, we recommend what we call an N+K upgrade strategy, which is you add two additional nodes, one for each node currently in a shard. So in this case we're gonna be upgrading from Valky 7.2 to Valky 8.0. We're gonna add a replica and we're going to designate it as the snowflake that's gonna become the primary leader. And then we'll have a secondary replica that will eventually take over the other replica. Responsibilities So to bring these nodes up to date, we do a two-part process. The first is we take a full snapshot of the load on the primary, basically all the data, all the keys and values. And then we send that to the 2 new replicas together. This adds a little bit of extra load on the primary, so we want to limit the amount of work that is done during this process, as well as limit the duration to limit the impact on end users. Once that snapshot is complete, we then stream all of the incremental rights that came into the that the primary during this period of time. And once that's done The replicas now in sync and able to serve data. So we can begin basically transferring the ownership from the existing primary to the new primary. We do this through a process called failover, which is a fundamental building block within our service, as well as the open source community, and this is the safe transfer of slot or uh shard ownership from the previous primary to the new primary. So during this process we start blocking rights so end users don't observe this because it tends to be a very short period of time. What they actually see is a slight rights latency increase as we basically wait to process those rights until the new uh primary takes over, and this allows us to make sure that the primary and the uh the new primary are caught up with each other. During this time, read availability is unimpacted. So once everything is in sync, we're able to complete this failover process and the 80 primary is able to take over. And this right that we paused earlier is now able to get processed. We then do the exact same mechanism we talked about earlier to restart the full sync to get these new nodes all in sync, but since we already have a point in time snapshot, we're able to skip that step and just stream the incremental rights. So let's talk a little bit about some of the problems with this approach and some of the things that we fixed. So one thing about this process is that full snapshot I talked about is not forward compatible. So, but it is backwards compatible. So if you're on a Valky 8.0 cluster, you're able to basically receive a snapshot from a 7.2, but not the other way around. A 7.2 cluster cannot receive a snapshot from an 8.0. In this case, basically the replica sends its version and the primary will reject that version. But one of the things we want to improve is, uh, basically customers came to us and asked us, they're like, hey, we wanna be able to safely downgrade, so I wanna be able to go to 8.0 and then go back to 7.2. So we actually worked with the community to implement that functionality so that when a replica requests a snapshot, the newer versions can actually send a downgraded version of that information to the older versions. This is great if you wanna have like a. Uh, blue-green deployment in production, but it also helps us solve a problem, uh, which is that if a replica for any reason is unable to connect to the primary, the new primary in a timely fashion, it will no longer have a valid point in time snapshot, and it won't be able to then get that snapshot because of the forward compatibility issue I mentioned, which results in these two extra nodes unable to sync from the primary, so they just kind of sit there and are unable to serve traffic. Which can result in if you want to fail back for any reason, you might have an availability outage. But starting in Valky 9.1, which is still not even released, it's a long way in the future, but, you know, we like to think ahead, uh. Valky version 7.2 will be able to successfully stay in steady state with these 9.1 clusters. Another thing that we worked on to improve is I mentioned that we want the full sync to happen as fast as possible because the snapshot puts some load on the primary. So one of the things we built in ValK 8.0 is what we call dual channel replication, uh, which is we're able to send both the snapshot and the incremental rights at the same time. This allows us to offload some of the extra memory pressure from the primary onto the replica. And we're able to do this in part because over time we've seen that network cards have gone a lot faster and we're able to transfer a lot more data at a given point in time. On older versions of hardware, we're actually typically bottlenecked on the network throughputs, and since we no longer have that problem with more modern hardware, we're able to stream the data sort of together. So that is the uh planned uh planned maintenance work improvements that I wanted to talk about. Next I wanna talk a little bit unplanned failures, and to start, I wanna talk a little about how Valky detects when a node failed and triggers a failover. So in our recommended high availability mode, Valky cluster, there's something called the cluster bus, which is all the nodes are periodically sending ping and pong messages to each other. And these messages include some information. They basically include, hey, what data do I own, what shards do I own, uh, something called the node epoch, which is. A unique identifier for the given node, basically it's like, hey, what version of this information do I have? Each primary in the cluster has its own unique node epoch. And then we're also gossiping this information so that other nodes can learn about it more quickly. But there's also another response which is no response, which is the note is dead, and we should do a failover for it. So let's look at this into a little bit more detail with a concrete example. So we have 3 shards, and they each have their own unique epoch, right? So we have shard 1 with the epoch 1, shard 2, epoch 2, yada yada, and each of these shards has 1 primary and 1 replica. So we just walked through how planned operations work, so in that case, there's a very safe transfer, and we bump Epoch as part of that transfer. So what happens when a primary fails? So in this case, we have shard 2, the primary failed in that shard. And the replica will eventually detect this through the gossiping through the cluster bust, and it'll basically request from the other primaries, hey, I would like to take over this shard. So within distributed systems we have something called a quorum, which is the minimum number of nodes you need to basically commit a decision. In that case it's the other primaries in the cluster. So this request will start with an epoch, which is basically, hey, this shard wants to take over this information with this epoch. And uh every other primary is able to vote once per epoch again, every node needs to have a unique epoch, so we can only vote for one epoch, and once it's granted, that replica will take over. So this is how the failover mechanism works today in Valky 8.0 and lower. So let's talk about a failure mode and how we sort of fix that in future versions. Which is that if we have multiple concurrent failures, we see this sometimes when uh maybe we see an uh zonal issue with like networking, so there's like a network split, and so multiple failover trying to happen concurrently. So in this case, uh, we now, we added a 4th shard, and both shard 2 and shardth have failed, and we're trying to get, uh, a decision to do a failover. Um, we'll assume that, uh, this isn't quite, this is too many nodes handle for quorum, but for simplicity we'll assume that this is 2 is quorum. And so we often have uh replicas are able to reach different primaries more quickly, so if they both started this election at the same time. They would both get their granted response from the closer rep uh primaries, and they will then get a rejected response from the further away primaries. Now this is perfectly fine in distributed systems. What we typically do is then retry with some amount of jitter or back off um and allow the system to to eventually converge, but this takes more time and as we talked about, we want the system to be as available as possible. So what we did instead is each. Replica will basically take its lexigraphic order and will wait a short amount of time until. Um, based on how, how far down it is in the lexigraphic order, so the first node with the, uh, the highest lexicographic order node ID we'll try to do the election first. And then once that one succeeded, the next refica will start its election and then get elected. So this is effectively the same thing that was happening anyways with this back off and jitter, but since we codified it, we were able to dramatically improve the failover times for large clusters. What we observed is about. Uh, an election that would often take maybe several minutes was often reduced down to maybe 10 to 15 seconds to get all of the nodes filled over for large clusters like 2 to 300 nodes. And so this is the city state. So just to go over what we talked about within the replication group, uh, replication improvements, so in VALKY 8.0, we introduced dual channel replication, uh, in VALKY 8.1, we built the functionality to do faster failovers. And then, uh, coming soon will be the Ford replication compatibility, which will be an improvement that we'll see in future versions, most likely Valy 9.1, and will come to Elastic Ash relatively shortly. So next I'm gonna move on to another section, and I'm gonna talk about how we made uh Valky more memory efficient. So this is a very exciting area for me because this is typically what saves end customers the most amount of money. So most caching workloads, most high performance workloads are bound by memory, and obviously memory is very expensive, we don't want to be spending. Uh, very much extra resources on something that's quite precious. So back in ValK 7.2, we did some analysis to basically figure out how much overhead is there for us to store these various pieces of information. So a simple key value pair in Valky uses 52 bytes of overhead. Now that might not sound like a lot, but when we did some analysis on the metadata within the Amazon Elastic cache. We found that the P50. Values for the key and value sizes was 16 bytes for the key sizes and about 80 bytes for the values. Which adds up to be about close to 100 bytes, so you're spending almost 33% of your overall, Like memory allocations, basically for overhead. And that's the best case if you're adding TTLs, again, extremely common, uh, for caching workloads, you typically like to add time to lives so the data gets kicked out. You're adding another 32 bytes overhead. And in cluster mode, basically what we think is like the de facto best configuration, you're spending another 16 bytes as well. So we really wanted to figure out how to improve this. So we actually started our memory journey back uh before the fork. We're actually working on this back in R 7.2, and our goal is to basically rebuild some of the core data structures in Falci to save memory without losing performance. So I'm gonna walk you through kind of what we do. So let's talk a little bit about CS fundamentals. So the main data structure backing VA key is what we call a hash table. So the way a hash table works, it has an array of buckets, and we are going to put those key value pairs into those buckets, and how do we decide what bucket to put? We use a hash function. So for simplicity, we'll say we have a key value pair called f bar. We will employ a hash function on and it will tell us that the bucket is 00. So we had a separate memory allocation for this, so this is a 24 byte allocation, a lot of the 52 bytes we just talked about, which its sole function is basically to hold all of the information, hold pointers to all the information. So it has a pointer to the key. So when we're traversing trying to find is, hey, is this our key value pair, we'll go uh follow this key pointer and we'll see that hey, it's pointing to a string called fu. We'll do that comparison from our original key value and we'll see, yes, this is actually the key value pair we're looking for. So then we'll follow the value pointer. And it will point to this intermediary object that we have called the Valky object. Within VALKye there's a lot of key value, uh, different. Types the value can be so traditionally it's usually strings, but it can also be stuff like sword sets, hashes, etc. So we need to include some metadata to determine which type it is. We also have some low level information like ref counting if we have multiple different pointers to this object. And then finally we have the the pointer to the actual value in this case's bar. And of course if there's a collision, so two values, so, so two keys hashed to the same thing, we have to resolve that, and we do this through linkless collisions resolution, which is basically as long as there's a next pointer, you have to keep checking the next pointers to find your value. So every one of these little light blue lines is a memory lookup which is both slow. In our scheme it's about 100 nanoseconds. A typical Valky command takes about 1-ish microseconds, so 100 nanoseconds is a lot, right, because we have to go to main memory. We'd like stuff to be in CPU cache. And it's also using 8 bytes of overhead, which we don't want to do, uh, especially with small keys and values. So what did we do? Um, the first thing we did is we moved away from this simple static structure. If anyone here is much of a seed developer, it's kind of annoying to have dynamically allocated structs and figuring out where information is inside the structure, but we're like that's worth the complexity, so we don't have to pay this 8 bytes for this extra pointer, and then we have variable allocation sizes. The next thing we did is like why do we have this generic container object at all? Let's just embed everything directly into the basically the container, and that container can become the V key object and this required us to rebuild a lot of the internals and the hash table to say like when we want to look up a key, every type has to declare how to find a key in the object it's looking for. So let's look a little bit more about that, the problem I mentioned about how annoying it is to find where data is inside a structure. So this was the balky object we had before. These are 8 bits, uh, sorry, all of this is in bits. So the value pointer, uh, for a 64 bit system, 64 bits. And so we basically have to keep track of information to say, hey, where is something inside the structure? So I mentioned that we started to embed the key, so we now need to keep some extra overhead that we know where it is so that we can then basically offset into that to actually find the actual location of the key inside the structure. Cool, so one thing in that previous slide that wasn't there was the next pointer. So we also want to get rid of that. So a traditional mechanism for solving that is to instead of using linked list for resolution, we use, uh, what's called linear probing, which is to say. There's a lot of extra empty buckets inside our table array, we want to keep it relatively empty. So instead of sticking an object exactly where it's supposed to be in the right hash bucket, we'll stick it in the next hash bucket. So this was pretty popular, sort of in the early 0s, and but one thing that doesn't take into account is most modern hardware is 64 byteware, so we call these cache lines. So when the CPU tries to fetch memory out of main memory, it actually pulls the adjacent 64 bytes with it. So an alternative strategy that we use is we employed some learnings from a paper that Google wrote about what was called a Swiss table. Which have these large 64 byte buckets, and these buckets are, you know, they have 7 entries and some extra metadata, and so when we have a collision, we just stick it in the next 3 pointer inside the bucket. So when we want to go look to see if something is in this bucket, we go and check every single one of these entries to see if something is in there that matches to our key. So earlier we talked about food. So, if there were 6 items inside bucket 00, we check every single one of those entries to look for our item. So that's not very efficient. So, um, and the people from Google were aware of this, and they actually took advantage of SIMD operations, which are single instruction, multiple data, to accelerate ruling out if some of those entries were incorrect. So I've talked a little about hashing functions and I want to differentiate basically the the lower bits from the hash function. So if say Hahfu mapped out to this long 24 bit string, the bits to the right are what's actually used to compute the buckets, and then the top bits are often discarded and thrown away because they, we don't need them to discern which which bucket an object is in. But instead we're gonna store this in this metadata I talked about. So we keep one. Bytes of information, which is 8 bits at the top of in this metadata block. And we're able to very efficiently go check through them with with uh. CIMD operations. So this is a very simple set of uh CIMD operations in C, which basically lets us say hey let's get all the hashes out. We'll basically have an array of 8 by hashes for what's currently in the buckets. We'll create a vector of the hash value we're looking for. And then we'll basically have the CPU go and check all of those in parallel. So this allows us to basically check which of these buckets potentially has the entry we actually look for and since we're using one full bite, there's about a 1256 chance we're going to unnecessarily look in a bucket that doesn't have the value we're looking for. So there's one last piece of unsolved unresolved problems that we had to solve with this implementation, which is that we had to go and check if basically this bucket is full, what do we do then we still do need to have a next pointer which is what we basically do around the entire bucket. So instead of paying 8 bytes for every object for the next pointer, we're only paying effectively 7/8 bytes, so about 1 byte. So let's look at some of the results that we had from this, um, so between VALKY 7.2 and VALKY 8.1, which is the version where we actually released this new hash table. Uh, we were able to remove those three main pointers we talked about the key pointer, the server object pointer, and the next pointer, which saves about 24 bytes, and we had one extra byte of overhead that we added in. So that means we're saving about 23 bytes for a simple key value pair, right? So that's the 52 to 29. Uh, there's some other benefits of this implementation as well. We actually removed 16 bytes from the TTL and we were able to completely remove the 16 byte overhead from the cluster mode implementation. Which means in the best case you're saving about. Um, 55-ish bytes of memory. Total. So let's put that into some actual concrete example. So we had a customer that we worked with that they moved from Valky 7.2 to Valky 8.1. This is sort of like maybe the best case. This customer had very small keys and values. Um, we actually like re-ran sort of their migration because this was on over a much longer time period, so they had about 150 million keys. Their values were relatively small. They were only about 24 bytes, and keys were also about 16 bytes. They saw about a 20% memory savings moving to 8.0. And then a further 27% memory savings when moving to 8.1. Now you'll notice I say 41% we need to multiply those together. We can't just simply add them. And now I mentioned that we wanted to improve memory primarily, primarily, but we also didn't want to regress on performance. So I also want to call out the significant performance improvements we saw with this architecture. So we did a bunch of micro benchmarking and macro benchmarking, and basically the actual time we spend fetching items out of this hash table was cut about in half. There are two main performance workloads you like to test with hash tables when doing caching. The first is the cash hit case. Ideally you'd want to be getting data out of the cache, but also if you have a lot of cache misses, you also want to make sure that the full miss rate when the item is not in the hash table are also being properly accounted for. So we saw the performance most improved on that missed case, and that's because of the SIMD operations we're doing. We're able to rule out basically all of the entries inside those buckets, uh, so we didn't need to check anything, which is a great improvement. We also saw a great improvement for just cases where we actually had to look up items because we're doing less memory references overall. So the last thing I wanna talk about for memory improvements is probabilistic data structures and specifically a bloom filter. Um, who here is familiar with what a bloom filter is and what it does? OK, a fair amount. So a bloom filter is a probabilistic data structure on top of a set. So a set is an unordered collection of items. And you typically ask one of two questions about is an item in a set and sort of like how big a set is. So Valy actually has two types of probabilistic data structures, the hyper log log, which solves the answer to the question how big is a set, and a bloom filter answers the other question whether or not an item is in a set. So they both use probabilistic mechanisms to reduce the amount of memory being stored while um keeping that same benefits. So with a hyper log log, it's an approximate of the cardinality and for bloom filters it answers, it doesn't exactly answer the question, is an IM in a set? It answers the question, is the I definitely not in the set or is the item maybe in the set. So an item might not have ever been added to the set, but it, the bloom filter might report it, which is uh what we call a false positive. So let me try to enumerate that with an example that we saw from a customer that we talked with. So they had a workload where they basically wanted to have a microservice that could detect and block malicious IPs. So they had a database of all the IPs they believed to be malicious, and every time a request came in they were going and checking this file and checking to see if it was in the list, and if it was, they were saying no. So at one point in time they moved this to all be sort of a distributed system and so they wanted to add a cache to include all this information, so they were hydrating the cache, specifically a set of all the malicious IPs, and so instead of checking the database they were going and checking the cache. So if an item was in the set they would block it and if it was not on the set, they would not block it. So what they were observing with all of this is it was consuming a very large amount of memory. It was very fast, it worked, it was pretty distributed, um, but they wanted it to be, you know, they wanted to use less memory, they wanted to save costs. So what they did is they replaced that with a bloom filter, so the same mechanism still existed, um, from the database they go and populate the bloom filter within the cache, and if the bloom filter says an item is not in it, we can trust that we can say, hey, this item is definitely allowed listed and it can go through, and we expect most of our requests to be valid requests. It's really only on kind of extraneous cases that something is being malicious. But on the malicious case we have to do a little bit of extra work, right? We said an item might falsely report if it's actually in. The bloom filter, which is like the false positive case we talked about, the item is actually maybe in the set, so we did, we do need to go back and check the database on that case. So this particular customer observed no material increase on the actual database. They said it was about less than 1% increase CPU utilization, but they were able to reduce 98% of the memory they were using within the cache itself. So, on top of uh what sort of Joe set up at the beginning, we care a lot about improving the fundamentals of Valy, and that includes both improving stuff like reliability, performance, memory efficiency. But we also care a lot about adding new functionality that sort of fits into the ecosystem of caching and high performance key value databases. So just to kind of recap this section about what we talked about and when it sort of all came out, so we had a 20% less overhead, uh, reduction for Valky 8.0 with the first part of the, uh, hash table stuff we talked about. There was an additional 27% memory reduction in VALKY 8.1, and we also launched Bloom filters as part of 8.1, uh, and this is, so all this stuff is currently available inside Elastic cache for Valky. So We, we covered basically, uh, most of the stuff we talked about, so I wanna do a quick recap of sort of everything before we get to what's next, so. Valcke is a kind of a vendor neutral open source project, and we're really excited and we think it's developing quite quickly. We talked about stuff like multi-threaded IO which dramatically improves performance. We talked about some of the reliability stuff which is improving unplanned failures with the faster failovers, the faster replication with dual channel replication. We talked a little bit about how we've been trying to make Valci cheaper overall by doing memory reduction. As well as adding new data types like bloom filters to more efficiently utilize um. So that users can choose data types that better fit their use cases. But let's talk a little bit about what's next. So one thing I'm kind of hopeful that some of you might come out of this conversation is that Valy is, uh, unlike a lot of other AWS services, our roadmap is fully in the open. I'll give you a slide in a second that sort of says, hey, you can go, go here and we're, you can just go find the code that we're working on. We're actively working on full tech search capabilities which, uh, we see a lot in caching workloads for like e-commerce websites, stuff where you wanna like return some results very quickly. It's also gonna help in, uh, support. Uh, hybrid search for things like factor similarity search, which I haven't mentioned Gen AI yet. I'm sorry, like this is the obligatoryen AI reference like if you're doing stuff like memory for agents. This can really help improve that performance. Um, we're also actively working on trying to improve durability. So, a lot, we talked a lot about Elastic yesterday. There's also a sister service called Memory DB, which is the durable version of Alki. We're working with Community to sort of add that built in to the community so that everyone can self-manage and get the same high durability guarantees that they can get from Memory DB, which I personally am very excited about. Um, I talked a lot in the reliability section about why it's important to do full snapshots quickly. So we're actually doing two more improvements there. We're trying to fully multi-thread that implementation so that it can happen significantly more quickly. Um, you know, Alaska Servili will be able to take advantage of this because we'll be able to able to dynamically add cores to our caching nodes, but you can also do this in self-managed stuff like Kubernes allows you to dynamically add cores as well. So we're really excited about all that. And we're also still working on trying to improve the data types available to end users so they can better utilize the. The hardware that they have. So the next big thing we're working on is time series data types. So any workloads where there's a lot of data that's being uh ingested and you wanna get like real-time dashboarding on top of it, that's another thing we're excited to build and work on. So, here is my ask, if you are curious, if you are excited, if you would, you don't even have to know C. I know Valky is all written in C, but even if you write in higher level languages, we do just want people to. You know, contribute and talk about the workloads that they want to solve in an open fashion. So if you're interested in learning more, uh, both me and Joe will be out in the main area. If you have a question about my section, feel free to ask me about it. If you have, if you have any questions about Joe, also feel free to ask me. Uh, we also. We also have uh a bunch more Alaska specific sessions uh during this reinvent, so feel free to go check those out. Uh a lot of these breakups typically even if they're full now, they will typically have space before the actual talk. Thank you everyone, uh, enjoy your reinvent.