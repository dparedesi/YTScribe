---
video_id: PGfyJX8eS48
video_url: https://www.youtube.com/watch?v=PGfyJX8eS48
is_generated: False
is_translatable: True
---

Hey, thank you all for coming. This is Advanced data modeling with Dynamo DB. I'm Alex Debris. I'm really grateful for y'all showing up here on a Wednesday morning. Uh, this is my 7th year talking at Reinvent, so. You know, in an hour I can't cover everything about Dynamo DB. I try to cover some different stuff every year, but if you wanna look at previous years, you know, these are all on YouTube. You can cover those sort of topics. Um, additionally, there are a lot of like really great talks this year, um, from all over the spectrum, data modeling, you know, architecture. There's a great one by Craig Howard that's tomorrow about the service disruption that just happened. So like a lot of really great stuff. Um, some of these already happened, some of them you probably can't get into, but like check these out on YouTube as well, really great speakers for all these. All right, in terms of what we're gonna talk about today, I'll start off with a little bit of background and some data modeling sort of goals and process, and then we'll, um, you know, dive into some topics around using secondary index as well. There's a really good release we had two weeks ago that I'm, that I'm super excited about, um, talking about scheme evolution in Dynamo DB because I, I get questions around that a lot. And then just like a quick anti-pattern clinic around some anti-patterns I see and how you can, um, what you can do instead. I like to think about it as, hey, we wanna first set some like baseline facts just so we're all on the same page, maybe have some like higher level concepts and things we should be keeping in mind as we're doing this modeling and then some actual application we're applying it to our data modeling. Um, I'm Alex Debris, AWS Data Hero. Um, yeah, you know, spoke here, uh, a few different times. I will say, um, I'm gonna talk fast. I have a ton of slides. They're, they're worried I'm actually not gonna get through it all. Um, so I probably won't be able to do Q&A in here, but I will do Q&A out there if you want to. I will also be at the Dynamo booth in the expo hall like most of this afternoon. So, you know, bring questions, bring your friends, let's talk Dynamo. All right, let's get started with the background, and I wanna start off with some unique characteristics of Dynamo because Dynamo is a unique database. It's different, like most people know relational database, right? And if, if you wanna model well in Dynamo, you need to learn some different things, you need to change what you're doing. You need to teach a lot of people on your team, so. Given that, I think you should be, you know, if you want to use Dynamo, you should want one of its unique strengths and, and unique characteristics. So the three that I always think about are, hey, number one, it's fully managed, right? And it's fully managed even in a different way than like RDS or some, uh, managed database service for you where. Hey, with Dynamo they have this region-wide multi-tenant, multi-service, self-healing giant fleet with storage nodes, load balancers, request routers, all sorts of different stuff where like you cannot take down Dynamo DB, right? That's not gonna happen like you could overload um a relational database or OpenSearch or some other database like that, right? So it's really fully managed and, and pretty hands-off operationally, um, compared to most other databases. Now in addition to that, it has a consumption-based pricing model that I love, right? Because you're not paying for that instance, you're not paying like a traditional database, you're not paying for CPU and memory and IOPs and however big your instance is on that stuff. You're actually paying for consumption. So with Dynamo DB you're paying for read capacity units, right? Actually reading data from your system. Um, every 4 kilobytes of data you're gonna consume a read capacity unit. Same with rights, every 1 kilobyte of data you write to Dynamo, you're gonna pay, you're gonna consume a right capacity unit. And then you're also gonna pay for the data you're actually storing. Um, and there's some unique implications of that. Right, I would say the big one that I like is like it's very predictable billing impacts, right? If you have an existing application and you wanna add a new secondary index, you should be able to, you should be able to calculate pretty easily how much it's gonna cost to backfill that index, how much it's gonna cost going forward, knowing your right patterns. Um, I also say like a bunch of Dynamo sort of costs you can do in Excel. You don't have to actually like spin up a database, see how much it's actually using. You can do this in Excel and it's pretty um nice to do that. There's also like a really tight connection between how efficient you are, how you sort of model and think about efficiency, and your bill. So like Dynamo is pricing it in a way that's like giving you signals on how you should be using it. I'd say take those signals, use it, and you'll save a lot of money on your bill. So I love that consumption-based pricing model, um, and then performance wise, you know, I think the, the one people think of is just consistent performance at any scale where you can have dynamo tables that are small, there's lots of, you know, megabyte level tables, but there's also lots of terabyte level tables, petabyte level tables, and Dynamo gives you consistent performance at any scale, you know, no matter which, which size you're at there. So I would say like these three things, hey, ops, economics, performance, like at least one of these should stand out to you of like. I want that from Dynamo, that's why I'm gonna learn Dynamo, that's why I'm gonna, you know, change how I do some modeling, um, to make it work within Dynamo. Let's do just enough architecture to understand that consistent performance at any scale part. So, you know, if we have an example, uh, data set here, this is, um, some users in my application, right? Those are gonna be contained in what's called a table. So you'll create a Dynamo DB table to store your records. Each record is going to be called an item. When you create your table, you have to specify a primary key for your table. Um, that primary key must be included on every record and must uniquely identify every record in your table. In addition to that, when you're writing items in Dynamo, you can include other attributes on it. Uh, you don't need to specify these attributes upfront. There's no schema that's managed by Dynamo DB itself, other than that primary key, right? The, the schema is gonna be managed within your application layer. Those attributes can be simple scalar values like strings, integers, or they can be complex attributes like lists and sets and maps. So that primary key concept, let's talk about that a little bit because it is so important to how Dynamo works, right? There are two types of primary keys when you're creating your Dynamo DB table. There's a simple primary key, right? Just has a partition key, that's our user's table, just, um, has that username that's unique identifying every record in our user's table. Uh, but we also have a composite primary key, right? And this is probably more common depending on your use cases, but, uh, composite primary key has both a partition key and a sort key. So we can imagine in our application, maybe users can make orders, right? In that case, we might want to use this composite primary key which has these two different elements, uh, that partition key of a username and that sort key of, um, an order ID. Note that each one of these are distinct items, even though some of those share the same partition key, the key there being, hey, the combination of that partition key and sort key needs to be unique. Um, again, with that primary key, you're gonna choose this type when you create your table, simple or composite. You can't change the key names or, or anything like that after you create your table. Um, and again, the combination of that primary, that, that primary key, whether simple or composite, has to be unique for each item. All right, now as you look at that primary key, both of those elements have that partition key element, which is probably like one of the more important things you need to know about Dynamo DB and using it well, right? Where when you're creating a Dynamo DB table, you create your table behind the scenes, they're gonna be spreading your data across multiple different partitions. So let's say you start with two partitions. Now, when a right request comes into Dynamo, say you want to insert a new user into our table, that front end, that request router, it's gonna look up the metadata of your table, understand what its primary key is, look at that partition key value, gonna hash that value and then figure out which partition it needs to go to. So in this case, that item belongs to partition one. And the great thing about this is, as your data grows, Dynamo behind the scenes can add a 3rd partition. Add 6 more partitions, add 1000 more partitions. It doesn't matter. That, that first step, right, of figuring out, hey, this, this item came in, which partition does it belong to? That's a constant time operation. So even if you have one of those 10 terabyte tables, it's still gonna be a constant time operation to get down into about 10 gigabytes of data. And those part partitions are all managed for you. You don't have to spec, you don't have to add new partitions. Dynamo is doing that behind the scenes for you. So that's where that consistent performance at any scale happens and I think again the most important thing you need to know is, is the combination between partitioning and how the Dynamo DB API like exposes that to you, right? Because again, these items are gonna be spread across your partitions by that partition key, and then you wanna be using that partition key, using that whole primary key to identify your items and rather than giving you a SQL interface, it's more like, you know, declarative and sort of having a query planner behind the scenes, you're basically getting direct access to those items through the API, right? So Dynamo has. What I call single item actions, these are all your like basic crud, right? If you're inserting a record, reading a record, updating a record, whatever, um, you're doing that with your single item actions. In this case, it requires the full primary key. You need to specify the exact item you want to go and manipulate in that case, and all your right operations are going to be with those single item actions. So there's not like an update where, where you can update a bunch of records given some predicate. Now if we go back to that composite primary key and think about that, and we think about how that partition key is used mostly to assign records to the same storage partition, we can see that, hey, records with the same partition key in this composite primary key table are gonna be grouped and stored next to each other, right? And then they're gonna be ordered according to that sort key. So it's really useful to us because sometimes we need to get a set of related records and that's where Dynamo gives you the query operation as well, right? If you have a table with a composite primary key, you can do this and you can fetch multiple records with the same partition key in a single request. Now, when you're doing this, it requires the full partition key, right? Cause it needs to know which partition to go to to actually fetch those records. And then you can also have conditions on that sort key to say, hey, maybe, uh, values before the sort key value or after the sort key or between those sort key values. Finally, it's only gonna let you get 1 megabyte of data per request, which is how it gives you that consistent performance at any scale. It doesn't want you to have a gigabyte of data that you're now you're pulling back and it's gonna just, um, change your response times there. Additionally, the Dynamo API has a scan API, um, you know, just fetch all. I'd say you're gonna use this pretty sparingly, um, other than like exports and, and different things like that. So given this like partitioning and the API, um, my sort of like mental model for Dynamo is when I make a request to Dynamo, I get one contiguous disk operation from some unbounded amount of storage, right? Dynamo is just giving me this infinite fleet of storage that's gonna expand for me as, as much as I need to. But I need to physically think about how I'm setting it up within my application to make sure I can get what I need when I need it, right? So if I wanna insert a new record, I need to identify, hey, what's the primary key to identify exactly where that goes, right? If I need to read a record, what's the primary key so I can find that very easily. Or if I need to fetch a set of related records, how am I arranging that so I can use that query operation? They're all grouped together, they're sorted as I need them. I can read up to a megabyte of data there, right? So that's the big thing here is like, hey, every API request, you get one contiguous operation from an unabandoned amount of storage rather than, again, like with SQL, you get a query planner that's maybe hopping all over the disk and doing a bunch of different reads. And given that, given that you get, hey, basically one contiguous read, I think like, I love the Dynamo API. I think it works really well with that partition key, and you need to know that. And given that, I would say don't start with the particle API. There is a particle API which is like a SQL-ish API you can use to query Dynamo. Under the scenes. It's basically just turning it into one of these operations, a single item action, a query, something like that, um, but I think it hides a lot of like what you should actually be thinking about in your application. How do I wanna arrange my data, um, to fit with that dynamo mental model. All right, last quick concept we need to run through is, um, we've seen on, on this table, right, we can, um, fetch records by that primary key. I can fetch the Alex Debris record, I can fetch Madeleine Olsen by the username, but what if I want to fetch someone by a different attribute, right? By their email address. And that's where the concept of secondary indexes come in, where you can create secondary indexes on your table and it's basically a fully managed copy of your data with a new primary key. And this enables additional read-based access patterns. It's basically repartitioning your data, uh, reorganizing them so you can fetch them more easily, right? So we can go back to this table here, we can create a secondary index on that email address, and now we have this email index which allows us to fetch a record, um, by a given email address. Um, there are two types of secondary indexes. The first kind is a global secondary index. You should prefer this in almost all cases. Uh, second type is a local secondary index, which you should really understand before using. Uh, I was gonna do a deep dive on like why and maybe when you should use LSIs. I had to cut it for time, but I would just say like an LSI is, is kind of a one-way door and it has some serious downsides. So make sure you like really understand an LSI before you're, you're putting one on your table. All right, so summary again, just to get us background and, and on the same page with some basic facts, right? Make sure you understand the concept of partitioning, using that partition key to spread your items across your your table, and then um how the data, the Dynamo DBA API works to provide you that consistent performance at any scale, right? The importance of the primary key in that. Then how secondary indexes enable additional read-based access patterns and then that consumption-based billing, which I think is unique and pretty interesting about Dynamo. Oh yeah, because you predictable and visible billing there. All right, let's move on to just some high level concepts around data modeling goals and process, and the first thing I would say is like, hey, you wanna keep it simple as much as possible. I think, um, a lot of times when I see error or like issues with people's Dynamo data models, it's just like more complex than it needs to be, and I was guilty of this for a long time as well. Uh, I saw this recently on Twitter about just like how a novice just does too much in a lot of different areas, right? And the, and a master really uses the fewest motions required to fulfill their intention, right? So keep it simple as much as you can. Um, and I like to think about, hey, regardless of what database you're using, what are like your modeling meta goals that you need to really think about and keep in mind? What are we doing when we're doing data modeling? And number one, you have to think how do I maintain the integrity of the data that I'm saving because ultimately like a database is, is like serializing and storing some state of the world, whether that's representing physical objects like inventory, offices, people, something like that, or, or digital stuff like social media, likes and comments and things like that, but you need to be able to maintain the integrity so when you read that back out and represent it right, you can still understand what you have in your database. And then within me, additionally, in addition to maintaining the integrity, you need to make it easier to, easy to operate on the proper data when you need it, right? If I have one of those big tables, how do I get down to just the records that I actually need? All right, so thinking about that in the constant context of Dynamo DB, how do we apply that stuff, right? Thinking about maintaining the integrity of the data you're saving. First thing you have to do is have a valid schema, right? Cause Dynamo is not gonna maintain your schema for you like a relational database would, right? So Dynamo is a schemeless database, you can just write whatever attributes you want in there, which means you're gonna maintain that in your application code. It's an application managed schema rather than a database managed schema, right? So you should have some sort of valid schema in your code. You know, I use a lot of type scripts. I use Zod, but you can use, uh, whatever you want. But when you're writing records into your, into your Dynamo table, especially if you're inserting full records, you should almost always be validating, hey, I know what I'm writing in, it's valid data. I can put it in there because you don't wanna put junk into your database. Same thing if you're reading data back from Dynamo, you know, you should, you should parse that out, make sure that shape matches what you expect it to match, and if it doesn't, like throw a hard error rather than like limp along. You like you should understand, hey, I'm, I'm getting something back that I did not expect, like where did that issue happen? You don't wanna keep corrupting your data worse and worse over time. So that's a big one, hey, make sure you have a valid schema somewhere in your data. Um, additionally, when you're maintaining the integrity of your data, um, you know, you wanna maintain constraints across items, right? You wanna, if, if you have some uniqueness requirements, you don't wanna have multiple users with the same username, right? You need to maintain uniqueness that way, or maybe you have some sort of limits you need to enforce in your application. How are you gonna do that? That's where dynamo condition expressions are gonna be your friend, uh, maybe transactions, um, but think about those constraints and make sure you're modeling for them. And then with Dynamo, sometimes we duplicate data, right? Sometimes we do a little bit of dennormalization. So think about how do you avoid inconsistencies there, right? So when, when you're duplicating data, think, is this data immutable? Is it ever gonna change, right? And sometimes you will have like functionally immutable data. If someone makes an order and you wanna store that payment method that they use for the order on that, you know, even if they change something about that payment method later on, you don't really need to go change that order. You're capturing a snapshot of what the payment method was at that time. But sometimes you'll be duplicating mutable data, right? So if I have mutable data, how am I gonna update my data when it changes, right? First of all, how many times has it been copied? Has it been copied 5 times or 1000 times? And how I identify the items to which it's been copied where I need to go update all these different records uh for this data that's changed. And probably the hardest question now, how quickly do I need to update it? Do I need to update it in the same request, where if I'm updating that parent record and now it's been spread out to 5 different items, do I need to do that in transaction to make sure they're all updated at the same time? Or can I do that update asynchronously? Am I gonna have data inconsistencies across that? What does that mean for my users or clients or whatever that looks like? So be thinking about this when you're duplicating data. So that's, that's what I think about when I'm talking about maintaining the integrity of the data within Dynamo, um, and then you also wanna think, how do I make it easy to operate on the right data when I need it. That's where our primary keys are coming in, right? If we're writing, think what's, what's my proper primary key to maintain uniqueness, what's the proper context? How do I canonically identify a record that I'm always gonna have available that I can, I can use to address that relevant item for, right? And when I'm reading, what are, what's the primary key structure? What are the indexes I need to filter down efficiently rather than significantly overreading. And filter after the fact. So those are some medical goals we'll keep in mind as we look throughout this. Um, just a quick buzz through the data modeling process. I always say like, hey, most of data modeling pro most of Dynamo data modeling happens before you write any application code. You should be thinking about this, writing it down, and then the implementation aspect is actually, uh, pretty straightforward, right? So first thing you need to do is know your domain. What are the entities in my domain that I'm actually modeling? What are those gonna look like, right? What are the constraints that I have. In my application, right? What's the data distribution, right? If I have a one-dimension relationship, how, how many can that related, um, aspect be? Is it, is it 10 related items per per parent, or is it 1000 or a million or something unbounded? Right, how big are these records because that's gonna affect modeling and, and some of the choices I'm making there. Then with Dynamo, you wanna know your access patterns up front and model specifically for your access patterns with your primary key, right? And I always say be very specific with this, right? You should, you should actually list out what are my right-based access patterns and, and go through those mechanically. Same thing with your read-based access patterns. How can you, uh, as you're sort of modeling your primary keys, you should make sure I know how to handle each one of these. If I have conditions in my right inspections, do I have that set up properly, all these sorts of things. So know your access patterns and then last thing, uh, just know the Dynamo DB basic, basics, the things we talked about before, the primary key API secondary indexes, that's gonna do most of it for you. So please just like keep it simple on this stuff. I think the basics are gonna get you a long way, right? Using those single item actions for your right operations for your, your individual reads, um, using some queries for range, range, um, lookups and list operations. Sprinkling in those secondary indexes when you need them for additional read-based patterns, and then sometimes using transactions, uh, sparingly for, for certain operations. All right, so that gets us out of sort of background conceptual type stuff. Let's go apply it somewhere. And I want to start off talking about secondary indexes, um, and the reason I want to talk about this is because there was a huge release, uh, just two weeks ago about how Dynamos now supports multi-attribute composite keys for your GSIs. This is a huge release. I think this will simplify a lot of things for people, um, but in terms of like walking through why this is useful, let's look at an example table we have here. That's, that's just tracking orders within a ware within, um, some system, right? We have multiple warehouses, we have assignees within those different warehouses that have to go process those, pick them, make sure they're all ready to be shipped, and different things like that. So we have these, these different attributes on our table. And we might have some sort of access pattern that says, hey, for this warehouse, for this assignee, for this status, like what are the things that they should be working on, right? And so you might see some sort of attribute like this in your table, GSI 1 PK GSI 1 SK, which are like these synthetic keys made up of other attributes that are already in your table, right? If you look at this, we have the warehouse ID is put in there. Then we have a pound sign, and we have the assignee ID jammed in there. Then we've got status, we've got priority, we've got created at all this is like made up into these synthetic keys in our GSI, uh, PK and SK. So this was a very common pattern we used to have to do these synthetic keys, right? Where we're manually combining these attributes to, to partition, to group, and, and then sort as needed, right? And it looked like sort of these, um, and I didn't realize how much I disliked these until this new release came out um, because there's a ton of downsides to this, right? And the number one is just the item size bloat. If you go look at that item that we have in our table. The meaningful attributes in our in our thing there are 100 101 bytes, so pretty small record, and if you look at the, the other attributes, these, these sort of synthetic keys, they're 92 bytes. So this is almost half of our item and it won't always be half of your item because right you'll actually have like larger other attributes there, but it will be, you know, if you have um. Two indexes you have GSI 1 PK, GSI 2 PK SKs as well. You might be talking about 200 bytes, which is 200 bytes you're, you're storing on every single item. You're paying storage for that. 200 bytes is 20% of a WCU, so it's likely to kick you over the WCCU limit a lot of times. So every time you write, you're paying for an extra WCU and um into index replications as well. This is just, this is just a lot of cost for very low value. These attributes are already on your table. So there's item size bloat, there's also just like the application modeling complexity when you're doing all your access patterns, setting up these indexes, you have to think about putting all these together and have I done it right? Have I implemented it right in my application. There's sorting complexity, um, just around like, um, you know, you're taking all these attributes and turning them into a string, but if one of those attributes is an integer, now you have to sort that integer like a string, so you have to zero pad it to like the longest length it could potentially be and and think about that sort of thing and then just the update logic. It gets harder, right? If someone comes and says, hey, update this, the status for this order, it's now, it's no longer pending, it's, uh, it's prepared or whatever. I have to know all those other values, and if I don't know all those other values, and I have to read that record to pull down those values just to do my update, right? So it's, it's kind of a pain to do all this sort of stuff. So that's the old way, right? But now we have these multi-attribute composite keys. Um, the way that this works is you can support up to 4 attributes each for the partition key and the sort key when you're creating your secondary index. Right? So if we have our existing table, and we wanna use this, this multi-attribute composite key pattern, what we do is when we're creating our partition key, we say, OK, I want my first element to be that warehouse ID. I want my 2nd element to be that assignee ID. I want that 3rd element to be my status. And same thing with the sort key, right? I want the 1st element to be that priority. I want the 2nd 1 to be that created app. And now I don't need those GSI 1 PK GSI1 SK values at all anymore. They're just reusing those when they create my secondary index to know how to sort of, how to set up that index, right? So if we go back and look at those downsides, how does this um work with our multi-attribute composite keys, right? We don't have that item size bloat anymore because it's actually reusing the actual attributes in our table. We don't, we're not bloating it up with another 100, 200 bytes, um, on our table. Right, it's a lot easier to reason about because I, when I'm, when I'm writing or updating an item, I don't have to think, OK, which other GSI synthetic keys do I have to update as well. We don't have that sorting complexity. If one of my um partitioner short key attributes is an integer, it sorts like an integer. It doesn't sort like a string, so you can just do normal sorting on it. And then my update logic is a lot easier because again, I'm only updating the actual attributes in my table. They're handling all the, all the work for that. So that's the old pattern why this is better. Um, in terms of how it works, right? Um, you get up to 4 attributes each for your partition key and sort key. So you can still just use one attribute for each if you want to, but you can specify up to 4. Now, when you're doing um a query operation, you have to specify, uh, in your key condition expression, you have to include all your partition key attributes, right? Because it's, it's the same thing, you still need to know exactly which partition you want to go to, where this data is located, so you need to make sure you have all your partition key attributes in there. You can do conditions on that sort key as well. Uh, the important thing is that sort key is gonna be ordered. The ordering of those attributes matter, and I would, I would think of it like a SQL composite index, right? It's basically left to right, no skipping, stops at the first range, right? So if you have 4 values in your sort key, you know, you can match on all 4 of them, you can match on the 1st 3 and do a range on the, on the 4th 1, but what you can't do is, is sort of do an equality match on the first attribute. And quality match on the 3rd attribute but not provide a value for the 2nd attribute. It's gonna stop at that one and, and just scan there. The one thing I will say is this will not solve your, your overloaded index issues, probably, right? So if you're doing like single table design in, in this case, right, where we have some user entities in one table, um, we also have some organization entities. And you can see we have these GSI 1 PK GSI1 SK values here for a secondary index. And if we look at our secondary index, right, we have an item collection, which is a set of records with the same partition key. We have an item collection that contains two different types of entity, right? We've like pre-joined these, these entities, org and user in that same item collection. Um, this is gonna be hard to do with those multi-attribute composite keys because it's unlikely they're gonna have the same attribute names across these different entity types, right? So like for our partition key, they, hey, they both have org name, we could use that as our partition key, um, but you can see in the sort key like the, the username is coming from that username on the users. The org is just like this static string that we have here, so I, it'd take a little bit of work to do this if you're doing these overloaded index patterns, so it probably won't help that one there, but in all other cases, hey, this is gonna be a huge win for you. All right, so secondary indexes again, use these multi-attribute composite keys like huge. I would, I would use this for almost all cases except for those overloaded indexes. Honestly, for existing tables, this might make sense too just to save on like item sizes like create a new index with this, switch over to it and, and start and, and drop your old index and you can stop writing that, that synthetic key that could actually save you money depending on your, your use case. Um, in addition to that, let's just talk about cost management with secondary indexes because I think this is undervalued because. Hey, every time you're writing to a secondary index, it's gonna consume WCUs. Like, secondary indexes are a read time optimization for which you pay rights for, but rights are more expensive than reads, right? They're 5 to 20 times as much as reads. A write capacity unit costs 5 times as much as a read capacity unit, but it's also only 1/4 of the max size, so it's, it's gonna be 5 to 20 times as much as reads depending on the size of your items. Um, so make sure you're getting the value from that. Um, just some cost management tips on secondary indexes. The first thing I think is, do I actually need a secondary index? Because I think a lot of times we'll write our access patterns, we'll solve that first access pattern with the primary key in our base table, and then we'll say, OK, every other read-based access pattern, I'm just gonna slap on another secondary index for that. But now in this case, right, I have 3 secondary indexes. Every time I write my item, I'm gonna have to replicate to each one of those. My right costs are now 4 times as much as they would be, right? So, make sure you actually need all your indexes. Um, sometimes you can reuse secondary indexes for multiple different use cases. I'd say the two areas I usually see this is like if you have a really high correlation or overlap between different read patterns, sometimes you can do that and, and I had a talk recently, which was like an order delivery app, um, I think like DoorDash or something, um, and imagine you want to show all the orders for a given restaurant over time they wanna say, hey, what orders did I have last month or the month before that, they're, they're grouped by restaurant, ordered by, uh, time. But also that restaurant wants to say, hey, what are my active orders that I should be working on now? I wanna put up on the board in my restaurant to make sure people are working on. Well, the thing is, like, all your active orders are gonna be the most recent orders, right? You're not gonna have an order from 2 weeks ago that you forgot to deliver and you need to be working on now. So like you just look at the last 50, 100 orders, filter out those that are actually completed, and those are your active orders. You don't need a secondary index for that one. The second place you can reuse a secondary index is just like when your overall search space is pretty small, right? So like searching for admin admin users within all users. Like if you have a SAS application where, um, you know, organizations sign up, they have lots of different users in there, and somewhere deep in your user management page you wanna look for just who are the admins in my application. Well, if you only have like 100, 200, 300 total users max within a given organization, you probably don't need this separate index just to show admin users. You can just fetch all the users and then filter down to admin after the fact, right? And my rough rule of thumb here is like if fetching that total search base, in this case, like all the users for a given organization, if that's less than 1 megabyte, which is one dynamo query page, I'd say usually don't need a secondary index for that, depending on how many times you're reading from it and, and different things like that. All right, so that's the first one, right? Do I need an index at all? Can I avoid having an index? The second one is, if I do need an index, do I need to put all my items into my index, right? And this is where the sparse index pattern shows up. Um. Thing about secondary indexes, right, is Dynamo is only gonna write items to your secondary index if they have all the index key attributes. So if you have that GSI 1 PK GSI 1 SK, or if you're using these multi-attribute composite keys, it has to have all those different elements to be replicated in your secondary index. And you should use this to your advantage, right? If we go back, we had that orders table that we showed um in the beginning, let's say we had an access pattern that said, find all the orders for a given customer that had a support interaction. Right, maybe what we do is when an order has a support interaction, we add this like support interaction at timestamp app on it just to indicate when that interaction happened and notice that not every record is going to have one of these. If we set up a secondary index on that on that table using that support interaction, right, we may be partition by that username, we sort according to that support interaction at. Now when we have our order support index, it's only gonna have the subset of items that have both of those attributes, right? We've filtered out all the records that don't have a support interaction. So yeah, again, use this to your advantage both from a modeling perspective. This is like a global filter over your data set. We basically said where support interaction is true, right? You wanna be doing filtering with your primary key, with your partition key, with your short key, but also with, with your secondary indexes using that sparse index pattern is another good way to filter data. Um, but it's also just gonna reduce costs, right? Because now you're not paying to replicate those items that you're never gonna read from this one into your secondary index, or you don't have to like overread and, and filter out records that don't match your, um. Match your conditions there. All right, so that's the second cost management tip, right? First of all, do I need an index? Second, do I want all the items in my index? The last one is, do I need the full item into my index, right? Where you can choose how much of that item to project into your index. And I always used to say, hey, just project the whole dang thing. Um, but that actually can get really costly, um, in a lot of ways, right? So think about our user record again, and maybe we have a user detail page that has a lot of information about that user, right? It's got this long bio section. It's got preferences, it's got an activity log, maybe we're pers persisting some of their most recent actions on there, just like a lot of stuff. But if we have a list users access pattern, we don't need almost any of that data, right? If you look at that, we need a user ID, name, email, just a few little bytes of information. So if we're creating a, a list of users in an organization access pattern, um, we don't need to, to replicate all that to project all that data into that index. Right, so with your index projection, just like think carefully about which attributes you're projecting into that secondary index. Because there's significant savings you can have from not doing that, and it comes in three ways. Um, number one, it's just gonna reduce the WCUs that are consumed for a right, right? If I have this 5 kilobyte user record, but I'm projecting less than 1 kilobyte of it into my secondary index, I'm reducing my WCUs from 5 to 1, that's 80% savings right there. But even better is like, hey, it prevents some, some rights entirely. If a user goes and updates their bio, and I'm not replicating that to my secondary index, it's not gonna update that record in my secondary index. I skip that right entirely, now I save 100% of that right, um, for that secondary index. Additionally, it's gonna help you on the read side because now when you're reading all those users, you're not paying, you know, over WCU for each record you have there, you're just paying for a much smaller, um, item, it's gonna reduce the number of pages when reading, so really like think about your projections carefully around this stuff and I'd also say it's not a one-way door. You can create secondary indexes after the fact, we're gonna look at that in the scheme evolution section, but like. You can, if, if you need to change your projection over time, you can create a new secondary index either with a larger or smaller um projection and then drop your old index and start reading from the new one. All right, so that's secondary indexes. Again, key takeaways here is use this new multi-attribute composite key pattern, uh, wherever possible. I think it's a, a huge addition, really gonna reduce your costs and simplify a lot of your logic there. Look into sparse indexes for um global filtering over your table. And then think about that index cost flow. Do I need that index? If I do, do I need to replicate all those items into my index? And then finally, do, do I need to replicate the full item into that index? All right, next, let's talk about schema evolution. Um, So I get this question a lot, uh, where, hey, we talked earlier about how you have to know your access patterns in Dynamo, and that leads a lot of people to say, hey, Dynamo's great if your application will never change. Don't screenshot that. I'm, I'm putting that, yeah, cause I don't think that's true. Like, I think like I've worked on a lot of different Dynamo applications, um, they've all like sort of evolved over time in different ways, um, so I don't think that's true. What I think is true is that certain patterns are always gonna be hard to model in dynamo, and sometimes those come up later and then you feel frustrated thinking, um, you know, this is too hard to do. So let's talk about some patterns that are just always hard in Dynamo before we actually move into scheme evolution, right? The big ones are gonna be, if you have any aggregations around your table, right? Dynamo doesn't have native aggregation functionality. You're gonna have to sort of write it in your application code. So if you have questions like, hey, how many transactions has this customer done each month? Or like, what's the largest purchase done by customer Y? It's tricky for Dynamo, you're gonna have to manage some of that yourself. I think the more common one, and the one that comes up when people are saying, hey, Dynamo can't evolve. If you have complex filtering needs, which I say is like, hey, if you're filtering or sorting by two or more properties, all of which are optional, that's really hard, right? Let's say you have just like a list of records in a table view, you wanna show your users lots of different attributes, and you wanna let them choose which, which fields you're filtering by. You wanna sort by different things, that's really hard, right? If I can go and say, hey, find me all the chips by this company. Actually, you know what, all the trips by this company that came out of the Omaha airport and maybe we're over 500 miles and within this time range, like this is gonna be a really hard pattern to model in Dynamo, even if you knew this on day one before you wrote any code for your table, like this is a hard pattern to model. Um, you can't, like, I talked about complex filtering more the last couple of years, so you can look at that. There's some ways to do it in Dynamo. Sometimes you wanna use something else like OpenSearch or QuickHouse or something like that. And sometimes you're just like, hey, you know what, this would fit better in a different database. But like complex filtering is a hard one to do. So, this is true, like, certain patterns are always hard and dynamo, and if they come up later, they're gonna be hard, but that's not because evolving dynamo is hard, it's because this pattern is hard and dynamo. So I do want to talk about more traditional scheme evolutions that you do see and how you, you handle that in Dynamo for, for access patterns that actually fit within Dynamo. Um, I'll do that with a just an example here, uh, which is a support ticket application, right, where we're, uh, you know, customers can come file support tickets, they get assigned to different users, uh, we'd have some sort of uh table like this with that partition key, uh, our tenant ID, we have our different tickets all in this table that have our different attributes on them. Now, as your application is evolving, I would say, hey, first you want to understand the type of evolution you're performing. And I think there are 3 main types of evolution that you're gonna see pretty commonly in your application, and the way you handle them is, is just a little bit different, right? So the first one is, you might have a schema change that does not affect data access, right? You're not fetching based on this, on this scheme of change. So if we go back to our support tickets here, maybe just on the left here. We've added these little badges, right, based on the tier of the customer, right? Maybe they're they're platinum customer, maybe they're gold. All we're doing is helping our support agents understand what that customer tier is, but you see there's no filtering on on that customer tier or anything like that it's it's just purely uh this little badge that we're putting on there. So in this case, right, we're adding a new but unindexed attribute. We're not indexing it. And this is generally the, the easiest type of evolution to handle with Dynamo DB, right? We talked about how Dynamo DB is schemeless, so that means you can just start writing new attributes to the table for new items as you want, right? So we have this new customer tier attribute that we're starting to write to our item. Notice that some items don't have them, existing items might not have this customer tier attribute, um, and we're OK with this, right? This is just like if you're changing your Shiegel table to add a new column with a default value, um, but now that default value is probably gonna be in your application code rather than a dynamo, right? So this is the easiest type of evolution. Again, what you wanna do is update that schema in your application code, like that's gonna be, uh, mostly where you handle that, add default values, change your schema to handle that. So we talked about having that valid schema, that modeling meta goal before, you know, if we had our different ticket schema here. We might add this new customer to your attribute on the bottom. It has the different values it can be. It has a default for items that don't have that particular record, um, depending on how complex our schema changes is, maybe now you need some like versioning of different schema, and the first thing you do is sort of detect that version. Maybe you have to parse the, the ticket differently based on what version it is and then sort of normalize it into some sort of schema, but you can, you mostly handle this within your application code. So, that being said, you can handle it complete within your application code. However, you might decide, I do actually wanna backfill all my existing data, right? There are a couple of reasons for that. Number one is just like you might end up with schema bloat over time where you have 20 different versions of your schema and, and it's hard to reason about like, OK, if I have a V2 item, how does it get to a V16 item or or something like that. So it might be easier rather than managing that where you just say, hey, I'm gonna backfill my existing records and, and handle those. Or another thing is you might be exporting your data to external systems, OpenSearch for Search or ClickHouse or S3 and Athena for, for analytics, right? And while you can handle the default values in your application for your OLTP stuff, now you also have to like communicate all those, those values to whoever's maintaining those systems, and it can be just hard to deal with. So you might just for long term data hygiene reasons, you might say we're gonna backfill, update this new value on existing items. If that's the case, now you're out of this first type of evolution, you're gonna be into the 3rd type we'll talk about in a second, but at the very least, um, what you can do is you can handle this completely in your application code. All right, so that's handling a scheme of change, adding new attributes, renaming attributes, things like that, that does not affect access. And again, this is a mostly easy application only change. The second type is a new index on an existing attribute, right? So if we go look at our support tickets, at first we're just showing them in a flat list, we have no filtering on them. And that works well when we have 5 tickets, but over time we're gonna have 5000 tickets or 500,000 tickets or 5 million tickets. And now we need a way to filter down to, hey, just my tickets. I'm a support agent, right? I wanna filter by that assignee, so I can say, hey, just give me the tickets that I have, right? And so this is going back to our modeling, modeling meta goals, this is making it easy to operate on the right data when we need that data, right? So what we need is a new index on an existing attribute. And the good thing is global secondary indexes can be added at any time. So you can go in and add a new secondary index to your table, and Dynamo is gonna handle that for you. If we go back, you can see here we already have this assignee value. We can set that up as our partition key. We can use uh created ad or ticket ID as our short key. Dynamo's gonna do the work to backfill that for us, and now we can query, um, from that accordingly. So they can be added at any time. Um, the general process for this is number one, you go, you add that index, you know, whether that's in your infrastructure as code tool or maybe you just do it directly in the AWS console, but you add that secondary index. Dynamo is gonna kick off a backfill for you and basically scan all the existing items in your table and write them and replicate them into your secondary index for you. Once that backfill is done, then you can start querying your secondary index. You can't query it until that initial backfill is done, so that's where you add the application access pattern, um, to start reading from that index. Right, so, again, this is a, a fairly easy, straightforward change, which is like I wanna access my existing data in different ways. Again, Dynamo is gonna do that backfill of your new index for you and it's, it's not um particularly hard there. And the third type of evolution that you'll go through, um, commonly is just now we need to make some change to existing data. And as we talked about doing a backfill before, um, the example I came up with here is, hey, we have a lot of records, you know, there's things like priority and status, but maybe what we want. Is to add this urgent button where I can filter down to just the most urgent tickets and things I want to handle there, uh, and if I click that button, it fil filters down these urgent tickets so I know what I wanna get down to. And let's say that that access pattern is, is kind of funky. It has some business logic, right? What is an urgent ticket? We've decided it's tickets where the priority is P1 or P2. The status is either open or pending, and then we want to order them by created at. And if we go look at the existing items in our table, there are some items that meet this criteria, but there's not really an existing attribute that we can use to index and and filter and get down to, hey, just these urgent items here. So what we might have to do is, when we're writing records to our table, when we're updating records, if it meets that criteria based on our business logic, we also add this is urgent true flag to it, right? Just something to indicate that this is an urgent ticket. And then we can go and create that secondary index using that partition key, you know, having multiple sort key values in there so that only the tickets that have that urgent is urgent flag are gonna get replicated into our urgent tickets GSI. This is that sparse index access pattern we were just talking about. Right, we have just those urgent records here. So this is if you need to change existing data. The problem here again is we have a bunch of tickets in our table. They don't have this urgent flag, but we need to add it for those ones that are truly urgent, right? So again, this is mostly gonna come up if you need a new index on a new attribute in your table or if you're backfilling, uh, a new attribute, even if you're not using an indexing like we were talking about in the, in the first type. So you need to backfill existing items with this new attribute. The general process for this is first you wanna update your application code to start writing this new attribute, right? Because what you don't wanna do is perform that whole backfill, and then once that backfill's done, all the new items are not getting that new attribute. So make sure you're updating your application code so you're writing this new attribute to new items going forward. Then you actually start your backfill, which means, hey, I wanna scan all the items in my table. I identify the ones that need this new attribute and run my update item operation. And then once that's done, you know, if I need a new index or whatever, I update my application code to actually use, um, my new, my new attribute there. And this feels like a tough process. Like that's not nearly as fun as just having Dynamo backfill your, your index for you. Um, there is some tooling around this to make it a lot easier, which is great. You don't have to just write your own scripts here. Um, so there's AWS Glue which can operate directly on your Dynamo DB table and do this sort of thing. Um, you can also use export to S3 to export your entire table to S3 and use Glue to operate on it there and right back into Dynamo DB. Uh, my new favorite is there's a bulk executor, Jason Hunter, who's on the, he's, he's a dynamo Dynamo DBSA. He created this new bulk executor tool. It's on GitHub, um, that allows you to do this sort of stuff and make it, make it pretty easy to perform these kind of migrations. Um, I've also seen people use AWS step functions if you're into that. Uh, if you're good with step functions, this can work. I prefer sort of the other ones, but any of those can work. All right, so that last one, that's a change to existing data. That's probably the hardest one, again, because it requires that backfill, but I would also say this is a lot less common now that we have our multi-attribute composite keys here, right? When I actually made this initial example, I was gonna, I was gonna apply it to that, that second example we had we're, we're adding a new assignee index on that. And I was saying, hey, it used to be very common. To create those synthetic keys where maybe I need to have my partition key be like tenant ID and assignee, right? So just so I'm uh making that unique across tenants, and now I'd have this new like tenant assignee value, some sort of PK that I need to add to my item, um, for my secondary index. So now I need to go and decorate every single item in my existing table to handle this new tenant assignee index, right? And that would be really annoying. Um, to do that sort of thing, those synthetic keys, but now because we have this multi-attribute composite key, what I can do is I can say, hey, for my partition key, you know, that PK1 value is that tenant ID, the PK2 value is that assignee ID, and then my sort key value is that ticket ID, and it handles all this for me. I don't have to do a backfill for these synthetic keys for my secondary indexes. So this should really help prevent those backfills, and now you're into those easier worlds of application only changes or having Dynamo do that index backfill for you. So that's a lot easier. Um, main takeaways I would say from the scheme evolution section, again, certain things are gonna be hard for Dynamo no matter when you do them. And if you find out about that use case down the road, like again, it didn't matter if you knew that upfront, like complex filtering is gonna be hard with Dynamo, so you probably should think upfront, am I eventually gonna need complex filtering type use cases? And if so, how am I gonna handle that when I get there? Am I gonna use an external system and am I gonna do it myself with Dynamo, or should I just choose a different database? Otherwise, I think most types of evolutions are pretty straightforward and fit into one of those three buckets. Um, there are certain types we didn't cover that are difficult, right? If you need to change your primary key, that is actually a hard one, right? You know, you need to like create a new table, you need to migrate, you need to dual right for a while and handle that sort of thing. I would say changing the primary key is pretty rare because the primary key is like, hey, what's the canonical way I'm gonna identify my record and, and operate on it. And I'd say usually you don't see that changing a ton, so I would say that's, um, it happens, but it's, it's not super common. Um, the more common one I see is if you need to like combine or split items, like you're doing some sort of denormalization. That can be harder to do, uh, than migration, not impossible, um, but a little harder. I would say like the more normalized your table is, the easier, um, dennormalization in some senses is good, but also, uh, the more normalized it is, the easier it is to make these sorts of changes. All right, cool. We got, we got 12 minutes. Let's zip through, um, the anti-pattern clinic here. And these are just things when, hey, people ask me, hey, will you look at my data model and talk through these things. These are things I see a lot that I, um, I think we can do better on, right? So. Just some anti-patterns I see, I think the biggest one is what I call the kitchen sink item collection, right, where you're throwing everything at the kitchen sink in a single item collection. So an item collection is all the records that say that share a given partition key. So in this case, right, someone will come to me and say, here's my data model. I have this particular partition key which is for this given user. And in this item collection, I have 44 different entity types, right? I have some payment methods for that user, I have the actual user record, I have their orders and I have their order items for all that sort of stuff. Right, and it can be OK to overload this, but What you wanna do is you only wanna put items together in the same item collection, give them the same partition key, if you're gonna retrieve them together, right? If you're gonna use that query operation to make that one contiguous read and you need to fetch those different item types in a single request. So if somebody shows me this, I'll say, OK, do you have an access pattern where you need to fetch the user record, all their payment methods, all their orders, all their order items, all that stuff, and they say, no, no, I just get subsets of it at a different time, just usually one item item type at a time. In that case, You know, you could split these up. This is the exact same thing functionally, but you don't have the complexity of putting it all into the same item collection, right, where you have a payment methods item collection that are all stored together, and now you can list all their payment methods very easily. You can fetch that user record directly. And maybe you do have item collections that combine the order and the order items for you so you can fetch those easily in one single request, but you're doing that intentionally strate strategically rather than throwing everything together in one item collection. So again, only put items together that are going to be retrieved together. Uh, the big reasons to group different item types together would be, hey, number 1, those items are frequently fetched together. You're basically pre-joining those records, that's the order and it's order items. I want to fetch those in a single request rather than making two separate requests for that. That can be a valid reason for it. Um, another reason, almost kind of the opposite reason, is sometimes you might take a single entity and, and vertically shard it, separate into two different, um, entities, right? Especially this happens if you have like a very fast moving, uh, frequently updated attribute, but you also have a lot of bigger item, you don't wanna pay for updating that entire item every time. You might split it into two different attributes and then you can query it, you can fetch the whole thing together, but then when you're doing updates, it's smaller targeted updates, um. One last reason is around if you need stream processing that has ordering across item types, but within some sort of partition key, you can do an LSI with this. I think this is pretty advanced. If you, if you really think you need this, come talk to me sometime. Uh, but those are the reasons to group together different item types. So I think that's one atom, uh, that's one anti-pattern is like, hey, we're throwing everything into the same item collection. I think related or or sort of the opposite of that is just creating an overnormalized model, right? And, and you see the world word tables, you're like, hey, I know tables from a relational database world, um, I know tables in Dynamo. You shouldn't bring your exact model from a relational database to Dynamo DB, right? You're likely going to have fewer tables in your Dynamo DB schema than you would in a relational database. Um, what you want to do here is use sensible denormalization. Where can I duplicate some data? Where can I use embedding? How can I like flatten that hierarchy so I don't need to, you know, functionally join together four different tables to handle that query? How can I put those attributes all together in a single item or a couple items? Um, but I will say like, hey, one table for all items is not necessarily correct, right? I think there's a lot of single table stuff that I've talked about before. You don't necessarily need to do it, right? It adds a lot of complexity. There's like blast radius issues. If you're doing stream processing, this can be hard if you have different types of entities, even just like configuration, right? All your TTL, your backups, uh, your billing mode, global tables, all that stuff has to be the same within a single table, and you might have different needs for different entity types. So, my general rule here is this. Again, only put different entities, if you have different entity types together in the same item collection, if you have at least one access pattern where you're retrieving them together, right? Order and order items, you wanna fetch those all together to get all the information about an order, put those in the same item collection, that's fine, but don't put payment methods in there. And then, oops sorry, given that, you should only put different entities into the same table if they need to be together in at least one item collection, right? So going back to our example, payment methods could be in a separate table, users could be in a separate table, and then keep orders and order items together in the same table, right? And you're still gonna get all the benefits with, and I think a lot of the ways that Dynamo has simplified things with these multi-attribute composite keys, with on-demand billing, with adaptive capacity, all this sort of stuff. You don't necessarily need to put a bunch of unrelated attributes together in the single table. Only do that if you're actually fetching them together in at least 11 access pattern. All right, so that's one thing I see, um, a lot. Another thing is just when people hide the Dynamo IPI sometimes they'll say to me, hey, I heard Dynamo has consistent performance at any scale. As we start to get bigger query sets, we're seeing slower operations, what's going on here? Um, it's almost always trying to abstract it with some sort of like abstract query items or query Dynamo DB items or something like that, right? Where with that query operation. Um, you know, you, you can filter on the partition key, the sort key, they take those as parameters, but then they add in like these filters and limit, and basically what they're trying to give you is complex filtering on top of Dynamo, but hiding what's happening under the scenes. And now what's happening is you have this like really selective filter, you're not understanding how limits work, it's like doing a bunch of pagination through all that to like satisfy this request, right? So I don't think you wanna hide the query API like this you wanna like actually be thinking about your data distribution, how you're gonna be filtering data, what that pagination is gonna look like and how you're gonna manage that because I think this can, can lead to issues for folks. I think relatedly on the Dynamo API just like not using the full Dynamo API, right? If you have an increment operation, I see a ton of people that first they read the entire item. They increment some value in memory and then they write that entire item back and if you do this now you need to manage um versions around that so you're not getting like race conditions you're basically doing like OCC there um it just adds a bunch of stuff where what you can do. Is just do an update item directly on that, right? You can do atomic updates to this counter record, you can increment that count, you can have conditions on there to make sure it exists and all sorts of things like that. So you don't need to do that read then write to handle this sort of update. Um, I think counters are also like a really fascinating area and there's a great post from Jason and Chris about resource counters on Dynamo, and I would say like. Counters is like one hard problem. This shows 7 different ways to handle it. It also just like is a good way to think about identitency generally in Dynamo DB. A lot of these patterns work for whatever sort of identitency requirements you have. So, uh, definitely check out that post. I was gonna talk about it and this covers it so well that, uh, I don't think I need to. Alright, so those are the big two anti-patterns I see. The, the last couple is just like some people just always use strongly consistent reads as a default, uh, with Dynamo, if you, if you opt into the default eventually consistent read, you'll save 50% on your RCU consumption there. I would say most of the time you don't need strongly consistent reads. The lag isn't that much, and, and, uh, usually it's not gonna matter. It's, it's really, um, I'd say take advantage of that eventually consistent read. Discount Um, another thing is just having really large items, not thinking about item size, not thinking about how that's inflating their costs, right? So thinking about some of that stuff and like, hey, how can I skinny down my items so it's not quite so expensive. And the last one is just like overusing transact right items. Um, the transact API I think is, is really useful and valuable in a lot of ways, um, but it does add cost, it adds latency. It also, if, if you, if you're working with an item that's involved in a lot of transactions, you have, um, concurrency issues and transact transact. Conflict issues, um, so I always like to say hey use transactions for, uh, low volume but high value operations you don't wanna be using it for every sort of operation, but if you have something that's very high value, you know, incrementing some bank balance or or or handling a transaction that way, hey, low volume high value, uh, but don't use it for everything. All right, we made it. Um, in terms of summary, hey, this is what we covered today, all those different things. I just have, you know, a few main takeaways on this. I would say biggest thing, make sure you really understand partitioning in the Dynamo API. Don't hide the API. Don't use particle, like understand what you're doing with the Dynamo API, uh, and just have a better understanding of like what that performance profile is gonna look like. Remember those data modeling meta goals, right? Your, your keys are making sure that data you write maintains its integrity, is valid, and, and can be used later and also, uh, enables you to act on it quickly, um, and get that consistent performance at any scale. Um, know your domain and consider your trade-offs, right? And in some sense you have to be the query planner. You have to know how you're gonna access your data and arrange it accordingly for those different things, uh, and then finally keep it simple. Don't, don't overcomplicate it. Hey, use these new multi-attribute composite primary keys. And that's it. Thanks for all for coming. I really appreciate it. Again, I'll be out there. I'll be at the, um, Dynamo DD booth if you have any questions today. But yeah, thanks all for coming.