---
video_id: gS_Snr7EmR8
video_url: https://www.youtube.com/watch?v=gS_Snr7EmR8
is_generated: False
is_translatable: True
---

OK, good. Good afternoon everyone. How are you doing? Good, good. Yeah. So, uh, I know we are just 60 minutes away from this session and your beer. So, uh, thank you for coming in. Uh, thank you for your time. Uh, I know it's been, been a long day for you, but I really appreciate your time. Um, as Slide said, my name is Patnesha. I'm a senior partner solution architect, uh, specializing in migration and modernizations. And, uh, I'm very excited to join you with my colleague Cedric and our AWS ambassador, uh, Gitika. So guys, you want to introduce yourself? Yeah, hi everybody. So I'm Cedric Bordone. I'm also a senior partner solution architect supporting one of our GSP partners, uh, globally. Yup. Uh, I'm Gehika, and, uh, I work as a principal consultant for AWS partner called Mentor Group, uh, which is Australia New Zealand based, and I'm New Zealand based. So very excited to share our thoughts with you. Yeah, excited. Good. OK. If you can sit down and relax. OK. So, what are we going to cover today? So, I know normal presentation, we'll start with agenda and everything, but I want to start something different. Let me start with one story. OK. So, once upon a time, um, there's an old tortoises running the post office in, in the deep jungle. And uh he's been like, you know, uh, as the post office been delivering the parcels and the uh post letters to the woodlands animals. And he's exactly following the, the best practice of the his grandfather and father has been following. But as the forest grows, so does its challenges as well. Now, due to this whole process, what he's been doing that is collecting the parcel, he's checking the parcel every time, uh, you know, following the same path, older path process, and checking everything multiple times. And due to these things, the old process actually, all the woodland animals have to wait longer time, you know, ages that, you know, parcels have been arriving 3 or 4 weeks now. So, this has been a very frustrating experience for the woodland woodland entire of the forest. Now, then come to the turning point. Um, so, one day the rabbit actually hopped into the post office and come with the very innovative solutions. They say, OK, good, it looks like, let's not overrun entire solutions or systems. Let's think about like how we can make these things efficient in a better way. So he suggested something brilliant idea. OK, let's put different animals in the group. So, like, you know, the squirrel can, you know, do certain things very fast. Um, you may have like, you know, rabbit and a monkey delivering this later very fast, and the deer can actually take heavy parcel, you know, go. So, he's trying to Bring that whole thing, new ecosystems around that. But the old, the thought are actually a bit hesitant to adopt these internal challenges. But finally he say let's adopt this new way of working it. And what happened in that case, like the post office has been transformed into the most efficient post office. Now later it's been arriving in in a Weeks, though it is taking like weeks now, it is arriving in hours and, and days as well. So, what this story actually told about is a very thoughtful Transformation of the legacy process into that. And this type of story, like if I talk about um the old post office stories are already there and you as AWS partners, those who are in here, has therefore the opportunity to understand and help your customers identify this story and help your customer to modernize the legacy workflow, right? And uh before I go into the next, I want to just quickly ask a question. Who has find certain kind of customer like our old toy toys friends? Anyone here who's helping any customers on the legacy modernizations we have here? And who has faced any challenges and resistance? So your end, yes. So, this session is exactly about that one. So, what are we covering today is like why modernization workflow is important. Um, And uh what is a typical Asiantic journey looks like. And we also have the demo, so stay tuned as well. Uh, just starting on this one is about business process transformation or business process service, uh, is undergoing the fundamental shift. And it has potential to grow, uh, basically from 214 billion from 2014 to 289 billion in 2028. So, since this landscape is in continually expanding, we also see the G AI and Agentic AI has been fundamentally transforming each and every customers and, and fundamentally, or each and every organizations. And I'm sure it's also impacting you or maybe transforming to your internal journey as well. But this is still day one with the conversion of these two different opportunities, like how do you do BPS and GNEI and agentic solutions, right? And how you can reimagine. The existing process, something into the autonomous and intelligent solution that. And so this is where, again, you as AWS partner bring the value here. You're not as a just a technical implementation, but basically as a trusted advisor and also strategic thinkers to work with your customer. You bring your AWS expertise and help your customer to modernize this entire workflow or business flow. Uh, fundamentally take from POCD production, right? What we are talking about, like how do you scale this kind of the process? Because when you think about the process, it's time consuming, very inefficient. So this is where your expertise about implementing the solution, understanding the business process, domain knowledge, and bring that help your customer to move out of the journey. Now, uh, let's take one use case, uh, and understand like how we can actually take an entire journey, uh, into, into modern additions, and I invite Si Dix to talk about that way. Thanks Prash. So our use case is about a premium wholesaler grocery item distributor, and he needs really to to improve its processes because its customer wants the goods right now. So when they order something, they want it now. Let's examine the legacy of today's workflow, and you will see how how it is cumbersome and complex. So the whole workflow starts with the customer sending a good request using WhatsApp. And that request is received by the customer support. The customer support logged it and sent it to the sales team. Then the sales team. Check for the product availability and the discount terms. Send the proposal back to the customer. Once the customer accepts the proposal the sales team. Order, place the order. Sends it back to the support team, and the support teams route it to the logistics so that we can schedule the delivery. At this time there is no information to the customer, so the customer needs to request for a status update. And the status update is provided by the sales team. Finally, The product is delivered to the customer as you can see, it's a pretty cumbersome process. It's a 12-step. Workflow with lots of human interaction, so this caused several challenges. The first one is that obviously it's a very, very poor customer experience. The second one is that it has some efficiency and productivity issues, and most importantly, the customer retention is really at risk at that stage. So let's see how we can improve that. So we added agents in that workflow. So here you can see some of the key improvements that includes that an agent is receiving the customer inquiry. It provides directly a cost and a product recommendation. If the customer agrees, then the agent. Develop a draft proposition, a proposal. The customer accepts it. It it gets placed automatically, and the agent also look with the logistics to put a first schedule in place. The updates are provided automatically to the customer at this stage, so you see how we did. Shrink that workflow from 12 steps to 8 very quick steps, and we just keep the human in the loop when something wrong happens, so the customer declines the offer or the scheduled delivery does not fit his schedule. So that's where we put the human in the loop to accommodate with the customer. So now we know where we are coming from and we know where we wanna go. But as, as every time when you wanna go somewhere, you need to have a journey. Uh when you wanna go somewhere, you need a journey. So how does the typical agentic journey look like? So everything starts with the assets. Uh, you want to start with assessing your workflow, but your workflow from a business perspective, but also from a technical perspective. Once you have the assessment you want to enable your technical stack to be ready to integrate with agents. Once your technical stack is ready, then you will need to build, to design and to run your agents. And finally, you want your agentic solution to be able to scale at a production grade. So let's see how we can assess the workflow. We're going to talk here about the technical assessment of the workflow of our grocery item reseller. You remember that cumbersome process we see at the very beginning. This is supported by some legacy applications. And if we take the example of the order application, it is a SOA application. So we want and we need to modernize it for the. For, for, for the technical, um, for the technical stack to be ready to, to get the agent. So for to doing that, we're gonna use Quiro. So we're gonna use Quiro to learn about the code and if by any chance there is still a documentation, we're gonna also use the documentation to learn about that application. Uh, While Quiro is learning about that application, uh, it will provide us a documentation. That documentation will will include the, the business flow, the data flow, the business process, the business features, uh, the data schema, and so on and so forth. Once we have everything documented, we can go to the enablement from the technical stack. That's where we're going to go for how to re-factor. What refactor design approach are we going to take. We're going to develop a new specification, and finally we're going to refactor the code. So let's see it in action. Thank you, Serge. So I'm going to show the demo actually how on this legacy application, which is SOA Base, uh, that can be helped to transform. So, uh, as, as Dick mentioned, we are going to use the Qiro. So anyone else use the Qiro or familiar with the Quiro? Yes. So, what I'm going to do is that on the first step is let's look at what is the legacy applications looks like. So, this is my um older legacy application. You can see from the flow, uh, it is, every step is manual, like order processing, order, even the order management, and also doing the logistic process here. So, everything this legacy application has been doing. Built up on the SOA or SOAP applications here. Now, um, because it's built on the SOA application, you can see the STL cycle there. Now, what I'm doing here is basically the code learning as the first exercise. Now, as I'm learning the code, oops. It's going back. Yeah, it's coming to that. Now, Kiro is pointing to my SOA applications that has been code registry, and I'm using the prompt-based approach to identify the learnings of the, what is inside the code. The problem with this application is that it has uh all the business process inside. So now the keyro has been like, you know, pointed and the learns what are the appli code application is inside. So it is understanding the business logic, what are the presentations layer, I have it. Uh, also getting into the, what is my system flow here. I also want to understand what is the database schema as everything is around that, because the code also had the code repository database schema. All applications around that. So, I want to understand, not only understand the code, but also I know some of the security aspect of that. So, it is doing the understanding the code, but also doing the code review. So this is the legacy applications. I also want to understand uh data flow diagrams and telling as well. So now the application is done. Uh, now I want to go for the high-level design approach. And this is where I'm going to use another prompt-based approach. To understand like what is based on my understanding of the code, what is my um high-level design looks like. Now, important here because you are using SOAP application has a description SOAP and why you want to transfer into the uh RIT API. So, XML to JSNN applications, how you're going to migrate is extremely important here. This is where it is going to help me to understand how you can migrate. Also, we're going to also migrate the database from SQL to all the different database. So, I want to understand what is the data transformation data strategy looks like here. So this is all based on, based on my uh code learnings. I'm coming with my migration model analysis strategy. From SOI to RIST API, right? So when it is done, what I'm going to do is I'm using the Keros specifications feature that when I understand the high-level diagram, all high-level things, I'm now creating the requirement and the design specification for that. So, based on my code learnings, I have my, everything's ready for a high-level design. Now, my requirements, specification, acceptance criteria has been ready, right? So, this is the document that has been created, produced by Quiro. Now, next phase, if you the specification, I have the requirement. I'm going with the detailed design phase. So earlier was high-level design. I'm going to each and every design that's focused on the infrastructure. I need a data, database as well. Most importantly, what is a high-level solution architecture looks like here? Uh, because we are transferring to RIST API. So how SOA can be or the XML base can be transferred into that RIST API and point, uh, error handling. Everything. So, these detailed design documents actually help me to take to the next step when I want to actually implement that. So, once it is ready, I'm going into the next phase of implementation side. So, I have all my requirement, acceptance criteria ready. Now I want to go into more implementation plan. And as part of the Kro specification, then this is one of the things you can get it out of the box is about implementation plan. It is breaking down all your design documents into the one small task that you You can run independently or that. So, at the end of the things, what I've done so far is a requirement document, design document, and the task-based documents on that. And now, final step is like implementations. I have the task break it down at a lower level. So I want to now start building my infrastructure and then go database migration, modernization in that case. So, this is like end to an example of the curo that how you can actually trans it. But this is the AI generated one. So what I've done actually for, to boost up my modernization for AI I have used AI here, uh, AI power. So, again, this is AI generated, so we also want to like human in the loop. So when you, each and documents that have been generated, either design document, requirement documents, everything, it's good to have your expertise to look into that. And this is where you can actually play with a key rope. So, when the design documents have been ready, requirement document is ready, you can actually update that and go in that modernization there. So, what we have done so far is basically uh Take my application is agenttic ready. So earlier was the legacy application, which is like Swaab based. I want to know my application is ready with the REST API. So, once it is ready, I can move to the next step, right? What would be the next step would be? Oops. Yeah So when you look at the initial typical journey that we have done the assessment, my stack is now ready. So my, I have transformed my SOA application into DES application. So now we're able to communicate with my applications, it's now able to communicate with my database. Now it is time to design, build, and run the agent. So let's start with the first things, like when you are actually Did or building the capabilities around the design, you need that. Framework, or I would say SDK that you can actually start building the design or build the agent first. And this is where I want to introduce the strands agent, uh, which is, um, open source by it, uh, anyone familiar with the SD uh strands agent here? Anyone has used? Yeah, we have. So, it's good that you're familiar with this translation. So, this is an open source from the AWS um SDK and it is on the model-driven philosophy. What does that mean? So, it rely on, uh, or it leverage, basically the LNM's capabilities to actions to uh take the decisions and actually focus on more on, on the decision making and take the appropriate actions around that. And you can have the full control, so you can customize um the agent the way you want it. Now, the, the next thing is about like how you can build that agent very quickly. And this is where there are a few lines of code, you can create the agent now. Now, because of this like faster pace of development, you can easily experiment, uh you can actually reiterate the things as well, and uh you can get the learning experience as far as possible quickly. Now, the building or the design of the agent is one thing, but you need the entire ecosystems to actually, because agents need to interact with your live ecosystems or live data in that. So how do you bring that, that whole experience together? This is where you can, you know, convert any of the functions with just Around the tool declarations, any of our functions, existing business logic, database connection will convert into the tool. So this is where the agent can get access. Through the tool, agent will get access to the live information, dynamic information, and it enhances their internal capabilities around that. So, with these two things, you have design is done, your, your builder um your agent, your tool is available, so you don't have to rebuild anything around that. But uh we also get more than 20% of, uh, 20, more than 20, I would say, um, tools available off the shelf for you it is production ready. And uh this tool actually help you to integrate with your agents. So you can integrate with AWS services like Knowledge Base, S3, um, And another important here, when you're working with the agent workflow, you require the human in the loop, right? So this is where the hands-off will be important. So when you have the workflow, you want to focus on the hands to user. This is the tools that actually hands up the user when you require the human in the loop. Uh, image reader, um, the diagram, and also E2A clients available for you. So, you, when you need to build the uh agent, this is what you need. And this is a trans agent will give you to kickstart your journey for the designing or building the agent. Once you have built the agent, you need a platform to scale securely and reliably. And this is where the Amazon Bedrock agent will, uh, you know, bring the value here. So you can deploy uh this on any framework. So uh the strands, uh land graph, QI or any custom framework on that. And um for the complex task, or we are, we are dealing with a complex workflow, you need a longer run of time. So this is where I just example of our uh on the right-hand side is architecture. That's basically how you can deploy a strands agent on a uh bedrock agent. And if you look at all the capabilities of different day runtime, which is actually allow you to run longer running your agent. So you can run up to 8 hours, um or agent here. And for each user interactions, you get a dedicated VVM so you get a CPU memory for that. So it is completely isolation when you do the interaction. That's for the Amazon runtime. Uh, and when you have the observ, so you need a uh uh agent to be understanding the traceability of everything. So that's why you have the observability there. And, uh, when you think about the state management, context management, this is where Amazon, uh, bedrock, um, I would say memories comes into that. You also need to have the interactions with your data ecosystems, right? Or your applications. So, agent, uh, core gateways. This is basically the service that will allow you to interact with your tool, and IDT access management for authorization authentications. So, with this um with this full capabilities, you can be able to deploy even the build and run your agent at the scale. So we look at that build and design phase and the platform that you can actually deploy the agent. Now, the next thing is about the some of the design considerations. This is extremely important because when you're designing um the agent, what are some of the things you need to consider. And we see many times actually customers or even the partners is asking like, if you, if you want to have, uh, if you want to build the agent, how do, how do we kickstart that journey on that? So, first thing is about the orchestration pattern. So, agent as a tool is one orchestration pattern. So here you are managing and dealing with the process here. So, you need to understand how your existing process is working. Now, if you apply this pattern as agent as a tool, these have the one agent, like a one orchestrator agent, actually dealing with the user interactions, taking the decision, and using all the different tools here. And in this example, I'm using one agent, which is actually, I call as a domain specialist on the clinical agent. It is using multiple tools to make the decision, autonomous dishes and all that. So, if you have the similar direct use cases, this is where the agent has a tool, uh, patterns will come to. The next pattern is a swamp pattern. Consider this is your kind of the project manager. Actually, it is leveraging the specialty or special resources to do the specific task. So, uh, in this scenario, a swamp pad, when you use this swamp pattern, they have the different agent working in isolation or working independently in, in a one graph and do, try to solve one problem. So, when to use this pattern. So, if you are dealing with a complex workflow or creative problem solving scenarios, this is where you can think about R&D workflow. Uh, this is where you can use this pattern. And another example here is basically, uh, I want to do the market research analysis, which is a very complex task, uh, and there's a multiple agent involved in that. So, this pattern is useful for this kind of the scenario. And the last pattern is a graph pattern. And what I call is, it is a deterministics workflow here. And in our order process management system, if I look at this pattern, it's about, these are the different steps involved here. Like, you know, when a customer place an order, it has to go through the certain check. You need to go for the product, inventory. It also product is not available. I go for the recommendations. You need to calculate the price, uh, and then provide the complete information back to the customer. If you have this kind of scenario where you have the dependencies uh on the multiple, uh, multiple stages, uh, you need different branches, you have to take the decisions around that. This is where the graph pattern is useful. So, these are the different patterns that are suitable for your workflow. Now, once you need, you know your patterns, the next thing is about the reasoning and the intelligent capabilities here. Now, uh, LLM works as a brain to your agent, right? So, that's what I mentioned earlier, like, you know, LLM is rely on leveraging the LLM's capabilities are there. But LNMs have been evolving continuously, right? Um, you know, we'll see every 33 months or every month, new LLMs are coming. The existing LMs are becoming more mature around that. So, The question here is like, which LLM I should use with my uh agent. And this is where you need to consider like, do I need a LLMs that has a more comprehensive, that providing me the more reasoning capabilities, or I can use a simple model. Right? So the question is that how do you select that kind of model? And this is where I can say, you know, we can use a simple framework that you can consider the task complexity where agent has to perform. Versus your latency or the cost, because why you require, there is no this framework basically not give you the one model or the best model. It basically help you to identify the select which model is suitable for your task. Now, if I consider I go back to the order management system, like you have a customer agent which only With your customer, which is a very simple task. This is where you can use Amazon micro model, which is like, you know, designed for the tax-only model for very heavy throughput jet into, into that. So, if you had a very simple task, it require very live information, you can consider that the Amazon micro model, which is like designed for that way. Now, think about another agent, which is a a sales agent that has to build a work on a Different documents or create a proposal, you have to, it has to manage with a multi-modal data like text, documents, image and processing. This is where you require a different model. Um, so there's Amazon Nova Lite or the Pro that model that you can leverage for that. You have another agent that can, let's say, Warehouse agent. And that warehouse agent had to do with the multiple complex tasks, like identify the route for the delivery, what is optimum times for that. It has to consider external things about like what is the weather looks like, what is my traffic look like. So, it has to require a multiple complex reasoning task. This is where you require a higher copyability model like Enthropic or maybe you will go with Amazon, Nova Model Pro as well. So, based on the task complexity, this is where you can select your model. But it's, you're not stopping there. If you have working with a very industry-specific scenario or industry-specific mode where you need a domain knowledge is extremely important, not on the general knowledge. Your precisions require, like say healthcare industry and life science, right? Your terminology is different, process is different. So This is where you require the domain-specific knowledge over the general knowledge here. And some of the models that are listed here, like John Snow Labs, Medical LLMs here, and BioTrips is more basically focus on the other R&D scenario. So, you can also consider for industry-specific scenario, uh, which is available for that. Uh, agent also need to interact with your RAG application. So this is where you required the multi-modal embedding models for, for RA applications for making the tools or the access information for your things, for your, uh, Agent here. And finally, once you selected all the models, you need to evaluate that, right? So, which is the best model basically from the cost, quality, performance side. So, the whole comprehensive scenario of like uh reasoning intelligent by selection of the model based on the task uh and model evaluations actually give you that better decision which model you distribute for, for your design consideration for a specific task. OK. So, let's move on to the next one. So, um, AI agent, um, and the models, um, you know, driven, basically, they are fundamentally stateless, let's put it that way. They are not able to remember the things, right? Now, imagine the scenarios that you might be dealing with your custom, uh, customer agent here, and you are working on certain things, you come back after a few times and you need to repeat everything again. That means you're able, your agent is not able to remember anything. So, this is where the context and the state management transform that experience. So how do you, how do you do that? So context management, are you aware about the context management? Anyone? So context management is, you see, when you are working with the agents or any any AI medic applications. You require certain information. So what is the information coming from, it's basically your system prompt, the user input. You also have that attached providing the rag applications or external document, or the tools that I mentioned earlier that can extract information out of that. So, the idea here is when you're gathering the information from the multiple source and giving to your agent, your agent actually will not be able to get you overlaps with all the information. So the goal here for the context context management is to minimize the information overload. That's number one. But making sure that the contexts are relevant. This is where you need to focus on the context management, providing the right information to your right agent or at the right time. But you don't want, you need your agent to be smarter as well. So, you want, how do you make the agent smart? You make them as a memory. Give them a memory, so like as you as a human. Now, If you want to have your use case, basically, this is again the use case-driven approach. So, you're working with in a industry that your agent has to remember uh the customer preference or core core dealing in that. This is where the memories will come into the picture. And uh Amazon um uh Bedrock agent core memory, uh, that's provide the end to end solution for that. And it works in two different ways, your short-term memory and the long-term memory. So, imagine the scenarios that you want to uh Understand that what a user has been interacting for the longer period of time. This is where you go with the short-term memory. So you want, you actually uh persist all the interactions with the user up to 7 days to 365 days. So this is your your short-term memory. But you, you have the use case that you want to persistently, continuously manage your user interaction from the multiple years on that. This is where you go for the long-term memory. Think about that long-term memory is the hard disk and uh I'll go the energy of the short-term memory is basically your RAM, right? So, uh based on the use case, you can select that way. So, by context management and the pro providing your The turn memory, your agent not only become a remember but also become a smart. This is what you want to go into the next step because when you're working with the multiple use cases or industry-specific use case, you want your agent to be more remembered. But also be adapted on the what the user has been interesting or what user has been interacting in that. So, uh, state context management and state management is where that you need to focus based on the use cases on that. So, this is one of the design considerations. And the final thing is about uh observ. Now, you have, everything is like managed. You have this model selection has been done, you know how to design uh on, on the agent side, you have identify uh which task you want to focus on that. But then you come to the observatory here. So why it is important? Because when you test, you need to understand the entire workflow or the traceability of your agent, how it is performing under the net. This is one. You also need to identify uh any debugs or improve the efficiency and performance of the agent. This is where you need that all the traceability and observability is extremely important for that. And this is where Amazon Core Observatories is a managed service uh solution that will help you to uh go all for the press, matrix and logs, a part of that. And it also compatible with uh open telemetry, so it will work with any of the framework also integrated with, uh, also integrated with the cloud watch. And with this one, you'll get a 360 degree view of entire agent workflow and the performance. So, uh, while your agent has been interacting, uh, with that, you need to understand how many tokens have been, what is the cost for that. So, all the entry and comparability you can use with uh Amazon uh core observ. Now, this is the partner session, so we also want to say like there is also a partner. That also providing that capabilities here. So, um, Arise AX, uh, the guys are here, um, and we'll see in the demo as well that also provide that similar capabilities, but also you can visualize your, and also evaluate the, the models according to as well. So, you have the choice of that, you know, that's what we know, you have the ISP solution that help you to, you know, the speed of the things again. So, that is one design consideration. One thing I need to understand is like tools, right? So, you have your framework, you have the platform ready, uh, there is a design consideration has been done. Now, you need your, um, the agents require the right information, and this is where you focus on the tool building. Now, when you're dealing with this kind of the uh the multi-agent, as well as the multi-complex task, you need to consider um unification way, like how do you provide the information to your agent in a unified way. Because if you look at on this diagram, there's a multiple data source available. And this, all the informations you need, uh, agent need to access to that. So this is where the model context protocol uh bring the value here. Anyone using MCP? Yeah, there's a lot here here. So, uh, I'll not go in more detail on that, but basically, when you see about the MCPs, the common protocol, the head of the servers and the client based on that, and the agent or AI solutions basically interact with your, with your client on that. And all the data sources have been, you know, unify integration with your, the agent, and you don't have to build a similar integration for each and every year based on your data source. And with MCP there are two different patterns that you can implement. One, on when you have the one to many interactions, when you want to focus on the design, where you have the security or compliance may require, that agent can only have access to the specific information. This is where you can do a kind of the 1 to 1. Um, mapping with the MCP server. And another thing is we have like 1 to 1 to many, where you focus on a like a centralized management system. So, you can develop and design the MCP based on, uh, based on the requirement. But those who are already using the MCP. They might be aware, like, you know, building and deploying and managing and maintaining the MCP server is quite cumbersome, right? So you have to, uh, you know, when the things are changing, you have to update that, uh, versioning controls and everything. So, management. So you can deploy the MCP server, uh, either on EKS, Lambda, or EC2 instance, or VC many in a target as well. But you have to manage everything by that, sir. And this is where, um, Amazon uh agent core gateway, basically add the values here, right? Now, what you can do with this one is you can convert your API gateway, existing MCP server, or um lambda functions or any business logic into the MCP server directly. So, we call this the MCP file. So, based on your target, um, this will allow you to have the central, uh, central work as a central server here. And uh you can see from very simplistic diagram that, you know, when MCP client will interact with them, um, the gateway is is actually that, you know, take a heavy lifting of the middle, middle, sorry, take exam uh act as a uh MCP server in the middle, and that unify that integration layer for you. So by implementing these scenarios that you, you can provide that, the tools uh to your agent when they need it. And you can unify that your data source applications or APIs, existing application lambda, you can convert into that. So, we are seeing a lot of things here, um, basically, on the design consolidations, how do you build, how do you deploy. Let's see this is in, uh, let's see this in action as well. So, this is the quick summary of the all the design pillars that I will talk about, like orchestration pattern, um, the reasoning and intelligence, um, the, the state management, operational governance, and the tool development, right? So, um, let's see what we can actually implement this entire solution. Um, we had a demo as well. This is the live demo, not pre-recorded, so hopefully it will work. Um, so, all the design considerations that we are implemented and we'll throw in the demo as well. So, Grika, you want to go? Yep, let Could you take the question uh later? Uh, we'll try to finish with that one, yeah. So, all right, so we've got uh our Order Shopfront, uh, which is WhatsApp application, and there's a WhatsApp integration with AWS which I'll talk about it when I talk about the architecture. But then here what I'm going to do is drop a handwritten shopping note. So, what I want to do is like, imagine the scenario that we, uh, Sidiq mentioned earlier about the legacy workflow, right? So, we are going to that show that how you can, the legacy workflow has been done, transformed into the, through the agent. So, here, there is a multiple interactions, right, happening with the agent. Most of the time, you might have seen this happening through the web applications. What we are planning to do is introducing the new channel, which is actually your customers might be interacting with the WhatsApp, with, with your agent, how you can watch, uh, so, in our Use cases where the customer actually is providing the delivery, providing the order information, the list in in a handwritten documents to um to the agent. So, when you upload these handwritten documents, imagine the scenarios that earlier on, like there is 5, 6-step process has to go through. Now, based on this customer information, you can see that all the informations are coming back. It will check into the product catalog. It is all the items are available or not. If items are not available, it is going the recommendations and providing the options. Either you want to go with option 1 or option 2. And uh if you see that, OK, I want to go with option 2, this is where I can, uh the customer say, I'm OK with that option 2 because I need additional um um additional information, then it'll go into the option two. Now, what is going to happen is that, again, uh, agent will fire up, uh, in, in the background and go with the, find out the option two and place an order for them, and provide the delivery informations out of that. So, With these initial interactions, um, you can see that the whole, the entire process had been squeezed in uh for uh placing the order, order recommendations, which actually has been happened to manually. Now, we got the response from the agent that this uh order has been placed and uh uh you got a delivery slot as well here. OK. So, this is just a demo. Now, let's see what is actually happening uh inside the house. So, what is happening at the demo is that Yeah. So, here, user is uploading the image uh through WhatsApp. OK. And then we have the orchestrator agent. And so you, if you remember that the design configuration, as I mentioned earlier, the pattern. So here we are using the graph pattern. We'll see in the, in the code as well. So, one orchestration pattern, uh, that agent, it is actually calling to the multiple agent here. So, for image processing, it is going to the image processor. agent, which is actually extract the the image from uh from the WhatsApp and stored into the S3 bucket. OK. This whole process has been done by the image processor. Then it will go to the catalog agent, which is again, we have developed the MCP agent. So, the catalog agent, it is actually checked in the Amazon Aurora SQL. Now, if you remember earlier when I showed the demo for the Quiro, which is actually transformed the Legacy SOAD into a SOAP API database modern edition, this is where I did uh Amazon uh Aurora PostScri SQL, which is at the product catalog. And then it will go on for order processing because why we have chosen uh Dynamo DB? Because the order processing going into multiple things. And the SQL here because you can interact with the natural query language. So this is where the agent has been doing that way. So, with the SQL versus Dynamo DB. And then you have the warehouse agent actually check the DMODB table and, and call that delivery slot. So, with this end to end, the graph uh patterns, we have implemented, you can see how the entire workflow that you already seen earlier. Now, let's look at the architecture. Yup. So, uh, here, what we can see is, uh, we have the WhatsApp, uh, integration. Uh, so that is AWS end user messaging, uh, which I didn't know which existed until I started with this demo. Uh, and what happens is once you link your WhatsApp, uh, uh, number with that end user messaging, whatever messages sent to that number would come into a, a. topic and you can trigger a lambda or email or whatever through that SNS topic. So what we are doing here is trigger a lambda, and once I get into the code, you can see that lambda receives all the information in the message. And in this application, the lambda would save the image into S3 bucket and then trigger agent agent. And here we have multiple agents, uh, as Pranesh mentioned. So we have the router agent or the orchestrator agent, which receives the lambda's message, and then it would uh check whether it's an image pressing message or option selection. Based on that, it will trigger because it is an image, image press agent, and then gets the content out of that image and then pass it into the catalog search where the catalog uh agent would have access to this uh. Uh, catalog, uh, database, which is, uh, Amazon Aurora, uh, through Agent Go Gateway, and then search the catalog, gets to a list of items and figures out what's available, what's not, and based on that, uh, figures out the suggestions, pass it back to the orchestrate agent, at which point orchestra agent goes back. To the lambda and message goes back to the user again and then when the user selects the option comes along the same way and now this time it's a text message, it gets down to a different route which is to order the order agent which gets the order confirmation and also it passes along to the warehouse agent which would do a customer look up and get the customer's postcode and then goes to an external warehouse API. Uh, pulls the, uh, latest of the available delivery slots for that, uh, postcode and then sends it back to the warehouse agent, and the warehouse agent is intelligent enough to figure out what's the earliest delivery date, uh, and then that gets back to the route agent and back to the user with the other order information. So that's how the architecture looks like and, uh, I will talk about the models when I go to the code as well, uh. Sweet. So we've got our lambda. And Of course, we have the lambda handler, that's where the message comes in, and it would create a session ID because that way you can create uh that interaction within that session. But the default session, if you don't uh, set up a session ID then uh there will be a random session ID generated and that session will be valid for 15 minutes. Or you can configure that time as well since recently. But then what we've done is like we are setting it up for 10 minutes. Uh, and then based on the image type, uh, the message type, whether it's image or text, you call different, uh, workflows. And if I go to Image handling image message. Essentially what it is doing is uh uh So, uh, creating this. And Setting up the payload and calling the agent go run time. And once you do that, it comes into agent core, which is our implementation here. So this is the entry point, uh, as you can see with the decorator. So the entry point, what it does is it's again prints some uh logs and then calls grocery list with the payload which is in our core file. And that's how we, uh, create the, uh, agents and uh build the agents, basically. And here, what I want to talk through is how we build the graph. Because we spoke about graph patterns, so that's the pattern we are using. Building order processing graph. So, we are initializing agents. I'll go to initialize agents in a bit, but then once we initialize the agents, we have the agents in memory, and then we're adding it to a graph. And you can see we are basically adding all the nodes first, and then you set up the entry point. So entry point, which is always the router because that's how we want to, want the graph to work. And then we add, we're adding edges, so router to image processor, uh, if the, if it is an image request. Uh, and then image processor to catalog and catalog to router. Or the path two is router to order, uh, if it is an order request, of course, and then order to warehouse and warehouse to router. And then also, last but not least, we set up execution timeout and Uh, maximum node executions because that is to prevent infinite loops because, uh, if you look at the logs, which we probably won't go into, but then, uh, uh, agents are like, although you, you would imagine it would take it in one go, sometimes it goes in a loop and tries to, uh, execute it multiple times. So to prevent uh any infinite loops, we are setting up. Uh, number of executions. So, once that is done, so now we, our graph is done, but then, as I said, I'll go back to the initializing agents bit because that's where we are selecting models. Uh And before going into the models, we have, uh, we are loading MCP tools. So that is what Pradesh mentioned, the tools, which the agent has access to. So we have a number of tools, and you can imagine this is a very small application, but then if it's a large application, you'd have Maybe hundreds of tools. So you wouldn't want to load all the tools into all the agents because while loading a tool, uh, that means you are consuming lots of context by loading the tool's descriptions. So you want to limit the number of tools given to an agent, and that's why we are selecting based on the agent, we are selecting which tools are going to included for that agent. So for an example, if the Uh, order agent, uh, doesn't have to work with the warehouse management, so then we are, we are not loading warehouse tools for the agent because otherwise it gets confused as well, and that ties back to what Pranesh mentioned about, uh, information overloading and also we want to conserve that context window as well. So, once we load the tools, and then here we are creating the agent, and we have system prompt for each agent because that's, that system prompt is where that specialty comes in. I'll go into the prompts and you can see how the specialty comes in. But then also, we load the tools because we created the tools over there and then the models. So we are using different models for different agents. And if I go. Yeah Uh, so, for orchestrator agent, we are using cloudson at 4 because we figured out orchestrator has a lot to do because it is, uh, routing between different agents and it needs to be a little bit more intelligent than others. So we use uh one of the best models there. Uh, but then also for the catalog agent. Uh, we realized that while the catalog is doing the search, the customer is waiting. We want to minimize the wait time. And in terms of the demo as well, I was joking that that will be the longest 2 minutes of my life because I don't know whether it's really working, right? So I have to wait for them to come. So we want, in reality, we want that response to come back very quickly. So Haiku is a very good model. It has like it's a balanced model, I'd say. You have quite a good capability close to Sonnet 4, but then also it is very fast. So that's why we use the Haiku and then also for the order model again because it is doing quite a lot, we used the cloud Sonet and then warehouse model in this case, we are only selecting. Uh, the latest, uh, uh, available slot, so we are not doing much. So we are using Amazon Noa Lite. But then if it is, uh, doing quite a lot, then we can use Amazon Noa Pro or something like that, some very, uh, high throughput models as well. And the image pricer, of course, we are again using Sonic 4 because image pricing we. We didn't want to get anything wrong there because that's the start of the process. And if you process the image wrong, if you get a wrong list of items, then everything falls apart from there. So that's the logic behind selecting the right model for the right agent. But once that is done, uh, Uh, I'll go to the prompts quickly as well. Uh, so that, as I said, this is where that the specialty comes in. So you can see, uh, for the agent, you say, what is your role and what are the available tools for you and what's the workflow. So, and this, uh, creating this prompt is not a single shot, uh, thing, uh, it's an iterative process. So you come up with a prompt and then you try with that and it's sort of a bit of a something gray area between art and science. So you have to go through, but what you have to really be careful. is not to something called duct taping the prompt because what you find is you write a prompt, go through several iterations, and you start writing things in the prompt to prevent or direct the agent in a particular way which is limiting the agent's flexibility. So you want to limit the agent's flexibility, but within certain time. So you have to really be careful not to duct tape the prompt. And, uh, and also, it's also a good idea to have that, uh, giving it uh some important rules. Uh, these are the, say, for an example, uh, Uh, I have something in catalog, uh. Say never auto substitute because uh sometimes the agent would say, uh, chicken thighs are not available, so I'm going to decide that I'm going to send only the chicken drumsticks. But uh so we are saying never auto substitute, just let the customer choose, send the options, things like that. Uh And then last but not least, I'll get to the tools. So that is how you define a set of tools, and this is what I said when you load too many tools, because that is loaded into the context of the agent. So, if you have hundreds of tools like this, uh, then that leads up into the agent's context. And then also you don't want to give access to more than the necessary tools for the agent. And uh with regards to observability, yes. OK. So, uh, thank you, Judy. I think that's, uh, I hope you have seen that glimpse of the, how the all, everything comes together when you define that multi-agent scenario. Now, as I mentioned earlier, the observ is also important here, and that's I want to to give a quick glimpse of um Arise uh AI platforms. Uh, guys are here if you want to interact with them. But this is actually give them a, you know, visualizations and the capabilities for everything. So, Uh, if I go with the query quickly on this one, the tool, uh, what we have done is like trying to put all the agent traces, uh, to the, uh, to the platform here. So, And if you want to understand for all, all the different uh capabilities here, there is a different invocation here. Um, so, you can click any of the invocations and see detail of the all the agent, how it has been working. So, the first thing is I will go with, with the trace tree here, which is give me the all informations about uh how the agent has been done. The another information that I would like to see uh is about that, this graph tree. And what actually does, it actually gives me the all information, how the agent has been started, uh, how the different tool has been gone through the hands-off on a different model. And then finally, if I go on, on, on the back here, uh, when, this is where the whole graph orchestration has been started. So, it's go through uh the different hands of here and then go on the loop. So, if you look at the graph pattern, the whole been started with the one orchestrator pen, go in different hands of there. And through this entire visibility, you can see how the tool has been called differently, how uh agent has been trans and to end here. And also, there's a different way you can also see how on OK. So I have the router agent here, which is basically talk about uh proving all the information the agent has gone through that. So, agent has the prompt, his user input, how it has been prompt, is it going internally, what is the tool has been called, how many tokens have been identified. So, everything that all the comprehensive information, you get it here as well, right. So, um, that is a quick glimpse of like how, you know, use a different IT solution, integrate with your agent capabilities here. Um, so, uh, with, uh, we also have the Amazon Bedrock, uh, agent Core Observatory, but also ISP tool that you can integrate with to see end to end that one. So, uh, that's a quick, um, uh, the demo, end to end demo of what we have seen so far, like design implementation, how you build it, how you deploy it at scale, and how you do the governance and observability around that. So, I'll just hand it over to So, Dick, we can go through the next step. They're corporate. So we've seen how to build uh an agent. Uh, now, let's see how we can scale it. So to scale an agent, it, it's not enough to have a, a, a good ground foundation, a good technical foundation. You also need your enterprise to be ready. So you need, uh, to, to make your enterprise ready, we are using a free layer architecture. Uh, this is part of, uh, of an AWS initiative that, uh, is called a process to agent. Uh, I'll talk a bit, uh, more in a few slides. So the three layer, uh, architecture at the top, you, you find the business process. So this is the, the business layer where you find all your knowledge about your business processes. You need to understand your processes and the process flows, and you also, uh, you need that to be able to, to reimagine your, uh, your processes. At the middle layer, you will find the enterprise agentic platform. That's where you will have all the agenttic governance, uh, all the agentic operations, uh, the pool of agents, uh, the different tools that we just discussed, and also all the external systems. At the bottom of it, you will find the core platform services, and that's where the AWS services comes into, into place. So let me describe a bit that that. That core infrastructure. So we're going to scale agents using event-driven architecture. So at the heart or maybe at the brain of that architecture you will find agent core. I think we already discussed it a lot. So this is where all the reasoning happens. But for reasoning to happen, you also need to bring data to it. So how to bring data to that brain, we're going to have two layers in front of it. So the first one is the interface layer, and this is where you will find all the UI, so the chatbots, the WhatsApp, for instance. You can also have Ed Edge application, sensors sending data to it. You can have file updates. So this is really where we're going to input the data. Then we will have the pre-processing layer. The pre-processing layer, this is where we're going to add some context to the to the inputs. We're going to add some metadata to the inputs. We're going to start request routing also and classification, and we can also have AWS1 function if we need to start complex workflow behind the scene. At the end of the architecture we're going to find two other layers. We have the postprocessing layer. So once we do the rezoning and we have the agent outputs, so we're going to go through a post-processing layer and that's where we're going to. That's where we're going to format and validate the output. We can also do event emission and routing from there. We can do anomaly detection, escalation. We can support. We can do support team notification. We can do workflow routing and so on and so forth. And at the very end we have the output layer, and that's where we're going to answer back to the chatbot, for instance. We can also update external application. We can find the dashboards. We can upload the file on an S3 bucket. And if you can see there is kind of a loop at the top of the architecture. This is because in the case of multi-agent architecture, you can have your post-processing layer or your input layer. Triggering another agent. So this is also how you can stack your different agents. So this architecture is, is interesting because it's uh it segregates all the layers. And it allows you to scale every layer independently and it's also, it also is flexible because you can modify each layer independently. So this is how you can scale really your agent solutions at a production grade and enterprise grade ready. So we've just seen how we can modernize a workflow in kind of being disruptive, but. Modernization is not always disruptive. It can also be adding a new channel. So let me introduce a smart communication, and we have smart communication uh people also in the room. If you want to talk with them after, they will be happy to talk with you. So smart communication is an AWS independent software vendor partner, uh, and they are using agents to modernize data collection in regulated industry using smart IQ. So Smart IQ is a low code enterprise form automation solution. That transforms manual processes and paper form into dynamic two-way conversation style data collection. In regulated industry, they face critical challenges in data collection because uh um you need the data collection to be deterministic and you need to gather the only the data that you need and only the data that is missing. Uh, And also the challenge that they had is that the customers don't want to add new business rules, so you need to be able to integrate into the, the business rules. So how did smart IQ or smart communication do it? So to solve those, those challenges, uh, they decided to add a new channel so that they integrate with the business rules that are in place and they are not changing everything. So everything starts with the user uh asking a question in a chat with uh UI. Then uh the request is passed to Bedrock. Bedrock loads uh the predefined prompts to try to make sense and to understand the request. Um, Once uh Bedrock understands the request, it connects through the MCP to the smart IQ backend. To determine which which form they need to start and which data are missing so that they can answer back to the user with the appropriate question and to gather the data that are missing. You can see that they are using Amazon Nova Lite. They started with Enthropic and they saw that it was too big for they could use something lighter, so they moved to Novaro. And then they moved to Novalaid, which is enough for what the reasoning that is needed there, and that's tied back to what Pragnesh said before about choosing the appropriate model for your need. So is adding a channel really valuable for the customer? So we have a first financial service customer that reduced the loan package down from 20 days to 57 minutes. Using that digital process, they also reduced the total loan turnaround from 2 weeks to 2 to 4 weeks to 2 to 3 days, and they they they downgrade, they reduced the not in good order. rates down to near zero. So not in good order is an industry term to say that some data are missing or the data are wrong. Another customer still in financial services reduced, reduced the not in good order rate from 33% to under 3%. They also get a very faster time to value thanks to smart IQ. And finally, an insurance company reduced their not in good order rates from 67%. Which really leads to a faster claim approval and payment for customers, which leads in a 40% faster turnaround time. Again, we do have smart communications folks here, so please feel free to discuss with them. Now how can AWS help you? Uh, we do have a new initiative that is called Process 2 agent, and it's a strategic initiative to establish leadership in enterprise automation by transforming traditional businesses, uh, processes into Argentine AI powered solution. So this this initiative relies on the 3 layers, on the 3 layers architecture that we've seen before, and it, it comes with commercial constructs and also it helps partners and customers to work together and and it adds also AWS service team to the game. Uh, I just want to give you a concrete example on, uh, on how we can combine P2A with the partner expertise. So, in the middle of the three layer architecture, we did find enterprise agentic platform, and this is where orchestrate AI fits in. So this is uh CAP Gemini enterprise aggentic platform, and it's designed to help financial services enterprise transformation through using agents. It really enables deployment agents that are pre-built, so there are 3030 plus agents already built there, such as know your customer cooperate or or risk portfolio management, for instance. So using that CAP Gemini expects 20 to 40% improvement in productivity and 25 to 30% in cost savings. So this is an example on how we can partner AWS and you guys to help the customer journey and also to get to monetize your agency capabilities. So I hand over to Pranesh for the end of the session. So I know we will, uh, thank you for your patience. So just a quick thing on a key takeaway. So the, and you see the, we've gone through the whole typical um agenttic journey. So, you know, you can consider when you are focusing on the uh typical workload modernization migration. So follow the typical journey. Uh, focus on a key design principle, extremely important for agentic design things, um, and, uh, scalability in the mind, basically, because you want to take the from POC to productions to drive the value. So this is where we talk about the development of the trans agent, and how you actually scale securely. through the agent core. And finally, combine your asset, um, with the program. So as your partner, so you have right tool, right access with the ABS program. So, uh, you know, you combine your assets basically uh with ABS program. So we have different programs available for you. You have access to that as well. So, uh, then the quick call to action. So, of course, uh, you can learn more about this is where today you are. So we, there's a lot of information and talk, uh, discussing here about, uh, the agentic solution, how you can scale it. So learn more about the, uh, the strengths agent agent and the Bedrock agent as well. Uh, important for you as a, as because you're a partner, so you can build the capabilities industry solutions around that. This is where your key differentiator will come on a specific around, uh, workflow modernizations around that. And of course, uh, you know, the list your capabilities, uh, to, uh, marketplace. And, um, uh, reach out to your ABS colleague if you are interested more to learn about the PTAs or anything about the workflow modernization transformation as well. So, uh, final. The thing is about, thank you very much for your patience. Uh, please don't forget to give us, uh, uh, the feedback rating is extremely important, so you can use your app to provide the feedback. Um, you can write, if you don't like anything, write a lot of things, but give us a higher rating. So, Uh, so, uh, that's what I think. Uh, thank you very much for your time. And I hope you learned something, and we are available here if you have any questions as well. So, thank you for your time.