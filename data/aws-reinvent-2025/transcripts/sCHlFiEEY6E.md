---
video_id: sCHlFiEEY6E
video_url: https://www.youtube.com/watch?v=sCHlFiEEY6E
is_generated: False
is_translatable: True
---

OK. Hello, and welcome everyone. Uh, my name is Mark. I'm an engineer on the DSQL team. And this is DT 439. This is a deep dive into Amazon Aurora DSQL and its architecture. Last year, my colleague Mark Brooker gave a really fantastic talk, uh, with the same title as this one, where we gave a broad overview of the service and its capabilities. There's a lot going on under the cover of DSQL. And so this year we have a series of deep dive talks, where we're gonna go much deeper into specific areas of the architecture. In this talk, we're gonna be talking about how DSQL manages connections, what the architecture of the query processor looks like, and the architecture of the session routing layer in front of it looks like. Hope you enjoy the talk. But, but, but first, let's, let's do a quick recap of what Aurora DSQL is. DSQL is a distributed implementation of a relational database. Relational databases are awesome, they're older than I am. You can run complex queries, you can evolve your schema, add indexes on the fly. Meanwhile, We have services like Dynamo DB. We've been figuring out how to build and run distributed services for quite a long time at this point. And the really cool thing about architectures like Dynamo, is that they don't have a single point of failure, that they can scale horizontally rather than vertically. DSQL is based on Postgress. We're running Postgress under the covers, and it's a database that has been designed for running transactional workloads. And so our customers have been asking us to bridge these two worlds. The world of relational, the world of distributed database. And at the same time, our customers have been asking us for a fully serverless database. One where there are no service to provision, manage, or patch. A service where you can pay per use. A service that scales up, but also all the way down. This is what it looks like to create a DSQL cluster. I'm really proud of this, of this page. This is something that we intend to keep as simple as it is. There's no scroll bar here. You can click create cluster, and in just 5 seconds, that's something we, we, we recently released, you can have a cluster ready to go. There's nothing on this page that you have to decide upfront, right? There is no maintenance windows, there's no VPC settings. Everything on here is something you can change later. Tags, deletion protection. You can, you can re-encrypt your cluster with a different key later. You can toggle the, the, uh, resource-based policies later. So if you want, you can just click, click red, and off you're going. Here's another feature our team released recently. This is the query editor. What's really cool about this is this is a Postress application running in your web browser, speaking web sockets directly to DSQL. Because DSQL is based on Postgress, our expectation is that you can pick up the vast majority of Postgress-based libraries, whether that's Lib PQ based, or JDPC or anything like that, and connect to DSQL. So how does this all work? We have an application, maybe that's a shell or something that you built on Lambda. It's connecting to DSQL. And we're sending SQL statements to a component called the query processor. This is gonna be the star of our show today. And the QP is receiving the Postgress wire protocol. It's receiving SQL from your applications, and it's interpreting and running those queries. The second key component is the journal. The journal is our distributed transaction log. It's a core building block at AWS. It powers services like S3. And the the journal is where we get our durability guarantee. In single region, when the journal accepts a transaction, it's writing it to at least 2 availability zones. And in a multi-region configuration, we're getting our transactions replicated to at least one other AWS region. And the third key component is storage. Storage is a service that our team built from the ground up, designed to meet the performance needs that we had of DSQL. And the way storage fits into this picture is that it connects to the journal, it learns about transactions that you've committed. And it's, it's, uh, taking those transactions and updating its local view of the world so that we can run queries. And so when we send a SQL select statement to DSQL, it's the QP's job to parse, plan and execute that query, and turn that into a series of low-level read operations against storage. Meanwhile, if you're running any DML like an insert statement. Any rows that you insert into DSQL are gonna reside in the memory of the query processor. Nothing's been written to the disk. If you run an update statement, the QP is first gonna turn that into a series of select statements to go and read the existing values of those rows. And then apply anything in the update statement like a set or you know, increment might balance by one to produce new versions of those rows in memory. And so as you run your transaction, the QP is doing all of this work ephemerally in memory. And if your connection breaks, or if you type rollback, that data's simply gone. Meanwhile, if you commit your transaction, the QP is gonna send that transaction over the network. Eventually it will be written to the journal, again to at least 2 availability zones. And this is the point of commit in our system. Storage again is subscribed to the journal, it's learning about these transactions, and it's keeping itself up to date. Now you may be wondering, what about uh consistency. DSQL is strongly consistent, and the way we do this is through a time-based synchronization protocol. When your transaction starts, the query processor is doing two things. First, it's using the EC2 time sync service, which is based on GPS satellite clocks, to get a really accurate microsecond accurate reading of what the current time is. The second thing it does is use the AWS clock-bound service to correct even that very accurate uh time measurement for any potential errors. And by doing this, we get what's called a linearrizable timestamp, which is to say it's a value that's guaranteed to be in the future of any commit timestamps. The QP is going to include that timestamp in requests to storage. And by doing this, we guarantee that storage has always seen the latest data. Storage implements multi-version concurrency control. And so we'll return results for precisely the timestamp that we asked for. Which means that as we run our transaction and many seconds may pass while we're going back and forth, we see a frozen moment in time. So because DSQL is strongly consistent, this means that we can open additional connections. These different connections can run on different query processes, on different physical hosts, in different EC2 availability zones, or in the case of multi-region, even in different AWS regions, and still get strongly consistent results. DSQL is also active active, which means that any of these query processes can also update data. And the way they do that is by coordinating with a service called the adjudicator, which sits in front of the journal. The adjudicator implements a concurrency control technique known as optimistic concurrency control. And the idea behind this is that we take that bundle of, of everything we wanted to do in our transaction, and the adjudicator is going to go and look at other commits that have, that have been made recently and check if there are any conflicts. We're not, we're not gonna go deep into OCC today. Uh, again, uh, in the talk, uh, last year with the same title, uh, we went into much more detail. And yesterday, we gave a 500 level talk about optimistic and currency control, if you'd like to learn more. DSQL implements a hands-free automatic scaling through a number of techniques. It does this automatically and out of the box. So on this picture, uh, any number of things might have happened. Let's pretend there's rescaling happening here. DSQL is continuously backing up your data. And as we open more of these connections that are able to drive more and more traffic to the service, DSQL will use those recent snapshots to create clones of storage. Those clones will go and connect into the journal. They will learn about recent transactions, they'll keep themselves up to date. And then the query processes will intelligently, intelligently load balance across those replicas. Another thing this picture might represent is splitting for size, right? Like as we put more data into the, into the service, these replicas are gonna become bigger. And so DSQL will try and keep them of a manageable size. And the reason we do that is because if we keep the data sets small, we can improve performance. And in the case that one of our replicas fails, we can bring a replacement online very quickly, because it doesn't have too much data to restore. DSQL can also scale out the journal layer. Um, and it does that through automatic and transparent sharing. And it will start to send some of your updates to some adjudicators and some journals, and some of your updates to other adjudicators. For both read and write scaling, DSQL does this, uh, very proactively, so it's monitoring and increasing load. And it, it, it implements these, these scaling techniques transparently. And it actually doesn't matter what the sharding strategy it picks is because DSQL is fully capable of doing cross shard commits. OK, that's our Crash Course out of the way. We're gonna now start to speak a little bit about how connections work in DSQL. Before we do that, let's set the stage and talk about how connections would work in Vanilla Postgress. This is Postgress that you may have downloaded off the internet, installed in your laptop, or Postgress that you're, you're getting uh on EC2, maybe through RDS or, or Aurora. So our application is gonna connect in, it's gonna do some kind of credential exchange. Postress has many mechanisms for doing this. Let's say we're doing a user password credential exchange. The server's gonna look at our password, it's stored in the database. It's gonna make sure that it matches what we asked for. And once it's authenticated and authorized our connection, the server is going to fork itself. This is the Unix Fork system call, and so we're gonna get a dedicated process for our connection. And as we open more connections, either from the same host or from a different host, we're gonna end up with more of these sessions. And each of our connections, each of these sessions is getting its own dedicated process. And because of this, each one of these connections consumes some number of resources on the service, even if you're not doing anything. And so at some point, we may need to get a bigger host simply to have more connections. And so what we're gonna do now is go through a little bit of a journey that you, that many of you may have been on several times as you've built applications against the relational database and started to scale or solve problems like uh how to manage heat, or availability, uh, or, or failover. And so we have our application. It has a single customer. We've just got it, we've just gotten started. And the first question that we may have to answer is, how do we secure this endpoint? Postgress is about a million lines of C. It's 40 years old. Um, and it's a very performance sensitive code, right? There's tons of optimizations. And so just by running the service on the internet, you have to run the risk of some kind of security breach. Um, it may not even be in Postgress itself, it may be a security issue with the operating system or through libraries like OpenSSL that Postgress is linked to. And even if Postgress itself is secure, we may have something like a password, right? Maybe you put it on a sticky note on your machine, please don't do that. Uh, best practice here is you can do some sort of a password rotation through Secrets Manager, but still you have a fundamental property that if somebody leaks that password, they may have access to your database. And so this is a, a common reason for customers to, uh, you know, run their, run their database in something like a VPC which can really, uh, complicate the lives of developers who are trying to connect to and work with the database. Now hopefully we have more than one customer. As that other customer starts to connect and use our service, we can't just have one connection open to the database, right? And so we're gonna need at least one more connection so we can do work in parallel. Now you don't wanna be opening that connection as that customer arrives. Opening a connection in Postress can take some time, sometimes as much as 1 2nd. Because we have to do a bunch of things, right? We have to establish a TCP connection, we have to set up TLS, we have to do multiple round trips to do handshaking. We have to do credential exchange, we have to fork that backend process. That backend process has to get ready. And then finally our application can prepare any data like prepared statements before it can get going. And so to solve this problem, many customers will create client-side connection pools that ahead of time open a bunch of connections, we move all of that slow stuff off the critical path. And so when one of our customers comes along with an API request, we can just pop a connection out of the pool and get going. Which leads us to a question, right? How big should this pool be? If our application is running at 10 transactions per second, and you look on your monitoring graph and it's just sitting at 10, well, that probably doesn't mean that you can use a pool size of 10, right? Because if we zoom into any minute, right down to the second, we may see spikes in concurrency, because our customers may not always be doing work at a steady rate. This is a problem known as peak to average, and it may lead you to require a much bigger pool than your steady state rate. Let's pretend it's about 100, 10x increase. This is a formula from the RDS documentation that says for every 1 gigabyte of memory that an instance has, you can have roughly 10 connections. And so if we need 100 connections for our application, that means we can use the smallest instance type, that micro over there. But of course, we don't want to run one copy of our application, right? We wanna be able to do deployments. We want to be able to survive the loss of an availability zone. And so if we're running in a, in a typical 3 replica scenario, um, you may think that we can set that pool size to 33 only, but it actually doesn't work like that because again, if we lose a host, then the surviving hosts need to be able to survive, to, to handle those spikes. And so now we need 300 connections. We can no longer use a micro. Um, this is a really common pattern that customers have to increase their instant size just to get more connections. But you'll notice that the second part of this formula, that 5000, says that even if you're using an M8 4 XL with 64 gigs of memory, or the beast of a machine, the 48 XL with 768 gigs of memory, you can still only have 5000 connections. To solve this problem, it's very common to deploy something like PG Bouncer or if you're an RDS customer, then you can use RDS Proxy. And the way this works is that you're connecting to the proxy, and the proxy's gonna hold those 300 connections and only use the 100 that you actually need on the back end, which is gonna save resources and improve performance. This can be a really great solution in managing this complexity, but it's just one more thing for you to worry about to configure and, and to pay for. Here's another challenge. What host are we connecting to? Right? We have a single writer, a single place that we can do mutations, a single place where we can get strongly consistent reads. And if it becomes unavailable or you can't contact it, then things are about to go bad. You have to somehow fence this machine off so it can no longer do rights. We have to find a standby. Hopefully we have one ready or we have to restore from backup. We have to kick the primary out of DNS. We have to bring the standby in. We have to make sure that our application is ready to reconnect to that machine. And even if you do all of this right, this can still lead to minutes or hours of downtime. And if you're using all of the best practices, really you shouldn't expect less than 30 seconds um of downtime. Here's another challenge, scaling, right? Uh, if we need to scale up reads, we're gonna add read replicas. We're gonna need to configure reader endpoints. We're gonna have to teach our application which of our APIs can tolerate eventual consistency in our read only. And then start to do some sort of traffic splitting in the application. Some of these read replicas may be further behind than others. So our load balancing is not simply distributing the load, it's also excluding hosts that we shouldn't talk to. And even if you get, get all of that, all of that right, you're still left with eventual consistency, which is very difficult to reason about and build correct applications against. And so as we were thinking about what we wanted to build with DSQL, we were talking to customers to understand these pain points. These are the kinds of things that kept coming up. And uh if you really think about it, many of these problems are caused by the single fundamental property that we have this writer, a single point of success. It needs to be there, it needs to be available, it needs to be scaled, it needs to be secure. And it needs to be have, have enough capacity to do the work that you need out, out of it. Meanwhile with DSQL we have an active active architecture. In DSQL, any of these query processes can accept reads or writes, and all of our reads are going to be strongly consistent. And so this was the start of our journey as we were building DSQL. How can we take this architecture, put something like a load balance in front of it, fully encapsulate all of the problems that we've just described, and offer customers a just much simpler experience. And so this is our vision, our North Star. Our customers are gonna come in with their application, and they're simply gonna be given a query processor that they can use for the duration of their connection. OK, so we're thinking about how to build this, and we're working backwards. And the first question that we have is, we have this picture in mind, but do we have one of these pictures per cluster? When you go to the DSQL API and you click that create cluster button, do you get a dedicated load balancer with your own query processes running behind it? Which leads up to the following question, which is how many QPs should be behind this load balancer. Now, if we had a dedicated endpoint per cluster. This question becomes very difficult to answer, right? We would require our customers to give us some kind of hint. Hey, I need 100 connections. I need 200 connections. And so the, now each of these QPs, they're just a Unix process, right, that we're forking to accept more work. And so our minimum unit would be whatever we can pack onto the smallest instance. Let's say the smallest instance can run 50 QPs. That would mean that if you created a DSQL cluster, And you went to that little uh box where you can hit plus or minus, the, the fewest connections you could offer is 50. But as we saw before, that's not enough because what if that machine died, right? We want at least redundant capacity in other zones. We'd also not be able to offer you 51 connections, right? It would be 50 or 100, or 150. And then we wanna, we wanna think about patching, right? How do we do deployments to these hosts? And so it very quickly became clear to us that we should not do this. Instead, what we should work towards is a single shared endpoint. An endpoint that any cluster can use, and an endpoint that simply has as many query processes behind it that are needed for any of our customers at any point in time. And if we're gonna build this endpoint, we need to play well with the Postgress ecosystem. Because if DSQL was simply an AWS service, we could design whatever API we wanted, such as, hey customer, you need to tell us which cluster you're using when you're running a transaction. But because we speak in the Postgress wire protocol, we need to fit within the bounds of what that protocol could offer. And so as we were looking around for our options, we very quickly settled on using a feature of TLS called SNI. And the way this works is when you create a DSQL cluster, we generate a random identifier that's that colored bit, YEA. And that, that, uh, cluster is gonna get its own endpoint using a feature of Route 53 called an alias record. So it looks like it's its own A record, but in fact, it's actually just a pointer to our shared endpoint. And the way this works is that when your client establishes a connection and starts doing that TLS handshake, the first thing it's gonna do is send a message called the client hello. And if you see that blue bit, it includes the name of the server that it's trying to connect to. And then on the server side, we can take a look at that client hello, we can strip that name out, and now we know which cluster is trying to be, uh, which, which cluster you're connecting to. So this feature, server name indication, is a, is a TLS feature. It's not a Postgress feature. Uh, but it's been integrated into Postgress since version 14. OK. Um, so now we have the shared endpoint. We have SNI. And what we're gonna do is that we're gonna run this big fleet of EC2 instances. It's the job of the service team to make sure that those instances are sufficient for all of our capacity needs, and that we're gonna be taking on the, the job of making sure that they're up to date. And so if our application comes in, we could have the low balancer do something like round robin and just pick out a QP. So here we go, 123. We're now connected to all three of the hosts. And now look at the top. We're gonna have another customer come along, this FUAB red cluster. They're gonna come in, they're gonna get a connection on the same host as us. All's well. Except, oh no, they're a baddie. They, they, uh, they're found a security vulnerability in Postgress, right? And they fully intend to exploit that. So they're connecting in, they're running some kind of bobby drop tables query, and they're gonna break out of Postgress, and now they have access to the environment that they've broken out into. And because they're just a process running on the same machine as our orange cluster, they're gonna have access to everything on that host, and they could potentially read or write data from other customers. So this is a, this is a big deal. And at AWS the bar for this kind of design is not process level isolation, it's not containers, it's hardware level virtualization. Now fortunately, this is the same problem that our friends over in Lambda faced many years ago, which they solved by building a, a, a hardware-level virtualization, uh, uh, library known as Firecracker, which gives us a micro VM. And by micro VM we mean it's almost exactly the same thing as an EC2 instance, but it's designed for machines that use much fewer resources. Now VMs launch in milliseconds and have minimal overhead, but even Firecracker out of the box wasn't quite good enough for the performance we wanted to offer our customers. And so what we do is we have a little agent running on our hosts, and these agents are going to create these VMs ahead of time into a warm pool. So here we are filling up a warm pool. I've shown 6 on the slide, but in reality, we're packing hundreds of these VMs onto a single machine. And then when our, when our application is coming to connect and it's been assigned this host. We can simply take one of these, uh, one of these QPs out of the pool. And then we connect directly into that QP. And then in the background, the agent is gonna go fill up the warm pool. Because this is an in-memory operation, it's essentially popping an item from a linked list and esta and establishing a TCP connection. This happens really fast and with minimal overhead. However, there is a really uh complicated economic challenge here. And so in order to explain this to you, let's look more closely at how connections work in standard post-growths. So our applications connected into the server, and you can see that that server is using some amount of memory, right? The dotted box represents memory available to the, to the system. Each of these blocks is something like a page of memory. And you can see that our server is using many hundreds of megabytes of memory just to be running. Now when we fork one of these backend processes, even though that, that new process has uh essentially the same amount of memory requirements to run, the operating system is gonna be cunning, and it's gonna use pointers to say that actually this underlying memory that I cloned from has not been modified, and so that both both of these processes can share the same physical pages. And so as we open more connections, we can keep doing this trick. And this is gonna significantly reduce the overhead of running these additional processes. Now each of these connections of course needs its own memory. Right, pay attention to the orange one, it's running some kind of query. And so it's claiming new memory pages, as it's doing something like loading all your data into memories that it can sorted or run some sort of aggregation function. Meanwhile, over in our firecracker world, we have something that looks very different. Here's a VM we've just launched it into the pool. And it's running a full operating system, a full copy of Linux. It's running our Postgras server that's gonna handle just one connection. And it's also got a bunch of additional services that our team installed into that sandbox, so we can do things like monitor, manage system life cycle, get logs out of the sandbox. And if you add all of this up, we're looking at many hundreds of megabytes, just pretend it's 700 megabytes. And if we do this a couple of times, we're almost at 3 gigs of memory. So what do we do about this? Uh, Firecracker has a, a really cool trick here, which was again, uh, developed by our friends over in Lambda, which we've blogged about extensively called Snapstart. And the way this works is that when our host comes up for the very first time, it's gonna create one of these VMs. I call it the seed because we're gonna use it to grow new VMs. And when the seed comes up, we do everything that you might expect. We launch Linux, we start post-growth, we make sure everything's healthy. And we get right to the point where we're ready to accept our first connection. And then we talk to our agent, we say we're ready. What the agent's gonna do is it's gonna pause that VM and it's gonna take all of them, all of those memory pages and write them to a file called the mem file. It's gonna be persistent on disk. And then at this point, our seed has done its job, we can kill it. Now that we have our seed, we can start new clones from that seed. We can grow new VMs. And these VMs have the exact same memory layout. In fact, they're identical copies. They think they have the same IP address. They think that time is what the, what the original seed had. They think they have the same Mc address. They're identical in every way. Except just like with fork, because we've memory mapped in this file on disk. The contents of this VM are actually just pointers to the original, to the original snapshot. So far we haven't saved any memory, but as we start to launch additional uh VMs from that same snapshot, we get to use that same pointer, that, that same pointer chasing technique. And just like before, as one of our VMs starts to do unique work, it can claim additional memory, and we're only paying for that unique memory. OK, uh, Instead of just doing round robin, um, across these VMs, we have a service that runs behind the load balancer called, called the relay service. And when your application connects in, and it starts doing that TLS handshake, it is the relay that it's talking to. Relay is a service that we built from the ground up in Rust. Um, and it uses the S2N library so that, uh, we, we can just have assurance about the security of the service. And it uses S2N's capability to parse that SNI uh value. Now the next thing your application is gonna do is send over what's called an authentication token, which I just wanna take a moment to explain. Typically when you, when you're using an AWS service like S3, let's say we're doing a get object request, that is an HDP request, right? We have a bunch of headers, we have a body, and your SDK is going to sign that request using your AWS credentials to produce a new header using the signature version 4 algorithm. And this signature is used to, to prevent any tampering. So if somebody was able to intercept that request and try and change the bucket or the object key that you were trying to download, then the signature would no longer match. The other thing the signature does is it allows the S3 service to understand who the caller is, so we can do any any uh authorization enforcement. Now S3 is a really cool feature called pre-signed URLs, where instead of actually signing that request and sending it over the network, you can just take a frozen version of that request. It looks like a URL. It has the signature as a query parameter. And you can share that URL with one of your friends, and they, they will be able to just go to that URL without an AWS SDK and download the object that you gave them permission to. You're not giving them your credentials, they can't write data to your bucket, and they can only download precisely what you asked them to. The other thing you can do with uh with SIGV4 requests is put an expiry in. For requests that are sent by your SDK they typically are expiring in something like 5 minutes. But with an S3 pre-signed URL you can customize that value for up to a week. So this is the underlying machinery that DSQL uses. It's actually the same machinery that I that IM authentication in RDS uses. And so what your application is going to do is use the DSQel SDK to generate one of these authentication tokens. This is very fast, it takes just a few nanoseconds, because this is something that AWS has invested in heavily. If you're talking to S3 or Dynamo DB every single request you make is being signed. But what we're doing here is just generating one of these tokens periodically or once per connection request. And this allows DSQL to integrate with the IAM control plane, sorry, the IAM data plane. And it's gonna do some uh fetching of your, of your keys for your account, fetching of your policies and your tags. And it does this very efficiently, so that, um, so that Relay can authenticate and authorize your requests in many times locally using its cash. At this point, we have established your AWS identity. We're able to enforce any policies. For example, you can say, uh, users in this group or users in this role can only access clusters tagged in certain ways. And if you've used resource-based policies on DSQL, this is a really great place to enforce things like, I must be connecting from certain IP addresses or through private link. After writing login attempts at cloud trail, DSQL has established the identity of your user and will talk to a placement service. Now, the job of the placement service is to keep track of our fleet. We'll go into that more in a few minutes and suggest one of these query processes for your application to connect to. So we've picked a host, we've picked a QP and then the relay is simply going to send your connection over to the QP. At this point, the relay doesn't really have anything interesting to do. You see, the relay is a service that we built from the ground up. It's written in rust. And those first few messages, the TLS handshake, the Postgress, uh, the Posgress authentication messages, this requires complex protocol pausing. The Postgress, uh, protocol, uh, can, um, can lead you to making some, some very silly security mistakes. For example, messages have a length, and if you get that length wrong when you're assigning buffers, you can have all sorts of overflow and underflow attacks or arbitrary code execution. And so the way we approached this in relay is that we designed it to process as few packets as possible. And when we implemented the pro the protocol pausing code for those packets, we were extremely careful and worked closely with our security teams to make sure that we had done this correctly. But now that we've, now that we've done that work, Relay really has nothing interesting to do. Everything else that's gonna happen on this connection should be handled by Postgress. And so what we do is we move the TLS session into the sandbox. Now this is super cool. And the way it works is that a TLS session is a layer 7 concept, right? It's happening right at the top of the, of the OSI model. And we take all of the ephemeral uh encryption keys that are used for that session, and we package them over the network and we securely send them into the query processor. At this point, we're gonna use a function that zeroes out the memory in relay. And so relay can no longer decode anything else that happens on this connection. And so if our application sends any data into the query processor, it's gonna take that data, it's gonna send it over a TLS connection, it's getting encrypted. It's gonna flow encrypted through the network load balancer, through the relay, and it's gonna arrive at the QP which now has the keys necessary to decrypt our data. OK, let's talk about pooling. When our application connects into one of these hosts, instead of simply taking one of these QPs out of the worm pool, and, and that's it, what we instead do is we move it into a per cluster pool, um, which is gonna hold capacity just for that host. And the reason we do that is, watch what happens when we open the second connection. We've not dragged an additional QP out of the worm pool. Now I forgot to fill the worm pool on the slide, but keep in mind that the agent is continuously doing that. And so now, now that first connection we opened, it started to flash. What's gonna happen is it's gonna be able to use one of these QPs directly from the pool, right? But we have two connections open. What happens if that other connection becomes busy? Well you don't have to do anything about this. DSQL is going to automatically take another QP out of the worm pool. Again, these query processes are already running. And so this happens extremely quickly with a very minimal performance impact. And the reason we do this is because think back to those problems that our customers were telling us right in the beginning. Like, as customers want to scale their application fleet out, they want to deal with this, uh, peak to average problem. And so typically open many more connections than they actively intend to use. And we didn't want DSQL customers to have to worry about, uh, anything like running PG Bouncer or running RDS Proxy, because that would just add another hop in the network. There would be something else you'd have to pay for. Um, you would have to run many of them to get high availability. And it would have to, it would have to be something that you scale in addition to everything else in DSQL. And so by doing, by doing it like this, we've built pooling into the service, and there's simply no need for you to worry about that. Now that placement service I mentioned earlier, what it's doing is continuously patrolling the fleet. So at a very high frequency, each of our placement services is connecting into these agents, and it's asking you questions. How busy are you? How many connections do you have? How many of these are being used? How much memory do you have? What is the CPU load? For these specific clusters, how many, how many sandboxes do you have? How many of them are being used? And it's doing this all the time. The placement service is another service we built from the ground up in rust, and it does all of this in memory. And so when the relay asks the placement service where it should place a new connection, the placement service can do this very quickly because it has very current information sitting in memory. And because of this, I'm not sure if you can read that, that bottom line, this is from our quotas page. By default, a DSQL cluster has a limit of 10,000 connections, which is twice what you get out of the biggest um RDS instance size. And if you want, you can configure it. We have customers who, who are running with many, many more connections than this. OK, so we're opening connections. Here we have 3 open, and it turns out one of our metal instances has failed, or maybe one of our relays has failed. What's gonna happen? Instead of experiencing a full application outage, we, we're gonna experience a 1/3 drop. In connectivity. Two of our connections are still good to go. And what our application needs to do is open a third connection again. It's gonna flow through the, through the NLB. It's gonna find a healthy relay, and it's gonna talk to a healthy placement service and get us a new query processor. Now, we can't uh tell your application to reconnect, right? There are many kinds of failure that can happen within DSQL. And for all of, for almost all of these kinds of failures, DSQL's gonna take care of it automatically for you. But we can't have your application reconnect. And so what we decided is to put a limit of one hour on connection age. And so after an hour, which is gonna be really early on into your experience building applications with DSQL, either you hang up or we hang up. Now, don't worry, we're not gonna close the connection while you're using it. Like if you happen to start a transaction, you know, at 59 minutes and 59 seconds, we'll actually give you some breathing room. We have some smart things going on like jitters, so we don't close all your connections at the same time. But the best practice here is to use a client-site connection pooling library, like we spoke about earlier. You wanna take all that slow and expensive TLS setup, credential exchange placement, move that off the critical path. That's the best practice anyways. So while you're at it, Trigger that client-side pooling library to have a maximum age of one hour. If you want, you can do health checks. And that's gonna mean that when this kind of failure happens, probably what's, what your application is going to do is simply retry locally against your connection pool and find a healthy connection. Now, Postress is a fairly chatty protocol. I think when it was designed, probably the clients and the server were on the same machine. But even so, when you're running these interactive transactions where we're we're sending selects from your client to the server, getting results back in the application, then your application is able to look at those results and run complex business logic, and off we go back and forward. Every time, every time we go across the network, we're spending time. And so what we wanted to do in DSQL is give you the best possible performance. And the way we do this is with the Route 53 feature, um, where your EC2's Nitro card is gonna send some additional information as part of the DNS request, which is gonna tag it with the availability zone that that server is in. And so when you connect to DSQL, by default, your application is going to go through an NLB host in the same zone, through to a relay host in the same zone, which is gonna talk to a placement host in the same zone, which is gonna give you a QP in the same zone, which is gonna talk to storage in the same zone. And by doing this, we're giving you the lowest possible latency. And so as we open additional connections, we're taking advantage of DSQL's, um, active, active, strong consistency. But in the event that one of our AZ's are completely unavailable, the load balance is gonna fall back to a healthy zone. This is gonna give you a connect, give you a connection with slightly higher latency, but the system will remain available. Any internal failures, for example, here one of our storage replicas in AZ3 has failed, is gonna be automatically handled by the service, by picking a healthy replica in the same zone, and in the event that there are none of those, it will also go across AZ. So this is a really cool feature of Route 53. You know, you can use it in your own applications if you want. And it's something that we're very happy about. Because even though DSQL being a distributed architecture would appear to have more hops, in this case, there are actually cases where connecting to DSQL can be faster than connecting to a single, to a single writer endpoint. Because we're able to give you a QP in the same zone, no matter where the primary is. Because in DSQL, there is no primary. OK, let's talk about pricing. Um, DSQL is designed for activity-based pricing. So if you're familiar with Dynamo DB where as you send, put item and get item requests, you're paying for write capacity units, you're paying for read capacity units based on the number of bytes that you're putting into the system, based on the number of bytes that you're visiting when you do those reads. DSQL is a very similar concept, but we're actually gonna take all usage and talk about it in what we call a distributed processing unit or a DPU. At the end of the month, you'll see a DPU on your AWS bill. But actually if you go into the usage metrics on your DSL clusters page, you will see that we break out these DPU usages in several ways. So let me explain that quickly. The QP is running postgress, and it's kind of like a lambda function, right, in the sense that you can ask this QP to do work. You can have it do Fibonacci if you want. It doesn't actually have to do any reads or writes. And so as you consume time on these query processes, we're gonna capture that time through a compute DPU metric, which is essentially the seconds that you were active on this QP. So it's per second billing. When the QP does reads of the network to storage, we're gonna capture the reads that you do in the same way that we would do in Dynamo DB. We're looking at the bytes that you're visiting. Um, of course, if you're scanning a table, right, uh, with a filter, you may you may visit many bytes on the table, but the return bytes may be smaller, right? So think about how many bytes you're visiting as part of your, as part of your cost optimization. And then on the right path, what we're doing is we're looking at that commit payload, and we're looking at how many bytes you're putting into the system, and recording those as read DPUs. And all of these are activity-based, which means that as your application scales up, you're simply you're simply gonna be spending more on compute, uh, writes and reads. And this is how you should think about it. You shouldn't be thinking about how big should my machine be. You should simply be thinking about how much am I spending on DSQL. Of course, when you put data into the system, um, we have a, we have to store it, right? We have to keep it there, um, and ready to go when you, when you connect in and run a query. And this is just gonna be measured in gigabyte hours, just like you would expect from S3 or Dynamo DB. Um, now DSQL can obviously scale for you, right? We may be adding multiple replicas behind the scenes. If you're running a very high throughput, you may have 1015, or 100 replicas. With DSQL, you're only paying for a single copy of your data, right? So don't think about the number of replicas. Don't think about those continuous backups that we're doing. Just think about what is the, the, the number of bytes that I've put into the system. And because of everything we've covered in this talk, if your application goes to sleep at night because you don't have any customers using your application, you can simply close connections. You don't even have to close connections, you can leave them running if you want. The important piece is that you're not sending us any SQL, right? If you're not sending us any SQL, you're not spending any time on the QPs. You're not spending any of the compute DPU. You're not doing any reads, you're not doing any writes. And so the only ongoing cost for DSQL in this kind of scenario is that storage amount. When morning comes, and you want to get going again, just open new connections. Um, everything's there, it's ready to go. There's really no difference in DSQL between going from 99 connections to 100, compared to going from 0 connection to that first connection, right? The way we think about this is you're just grabbing a QP. QPs are always there, they're ready to go. Um, and so this is gonna really simplify your lives. OK, so, uh, wrapping up. Connections in DSQL. Connections are secure. In today's talk, we spoke about how every connection in DSQL is running in its own micro VM, um, using best in class hardware level virtualization and end to end encryption between your application and DSQL. Even, even members of the DSQL service team cannot run something like TCP dump and see your data. There is ET and encryption. It's secure out of the box. Uh, we actually encourage you to just run against our public endpoint. Uh, take advantage of those tokens, because you don't have to worry about credential leaks. Keep them short-lived. But if you want, you can set a private link and configure resource-based policies to lock down those connections to your VPC. Connections in DSQL are scalable. It's the job of the service team to make sure that there's enough capacity out there for every customer, even if they all spike. You can use as many connections as you need or as few, and you're only gonna pay for what you're doing on those connections. So if you have 100 connections and you're only using one connection, you're only paying for the activity on that one connection. Connections in DSQL are designed to be as simple as possible. There's no patching, there are no maintenance windows in DSQL. There's no need to run PG Bouncer or RDS proxy. And there's no single point of success in the system, right? This is a system that has no single point of failure. Uh, it's strongly consistent. And this is really gonna simplify your job as an application developer. And then connections are snappy. They are fast. Think about everything we spoke about today. We have AZ local routing. We're going to a, a very fast service written in Rust. We're doing placement out of memory. Um, and we're grabbing a connection out of a warm pool. Um Once you've got that connection open, uh, connections are fast. You're basically speaking directly into that QP that's ideally running in the same AZ as you. And the service has been designed to handle mass reconnect storms. So if all of your connections die if there's some kind of networking event, we have extremely generous default throttling rules that will allow you to get back online as quickly as possible. And with that said, that's the end of the talk. Thank you so much for attending. Uh, my name is Mark. I'd love to chat to you after the talk if you have any questions. Thank you for attending. Have a great reinvent, and please do complete the session survey in the mobile app.