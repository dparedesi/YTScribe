---
video_id: gcGWZlsHHvQ
video_url: https://www.youtube.com/watch?v=gcGWZlsHHvQ
is_generated: False
is_translatable: True
---

Good morning and welcome. My name is Mike Neville O'Neill and I'm the head of product at Bronto, and today I'll be joined by AO Mahoney, uh, engineering manager from Teamwork.com. Benoit is currently occupied with booth duty, so you can check him out over at 1757. And today we're going to be talking about logs at scale without compromise. So picture this, it's 3 a.m., something's on fire, and you need to figure out which of the seven logging systems your company uses the data resides in. Is it in Cloudwatch? Is it in Elastic or Greylog, or is it in the S3 bucket with all the Athena queries that nobody remembers how to run? Uh, if any of this sounds familiar to you, you're not alone, and it's a symptom of what we call the 3C flywheel of compromises, and it starts with cost. Uh, legacy logging solutions are prohibitively expensive at scale. Whether you're running a data dog or a new relic or you're running something on-prem like a gray log or elastic, as soon as you hit scale, cost becomes the major constraint for every subsequent decision that you're able to make. Uh, the first thing that you're likely to do is to start cutting retention of your data sources from 30 days to 7 days, uh, maybe then to 3 days, and once you've shortened retention as much as you can, you start cutting data sources, CDN data, debug logs, anything that's remotely considered noisy ends up on the cutting room floor. This invariably leads to coverage gaps across your infrastructure, uh, where it's essentially like trying to understand a novel with several chapters torn out of it. Um, teams notice these coverage gaps and visibility is still required into this data. Just because it doesn't fit into your budget doesn't make it any less business critical and so teams invariably will spin up workarounds, uh, in order to get visibility into this data which leads to a proliferation of tools in your environments, uh, and increases. Cost not just in terms of the cost of total cost of ownership of the products, um, but the cost of the friction as engineers are trying to move across different tools with different query languages, different retention policies, uh, different access controls to stitch together a cohesive picture of what actually occurred in their environment. This is the exact cycle that Bronto was built to break. So, what is Bronto? Bronto is a logs first observability platform where you can retain all of your data for 12 months hot by default. Um, we've been tested at petabyte scale and although most of our customers are operating in the multi-terabyte range, we're more than ready for what's coming down the pike. Um, we, the entry point is cents, not dollars per gigabyte, and it's our efficient architecture that enables this cost efficiency that I'll be talking about a little bit down the line. Um, we have exceptionally transparent usage and billing. One of the biggest problems when a bill is spiked in a logging platform or an observability tool is understanding why that spike occurred. With Bronto, you always know why. We can integrate with any agent. We do not have a proprietary agent of our own, so any telemetry agent you care to name can be used to send data to Bronto, whether that's a proprietary agent like a new relic or a data dog, or it's an open source collector like Otel or Fluent Bit. Uh, if it speaks HTTP you can send logs up to Bronto. You can even curl up parallel world in your terminal if you so desire. Um, in terms of AI powered log management, the AI features we have are available in shipping and production today. This is not roadmap theater. Everything that we're going to be seeing in the presentation and the slides is on product and ready for you to use. Lastly, we're able to deliver subsecond search against multi-terabyte sized data sets and our, our efficient search architecture enables that capability, which I'll be talking about in more detail in just a bit. Here you can see a couple of aspects of our interface on the left side of the screen you'll see our dashboard which is providing some visibility into CDN data where we can see cache hits by Pop, uh, requests by geocountry, the sorts of operational visibility that you would. Expect from a tool like this on the right we see the usage explorer and this is what I was talking about earlier with transparent billing and ensuring that you're no surprises in terms of uh the drivers of cost in your account. So the usage Explorer is primarily designed to help you answer three questions. Number 1, how much data am I ingesting? Number 2, what teams are ingesting that data? And number 3, is anybody actually using the data that we ingest? Uh, we give you visibility granularly into search and ingestion metrics so you can figure out, am I actually using the things that I'm sending to Bronto and make more informed decisions about the data that you're choosing to keep in the platform. Next, let's talk about features and capabilities. Uh, here at Bronto we are obsessed with reducing MTTR and removing toil. Uh, we think that there are a number of problems that LLMs can be applied to, uh, to reduce the amount of work that you have to do to make your data and your observability tool of value, uh, starting with AI dashboard creation where one of the biggest pain points of. Using or migrating to any new observability platform is creating new dashboards, creating new widgets, understanding the query language and how the platform works. Uh, we have a natural language dashboard creation feature that gives you the ability to, uh, specify a prompt to get what you need. So it's really just as simple as typing in, show me all of my 500 errors across all of my services and you're off to the races. Speaking of reducing toil, we've also implemented AI generated parsers. Uh, we're able to automatically identify the format of your data, create a parser for it, and then apply it. So there's really no more need for you to be writing regular expressions or grok patterns, uh, unless it's something that you want to do. With pronto, it's no longer something that you have to do. AI investigations are where we're bringing LLMs to bear along with our context engineering and domain knowledge to shorten the time that it takes to investigate uh errors in your log data. Uh, we go a lot further beyond than just explaining what a particular error is. We go towards understanding what. The source format of that is what the key fields are, for example, if we see a 500 error, then the key fields might be something like client IP endpoint, uh, customer ID, and more. We'll automatically launch queries across your data set in order to build a coherent timeline of what occurred. Uh, suggest an RCA and also generate additional queries for you to run to continue your investigation to understand what the blast radius is. So with the error, uh, with the brontoscope or the error explanation feature, you can cut down your investigation time from 15 to 30 minutes to 15 seconds. Statement IDs is something that is brand new and is unique to Bronto, and it is a technology that allows us to associate a unique fingerprint, uh, with every log message that you send to the platform. This allows us to do a couple of different things. One of the things that we can do is give you the ability to go directly from a log line to the code that generated it in GitHub. Uh, this is incredibly powerful from going from a monitor notification directly to your code so that you can go straight from finding to fixing. The other piece where this becomes interesting is if you're familiar with something called log patterns, uh, where with statement IDs we can understand which of those messages occur with the most frequency and perhaps more interestingly which happened with the least frequency. Where are my outliers? Where are the new messages that I've never seen before? Uh, why are messages changing in a sequence or transaction, uh, where it's almost always the same type of logs? All these types of outliers can be, uh, identified rapidly using statement ID. This quarter we've also released distributed tracing, uh, particularly around cross-service correlation. Um, although we are logs first, we are not logs only, we're not zealots, and we believe that there is a place for telemetry signals to live, uh, in our platform to kind of assist in getting to the log data. Traces are exceptional for understanding where something has broken, and with the log data automatically correlated, you can very easily understand why. Um, this is a smaller thing, uh, but maybe less small if you're trying to understand a monitor definition at 3 a.m. with bleary eyes, which is that we'll use AI to automatically name, uh, things like alerts, parsers, dashboards, etc. so that you're not required to come up with those convention yourself. Next, let's talk a little bit about how we built Bronto. Uh, the first thing that's worth noting is that we built Bronto entirely on AWS and that our architecture decouples storage for compute, which is really the key to our cost efficiency. Uh, I'll kind of take you through the diagram starting with ingest, and we can see that data comes in over HTTP via any of the agents that you care to name that I discussed earlier, uh, before streaming the EC2 instances behind load balancers. From there we stream the data to MSK where it's written to S3 in a proprietary format. I'll discuss in more detail in just a bit. In terms of the web UI, uh, and API, we're using a combination of EC2 and Cloudfront to make those things available to our customers. Um, in terms of storage, we're obviously using S3, but the way in which we're using it is where we really start to diverge from the other solutions that you see in the space. Um, we actually write to S3 in a proprietary format that's been optimized specifically for log data, uh, and this format makes use of all of the techniques that you would expect from a modern analytics engine, aggressive compression, columnar storage, bloom filters, data partitions, all built from the ground up to return your logs to you as fast as they possibly can be. Um, it's kind of this emphasis on, on, uh, the proprietary format that allows us to deliver speed in a way that, uh, other tools cannot, but storage isn't quite where it stops. Uh, we, when we're talking about search, we have to mention lambda as well, which in many ways is the backbone of our query experience in the sense that it allows us to deliver incredible power but also maintain exceptional cost efficiency by avoiding overprovisioning. So for example, when you run a query that needs to scan a very large volume of data, we have the ability to spin up lambdas in parallel to process it. Um, on the other hand, we are only paying to use that compute when it is actually, uh, being used or when the lambda is actually active, uh, and that gives us the ability to, uh, be exceptionally cost efficient as well. So. Essentially if you're running a query with a large volume of data, we can spin up hundreds of lambdas if needed to query S3 concurrently and get you those results as fast as we can and because we're only paying for the compute that we're using, uh, we can run a cost efficient platform and maintain that cost advantage and pass it on to you. Uh, in terms of security, uh, we're using Cognito for A verified permissions for our back, uh, and on the vulnerability management side of things we're using inspector, uh, as well as AWS guard duty, um, in terms of kind of the, the cost model that we've come up with, it's going to be a bit challenging we think for other tools in this space to replicate that, um, because the cost models kind of inherently assume an expensive indexing cost for every gigabyte that you send to them. And we're built fundamentally differently, um, to assume that the, the price for that should be, uh, cents rather than dollars. So what does this ultimately unlock? Uh, what does it do for our customers? I want to draw your attention to the lines that you see on the graph here where the X axis represents the size of the data set being queried in gigabytes and the Y axis represents the duration of the query in milliseconds and just check out all of these lines. It really does. It doesn't matter what you're doing, whether you're trying to count log events, uh, detect a sequel injection attack, find a rare UUID, that classic needle in a haystack use case, or list all the events with slow response times, ultimately all these queries complete in under 500 milliseconds against a 5 terabyte size data set. This is the kind of performance and experience that our architecture unlocks for you. Um, but given where we are in the talk, I think it's probably better to hear directly from what the benefits of bringing Bronto to teamwork have been for him. Thank you, Mike. Morning, everybody. So, uh, my name's E O'Mahony. I'm an engineering manager at Teamwork.com. Uh, we're based and then, we're founded and headquartered in Cork in Ireland, founded in 2007. Uh, our founders, uh, Peter and Dan ran an agency for several years before that, so they saw the need in the market for a project management solution. Um, and they built out, uh, the teamwork suite, which is easy to use project management with streamlined operations. Uh, we were very proud of being self-funded for years, but the last few years we took on a big 70 million investment to help us, uh, scale the growth of the product and add really cool new features into it. And as of recently, we have over, uh, 20,000 customers worldwide, and having that many daily users is, uh, the reason why we have logging challenges which Bronto have been very helpful to help us solve. Uh, so just another quick, uh, rundown what teamwork is. So we're a project and resource management product, uh, powered by AI built for service teams, or, um, it's to help teams adopt quickly and get value from their, uh, the work that they do in their business and make sure that managers have, sorry, yeah, uh, make sure managers have, uh, key insights into their financial performance. So it's all about getting work done and getting paid to get the work done and leaving the friction of the product to one side. Uh, the, the main areas that we cover, uh, project management was always the core of, uh, of the product when launched, but over the years we've also added in resource management and financial management, uh, which when put together give real value to customers. And then we have some supporting features to the side. We have reporting, we have integrations, deep automations. More recently, AI is a fundamental part, uh, of our product stack, and we're shortly releasing a, a new gente feature called Teammates, which will really help, uh, give customers extra value with the product. And dashboarding as well then for insights into their usage. And then as well we have uh Teamwork desk, which is our help desk product and it, it supports client communications, B2B comms, uh, spaces, is our knowledge base and help and, uh, documentation store. And a recent product we added as well is Teamwork Connect, which is a way for customers to integrate their own. Uh, BI tooling directly into our teamwork data again to get better insights. And the overall aim is to give, uh, insights into increased billable utilization and improved profitability, helping customers to get the work done, to get the team's, uh, work build, and then to, er, iterate on that, er, each month through the rest of the year. So, uh, our logging challenges before Bronto came along, uh, we have a conflict infrastructure. It's a mix of self-hosted open source, uh, cloud-based and custom solutions, uh, with a lot of difficult query, structured and unstructured data, indexing issues. We roughly have about 33 terabytes of logs a day, uh, sorry, a month, uh, being ingested, billions of logs from different systems. Um, that require, that causes over operational overheads. Uh, we operate in 3 AWS regions, so we're a big AWS customer, uh, 2 production regions in the US and the EU region, and then, uh, replicated, um, staging set up, uh, for testing. Uh, so that requires a lot of ongoing maintenance when you have a lot of self-hosted systems. So things like Greylog, having to replicate Greylog configurations into all these different accounts is, is time consuming. Uh, we had limited coverage. So, uh, the two main systems we were using were for our customer traffic that was coming in, traffic logs. We were using uh CloudWatch, but we had retention rules and Cloudwatch which only retained about 2 weeks' worth of data for cost reasons, which is a challenge and because you just don't have a long-term view. On the developer's side, everything was going into a giant grey log store which was backed up by open search. Again, because of costs, you have to set a limit on your open search node sizes, which effectively turns it into a first in, first out log store. Very noisy teams would end up er expiring logs, so you could never really rely on having a long-term view of your logs. Um, and you just had no real insight into which team was using the log allocation. Cost inefficiency then, operating all this stuff was very expensive, even though it was very, it wasn't very heavily customer facing, so it wasn't giving sort of direct value to customers, but it was costing a lot of money to run it. So it was good for operating the business but not for customer er features. And it was inefficient and it, it was just brittle. So that's where Bronto came along. So, what we have now with Bronto, simplicity, um, the unified logging solution has replaced a ton of legacy tools like Grey log and the cloudwatch logs, custom solutions. We've no index management. We don't have breaking open search indexes anymore. Uh, we have a much longer retention, so standard 90 day retention, 12 month retention for our customer traffic logs, so we get really good detail of our, uh, long-term customer behavior. Uh, we've been able to save 42% just in software costs by switching to Bronto. So that doesn't include the maintenance effort, but, uh, freeing up our sys ops engineers who have to maintain these, these custom solutions. Um. And when we were doing comparisons of Bronto to other SAS solutions, some of the other providers, their ingest cost alone was gonna cost more than the entire operational cost of Bronto for the year. So they're, they're, from a price point, it was very, uh, a very positive experience. And then some of the new features we have now with Bronto we never had before, so the Team Explorer, uh, the usage Explorer, it gives us very good detail into what teams are actually creating the logs and where the, the usage actually is in the system. And the bigger one as well is we can actually put the logging. UX in front of teams and staff that are not very technically, er, er experienced or or confident, so support teams, uh, product managers can now interrogate logs in a much easier way and it, it reduces the burden on engineers to have to do all that preparatory work, uh, to provide, uh, logging outputs. So all in all, it's a great experience. And as well, the other quick thing is just the customer experience of dealing with the guys. Any changes, we've asked for feature additions. They've been very responsive to make them. They've they've just been a dream to deal with, so it's been a really great experience. Back to Mike Thanks very much, Ay. Uh, appreciate your time and attention today. If you've liked what you've heard, come over and talk to us over at booth 1757. Uh, you can also sign up for a free trial at Bronto.io. No credit card required. Thanks again. Have a good day.