---
video_id: xE--P0NjUF8
video_url: https://www.youtube.com/watch?v=xE--P0NjUF8
is_generated: False
is_translatable: True
---

How y'all doing? Alright, I'm glad 3 of you are good. That makes me happy. How y'all doing? Alright, there we go. There we go. I'm so excited you're here. We started early. It's not, it's not even time yet, but we did it because we're excited. Uh, my name is Eric Johnson, uh, and I'm super happy to be here. I'm a principal developer, uh, advocate at AWS, and let's get our, we're gonna slip our slide on here real quick. There we go. Alright, now we're seeing what we should see. And Matt, introduce yourself. Hi everybody, my name's Matt. Uh, I'm part of the specialist team here at AWS. I'm a container specialist. I spend most of my time helping customers build cool, exciting, interesting, uh, architectures with ECS and LAMBDA and all of our cool A mod technologies. I have to be honest, I'm super thrilled to talk with Matt. Matt and I have got to travel a lot together. I have now learned to say his last name properly. It's Mecky. Not meeks. At least that's what he tells me. I nailed it, didn't I? Yeah, so how many of y'all have heard me speak before? Oh, ooh, a, a, a good room. OK. Do you know the rules? All right, a couple, you know the rules. Mark out here knows the rules. All right, well, for those of you who don't know the rules, do you know the rules? I don't know the rules, Eric. OK, well, here's the rules. So when I'm speaking, rules is probably a strong word, but you know, it, it'll really help you kind of, uh, as we go through today because I like to have a lot of fun. But these are the rules, right? There's 3 of them. OK. The first is this is any number I want it to be. OK, now I'm gonna hold this up and I'm gonna say there's 3 rules, OK? And someone who comes in late is gonna be like, that's not a 3. Well, it is because that's the rule, right? OK. The second is these are quotes, not apostrophes, and I know that, OK, because this looks dumb, OK, so I do this, right? And finally, these are thumbs, because this will get you beat up. Alright, so those are the three rules that helps a lot. I do like to make a lot of one finger jokes. I will make jokes as we're going through today. I'm very comfortable. I was born this way. I didn't wake up this way for the first time this morning. I did wake up this way, but not for the first time this morning. Um, but again, I'm, I'm very comfortable with that. However, if that makes you uncomfortable. I'm also comfortable with that, so I'm good and we'll have a good time today. Super happy you're here. Um, Matt and I have been working through this, uh, excited about this deck. How many of y'all are using, by the way, if you're not familiar with y'all, that's everybody in the room, so that's a Texas thing. So there you go. But there's all y'all too. So how, how many of y'all are using ECS now? Oh, OK. All righty. Good. How many of y'all are doing event driven architectures with ECS? Well, why are you here? Alright, so you wanna, you wanna learn how to do it better. OK, we really gotta, we gotta meet the bar. Yeah, exactly. Alright, there's a lot of pressure on us, uh, to hit this. So that's what we're gonna be talking about today is event driven architectures, uh, but using Amazon ECS with AWS Fargate, and that may not be the first thing you think about. You may not go, Oh yeah, EDA, we think Fargate, we think UCS, but it's a very viable solution. Today we're gonna talk through that. All right, you ready? Let's go. Here we go. All right, we wanna introduce you to a buddy of ours. This is Sarah. Now I know we've all been, how many, let me ask you this, how many of y'all are developers? OK, how many of y'all dabble with code but don't claim to be a developer? Alright, so you're a developer, stop, stop hinging on that, alright? So if you dabble with code, you're a developer, right? This is Sarah. Sarah runs a mid-size development team, right? And they use ECS. They primarily use the ECS and they love ECS and they love Cerra. Right, and here we are, and we've all been there. This time it's November twenty-seventh. Do we have the dates right? That might be wrong. Did I change that? We'll see anyway, it's Black Friday. Maybe I have the dates wrong. We'll see on that, but it's Black Friday. It's late in the evening. Anybody been watching their architecture right before Black Friday? If you haven't, you haven't been a developer long, right? I've been that developer in the middle of the night and you're watching, you're good, you're solid. Things are looking OK. Maybe there's a little hiccup, but we're OK. And then all of a sudden 10 minutes later, everything falls apart, right? 10 minutes after you drop a brand new SKU, something exciting is coming out, your system can't handle it. And this is where Sarah's at. And we've all been there at 3:30 in the morning. If you haven't had your head on a desk crying, you're not a developer. Right? Or put your head through a wall, right? So we've all been there. But here's the truth, OK? This isn't an ECS problem. It's an architectural problem, right? If you don't know who that is, that's me. So I don't, I don't, or Matt. It really could be Matt too. Really it's just a beard that makes a difference, right? So what went wrong? If you think about this, what went wrong? Well, like so many people, when Sarah built her architecture, she built it using synchronous microservices. Now microservices are a good thing, right? We're built, we, we, we hear that all the time, microservices, small, you know, independent services that. Run together we love that, but the synchronous part can kind of get us into trouble. And why is that? Well, when we make that order we get it, we hit the discount, everything's good. The next thing we're gonna do is we're gonna check our loyalty and oh no, our loyalty blew up. So what happens is the whole thing kind of fizzles and falls apart. Because we've tightly coupled this architecture and it's all kind of dependent on each other and this probably isn't news to a lot of you, right? You're like, oh yeah, yeah, I've seen this before, but you might have some architecture this way. Anybody responsible for that old 1988 code that's built like this, right? I've been there, right? And so it's like, well how do we fix this, right? So in reality, Sarah needs a different way. So, when we say a different way, you know, let's, let's look at the synchronous model and see kind of why we wanna change that. So you think about synchronous response to request models or request and response models, you think, well, these are good, right? They've got some advantages. They're low latency. They're very fast, right? It's simple. Yeah, I can, I can, anybody can build a synchronous microservice, right? And it fails fast. It's very often, or it's not very often that we like to fail, but when we're gonna fail, we want it to fail fast, and we want it to fail in development, right? So that's a good thing. But there are. Some pretty big disadvantages to this as well, ones that are worth probably looking at getting out of it. So right, because the first is we can get throttling, right? So when you build these out, your producer, it needs to know about your consumers, and as it's going through, uh, it gets complex. It can get throttled because it can't send everything, or maybe your consumer gets throttled because it can't handle everything, uh, and then you get resiliency issues and you have problems on Black Friday at 3:30 in the morning. And that's where you're at. So how do you fix that? Well, that's what we're gonna talk about today, and we're gonna talk about that in relation to, you know, running microservices and running things on ECS. Now. How do we, how do we think that you should do this? Well, really, in that this is gonna be a little oversimplified, but it really comes down to not using a or synchronous microservices, right? It, it comes down to what we like to do is insert a broker of some type. Right, what we want to say rather than in that previous model where the producer talked to the consumer and the consumer responds to the producer, instead we're going to put a broker in. Now this broker is a very kind of, kind of just a term we're using. I'm not picking a specific one. We'll actually get into that in a little bit, but this is the idea of something's in the middle and their entire job is to go, hey, I got your request. Have a good day. And then the consumer either through polling, pulling or pushing or any different kind of models will say, hey, give me that or hey thank you for that, and I'll acknowledge and maybe they acknowledge to the broker, maybe they acknowledge back to the client, there's different patterns we'll look at that. But this idea of having something in the middle to help break these apart and decouple the producer from the consumer is where we get into a lot more flexibility and reliability. Right? So, We're talking about this idea in what we call event driven architecture. Now, let's look at what does event driven architecture mean. I'm gonna read this to you. Event-driven architecture, EDA is a distributed computing paradigm implementing asynchronous message passing semantics through intermediary message brokers enabling temporal and spatial decoupling of system components via non-blocking IO operations and persistent event storage. You can go. I've given you all you need to know. Anybody understand that? I, I, I intentionally had Claude give me, give me a complex explanation because sometimes EDA is a mystery to us. We look at it and go, oh, that's scary stuff. But in reality, I'm gonna give you a more technical definition of EDA. And if you were ever gonna write something down or take a picture, this is the time, because this is gonna get super technical. I might have to do it a couple of times. So let me explain EDA to you. You ready? Matt still doesn't understand this, but I'll give it to you, alright? I'm getting that. OK, yeah, alright, so here's a less technical but it's still a technical definition of EDA. Something happened I And we respond We react. I got it wrong. We're gonna do it again. Did everybody get that? Let's try it again. Something happens. And we react That's EDA. Yes, that's oversimplified, and yes, it's a little tongue in cheek, but it's the idea of rather than our consumers going, What are you doing? What are you doing? What are you doing? What are you doing? What are you doing? Or our producers going, Here's what I'm doing. Here's what I'm doing. Here's what I'm doing. Here's what I'm doing. Here's what I'm doing. And, and the logic getting very coupled, we're able to say, let's break that apart and make them kind of, you know, decoupled from each other. So what does that look like? Well, here in the before EDA we have this producer who's talking to the consumers, and the producer needs to know about each of these consumers. He needs to know, hey, who's consuming my data, uh, and you know, and so it needs to send to each one. So if something happens, it's gonna send to the consumer 1, send to consumer 2, send to consumer 3. We kind of saw this pattern already. So let's throw in a broker, some kind of broker, and we break this down and we say, OK, now instead I'm gonna just send out an event and I don't care what happens after that. I mean overall and you know metaphorically and and and uh and allegorically, I don't even know the right words it we care but the producer doesn't care just says, hey, I did my job here's an event whoever consumes it, do it so these consumers go, OK, yeah, I want that. I want that. I want that. And so there's some benefits to this. When we build this out and we say OK, we're gonna break these apart, it gives us the ability to build very scalable applications, right? And so we're able to, to build bigger because we're not so tightly coupled. It also gives us resilience, right? So if consumer A goes down, consumer B is still chugging along. If there's some dependency on each other, we can. Handle that, but they can still they can handle that gracefully. We can degrade gracefully, right? And finally, it gives us a lot of agility when you build this way rather than constantly updating that producer to send it to, I, you need to know more logic to talk to these consumers. We need to then all we do is we add another consumer and say I'll consume that as well so we can do that unbeknownst, that was a big word that was almost a British word right there, unbeknownst to the producer, we can do that right. This is a really cool pattern, however, here's the catch. Like so many of you, Sarah's asking the question, how do we go from synchronous microservices to event driven architecture? And that's why we're here today, right, so I'm gonna turn it over to my buddy Matt, and Matt's gonna walk you through some of this, and we'll be, we'll be talking about some things. So I think this is often when someone like me gets involved, so you've got a customer who's had a, a major problem, um, they wanna make some sort of transformation or modernization, and we've gotta think about, you know, what's, what's the kind of mental model we're gonna use to go through this journey from kind of monolithic synchronous services to EDA. I like to think of. Three real buckets of um kind of categories of questions we want to think about. First we've got our business logic, so the actual stuff that we care about as developers, the stuff we've wrote, we've got, we've gotta run that somewhere, so we've gotta think about what's our compute. Um, uh, Eric talked about event routers and he just, you know, put, put a kind of, uh, a nice green circle in his diagram and said, hey, we've now decoupled. Uh, there's a lot more subtlety to that, so we've really got to think about what event routers are we going to use, what are the, what are the trade-offs, what are the patterns that we can apply. And then we've got to take those uh components and we've got to combine them into all of these architectural patterns and blueprints that we want to do at scale, because whilst something happens, we react to it. It's really, really simple on that side, actually there's a lot of subtlety. Actually within the category of event driven architectures, there's a whole bunch of um quite precise sub-patterns we can use that have their own trade-offs and complexities, and we want to kind of codify those so we can use them at scale across our organization, baking best practice into what we do. So first off we've got some, some options around compute, and I think that one of the really common things that I see customers do when they come out of a big outage and, and, and your CTO is, is, you know, grumpy, hey, we built this modern microservices thing, we're running it on ECS and we, we still went down, um, they kind of throw the baby out with the bathwater. I, we're gonna throw all this away and we're gonna refactor to lambda because we've seen that lambda is, is scalable and lambda always, you know, AWS takes care of it and we don't have to think about it. You know, that's, you know, lamb, lambda's great at scaling, but you know, you can still with bad architectural patterns, get yourself caught into, into, um, some, some dead ends. Or let's, let's throw away this ECS thing. What we, if we move to Kubanettes and the Cuban Etes ecosystem, maybe we can solve all of our problems with this technology change, and, um, yeah, that's gonna be more scalable, more powerful, look at all these cool open source things that we can buy and we can use this whole ISV ecosystem. Well, well, well, well, actually, you know, I, I'd like to propose that when you're going through a big architectural change, you wanna, you wanna really focus on the bits where you can add value, and you probably don't wanna change everything at once. If you're changing programming language, don't change architectural pattern. If you're changing architectural pattern, maybe don't change your compute. You know, so limit yourself to the, a sort of a, a, a set of things that you can change. And also, we wanna go on a, on an incremental journey. We don't wanna say, hey, we're gonna. Spend the next year building a brand new platform that's gonna be ready in early November, so when Black Friday comes around we can throw a big switch and underneath them. We wanna make a series of changes. Every week we're iterating towards this path over the next year, so when our next big sale comes on, we're, we're all ready. So, my hypothesis is, let's go and do this, let's stay on ECS and let's see how we can, we can build uh this with, with ECS because we're gonna make a whole bunch of other uh interesting changes. A little bit of um info about what ECS is, I think we've got a lot of hands raised in the room, so I won't belabor the point too much, but I think it's, it's, it's, it's a pretty cool service run by AWS that allows you to deploy, run, launch containers at scale, does a whole bunch of optimisations, uh, behind the scenes for you, so you can focus on, on delivering business value and. Customers tend to agree, so something like 3 billion Amazon ECS tasks, uh, are launched every week, which is like a, a completely wild number. And, uh, a really interesting one that I, that I really like is that when people start building with containers, they, they, the majority of them start building with, with ECS. Um, so it's a really, really common, uh, pattern. When you're using the ECS um control plane, there's a couple of options with, with your data plane. You could use Fargate, uh, our kind of serverless data plane where you just define, uh, your, your tasks and your services and, and we'll manage that compute for you. Um, we have a isolated, um, workload model where you get kind of strong tenancy, um, and you only can pay for the compute that you're using. However, you know, we're making some choices and, and, um, on your behalf. We think they're sensible choices, but they're not the right choices in every case. You can also bring your own data plan, so you can say I want these specific instance types in this size and I'm gonna manage scaling them out and and and Fargate is gonna go and run them. One of those EC EC2 instances. Um, one of the big pre-invent releases a couple of weeks ago was ECS Managed Instances. Um, just, uh, I'll do another little poll in the room, who's seen the ECS Managed Instances release in the last few weeks? Has anyone heard of it? Yeah, a few hands, but um it, it's, it's pretty, pretty early still. EC um ECS managed instances and the Lambda Managed instances that was launched last night are really, really interesting models where you can bring your own data plane of EC2 instances, um, but AWS does all the management of those instances on your behalf. So we'll, um, do all the patching, OS OS patching, um, management of those instances, you can just define what instance types you, you wanna use. And um so you get a lot of the benefits of um using uh the kind of flexibility that I want a specific graviton instance um in, in my fleet because I've worked out my Java code runs best on, on, on those instances. Um, but you don't have all of that, all of that overhead. It's also got a really interesting, um, modification to the capacity provider, that means it scales faster than running your own EC2 instance fleet, which is really, really cool. So that moves the shared responsibility, a bit, a little, uh, a little bit, so instead of having to think about your own auto-scaling and launch templates and uh availability, uh, distribution and instant selection, you can hand that over to, to AWS, you can define the instances that you want in, in your, in your task templates, and, and we'll take care of it. And you can get all the benefits of, um, your savings plans and, and all the kind of capacity planning, etc. However, it doesn't mean that Fargate isn't still the kind of default way that people are building on, on ECS, and I think that most people should probably start with the most serverless option, which is, which is Fargate and, and only break out they've got. Um, more complex, uh, requirements. So, we've chosen ECS, we're gonna continue running on Fargate, we, we think that's, that's still a sensible option. There are a couple of different models when, when you're running with, with ECS. The first thing is running with a stand-alone task. So with a stand-alone task, something happens, we get to run a task event. And the Fargate service goes, OK, great, something's happened, let's react. Uh, so we're gonna go and provision some compute to deal with that event, and we're gonna, um, take some time provisioning, so we'll go through a provisioning state. We're then gonna go and run that task, uh, so we're gonna do some running, and then afterwards we're gonna get that container and we're gonna throw it away, uh, stop it, and, uh, not reuse it. Now that's a really, really good pattern. Really powerful, uh, but it is newing up a container to, uh, run each of your events. So it's not really appropriate if you're sending thousands of events, we're trying to do our Black Friday sale, we've got 1000, uh, uh, orders a second coming into our e-commerce store. We don't want to be using this model for that. It's gonna get us into some trouble. We also have this, uh, this other model where we can run as, as a service. So this is, uh, we can define, um, some compute, so we're gonna say we want a VCPU, we want a couple of gigs of RAM, uh, we wanna have at least 4 copies, and we wanna scale that out with some scale parameters. And uh Eric's gonna talk about some of the metrics you can use to scale out later. Um, and so we're gonna go and build a target group that, that's gonna be running, and actually we we can think about how we can send events that kind of statically stable compute and how it's gonna scale out and and Eric will go into some more details around that. So, we've got these kind of big patterns and, you know, my, my hypothesis is these are, these are really powerful, really useful uh characteristics to use when we're building EDA. We've got scalability, we can have kind of target tracking, we can build all sorts of interesting target tracking with custom metrics. Uh, it, it, containers and microservices and, um, these kind of patterns are, are, are, are, are very comfortable, uh, together. And because it's an AWS managed service, it has native integrations with all of the building blocks we use to build event-driven architectures. Things like step functions, uh, EventBridge, uh, even SQS and SNS all play very, very nicely with EventBridge. And you can use all of those things like managed instances and savings plans and Spot to make sure that you're uh getting the um the best uh price for your compute, and the best total cost of ownership. So, settled on some compute. Our next big category of questions is, um, what event brokers are, are, are we going to use, what, what routers are we gonna use? And so our first category of brokers are routers. Routers are, are a pattern where um the router processes one event at a time, you can point that event at multiple targets and protocols, so it's a one to many pattern. You're building logic at the router, so you, you can build rules and uh filtering rules and, and targets at the router, so you've got quite a smart, sophisticated router that's doing a whole bunch of work for you, and that means that the router can handle things like retries, dead letter queues, error handling, even things like throttling and rate limiting. So, it's, it's a pretty, a pretty smart router, uh, which means that it's really simple to, um, uh, have simple logic at your receiver and your sender. So for your receiver, a single, uh, SDK call, your, your, your receiver just has to either, um, pull an event through a native uh integration or a simple API call. So you don't have to build a lot of error handling and additional logic in your producers and consumers. And so that allows you to really, really efficiently reduce coupling. In the Amazon world, we're usually talking about EventBridge when we're talking about this pattern. So it's a, it's a server service, has native integrations with, with most of the things on AWS, allows you to build global architectures, um, with global endpoints. And so the EventBridge architecture, it starts with defining some event sources, so that could be an AWS service or custom events, and in our case, a a lot of custom events. Those then go to a number of buses, so a default event bus, a custom event bus, depending on whether it's an AWS event or your own event. And then we have these rules, so a rule is the way that we then map those events into the downstream services. So the first thing I'm gonna do is define uh. What is an event, so this is one of the things that we have to be really deliberate about when we're, when we're building event-driven architectures, is that we go sort of one step deeper than something happens and we react, uh, something happens in the past. And so we emit a signal that state has changed, um, so an order has been created, an order was created or a channel was created or the lights switched off. Um, we can't change it, so we can't unswitch off the lights, we have to submit a new event that, uh, says, hey, lights are switched on. And these events act as this contract between our producers and our consumers. Uh, in EventBridge world, that's a piece of Jason. And an event bridge rule is pretty simple. So we've got a bit of Jason, which is our event that our producer has omitted that event. And our rule has another bit adjacent, which is, hey, our source is com.flix, our region is uh Australia and New Zealand, and we want to get all of those events and pass them to a particular consumer. Uh, they've matched, that's great. And so those consumers, uh, can be a whole bunch of different patterns, so we can point to lambda, um, and lots of event-driven architectures are built with lambda, but we can also send that to Amazon ECS as we discussed earlier. Um, you can also talk to API destinations, and that's a really, really powerful feature of EventBridge, and and Eric's gonna go into some detail around the nuances of that shortly. And a whole bunch of other services. However, this is not the only pattern we've got for event routers, we've also got topics, topics are a subtly different form of event router. Um, here we can broadcast to multiple consumers, we still can filter consumers, uh, at the topic. However, this is really powerful for scaling out to um lots and lots of consumers. We can support multiple protocols, things like SMS SMS um push certifications. We also have options to maintain order with first out, first in, first out topics. And so, um, A fully managed service for doing this in AWS is SMS. It allows you to, um, send nearly unlimited messages per second, and it says that in our documentation, nearly unlimited, and so, so we're kind of serious about it. Um, you can, uh, it's very, very difficult to put any strain on the SMS service, and you can send out to 12.5 million, uh, subscriptions on each individual topic. So if you're a fan out and you wanna send, uh, codes or messages to everybody, um, that's really, really powerful. So we talked about routers, our next kind of category of things are events stores. So an Events Store allows our receiver to process events in batches and control how it's doing processing, um, so it can, um. Control its order, it can configure its own retry, it can, uh, it, it can configure its own dead letter Q. Now one of the important things about a queue is only one receiver can process a message, so it's a 1 to 1 integration rather than a one to many integration. Um, you can manage that, that ordering, but it's typically not used as the backbone of our adventure driven architecture because of that 1 to 1 pattern, it's used more as a kind of some of the sub-patterns we're gonna, we're gonna think about. In, in AWS, uh, the, the, the main service for doing that is SQS. It's another one where the docs say we scale nearly infinitely, super easy to use, uh, simple API, download the queues, first in, first out, all the kind of features we expect. We also support some of the open source uh services, so things like Amazon MQ, um, which supports Rabbit MQ or ActiveMQ. So if you're using some kind of integration with existing open source stuff, definitely a sensible option and supports many of the same patterns that you'd be using. And then the third and final, uh, big bucket of, uh, brokers we can think about are, are streams. So streams are um subtly different again, uh, here we can do one for many, so we can send messages to uh multiple consumers, however, all the consumers on that are reading a stream are gonna get all the messages, so the consumer itself has to manage the filtering, um. You've got ordering enforced uh in the stream, but again the consumers have to um manage their own kind of error handling and uh processing to ensure that that ordering is enforced. Uh, really, really powerful service, particularly if you're doing really, really large, um, throughput of messages. Um, and so we've got Amazon Kinesis as an AWS native, uh, version of dealing with streams. You could do gigabits per second of streaming, um, all the kind of integrations that we talked about with, with native integrations. And the big player in the open source space here is MSK or, or, or is Kafka. Um, so Apache Kafka, absolutely huge project, really, really powerful. streaming solution which has loads of open source integrations and used for lots of event-driven architectures. Um Uh, our managed service for that is MSK, so you can use all the kind of Kafka goodness and, uh, get lots of the management done by AWS. So we've now got this kind of box of toolkits, we've got some compute, we've got some routers, and we've got all these kind of services and, you know, really, really, uh, easy to start putting some of those services into our diagrams and, and starting building things with, with, with these things, particularly where, um, you have to send, set up a, an event bridge bus, you know, a, a few lines of code, a few lines of SAM or CDK or terraform, and, and, and you can be up and running. But they're actually, you know, they're very, very powerful, there's a lot of places you can go wrong. So, so the kind of, the, the, the meat of what we wanna do is, how do we turn all these Lego bricks, all of these, uh, different building blocks into useful things that we can build. Uh, so Eric, I think you're gonna talk to, to us about some of the, uh, patterns we can apply. All right. So we've talked to you about, you know, what is EDA. One of the, we obviously all the, the, and, and for those of you here in America, routers is routers. So just throw that out there. I know some of you are like, well, I don't even know what a router is, but it's a router, so I got you, yeah, I got you. So alright, so we're gonna talk about what are these patterns. How, how do we apply this to ECS? How do we make that work, right? So let's jump in, uh, and we're gonna talk about long running containers, right? And this is kind of those services that Matt was talking about earlier. Um, and so this is like it's a push or pull, it's kind of an end to end, right? It's how many do you wanna send, how many do you wanna process, OK, so a traditional long running container looks something like this, right? You have an ECS. You got many different type of, of tasks with different containers running in that, you know, a lot of ECS users in there. I don't, I don't really need to probably teach you about that. And of course you have your capacity providers with our new, uh, again, the mayor's instance, uh, which is super cool. You could do a lot with that. So I wanna show you we're gonna, we're gonna go through some different patterns and the first pattern is probably the most simple, right? And this is a very common pattern. Pattern number one is the public API. Now what this usually is, is you're gonna have some type of, of consumer or producer, uh, uh, I'm sorry, producer pushing data to a consumer, right? And it's gonna go through an API. So somewhere another machine they've got a signed, you know, uh. Uh, URL or they've got something and they're they're pushing to this data, but your consumer, even though you run this container, maybe it doesn't, you know, again we have that 1988 code and it's not ready to handle all that so how do we put a broker in something like this? Well, Event bridge is where we is where I generally start. This is a really good way to say events coming in and I'm gonna send those and then I'll push them on to the container through the API. And how that works is the producer is gonna send directly into the event bridge. He's an IM I IAM IM that's a hard one. IAM uh rolls. It has permissions to do that. It's gonna. Push data into the event bridge and EventBridge is gonna use this this API destination to make a secure connection to the API and drop data into or drop drop events into that, uh, the consumer and it can run, uh, however many we need to and we can kind of control that through, uh, through event bridge and how many are we sending different things like that. Now you may say Eric, that's great, but not all of us are, are doing machine to machine, right? What if it's clients? That's all right, I got you. So you can actually do something like this where you can say, OK, I'm gonna have an authentic authenticated user send data to API gate. But let's say they're sending, I don't know, analytics or something like that, and then I'm gonna use anybody familiar with the VTL? OK, do you love it? Yeah, I know exactly. He's like, I, I wanna tackle you for saying it. So VTL is Apache velocity temping language and it runs that API gateway and you can actually transform a request come from a user and put it right onto a bent bridge, and I've got some examples out in the wild of how to do that, but this allows you a secure access where they can continue to push that and I don't have to run a lambda function in in between and I can still get data to the container. So this is pattern number one, and writing a VTL is, is no fun at all if anyone's tried it, but Quiro is really good at writing. Oh yeah, code. Oh yeah, yeah, yeah. A couple of prompts and you'll, uh, you'll have something working really, really quickly. Yeah, BTL is back because of, of Quiro and, and but it's velocity templating language, by the way. I'm just proud I know that. So there you go. Alright, so the second pattern looks a whole lot like the first because it, it, it really is, right? And so what we're doing here is we have this ability in EventBridge with the and we announced this last year with, with, uh, private link and VPC lattice. You can actually connect to private endpoints. Right, so this is super helpful if you have to look, I need to throttle how much is getting in there. So let's go ahead and drop event, uh, uh, data into event bridge, and then we'll use the private endpoint connection. And now we've done that. So those are, those are the first two patterns again, kind of starting easy, and we'll climb up there, right? So the next pattern we're gonna do is what we call point to point. Right, and so this, this looks pretty simple, but what we're doing here is we have ECS containers that pull for events. So instead of things getting pushed the ECS container, it's gonna actually pull data off the queue, and so you manage the rate limit in the code. You can say grab 10, grab. 20 but you can also do some of that, you know, the window and and control on, on SQS as well, and your retry and failure are managed at the queue as well. So if something doesn't happen, you get a retry out of that, uh, and so this gives you a lot of control and downstream protection right now. When you're running queues of pulling patterns, it looks kind of like this. You have, it'll basically take, uh, let's say you have 6. So I'll grab 6 off there and my different tasks will handle that, uh, kind of like Matt was showing how the cus work on that. And then, so how SQS works if you're not familiar is when a task is grabbed, it doesn't actually disappear out of the queue, it's made invisible. It's magic. It's the invisible cue or the invisible event. And after a certain amount of time, if the if the consumer hasn't said yep I took care of it, then it's made visible again and it can be redone, right? So this works really well when you have, you know, your traffic's metered and you can do that. However, what if your traffic is unpredictable, right? It's like, oh, I'm starting, I get, I get a lot of traffic and then I get a little bit of traffic and then I get a lot of traffic and, and it goes up and down like that. Well, that's all right. We can handle things like that through what we call predefined metrics or, or through, through auto scaling, right? So with auto scaling we're actually able to do step scaling and SQS through using the message count. So we look at the message count and how do we step that up to handle the auto scaling and how to bring it back down. In this particular example, I'm doing +1 task at 5 messages, plus 2 at 15, so it's watching that and it's going up as need be and it's coming down. And then we put it in a cool down of 300, 300 seconds so we don't, what you don't want is your auto scaling doing this, right? You want it doing this. How relaxing was that? That was cool. Alright, I feel good. Alright, so you want, you so you wanna do that. So that's why we put in that cool down and then, and then you get multi-period evaluation. So we're looking over, we're not just looking at one and reacting, one and reacting, one and reacting. It's looking, OK, how's the scope look, right? So this works really well for a lot of your traffic. So you, you, you use these predefined metrics, uh, and, and, and there you go. However, not all traffic is the same size, the same shape. Traffic looks very different, right? I don't know about y'all, but you never know. Am I processing a 2 minute video? Am I processing a 2 day video? My, you know, those kinds of things. So what if this looks really big and your, your size is not consistent? How do you do that? Are these pretty fine metrics gonna work? Probably not. So then then we get into custom metric math. Now I've never been great at math, I'm not gonna lie, I can only count to 2. Alright, some of you are awake. I, I can actually, here's the truth. I can get to 4 if I take my shoes off, but that gets weird for everybody, so we'll go with 2, right? So the idea here is we're gonna use custom metric math and we're gonna combine some metrics, right? So we're gonna do metric messages divided by the running tasks. We look at the pack log per task, and we're gonna go for a target value of acceptable latency. Divided by processing time, right, so we're looking at some different things and then we set this up, we still do our cool down, so we're not, you know, reacting really fast and it and, and we prevent division. You can say I've got some code in there, uh, by doing a max function so you prevent division by zero. And so this allows us to do some mathematical, uh, equations. I'm saying all kinds of big words. Some math to figure out how we're gonna do it, but not just one metric, you're using multiple metrics to do this. And so this is an idea of how that policy would look and how you set that up. Well, well done, Eric. I've been watching Eric rehearse that for the last two weeks and trying to get his maths out, so I can't do math to save my life. Like, I can't, I, I don't know how to do this, so Matt, Matt's been tense up till that moment, so, alright, thanks buddy. All right, so that's how you do that. So when you're gonna do metrics, don't just let your system run and go, Oh, I sure hope it makes it, right? You have the story. You have the observability to go, OK, we can follow that. That's really important anytime you're doing anything but really specific in EDA because you can build really optimized systems that work, uh, that, that, that run with your load rather than just saying, alright, provision till, you know, we can handle anything and pay for that, right? OK. So the next thing we're gonna get into are what I call event-based containers. Now, I haven't made that a popular term yet, but every reinvent for the last two reinvents I've said this, someday it's gonna stick. I'm coining this mark. I wanna see it in a blog. So alright, so event-based containers. So what I mean by this, it's kind of like you've. Even driven containers but but a little different and and maybe I'll explain that and maybe I'll forget but it's this idea of a 1 to 1 right so one and and and Matt explained it earlier when we wanna we wanna run one container for an event so what does that look like? Well, so we got a producer and we're gonna kick an event into event bridge. Now, normally if I'm running a container on Fargate, I will have a service and that service will have a task definition. This will all be at at the container level. But now with this I can actually do the the task, uh, definition at Eventbridge that actually will come with the event. I can say, hey, here's a definition. He's an event, do your thing. Right now I'm not gonna get any data back. I'm just gonna get a simple, it's, it's basically a fire and forget, but that works for a lot of what we're doing, right? So your producer is gonna kick a single event out that, uh, event bird's gonna take that and it's gonna send the, send the task to it calls the run task API, and it's going to run that, and you get one task, uh, uh, versus to one event, and actually it's not necessarily one container. Uh, you, you wanna look at it, it's probably it could be multiple containers because you might have multiple tasks, sidecars, things like that. Alright, so this is the pattern 4, this is the event bridge, uh, pattern. This is very popular, and this is really here. You don't wanna use this for dump a million events in event bridge and bring up a million, uh, containers. That's not how the, the run test API will get overrun. It's not designed for that. You wanna do this. We, we actually built this, this exact example here a couple of years ago. We were doing video processing and serverless because it can be done. And we were running video processing and we would say, OK, if the video has these these maybe file size or or time limit because we had the metadata, if it's under 2 minutes, process it in a lambda file because that's all we need. If it's over 2 minutes, spin up a container. Most of our videos were under 2 minutes, but every once in a while we had those one-offs, right? So this is a, this is a really good example of doing that. We could do rules for that to set it up. Alright, so the second, uh, one looks somewhat like this pattern 5 is step functions. Who uses step functions today? Oh, do I love step functions. I'm a step functions nerd. I believe step functions first, uh, and I'm super excited about it. So step functions can work much like Eventbridge in that it can actually have the task definition and you send that to Event bridge. But it gives you a little bit more flexibility. It gives you synchronous, synchronous, synchronous and asynchronous options. Now that means blocking and non-blocking, right? So. Let me show you how this can work, OK? So, the as sync pattern, this is the first one we're gonna do, and this is kind of a fire and forget just like event bridge. So what we do is we use a run task, uh, and so we do, it's an async, it's a fire and forget, and it returns an acknowledgement of the request. Yes, I got your request. I'm working on it. Move on. And the task, the step function will actually move on from this point. It won't wait. You don't get any data back, off you go. However, sometimes there are times when you need to stop and wait. You need to know that that was successful or failed. So we also offer what we call the dot sync pattern. This is kind of cool, not kinda, it's really cool, because it handles the polling for you, right? So what happens is when it sends, it sends to the run task API and then it pulls the described ECS task for a status. If it worked, then you get a 200 back. If it failed, then you get a 400 or a 500 or whatever going on, right? And so, however, you still don't get data back from the ECS and a lot of times this is OK, but this is if I need to poll services, and this happens a lot. OK, but you may say, Eric, yeah, we need data back. How do I do that? Well, let me show you. So, next pattern we're gonna talk about this is the callback pattern using the token and send uh task completion. Anybody use this now? All right, a couple of you, OK. Anybody like, wow. Well, I haven't explained it yet. Let's wait on that question. All right, I will get a wow out of you. All right, so the way this works is when you come to this task, it's gonna generate a token, and it's gonna send that token with the data and the task uh definition to the run task API and it's going to bring the task up, right? And then it's gonna give all that data to the container. And the container will turn on and it'll sit and wait and it'll wait for that task to be done. So in the container when the job's all done you actually call the service there's an API endpoint send send success or send fail, um, and you call it with the callback token. And step functions goes, oh, I know which workflow that is based on your token. Here you go, success or fail, and here's the return data. OK, so you're able to get data back to the step function and modify and work on that later. So this is a very, very powerful event driven model that you can do in step functions, right? OK, so then the task needs to invoke step functions and the API goes to complete and, and off you go. Now, There's one more pattern I wanna show you that I'm gonna be really honest, I just learned about last year. And I think it is super cool and I think you're fairly you just learned about I, I learned about this in, in practicing, uh, when you talked about it a few months ago, Eric. So, uh, is anybody using activities and step functions? OK, anybody heard of activities have functions? OK, one person, but he knows BTL too, so. He's kind of a nerd. All right, so, let me tell you about activities. So here's how, how activities work. When you create, you go into the console, you can do it through the API, but you actually create an activity outside your step function. It's, it sits on its own. So you create this activity and what happens is step functions creates a managed SQSQ and it's gonna manage it for you and handle all that stuff, right? So then what you do is from your step functions, and not just one of them, but any of them, you can pump data into the pump events into this activity queue. And then you have a pool of ECS workers, yeah, we're tying ECS in here. This pool of workers then much like any other pattern we've seen, pulls for this data, does the work, it's got the same kind of interaction as SQS because it's an SQSQ and then sends back to step functions with the task token and the data. And now you've sent you, you're, you're handling tons. This is when you have, I have thousands upon thousands upon thousands of events and I need them to be processed from multiple workflows. This is the pattern for this now we did, we talked about this last year in, in, in this very session actually. And I will tell you this stuff really works. Don't just take my word for it. This tweet came out in September. It says a few months back, inspired by a great session by EDJ Gee. That's all I wanted to read. No, I'm just kidding. Let's go on. We made changes to to an AWS step functions reducing its cost from $450. Per invocation. To $1 If you're intrigued, check it out and, and, uh, I'll post that in the resources, but, uh, and you can find it. Now the first question is, oh my gosh, what are they doing that costs $450 per invocation? Right, but they were doing a lot of work, a lot of transitions, hundreds and hundreds and 100s and hundreds of, of like minuscule microtransitions that probably made more sense to run in an ECS cluster. Right? So this activities pattern handled that form. All right. So activities gives you several benefits. First of all, you manage Q, you get a 1 year retention then. Second, connection on-premise. You can, you, this is where, oh hey, I need to, I need to process this with on-premise stuff. You can do that from there. And finally, uh, loosely coupled architecture and flexibility to use spot, which is a lot of people don't think about this, but if I'm using clustered ECS I can use Spot for that. All right, so let's talk about how does this help Sarah. Sarah was in trouble. Let's show her. So now we've got a whole set of tools that we can combine with our, with our recess uh architecture. We've got all of our different uh brokers that we can apply, we've got all of these different patterns, uh, that we can apply to our architecture. So, so what, what does that actually look like? Um, we talked before, we are a relatively, uh, sensibly designed microservices architecture, containerized, um. The first thing we can do is start publishing some events, so we've got our, we've got our order service, um. Every time an order is created, we can go into create an order created event, we can make a single STK call, that's gonna publish it. To EventBridge, we now want to go and do something with that event, so maybe we want to take out our payment service that um. Uh, it's currently happening synchronously, uh, but depends on some third-party APIs, we want to go and make that asynchronous, so we accept the transaction, we use some validation on the payment details. Uh, we assume that our payment process is gonna, is gonna process it, and if there's a problem down the road, we can actually go and send the customer a message and say, hey, can you come and, um, retry with some new payment details. Uh, we can do that with our existing, uh, service by having a rule that's got that API integration. We can do a private API integration into our VPC using all the stuff that, that Eric showed, and, so pretty quickly we can take two synchronous services and break them up using a really, really simple pattern, uh, and make them asynchronous. We also want to go and send a bunch of messages to our customers, so instead of having a direct SDK call to one of the messaging services inside our inside our code, um, maybe doing it synchronously, we can break that out, so we'll have a separate rule that's looking at those orders and sending you an SMS message saying, hey, your order's been confirmed, or actually you need to go and update your, your payment details. We've got our loyalty service, so a year ago this is the service that blew up, we've got a third party service that we were expecting to respond, uh, immediately. Now it's important that our customers receive loyalty points when they make a purchase, but it probably doesn't have to happen immediately as part of that flow, in fact, we're probably happy if it happens at some point before the end of the month. Here we can send all of those uh orders and the order details into a queue, and that loyalty service can process them on their own time, we don't need to scale out that service, it can just chug chug away uh and make sure it's it's kind of catching up with that queue. And then finally, we've got this really cool activity pattern that that Eric taught us about. Um, once a month we wanna go and run a process that's gonna look at all of our customers' sales and loyalty points and tier changes and do a whole bunch of processing and go and uh send them some personalized discounts, uh, to help them encourage them to come back and buy some more stuff. So here we can use step functions and activity pattern, and we can actually do that batch processing in a, in a really, really efficient event driven way. So we can keep applying these patterns in, in, in different ways to our architecture, sort of event bridge at the heart of it, using ECS for our compute, and quite quickly, uh, get to a, a, a new solution. So next year in, in November when, when things ramp up, uh, Sarah sits at a desk and, and nothing happens, and, uh, uh, boss is happy, everyone gets to celebrate. Oh Sarah, we're happy for you. That's right. So Sarah's happy She's now EDA partially and she's moved part of it. She's well, a good part of it, uh, and she could do a lot more. So here's what I wanna help you with when you walk away from here. Uh, if you're not doing EDA now, uh, it's not a flip of a switch, right? Uh, I, I love this statement. Berna Vogels, VP and CTO at Amazon says systems that don't evolve will die. He's also the one that says everything breaks all the time, right? 22 very well known quotes. So it's important that we evolve. If, if your system is good enough, it probably won't be soon. You always want to be watching, even if it's working. Is it working efficiently? Is it saving you money? So one thing we encourage when you're looking on your EDA journey is, where do you start? Well, you may not have full understanding. It's EDA like anything else, you learn as you go. So I encourage you to start small, start, start with, you know, some an implementation that you can start working on, and your understanding will grow with that. Uh, obviously don't start in production. Uh, just to hopefully you know that already, but there it is, uh, and, and, but as you grow with that, uh, you know, you can, you learn more, you go. So here's some key takeaways I would give you. Synchron of microservices crack under pressure. You learn that Sarah didn't fail because of ECS. She failed because the tight coupling, things like that. EDA is the ultimate decoupling superpower. It really is. It, it allows you to really build that scale and reliability. ECS plus Fargate brings the muscle. Service containers, 0 servers to manage, instance uh scale, things like that. Uh, and you've got routing options for days. It can give you a lot of flexibility to do that. ECS runs every EDA pattern like a champ, right? So we just talked about push, pull, sync, as sync, callbacks. Auto scaling becomes your superpower. Uh, ECS managed instances equals less hops, so you're gonna, you're gonna be able to, obviously we talked about the managed sys, how those are gonna help, and step functions unlock next level orchestration. So you're able to do a lot of orchestration on that. And finally, loosely coupled system systems. Uh, and yes, Sarah does win by running this route. We promised you some resources. I encourage you to check these out, um, S12D.com CNS 307-25. All right, here's a couple of other resources I'm gonna give you here. Let me go back. I saw some of you still trying to get that, so let me throw that up here for a moment. All right. And then, uh, the next thing I'll tell you is, uh, we've got a couple other resources I would encourage you to check out. Uh, I tried to get ones after this, uh, but the, uh, the best practices for service developers, uh, super strong, uh, session. This is a great one. API 207 using event-driven architectures to modernize legacy, more EDA, and then Serving espresso is a great example of event-driven application at scale. Uh, get the coffee in the expo hall or the certification lounge and then go see how it was built. I really encourage you to do that. If you wanna learn more about service and and uh application integration resources, there's, uh, we, we have a lot out there. Uh, on behalf of Matt and myself, I really wanna tell you thank you Matt. You have anything else you wanna throw out, shameless plugs. Uh, if you can make it to the Mandalay Bay at 5:30, we'll be doing a chalk talk diving into a lot of these patterns in, in more detail and, uh, really helping unpick them with the audience. I think that's the last one. All right, and for my shameless plug is on Wednesday I'll be discussing. At 10:30 And I can't mention what that is right now, but I encourage you to keep 10:30 Mandalay Bay open. If you love Servius, you'll love me for this. So that's all I can say. I wish I could say more, but I encourage you to do that. With that, I wanna say thank you very much. Please fill out the survey. Let us know. We always like to know how we can do better. Enjoy reinvent. Thank you very much.