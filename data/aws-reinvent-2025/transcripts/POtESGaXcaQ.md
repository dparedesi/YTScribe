---
video_id: POtESGaXcaQ
video_url: https://www.youtube.com/watch?v=POtESGaXcaQ
is_generated: False
is_translatable: True
---

Uh, and I'm, my name is Matslan Nir. I'm the director of engineering for ECS, so, uh, responsible for all things ECS and related. With me I have Alexander. Hey folks, I'm Alex. I'm a product manager for Amazon ECS focusing on Fargate. And we also have our distinguished guest. Yeah, hi, Ruben, uh, quantitative technologies at Q QRT. Nice to, nice to meet you. Cool. So, today we're gonna talk a little bit about what Amazon ECS is, what it works. This is a 300 sessions, so we assume that you know the basics of ECS since you chose to join us. Um, then I'm gonna talk about capacity provisioning with ECS, how you get to compute you need to actually run your applications. After that, Alexander will take over and talk about security and compliance aspects of running your applications on ECS and how we help you there and also cost optimization, how, what we do to make it cheaper for you to run your applications. And then perhaps most interestingly, uh, we'll have Ruben talk about the journey that CUBE Research and Technologies have been on ECS going from um a reasonable scale application to a very large scale workload on ECS and Fargate, which has been super exciting to see. So, what is ECS? Um, the simplest way of thinking about it is that ECS is a container orchestrator built for use on AWS where the guiding principle is that we take on as much of the operational chores as possible so that you can focus on your application instead. Um, there's no control plane to manage because we take care of that. Uh, if you use Fargate, there's no compute to manage because we take care of things like that. Um. So, looking at ECS, uh, there are 4 pillars we think about, um, for how ECS works. Uh, and what we build it for. So, the first one is that, is to launch containers at scale. It doesn't matter if, uh, your workload requires one task and runs at a fraction of a TPS, um, or if it's a very large scale application with tens of thousands of tasks running at the hundreds of thousands of TPS, uh, ECS is there to support you on that whole journey, giving you a consistent operating model regardless of what your scale is. Um Second is that uh you only pay for what you use. And what that means is that uh there's no charge for using ECS itself. Um uh what you do pay for is the resources that we provision for you. So, if you launch EC2 instances, well, you pay for these two instances, EBS volumes, um, low balancers and things like that. Uh, but you only pay for the things that you actually need to run your application. If you're using Fargate, for example, if you're not running a task, there's no, uh, no charge for Fargate, right? You only charge while you run Fargate tasks. Um, going back to what I mentioned earlier, um, we're all about, uh, improving speed and agility for you all. Uh, we want you to be able to focus on your application, be able to reach scale and production with your application as fast as possible and not have to fiddle with a bunch of infrastructure mechanisms. That doesn't mean that we lock you down so that you can't uh do advanced scenarios, but the default experience is to get you up and running with AWS best practices, and then when your use case requires it, you can dig into to more advanced capabilities and take advantage of those. And finally, um. As I mentioned a little bit earlier, uh, we embody AWS, uh, security and operational best practices in AWS. We try to set you up for success. Uh, part of that is that if you use things like Fargate, you don't have to worry about how to correctly configure an instance to run your workload. We take care of all of that. Um. With Fargate, you get task isolation. So every task you run, runs in, in, in its own easy to instance. And in easy to instance that is never reused. It's only used for the life cycle of the task and then it's recycled. Uh, and to give a little bit of context, uh, about ECS and how ECS is growing, um, so, uh, as of about two months ago, um, customers launched about 3 billion tasks on ECS every week. That's across all of our regions. Um. And looking at new customers who come to AWS, start running new workloads, and choose to use containers to run those workloads, we see about 65% of those customers choose to use ECS and the vast majority of them choose to use Fargate with ECS because it just makes their life easier. Additionally, a lot of Amazon also runs on. Um Both AWS services and um uh Amazon.com services run on ECS. Uh, and one specific example that we can share here is that during Prime Day this past summer, uh, the team supporting Prime Day launched about 18 million tasks on Fargate every day, uh, during Prime Day, uh, which doesn't quite include everything they launched leading up to Prime Day, but still, um. So it's, it's an indication of Fargate and ECS is ready to support the biggest workloads out there. So why do customers choose to use ECS and trust ECS to run their applications? Part of this simplicity. Not simplicity in the sense that ECS is limited and only gives you a limited set of capabilities, but simple because it simplifies operations for customers. Um. As I've said a few times now, we really want to focus our effort to help you focus on your application. We'll take care of the other operational chores for you. Um Other thing is secure, obviously, uh we, we all want that. uh, so we make use of uh AWS security best practices, we set you up for success with that from the beginning without you having to do anything explicit about it. And finally, efficiency. So, we're very focused on driving high utilization for you so that you can run your application at scale with high availability at the lowest possible uh cost to you. Uh, and we do that in a number of ways. So for example, with Fargate. Yeah, you tell us that you need a 2 VCPU task with 8 gigs of memory. Well, that will be provision for you and that's what you pay for. Um, if there's any overhead on our end on that, that's our problem to solve, not your problem. You tell us you want 2 VCPUs, we give you 2 VCPUs. Um, all kinds of things like that we do, uh, including scaling up, uh, when you need to scale up, scale down when you need to scale down. Things like that, that will, uh, make it easier and lower cost for you to operate your applications. So digging in a little bit into capacity provisioning, so how do you get the compute you need to actually run your application tasks? Um I'm not gonna go through everything in this slide, uh, but I, I do want to, uh, give the context of AWS provides a wide range of compute options, right? Um, and at the end of the day, most of it, uh, relies on EC2 in one way or another. Uh, the exception would be if you use ECS anywhere and you run on-premises, in which case, you're on the hook for providing, um, the actual compute. Yeah. But you can use ECS to run across all of these options. You get a consistent operating model. But we also see that the vast majority of our customers uh use either ECT or they use Fargate. Uh, so I'll dig in a little bit to what the trade-offs and benefits are um between using Fargate and EC-Two. Uh, so Fargate, uh, which many of you are probably familiar with, uh, provides a fully managed, uh, data plan for you. Uh, we abstract away the compute to the point where the only thing you need to think about is, uh, the amount of VCPUs and the amount of memory that you need. Um, there are a few other things that you can control as well. You can request more storage and things like that, but, uh, centrally, it's, uh, CPU and memory that you think about. So, we take care of everything else, right? If you need to launch 2 tasks, we'll provide a computer that can run 2 tasks. If you need to run 500 tasks, we'll provide that compute. Um, we take care of, uh, providing workload isolation by design in Fargate. As I mentioned earlier, uh, every task runs on a separate EC2 instance. That EC2 instance is provision for that task. When the task stops, we throw away the EC-2 instance. Um, so every time you launch a task, you know that I'm running on a fresh compute. There's nothing left from previous tasks or anything like that. There can be no toss, cross talk or anything like that, which for many customers is a key feature in Fargate. Yeah, I mentioned already about how you only pay for foggy tasks when they're running, right? As soon as the foggy task stops, there's no more charge for it. Um, so you, you get a lot of benefits with Fargate, but you also give up control of things. So if you choose to use Amazon EC-2 as your compute, you get a lot of control. You get to choose exactly which ECTwo instance types and sizes you use for your application. Whereas with Fargate, we use a wide range of applications, or sorry, instance types uh and sizes for your tasks. Um. Depending on region, depending on availability zone, uh, you may see us use, say, an M8I for a task versus an M6I, uh, depending on availability. We always focus on being able to launch customer tasks, uh, uh, prioritizing that over consistency in the underlying compute, uh, which is an important aspect of, um, Fargate. For EC2, you control all of that. But that comes with a cost, because now you're also on the hook for providing the army that we use for EC2 instance. So you have to make sure that that army and that instance is correctly and securely configured. You need to make sure that you patch your instances. Um, that's on you to take care of. Um, but on the other hand, you do get to take advantage of all these two, purchasing models. You can use reserved instances, you can use on-demand capacity reservations. Instant savings plans, things of that nature. Both Fargate and EC2 support the compute savings plan, so you do have that option available regardless. So, there are, there are a few tensions between using Fargate and EC-2. Uh, and we've had lots of feedback from customers telling us that, you know, we, we love Fargate, but we need a little bit more control um for our workloads. So in response to that, we launched ECS managed instances about 2 months ago. And what we're trying to do here is strike the right balance between um managed compute that we give with Fargate versus giving you all more control over what we use to run your workloads. Uh, so, a couple of key differences, uh, is that, um, with ECS managed instances, we launch EC-2 instances in your account. So you will see these EC2 instances, but you have no control over them. You can't take any mutating actions on these instances. Only we can do that for you. Um, but that also means that we control the entire life cycle of these instances. Um, we make sure that we launch them when they're needed, uh, because you need more compute capacity to run your applications. Uh, we make sure that after roughly 2 weeks, which is very similar to how Fargate operates, uh, we will retire the instance and we will shift your tasks over to a new instance. And we do this so we can patch, um. Your instances and make sure that we're always meet patching and solace. Um, you have control over the instance types. So you can either choose to say, no, AWS, I trust you, pick the most cost optimized instance types from my application, or you can say that, no, I know exactly what I need. Uh, I need network optimized graviton instances for my workload, and you can choose to use that. Or you can say, hey, I want GPUs uh for my workload. You can get those for ECS managed instances. Uh, we have a more robust cost optimization in ECS managed instances. So one is that we've rebuilt how we scale up additional compute for you. So we can scale up additional compute faster with ECS managed instances, uh, than we can with, uh, both Fargate and uh EC2. Uh, but we also uh implemented cost optimization that continuously looks for idle instances or underutilized instances. And when we find underutilized instances, we will shift tasks over to other instances and then be able to shut down those instances and reducing your costs as a result. And finally, um. We, we give you more control. So, many of you who are Fargate customers today, um, will know that you can't run privileged containers on Fargate, right? You can't make use of agents that use EBPF for example, uh, with managed instances, we give you this ability. Uh, by default, um, uh, nothing runs as privileged, but you can say in your task definition. Hey, I have this sidecar container. It needs privileged access uh because it requires EBPF and you can do that. So going back to the earlier slide, um. Where does managed instances fit into the picture? So it's right in between Fargate and EC2. Um, you get more capabilities, we still give you a fully managed, um, uh, compute experience, uh, but you need to be more aware of that we actually provision easy to instances for you, even though we completely manage them, uh, on your behalf. So a core concept in EC is capacity providers. We introduced this back in 2019. Uh, and, uh, originally we had the Fargate capacity provider, which is actually two capacity providers, the Fargate on-demand and Fargate Spot. Um, and we introduced the, uh, all scaling group capacity provider for easy to use. So with managed instances, we've introduced a new capacity provider, uh, that works closely with the whole, um, task life cycle cycle in, um. In ECS to make sure that you have instances available to you when you need them and only when you need them. So, looking at the target capacity provider, um, as I mentioned before, it's simple, right? You don't have to worry about, uh, two instances, you need, need not worry about, uh, configuration, patching, anything like that. Um, when your workload increases and you need more tasks to run your application, we will launch more tasks for you. When your workload decreases, we will scale down those tasks. Um, and all you need to think about, as I mentioned before, is the amount of CPU and the amount of memory that you need. For managed instances, um, I touched on some of these things already. Um, so this new capacity provider is tightly integrated with the rest of ECS to know when it needs to launch additional EC2 instances and when it should shut down EC2 instances. We launched these instances in your account, but they're fully managed by us. Uh, you can see them, but you can't terminate them, you can't take any mutating actions on them. Um And we control everything about the management of these instances. So for example, uh we provide the army use for these instances. You, you cannot control that. We use Polar Rocket as our container operating system because we think it's the right Linux OS to use for container workloads. Um We make sure it's configured correctly and as I mentioned, we run for about 2 weeks and then we replace instances shifting your workloads transparently. Um, one thing that you get with ECS managed instances is that you, uh, get to use easy to event windows. So if you want to control when we replace your instances, you can create easy to event windows and tell us, hey, use this 4 hour time window on Sunday mornings between 10 a.m. and 5 a.m. and we'll uh make sure that we only initiate operations in that event window. So this has managed instances you have the option uh to map the instance type to your workload needs, right? Uh, so you can use the default capacity provider then you don't have to do anything other than just selecting it and and trust us to pick the right instance types and we'll focus on cost when we make those decisions. Uh, obviously, uh, the tasks you run also matter. We try to bin pack, uh, a task onto uh multiple tasks on an instance, so we'll take that into consideration as well. Uh, but when you want to explore more capabilities, you can use the custom capacity provider and then you get to use what we call attribute-based instance type selection. So you can define things like, hey, I need a GPU. I need a GPU of this particular type. I need at least this number of VCPUs on the instance. I it needed to be uh network optimized. Any number of attributes like that you can control. So you don't have to pick specific instance types, although you can if you really want to. Instead, focus on the attributes of these instances, um, that map to what you need for your workload. So now we have EC 2, we have 4Gate, and we have managed instances. So when should you use these? So, um, we strongly recommend that if you're using ECS and EC2 today, you should go take a look at managed instances because we think we can give you a much better experience using managed instances. Uh, for Fargate, Fargate is the default in yesterday. It will remain the default because we think that is the best experience for the most customers. That said, if you're on Fargate today and you run into limitations on Fargate, then managed Instances is there to support you. So for example, if you need to run a 48 BCPU task, well, Fargate does not support that, but managed Instances does. Uh, if you want to use a logging site card that requires EBPF, well, Fargate does not support that, but managed Instances does. Um, if you want to, uh, be one of the cool kids and use a GPU workload, well, Foger does not support that, but managed instances does. So those are the kind of things that you should think about when you want to use, uh, managed instances. And with that I'm gonna hand over to Alexander who's gonna talk about security and compliance. Well, thank you, Maz. Security and compliance. What an exciting topic, right? So let's try to make it more or less. Painless. When we talk about security, we usually start with Looking at the shared responsibility model. And let's take a look at the screens right now. That's the situation with ECS and self-managed ECU. Well, AWS is responsible, of course, for the ECS control plane. It is responsible for all the foundational services, storage compute network and all monitoring, and of course, the global infrastructure. Basically, regions, local zones, everything that is actually physical. But you have to worry about And secure all your compute capacity. You need to think about how to provision that capacity, how to scale it up and down, how to distribute it across different availability zones. You need to think about instant selection and more than that, each time you launch an instance, you need to place the right. Ay, you need to make sure that it has the right version of the agent, you need to patch it, and of course, you need to keep monitoring all that stuff. On top of that, your application runs yet another layer of complexity. So that becomes rather complicated, I would say. That's why with Fargate and now with ECS managed instances, AWS takes on more responsibilities. So you don't need to worry about things that you're worried about with self-managed stuff, things like capacity provisioning and ECT instances. Everything that runs on them is fully managed. By AWS in different ways. And I'm gonna talk about it in a minute. So, the key point is that you're still responsible for your application. How the load balancing happens, how the security groups are configured. So you can configure security groups, define firewall rules, and of course, IM roles. So, basically, this is your responsibility, everything else is AWS. So, as you can imagine, that change in the shared responsibility model. Gives you way more time to focus on what actually matters to your business on building the the application, building the business logic, serving your customers, while AWS takes care of pretty much the infrastructure and configuration. Now, let's take a look at the isolation boundaries, and that's where we have a difference between Fargate and DCS managed instances. As Matt's already mentioned, Fargate provides a really strong isolation boundary security model. Each task is launched on a separate easy to instance hidden in AWS account. And we do not reuse these tasks, so we never place two tasks in the same instance. We never reuse the same instance for two tasks in a row. We always discard it, scrap the memories, scrap the ephemeral storage, so there is absolutely no risk of data cross contamination. With managed instances, we launch easy to instances in your accounts, and we do believe it's the best option for you to actually launch multiple tasks. So we take care about of bean packing. To optimize the infrastructure, to optimize the utilization. So with that approach, we can probably launch more tasks on less compute and really help you reduce your AWS bill. And of course you have controls there. If you're not OK with launching, say, tasks from different applications on the same instance, you can use different capacity providers, so you don't, you, you, you have certain controls there and of course you can control the size of your instances to pretty much simulate this single task experience, but we will keep reusing, we will keep these instances running. For longer after they become idle for a very short period of time, it is now configurable. You can actually set it up to one hour, so we don't. Just release that capacity. Why? Well, because each next task that is launched and already running in Kingston launches way faster. Like 3 times faster than the task on a brand new instance when we have to spin it up. So you have these controls with managed instances. Let's take a look at the best practices that we use with Fargate, and they're very similar between Fargate and managed instances. Well, except for the isolation point, task isolation is the key, probably the key feature of Fargate. We do use the attack surface. So we don't have. Access to underlying OAS hardware. You don't have access, we don't really have access to that, so we have access only to logs that are important for in telemetry, for, for troubleshooting, but we don't have access and we don't give you access to underlying hardware. We do run regular patching, and one very important thing that is often overlooked, AWS VPC networking mode. Why it's so important? Well, because now you can, each task gets an individual IP address, and with that you can use VPC flow logs to understand what's going on with your traffic. You can use security groups to have fine-grained control for each individual task. So you have really full control over your network security and that's a major thing. Something that you cannot achieve with network address translation if you're running, for example, bridge mode uh with DCSSC2. You can't really do that. So it's a really strong and very secure solution that follows pretty much all AWS security best practices. Managed instances, look at the list of benefits. Fully managed compute. Compliance and patching every 2 weeks. So if you tell me it looks like Fargate that is launched, launches in instances in your accounts, you won't be wrong. Technically, this is Fargate that is exposed to your account, so you're getting the same experience with a couple of differences. Well, bottle Rocket, as Matt mentioned, it's a UOS, uh, that is developed by AWS. It's an open source, uh, uh project that We believe is the best way to run containerized workloads. It has a very minimal set of packages required for containerized workloads. We don't have anything extra. Resulting in a reduced. Uh, attack surface. And of course, all the instances that you launch in your accounts, you don't have access to them. You can still see them. Why you still see them. Well, I can give you an example. Imagine if you don't see them, and then, but they already exist and run in your account and then you have to go and, for example, delete a subnet. What's gonna happen? You're gonna get a nasty error message saying, hey, you have instances running, so you'd better see them. So you have visibility, but you don't have control, you don't have a way to change them directly using QC2 APIs. Again. Everything goes through the ECS API and added layer of security. And of course, Matt's already talked about event Windows. It's a really neat feature. Your EC two instances have to go through. Hardware replacement from time to time, etc. so you can define these windows. Now you can use the same concept. To tell us when you're OK with patching, so it's not going to happen on a random day, it's going to happen in the interval, time interval that you tell us to. Well Compliance. Really short. Targate and managed instances really help you achieve better compliance faster because now you can point to AWS and say, hey, AWS takes care of pretty much everything but my application. And now to achieve compliance, you need to provide a much, much shorter list of evidences that you actually have meet these compliance requirements. Everything else is on AWS so we have all this list of Things that we provide as an evidence that we actually meet the compliance requirements. So we help your workloads achieve compliance faster. And of course, you can find the list of services and scope. If you go to that page, it's a really neat website. Pretty much everything you can find there. Amazon ECS will be there. So, compliance is a great topic. But now let's chat about something that is very dear to my heart, how to save money. Writing for clothes on Amazon ECS and of course we're gonna talk about. Cost optimization strategies. 4 things. Choosing the right hardware. Choosing the purchasing options that work best for your work clothes. Figuring out how to scale and optimize your workloads, and of course, observing it and making sure that actually all that works to your benefit. Starting with hardware. We have today, we offer a DWS3 different CPU options, Intel CPUs, AMD and Drivitone. And of course, With Fargate, you have control whether you want to use X8664 or ARM CPU, so gravitons or ARM, so that's your control, but you can't really choose whether your tasks will land on Intel or AMD. And what if you're Um, application takes a dependency on ADX 512, for example, uh, instruction sets that work really well on Intel. Or what if your application benefits from a super strong, uh, single core performance of AMD CPUs. Well, that's where managed instances come to play. You can choose the CPU type, you can choose exact instance types that are the best for your compute. And guess what? If you run something like tasks, tasks that are jobs, uh, time-bound jobs. If you finish your work running your workloads faster, you pay less, so it's an excellent way to choose the right CPU for your workload to actually pay less or just get the latest graviton generation with just phenomenal price to performance ratio. Switching to purchasing options on demand. That's the default, excellent for spiky work clothes. The most expensive Saving plans, when it comes to predictable workloads, and if you can commit to spending a certain amount of money on AWS over 1 year or 3 years, you can get significant discounts with that. So, if you have predictable workloads, please, please, please use saving plans. It's a very powerful tool that you have in your arsenal, something that you can use. And on top of that, spot. Spot is a great way if you uh to save, if you have fault tolerant applications, 80% savings on compute easily. Well, with the caveat that we can reclaim this capacity at any time, so really for fault tolerant applications, this is actually something that is not available with managed instances yet. Let's take a look at Fargate and how Fargate helps you optimize costs. So first of all, Not really obvious, but You always get 100% utilization. Matz talked about it. But we give you exactly what you asked for. You don't need to reason about being packing your tasks on that instance and figuring out what happens when the task goes away, what to do with that empty slot on the instance, what else to place there. Another thing that is really important, you can speed up the task launches on Far gate with seeable OCI lazy loading. The idea is that we get the container and we start it before it's fully loaded. It's an excellent feature. Please, please, please use it. It works better with large images. And of course we have compute optimizer now. It looks at your workloads and says, by the way, you can stop these instances, you're not using them, or it's better to write size and just, it suggests a better size if you, for example, run them at a very low level of CPU usage it says go down the size, don't, you don't need that much capacity, saving you money. On managed instances, the situation is slightly different. Remember multiple tasks. There's been packing problem. What do we do with empty slots? Well, now we actively fill them. We look at your entire cluster, we find them underutilized instances, and we compact your tasks to better to achieve better utilization, and we shut down stuff that is idle, which is really powerful active optimization that goes on all the time. You can tell us, nope, I don't want that. These particular tasks need to run for a long time. I cannot interrupt that. We have this way, and you can actually disable this feature completely, which is handy when you use something like reserved instances and on-demand capacity reservations. You already paid for that hardware. You already paid for that stuff. So why, why do you want to stop these instances? Maybe you want to keep them running all the time. So you can launch new tasks faster. So there are different ways to think about it. And of course, since we have multiple tasks in a single instance, now we can use container image cache. So, instead of downloading the container, Uh, image from ECR every time, your tasks can just grab it from that instance, immediately. So, to wrap up, you need to think about Fargate and managed instances, and again, we believe that Fargate is an excellent choice for the majority of workloads, but managed instances come incredibly handy when you need things like uh stuff that just does not work on Fargate. EBPF is one example of that. Another one is related to cost when you have short-lived tasks on Fargate. I'm sorry, it is not efficient. You're spending way, way too much money because you're paying for the time, the image is being downloaded from VCR. Managed instances, as I said. Container image cash and the ability to reuse instances for new tasks, it helps you save money. So everything that runs under 2 minutes, I'm confident it's gonna be way more cost optimal on managed instances. Please consider that as a, as an option. And of course, if you have your reserved instances or you have um Can instant saving plans or DCRs managed instances is a simple way to use that capacity and again save money with, with, with DCS. Well, with that, we're coming to the most exciting part of this presentation. Ruben, please come on stage and uh chat about puberty's journey. Thank you very much. Matt. My my work. I don't know no, no, we start. Hello. Let's wait, OK, we have it. Hello, I hope you're getting the best of the first day of Reinvent. My name is Ruben, and before, like diving into the journey and the technical talk, I would like to introduce myself a little bit. The first thing you need to know about me is that I'm Italian, and that's important for one reason. I have a challenge with myself today to try to deliver at least half of the volume of information I can do with my hands by using words. So you're gonna let me know if that works at the end of the presentation. And the second element is, uh, I would like to talk about, uh, my previous life before selling my sole finance. Um, I was actually a rocket scientist, like I have a, a PhD in applied mathematics and I was simulating rocket engines. I must admit I don't regret the choice to switch, but sometimes I miss the random explosion in the lab sometimes. Um, What is QRT? After introduction by myself, I would like to talk about QRT. Cube Research and Technologies is a global systematic asset manager. Let's dissect that a little bit in the right order, which means starting from the end. Um, asset manager means we get money from our investor. So, and we put it on the market to make more money with it. Um, by systematic, what we mean is the vast majority of our investment strategies are data-driven and automatic. So we have a lot of algorithms which trade on the market in a daily basis and in an automated way. By global, means that QRT is operating across the globe and across all the types of assets, equities, futures, crypto, you name it. And what's the mission, QERT? Yeah, I already introduced, introduced it a little bit, but it's to deliver results for our investors in our market condition. Easy to understand when market go is, uh, is, is bullish and it goes well, we wanna do better, and when things are less, uh, well, then we try to, we try to mitigate that. All right. Start we were, we're good with the, with the introductions. Now we, we wanna dive into the, the nerdy talk. Um, when I joined QRT uh almost 4 years ago, um, there was already an existing solution in place. And it worked more or less like that. Uh, we have researchers. Researchers are basically the bright people which come up with ideas for investment strategies, right? Most of them, especially in recent times, they're used to use, I mean, they're used to do their research with Python because of, you know, the rich data science, ecosystem or scientific computing. So they were used to do their research with Python. And then they had somehow basically paired up with a quantitative quantitative developer and basically rewrite the research in another language, uh, compile that artifact and then deploy it on an existing on-prem infrastructure, sharing um resources with other type, with other, with with other ideas, let's say. I mean, that worked pretty, pretty, pretty well. I mean, QRT has been a successful uh enterprise, but when we when I joined, we were tasked with another, with another task basically was, which was, let's try to build something which is natively scalable. And Cloud Native, Native. So, what we said is, OK. Maybe we can skip the detour. Uh, let's keep what works. Researchers really like to work in Python. Let's keep them working in Python. And then, let's, what we decide to do is containerize our Python workloads. OK. Now we have our container, and we had to decide which kind of orchestration system we had to, we wanted, we wanted, we wanted to use. I guess the fact that I'm on this stage is probably a spoiler, uh, but we ended up using Fargate and ECS. Why? I will try to explain to you why we chose Fargate as a capacity provider and ECS as an orchestra as an orchestrator. First of all, at that time, We were a very small team. We were 4 when I joined, and we wanted to focus on the business. The team was not particularly expert in cloud computing. But we were, I mean, we had expertise in the finance financial part and the standard engineering part, so we wanted to focus on our business. Farget gave us that serverless vision which made us focus on our uh core business. Um, We tend to build things um within the team, keeping the intellectual impedence at the lowest we, we can, and again, Fargate really helps you with that. At the time when we started our, our endeavor, uh uh we didn't know if it would have been successful or not. So we were looking for something which could help us start small but scale if we need it. And if we needed, we needed the capacity to keep up with our growth. Last but not least, I mean, financial financial industry is a very regulated industry, so we needed a by default, uh, compliant security posture, and the fact that in Fargate you can, everything is isolated, is encrypted, you cannot access it in your machine. I mean that really played well for us. So these were the design principles of of our choice. Um, now let me try to explain how things work a little bit more in details. Some schematic here, just bear with me a little bit. Um, in our system we have two types of services. One type of service is called, we call it quant computational unit or compute units, QCUs that you see here on the bottom of the slide. And then we have another type of service, um, which is kind of a server that here is marked logical server. We also call it had node. Um, the role of the head node, basically the QCUs cannot talk, and then we have, as you, as you see in the, in the top of the slide, then we have our, you know, underlying firmwise database with all the data. Uh, basically the QCUs are designed to not talk directly to the database for performance reasons. So, so the, the role of the head node is is twofold. One is to aggregate requests. So imagine one of your units needs, I don't know, Amazon, uh, prices for the stock for the last 10 years, and another one needs it for the last 5 years. Basically the head node, um, queries the database for just 10 years and then it does the slicing locally. And the second one, as you can imagine, is caching. We cash some of the requests in-memory. I talked about the fact that this is a logical server. Why? Because in reality, our had no deployment is composed by several ECS services, actually, in the bigger deployments, several hundreds or even several thousands. In this slide, each square is an an ECS service. And one particularity of this is that each service has one task, only one task. This is a bit exotic with respect to the best practices AWS recommends. Why you have that? Again, simplicity. If you have two units basically publishing almost the same thing, then you need to decide which one is the right one. We decided to accommodate a little bit of lack of less residency for simplicity, so they, we are just one unit publishing. All right, and another important element here is everything talks, um, data-driven GRPC protocol. So the QCU talks GRPC to the head node, which talks to GRPC to the shards, and the shards talk GRPC with database. The shards here means that obviously you cannot host the entire data you need in a single service, so basically the shards split this data based on several criteria to have to have a distributed, a distributed deployment. Something done as I told like the QCU is done when they finish elaborating what they need to elaborate, they will publish the results of their uh computation back into our database. Another important element here is that. Service discovery. In our experience when we tried other orchestrating systems, uh, we realized that scaling DNS correctly is kind of challenging. So here we decided to use cloud map, which is again serverless, and you, we basically when we deploy the head nodes, we register a DNS entry in cloud map and that's it, it works, trademark. All right. How did it go? I guess again, the fact that I'm on this stage is probably a spoiler, but it went very well. I would like to promise you, you know, to give you the, the reality of things like the scale we are at. Unfortunately, the compliance colleagues, they hide in shadows, so uh I cannot, I cannot tell you the exact figures, but we had 100x growth, which means everything and nothing, um. What I can tell you is that we have right now deployed several 10s of thousands of services currently running, where the several 10s of thousands is on the higher range of the 10s of thousands. That's what I could negotiate, I could tell you. Uh, but, on, based on the, you know, the schematic I just presented, you, I mean, you start facing some challenges when you, you begin to scale, for example, quotas on the number of services per cluster, um, number of CPUs per account, uh, etc. etc. So actually we had to evolve our deployment. We actually tag tag tagged tag team with AWS with a process of engagement that we ended up with an evolution of the architecture I just presented. Which is the following one We already talked about Hanos. We started to put headnotes in dedicated accounts. So, we have some accounts for headnotes, some accounts for the QCUs. All these accounts then are within the same VPC which is stretched across all the accounts. A few caveats here When you do this, you need to first define the VPC in a single account, and then when you do that, then, then you can stretch it, which means a couple of things. The most important one is you can define, you can register your DNS service discovery only on the accounts where you have defined, you have created the VPC initially. And which means in our case, we created the BBC in the head nodes accounts because the head nodes are the, the thing we want to discover, the QCs want, want to discover, and then you stretched around. If you wanna use. Um, API based, so not DNS query based, but API based service discovery that works across the VPC without any problem. Second important element, and this is really the secret sauce to become a success, a successful asset manager, like systematic asset manager is the naming system for the clusters. As you can see here, we're using flowers. Uh, it's a really, really the secret sauce for our, for our deployment. I recommend you to do that. It's, it's great. Um, it's fun. All right. Let's make a little pause. A lot of, a lot of diagrams of schematic. Um, we grow, I mean, we grew from few services in a single account, single cluster, single account multi-cluster, which I didn't show, and now we are multi-account, multi-cluster. You can realize now that this is kinda, it becomes to be challenging to control, so you really require, you're really required to have a. The right observability posture, you really want your system to ping you instead of you polling the system that's much more scalable. Um, when you approach observability, at least in our experience, uh, with ECS, you have several, several, um, options. The one which is most documented is, you know, you, you spin up a sidecar on your, on your surfaces and, um, with fire lens and then you scrape, um, your telemetry, for example. Still within the tag team with AWS in the proser engagement, we decided to go with a different way, which is a little bit more exotic, but it works for us, and I will present that to you again. Single VPC we already talked, single VPC multiple accounts, one for Ednode, I mean, some for Ed nodes, some for QCUs. Those accounts, they basically publish their logs in a serverless way using CloudWatch. And they also publish um the metrics if you turn on container insights in a severalless way into cloudwatch matrix, right? Each account does that. What we did is connect what is called log subscriptions and matric streams. Those are basically some again serverless mechanisms which react when a new log appears in cloud watching whichever account or when a new metric appears in whichever account. They react. You can connect them to fire hose, kinesis, and to a lambda, and what we do, we use those which are in a single account and we use those to send our telemetry to our third party observability vendor. That's for infrastructure related um. Telemetry. For what concerns Your application telemetry, um, what we did was deploying an open telemetry collector within the VPC we just discussed, spanning across accounts in a dedicated cluster, and this open telemetry elector has been, uh, configured to auto scale. What's a bit different with, you know, standard practice, at least based on our experience, is that normally what you do, you configure collectors to scrape your services. In this case, our services are pushing to the collector. So why that? No sidecars, you don't have the problem of monitoring the monitor. Um, you don't have the problem to decide how much compute you need to allocate for the sidecar, and you have one single point of entry to control everything, which looked easier to us at the scale we were. And again, the telemetry collector, the telemetry collector then pushes through a private link again back to our observability vendor which then holds everything. You have logs, you have metrics, and you have traces if you're using it. All right, we're next? We want more. We want more, and we are still growing, but right now we're at a certain scale where we cannot afford. Anymore to just grow without thinking, so we need to uh to adopt a sustainable growth approach. There is a lot of appetite in the market and and also by by extrapolation in the firm to leverage hardware accelerators, GPUs, but not only GPUs. As you might know, when you use FAgate, you are limited to 120 gigs as the top memory level you can choose. Some applications on our side require more than that, and so we are looking at alternatives, and the alternative, uh, spoiler alert is we are already testing and looking at me distances which looks really the sweet, like like the sweet spot where we can adopt all these elements and make our business grow and our deployment grow. Orthogonally to this. Um, we wanna keep, uh, the, the resiliency posture we already have. We increased it, going multi-account, and now we are, um, starting to think to go multi-region because right now we were all the accounts were deployed, I mean were initialized in a single region. So, we started to think how to do that and take the unit I already discussed, you know, the VPC unit with all the accounts, take it and move it into another region, have a multi-region deployment. Which will give us advantages on flexibility for for compute, but also, you know, residency, a high residency posture. And that was it for me. Thank you for your attention and back to Alex. Thank you very much.