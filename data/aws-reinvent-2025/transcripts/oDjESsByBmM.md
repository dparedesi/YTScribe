---
video_id: oDjESsByBmM
video_url: https://www.youtube.com/watch?v=oDjESsByBmM
is_generated: False
is_translatable: True
---

Good afternoon, everyone. Thank you for making the trek to Mandalay Bay. And welcome to our session, bridging the gap from prototype to production. Over the last year, we have started seeing AI agents transition into autonomous systems that can reason, plan, and adapt in pursuit of user-defined goals, completing tasks on behalf of humans or other systems. Things like compiling research, remediating infrastructure issues, or even generating full stack applications from a single prompt. The advent of reasoning models, agent frameworks, and open source protocols like modern context protocol have made it easier than ever to prototype agents. And as a result, we are seeing an explosion of agent prototypes across both startups and enterprises. But when we look at which ones actually made it make it into production, the number is in low single digits. Quick show of hands, how many of you have built or experimented with an agent in the last 6 months? That's a lot. Now keep your hands up if that agent made it into production and is running reliably today. See that drop off? That's the story we're here to talk about. We're still in the early innings of the agent era. There's enormous excitement, enormous potential, but the value only comes when those agents operate safely at scale. I'm Vivek Singh. I lead product management for Amazon Bedrock Agent Corp. I'm joined today by Mark Roy, uh, tech lead for Asian Tech AI at AWS, and Sarbashish Das, who's the Gen AI tech lead at Ericsson. Today, we'll cover the key challenges that developers face in moving their agent prototypes to production, and how Bedrock Asian Corp provides developers the building blocks to cross the prototype to production chasm. We also have a few demos for you to show Agent Gore in action. And then Sarbashish will cover how Ericsson is leveraging Agent Gore to power Agentic AI innovations. So let's get into it. AI agents are autonomous systems, so even a basic one requires multiple moving parts, such as orchestration, tool execution, state management, and error handling. That's a lot of wiring to get right. And here is where frameworks like Landgraf, strands agents, and crew AI help. They provide ready-made abstractions for behaviors like multi-step planning, a coordination between different sub-agents, tool invocation. That means developers no longer have to start everything from scratch. They can focus on defining the agent's behavior, while the frameworks take care of the lower level plumbing. So using these frameworks, you can spin up a coding assistant, a customer support bot, or a sales agent in a few days. But moving that same prototype to production with thousands of users, reliability requirements, late compliance requirements, that's a completely different ballgame. Let's make that concrete. Imagine you build an agent, let's call it sales assist, that helps your sales reps close deals faster. It answers product questions, it pulls customer histories from Salesforce, it drafts personalized emails, and it can even suggest next steps based on prior deals. You can get that prototype up and running for one sales rep locally in a few days. Now imagine deploying that across 10,000 reps in 5 different countries, each with dozens of daily conversations. Each agent accessing Salesforce, Confluence, DocuSign, your internal pricing APIs, all while acting under a rep's individual permissions. That's where the real work begins. Let's unpack what makes that transition so challenging. When you try to scale something like sales assist, 4 categories of challenges emerge fast. Performance, scalability, security, and governance. Before you even think about scale, the agent has to be responsive, reliable and accurate. It has to be able to reason well, reliably call multiple tools to take action, remember pertinent facts from prior interactions. For example, sales assist might ace a demo with 1 or 2 tools, but in production, it's tens or hundreds of tools, each with its own art, schema, rate limits, and unpredictable failures. Turning all of those into uniform, reliable MCP compatible tools becomes a massive engineering challenge. Secondly, LLMs are stateless, but agents are not. So sales assess must remember the deal history. It must remember pertinent facts from prior customer interactions, such as the quote it gave to the customer last week. Without a proper memory layer, the agent will lose context, misremember things, and can even hallucinate the past state. And without continuous evaluations for things like correctness and helpfulness, the aging performance may even drift in production, leading to widespread customer impact. So performance is not about picking a bigger and better model. It's the system around the model, the continuous evaluation, the tool integration, the persistent memory that keep the agents reliable and consistent. After performance, the real challenge is scale. Sales assistant production means thousands of concurrent agents, each making multiple tool calls, running long workflows, and handling sensitive customer data at the same time. So you need a secure, resilient, and elastic runtime that can scale up during peak workloads, scale down when idle, um keep each agent session isolated, maintain state during long-running multi-step reasoning workflows, and quickly recover from failures like API timeouts or service disruptions when it's calling multiple different tools and services. Then comes security. Agents act on behalf of humans, so every action must respect fine-grained identity and access controls. If Sarah in Enterprises is using sales assist, it should see a list of her Fortune 500 accounts. But when Tom and Startup does it, he shouldn't. The sales assistant shouldn't see the enterprise accounts. That's per user identity, not shared credentials. And doing that securely across thousands of agent sessions becomes, requires deliberate design. Finally, you need visibility and guard rails to know what each agent is doing and to control unsafe actions before they happen. If a deal goes sideways, can you audit what data sales assist accessed, what email it sent, what pricing it suggested? Additionally, you need a way to enforce your business rules in real time. For example, never share unannounced features, never discount over 20%. So governance isn't something that you bolt on later. It has to be built in in your application from day one. And doing all of these things, essentially means building a distributed enterprise scale system that can handle thousands of concurrent sessions, millions of tool calls, adhere to strict latency and reliability requirements, all while leaving very little room for error. And that's the difference between a prototype and a production system. And right now, developers have to do significant undifferentiated heavy lifting and building all of these pieces for a combination of a framework, model, and a set of tools. And as your business case evolves, and you want to leverage newer models, newer frameworks, and newer tools, you start all over again. And that is what's slowing down organizations right now in realizing their agent tech vision. To address these challenges, we launched Amazon Bedrock Agent Core. It's a generally available service. We launched it earlier this year. It's an agented platform that provides a complete set of services, purpose-built to build, deploy, and operate highly performing agents securely and at scale. Let's look into some of the key characteristics of Agent Core. First, Agent Core helps you optimize time to value, because you don't have to manage any infrastructure. All Agent Core services are completely serverless and you pay for what you use. For example, if you're using Agent Core runtime, you only pay for the active compute and memory that your agent consumes. In fact, you don't even have to pay for the IU wait time, which is the time your agent spends waiting to get responses back from LLNs and different tool calls. And in most of the agent session, IU8 could make it up to 60 to 70% of the overall agent session time. Secondly, Agent Core was built with flexibility in mind. Each Asian Core service is completely module modular, so you can use them together or you can use them independently. And each agent core service works with any framework or any model out there. So if you built your agent using Landgraph or cloud agent SDK or if your agent is using OpenAI models or Gemini models, you can still use any of the agent core primitives with your uh agent. And finally, Agent Corp provides the controls, the access management, and the observability, which are crucial for enterprise deployments. So overall with Agent Core, you can accelerate prototypes into production with the scale, reliability, and security, which are critical to real-world deployments, eliminating months of infrastructure work. Now let's look into some of the key services that Agent Core offers. First, To build highly performing agents, Agent Core offers a memory that helps developer developers build context their agents. It automatically extracts and stores short-term memory across multi-tter user interactions, as well as long-term memory patterns such as semantic memory and user preferences across longer horizon and multiple sessions. Gateway enables you to compose your tools and agents across your ecosystem into a single MCP server interface that you can then expose to your agent remotely. With Gateway, you can combine different tool sources from your REST APIs to your lambda functions to your existing MCP servers into one unified interface without managing multiple tool connections or implementing any integrations. Agent Core also provides fully managed tools that are essential to most agenting workflows. Code Interpreter enables agents to securely generate and execute code in isolated environments, and browser enables agents to interact with web applications at scale. Then to deploy and scale agents, Agent Core provides a secure and serviceless runtime that is purpose built to host agents as well as tools, and agents and tools could be built using any framework, any protocol, whether it's MCP or A2A or uh or any model. Agent core identity simplifies our agent identity and access management. Allowing agents to securely access AWS services and third party services on behalf of users with pre-authorized consent. And we also launched policy today. This was announced in Matt's keynote, which provides you real-time deterministic controls over how agents interact with your enterprise tools and data. And you can define these policies in natural language. For example, block all reimbursement requests which are over $1000. And these policies are evaluated and enforced in real time in milliseconds, so that every agent action is operating within the boundaries of policies that you have specified. And finally, to operate trustworthy agent, we have observability that provides rich visualization into each step of the agent workflow, as well as operational metrics like token count latency via unified dashboards. It also emits the data like logs, metrics and traces in open telemetry compatible format, so you can plug it into the monitoring tool of your choice, such as Cloudwatch, Data Dog, Lang Fuser Lang Smith. And we also launched evaluations today, which is a new agent core service that helps developers continuously inspect the quality of agent behavior, uh, so that you can catch issues before they cause widespread customer impact. Overall, with Agent Gore, you get everything you need to take your agents from prototype to production, built on the AWS foundation that customers already trust. Now, I'm going to hand it over to Mark to go a layer deeper into these services and show you a few demos of how they work together. Thank you, Vivek. Great introduction to Agent Core and uh super excited to be here for my 8th reinvent. Uh my name is Mark Roy. Uh, I'm with, do we go to the wrong slide here? Sorry, I might have hit the wrong button. Uh, my name is Mark Roy. I'm with AWS, uh, and I'm a global tech lead for Agenech AI. And although you may think I'm only 29 years old, I've actually been building for about 30 years now, so. Um, Vivek talked a lot about the chasm of production readiness. I really love that term. I'm wondering who here after that description feels like they're in that chasm of production readiness, uh, already. Anyone out there? OK, not too many hands, but let me tell you, it is real. Uh, for the last 2 years I've been working with hundreds of customers trying to build and deploy agents and get real business value, and I see 3 things showing up every time one. Great looking prototypes, amazing capability, looks great. Compelling business value that everyone's excited about all the potential and then months and usually quarters of heavy lifting and pain and suffering, uh, in between. So for the next 15 minutes I'll drill down into what are those real challenges, and I'll go one level deeper in agent core and explain how these, uh, services help you solve those compelling challenges. So with that we've got a lot to cover, so buckle up and let's, let's jump in. Uh, let me start with the very most common and compelling challenge, and that is, where am I gonna run my agent? I can't run my agent on my laptop and just share that out to, uh, users. So you're an agent builder and you're thinking this through, you've got pressure to deliver some real value here. You know that your agent is gonna be used by 10,000 users. You need to ship it now. You've got a security officer breathing down your neck saying you've gotta make sure this thing isn't going to do the wrong things, uh, and for most of you, you probably have got a platform team you're working with as well, and they're thinking beyond your one agent they're dealing with multiple different teams and guess what? you're using Landgraf, there's some other teams using Crew AI, other teams using OpenAI, and it's a bit of a mess. And guess what? If you look at across these business units, they've got 1000 use cases lined up. So you've got a big dilemma here. How am I gonna do this at scale and securely and not spend the next year getting ready? So that's where Agent Core runtime steps in and gives you, uh, the first major component of your platform. Runtime lets you use any framework, any model, host your agents securely and at scale. You can scale from 0 to 000s of concurrent sessions. It, it comes, uh, built in with the ability of the host MCP to use uh A2A for agent interoperability, uh, Otel for observability, OO for security. All of these things help get you that time to value where you can focus on building agents and not building infrastructure. Another key point here is that although you're probably working on let's say an intelligent chat experience, that's kind of usually the first step, there's a lot more out there. There's voice agents, there's long running deep research, uh, there's large payload to worry about this complicates what am I gonna do about hosting. It's not as simple as just popping it into a lambda. That sales assistant that Vivek talked about, let's say it's a multi-agent system. You've got one agent running with cloud SDK using cloud models, another one using Lang graph and GPT models, maybe a third one using Crew AI and Gemini with strands agent on top of it doing orchestration using Bedrock models. All of this can be done out of the box with run time. And literally it's just 4 lines of code to take an existing agent and make it ready for agent core runtime. And then you can deploy it to the cloud, and it's ready to scale and it's ready to be secure. A2A uh built-in, no problem. You've got full interoperability there. And if you wanna host MCP servers, uh, you can pop your tools into Agent core runtime as well. So I talked about security. I'm not a security expert. Uh, I'm kind of glad about that on some days. Uh, when you're building agents. You're using agents because you want them to be a little bit more autonomous. You're not wanting to build a hard-coded traditional application. Now what about when you're dealing with sensitive information? That's my Social Security number there don't, don't grab it, please. Uh, you might be giving out credit scores, you're doing very sensitive transactions, and while your agent's being used by one user, there might be another user doing something similar with also sensitive data. So I've got a question for you. Who here, raise your hands high, is ready to stand with their security officer and say they're 100% sure that those conversations are secure. We've got one guy in the front row. I wanna talk to you. Uh, this is a huge challenge, this for me as a builder, this is where I wake up with a cold sweat. How am I gonna deal with this? Agent Core Runtime does this out of the box. Secure session isolation. For no uh no coding involved, it's just there. Safely execute those conversations. There's no risk of leaving temporary files lying around, no risk of getting uh permission escalations, all taken care of for you. You get multiple concurrent sessions, no problem. When the sessions are done, poof, they're gone. This is using our micro VM architecture, so it's not just a container level isolation. It's literally a micro VM that ensures that this is going to work, and it does so at scale and with low latency. This is something that I wouldn't recommend trying to build on your own. OK, problem number 1, let's put a check box on that. I think hosting Agent Core has a pretty good story there. What's the second biggest problem? In my mind, agents, you know, what separates maybe a toy agent from closer to a real world agent, real business value, is it, is it able to learn? Is it able to remember? Is it able to improve? How do you make that happen? So, first of all, your agent better remember what's been discussed over the last few minutes of a conversation, and then ideally it's remembering what happened in the last few months as well. So that's called memory for an agent, uh, and it better work reliably, so even if the agent needs to be restarted, uh, you better still have that there and it better be secure as well. And yes, you could build this on your own, do it yourself memory. A lot of people out here have probably tried it, uh, wouldn't necessarily recommend it, but with Agent Core, you get a memory component out of the box. So what does that mean? Pick your favorite framework again and just plug in agent core memory. When you've got that, your events, your conversations feed right into short-term memory. It's got low latency, it's got security, it's there for you to have good conversations. More exciting than that. Is automatic memory extraction into long-term memory. What does that mean? All of these conversations get fed into a process that runs behind the scenes. And extracts knowledge it extracts facts it summarizes conversations. It even identifies episodes as of yesterday we've got episodic memory now built in, uh, and it will actually look across episodes and reflect and get insights. All of these get put into what's called long term memory. And this long-term memory can be easily plugged into your agent. So now instead of your agent having this long list of conversations that it's trying to take advantage of, you can selectively pull out long-term memory, manage your context, and make your agent learning from the past. And that's hyper personalization and that's again making really a real world uh experience for your customers and doing so fully managed, serverless, secure and plugging into any framework. OK, so 2 checkboxes. We've got memory, we've got run time. What's the next big challenge? To me, an agent is pretty useless unless it can take action. You've heard uh Matt Garman and Andy Jasse say, data is your differentiator. And really that's true. You could just use an off the shelf shelf LLM and get generic answers, that's not gonna take you too far. Your agent needs your data, your APIs, your services, and it needs uh your APIs to take action as well. That's the power. So why is that so difficult? Well, we've got cool things like model context protocol that showed up late last year and is now used everywhere. But how do you put those MCP servers together and how do you integrate with all of your existing capabilities? That's a lot of heavy lifting, that's a lot of time, a lot of wrapping, uh, and uh how do you make that secure as well all of these things add up to a lot of work. So how does Agent Core help? We've got something called Gateway that automatically lets you map those existing resources and surface them as agent ready tools. So it exposes those APIs as MCP. And those MCP tools plug in strands and crew AI, uh, land graph, whatever you're using securely. Let's look at it a little bit more closely here. So gateway, you can create as many of these gateways as you'd like. When you create a gateway, you can then add targets to that gateway. So add a bunch of API endpoints in there, maybe throw in a few lambdas, get a couple of existing MCP servers that have been built or maybe third party ones, and now you've got a gateway and you can hand that gateway to an agent builder and say these are your tools. Or you could have multiple of them, and they can discover different gateways, uh, but basically now you've got agent-ready tools, securely at scale to plug into your agents. Massive boost in time to value here. Last cool thing on Gateway. So you started with maybe a lack of tools, you can easily get into a situation where you've got too many tools. Hard to believe, but pretty quickly you can see hundreds or maybe thousands of tools. And guess what happens if you give 1000 tools' worth of MCP metadata to your agent. You don't wanna know. It's not very pretty. Uh, we have a built-in search, so instead of handing all 300 tools in this case. We give you a built-in semantic search, so as you add tools, they're automatically indexed. You can call this MCP search capability. Now you can do dynamic tool selection. Instead of doing 300 tools, give a handful of tools that are built for that context. Now you've got faster, cheaper, and more accurate. It's pretty rare to get a triple play here, but faster, cheaper and more accurate, all from doing semantic search. OK, There's more. So today, uh, in the keynote from Matt, uh, as, uh, Vivek mentioned, we introduced policy for Aging core Gateway. What's the big deal here? So, you can give permission for who can access the gateway, who can access tools. Uh, and you can, uh, have requested two calls and then which ones are allowed. But what you really want is something a little bit more granular because what's happening here is your agent has more autonomy and you're balancing the need for that innovation with the need for control. You don't wanna risk getting on the front page of the Wall Street Journal with an agent doing a $100 million mistake. So you wanna not trust your agents to do the right thing. You don't wanna trust your developers uh to code up the right controls. So with policy, you're able to intercept every single tool call and apply policies that your administrators uh can put in place and attach to these gateways. It applies those policies on the fly with low latency. And it does so in a way where you can define these policies in natural language and they're verifiably correct verifiably enforced, uh, and it's deterministic so although your agent is non-deterministic, which is a good thing. Enforcing policies needs to be deterministic and that's what these policies end up doing for you. Of course we've got observability because you wanna know which policies were enforced, why they were enforced, why do they get denied, why do they get allowed and be able to audit that, and then I clicked one too many times. Uh, so this is keeping your agents in their lane, keeping them inbounds while still allowing, uh, innovation. OK. Great, we've got a couple of built-in tools as well. So beyond just using gateways to get access to your MCP tools, what about the power of LLMs to generate great code? Why not be able to plug that into any agent and make it an instant data analyst. So here I'm saying in that sales assist app, how are my accounts doing? So we can go grab some data, but are LLMs good at analyzing large amounts of data? Not, not necessarily. They're a lot better at generating code, and now you've got a secure sandbox that you just plug in as a tool, and your agent says, OK, I know where to run this, uh, and you've got great results, you can generate visualizations, uh, whatever you need actually, that the agent can generate, uh, on its own. Secondly, although I mentioned earlier a lot of nice APIs, data sources, who here has a few legacy applications? Maybe they're built 5 years ago, maybe 25 years ago. Uh, they're still running mission critical processes. So we're not gonna reinvent those in order to build an agent. Why not use a browser and automate access to those? L1s are great now at interpreting screenshots, and then you can have your agent click at a certain field, navigate to a different screen, scroll down, scroll up, copy, copy the data into another field. All of this is possible, we give you a headless browser, you just plug it in as a tool, and you're off and running. OK. Now let's quickly cover 3 cross-cutting concerns. Uh, first one is security. Uh, it's all well and good to come up with a great agent, but there's table stakes here. It's got to be secure, and this is a challenging problem user talking to an app, talking to an agent, talking to another agent, talking to tools that are internal and third party tools. You better get that right. You can't afford to let the wrong user get access to the wrong data or take the wrong actions. So this is pretty difficult. An agent core built in to run time and gateway takes care of inbound and outbound off. On the inbound side, you use whatever identity provider you'd like. Maybe it's Microsoft Entra, maybe it's Ping or Octa or AuthZero. Plug that in, and we take care of who is the user and are they allowed to use this agent. On the outbound side, we can say, is this user allowed to, uh, using this agent, get access to these tools. And we plug in again on this side with your credential providers. You need Salesforce data. You want ServiceNow or Workday, Jira, whatever it is, there's either an API key, uh, or an OA, uh, credentials, uh, maybe IAM in some cases. We automatically let you configure those as outbound providers. And then we make sure that there's secure access, uh, end to end on behalf of a user or on behalf of an agent, uh, autonomously. This is super important. And please don't try to do this one at home, uh, don't try to do this one yourself. Uh, it's scary, complicated under the hood. I've tried to make it look pretty easy right here. Another table stakes, uh, piece of the puzzle is observability. Never trust your agents. Never put an agent into production without being able to know exactly what it's doing. You've got to be able to go back and find out exactly what it did. Here's a request. Here's the plant put together, here's the steps that it took, maybe it redirected and tried something else. This is the inputs that it passed to the tool, this is what it got back. Maybe it got back in error and it retried. You need every bit of that. Auditors are gonna come in, legal is gonna come in. Why did this agent do this? Uh, you've gotta be able to have full observability. We've got that across all of these services. We also, uh, give you dashboards so you can easily see not only typical metrics like latency, error rates, and so forth. You can see the end to end visualization, a hierarchical, uh, timeline, uh, the full trajectory, the inputs, the outputs, it's all there. And in case you were wondering, yes, we have open telemetry. So if you wanna use Dynatrace, or Data Dog, or Lang fuse, anything that you'd like, go for it. OK, last, cross-cutting concern. Arguably the most important one so far, drumroll please. How do you know how good your agents are? You've got to be able to measure, are they doing the right thing. Right? So, this includes, are they being safe, are they being responsible? Are they being polite, do they have the right tone? Are they giving back the right answers? Are they making the right tool calls? Are they passing the right parameters? If you don't know the answers to these, you're not doing a good enough job for mission critical agents in production. So in today's keynote. We introduced aging core evaluations. So what does that do? So, uh, in under a minute, you can go to the console, pick your agent, pick a set of metrics that you wanna turn on, uh, say where your traces are coming from. Turn on a sampling rate. And then say go. Any traffic that shows up in that agent is gonna get automatically evaluated. So agent core evaluations will evaluate each of those metrics on those traces and give you scores. And then not only will it give you a number or a label, but it will say why. So why did it decide that that was the score, you'll see the reasoning, uh, and that's all logged, you'll get dashboards, and you can even do on-demand evaluations. So this is huge, critical, uh, and it was launched today, uh, in public preview in Agent Core. So this is the entire uh agent core platform. It'll probably get bigger and better over time, but this is where it stands today, uh, and as Vivek mentioned, you can pick and choose here. These are composable services. Decide which ones you're having challenges with, kick the tires on that, integrate what you need, use any framework, any model, OO, OTel, A2AMCP. All of the right acronyms are, are there. So, Let's see a quick demo here and then I'll bring you uh Saarcis from uh Erickson to, to dive in even further. So here's the scenario. I've, uh, I've said any framework, any model, and I've said A2A, but if you're not sure if you really believe that that's true, let's see it in action. So, uh, in this example, I've got an orchestrator, uh, using Google 80K and Gemini. Uh, you can see a couple of sub-agents, one using strands and bedrock models, another one using OpenAI and GPT-4, and they're communicating using A2A. Uh, and, uh. Are you gonna boot me off the stage, Precious? Am I running too far? OK, I'll be done in about 2 minutes. Uh, so we're using A2A. Uh, we're using Asian core gateway to access tools. We've got memory and observability, and let's keep going here. Here's the UI. We've got a JavaScript UI on the left we're showing you. Access to A2A agent cards. I'm handing it a task. I'm asking it a question. It's actually delegating that to one of the sub-agents. It's using a different framework, different model, uses A2A to get there. It comes back with an answer. Uh, I'll scroll ahead here, too far, sorry. I don't like this laptop. Let's see. Oh man, isn't this great? My apologies. OK, it's not gonna cooperate with me. Uh, so we're using ATA to have a conversation. It's using, uh, multiple of those servers. This is, uh, at the end here we're showing, uh, agent core memory as well. So we're browsing, we're showing you the short-term memory, the detailed conversations were tracked, showing you the long-term memory where it learned some facts and it was able to surface those. So, uh, with that, let me hand it off to, uh, Saarci from, uh, Erickson. To talk about Agent Core. Thank you, Mark. Hello everyone. You just heard about agent code and how it bridges the gap between proof of concept to production. Now I'm going to show you how this looks like in practice. My name is Sarvasis Das. I am a principal data scientist and tech lead at Ericson. Within Ericsson, we are building a number of agentic solutions across different areas. And today I'm going to show you one of our agenting solutions that reduces our network engineers' research time from days to minutes. Before that, let me give you some context about Ericson. If you think about global connectivity, Ericson is at the heart of it. Right now, as I speak to you. 50% of the 5G traffic throughout the world passes through the technology that we build. With such a scale, it comes complexities. The question is that how do we keep our network engineer efficient in such a complex environment. In reality, our network engineers were trapped in knowledge silos. Let me explain A network engineer working on his features needs to look into thousands of different documents just to get to understand what this feature is all about. Then they need to find out what exactly this has been implemented in our millions of lines of code base that is distributed on hundreds of different subsystems. You can imagine this takes days. Not because the work is hard, but the information is fragmented, the knowledge is isolated. I can imagine that many of you have also seen this in your organization. Our goal here is to fuse the information across the different knowledge silos to make a unified understanding of the system. How to do that. Here is our solution. We have created a three-layered architecture to eliminate the knowledge silos. as you can see, at the bottom layers we have industry standards, we have our code base, we have product information. These are all fragmented information. We need to process this document in a way that it is ready to consume to the next layer, the knowledge layer. That's why we have developed advanced data processing pipeline. In the advanced data processing pipeline, we fetch. The data across different knowledge silos passed through Gen AIP processing pipeline. The goal for the Gen AIP processing pipeline is to fetch, is to process the data, clean the data, and also the key is to extract information from different modalities. Once it is done, it's stored the data to different AWS storage, depending on what data type. It can be AWS Bedrock knowledge base or it can be a graph database. Once the data is ready, it fades to the next layer called knowledge layer. These knowledge layers consist of three components, as you can see. We have specialized agents. These are Agents that are experts you can think about in different areas. So we are running a number of this type of specialized agents. Second, we have specialized model. These models are specialized in a sense that we have trained those models with our in-house data and also the code base. And the next part are the developer's tools. These are the tools our network engineers use on a daily basis. The knowledge layer is connected on the top with an orchestration layer where the agentic orchestration takes place and communicate through the, through the knowledge layer using MCP protocol. The key here, we are not just connecting different tools in this layer architecture. Instead we are enabling system comprehension. The system that understands the whole workflow of our network engineer. Let me show you how these systems looks like when you deploy this on production using agent code. So this is our architecture that is deployed on agent code. Our network engineers interact with the system using natural language just like asking a question to your colleague. Uh, that is very commonly one question people are asking that what is a hard timer? Any of you have any knowledge about hard timer? Could you raise your hand? I can imagine that this is a very specific telecom specific query questions, so. Then the next step is the the Amazon Que developers or the front end. These are the agentic system that basically interprets the queries and try to identify what specialized knowledge needed to answer this question. And based on that, it requests securely through the gateway to different specialized agents that are deployed on agent core runtime. Agent code runtime gives us the flexibility to deploy any framework, any open source framework, uh, on this, and these are server-less. So in this example you can see that we have 3 GPP agents, which is an expert in telecom standards. We have code generation agent. This is an agent that helps our network engineer to generate code. This agent specially powered by in-house trained LLM. Ericsson builds its own AI, and the architecture is just a proprietary information. Of the self-coding models that is available today is not aware of those information. And then another example, you can see the RAND System design agent, which is, is an expert in designing the system and as well as how we are configuring within Ericson context. All these agents sits on in strong knowledge foundation as you can see on the bottom part in the knowledge layer, and that has been created using advanced data processing. The key here is that all these agents that you can see running on the on this agent called runtimes, they do not work in isolation. Instead, they collaborate with each other, they set the context, and they build their insight based on each other's findings. We are also leveraging other agent core services that makes this architecture enterprise ready. For example, agent core observability gives us the flexibility to check at a granular level how our specialized agents behave and why. The identity also gives us a secure way to keep access control. Now the question is that how our network engineers uses this system. So you can see that generally, as I mentioned, they interact it with the with natural language. So this is an example question. Explain how hard timer works and elaborate in downlink design. Identify and list the main implementation component in the code. As you can see, this has multiple parts asking about hard timers and also implementation in the code. As you can imagine that a typical rack solution is not going to give you the answer that our network engineer is asking for. In our case with our agentic solution, first the RAND system design agent kicked in, and it is a deep research agent. What it does is it looked for hundreds of different documents, tried to identify what is this hard timer, and generate a deep research output. And this deep research output then fed to the next agent is our AI code search. AI code search is able to find out from natural language where exactly it has been implemented by looking at our millions of lines of code base. Once it is done, it passes on the output to the next Ericsson Silicon LLM, and this is also another in-house trained LLM capable of explaining the code. As you can see, all these agents are not going to work in isolation, but they collaborate and share the context at the end. The network engineer will not just get a chatbot response. Instead, a system level understanding from the theory to the architecture to the implementation in the code and all connected, how the features works. Let me show you quickly two examples of the output. So this is an output generated from our deep research agent. As you can see that it's a very comprehensive documentation and also it generates. The images on the fly that helps our network engineer to understand the concept easily. This is another one where our Ericsson Silicon LLM explained the code and you can see that it's not explaining just a syntactical explanation. Instead, it's trying to connect it with the concept. Now imagine a network engineer started working on these features, had to do all these things manually, looking at the features like deep research on that, finding the implementation in the code base, and connect, and this takes, depending on their experience, 3 to 5 days. Using our solution deployed on agent agent code, we can get it in less than 7 minutes. This is a 99% reduction in the research time for our network engineer. We have built the system. The next question is that how do we evaluate this system. For evaluation, we have, we are using a dual approach. First of all, using an automated script or framework, we can easily identify the consistency and correctness, but it's also important to evaluate through human expert for the accuracy and whether it meets our network engineer use. Our network engineers' expectations or not. By combining the human feedback that we have received and also the value, it gives us the confidence on the reliability of the system. While building this agentic solution, there are a lot of learnings for us, and let me share the important ones. It's a perfect timing, we say our agent cores. We have been building this type of specialized agent for some time, and we reached to a point where scalability was an issue for us. When we see agent core, it, it feels like it's exactly what we needed. Next is that involve the domain expert early. What I mean is that if you are developing an agentic solution, my recommendation, you involve your domain expert from the day one. For us, the feedback that we receive from the domain expert about how our network engineer works, what are their behaviors, and what are their pin points help us to design these specialized agents. Third is the importance of unifying the knowledge. When you connect your data and make a unified knowledge base from industry standards, code base, your product documentation, you get a system level understanding, and that's overall improved the accuracy of, of the answers that you get and also at the same time it's removed any ambiguities. Last but not the least. Infrastructure matters even more than we think of. Using agent core, our network engineers focus more on developing the agenttic structure, agenttic structures, and as well as how the agent will behave. All the heavy lifting is done by the agent cops. As a result, the development life cycles improve significantly. Let me summarize here. With Agent Core, we have moved beyond just answering questions. We have developed a system that understands how our network engineers work and think, and we are excited what comes next. With that, let me close here with a perspective from our organization head, Doug Limbo. By unifying the data and the information, agent core, let us build specialized agents that are scaled over our tens of thousands of network engineers. Thank you. Over to you, Mark. Thanks Saarcis. OK, uh, since I was so great at demo one, I'll do a, uh, a quick pass on demo two here. I'm just gonna kick it off and not try to click any mouse. Uh, uh, what this is trying to show here is, uh, I get a lot of questions about how well if I'm using a coding assistant, uh, am I able to, uh, easily use Agent Core, uh, and yes, what this is showing you is a scenario where I've got an existing REST API. I've got uh a a URL to it and an API key and I kicked off a session with Kiiro to say take that existing API, give me a gateway that will use that the MCP, give me a strands agent to use that. And then deploy that agent to the cloud, write a client, uh, test that client, and even write a load test for me, uh, and run that, uh, so here it goes, um, so I gave it some context. I gave it the URL, uh, I gave it the API key. Now it's exploring the agent cop command line to understand what's offered. It figures out, oh, I can do this. I can create a gateway. I see how to do that. Uh, I know I've got access to the, uh, the API, so it probes that rest interface, extracts the open API spec. Uh, earlier you saw the swagger user interface there describing the API. It's able to download that. It pops it into a JSON file, uploads that to S3, and now it's ready to, uh, create the gateway, so. Uh, it creates the gateway and here on the left I'm showing you the actual API spec. It was able to retrieve that on the fly and again I still haven't written any lines of code. I haven't touched any command lines at all. I just said, hey, go do this for me please. Uh, and the gateway is already created here we're just waiting for the DNS propagation to complete. It takes about a minute, uh, so it's, uh, uh, got it all set up. It's got it configured. Uh, in about another 10 seconds here it'll be ready. Once it's ready, then it's going to add the API target to the gateway. So what is that all about? It's got to, uh, configure an OA authorizer. In this case I've told it we should use Cognito. That's the standard that we were using in this environment. It could be whatever you'd like. Um, and, uh, it looks like it's already done it now, and now it's on to, uh, testing. So I asked it to test. Given that it's standard MCP, just use HTTP and do a list tools, uh, do an invoke tool on any of the available tools, and so now it's, uh, it's got the gateway that's API, uh, sorry, the API is now agent ready. Uh, now it's on to testing. Uh, so let's see what happens here. So it writes a little code to uh try to test it out. Uh, of course it's going to be successful, no problem. Uh, you'll see uh in a moment here it's going to uh list the tools. Uh, first it's getting a, uh, an access key from the authorizer. Uh, you've gotta do a secure invocation here, uh, using the MCP protocol. And voila, there's all of the tools automatically coming back as AP as MCP even though we started with just a rest API. So it's done that mapping. It's got it live. You can plug that into any agent and here we tested out invoking a tool. So did MCP call tool on one of the tools to list customers who was able to do that. Here it's retrieving the details on a particular order. So we've got all of the basics here of, uh, managing orders based on existing APIs and it's uh available uh as MCP now. So then it's creating a simple strands agent. Uh, and any of these agent frameworks makes it easy to just plug in an MCP server. So that's what it's doing here, uh, and, in less than a minute here, uh, it's gonna have a running, see, it's actually executing right now. It's actually listing servers, getting, uh, listing orders, getting order details. So now the strands agents already working. We're only a few minutes into this job here. I still haven't done any work, uh, kind of nice. I like this, uh, and now, uh, what I'd like to do is deploy it in the cloud. It's great on my laptop. I want it in the cloud. I wanna be able to hand it to app developers and app builders. And so, uh, Quiro says, OK, let me see what it takes. It checks out the commands. It figures out all I need to do is configure. And then launch So it does a configure, configure sees that it's got your agent, uh, it knows how to just then at that point package the code. Upload it and then create a secure endpoint for that agent. So that's what it's doing right now it's doing the packaging, uh, it's getting it uploaded, it's setting up the endpoint in the cloud, another 10 seconds or so that'll be running. Once it's running in the cloud, Kiro says, OK, let me test the uh agent via the command line, so we'll do an agent core invoke, uh, prove that it works. Here it goes right now, it's doing an invoke. Uh, and let's see if that works. Hey, what do you know? It's able to list the customers. Uh, you can get the details of a particular order, no problem. It's got session management. It's a secure session isolation. It can handle those concurrency at scale, uh, and just to prove that out, uh, I ask it to, um, write a simple, uh, client script, uh, to prove that you can use it from Python from VO 3 remotely. And then I had to write a little load test to spin up uh a bunch of sessions concurrently and prove that that works uh as well. So there's uh uh a nice little demo of not writing any code, taking an existing API, making it available via gateway, writing an agent, hosting an agent in the cloud, all with the magic and wonder of Agent Core. So let me give you quick, uh, 4 quick takeaways. One Business value only comes when you're in production. Uh, maybe, maybe it's obvious, but there is that big chasm, uh, and you need to address that in order to start getting real value out of these agents. Second, know what your agents are up to. Never put an agent in production if you don't have a good mechanism for observability. You need all of the detail there, and you need an easy way to get access to that and then find a way to iterate and improve your agents, as well as troubleshoot and debug when things are going wrong. Third, security is not optional. Scale is not optional. Uh, you're not building toys, you're building production ready, uh, agents to, to deliver real value. You've gotta have security and scale figured out. And then lastly. Don't waste time crawling around in that chasm. Use Agent Core and get a smooth path right over that chasm from uh POCs into production ready, uh, real business value. So with that, here's some resources you've got great documentation out there, quick starts, tutorials, we've got workshops, self-service, or you can use uh our uh help with those. We've got a pretty robust uh repo with tutorials as well, examples of integrations, A2A, multiple frameworks. If you're looking to learn more about Agentic AI overall, this is a, a skill builder capability. And then with that I wanna thank you all for coming. uh, hopefully this is giving you a good idea of how to cross that chasm from POC to real production ready agents. Thank you.