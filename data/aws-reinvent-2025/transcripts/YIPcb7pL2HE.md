---
video_id: YIPcb7pL2HE
video_url: https://www.youtube.com/watch?v=YIPcb7pL2HE
is_generated: False
is_translatable: True
summary: "This session, delivered by Aaron Daly and Jim from AWS, focuses on optimizing self-managed database deployments using the Amazon FSx family of file systems. While fully managed services like Amazon RDS are popular, the speakers explain that many customers opt for self-managed databases on EC2 to retain granular control over infrastructure, software versions, and backup schedulers, or to optimize licensing costs. The core argument of the talk is that storage choice is critical in these architectures. They introduce three specific FSx services tailored for database workloads: **FSx for Windows File Server** (ideal for SQL Server via SMB), **FSx for OpenZFS** (high-performance NFS for Oracle), and **FSx for NetApp ONTAP**, which is presented as a \"Swiss Army knife\" supporting multi-protocol access (NFS, SMB, iSCSI) and advanced data management features.

A major portion of the session details the unique capabilities these storage services offer over traditional block storage like EBS. Key among these is the ability to create **instant snapshots** that are database-consistent and independent of database size, allowing for backups in seconds and recoveries in minutes. Closely related is the **cloning** capability, which allows engineers to create multiple thin, writable copies of a production database almost instantly. These clones consume negligible storage (initially just metadata) and are game-changers for creating development/test environments, testing upgrade patches, or running \"what-if\" scenarios without impacting production or incurring significant costs.

The speakers also highlight the financial and operational benefits of **storage-level replication**. Unlike database-level replication (e.g., SQL Server Always On Availability Groups), which consumes compute cycles and often requires expensive Enterprise licenses, storage replication (like NetApp SnapMirror) handles data movement at the infrastructure layer. A customer case study is cited where switching to a Failover Cluster Instance (FCI) deployment with FSx shared storage saved 60% on SQL Server licensing costs. Other customer examples include S&P Global using SnapMirror for disaster recovery and Amdocs achieving better-than-premise performance through features like inline compression.

The session concludes with extensive technical demos illustrating practical workflows. The first demo showcases a Disaster Recovery (DR) scenario for SQL Server using FSx for NetApp ONTAP, where a snapshot is replicated to a DR region and cloned to verify data integrity without interrupting the replication stream. The second demo features an Oracle deployment on FSx for OpenZFS, where an 18GB production database is cloned for development use, consuming only ~40MB of additional space. These examples reinforce the session's practical focus on using advanced storage features to improve agility, reduce costs, and enhance data protection for self-managed databases."
keywords:
  - Amazon FSx
  - Self-managed databases
  - SQL Server
  - Oracle
  - FSx for NetApp ONTAP
  - FSx for OpenZFS
  - Database Cloning
  - Disaster Recovery
---

Hello everyone. Welcome to our STG 337 session. Uh, hope everybody's been enjoying their reinvent week thus far. Uh, in this session we're gonna, uh, cover improving self-managed database, uh, performance and agility with Amazon FSX file systems. So my name is Aaron Daly. I'm a senior specialist solutions architect here at AWS, uh, and I cover our file services. With that I will hand it over to Jim. Thank you, Erin. Can you hear me? OK, great. Uh, before we go on, how many of you have familiarity with Amazon FSX? Just raise your hand. OK, great, good to hear. Uh, so what we're gonna talk about today obviously is the importance of choosing the right FSX file system for your databases, but we're gonna go through a couple of other things first with respect to. Um, how exactly customers tend to look at these types of decisions. So the things we're gonna talk about today are sort of generically why are people coming to the cloud, why are they bringing databases to the cloud first and foremost. Secondarily, what sort of important decisions do they need to make, uh, in terms of deployment options? We've got, uh, fully managed options. We've got self-managed options which I'll talk about a little bit. Uh, why storage matters for choosing databases and oftentimes, uh. I don't know about if this happens in your organization, but oftentimes we see customers overlook, uh, storage features with respect to what can storage do, what can it add to the mix. When you think about deploying a database, you've got storage, networking, and compute, obviously, uh, that's the important infrastructure stack that we need to think about as we're deploying. So how does storage enter into that? And then I'm gonna turn over to Aaron for the last couple segments here, which is some practical advice, and we'll go through a couple of demonstration scenarios with respect to. Uh, a SQL Server database scenario and an Oracle database scenario. So that's our agenda for today. Um, I won't spend a lot of time on this one, but as we think about why customers choose to come to the cloud, and I'm sure you've gone through some of these same scenarios in your own mind, oftentimes it's because there's a mandate to close down or reduce the number of data centers that might be under management. There's usually a cost element associated with that. Sometimes customers say, I build cars, I build chips, I, I manufacture sneakers, whatever it is that my core business is. Um, maybe IT is no longer something that I should focus on and no longer something that I should be as invested in in terms of procuring hardware and software. Obviously I still need to deploy an infrastructure stack. I still need to manage an infrastructure cloud, uh, stack in the cloud, but what if I didn't have to manage the facilities? What if I didn't have to worry about hardware upgrades and changing hardware in and out, uh, when it either fails or. Um, runs out of gas. Digital transformation seems to be on the mind of many executives these days. Uh, really behind that, I think that the message is customers want to move faster. They wanna be more competitive. They wanna be able to get, um. Uh, perhaps a competitive edge, uh, by doing things differently with their application set which will allow them to either make decisions faster or build things faster or again whatever their core business is to allow them to do those things faster, uh, and get an edge against their competitors over on the right hand side, um. In addition to increasing agility, um, we often see customers who will say that their deployment in the cloud is actually more secure than it was on prem. Now we could argue that that's not always the case, but I found it really interesting when customers say, I feel more secure in the cloud than I felt on prem. There's a few reasons behind that which I don't really have time to get into, but the last thing I wanted to talk about today, um, is putting data to work. So we have a lot of, uh, as organizations we have a lot of data. Uh, what we don't often do well at is looking at the insights trapped inside that data. What does that data contain that will help me to be more competitive or to move faster or to learn about myself in terms of how I do business? And so this notion of putting data to work is far simpler in the cloud. Um, once your data is in the cloud, there's a variety of services. There's actually tens of services within AWS, for example, that will allow you to drive additional analytics against that data or to put it to work in useful ways for Gen AI, Gen IML, for example, and so these are just some of the drivers that we see. Obviously databases are part and parcel of all of these decisions that are made when coming to the cloud. And there's a couple deployment options as I alluded to earlier. You can choose fully managed databases. There's a whole bunch of them represented on this slide. My intent is not to walk through each and every one of them, but categorically, um, you've got relational databases and more than likely you have some non-relational databases in your infrastructure. And so when you think about bringing those to AWS. Starting in the lower left, um, these are standard, um, relational databases that we've all known for decades in many cases. Some of them are newer, but what if I want to bring those to AWS and what if I don't want to manage them myself? What if I simply want to take over at the layer where I've already got the database, I create my tables, I manage how I use those databases, etc. Um, this is, uh, what we often see customers do who want to get away from the complexity of managing databases on-prem. Uh, with respect to non-relational databases, you can see we've mentioned a few there. Oftentimes when, um, customers come to AWS, they will look at things like Dynamo DB or Elasticash, for example, uh, to replace those non-relational databases they may have run on premises. And then over on the right-hand side, uh, lower right, I'm not gonna spend a lot of time on that, but the idea is I might have a big data farm. Maybe I've got some Hadoop in my environment. And uh I wanna bring that to AWS. I might choose to instead bring that to EMR and uh host that big data and the applications that use it uh in EMR and of course a variety of analytic solutions including Elastic Search. So again, lots of ways that you can come to data to um AWS and bring your databases. Now I wanna segue into this notion of self-managed databases and why would I choose to self-manage a database? Why would I bring Oracle to AWS or SQL Server to AWS, for example, and not choose RDS, which would be the fully managed implementation of those databases? There's some reasons we've listed here. It, it tends to be a combination of business drivers and technical drivers and. Uh, cost is almost always one of those drivers op optimization of TCO. However, um, if, if we look at some other things which tend to be, um, pretty important in many cases, um, with, with RDS, for example, you get sort of set. Ways to to back up and protect your data. Well, what if I want to do that on um a, a more granular interval or what if I wanna be um have complete control over the ability to drive how I back those databases up and how I recover them. Um, that's not possible with RDS, but with the self-managed implementation of a database, um, it certainly is. Or, you know, what if I want to scale my database over multiple regions for, uh, or multiple AZ I should say for the highest resilience, um, that's also possible with, uh, a self-managed database scenario. And of course there's some technical drivers. What if I want to control the entire infrastructure stack? I want to choose my Oracle version. I want to choose my Linux version. I want to choose other things running on that database server that maybe complement that database deployment. I can't do that with RDS. So these are but some of the reasons why our customers are choosing. Self-managed databases. Now, if I didn't say this before, we almost always see customers choose some combination of self-managed and fully managed databases, so it's not exclusive one or the other. It tends to be a mixture of the two as we look at um kind of the things that are important with respect to deploying self-managed databases over on the left-hand side, we have different database engines. You've seen those on previous slides. Over on the right hand side we have some choices that you as customers would need to make when deploying those self. Managed databases, namely at the top of the stack you've got compute various varieties of compute, whether it's native EC2, whether it's ECS EKS from a containerization, uh, perspective, or whether it's, uh, Elastic VMware service, which is one of our newer compute services, EVS, that's VMwaree running in AWS, uh, and then, uh, kind of the heart of our discussion today really is around storage and what storage am I gonna choose and why. So as we think about deploying storage under those databases, oftentimes. Uh, customers choose block connectivity from their database servers to their storage, but not always. That's of course not a requirement. Sometimes they choose file protocols, uh, namely NFS and SMB. So what we're showing here is that you've got a couple of broad options, one being EBS, which I think we're all familiar with. That's block storage and block storage only, um, works very well. Uh, it's been around for a long time, and we have a tremendous number of databases deployed on EBS. From a FSX perspective, uh, what's cool about FSX is you get the choice between block or file depending upon which FSX, uh, family member you choose. You might choose block or file, and we'll get into that in a second. So lastly here at the bottom, um, you get to choose your database engine, you get to choose your compute, and you get to choose your storage with the self-managed database deployment. So let's talk now about FSX, uh, and, um, FSX has been around since about 2018 when we launched our first two services that was Windows and Luster back in 2018. We, we launched them at reinvent back in that time frame, and the way to think about FSX is, um. F and S stand for file system if that's not obvious. X is a variable just like it was in your high school algebra class and in the case of X, there's 4 different possibilities that X can be equivalent to one being luster, which is for HPC workloads, not really. Uh, we're not gonna talk about that today because that's not really for databases, but, uh, FSX services that are appropriate for databases are Windows, Open ZFS, and ONTA. Uh, and so we'll, we'll talk about those three. So again, you get your choice here based on what your requirements are, and really the design goal of FSX is to build storage services based on the world's most popular file systems, if I were to say it simplistically. So we look at file systems that are, um. Used predominantly and in use by many, many, many customers and we bring those to FSX and again the design goal of FSX is to build a like for like experience in AWS so if I. I love Windows and I'm used to running Windows as my file server on-prem. I can run FSX for Windows and the way I administer that is exactly the way I would administer a Windows server. There's some nuances on top of that are that are AWS specific, but the idea is we want to build a like for like experience. The same is true with Open ZFS or ONTA. So what you're accustomed to on-prem, if you built automation around that, if you have years of um. Uh, of, uh, experience with one of those and you wanna bring all of that experience with you and all the automation, scripting, etc. to AWS, you can do that, uh, and they operate exactly the same way in AWS. So again, these are the three. Services that are most interesting in a database environment. So we've got, uh, as you can see, Windows, Open ZFS and OTP, and we'll talk about each of them. Um, they all have different sets of features and capabilities. There are reasons why you might choose one versus the other. And so at a high level, um, I've got a couple slides on this, but at a high level, um, I want to just talk through starting on the far right. Um, we believe it's important to think about familiarity. So what I was saying on the last slide was if you're accustomed to Windows or ZFS or on tap, more than likely you want to remain accustomed to that in AWS. There's no reason to change specifically, right? If you want to bring that familiarity with you. So, um, we always like to, as we work with customers, think about familiarity. What are you using on premises and why? And would you like to preserve any or all of that when you come to AWS? Um, I mentioned that each service has unique features and capabilities, but it's always important to, you know, a list of features and capabilities don't really matter if they're not what you need. And so which of those features and capabilities are important to you. Coming over to the left hand side of the slide, we're gonna start at the bottom. From a resiliency perspective. Each of these services supports either a single AZ or a multi AZ deployment, uh, providing the highest resilience. As we go up to stack, um, we've got two of these services that provide a richer data management experience that's CFS and ONTAP, as you can see, uh, and we'll get into what that really means in a little bit, uh, and then lastly, um, OnTP is the only one that offers multiple protocols. It offers block and file. If we look at, uh, uh, Amazon FSX for Windows, that's SMB. FSX for Open EFS is NFS. And then FSX for on tap actually supports 4 different protocols, both SMB and NFS on the file side. On the block side, it's ICEGUzzy and NVME over TCP. So if you're looking for a sort of a Swiss Army knife of storage in terms of protocol support, that would be on tap. Let's dive a little deeper. So this is uh a, a small decision tree, maybe a decision bush if you will, it's quite small, but as you can see there's a couple of things that it that it asks and questions that it poses. Uh, I wanna start kind of in the middle. All this, this isn't explicitly called out um the protocols are named here so if you say. Me, um, I only connect my database servers over a block protocol to their storage, and you have one choice here, right? That would be FSX for ONTA. If you say I'm good with a file protocol, I'm happy to run Oracle over NFS, for example, then you could choose ZFS or ONTAP. If you say something similar about Windows, which is, I'm perfectly happy running my SQL Server databases over SMB. We have either Windows or on tap that could satisfy that requirement, so protocol is important. There's 3 other things here on the slide I want to talk about, um, and they're the words in orange starting right in the middle at the top performance, um, these do have different performance profiles, um, and, uh, 6 gigabytes per second is a relatively important delineator here. Um, Windows and ZFS each have the capability to perform above that. In the case of ONTA, that's essentially, um, it's ceiling today. Um, so we have to think about performance and what kind of performance can a single EC2 host drive, what performance will my database on that EC host, EC2 host drive, etc. um, kind of coming around clockwise from a data management perspective, this is where the services start to differentiate themselves a little bit, uh, beyond protocol. Both ZFS and ONTA have a rich set of data management capabilities, things like onboard snapshots which allow. Um, you to run backups in seconds, for example, and recoveries of your databases in minutes. That's irregardless of the database size. So, so you might be thinking, so wait a second, if I have a 1 gigabyte database or I have a 100 terabyte database, you're telling me you can back either of those up using a snapshot in seconds and recover them in minutes, and yes, that's what I'm telling you. It's independent of the database size, so snapshots are a really unique way to keep multiple recovery points that are database consistent, by the way. So when I recover from a snapshot or roll back from that snapshot, when I start the database up, there's no roll forward recovery required. It just starts, OK, based on the time the snapshot was taken. Um, also, the, uh, ZFS and ONTA have the ability to replicate from one instance to another, so I might have, uh, either ZFS or ONTA in region one, and I might have another one in region two, cross country, across town, whatever it is between regions, and, um, at the storage level you can do that replication. Now, uh, there's always this sort of this discussion about should the database do the replication or should the storage do the replication. And that's of course another choice that you get to make when the storage does the replication, um, you can, um, literally save licensing costs in some cases and you can also save um costs around the EC2 instance size that you have to deploy. If I'm asking my EC-2 engines to not only run the database engine but to also drive replication to its partner somewhere else in another region, obviously there's CPU cycles associated with that and required to do that, and therefore I likely need a bigger EC2 instance. So, um, those things are important in terms of considering do I want storage level replication or database rep level replication. Um, let's see what else. Um, clones. There's, uh, in both OTA and ZFS there's a capability called, um, cloning. Uh, different names for it based on different services, but the idea would be is I want to create a thin copy of my database environment, and what a clone allows you to do is in a matter of seconds, using one of those snapshots I talked about earlier, create a clone. I can then on top of that clone, which is thin and doesn't occupy any space, I can actually start another instance of the database which looks through that clone. At the same disks where my production copy of my database resides, so this could be for a what if scenario. This could be, I want to upgrade my database engine. I'm going to create a clone. I'm going to run through the upgrade scenario on my clone, prove it to myself that I can do an upgrade. Without any problems and then once I, I might have to do it several times, but once I get through that process and I'm satisfied that I can do it on this clone, then I, I'll be perfectly happy and confident that when I go to upgrade in my production environment, it's going to work because I've already proven it to myself in this cloned environment. What we also see is um customers populate lower environments using clones and use those lower environments either for short term or long term potentially. Um, for development and tests for training, for other enabling purposes, so the idea that um I might have to start a copy, you know, Friday night and then Saturday at noon after my kid's soccer game, I have to log in and check to see if that copy was done. Uh, you don't have to do anything of the sort anymore. You can use clones to literally create. Um, these database copies in a matter of seconds, and you can create up to 100s of them if you so desire. Now, do customers actually create hundreds of database copies? In practice they really don't, but the capability exists to create many, many copies of your databases using clones. They're all thin. They don't cost very much as a result, uh, to create. So again, those are, those capabilities are unique to, to ZFS and ONTAP. Um, let's come down to the bottom. Um, I've kind of already touched on this, but resilience, this notion that I can deploy in a multi-AZ configuration, which means I've got synchronous replication at the storage layer between two AZs. Now if I build a cluster on top of that that my database engine runs on, if I have a problem in AZ1. That cluster software or database and or database software fails me over to AZ2. I've got a synchronous copy of my data there, so my recovery point objective is is exactly 0 because I've got synchronous replication going on between the two, and the time it takes me to affect that failover is simply a matter of how long it takes that cluster to accomplish that failover. Usually happens quite, quite quick, OK. Um, let's talk now about when I'm deploying SQL Server again. I mentioned we've got two options using the SMB protocol, um, which is Windows and ONTAP. I can also use ISGzzy or NVMU or TCP, as I mentioned, for deployment on ONTAP. So as we look at some of the capabilities of, of Windows, um, Windows has VSSa FSX for Windows has VSS compatible snapshots. So volume shadow copy is completely built in. It's we're effectively running Windows here, um, so you would expect that capability to be there, um, obviously the SMB protocol is fully supported Active Directory integration for both authentication and authorization, um, and, um, from a reducing cost perspective, um, we've got on board compression and deduplication built into Windows file server which is helpful from a TCO perspective. We're lastly we're gonna talk about some minimum sizes here, um, and this could come into play when you have. Um, maybe you've got a lot of small databases or you've got maybe a lot of large databases, but you want to figure out, you know, does it make sense to choose one or the other based on what my database database sizing might be, um, and you can see the sizes there and what we're calling out is the minimum size file system is 32 gigabytes in size, and the minimum throughput capacity we call it, which is the IO performance that you get from that file system, is 32 megabytes per second. Um, let's talk about a customer here that's running, uh, a SQL Server on SMB, uh, using FSX for Windows. This is, uh, Ava AVO Arrow, as you can see, and I think the thing to call out here is, um, I mentioned earlier that there might be potential cost savings in licensing that are interesting. And so this is exactly one of those use cases where they're running, um, again SQL server, uh, it's an FCI deployment, so a failover cluster deployment. Uh, in this case, they're, they're not using um enterprise licensing, they're using standard licensing. And you may know that the difference between SQL standard licensing and SQL Enterprise licensing can be about 2 to 4X. So in this particular case, this customer is reporting that they saved about 60% on their Windows licensing because they went from an always on deployment to an FCI deployment, meaning they don't need to license Windows and the SQL components on the other end. Um, the sequel components I should say on the other end. So, uh, great story here, uh, obviously a public story. Um, these are those two options. I won't spend a lot of time on this, but on the right hand side we've got the FCI deployment. On the left hand side we've got the always on availability group deployment. The difference here is on the left hand side the database is driving the replication. That's why I need SQL license on both sides. On the right hand side, the storage is driving the migration. So again, there there's almost always cost savings associated with an FCI deployment relative to a um availability group deployment and and that's a function of Windows licensing and also a function typically of those EC2 instance sizes that I spoke of earlier. OK, let's talk now about an Oracle deployment. Uh, I have two options here, ZFS and ONTAP. Uh, again, ZFS would be NFS. ONTAP would be either NFS or one of the two block protocols I spoke about. Excuse me. Um, with FSX for Open ZFS, I think I've talked through most of these capabilities, if not all of them, but, uh, a rich set of capabilities, um, you can see the NFS versions that are supported there and kind of working up the stack. Um, you've got the. Ability to do the replication that I spoke about snapshots that drive really fast backups and recoveries, etc. From a sizing perspective, you can see um slightly larger with ZFS versus Windows. They were 32 and 32. You might remember on the Windows side. With Open CFS, the smallest file system is 64 gigabytes in size and the smallest throughput capacity being 64 megabytes per second, so you get a little bit more performance always out of the box, um, with even the smallest file system, uh, in the case of FSX for Open ZFS. Um, here's a customer example, AMDdocs, uh, you may be familiar with them, um, they do a lot of work in the healthcare industry in particular. Um, but what they found in, in their deployment was they were able to through the use of compression with um FSX for Open ZFS onboard compression within FSX for Open ZFS, they were able to reduce their overall um database storage and um I think what's significant too is the um. The performance that they saw, uh, sometimes people like look at me cross-eyed when I say that when they bring applications from on-prem to AWS and they actually see better performance in AWS and they're like that can't be possible. I don't know how they did that. Um, this is one of those examples where this customer actually saw a better performance, um, obviously had to do with potentially a suboptimal deployment on prem. Um, and a maybe more thoughtful deployment in AWS in terms of let's make sure we size everything properly, uh, but we do see this. Uh, from time to time. So for FSX for ONTAP, this is actually a similar list of features and capabilities. Um, it's actually maybe slightly larger than the one you saw for ZFS, uh, namely multi-protocol on top, not capable as we talked about in ZFS, but some of the same capabilities in terms of backups in seconds, replication cross region, uh, certainly possible, TCO optimization through storage efficiencies. Um, one of the differences here too as well is for those customers that are running OTP on-prem and they choose to migrate to AWS, ONTA has a feature called Snap Mirror which is replication between two ONTA instances. And so what we see from a migration perspective is it's really simple to take a database or any other workload that's um hosted on ONTAP on-prem and bring that to AWS because you can simply fire up SnapMirror, uh, you set up a SnapMirror relationship between the on-prem. ONTAP system and the FSX for ONTAP system and AWS. Once that relationship is set up, you press the go button and replication just occurs, um, on its own. From a sizing perspective, um, FSX for untapped file systems tend to start slightly larger. You can see the smallest file system, uh, size is 1 terabyte, uh, and the smallest amount of throughput capacity comes in at 128 megabytes per second. So again, if you have a bunch of 32 megabyte databases, you might say. This doesn't work for me because the smallest file system is 1 terabyte, in which case you have a couple of options choose a different service, namely FSX for Windows, or you would say, Hmm, I'm going to stack several SQL Server databases onto the same file system, in which case I can easily consume 1 terabyte of capacity. Alright, a couple of examples, um, that I'll go through quickly. This one is S&P Global. This is, uh, literally many hundreds of SQL Server databases that they, that are core to S&P Global's business. Uh, they brought these from on-prem and, um, the, um. These databases were not running on OTA on-prem, uh, as it turns out they're running on a different storage solution, but again there's hundreds of them that they brought to AWS and um what they were doing on-prem was they were doing database level replication. Now they've got a multi-Ay deployment in every case under these databases, and we've got storage level replication going on. Uh, they are using SnapMirror also for disaster recovery to protect those within AWS. So the primary database is an AWS on FSX, and they're also snap mirroring to another region, i.e., replicating to another region for purposes of disaster recovery. What they've done actually is they've got um multi AZ instances where the primary databases are running. They're replicating to single AZ instances to reduce cost. And those single AZ instances would act as their disaster recovery failover option should they need to go there. The next one is Pearson. Uh, I think we've all had exposure to Pearson over the years, taking examinations through them or at their facilities. Um, this is both Oracle and SQL Server. Uh, by the way, I should go back. This is deployed on, on, uh, SnapMirror. I don't think the slide says that. Oh yeah, uh, excuse me, iuzzy. The slide does say that. So they used iuzzy. Pearson also used iuzzy, so block protocol. Um, this is both Oracle and SQL Server. Uh, workloads and this is their core, um, customer system. So when you register for a class, when you take a class, um, and all the course content for that matter is all hosted on these two key systems, uh, that they brought to AWS, they were actually, they've been on, um, FSX for OnTap now for I would say 3+ years. They were one of our first customers. OK. All right, I'm gonna go through a couple of features. I think I have 2 more slides, and I'm gonna turn it over to Aaron. So I, I just want to try to illustrate here how snapshots work and although this is simplistic, um, what I want to call out is a couple of the bullets on the slide, um, namely that these are point in time snapshots, that's probably obvious, but you can create and maintain literally hundreds of them if you choose to do so. Uh, again, I don't see customers in practice create hundreds of copies of snapshots and maintain them. But you might keep 5, 1015 copies of your database depending upon how far you want to go back in your recovery cycle. Um, you can replicate these snapshots to another region if you choose to do so. In fact, one of the use cases we sometimes see is customers will create snapshots of their databases in region one. They will replicate using um either uh Open ZFS's replication capability or ONTA's replication capability. Those snapshots to region 2 and then they will clone those snapshots in region 2, that way they can maintain replication for purposes of disaster recovery, um, and they use those clones for development and tests. Including testing disaster recovery. If your boss comes around and says at 2 o'clock on a Tuesday, I want you to prove to me that you could recover this database or set of databases in a disaster, you can say, Hang on a second, boss, while I clone those databases and I show you that I can bring them up and I can drop a table and I can then look over at my DR copy and that table's still there, etc. etc. There's all kinds of scenarios you can go through. Aaron's actually gonna show you some of those in a minute, but, um, snapshots are. Really, really useful in terms of keeping around copies. These are virtual copies of your database that you may want to recover from over the short term. That's not to say that you might not also want to use a cohesive or a rubric or a convolt to, to create longer term backups. You can certainly do that as well, but snapshots can be a secret weapon in terms of being able to do really, really quick, um, disaster recovery or, or recovery from maybe something that's not a disaster per se, but that's, that's not for the business. Uh, my last slide. Um, clones, I talked about these a little bit, but what we're trying to do on the right hand side of this slide is illustrate what a clone looks like and how it works. So I've got a primary copy represented by that blue disc, and I've got in, in, in this case, a couple of clones, um, one I'm using for development and one I'm using for. These could be based on the same snapshot or they could be based on a different snapshot taken at different points in time. And if, if I decide that I want to refresh my test environment, I simply create a new clone. And I mount that clone and I start my database. So if I'm 24 years, sorry, 24 hours in arrears, um, in terms of the difference between my production database and my primary database, uh, or my, um, my, my, my secondary database I should say or my test and development database, I can very quickly refresh, refresh those clones, um, and be right up to speed. So, um, lots of capabilities that these clones can be used for. But the last thing I want to point out is that um they, they only use um incremental capacity. I mentioned earlier that they're thin. When I create a clone, virtually no additional space is required to create that clone. There's a little bit of metadata. Um, but it's measurable usually in megabytes. Um, now, once I start the cloned database or the copy of the database, let's say that I now add a table to that clone of the database. Obviously I need some space to occupy that new table. So in that case, um, my clone will be exactly the size of that metadata associated with that database copy. Which again is small and that new table that I just added to that database so you can bring those databases out of sync and do a what if scenario and you can tear that clone down, you can build a new clone and you can start all over again with whatever test and development scenario you're interested in. Um, and then the last thing here is if you have a desire to split that clone, you can do that and then it becomes a 100% copy, a full copy, in which case you can then clone. That split clone, you can do all kinds of crazy things with these copies of data, but we have customers doing really creative things with, with snapshots and clones which completely changed the game. And if you're using EBS right now for your databases, EBS is a wonderful, very capable service. It doesn't do any of these things, right? So if you have a desire to, to utilize your databases and do different things with how you copy your databases, how you protect your databases, then we would ask you to consider the FSX family because it really is a game changer and this goes back to the statement I made at the very beginning, which is why does storage matter when I deploy a database. Some might say it doesn't matter at all. Actually, it can matter a lot if these features and capabilities are useful to you. So with that, I'm gonna turn it over to Aaron. And he's going to walk you through uh a couple of other reference diagrams and then we're gonna get into a demo. Thanks, Jim. All right, thank you, Jim. Uh, so I'd like to quickly go over the different, uh, deployment types that we have available in FSX that we typically see customers utilizing when, uh, managing or deploying their, uh, file system for self-managed databases. Uh, we have several different deployment types within FSX, but specifically there are two deployment types that we typically recommend and see customers using. Uh, so we'll start out here with our single availability zone, highly available deployment type. Uh, starting on the left hand side we have our Oracle production server. This could be any database engine. We're just using Oracle in this example. It could be Postcards, SQL, uh, Maria DB, etc. Uh, but when you deploy this file system deployment type, uh, as part of the service, we will deploy a primary file server for you and a standby file server for you in the availability zone that you indicate you'd like the file server to, to reside within. All your reads will be serviced from that primary file server, uh, and any rights will go to the primary file server and then be synchronously replicated down to the standby, uh, and once committed on both will be acknowledged back to the clients. Now if, uh, that primary file server fails for whatever reason, uh, you know, like a hardware failure, or if we're doing, uh, patching and maintenance on the file system during the maintenance window that you give us, uh, for this, uh, file system, uh, the client, the standby file server will be promoted to the primary. And as long as your NFS mount options or your IScuzzy MVME multipathing is set up correctly, your clients will seamlessly, uh, fail over to the standby file server. Uh, there will be a short period of increased latency while that failover occurs. But once, uh, the clients are all connected up to the standby, which has been promoted to primary, uh, they will go on operating as they were previously on the primary file server. If the hardware failure occurred, we will actually replace that primary file server or again, if this was just during maintenance, we rebooted that primary file server as soon as it comes back online. Your clients will then fail back to the primary and standby will go back to standby as it was previously when we initially deployed. So the 2nd deployment type that we typically recommend or see customers using for self-managed database is going to be the multi-availability zone highly available file system. So starting on the left hand side again we have an Oracle production server. Again this is just Oracle in this example. It could be post grads Marie DB, etc. uh, pick your flavor of database engine, um, but when you select this deployment type for the file system, we will deploy a primary file server in the availability zone that you indicate as being your primary production, uh, uh, availability zone. And then we will deploy a standby file server in a 2nd availability zone that you again indicate to us as being your standby for your environment. Again, all reads will be serviced from the primary. Uh, all rights will go to the primary, be synchronously replicated over to the standby, and once committed on both, uh, be acknowledged back to your clients. Uh, if not only the primary file server fails in this scenario, but the entire availability zone fails, uh, we will promote that standby file server to, uh, primary, and if you have a, uh, optional standby, uh, database server running in that availability zone, you could mount, uh, the file system, uh, or the LUNs, uh, in this case that we're showing here, uh, and start running your production from the second availability zone. Now just be aware that when the uh AZ1 comes back online, the file system is going to fail back over to the primary. We're always gonna try to run the file system from the availability zone that you indicated to us initially when you deploy the file system as your primary um AZ. Alright, so when Jim and I were putting this, uh, presentation together, um, instead of just PowerPointing you to, you know, giving you PowerPoint after PowerPoint, we thought it'd be a good idea to actually give, uh, some actual real world examples of how we see, uh, some of our customers leveraging these advanced capabilities, uh, within FSX when they're running their self-managed databases on an FSX file system. Uh, and so one of our examples here that we come across very often with customers is that they're doing disaster recovery, uh, you know, from one region or one AZ to a second region or second availability zone, and they want to test and validate that the disaster recovery environment would actually run their production environment if they needed to, if they had a true disaster and needed to fail over. Um, but they also want to be able to do that testing and not impact the RPO requirements that they've agreed upon with their business stakeholders. So, uh, this is gonna be the environment that we're gonna be working within, uh, during the demo, uh, and so over on starting over on the left hand side you can see I have a Microsoft SQL FCI cluster. Uh, that cluster is spread across two availability zones, AZ1 and AZ2, uh, and in this example we've deployed an FSX for NetAO tap multi AZ file system. So our primary file server is sitting in AZ1 and our standby is sitting in AZ2. Uh, and that file system is presenting ISCcuzzy LUNS, so block-based, uh, LUNs to our, uh, Microsoft, uh, SQL FCI cluster. We're then taking advantage of the block-based replication within FXRONTA and replicating those ones so that, uh, production database uh storage in our VPC on the left. We do a second VPC and second file system which is a single availability zone FSX ran up on tap. So we deployed a single AZ over in the DR just to simply save on costs and be more cost effective. Uh, and then we have a standalone Microsoft SQL, uh, database server over in the disaster recovery environment, uh, that we could bring up, and again that's, uh, being ran as a standalone, uh, just to save on cost. Now in this example this is all running in a single region. Uh, I'm showing this in the US East one and we're just going from one VPC to another VPC uh but with FSX RTA's replication capabilities this easily could be going from one region to another region. It could be going from one account to another account. Uh, it could be going from. Uh, one region and account to another region and another account, uh, we have a vast, uh, capability of flex or a lot of flexibility within that block-based replication within the FSXONTA file system, uh, to be able to replicate to basically wherever we'd like to as long as we have the networking in place. So real quick, the, the high level steps that we're gonna go through in this demo, uh, we'll review that production SQL database configuration. We'll, uh, create a snapshot on our FSX, uh, for on tap production file system. We'll replicate that over to our DR environment so that we have a point in time of our production environment that we can work within in the DR environment and then we're gonna create a clone from that snapshot and present that clone to our DR SQL database and that allows us to bring our DR database up in a read-write state, uh, and be able to uh do our DR testing and validation against it. All right, so You can see here, this is our production sequel database. You can see we have our uh STG 337 database here. If we take a look at the properties of that database. We can see that the data files are gonna be residing here on our S drive and our LDF or log files are sitting on the L drive. Uh, if we go into Sequel Management studio, or I'm sorry, if we go into the computer management, take a look at that, uh, S drive, and we'll take a look at the properties of this disk. We'll see that that indeed is coming from a NetApp iSuzzi based Ln. So this is coming from FX firm NetApp ONTA as an iSuzzi Ln. Our LDFs or log drives are sitting on disc two. If we look at the properties of that disc, it too is a net up I scuzzy baseline coming from FX for on tap. Now if we go into Sequel Management studio, we'll run a quick query, uh, selecting the top 10 rows, uh, from our customer's table here. And we can see that we have my uh extremely original database here of customer ID 1 through 10 with their first name, last name, and email addresses. Now this query here is just gonna be uh for us to compare when we bring up the disaster recovery database. Uh, it gives us a quick validation, right, that uh our DR database is looking, uh, similar to our production database and we can feel comfortable that yes indeed we do have a, a working copy of production over in DR when we bring that DR environment online. OK, so here on our uh production FSX for ONTA file system we're gonna create a uh snapshot. And it'll be this snapshot that we will work from, uh, and to create a point in time reference of production over in our disaster recovery. So we just created that snapshot called the DR_test. We'll go over to our disaster recovery FSX R tap file system and using SnapMirror, the block-based replication, we'll pull that across and we can see here that the, uh, SnapMirror relationship is now idle and it's still healthy. It's showing healthy is true. That indicates that we have successfully pulled across that uh snapshot, that reference copy of production to our disaster recovery environment. Now I did this all within the CLI to show you step by step kind of what happens uh if you feel more comfortable working within a GUI or you'd like to make this a little more, uh, easier to work with and automate, uh, you can use NetApp's Snap Center tool against FSX for OnTap. You don't have to do this all from the CLI. I just wanted to show you what steps are actually happening. If I did this in Snap Center, uh, you wouldn't see a lot of the behind the scenes of what's actually occurring. It would just do these things for you. So now that we have that snapshot, uh, and we pulled it across, we can create a clone from that snapshot and you can see here our parent snapshot name is DR_test that's that DR_test snapshot I created over in production. And now that we have that snapshot, uh, pulled across, we've got our clone, our read write copy created. We can see now that we have these two new LUs over in our disaster recovery environment called SQL Data and SQL logs and uh really quickly I'm just gonna map those LUs, uh, to our disaster recovery standalone, uh, SQL database server. And again this is all just done in the CLI to show you step by step what's happening. You could use Snap Center to do this from a GUI if you'd feel more comfortable. So here in our disaster recovery database, you can see I have no databases currently. And if we go into disc management, I just have the boot disc. I have no other discs, but if I rescan, uh, for new discs, those two i scuzzy luns that I just presented from the cologne volume show up. And if I online those discs, I now have my SQL data run, and if I online the second disc, I'll have my LDS or uh SQL log files. So now if we go back to uh Sequel management studio here. We can attach uh that database that is from the snapshot of production again that we've pulled across over to our DR. We'll point this to the MDFs, to our data files. And then, uh, it still thinks that we're on our production environment because this is a snapshot of production so we have to point it to the right, uh, drive letter for our LDS or log files so we'll go ahead and do that. And as soon as we do that, our production database is now mounted and available in our disaster recovery, uh, to work from. If we do that same quick query, uh, of our top 10 customers, uh, we will see that it is identical to production. So we took that reference point in time snapshot of production, pulled it across to the DR, and we were able to open up our DR database. Now, in a true disaster recovery scenario, uh, you wouldn't need to do all those steps, right? This is just because we wanna do testing and validation of DR, um. And not impact our existing RPOs, uh, in a true disaster scenario, you would simply break the snap mirror relationship that would bring the DR up in a read-write state and you could then mount the production database. Um, To show that this is indeed a read-write database, a read-write copy that we could work from. Uh, I'm gonna insert myself as another customer into this table, and you can see this was successfully executed. Uh, I can indeed do reads and writes from this DR database. So, uh, at this point we could bring up our application servers over in the DR environment, do all the kind of testing validation we need to to make sure that, you know, our disaster recovery environment is working as we expect and that we could truly, uh, survive a disaster and fail over to this environment and run production from this environment, um. If we did have to do that, if we had a true disaster, uh, we could always run from the DR environments for a short period of time. Once production's back online, we can replicate those changes back to production and bring every, you know, fail the application servers over to production when, uh, a time is best for us, uh, and then bring everything back to the way it was previously. Now the reason we took that snapshot, the DR_ test snapshot, is because if you recall our objective was to not impact our existing RPOs. The reason that we did that is because in the background SnapMirror, which is that block-based replication of FSXONTA, is continuing to run on the schedule it always has been. So it's continuing to run in the background on that schedule, uh, based upon our RPO requirements that we agreed upon with our business stakeholders. So if we happen to have a, a disaster while we're doing this disaster testing, we truly could just break that snap mirror and we would still meet our RPO requirements that we had agreed upon previously with our stakeholders. Alright. Switching gears, so that was scenario one. Another start we often see our customers taking advantage of FSX when they're running their self-managed databases is, uh, Jim talked about this earlier. Uh, they wanna create a like for like environment for their development and test environment from production, but they don't wanna duplicate storage capacity, right? They wanna be cost, uh, sensitive and cost efficient, uh, in that process of creating dev and test, making it look just like production. And we can do that through FSX, uh, file systems. So if we take a look here. In this example, we're using an Oracle production server on the left. Uh, we've deployed an FSX for Open ZFS single availability zone, highly available file system, so we have our primary and our standby file servers, uh, and following OFA best practices, Oracle flexible architecture, we have our Oracle data, Oracle logs, and Oracle binary, uh, volumes that we're gonna present to our production, uh, Oracle server. So production Oracle is running via NFS version 244.2, sorry, uh, to our FSX for Open ZFS file system. And what we're gonna do in this example is we're gonna stand up a development Oracle server and we're gonna clone those uh production database volumes uh and I'll be able to show you that we're gonna consume essentially no additional capacity on the storage on the file system uh outside of the metadata required to create these clones. So at a from a high level, uh, perspective, the steps that we're gonna take, uh, we're gonna review that Oracle production database config real quick and the FSX Open ZFS file system. We'll create an FSX Open ZFS snapshot, uh, of our production database, so a reference point that we can take at that point in time of production database, uh, and then we're gonna create clones, uh, from that, those snapshots that we can present to our development, uh, database environment. We'll mount those clones on the development environments and open up the Oracle database so that we could start spinning up application servers and doing dev and test against that database. So there are two scripts that are gonna be ran uh in this demo or this example. So I want to call out, you know, what those scripts are actually doing behind the scenes, right? There, there's two, primary calls that are being done in the first script. That first script is creating those snapshots for us of production to work from, and the first thing that it's doing is doing an, uh, AWS FSX describe volumes API call. So we're issuing this API call against the file system. We're gonna describe the volumes on the file system and that allows us to iterate through each of those volumes and if you recall those are or data or binary or logs and then run the AWS FSX create snapshot API call to create a snapshot of each of those volumes. So that's what that first script is doing. The second script is gonna use the A AWS FSX described snapshots API call and all that's doing is, uh, pulling those scraps snapshots that we just created. Iterating through those and for each snapshot we're gonna run the AWS FSX create volume API call. Now I called out here one of the important flags for that create volume API call. And what we're doing is we're passing in the snapshot ARN, so the, the ARN or the snapshots that we just created a production, and then we're using that copy strategy of clone, and what that tells the file system is, uh, I wanna duplicate my production environment but only using clones. I don't want a full copy. Full copy is our other option. Uh, I don't wanna duplicate the database. I just want a clone of the database to work from so that I'm minimizing my, uh, storage capacity requirements. OK, so here on our uh Oracle production database server. Uh, we're going to connect into the database and we're going to take a look at where our data files currently are residing. Uh, so we can see here data files are sitting on U 02. And if we go ahead and uh take a look at that mount point we see that U02 is uh FSX file system ending in C9DB. Now, uh, I'm gonna use another AWS, uh, FSX API call for described file systems, and we'll see that that C9 DB file system is indeed an FSX for open ZFS file system, uh, as I showed earlier in the architecture. Now we'll use another uh FSX API call and that's the described volumes, and that's just to show you the volumes that reside on this FSX or Open ZFS file system. So you can see here we have our or data ora logs and or binaries down at the bottom and on the far left here you'll notice or data and or binaries are using data compression. Uh, we're using the LZ4 compression algorithm that we have available in FSX or Open ZFS, and that's simply to shrink the, uh, size of the database on disk and again be more cost efficient, uh, with our storage capacity. So We will now uh go ahead and connect back into the database and we're going to take a look just really quickly at the database ID and the database name. And then I'm gonna run a cloud watch API call. And uh the cloud Watch API call is to get the current capacity of the of the file system and the reason for this is we're gonna reference this back after we created the snapshots and the clones so that we can see how much additional capacity that we're actually using on the file system uh by creating these snapshots and these clones of the production database. So since we're we're gonna go back to that, I'm opening up a new SSH session, uh, to our production database and we're gonna run that create snapshot script I referenced earlier. You can see here we put the database in backup mode. Database goes into backup mode, and then we iterate through each of the volumes, uh, taking a snapshot of each one of those. For time purposes this was obviously sped up, uh, but you can see here in real time it took about 54 seconds for us to put the database in backup mode, take a snapshot of the database, to give us that point in time reference to work from, uh, of the production database, and then take the database out of uh backup mode. So this is what Jim was referring to when he said, you know, back up in seconds and restore in minutes. So we'll now run the clone script. So this clone script is going to, uh, go through each of those snapshots, iterate over each of them, and as it, uh, finds those snapshots, it's gonna run that, uh, create clone API call, and we're gonna create a clone, uh, from those snapshots. And again, the clone is what gives us that read write capability. Snapshots are read only. They're just point in time references. The clones are what actually gives us read, write, uh, capability. Now again this was sped up for time purposes but you can see here it took us about 2 minutes, 2 minutes and 16 seconds to create a clone of those snapshots and uh down at the bottom you can see we have our 3 clone volumes now, uh, a clone for binaries, data, and logs of the database. So now that we have our read write clone database, uh, clone volumes, we can go over to the development database. If you take a look, you'll see we have no U zero mount points currently, and if we try to connect into SQL Plus, uh, into the database, it doesn't work because the binaries aren't there. They're not mounted. So we'll go ahead and we'll mount up those clone volumes that we just created after we took the snapshot from production. And again following OA best practices we'll have a U01 U02 U03 now. And you can see now we've mounted each of those clone volumes and now that we have the binaries mounted, SQL plus will actually work so we'll be able to run the SQL plus command and connect into the database. You'll see that we connected to an idle instance, and we'll go ahead and start the database up. Now upon startup, the database is gonna give us an error, and this is expected. So we'll see here in just a second as the database is coming up. That the database is complaining that it needs to either be taken out of backup mode or the media recovered. The reason that this is expected is if you recall. That script that created the snapshots, we put the database in backup mode, we then took a snapshot. And then we took the database out of backup mode once the snapshot was created. This clone that we're presenting to our development database hasn't had the database taken out of backup mode. It's at the point in time where the database was put in backup mode and then the snapshot was created. So like I mentioned, this is expected, not a problem. All we need to do is run the recover database command and uh Oracle will recover the database, take it out of backup mode. Uh, we then shut the database down. We'll see here the database will get dismounted. The database will get shut down. And we can now cleanly start the database up now that it's been taken out of backup mode. And we will see here that the database will successfully uh start up. It gets Oracle instant started. Database gets mounted and database is opened. So we now have a working environment for an Oracle database in our on our development, uh, database server, and we could start running uh development applications against this database that's a like for like copy of production at the point in time that we took that production snapshot. Now, uh, we'll go ahead and, uh, run the select name and DBID and this will just give us a way to compare the production. Uh, and validate that this is indeed production database, uh, but we're also gonna run that cloud watch, um, API call, and you can see here it ends in 475 megabytes, so 475 megabytes now. If we go back to that screen that I initially uh showed you earlier, we were at 434 megabytes in the last three digits. So with about 40 megabytes of storage capacity we have just duplicated our 18 gigabyte database uh on storage. So we can now have 2 copies of our 18 gigabyte database with only consuming an additional 40 megabytes. Those 40 megabytes are the metadata required essentially for those uh clones. So this shows you how efficient you can create duplicates of your production database from the storage perspective um and we can continue, we could keep creating more clones from that initial snapshot we could create new snaps, new clones, uh, but we could begin working against this environment with very little additional storage capacity. If we go back to our development database just to show that this is indeed a read write environment, uh, we'll connect back into the the database here. And Just create a new table and you can see the tables created successfully on our development environment showing that those clones are indeed uh read right uh and you know maybe in this development environment we're building some new portion of the application that requires a new table uh we could feel comfortable that after we've created this table we've done our development work done all of our testing in this dev environment that it will work in production because we're working from an exact. Copy of our production database. So, uh, as Jim mentioned some other scenarios, you maybe you would use this for testing database upgrades. I'm showing creating a table here, you know, you could do many different scenarios that we see customers using this snapshotting and cloning capability of your production environment. Alright, so that ends the demos, uh, section, uh, and some next steps for folks to, uh, get started, um, upper left hand corner we have our, uh, recent blog that was written about getting started with self-managed Oracle databases on FSX for Open ZFS. The lower left we have a uh best practices around Microsoft SQL deployments on FSX for Windows. The upper right, we have best practices for FSX for ONTA when running Microsoft SQL databases. And then in the lower right, um, we actually have a, uh, hands on workshop that you all have access to, uh, that you could go through many of the steps that I just showed, uh, hands on yourselves to get some experience doing these operations, uh, yourselves in your environment. You could also talk to your AWS account reps, uh, and have them, uh, potentially run this work workshop for you as well in an AWS account. So with that I just wanna say thank you everyone for your time thanks for attending, uh, and if you get a chance, if you could please fill out the survey, uh, we'd really appreciate it. Uh, Jim and I will be available after the session over there, uh, and happy to answer any questions or, uh, go through any scenarios you may have. Thank you.