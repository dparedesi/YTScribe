---
video_id: IQ9YWcgbAY8
video_url: https://www.youtube.com/watch?v=IQ9YWcgbAY8
is_generated: False
is_translatable: True
summary: Building cost-efficient multi-tenant knowledge bases for SaaS applications requires sophisticated architectural decisions addressing data segmentation, tiered performance requirements, hybrid search capabilities, and data lifecycle management while maintaining absolute tenant isolation and supporting diverse vector store backends through abstraction layers that prevent vendor lock-in and enable experimentation with different latency-cost trade-offs across customer segments. The fundamental challenge centers on enforcing data isolation as the non-negotiable core tenet where infrastructure must guarantee tenant data never spills between boundaries, not through application-level checks but through defensive architecture that makes cross-tenant access structurally impossible even if malicious prompts attempt injection attacks requesting data from other tenants, achieved through metadata-driven filtering at the vector store level that explicitly constrains queries to specific tenant and user identifiers embedded within document metadata during ingestion. Tiered performance architecture acknowledges that not all customers value the same dimensions, with free-tier users evaluating offerings potentially suited for high-latency pay-per-query S3 vector buckets that eliminate provisioned infrastructure costs, while established customers demanding sub-second response times justify provisioned OpenSearch or PGVector deployments, with the architecture supporting simultaneous operation of multiple vector store types serving different customer segments without requiring separate application codebases for each backend. The demonstration architecture implements Amazon Bedrock Knowledge Bases as the critical abstraction layer enabling single-codebase support for OpenSearch Serverless, Aurora PostgreSQL with PGVector, and S3 vector buckets, where infrastructure-as-code using CDK provisions knowledge base configurations pointing to different storage backends, all exposing identical query interfaces that accept metadata-based filtering expressions regardless of underlying vector store implementation, eliminating the need to understand OpenSearch index syntax versus PostgreSQL SQL queries versus S3 API calls, as Bedrock Knowledge Bases translates unified metadata queries into appropriate vector-store-specific operations. The ingestion pipeline demonstrates custom data source configuration rather than bulk S3 imports, enabling per-document control where users select specific files for indexing with configurable expiration dates, with the ingestion Lambda function constructing document structures containing content identifiers, S3 locations, and critically, metadata dictionaries embedding tenant ID, user ID, and file identifiers that become filterable dimensions during queries, while simultaneously recording ingestion events in DynamoDB with Time-To-Live attributes that trigger automated cleanup workflows when documents expire, ensuring vector stores contain only current, relevant information without manual intervention. Query execution constructs filter expressions combining tenant ID, user ID, and optionally specific file ID lists before invoking retrieve-and-generate operations, where the filter expression constrains semantic search to explicitly approved document subsets, with retrieval parameters configuring embedding models, result counts, and hybrid search behavior, all abstracted through Bedrock Knowledge Bases APIs that handle vector store communication, result aggregation, and response formatting without requiring application code awareness of whether results originated from OpenSearch, PostgreSQL, or S3, enabling transparent performance optimization through vector store migration without application refactoring. Data lifecycle management leverages DynamoDB TTL triggering automatic document deletion workflows, where expired records generate events invoking Lambda functions that extract knowledge base identifiers, data source identifiers, and file IDs to invoke deletion operations through Bedrock Knowledge Bases APIs, which remove document vectors from underlying stores while preserving S3 source files for potential future re-ingestion, allowing per-tenant or per-document retention policies ranging from minutes for rapidly churning real estate listings to months for stable reference documentation, configured at ingestion time without global policy constraints. The architecture addresses real-world challenges including supporting ten-plus document knowledge bases experiencing API timeouts when listing indexed documents, suggesting metadata-driven pre-filtering combined with DynamoDB-tracked file inventories rather than querying vector stores directly for UI population, and managing quota limitations on knowledge base counts versus data source counts by implementing single knowledge bases serving multiple tenants through metadata segmentation rather than per-tenant knowledge base proliferation that exhausts account limits. Vector store selection trade-offs balance OpenSearch Serverless providing low-latency hybrid search with structured and unstructured data integration requiring provisioned costs, PGVector offering similar latency with relational database co-location benefits for hybrid queries joining structured tables with vector similarity searches, and S3 vector buckets delivering serverless pay-per-query economics accepting higher per-query latency suitable for evaluation tiers or batch processing scenarios, with embedding model and chunking strategy choices significantly impacting result quality independent of vector store selection, requiring empirical testing with representative data using AWS-published performance testing samples before production deployment. The bidirectional AI-data relationship manifests as "data for AI" where structured system data and unstructured document repositories feed large language models generating organizational insights, and "AI for data" where artificial intelligence automates metadata tagging, lineage documentation, profiling, and cataloging operations centralized through Bedrock Knowledge Bases rather than distributed across fifty disparate systems, with improved data quality indexes ensuring machine learning models receive richer training inputs by channeling all data through unified curation pipelines before vector embedding. Customer adoption patterns reveal mixed readiness where approximately fifty percent qualify as power users capable of independent operation after initial knowledge base configuration, while the remaining half require ongoing data literacy programs and guided support to reach confidence levels enabling self-service analytics, suggesting successful implementations pair technical infrastructure with organizational change management ensuring users understand data product organization, access patterns, security boundaries, and query construction techniques. The solution's extensibility allows experimentation with diverse vector stores beyond demonstrated examples including Pinecone, MongoDB Atlas, and Neptune Analytics vector storage, with knowledge base abstraction enabling backend swapping for performance optimization, regulatory compliance, or cost management without frontend disruption, while CDK-based infrastructure-as-code supports rapid environment provisioning for testing alternative configurations, ultimately empowering SaaS providers to deliver differentiated customer experiences matching performance requirements to pricing tiers while maintaining operational simplicity through unified development patterns across heterogeneous vector storage backends.
keywords: multi-tenant knowledge bases, Bedrock Knowledge Bases, vector stores, data isolation, SaaS architecture
---

You are in the SASS 412. We're going to talk about, uh, cost efficient multi-tenant knowledge bases. If this isn't the talk you wanna be, you're not gonna hurt our feelings. You're welcome to go and, and find one that's a better fit for you. But, uh, my name is Mike Gillespie. I've been with AWS, uh, a little over 9 years, and I really like these types of engagements where we're talking with our customers, learning about what problems they're facing. Uh, so, as a, as a code talk, this is going to be very interactive. Like this, these are very dull if you guys don't participate. So we're leaning on you depending on you. I will, uh, when the time comes I will go around and bring a mic so everyone can hear your questions. Uh, there'll be times when you stump us. Uh, that's fine, you know, we don't know everything, but that would never happen. That would never happen. Uh, it's happened to me. It won't happen to you. I, I, yeah, that, that, uh, that's what happened. Uh, Steven, do you wanna introduce yourself? I, I'm Steven Warwick, solutions architect at AWS, uh, for the last 5.5 years, um, but definitely a builder. I was an engineer before for God, 30 years or so building various things and all across different industries, so. Uh, enjoy very much the SAS-based architectures though and been working with customers on that ever since I started at AWS. So that's what we're gonna dive into today, and we both work with software companies, so we're, you know, familiar with the challenges that software companies face every day and, uh, the origin of this came from something Steven was working on. Uh, people came to him, asked him about how to build knowledge bases. He's like, well, happy to help you. And then it came into, well, we need to have, you know, different tenants and different, we need to section things out imagine that yeah section them out people look at the same stuff and get the same answers so. We'll start off by talking about what our knowledge base is. I'm, I'm sure with, uh, AI many of you are already familiar with that, but we wanna set a baseline of what we're talking about today and then some of the challenges that you face running reg type architectures with multi-tenancy, and we'll, we'll focus on a few of those. We have a demo which will then go right into the code and we'll show you, you know, where in the code that these decisions are being made, where the logic's happening, and. And that's where you come in. That's where you can ask questions, uh, see how we think through these things. And like I said at the beginning, I love these code talks and chalk talks because it gives you as a customer a window into how AWS thinks about these problems. You know, we see these every day working with our customers on AWS, and we have a certain point of view, and this will help you see that and, you know, be able to translate that into something that you see every day. So go ahead. I was gonna say there's not always one right answer either. Our answer is definitely one of the ways you can do it, but for sure I think we've had lots of comments from even the sample code is out in the world too. So, uh, if, if that's one thing we wanna get to is feedback from today, uh, for sure. What do you see out in the world and how are you using things? Yeah, and that, and that's part of what our point of view is, you know, this is how we see things, but you may approach things differently because you have different problems to solve. So jumping right into it, you know, what are some of the main design decisions that you have to face when you're building multi-tenant, uh, knowledge bases? One is data segmentation. You have tenant-specific search. You have one tenant that wanna upload documents or, uh, other resources into your application and then be able to have a chat against those or be able to search against those in a, a semantic way. But of course, the one thing you can't do, you have one job, and that is to not let the data spill over from one tenant to another. So what are the, the mechanisms that you can use to enforce that? We also have a hybrid search, so it's not just semantic search but also a keyword search. And then enabling that with AI. So how do you hook that up to a chatbot or be able to run AI type queries against that, uh, within the scope of that tenant? There, there's a little other paradigm that we're gonna talk about today too is how to do that easily across different types of vector stores and things like that. We wanna make sure that. A customer A might want super, super low latency. Customer B, not so much. They just wanna pay less and they wanna have bulk stuff. But how do we do that in just an easy way across the board so that we don't have to do a lot of work to set that up that way you don't have to write a whole new application for each way you access your data. So I've talked about this already a little bit, but data isolation, um, you know, that is like the core tenet of multi-tenant application is how do you enforce data isolation and you know the philosophy I have is don't leave that to chance. You have to build the infrastructure around it to enforce that that happens. It was Steven called out tiered performance. Not all of your customers are the same. You may have a free tier where you're just trying to get customers a sense for what your application can do, but you wanna minimize the costs or really use a pay for what you use model versus established customers that really value. The latency, you know, working with software companies that build chatbots, the number one complaint customers get is latency. I, I type in my question, and they have to wait 30 seconds for an answer. That's not acceptable. So how do you make your application be able to balance both the cost effectiveness but also meet the latency requirements of your customers. And because uh the vector stores that power the knowledge bases are some of your pricier data stores, how do you make sure that you have the right data life cycle management? And there's a couple of reasons for this. One is certainly for cost, but also. Unstructured data tends to age, uh, very quickly. uh you may have an updated document that now reflects a new version of the state of reality. How do you age those out in a way that makes sense for uh for that particular tenant? I miss anything? Yeah, this is just different industries you'll see this a lot. So like the real estate industry, they might want to have like segments of their market set up, but that real estate cycle is changing, uh, you know, quickly, or patient records if they're throwing it in a knowledge base, it's like the doctor might see someone every 6 months and they might wanna throw that in and then take it out maybe for regulatory things as well. They wanna make sure it's only in there for a certain amount of time, but also allows them to shrink the amount of usage that they have to use and only do it when they need to do it. So looking at tiered performance, the underlying vector store that powers your database makes a big difference, uh, comparing three of these options, and there are many more, uh, but looking at open search is a very popular vector store for the customers that I work with. It's very low latency, uh, it allows you to do hybrid search, uh, and you can have the, the provision cost to meet your demand. Compare that to PG vector, which has the advantage of it's also low latency, but it's also integrated with your structured data. So if you have hybrid queries that query against structured data and your your unstructured data, you can do that in one spot. And then you know last year at Reinvent we announced the S3 uh vector bucket which allows you to run vector queries against data stored in S3. Really cool because you only pay for what you use. You pay per query and for the data that's being stored. You don't have that provisioned cost that you would in open search. However, that does come at the cost of higher latency per query. So you know, the, the men the case that I mentioned before where you have a free tier, you may want to have that on the vector bucket so that you're only paying for that evaluation use case that your customers are just kicking the tires. Whereas your more established customers may, you may want to have latency. But as Steven mentioned, we don't wanna write 3 different applications more or less to interface with our vectors store. How do we abstract that out so that we get the simplicity of. One way to access our data, but the flexibility to drive the back end how we want it. Another challenge with the life cycle management, you know, how do we define rules for data retention. How do we ensure that you're running against the most up to date and accurate information? I know working at Amazon we have an, you know, internal wiki which is usually out of date, you know, shortly after you published to it, right? Yeah, so, so how do we make sure that the data that's in there is actually up to date and age out the data appropriately. And the, the tenant tiering, uh, that we talked about is, you know, how do you have different rules based on the tenant, you know, as Steven mentioned, you may have a real estate with a very high churn rate of, uh, if the data in your documents in your vector store versus ones that may have, you know, much longer and be able to configure that per, uh, per each tenant. Yeah, it could be file sizes, it could be file types, that kind of stuff as well. So given that challenge, uh, so the architecture that that we're demonstrating today and again as Steven said this is not the only way to do it, but you know we found this to be, uh, a very effective way to abstract out. What that vector store is so that we can simplify our code but also give that flexibility on the back end. So walking through it, uh, you have the user interface. It's secured with Cognito and that that's important because that Cognito stores which tenant that user belongs to. So that will be part of the jot token that gets sent through so that the back end can then make decisions based on which tenant they belong to that gets sent to the lambda function which uses Bedrock to do the inference on the LLM. And then uses the bedrock knowledge bases to interface with the, the vector store on the back end whether it be in uh OpenSearch S3 etc. Yeah, I, I think that's like one of the key things we're gonna show is like how we're using the custom stores and, and the custom data types and stuff so that. You can do this in other ways where you're storing your files in different areas of S3 or something like that and you can use metadata files and things to to pull it in, but it's an ingestion pattern that is kind of like a fire and do a batch pull in. So we're gonna dive into a little bit today of how you can do that in a little bit more bespoke manner, and we'll go through a little bit of the code of what that looks like in the main key points. We only have so much time, so we're not gonna go through all the code that's there, uh, but we wanna make sure that you understand. Hey, these are the points you need to make or to, to build into the system to derive, uh, like individual files that you can pull into these stores for tenants, for users, whatever way you wanna break it down as well, right, so I'll switch over. Right, and remember this is interactive. If anybody's got questions and things, feel free, uh, see if it's coming up. So we're gonna do a little bit of demo just to kind of show you and see what is possible again. This is just kind of a tech demo to show you what's out there. So, uh, the first screen that you're seeing here is 10 and 8. So I've logged in and I've got a bunch of files in here that I've added. I'm trying to pre-populate this because we all know that the demo gods don't love us very much and we wanna make sure that it runs. So what you'll see here is I've got a, a, a few files in here for 10ant A, which is an S3 vector store. Um, there's, you'll notice there's a few files that have been timed out and then some other files have an expiration date. So those are the ones that I've gone in and said this, uh, this file particular to me, I wanna go and add to the knowledge base. So it's gonna say you've got so much capacity again. When you're building these systems it's probably baked in. You won't be showing these screens to the users and things like that. Uh, of course it's gonna demo gods are gonna hit me in the face for this one. But so imagine that gets ingested. So now I'm tenant A. I am a user in the system. I've got my indexed files in there and then I can go and chat with that those particular. files. So I've already done a chat before because of course again the demo gods, um, I'm only seeing and interacting with the files that I have ingested and this could be tenant per tenant based or per user base so I've ingested these S3 files. Uh, and I asked some questions on it. Now I'm gonna switch over quickly to tenant C just to kind of show you like tenancies on open search, they've got a lot more files they're allowed to ingest, a lot more file usage. They can do a higher query rate, things like that, uh, and you'll see that I've indexed again and but they've got a way longer expiration period that they can have as well, so. Um, while they're going through they can do another chat and they're gonna be chatting again, uh, against their own files but per tenant per user in this case, um. So when I go into a chat in here you'll see that I've got my index files and I can go and ask it a question like what programming language. This is, let's see if we can actually get if I can spell using I can't spell at all. It's like it's like horseshoes, right? just because. Let's see now I think my network is failing like crazy in here, so we're gonna pretend that that came back. That's why I had the other chat pre-populated, um, so what we wanna do though is just give you an overall view of like what's possible, right? So if you break it down you can break these files down, but we wanna get into some of what the code looks like and how we make this happen. So I'm gonna switch over to our friendly neighborhood kiro system over here, um, zoom in on that, we're gonna friends in the back for the people with the eyesight that we were just talking about up here. We're getting to that, uh, um, what, what, what prescription? I think I need a new one, OK, so in here this is my main stack, and what I'm doing is I'm creating a couple of the knowledge bases here and you'll see that I've got ones where I might wanna set it up or I might not. But I wanna show you this because we're pausing one second when you say main stack, what do you mean by that? That's right, this is the CDK application that is going to build out the infrastructure inside of AWS for us, and this part of the stack is the primary stack that runs first and is building out. I'm calling out to a bunch of other things to, uh, build out the knowledge bases and do the connectivity between everything. So I want to call this one out because we're gonna jump into this knowledge base, uh, class as well in a, in a second here, but you'll notice that there's a couple of them. There is the open search one and that there's a post gress, uh, knowledge base one. Now I noticed, note that we said S3 vector as well that, uh, the CDK constructs and and cloud formation constructs for S3 vector are still, uh, in the mix, I should say, or like kind of being turned on. So that is built off in a different stack because I have to use the APIs directly for those right now. So there's custom resources to build that out, but for the end result you're still going through a knowledge base for the querying which we'll get into in a second here. So I'm going to go over, so the key takeaway there is that you're building infrastructure to have the knowledge base and it's connected to the back end vector store. Yes, exactly, so this is that knowledge base stack that I was talking about and I'm gonna zoom in here. So we've got a bunch of stuff at the, the top that's setting up a few things is sanitizing names, things like that, um, but one of the key things we wanna do first is set up the knowledge base and I'm just going to copy some things in here to save time of me typing because nobody wants to. See me just type away so we've got our knowledge base we've got the you know we're setting up the name and things like that um now the next most important thing is the knowledge base configuration so I'm gonna copy that in here um and you'll see what I'm, I'm setting up is that the vector type you can set up these, uh, embedding models and things like that whatever you want across this is gonna you could pass these in as variables again for the particular knowledge base that you're using. I've set them kind of across the board for the knowledge bases that I'm gonna be setting up, um, and then we want to do the storage configuration so in this instance this is where uh I'm passing in I'm creating these this knowledge based storage configuration and I'm passing in my open search for my Aurora stuff so if we jump into there you'll see that for RDS I'm setting it up a a specific way. So I'm saying, you know, I've got my embeddings and chunkings for RDS, um, and if it's Aurora, um, then we're gonna be setting up a different way and then, uh, for open search we have to use we're, I'm setting up with open search server list and again I'm going into each individual one and I'm setting up the individual vector stores but once we get to the actual querying and things like that. I don't have to worry about that. I can just say this is the one that I'm going to, and it's also each one is set up, and I can just point it to that, and it can go. So that was a lot to cover, yeah, um, really wanna open up to questions or comments or like if was there points like why did you make that decision or what, uh. Uh, you know, why did we do that? And if no one asks questions, then I'm gonna ask questions, and I'm probably not gonna ask the same question you are. So, uh, any questions, raise your hand, and I may have to step to the side to see. I, and while, while he's going to find it, there's, uh, the next thing that we need to set up. We've got the knowledge-based configuration, but we do need the data source, right? We need to tell it where the data is coming from. So I am going to set up a data source in my code. One question is related to the different vector stores, stores, what is the pros and cons for each one? Sorry, I can't see. I, uh, did you hear the question? I did not. The question is, what are the different pros and cons for the, the vector stores that you had set up? Uh, so the, the pros and cons, um, I think Mike went over a little bit there, right, but it's, it's typically what I've seen with customers is, is the price to latency ratios that they're seeing in there. Um, so if it's, uh, a, a customer like you, you know, you might have a, a free tier, right, where you really don't wanna spend a lot of money on that. That's just the. The kind of the the way to pull the customer in to see what you're doing so you might use an S3 vector store there because the the latency on every question might be a little um longer um depending this is bang for your buck you're gonna have to go in there a little bit with yourself depending on the file types you're using and things like that so um it's, it's kind of on you to do a little bit of research to figure that out but. Uh, we have seen, you know, also there's, if you're using Postgrass already and you're using OpenSearch, you might just wanna go those routes, um, to, to add on to that, I mean, that's typically what I've seen is like we are familiar with Postress, we know how to run it, we know how to operate it, and we need a low latency vector store. It's a good fit. Um, for versus open search, you know, we have many customers that run open search for other purposes. It's just a natural fit to put their vector in in postgrass, but, you know, there's many others, uh, you know, Pinecone would be an example, um. So there's probably a whole talk just on, you know, pick your vector store, but, uh, you know, the really the point here is one size does not fit all. Like you'll, you'll likely wanna have multiple options with different dimensions and the kind of the extreme with latency being S3 versus open search or one other. Does, does that help? Or yeah, I'll be, I'll be right back unless you can yell and I'll repeat the question. I'll come back. You can jump back there. There you go. Um, I was curious, so right now we have a knowledge base with fairly small, uh, tabular data that's, uh, loaded in S3 and use an open search for that. I was wondering if for small tables, it will be worth it moving to the, um, Posgross vector since it's tabular data, um, and will there be any, um, accuracy implications for the agent that uses that knowledge base. Uh, that's gonna be a tough question for me to answer. There's, um, if you look, there are some samples out in the AWS samples GitHub repo that kind of on the, they let you run your own kind of performance testing on this kind of stuff. What we found is it really depends on the type of data that you have and the size of data and to, to your point of like how, how many columns there might be in there, different types of, uh, things that has to chunk out, and there's quite a vast difference between. The embeddings that you're using, the chunking size that you're using, so it's really difficult without looking at your data and doing some performance testing first to really figure that out, um, that's why we kinda usually go back to the customer and say like let's do a couple demo runs of this and use some of the samples that we have out there on your data to figure that out and I'll, I'll stress that the embedding and the chunking have a much bigger impact on the quality of the search than. Necessarily what the the engine running the math is. Thank you. Alright, well, he's jumping to another question I'm gonna add in the data source here because I think this is one of the key things, um, when you're creating the data source for the knowledge base, um, we're setting the data source configuration to be custom and this is one of the key points in here is this allows us to actually say. Instead of going to the data like to have an S3 and just pulling everything in when the files files are there, we wanna do this very bespoke and we wanna do it custom so that's we're setting it up to be exactly like that, um, and you'll see once we get into the ingest part so I'm gonna just copy the rest of this in here just so you see what it looks like. So I've set up my data source now. I've got the custom. Um, and we're going through stages here, right? Set up the data source, set up the knowledge base, we're gonna get into the ingest next and the query, and then what's what it looks like when we time out data and cleaning that up, um, but sorry, Mike was. Did you still have a question on that, yeah. If we need to inject. The caching layer on the previous architecture, how it can fit. Sorry, I didn't quite hear that. How would, how would we put a caching layer on top of that previous layer on top of the vector store? That's a very good question. Not one that I've thought of. I mean, you, if it depends on if you're using like Cloudfront as a fronting mechanism or something like that, you could try to do some caching of the results in there. Um, mm, I'm sure you could probably use Elastic cache and some other tooling in, in there if you wanted, but I don't have a bespoke answer for that on, uh, from here, and that the caching wasn't part of this system per se. Oh, other question, one more, then we'll move on. Uh, what are some advantages you've seen to adding the knowledge base layer versus, uh, going directly to individual, yeah, actually, so then we'll, we'll cover that stuff now we're gonna kind of go into that and kind of show why we might wanna do this. So, um, ingest and querying are part of those things. So I'm gonna go to the ingest section here, so. This is a Python file that's gonna be doing my um ingestion of my documents. So this is when I clicked a couple of those files and I said, hey, run the ingestion for just these files that they're sitting in S3 so my users uploaded them. I don't want them to be in my uh vector store at all times. I just wanna do it at particular times. So you'll see in here that I'm running through the files and I'm gonna set up a document structure and this is one of the key things that we're gonna go through here is we're gonna set up a content type and. Oh yeah, hey, thanks from the audience. Uh, so we've got a document and we've got a content and again the data source type is custom and we're giving it a file identifier and the location of the file and we're saying it's S3 so that's we're setting up saying we know where the file is, but next we're gonna add in some extra stuff which is, uh, arguably the most important thing is the metadata for these files that we're passing in. So if you're doing this in a bespoke manner you're gonna have to do this for open search differently for S3 vector different postcode, whichever, you know, name your vector store you're gonna have to figure out how it's doing these filtering mechanisms and do it for or do it through those, um, so like. If you're using open search, you're also on the query side you're gonna have to figure out how do I do that query or if it's PG vector I have to write the SQL query right to do this with the filtering in place. So I'm gonna add this in here so we all know exactly what I'm talking about. Uh, so I'm adding in metadata and again you could do this with S3 if you wanna do bulk congestion. There is the ability to put metadata files in there, but what this is allowing me to do is to check those files and individually pull them in. And then also what I'm doing here is setting the metadata, the user ID, and specifically the tenant ID so that when I'm doing the querying I can come back and do the exact same thing in reverse I guess is set the, the tenant and say hey I want a query for that tenant. Um, there's one thing that I am not showing in here is I'm also adding these, uh, like I add them into the knowledge base, uh, into the vector stores, but I'm also adding a record into a Dynamo database, and in that database I'm setting a time to Live, a TTL, which is, I think, arguably one of the most underrated things that people just don't use for some reason. Um, it's awesome. You can do TTL or or events and, and triggers from other systems in AWS, but the per record way of doing it in Dynamo, and it's, I arguably say very cost effective to do this, um, that's what lets me set up that expiry date in the files that I showed. The demo, right, there's a, you know, it's an 8 minute window or 8 day window or a day window, 24 hours, whatever it is. So I've now gone through the ingest. I've set up the metadata to be able to know that I can filter on those, and these, this metadata is in the vector stores tied to the vector stores. This isn't in Dynamo. Um, a lot of this, right, add some color to that. I mean, the key is that regardless of what vector store you're using, it's using this structure, right? Yes, that way you don't have to know that it's a specific index and open search. I don't have to duplicate this. This code is like one set of code, one set of queries that I have to do. Um, across the board for whatever vector stores that the, uh, Bedrock knowledge base supports, so to answer the question that came up, like why wouldn't you just talk directly to the vector stores? Well this gives you an abstraction layer that allows you to work at the metadata level that would then push that into the vector store regardless of, you know, how that's being stored. All right, so now we have a question over here. Oh yeah, I'm keeping an eye on time. We're, we're plenty good on time, so. I feel like I should be standing too, but then I'm typing really odd and uh, my question would be like, um, how would the system react to nested metadata. So for example, let's say, um, we have a book, a book has like multiple authors, so and authors would have like names, and these names could be like, um. In Japan you would have like names in the Japanese characters in UK or US it'd be an English character. So how would you design your metadata for Bedrock or like you know for. More efficiently, uh, I'm not sure if that would, uh, so I think I understand your question of like Unicode characters or other type of characters if they're in there and or or not, um, in this form I'm not sure if that would really come into play because in this. This form I'm, I'm saying a tenant ID and I'm filtering by a tenant and a user, not specifically the content. The content gets ingested as, you know, whatever vector store is ingesting. So it's using that embedding model to break it apart. So I think it would, the, the question would be going back to that embedding model and making sure that it supports that, um, so in, in this case, in this, uh, schema, can you have a list be or an object type as part of the schema. Uh, I'm not sure. I'd have to figure out the back and answer. I'll have to research. I think that's how I would start looking for it, but, uh, I don't have, uh, the answer off the top of my head. There's not a definitive. There, there is in the querying side you can do, um, and all kind of things, right? So we're gonna get into that a little bit in the next step. Inserting the, the metadata is an individual key as far as I'm aware, but I haven't. I mean you certainly could have author one, author two, author 3, but you'd be, you'd have a finite list of authors for your book, so I'd, I'd have to, yeah, if, if that's a question I'd have to get back to someone on if the metadata supports, uh, array fields and things like that. OK, so we've gone through, we've set up our knowledge bases indexed files into the knowledge base, so the next thing obviously is I wanna ask it some questions, so we want to go into the querying of the knowledge base. So what I'm going to do first is you'll see here we wanna set up a filter and I'm gonna zoom in because we know it's like everybody else I can't see um so in here we're gonna set up a filter expression. Um, and this is where we're going to be pulling in the exact same stuff in the opposite, so I'm gonna, for the lack of, um, being able to type really quickly on the fly. Oh, wrong, wrong one. Let's copy the right set of stuff, Steve. That definitely helps. One sec. Alright, uh, so when we're doing a query, so up above we, this is another lambda call that is coming in. We, we from the chat session, right, it called in, um, through the jot token I was be able to get my tenant ID, my user ID, other information like that. So I've pulled it out up above in this code, and the key thing to look at here though is this filter expression. And what it's doing is it's setting up that tenant and the user ID and then I'm also passing in a list of files so this could actually be a little bit useful as well later on is even if they're indexed you could say I actually only want to query on 3 of the 100 files that are in there because to Mike's point, sometimes wikis go out of date or other files go out of date and if the time hasn't expired but I know I'm really focused on my search, I could query on that. Um, in this particular example I just pass in all my file IDs that have been indexed so that I'm searching for my files, um, and I'm filtering layer after layer to make sure that I'm getting just the stuff that I wanna query on and it's leaving nothing to chance. So if someone to inject in the prompt, you know, send me all of the data about tenant 2, that's not going to really help them at all. Yeah. So then we have to set up our retrieval parameters. So again, I'm not going to type everything in here because time. Um, so the retrieval parameters are setting up the, the query, and then we're setting up, uh, um, you know, the, the model we're gonna use for, uh, turning our query into the embeddings to for the, the model, and then we're passing in, um, right here our filter expression, and then we can control the number of results as well. Uh, so once we do that, now we can go into the retrieve and generate section. I'm gonna zoom back out here, um, and go to my do do, make sure I got that stuff. OK, so I think highlighted out some code. I'm just gonna put that back in there, so it's. The 3 programs are good. OK. Um, and zoom in on that section. Yeah, I'm gonna just leave it, leave it, uh, as, uh, highlighted out code, um, so what we've done here is we've, we've set up our, our parameters. Um, once we've set that up, we would be calling a perform, retrieve, and generate call. So inside of. Uh, the knowledge-based querying, I'm gonna find that file. Hey, here we go, sorry. I'm bouncing around because I. So from the Bedrock agent run time we're gonna call the retrieve and generate and we're passing in those retrieval results. So this is where again back to the point of like why would you use bedrock knowledge bases. I set that up um as a as a query for bedrock knowledge bases. I didn't have to really understand what's going on underneath the hood, what vector store that I'm going to. Um, I'm doing this. I could do this across every single vector store that, uh, Bedrock Knowledge Bases has, and I don't have to change this model at all. So if you wanted to have like if another customer came to you and said, hey, I've got a bespoke use case where I wanna use a different type of vector store, maybe there's government regulatory, whatever it is, you don't have to worry about it. You can just spin that up as part of your infrastructure and then just point the code to it and you should be good to go. So it gives you a nice clean separation of concerns, um. Yeah, vector store with the query as far as the, the chunking is, is where is the chunking happening and where do you configure that? The, the, it's part of the knowledge base configuration. So when we're setting up the knowledge base, we're setting up the, the, the data source, and we're gonna set up some of the chunking strategies inside of there. But also when we go back to the indexing, um, we've set up some of it in here. I sprung that on him, so yeah, um, we. When we, when we're setting it up though, we, we're setting up individual data sources and data, uh, vector source as, uh, a specific chunking strategy and things like that. And then in the query side we are doing the same thing with the query. So you're setting up the model that you're gonna use, the foundational model, and some of the options that you have there. It's not really the chunking there. You're just basically picking the model to be able to do the embeddings, but. Any other questions? There we go. After the question we go on to the last little bit is just the clean up part um I have a question um maybe it does it does it automatically but. If I, if a Tenant 2, for example, is chatting with uh their own uh index of documents or whatsoever, but and I update one of those documents. But how does the tenant and it knows that the document is updated and the embedding is regenerated again but I don't want and but I don't want to lose the context of the chat basically so the document is updated. Um, and the building should be regenerated. But the context, yeah, you, you, if I get your question right, it's like if I'm continuing on a chat and I wanna search more and more things, you just have to keep passing the context in of the previous stuff, so you're, you do start getting a longer and longer context window. That happens there and I, I don't have a, a good answer for you on seeing if we can slim that down at all. Um, I think you'll see like even in Quiro and some of the newer systems out there they start, they'll hit a limit and then they'll summarize for you and try to push that off to something else, um, so those are kind of the patterns that have come into play today these days, uh, until someone smarter than I comes up with a better pattern for that. OK Thanks. Any other questions? Alright, the last little bit of this is just the clean up. So we've gone through again the stages. We've ingested, we've queried now, um, as we mentioned, we've got a, a, a TTL system, right? So whenever the TTL triggers we've got a. Go back and make sure because we're a custom store there's nothing going on that specifically tells us to clean up all that information so we've actually got to go in and extract out the file information and everything like that from the TTL trigger so in the Dynamo database, um, basically there's a couple things in there it's the the file ID, the data source that it's in, the knowledge base that it's in, and um we're gonna use that information and we're going to extract it out and. Uh, delete those records from the knowledge base and again in this particular spot we don't really have to care what knowledge base it is we're just giving it a command and it figures out what that needs to be and goes off and deletes it, so I'm going to. Grab that chunk of code here. And it's a rather large chunk of code, so I'm not gonna type it in here. Um, sorry, one second, while you're doing that, I mean, when Steven showed this to me, I thought this is one of the more clever parts of the solution because the time to live is a really important problem and you can't just have a single rule for all of your, your documents. Like how do you manage that so. You know, keeping a a record of that index in Dynamo and then using the Dynamo DB stream to essentially trigger that action gives you a lot of flexibility because you can use the built-in time to live within Dynamo or you could delete the record manually if, if there's like a right to forget or if you're trying to expire a document like you said, you just delete the record in Dynamo and it will just flow through the whole system. Yeah, so, uh, I, I was, uh, a little off there. It's not a, not a super large chunk of code, but this is definitely, um, as I mentioned, all I. Have to keep track of in the Dynamo database is the knowledge bases coming from the data sources coming from, and the file ID that I wanna remove technically I could probably put some extra parameters in here if I wanna be very bespoke about like the tenant ID and things like that and make sure we're going through, but for deleting from a knowledge base really just needs the file ID from wherever the data source was. So I'm passing in that file ID and then it's going off and deleting it out of whatever knowledge base so it handles the creating the query and things like that. That said, it's just deleting out of the vector store. It's not deleting out of the storage underlying. So these are separated off systems, uh, that we're dealing with too. So I've still got files sitting there in S3 that I could re-ingest later if I really needed to, um, but I'm just not taking up all the, uh, the use of those particular knowledge stores or sorry, vector stores as well. So that kind of ties it up. We're, we've gone through all the stages. We cleaned up the data and now the users are hopefully happy because they've got a better utilization of their system. Uh, hopefully their costs are going down a little bit, um, but I would love to hear from everybody too, like what vector stores is everybody else using or could, did we kind of peg it with the post grass open search? Has anybody started using S3 vector yet? So one person there say yeah, has anybody found it useful and good compared to others? Yeah, OK, I, I, I think the when the uh cloud formation kind of catches up on that we're hoping that people will start really kicking in and using those things. Any other vector stores that you're using? Well, I had more of a question, basically, um, we use open search server list, and one of the major issues that we're basically having is that in the listing of the documents within the data source, if we use, um, open server list, what's happening is that the API that we use from our internal services to query the database that kind of times out if you have like even greater than like. 10 documents or something. So is there like a workaround we can use to kind of pre-filter, uh, the knowledge base so that, you know, we retrieve the results and, you know, less than 10 seconds or so? Uh, you can hold on to that. I have some follow-up questions for you. Um, what type of metadata do you have on the documents? So it's basically like file ID mainly, um, and then, you know, we have, uh, the agent ID in there to kind of know like this is the file that we need to query, get the results, and that, that, that works perfectly. The major issue that we're having is like when we have the user interface for the customer to see the files that they've ingested into the knowledge base. That's where it times out, uh, because, um, the pagination is like it happens very weirdly like it's super slow. So is there like a workaround that we can use to when we are, you know, listing the knowledge based documents, is there like a metadata that we can pass to kind of pre-filter? The documents. So that's maybe faster. So it sounds a little bit more like open search specific type of question. Exactly, um, I would love to get you set up with an open search specialist if you haven't talked to one so far, um, but see, it sounds like though, I mean if you went behind like a bedrock knowledge base and you use something like this, then you could maybe get the users to. Try to track down or or have like different sets of files that or or documents that they're looking at I think uh one thing that we've noticed is knowledge bases can get really big. And the stuff that individual people want to look at is like maybe 10% of the overall, so you could segment it down to by department or by something like that, right? So putting in some of that metadata so you could filter it down that way before you start to do the retrieve and generate might help you out on that. So a follow up on that. So let's say since it's a multi-tenant system. And I think there's like an upper limit or quota for like the number of knowledge bases that we can have in a single account versus the number of data sources. So how we kind of tackle that, I guess. So we run into a bottleneck eventually in terms of scaling, right? That's the, that's the main issue I guess. And then how Steven addressed that in his demo is he kept that record of files in Dynamo. So if you're just listing the files, it's not actually hitting the knowledge base or open search, just hitting Dynamo instead. Um, and then the tenant ID, you don't have a knowledge base for tenant. You do the segmentation using the metadata on the, the, uh, yeah, so maybe like I think I guess moving to a Dynamo DB based like file information or something that could help us I guess, yeah, if you, if you're just tracking pure file information stuff, I, I would say open source is maybe not like open source is really good at what it does, but it's you're if you're just querying out flat-based information. And Dynamo is probably gonna be a good spot to put it in and it alleviates some of the load on your open search as well, so you don't have to like increase the size of your cluster at all. All right, thank you so much. Good question. I'll be right over. Um, so in terms of, uh, vector DB, a year ago, uh, before starting using, uh, KB we were using Mongo DB at last. Um, found out that the LLM wasn't very good to do a text to a Mongo query, so we have a few issues. And so since now we're using OpenSearch and, uh, and and not, not S but Prid, but we still have a footprint on Mongo. And so I'm, but we've, we're doing like normal rack without KB anything. So I'm wondering if uh We could use KB with Mongo at some point. I, we just checked this too. I don't know if Mongo is on the list. I, I'd have to get back to you. Uh, I'm not sure if Mongo is on the bedrock knowledge base list of, uh, vector store or, or support. So but if it was, then it would be, yes, you could do that, and it would just generate the queries and everything for you. I told them they'd stump us, yeah, yeah, and it's funny. I was just looking over these things like I know someone's gonna ask about that. Uh, we've been interested in using, uh, Neptune Analytics, uh, just for general like knowledge graph, but also, um, some a little bit of vector storage because I think there's that feature within Neptune Analytics. Uh, how have you found the integration between knowledge bases and with other vector stores? Like, is there a little more customization that needs to be done for, uh, Neptune? Uh, I, I haven't seen too many, um, like I, I know like Neptune vectors is, is, are, is, I wouldn't say new, it's been around for a little bit, but yeah, new-ish, so I haven't come personally come across too many that are mixing those together, so, but it. I mean when you're setting up the data sources to the knowledge base through Bedrock it's pretty much the same configuration stuff that you would do for like whatever um vector store that you're gonna go to you have a lot of the knobs and dials that you're gonna tweak so I would, I would say if you're tweaking through without it then you're probably gonna be doing the same thing through the knowledge base. You're just saving the switching between the knowledge bases and having to rewrite queries and things like that, um. It'd be an interesting question to the knowledge based team to see how they're doing the performance enhancing right because they're gonna be doing the queries for you into the knowledge store, um, so it'd be very interesting to see like if you're if you're finding that you're having performance issues or anything like that I'd, I'd love to get like that team in front of it and see how they are doing those queries and like what data specifically you're using. So maybe Bump into me after this and we can figure that out. Any others? All good. Yeah, I think, uh, if there are no more questions, uh, we've solved everything we've solved every problem, so that's good to hear um, really appreciate you taking the time make sure to, uh, put the feedback in the app and, uh, I think we're probably between everyone and lunch as well, so letting out a couple of minutes early is not the end of the world. So thanks everyone for attending.