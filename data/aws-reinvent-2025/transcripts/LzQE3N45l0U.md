---
video_id: LzQE3N45l0U
video_url: https://www.youtube.com/watch?v=LzQE3N45l0U
is_generated: False
is_translatable: True
summary: "This hands-on code talk by Chris Azer (Principal GenAI Labs SA) and Raju Gota Mukalla (IoT Specialist SA) demonstrates building agentic workflows for industrial analytics and predictive maintenance, addressing IT/OT convergence challenges including slow digital transformations, data analysis skills gaps, costly downtimes, and aging workforce knowledge loss. The data foundation layer uses AWS IoT SiteWise, a purpose-built industrial data service handling the end-to-end value chain: collecting data from machines via protocols like Modbus, OPC-UA, and MQTT at edge; buffering and intelligently publishing to cloud; storing data securely; and exposing it for AI layers. SiteWise now includes native anomaly detection that can be enabled without ML expertise, outputting anomaly scores (0-1 scale, 0.5 threshold default) with explainability. The AI stack leverages Amazon Bedrock (Claude Sonnet 4, knowledge bases for vendor manuals), Agent Core (runtime, gateway, memory), and Strands SDK (open-source agentic framework). Tutorial 1 demonstrates a multi-agent repair plan workflow for an e-bike assembly line's welding robot. When SiteWise detects an anomaly, it sends an MQTT message to IoT Core, triggering the agent workflow. Three specialized agents built with Strands SDK work in sequence: (1) Ops Data Collector retrieves telemetry and anomaly data from SiteWise via MCP; (2) Knowledge Retriever fetches equipment manuals and SOPs from Bedrock Knowledge Base via MCP; (3) Report Generator creates an HTML repair plan with root cause analysis, error codes, action items, maintenance schedules, and OEM contacts, uploads to S3, and returns a pre-signed URL. The code walkthrough covers importing Strands components (Agent, tool decorator, MCP client, Bedrock model), creating MCP clients for three servers (SiteWise MCP for telemetry, Bedrock KB MCP for documentation, AWS API MCP for S3), filtering tools to only necessary ones (improving response time), defining system prompts in markdown files for each agent persona, creating custom tools using the @tool decorator, and orchestrating agents in a workflow pattern (sequential handover vs. graph/DAG or autonomous swarm patterns). Tutorial 2 addresses advanced analytics for a process engineer named Sarah investigating excessive anomalies. This uses Agent Core's Code Interpreter—a sandbox for generating and executing Python/JavaScript code on-the-fly—combined with Agent Core Memory for conversation persistence. The workflow loads historical sensor data (1000 rows, one month sampled every minute), creates a code interpreter tool, and configures an 'Industrial IoT Data Analyst' agent with memory hooks that trigger storage after each response. When prompted about welding robot anomalies, the agent autonomously imports pandas, numpy, and matplotlib, generates correlation analysis code, creates scatter plots and distribution visualizations, identifies that joint zero requires urgent attention with 27 extreme temperature anomalies reaching 87°C, detects data quality issues (67% missing data across joints), and produces actionable maintenance recommendations. The session emphasizes that Strands SDK is excellent for prototyping, while Agent Core handles production scaling with discoverable MCP servers. GitHub repository includes self-sufficient Jupyter notebooks that prepare the entire environment from scratch, including data simulation and knowledge base setup."
keywords: Industrial Analytics, Predictive Maintenance, IoT SiteWise, Strands SDK, MCP Servers, Multi-Agent Workflow, Code Interpreter, Agent Core Memory, Anomaly Detection, Welding Robot, Repair Plan, OT IT Convergence, Bedrock Knowledge Base, Process Engineer, Data Foundation
---

All right. You got everyone's attention. Hello everyone. Welcome to our, our session, our cold talk on agentic cogeneration for industrial analytics and predictive maintenance. So I don't know if you've, uh, attended our workshop. We actually had one as well. See if hands if anyone attended a workshop we did as well. Well, it's our fan fan club. It's good. So it's actually a little continuation from that so just special for you you'll you'll get to see what we do here so. All right, so my name, I'm Chris Azer. I'm a principal Gen AI Labs solutions architect. So I'm part of a, a team where I'm a tech lead helping to drive, uh, industry applications with Generative AI and just showing how we can improve and. You know, operations and other types of use cases, um, and so I've been with AWS 8 years and doing industrial automation for 20 years. So, um, happy to share this, this progress in, in where we're at today, right, where this next form of transformation and manufacturing really can accelerate and create agility and being able to drive operational improvements. And so I'm joined with Raju. Yeah, thanks, Chris. Hi everyone, uh, I'm Raju Gota Mukalla. And I'm an IoT specialist solutions architect, and my focus has been really, uh, working with industrial and manufacturing, uh, organizations, uh, in terms of, you know, helping out throughout their digital transformation journeys, right, starting from how do you unleash data from machines using, you know, hundreds of industrial protocols all the way to building, uh, uh, intelligent models, you know, that can respond, uh, when there are issues with those, uh, machines. Um, yeah, over to you, Chris. All right. So for our agenda we're gonna kind of just talk about some common challenges in this space. Why if you're in from the industrial world, you may be familiar with those already. Uh, we'll then jump into our AWS stack and just show you what we're gonna use today, right? Which AI services, which IOT services will be used, uh, and then gonna go through a couple of tutorial, you know, coding sessions. Um, again, this, this will be code that you'll have access at the end. So there'll be a QR code, you can, you can, uh, get access to our code on, on GitHub. Uh, so the first one will be around, you know, looking at anomalies and creating a repair plan around this, and then we'll look at it kind of going deeper, right, and using advanced analytics that can actually look at maybe excessive anomaly detections you do have and really trying to look at a bigger picture through analytics, um, of what could be a an issue or, um, relation between maybe multiple uh parameters. All right, so some common challenges, right? We, you know, when we look at on the left here, IRT and OT. I'm sure people heard this acronym, uh, we're, we're trying to merge these two different worlds together, operational technology and information, uh, and they just, they operate different. Um, they have different processes and different software, um, in different environments. So there's a lot of challenges in this space whether it's um you know lack of um response times, slow digital transformations because it takes a certain skill set to implement some of these latest transformations tech technology whether it's IOT or um you know, AI or digital twins. There's also this data analysis gap, a skills gap that would exist for being able to create these predictive models, and with generative AI today we've really gotten that ability to give that to the power of many people, right? Being able to do this kind of complex analysis on time series data and really try to find anomalies or patterns that may exist. Uh, so that kind of skills gap doesn't exist in both of these orgs, you know, readily available, right? So this can lead to costly downtimes, just accepting that there's a downtime that happens with this, this machine or this vendor's machine, and I have to do scheduled maintenance more often. There's also a manufacturing skills gap. There's an aging workforce, you know, there's people that are just not there 30 years anymore and that kind of loyalty, right? They're, they're leaving with that knowledge. So how do you enable that new workforce to be able to make quicker decisions and really minimize that response time in the meantime to respond to a failure. Other common technical challenges service complexity. I'm not saying our services are complex, but just in general there's a lot of services that may exist, a lot of different technologies, and being able to optimize the use of those services in, in a way that would be able to solve that problem quickly. So we don't have that, that learning curve to understand the services that I need to use, the SDKs I need to use, um, can really slow down that advancement and agility. Yeah. So, um, we talked about the challenges. Uh, let me, um, start with, uh, one of the common challenges I see with my customers, right? Is, uh, when you're in a manufacturing facility, you got to deal with different systems, different control systems, uh, OT systems, IT, uh, from different vendors, right? So, the data is siloed in most cases. So the, the real challenge is how do you unleash data from Uh, heterogeneous systems, and then how do you create those contexts? So you can then build, uh, analytics on top of it, right? Now, and then you need a reliable data foundation layer before you can actually go ahead and create agentic AI workflows. So, before we talk AI, uh, let's talk the data foundation that you need, uh, to build the AI layer. Uh, so when we talk about data foundation, uh, layer, you need a way to collect data from machines, uh, be it, you know, Mdbus, uh, OPCUA, MQTT, or any other industrial, uh, protocol. Uh, you need to collect the data, model the data, model the machine, store the data securely, uh, depending on, uh, your price latency, uh, trade-offs, right, for your use case. It could be a real-time, uh, operations dashboard type of use case, or it could be, uh, some post-factor analysis that you do, uh, every quarter. Um, and then once you store the data securely, then you need a way to egress, uh, the data from the foundation layer, right, to run your analytics. Now, this is where AWS IOT site-wise comes into play. Uh, it's really a purpose-built industrial data service for industrial and manufacturing use cases. Um, essentially, it helps you with the end to end value chain for industrial data, starting from collecting data from machines at edge to buffering data at edge, to intelligently publishing to cloud cost efficiently, and then storing data in the cloud and then making it available for the AI layer, right? So, one unified platform that gives you access to all the telemetry, uh, and the knowledge-based data. So we'll use this, uh, site-wise in our, uh, tutorials today, uh, and then show how you can build that foundation layer, uh, before we build AI agents. So with that, let's talk about AI, what we got to offer, and, uh, uh, on top of this data foundation layer. Over to you, Chris. Yeah, so before we get into any questions you may have, this is open, open, yeah, this is interactive, so you can ask questions, not like workshop. Any questions on uh there you go, yep, so I would say probably one of the biggest things that I noticed with inside the IOT space is like Kafka is like nonexistent for like a. Is that like inside the stack or do you guys see that like more and more that cop does is like taking over the MQTT space or That transport As do you see that you're saying that you're seeing that more than use Kafka more than I'm actually seeing less innovative use it's like not dipping into the IT space as much, OK, yeah, no, you see it differently like with your guys' lens in the industry. Yeah, uh, so it depends on the machine, right? So, and also the throughput. So NQTT is a good fit, you know, when you're dealing with, uh, resource constrained devices that transmit messages frequently, uh, but, uh, not too big, uh, in terms of the message size. That's a classic scenario for NQTT-based messages. So. Yeah, yes. Yeah, so, yeah, that's a good question, um, because, uh, in fact, we're working on, um, a few enhancements. Uh, it's still under design, um, but the idea here is, uh, Sitewise has all the contextualization done in it, right? Now, you need a way to extract the data. Today, we provide different ways to do it using, uh, SDK MCP. Uh, but then one common ask is, can we extract data in a data stream format, right, to Kafka. So today, you could do it through a, a middleware, uh, through IoT Core. Uh, Sitewise can emit MQTT messages to IoT Core, and IoT Core has a rules engine that can route messages to 20 other services including Kafka. So you can do it today, um, but we are working on some enhancements to make Sitewise to Kafka data egress much more efficient. But there is a way today, uh, for sure that's available for you. You don't see as many people doing it, uh, no, we do see, uh, customers, you know, asking for it. Uh, we build, uh, we work backwards from the customer because we had customers asking for it, you know, we are building, uh, some more enhancements. So the quick answer is, yeah, it's a very common pattern we see nowadays. Uh, how do I extract data, the contextual data to your, uh, streaming layer, um, and then we, we are providing some more tools for you to do that. It's a good question. Any other questions? We'll, we'll ask you again later, so. OK, so jumping into the AI stack. You know this is probably out of date just after, you know, yesterday, right, but we, we're just constantly developing in the stack and the way you look at it, it starts from the bottom where we have our infrastructure, right? So we talk about AI compute whether that's, you know, silicone like AWS ranium and other types of um inferentia and other types of GPUs. Um, we have AI, uh, Amazon Sagemaker for that fine tuning and that, you know, data scientist layer, right, in terms of that kind of work you would be doing. Um, we're not gonna be focusing on that, right? We're actually. In the middle layer, right? Um, and in the top layer. So in the middle is where you actually build and use services, configure and use services that runs on top of this infrastructure. So you're not necessarily saying I'm gonna use that chip set. I'm gonna, I wanna use this chip set, right? We actually abstract that for you and you just use the service or software. So there's um Amazon Bedrock, which is essentially this platform that handles many different capabilities, starting with your choice of models you want to access. And in this case, we're going to use cloud Sonnet 4. Um, that's one of the LLMs you can access through Bedrock. Uh, there's other capabilities like guard rails and, you know, other things you can do, which we won't use. We're gonna use the knowledge base to actually access vendor manuals and specifications that would be used by the agents later. And then there's agent core, right? That's where you have a bunch, a variety of models you can choose to pick and choose to use. You can decide to use runtime and run your, your agents there. Um, you can use a gateway to access tools. So there's a variety of these different modules. Um, and so what we're gonna do is actually have a strands agent on the top, top right. That's an open source framework we have to build agentic applications. And we're gonna be able to use memory from Agent Core so we can contain that memory in the conversation that someone like a process engineer may need to have access to. Um, on the top, we're gonna have an application layer using Quiro. Uh, anyone of you have heard of Quiro, I'm sure at this point, right? This is a, a developer environment we have and now recently announced an autonomous agent that can run off, off the IDE in the cloud. And so we're gonna see that combination of services used. Ultimately, we're gonna help you walk through building this application. Um, and then executing some of the code and, and walking through the code that we would use to perform these different tutorials. All right, so we'll start the first one. OK to you. You have to switch over. Uh, any questions before we move on to a live tutorial? And how many of you use strands SDK today in your day to day or weekly or monthly? Anyone, strands? Anyone new to strands or? Are some building agents, agents, so not many. Yeah, she's like we just wanna make sure so we know, you know, how much time we want to spend on strands because, yeah, this session is not about strands but more on, you know, how you use strands and agent core for a, a specific domain use case. Um, so yeah, I'll try to cover some basics, uh, yeah, when I do that. OK, so in the first tutorial, so we'll split, uh, we have two tutorials total. Uh, so probably we'll spend like 20 minutes on each tutorial. And yeah, if you have any questions, please do ask. So in the first tutorial, um, so we'll show you how you can create a workflow, agentic workflow that, uh, it's made up of 3 agents, and each with a specialty, we'll go into the details. But then, um, The workflow, the, uh, the, the goal here is, uh, to create an autonomous maintenance response or a repair plan when there is an issue going on with the machine, right? So the prerequisite for this tutorial, again, it's all in the GitHub report, uh, we'll share that in a moment. Uh, is to have the environment ready, right? So environment, when I say, uh, is site-wise with, with your assets onboarded to sitewise, data stored in site-wise, and, um, to those new to Site-wise, uh, we came up with a new, uh, anomaly detection module of Sitewise natively. So you can enable anomaly detection on a machine without ML expertise. Um, so we do use that as an input for this workflow. So the prerequisite is to prepare your site-wise environment with all your data. An anomaly detection enabled at least for one asset. So you have the anomaly result coming out of that asset, right? The anomaly score, uh, or some moral explainability, right? So that's the starting point. Now, when you have an anomaly, so we show how you can create a workflow that gets triggered, and then that workflow can then automatically create a repair plan, right? So you can, your operations team can then take the repair plan. And then go ahead with corrective actions, right? And we show how you can do that in an agentic fashion using 3 agents. We use SANs SDK, uh, to build these agents. And then, uh, when Chris talks about the second tutorial, we'll talk about agent core, right? Um. So let me put down the architecture for the first tutorial. Uh, so you see, pretty straightforward. On the left, we have site-wise edge collecting data from machines. So in this, uh, example, we use, uh, e-bike assembly line as the sample scenario, and we collect data from, uh, whaling station, welding robots, and so on, the conveyor belts. Um, and then we'll use one specific asset welding robot, uh, to, to walk through the workflow, right? And then on the right, on the cloud side, you are sitewise cloud, uh, that's where the data is modeled, uh, sets are modeled, and data is stored securely. Uh, and then we show the agent workflow that's on the right, um, that has a number of data sources, one site-wise, obviously, and then there is Bedrock knowledge base. So this is where we store all the manuals for that equipment. This could be equipment manuals, uh, standard operating procedures and so on. And these are all connected through MCP to the agent, right? And, uh, when the workflow is triggered, again, it's not one agent. We, yeah, here it looks like, you know, it's, uh, we're showing it as one agent, but it's actually three agents working, uh, in a workflow manner. And then, uh, one, the workflow generates a repair plan when it's triggered, right? So the output is, uh, we generate a HTML report. Uh, we'll store it in Sary. We'll give you a pre-signed URL so you can access the report, right? In real-world scenario, this could be, uh, a ticket in, you know, that gets created in, you know, uh, SAPPM or IBM Maximum, but here we're just, uh, creating the report and then putting it in S3. Yeah, that's a high-level architecture, uh, for the first tutorial. Any questions before we move on? Yeah, what's the trigger point from Site wise in the middle to the predictive. Yeah, the algorithm that's actually I actually go to that, yeah. So the trigger is, uh, an anomaly, uh, being detected in sight-wise. So that's the trigger. So, now you could define the threshold. We use 0.5 as a threshold for anomaly scores. So it's 0 to 1. Uh, Again, you can adjust depending on the criticality of remission, but 0.5 is the default. So when we say there is an anomaly, uh, that's when the workflow gets automatically triggered. Now, we use, uh, IOT core behind the scenes, uh, for event-driven workflows from Site-wise. So today, that's the recommended approach. So when site, when there is an active anomaly in Sitewise, Sitewise sends an MTTT message to IOT Core, uh, on an even-driven basis, and then IoT Core, uh, will trigger the, uh, the agent workflow. Yeah, that's the flow today. But for this tutorial, yeah, we'll manually trigger the workflow, but that's how you do it in production, yeah. Yeah. Uh, can you repeat your question. Uh, yes. So in this case, uh, uh, we are simulating data for the welding robot. So, in fact, uh, we have a physical demo that we showcase, uh, even at, uh, Venetian, you know, we, there is an e-bike demo that you'll see. So what we did is we took the data, uh, from a physical demo and then we're replaying the data, uh, using EC2, right? Yeah. But right now, uh, we, the data for Sitewise coming through AC2, not through Sitewise Edge. But I still have Sitewise Edge there because that's the recommended approach. That's more cost-efficient, uh, way of, you know, ingesting data to the cloud because it batches data for you automatically. Yeah. Yeah, please, can I rip out sitewise and put something else there but still use everything else? Yeah, yes, is that just MQTT topic going in to actually communicate or Depends on the triggering mechanism you would do in your pipeline to drive an agent action, but Yeah, it could be a lambda, you know, triggering as well. Yeah, you don't need sitewise, uh, we, we're just showing site-wise as an example for the industrial data foundation layer, but it can be something else as well. Yeah. All right, great. OK. So we're gonna switch over to. It's a demo, yeah, right. Uh, is it readable? Should I zoom in a little bit? OK. So, the report that we're going to share with you. So here is a report on GitHub. Uh, sample agency code generation for industrial analytics. So what you'll see here, we're just going through this report, uh, today, and, uh, we did all the prerequisites to save, uh, you know, time for you. We have a QR code, you know, we'll share that with you. We do have, yeah, uh, so what you see here is essentially two, Jupiter notebooks, um, so under predictive maintenance. Yeah, this is the first one. Uh, yeah, first one. So you'll see some prerequisites. So this installs the knowledge base, uploads the knowledge base documents. Uh, again, one more thing to note is this, uh, report is fully self-sufficient. So you don't need to know site-wise. You don't need to know Bedrock. Uh, we prepare the whole environment for you. You can start with a blank, uh, account, AWS account, and then use this report. We do everything for you from data simulation. To preparing knowledge-based end to end, right? It's self-sufficient. So I did the prerequisite already, so I'll not do it, but I can just show you. Uh, it's just preparing the, the knowledge base using uh Python SDK. And in the second, Uh, example. So this is the tutorial for the uh, the repair plan creation. Um, Using strands. So these are updated architecture, but it's pretty much similar to what we've seen earlier. You can see 3 agents working in a workflow, uh, pattern. Uh, so, and then, uh, we'll go through, I'll explain some of the highlights from this, uh, notebook. You can try it later, uh, in your own account as well. And then on the 2nd tutorial, yeah, we got a separate. Uh, notebook. This is for advanced analytics. So Chris will uh talk about this use case. Uh, yeah, so that's the repository, um, to begin with. So I did a clone of this locally, and then I'm using Quiro. Uh, ID. So I cloned this report, and then I'm going to start with, uh, so I have a virtual Python environment, so I install all the packages. Let me show you some dependencies here. So because we're going to use strands SDK and strands tools, um, so we have those installed already, and then we use YAML for some configuration, Boto 3, to make certain calls to, uh, uh, er less services. So it's all installed. And now let me actually go through the Uh, tutorial. So the whole narrative, so whatever is the use case that we talked about, about around e-bikes, so it's all baked into the notebook, the whole scenario. I'll skip it for now. OK. So I'll install the requirements for efficiency. It's all, uh, Pre-install. So we'll import some modules. So to those new to strands, so it's, uh, pretty important, uh, to look into these three sta uh import statements. So first, we're importing the agent, right? So this is used to invoke the agent once we configure, uh, the agent with, uh, prompt, system prompt, and, uh, foundation model. And then tool, this is to create a custom tool, right, for your agent. So this can extend the agent capabilities, uh, beyond, uh, normal summarization, right? Uh, we use a tool here to create the HTML or to save the HTML document locally to the file system. And then we use MCP client. So we, we have 3 MCP servers we're using in this uh tutorial. So we got MCP client to talk to those servers, and then we use a Bedrock model, uh, anthropic, uh, model to, uh, Uh, to tie it to the strength agent, are people familiar with agents and tools and those terminologies? Like, is that something you anyone needs more information on? You wanna do, yeah. So if you look at agents, it's gonna think of it as gonna be like a persona, right? We're gonna, it's gonna be our maintenance person. It can be, uh, an operational person, right? And we're giving that agent or person tools to do their job, right? And those tools could be access to data, access to an API. And so when we say MCP and when we say tool, that's really the same thing. And then they confuse you a little bit more, an agent is also a tool. So an agent can have a set of tools and a tool can be an agent. Or it could be an MCP tool which is access to like APIs right now an agent is configured with an instruction. That's what's different between tools and agents is you define it in instruction and the tools it has and you'll see that here as we configure these agents. OK. Um, so we've imported the modules. So we have some configuration. So we need to pick the bucket, uh, where we will write the HTML, uh, report, and then, uh, we're configuring AWS credentials using a profile. Uh, so I'm pointing to a specific AWS, uh, account where I'm going to create that, uh, report, uh, into. And then, um, so Chris was mentioning the profile for each agent, right? So this is how you define the characteristics of the behavior of the agent. So we create those prompts in a markdown files. We call it, uh, SOPs or these are system prompts essentially. Uh, so you can find them for each agent. So there is a separate markdown that explains, uh, the behavior, uh, for that, or the profile for that agent. So, for example, for ops data collector, so this is the agent that collects data from sitewise. Uh, so you can see, uh, the definition for that, uh, agent, including, you know, what it should do, the, a quick overview of what it does and what's expected, uh, in terms of the sequence that the agent should follow broadly and some constraints. So it's not completely unadministic. So you'll, you'll find that SOPs for each agent. Uh, so we're loading those system prompts. So when we create agent, we'll tie this, uh, prompt mark, which is in the markdown file to the agent. And then we are loading them, so I'll run it to load the SOPs. So, the next step is now, we'll connect to the MCP servers, right? Now, let's take Sitewise as an example. You can talk to Sitewise, you know, in a traditional way using SDKs that we offer, uh, but then now you can use Sitewise MCP server that we offer and then plug your agent, so the agent can pick the right tool that's available to talk to Sitewise, both for reading and writing operations, right? And this tools, there's a QR code at the end for this MCP tool. Yeah, these tools are, uh, publicly available on GitHub. We have a bunch of MCP servers. Uh, there's some specific to a service. Uh, there are also some that are quite generic. There's an AWS API MCP server that can talk to almost any service. So we also use that, uh, here. So now we'll create MCP clients to talk to because MCP is client server based architecture. So we create MCP client to talk to those, uh, servers. And here, these are the three servers we are gonna use, right? Site with MCP server, uh, to collect telemetry data, uh, for a period of time and anomaly status because that's all in living in sitewise. And then, uh, another MCP server, uh, to talk to Bedrock Knowledge Base. So this is where we retrieve, we retrieve the, uh, knowledge base details around that equipment from the equipment manuals and SOPs and finally, the API MCP server. Um, this is a generic MCP server. We use it to, uh, create a file in S3, right? So I'm not using an S3 MCP server, but rather a common API MCP. And then now we're going to create, uh, some helper functions to, to create these MCP clients. And when you create a client, you can filter the tools that you want to make it available for the application. So, for example, let's say in site-wise, uh, you get about 50 or so tools, part of the MCP server, right? But you don't need all the tools, uh, for all your agents, right? If your, if your, if your agent is supposed to just collect data, live data or historical data. You know, from an equipment, you just need to use like maybe uh a fraction of those tools, right? So it saves, uh, you, uh, in terms of the response times, execution time as well. So we are fil we are, we are filtering some, uh, tools from site-wise in this case, so I'll show that in a moment. But then here, this helper function, all it does is it creates an MCP client for you, um, based on the, uh, tool filters and, uh, the command that you provide, I mean, which is the identifier for the MCP. So I'll run this. OK. So the, the helper function is created, and then I have one more to initialize the MCP clients. So this is where I plug in the environment, and then you can see I have 3 clients that I'm creating. So client number 1 is for site-wise, and here is the identifier for the sitewise MCP server. And then I'm using specific tools, not every tool. I just need 5 tools, uh, to retrieve data from Sitewise. Similarly, uh, I got 2 more clients. So Bedrock knowledge-based client. So here is the identifier for Bedrock NCP server. And third is for the AWS API MCP. So this is the common MCP server that I mentioned earlier. So, 3, MCP clients, right? And then we are just returning those clients. So I'll execute. OK, so it's done. So finally, so so far, we have created helper functions. Now, we'll actually execute and initialize those three, MCP clients and we get back those three client clients. Yeah, it just takes a minute or so. So any questions on this configuration you're seeing or something you might wanna apply to yourself um. that the, uh Likewise, Bryans and us. On this machine you mean like or where does a client connection occur? Yeah, yeah, so wherever the agent is running, which could be agent Cores runtime as an example, uh, it would be the client and you would have to give it its authentication authentication, but the tool is brought down to that agent that's running, yeah. OK. So, yeah, we're done. We have, uh, the clients created and connected to the servers. And you can see for each client, the number of tools available. Um, And, and with that, let's move on to, uh, creating the agents, right? So we have the foundation ready with the NCP. Now, when you create agents, you can also define your custom tools. So so far we haven't created any custom tool, right? We're just using what's available out of the box from AWS, uh, uh, part of the MCP, uh, servers. So, for us, in this case, just to show you how you can extend the capability of existing MCP servers for your agent, um, here we're creating a tool to store the HTML report that we generate. Uh, locally, right? And then the, the report gets copied to S3. So this tool, all it does is it takes HTML, uh, as input and then stores locally. Now, to do that, to, to, to store a file locally, either you can use, uh, like a file server MCP, you know, that has access to your file system, or you can do something like this, right? Create a tool, um, that does a very specific task. So here, uh, we're creating a custom tool. And the way to do it in strands is using the tool decorator. All it is is a Python function. You add a tool decorator and it becomes a a tool, uh, that's available for your agent, right? So here, this is a, a tool to save HTML file. You can see the, uh, the definition of it. All it does, it, is it, it basically saves the content to your file. So nothing special here. So I'll, uh, create the tool definition. Yeah. So now we are done with the tools, right? We have all the tools, some through MCP, some custom. Uh, the next step is really creating those agents, right? So here, we'll create some helper functions again to create the agent. So first, we'll initiate, uh, the, the board of 3 session. So here I'm pointing to a specific AWS account using profiles. And then I'm also picking the Bedrock model, the foundation model that I'm gonna use, right? So for this, uh, tutorial I'm using uh cloud sonnet 4 so you can change, uh, under the model ID. So this is a common model I'm using for all, uh, three agents. And so with strands you don't have to use Bedrock. You can actually use other types of LLMs in your choice. Uh, so that's an option. Yeah, it's not tied to bedrock. Yeah. And then, um, yeah, it helps you initialize those agents, uh, as you can see here. And while creating agents, uh, there are 3 things you need to, uh, note, right? So one is the system prompt, essentially the profile, uh, definition for that, uh, agent, which we created in a separate markdown file. So, agent 1 is ops data collector, which is responsible for collecting all the data from sitewise. And, uh, we have system prompt. We, we are now attaching the tools that are available. And the bedrock model, right? The common model that we defined earlier. So that's the agent one. Agent two is your knowledge retriever. So the job here is to retrieve, uh, uh, Knowledge related to that machine. Here it's a welding robot from the knowledge base, right? So here we have a separate system prompt, so I can show you how it looks like. So the definition here it says is for a given asset, the SOP retrieves relevant information from bedrock. Pretty simple. You can again fine tune this. Uh, and then we have separate tools, right, from the MCP Bedrock tools. Uh, and then we're using the same bedrock model. And finally, a report, uh, generator, right? So the job here is to summarize, uh, and then create a, uh, HTML report and then to copy that report to S3 and then to give you a pre-signed URL which is a temporary URL that's valid for 1 hour, right? Again, in the real world, this could be integration with, uh, with your ticketing or asset management system, right? So that's pretty much it. So I'll Create the helper function. And finally, I'll initiate, initialize the agents. So now, so far, we've created MCP clients, uh, we created a custom tool, and then we've created 3 agents, right? It's creating. The agents part is still going on. It should take a minute. And we're almost there, right? The final step is then to execute the workflow. So 2 agents who created already? Any questions while that's executing? OK. OK. So, yeah, we're done with the, with creating the agents. So finally, um, now, yeah, we need to execute the agent, right? So, there, there's some, like when you deal with multi-agent, right, because here we're not just talking about one agent, there are three agents. There are different ways to design patterns for multi-agent architecture, right? And it's well documented in strands SDK documentation. Some common patterns are, uh, listed down here. Uh, there's form, graph, and workflow, right? It really depends on how the, the, uh, it really depends on the order of execution that's best suited for your use case, right? If the, if your use case demands a sequential execution of agent 123, which is probably the best fit for our, uh, tutorial, right? Then you can take a workflow approach. There is also a graph approach using a DAG, you know, you can execute your agents in a DAG following a DAG, uh, topology, or you can have autonomous coordination between your agents, right? You don't define the order, you leave it to your agents to decide the handover process, right? It can be agent 1 to 2, back to 1 to 3 to 2, right? It depends on, uh, the question. Um, so again, it depends on really the use case. Uh, you can pick, you know, one of those patterns. So here, we're using workflow pattern. Uh, because all we need is, uh, a report following a sequential, uh, flow. So, for So here is a helper function to create, uh, or to trigger that workflow. Uh, we have all the three agents, and then you can see the handover process, right? Because it's a workflow, we are defining the order. So step one, we are, uh, calling the agent ops data collector agent with the query process anomaly for asset name. So, so the prompt here is, uh, to process anomaly for a single asset, right, which is a welding robot in this example. And that goes to the agent one. So agent one will come back with all the data, uh, from Sitewise for that specific asset, right? And then it, uh, it will then hand it over to the knowledge retriever, the agent 2, right? In the prompt for the agent 2, along with the asset details, we're also providing the raw information, uh, gathered by the agent 1, right? So, that's step 2. And then now, you have also in addition to sensor data, the knowledge, uh, information as well. Now we are feeding all that to step 3, right? And then the prompt here is to generate a pre-signed URL. Uh, for the analysis, and then to store the HTML report locally and then to copy it to estuary. And then we have some parameters to point to the right location in estuary. And that's pretty much it, right? So this is essentially the workflow. In strands, there are, uh, new tools that, that will help you to manage workflow. You don't need to manually define a workflow. There are better ways to do it, but right now, um, there's one way to do it. So now I'll trigger this workflow. And I can see the, the whole, uh, interaction between the agent and the MCP server, right? You can see the tools that the agent is, uh, calling to get the relevant information depending on the prompt. So the first step is to figure out to identify the asset, right? So we're asking the agent to process anomaly for a welding robot. So it has to figure out where that asset is in site wise. So, so the first step is it'll really find the asset ID for that welding robot, and it does it by calling list asset models, list assets, describe asset, and then it found the, uh, uh, asset, uh, sorry, the uh asset ID and then using the asset ID it's making a batch call because in the. Uh, in the system prompt, we were explicitly asking the agent to use batch calls wherever possible. So it is using batch get asset property value to get the, uh, to get the sensor data from the machine. So it's really cool. The agents able to look at the tools and know that the order of which you should call which API to ultimately get to that ID, right, which is Yeah. And then now you can see the flow from agent 2. So agent 1 flow is done, uh, dot back all the sensor data. So agent 2 is now retrieving the data from the knowledge base, as you can see. So it's doing a list knowledge base, it's picking up the right knowledge space to query. And then, um, Yeah, you can actually see the response from the knowledge base too. Uh, yeah, it shows you a, a high temperature warning, around 80 degrees for joint zero, and some recommended actions, including the error codes. So this error code is coming from a knowledge-based document, right? Stored in estuary through Bedrock, uh, knowledge base. And then some recommendations, uh, you know, like looking for obstructions, uh, And so on. And then it also did some analysis around the joint velocity. To give you more insights, some risk assessment. Uh, some contact details as well from the OEM, uh, again, which is part of that knowledge base, and then it's generating the report, and then it's saving, uh, locally. Let me show you how it looks like on S3. Oh, it's gone. Uh, just give me one second. I'll take a. Yeah, I'll show you a sample report while it's running. Yeah. So this is the one I ran like one hour ago or, uh, Yeah. So what you see here is a, a report for that anomaly workflow, right? It gives you the life status of the machine, uh, the results from the knowledge base. So, like I said, the, the issue is caused due to high temperature and high velocity at that joint. So it gives you the error code, uh, mapped from the knowledge base, some root cause analysis from the knowledge base. Action plan. Again, this can be formatted, right? You can fine tune the system prompt to format the report, uh, that's possible. So, yeah, I see the maintenance schedule, some warnings, contact details, and so on. So, yeah, that's pretty much it. Uh, I wanted to show in this tutorial, uh, you know, like how you can start all the way from no agent to prototyping a multi-agent workflow to, to, to help you, uh, uh. You know, troubleshoot issues with your machines. Now, This much, the, the, this is pretty much for prototyping, right? So when you, when you are trying to product, when you're trying to move these agents to production environment, so that is, that is where you, you would probably look into something like agent core, right? Uh, to, and where you make your MCP servers and tools more discoverable using agent core. Right now, we're doing all locally using strands, uh, really great for like prototyping in this case. So now I'll hand it over to Chris. So before I do that, let me give you a quick context, right? So in this tutorial, we did a, uh, we processed one anomaly, right? In an operational context, in a live operational context. But then when you see anomalies occurring regularly, frequently for a type of mission from a, from a vendor, for example, for a period of time, you may want to dig deep into, you know, what's going on, what, what are those common patterns. Right? Uh, so that's where advanced analytics comes into play, right? So this is where some, uh, someone from the operations team or a data scientist would actually go into and see, look into, you know, different equipments under the same equipment class, same equipment class, for example, uh, to see, you know, what's causing those, uh, frequently occurring anomalies. So Chris will show you a different workflow. Um, that will help you do exploratory analytics, right? On, again, the same, it could be the same machine class like welding robots to look at a historical data set, right? Let's say last 2 years or last 2 months or 6 months. And to give you some common patterns that you can use and uh prevent anomalies from happening, right? But this workflow we saw is let's say you have an active anomaly, how do you respond, right? That's what we're showing in this tutorial, but in the next tutorial we'll show more on historical data analysis, right? That's where advanced analytics comes into play. OK. Any questions before we hand it over to Chris? OK, great. All right, thanks, yeah, so we're gonna, um, like you said, go into some advanced analytics and so we're just kind of paint the picture, uh, there's this trying to address the manufacturing skills gap, right? So the challenges that we have a need to access, you know, data insights and really do this quickly without having to rely on external IT teams and data scientists to really analyze the big picture of why I have maybe an excessive anomalies being detected. So think of uh Sarah, she's a process engineer and she sees that motor 01 is sounding off again, right? One of my joints is triggering again an issue. Um, so with this we wanna show how we can use Bedrock Agent Core and first party tools that it has, tools that we build and maintain to actually execute and generate code on the fly. So you ask it to do some analysis. It will start to pull NumPy and uh uh SkyPie and all these other libraries used for data analytics and really start to take that data and bring it through a whole um you know, even creating machine learning uh linear regression models. It's really pretty fascinating. You'll see what it does. Um, so it looks up, you can look up equipment specs, you can look up other, other information you need in this process of analysis. And, and this all that happens in the sandbox within that Asian core environment. And so that's what code interpreter does here, right? It lets you execute and generate code securely. And you can do this on large scale of data, right? We're thinking like, it could be gigabytes of data you might have that you want to take through this analytical process. And it makes it easy to use. You can execute code in JavaScript and TypeScript, um, Python, and other types of languages. All right, so that's what we're gonna see a process engineer, we'll talk to the agent. It will then pull access from code interpreters and other data points to, um, run that code. And then store it in memory. So with Agent Core memory, we're able to actually store these conversations and these results. And then Sarah can come back and say, hey, you know, I have a follow-up question, right? And essentially with memory, um, module for Agent Corres, you can create short-term memories and long-term memories. So imagine like you have this lot, a lot of conversations that occur over a period of time, you might be able to create the semantic relationship between things and it can recall that memory for you. When you talk to it. All right. Let me go back to switch to a demo. OK. So we'll go to advanced analytics and close out some things. That's Africa. OK. So here's um Initial start here, we'll actually go ahead and install the libraries like we need, like before, uh, so we won't have to go into that detail. Um, we'll actually create this code interpreter module and instantiate that and it's based on regions. So we supported a couple of regions. I can't remember all of them, but we're going to use US East One, which is Virginia. And this is part of the Asian core uh libraries that we're using. So if you look above, there's actually this Asian core Python library that we'll use for accessing code interpreter. All right. Let's go ahead and load the equipment data that we have from the anomaly test that we did, right? We're seeing now we're generating excessive amount of anomalies. Uh, and we need to really do some deep analysis into that. How many rows are in that data frame right now? How many rows 1000. We want it to be done quickly, so it's pretty. Hit execute and we'll wait for 20-30 minutes. It's just one month of data sampled every minute. Yeah, so you see several joints, temperature, velocity, and various types of joints that exist in that robotic arm. Um, and so we're trying to get through that. All right, that's done, right? OK. We need tools, right? Heer functions. This is like you saw, we won't go into that detail again, but it's something that, you know, Raj, you mentioned, right? You need some helper functions to open files and also prepare for the sandbox environment. So we're taking some of that data and put it into a CSV into that sandbox. And then more helper tools, right, to be able to call that tool. So these are not that important, but writing and listing the files you have in the sandbox. So we're going to create that as well. Now here's a great part, right? You're the fun part, you're creating that agent's persona, right? So you're telling them you are an industrial IIT data analyst, analyst, um, analyst assistant, right? And so you're specialized really in detecting this equipment, health and monitoring and, and it gives you all these principles you have and the tools you might want to access. So you need to execute Python and see the output. So we'll go ahead and create that system prompt, which we'll use. Coming up soon. Now we want to create our tools, so I'm going to go ahead and create a tool for code interpreter that strands will use. I'll create one for the equipment knowledge. So we'll create that as well. And then we need to enable memory. So in this example, you'll see it's, it's actually, you don't want to use this code in in production, but it's, it's looking for the existing memory and it's like wiping it out, right? So you can start fresh. Uh, so you don't wanna be wiping your memory every time this thing runs. Um, but it just kind of gives you an example of how you can. Set up your memory and create hooks. So with Agent Core, you can create this hook where every time there's a response, the hook triggers and it takes the memory and stores it. So that's, that's what we're doing in this code. And so if we go down to the bottom, you'll see that it's found that existing memory, right? Well, it's active. I'm cleaning up your old events, deleted 18. Memory is now ready for a fresh analysis. Uh, so just be aware of that, right, when you're looking at this code. Now, when you, with strands and agents, you need to now configure that to be aware of the memory module. You need to be aware of the tools it has. And so that's what we're actually configuring here, is configuring the model to use, the memory hooks that we're going to have for the memory module. And then This line of code, we, we want agent with memory. So we create that agent, we subside a model, the tools that will have that system prompt, the hooks that will then trigger the memory module to be used. Any questions on that, that line? That's an important line, right? Because you're actually like really setting up the, the agent's capabilities. OK. Now One more helper function for displays we'll go ahead and run our first analysis and that's probably the time we'll have but there's actually more follow up questions we can do to really test the memory, but we're gonna actually run this example where um the welding robot has several anomalies lately and we wanna understand, you know, what kind of, um, overall analysis you can perform on that data file. So let's go ahead and run this. And I'll minimize this code, see. So look what it did, right? Code generated code, import pandas, import Numpi, import mathPlot, right? It's actually like a data scientist creating the code that it will use to do this analysis. It starts creating its own plots, right? Uh, so it creates a skullable, so you can see. This takes about a minute and a half, right? So. As you see it, create, uh, you know, the different graphs, um, for anomalies, different bound, lower and upper bounds. These are things I don't know myself, right? I'm not a data analyst, but really, um, gives you the power of, oh no, what happened? OK. I think it's a throttling issue might be. Yeah, we are limited to how many calls we can make to Bedrock model during this. Weak. Yeah, you could hit again. Yeah, hit again, yeah. We'll give it a second chance. Yeah, this is, um A lot of usage on cloud for sure. Um, but it gives you an idea, you'll be able to execute this hopefully, uh, as well. If this fails again, hopefully it doesn't, but you'll be able to execute this on your own and see the results. Yeah, OK. All right, well. What you would see is essentially just uh as a result, it would have the um. It would actually show the correlations between these different anomalies and different sensor issues. And be able to tell you that oh OK this is um something you need to perform a repair plan on or perform more scheduled maintenance against and so there's different types of uh advice I would give uh that's really. Um, helps you create that direction on how you can optimize your equipment. Uh, maybe there is specific specifications or, or settings that you need to adjust to the machine specs. So it's really cool. It gives you a whole report. I wish I could show you that that in here. Yeah, you wanna try one more time. Sorry, one more time. I have a video file back to you. Yeah, yeah, let's pull out the video and see if it, yeah, works this time. It's like the meme where it's like sorry boss props down but I ran out of it yeah I know right. Yeah, I know, it's like Uh, Claude gets hit a lot during reinvent. So it's, uh, all these workshops and. Yeah, but you can already see the analysis right in the print statements, right? So it says there's no strong correlation between sensors, uh, so it's trying to print into a report where it's failing is generating that report, but the analysis, it's already being done. You can see the distribution of simultaneous analysis. You can see that it's finding the common patterns, the baseline behavior, uh, for certain sensors. And also the data quality, it also checks that data missing data, gives you a data quality report which could be causing maybe false positives, uh, as anomalies in site wise. Yeah, that's another insight you could get out of this advanced analytics report. Or I get lucky, 3rd time's a charm. Any questions while this answers? We have 3 minutes left, so that's your chance. Uh, you want to show the console, maybe in the meanwhile? The consul. What does production look like? Look at scale. For this type of solution like a. And you're providing it for the analysis point point of view, this is the advanced analysis, yeah, so you're giving that access to. license. 50,000 devices, I mean, whether you're performing across all those, right? There's. You were trying to look at it holistically as uh an issue that might be occurring across many devices and sensors like this this right here, like how does this scale out I mean even if you take like manufacturing plant. 500 to 1000 devices inside of same thing. Mhm. Like from a production perspective. Right, are you heavily reliant on site site wise to be able to actually go and pinpoint what gets sent into here? And so forth. Yeah, it leaves that up to to the human to decide some of those things, right? Like it's as opposed to like giving it all the data points and saying make sense of this, right? If there's a particular operation or process area that you're looking at and you're trying to address potential issues, then you might be able to narrow your scope. Yeah, but from a data management, uh, aspect, yeah, Sitewise is fully managed. It's highly scalable, so we have customers using Sitewise for thousands of assets. They onboard those assets and then Sitewise can. Uh, scale in even in terms of anomaly, uh, you know, giving out to those anomalies detecting, um, and triggering even based workflows, uh, that's highly scalable through sitewise because it's fully managed for you, so it actually ran for time. So, uh, we see critical issues found, right? Joint zero urgent attention required. We see 27 extreme temperature anomalies reading up to 87 degrees Cels Celsius and so it shows this like correlation between the during temperature and velocity anomalies. Um, and really starts to break down these issues, right? It also looks at what are normal operating ranges and nothing. There is some specs that it pulled from the machine to know these are my operating ranges. It looks at the data points, correlates that together, creates all that code to perform that analysis and create those plots like you, you saw in the code. I mean, the more context, the better, right? So in this case it's just the sensor data and the knowledge-based documentation, but you can bring in more MCP servers to talk to more, uh, systems you get a better, uh, analysis. We also said like we intentionally made some data missing and it detected that too, saying, hey, I see data quality issues, you know, 67% of this missing across joints and. So that's, uh, I, I, I saw your hand up there. Yeah, what's your question? We found something. How would you make sure R Right, by updating your index and so there's a mechanism to go ahead and go ahead and update your embeddings and. You know that joint temperature range is it like. You wanna only update part of your embedding. Oh, OK. That's a good question, um. If you identify the chunk where that's we we extract that from, um, but I can get, I, it's a good question. I need to follow up and with you after and see how we can do that so. Um, but yeah, it gives you a full report of what you need to do and maintenance recommendations. You can further enhance it to show like plots. So we're not showing plots here. Uh, you can do pair-wise plots, correlation analysis, you know, you can do a scatter plots and all, uh, that part of the workflow. We're not displaying, we're actually doing it but not displaying. Great. Yeah, oh, we have time. Any questions on. On what you've seen. Yeah, the QR codes. Yeah, the first, uh, link is to the public report, um, and you can try it in your own account. And the second is the list of MCP servers. Um, I think we're pointing to site with MCP, but in the same GitHub report, you'll see all the MCPs that we provide to talk to a number of AWS services. And third is some examples to get started with agent core, uh, to really, uh, scale your agents to production, right? Yeah, so. Awesome. Well, thank you. Thanks for joining our session. Thank you. Thank you all for spending time with us.