---
video_id: qwOob9s3kaI
video_url: https://www.youtube.com/watch?v=qwOob9s3kaI
is_generated: False
is_translatable: True
---

Thank you, everyone, and welcome to SDG 419. Um, quick show of hands, who here has been to a code talk before? Anyone? All right. Who, uh, came to this session specifically because you saw that we were going to show code instead of just PowerPoint slides? Anyone? All right, that's good. Who came because it's after lunch and they're looking for a place to take a nap? All right, yeah, we'll try to keep it down for you, sir. Um, my name's Matt Boweed, and with me is my colleague forbirakir Sakri, and we are solutions architects with AWS. And today we're going to be exploring how to modernize your, uh, managed file transfers and automate your file transfer and, uh, file processing. And so with that, um, let me quickly walk you through the journey that we're gonna be taking you on today. We're going to start by talking about managed file transfers, what they are, why they matter, and some of the building blocks that we'll be using for our modern file transfer service that we'll be building today. Then we're gonna do a quick overview of Transfer family and its features. And then we're gonna dive into our use case and a target architecture, followed by a quick sprinkling of agentic AI since we'll be using that as part of our solution. And then we're gonna get into the code. So we will be doing hands-on coding and deployment, uh, in real time, and we'll hope that nothing breaks while we do it. All right, so let's get started. What is managed file transfer? Uh, simply put, it is the exchange and processing of files securely, uh, often between two known business partners or business entities, but it can also be between internal or external systems in general. And believe it or not, managed file transfer is critical to almost every industry and vertical. Um, in finance, it is used for clearing house settlements, for example. In logistics, it's used for supply chain tracking and in any analytics use case you usually have to ingest files and content and that is also typically done using managed file transfer at some capacity. And so for most organizations, managed file transfer is not just the process of transferring files, but it's core to their business functions and their business processes. And so with that, um, what many organizations need to do is they need to modernize their managed file transfer systems and move away from legacy tooling and operational overhead. And so today we're gonna be showing you how to build a modern MFT system. And these are just some of the building blocks that we're going to be using today. Um, first, we're gonna be using AWS transfer family, fully managed service for file transfers, and I'll talk about that in a moment here. Um, we're also going to implement some malware scanning, because most organizations need to do that when they receive malware from an organization outside. And so we'll use guard duty malware scanning for S3 for that. Now, for the Agentic workflow or for our workflow after we receive that file to process it, we're gonna remove human manual labor, and we're going to use an Agentic workflow uh for that processing of the files that we receive. And then pulling it all together, we're going to use infrastructures code and terraform for our deployment and automated deployment processes. And we'll use an event-driven architecture with Amazon EventBridge. So briefly, I want to cover the key components of the AWS transfer family. There's actually 3 main services or features within the, the transfer family portfolio. The first one is file transfer servers. And these are fully managed file transfer servers that you can deploy, um, very quickly, and they scale automatically. They're backed by S3 storage, and they support the most industry standard protocols, um, SFTP, FTPS, and AS2. Now, if you need to send files to a remote SFTP server, or maybe you need to download from a remote SFTP server, Transfer Family offers SFT SFTP connectors. And SFTP connectors are essentially a, a fully managed uh SFTP client that you can use via an API. So no infrastructure to maintain, to download or send files via SFTP to remote servers. And then finally, if you have use cases where you need to provide a web-based access to files stored in S3, and you need to do it securely with authentication for human users, and in a user-friendly interface, we have Transfer Family web apps. This is a fully managed web app that integrates with S3, and. We'll be showing that as part of the solution today as well. One other call out here is that with modern file transfers, you often want to adopt an event-driven architecture, and Transfer Family integrates with Amazon EventBridge. So for example, you can uh automatically trigger downstream file processing when you receive a file. All right, so with that, I'm gonna pass it over to Prebier, and he's gonna talk about the use case that we're going to be building for today, and he'll show you the current state, and then what we're going to be building toward. Thank you, Matt. All right, so for today's use case, we'll be looking at modernizing a traditional insurance claims processing system. Uh, but just, I just wanna know, like, with a quick show of hands, how many of you are in like the, uh, insurance financial services domain? OK, quite a few nice. And what about some other domains maybe you can shout it out like what industries are you from? Healthcare's hospitality perfect. So the use case I'm gonna, I'm showing you today actually can be applied or can be seen also in other industries as well. It's and and the way traditional insurance claims processing works is typically it has three different phases, right? So typically we start with it in just phases, which this is where you will have, um, most companies ingest files using SFTP. Uh, in our insurance use case, these could be files such as policy documents, could be images and repair estimates. We often have an extraction phase. So this is where most organizations do have some kind of an OCR or basic OCR, uh, and for those who don't know OCR, it's, it's basically a technique that's used to extract text from images. The challenge of basic OCR is that it is somewhat rigid. It only expects files and formats in a certain way. And finally, in our analysis phase, this is where you may have some kind of manual processing you may have some intelligence with rules-based engines, but most of the times it is manually, uh, processed and, and the challenge with all this approach is that not only this is error prone, it is time consuming and it just doesn't scale. Now we're going to show you how we'll transform this using cloud native architecture. Now here's a modern approach for this use case which demonstrates all the principles that Matt spoke about. We'll be using secure file transfer er the transfer family. We'll be setting up malware scanning using guard duty, and we will be leveraging agents, AI agents to do the manual processing. Now, all of this architecture is going to be orchestrated using Ebbon Bridge. And yeah, without further ado, I'm gonna dive deep into each layer and we'll talk about it a little bit more. Now our stage one is where we are building a secure foundation. We're gonna be starting with replacing our legacy SFTP servers with Transport family, which means that there's no infrastructure to manage. It is fully, um, it, it offers automatic scaling and high availability. Now we want to authenticate external users, right? This could be, especially for our use case, external partners, repair shops, and we want to do it in a secure way without managing identity. So this is where we're gonna be leveraging our transfer family's custom identity provider toolkit, and Matt's gonna do a demo on that a little bit in just in a while. But by using that, you don't have to manage separate credentials. You can integrate with your partner's identity provider. It could be anywhere from Octa, from Ping, from Ontra. And you want to store everything in Amazon S3 for its unlimited scalability and high durability. Now moving on to our next stage, we want to add automatic malwase protection. Guard duty will scan every file as it arrives. It offers immediate threat detection and intelligent routing. What what I mean by that is that you'll have all your clean files that are going to be automatically moved to a clean bucket and any malicious file or a suspicious file will move to a quarantine bucket automatically all done through event driven architecture. And moving on, this is where we're seeing a major transformation in how organizations are processing files or modernizing, I would say file transfers. This is where we're going to be using AI agents to replace somewhat of the manual processing. We're going to be using Amazon Bedrock agent core that's going to be our our orchestrator for all our agents. And the agents themselves, they can use the most sophisticated models that are available not only in Bedrock but also from other cloud providers or other partners. So, you know, providers such as Anthropic, uh, even our own Amazon Nova models. And these in general provide a lot more flexibility and uh higher accuracy than traditional OCR. Now with just this approach alone we've now automated what used to take maybe a team of people hours to days, and that's something can be done in just a couple of seconds or minutes. Now let's make all of this accessible to our human users. Now this is where we're going to be leveraging Transfer Families' web UI or web apps service. Um, this is especially for your end users because you don't have to know SFTP commands, right? It offers a simple browser base access. It is completely self-served and has, uh, security built-ins. Which means that you can have rules-based access, right people have access to the right files and at the right time. Built on the principles of zero trust. Now we're going to be deploying all of this using Terraform. The transfer family uh service team and in fact it's endorsed by our solution architects because we build this, we support an official module that is available in the terraform registry. By the way, anybody using these terraform modules from Transfer family. You are, oh great, awesome, um, anybody using Terraform in general to automate perfect beautiful great awesome so um you're gonna really enjoy what we have for you today and uh for those who are not using our modules, uh, this is, uh, you know, I, I strongly encourage you to use our modules that's, uh, that's perfect. Using modules basically allows you to improve standardization. You can deploy all of your infrastructure across your environments in a standardized way. Um, and you widely want to track everything in Git, right, for tracking of changes, for rollbacks and auditing. And lastly, we recommend that you deploy everything using a CICD pipeline. All right, now it's time to build. Now we're gonna go start getting into the code. And if you want to see the actual code yourself, please grab the QR code. It will take you to the GitHub repository that we have for you, and also the branch, specific branch that we're going to be working off of. We'll share the QR code again in the end. So for those who have missed it, that's, that's perfectly OK. We're gonna be showing this a couple of times actually, right? I'll just give everybody maybe 10 seconds. I see some people are still driving the code. All right. So I think Matt, you have some stuff to show them, right? I do, yeah. So, um, before we get into the first part of the code here, um, I'm gonna show you what we're gonna be building, and, uh, you may, may have caught it. Prebier was showing the various stages of our architecture, stage 1 through 4, and that's actually how we're gonna be building out our architecture today into various stages. So the first stage is, um, to create our AWS transfer family server. And so we'll be using the Terraform modules for that. But we also need to incorporate authentication. And so that is where we're going to be deploying the Transfer family custom identity provider solution. And I'll talk a little bit more uh on that when we get into the code itself. But it's essentially a solution that is endorsed by the Transfer family service team for integrating the most popular identity providers. And we have a, a pre-built terraform module for this as well that I will show you. So with that, I'm gonna go ahead and get into the code and prepare, if you could switch me over. There you go. And we are now in the terminal here. And so very briefly, and, and again you'll be able to see this if you go to the code in GitHub um after the, the session. Um, we've structured our terraform into various stages here and we have a couple of additional modules that we put into this solution. Um, and so first, the first stage is our transfer family server stage here. We did also deploy a couple of prerequisites IM identity Center, uh, Cognito user pool, that's in stage 0, but we've already deployed that. So before I go into the code, I'm gonna go ahead and start the deployment, and we have several scripts to just help us with getting the deployment going. And we will do stage 1 deploy.sh. And this is just quickly showing what we're actually deploying here. So we have terraform apply, and then we're using a variables file. And I'll show you that variables file briefly here. We've actually broken up the solution into different stages with um feature flags in our terraform. And that's really how we're controlling what we deploy and when throughout this. Obviously, in, in most environments, you would not need a stage like this, but we're doing it for the purpose of the code talk. Let's kick off the deployment. And while that builds, I will jump over to. Stage one, our transfer server. So, a couple of key callouts here. The first thing that we are actually building, and I'm gonna bring this down to give us some space. Is our custom identity provider solution. And so for that we're using the Terraform for AWS Transfer family custom IDP module that is already available in the GitHub repository. So this really simplifies the deployment of the custom IDP solution itself, which is, which consists of a lambda function with authentication logic and modules for various identity providers, and then Dynamo DB tables that are used for configuration, um. And then for some identity providers, they can't be reached over public internet, and you may need to be, need to attach them to a VPC. And so, in order to do that, you can modify this use VPC parameter, set that to true, and you can specify the subnets, um, and the security groups that you need to attach the lambda function to, so that it can communicate with your identity provider. There's several other optional features. For example, you can put an API gateway API in front of the lambda function. Um, if for example, you wanna attach a web application firewall for additional security, um, but this is really all you need, um, out of the box for this custom IDP solution to be deployed with Terraform. Now the second thing we need to do once we've deployed the custom IDP solution, is we need to create our transfer family server. And again, we're going to use the Transfer family um terraform module for that. And In order to configure the transfer family server, we have a couple of parameters here. The first one is our domain. So we'll configure either S3 or EFS for the storage. In this case, we're going to be using S3. And then we have um options for the endpoint type and the protocol. So we're going to set this to a public endpoint, but you could also attach this to a VPC for example, if you wanted to run a private SFTP server. And then for a protocol, we're going to be using SFTP protocol. Now, for configuring authentication itself. We need to change the identity provider parameter to AWS lambda instead of service managed. And then we need to specify the ARN of our custom IDP lambda, which is one of the outputs from the module. And so that's what we've done here. And you can see now, this will wire up authentication to forward all authentication requests to our lambda function. Hey hey Matt, before you go further, I, so this is cool, but I wanna understand like how do you manage users and how do you map them to these identity providers? Yeah, that's, that's a great question, Prabir. And actually all of this is done via configuration in Dynamo DB tables. And so there's a couple of ways for you to configure those dynamy Dynamo DB records. Um, terraform is one option, but you could do that, do it externally as well. And the solution has really two tables. One is for the identity provider configuration. And that's what this Dynamo DB table item record in Terraform represents. Um, we give our provider a name, in this case, it's Cognito pool because our repair shops are going to authenticate via an existing, uh, Cognito user pool. And then we specify the configuration. And in the Cognito configuration, we need to specify an app client ID for our user pool and then the region that the user pool resides in. Um, then when it comes to the user, we can create records for each individual user. And in this record, we have our user, in this case, um, it's actually the any company repairs user, but we've, um, we're using a variable to specify the username. And then you link your users to the identity provider that you've created in your identity provider's table. So in this case, the Cognito pool provider. From there, you specify what access and entitlements you're going to give for that transfer family session. So, transfer family supports this concept of logical directories or virtual directories, where you can map any path on the SFTP server to an S3 bucket or an EFS volume. In this case, I'm mapping the root folder to a bucket that we have provisioned where our, uh, claims that we're uploading are going to land, initially. So, our deployment looks like it is complete, and I've got some output here, we can see our transfer family server endpoint. And I have a test script just to make sure that everything is functioning as expected. So we're gonna go ahead and test out this code quick that we've deployed. And we'll do stage 1-test.sh. So what we're gonna do quickly is retrieve credentials that we stored in Secrets Manager, and we're gonna use those to authenticate to the transfer family session. And you can see our SFTP session is going to use any company dash repairs as the user. So let's go ahead and kick that off. Say yes to the fingerprint. And I'll enter my password. Now, when I enter my password, if it's successful, we'll immediately upload one of our claims files just as a test. And it looks like we were able to upload that file successfully, so that's a good sign. The last thing we're gonna do just to make sure everything is working as expected, is verify that that file is indeed in the S3 bucket where we expect it to be. And simple S3LS we can see claim 1.zip was uploaded and we are all set. So, um, in this stage, we just quickly built a transfer family server. We integrated the custom IDP solution and we integrated, uh, our, our tran our Cognito users, um, for authentication. And so with that for beer, I think our next step is to. Implement some malware scanning, is that right? Yeah, that's right. Chris time, do you remember which service we intended to use for malware protection? Guard duty. There you go. Somebody's paying attention. Perfect. So for anybody using guard duty for malware scanning today. All right, see, I see a few people over there. So if you're using guard duty for mal malware scanning, so the way the guard duty works is that it scans every object that lands into a protected S3 bucket, and the way it does it is that it adds an object tag based on the scan results. So if it's a clean object, it'll add a clean tag. If it's a malicious object, it will add, um, no threats harm tag. Um, and what we've done with this is that we've expanded this architecture to leverage event-driven approach. So we have I Bridge that is listening to guard duty events, and the scan results are being passed on I Bridge to an SQSQ. Now, we've added SQSQ just as a matter of decoupling and best practice so that we can scale and offer resiliency, uh, but also like if our downstream processing is unavailable, we can always replay these messages using SQS. Now, SQS further triggers our lambda function. So our lambda function is basically reading off all the the events in our SQS queue and processing them. Lambda over here is kind of the brain or the logic that decides, based on the scan results, which files to be routed to which bucket. So once again, clean files will go to a clean bucket, malicious files will go to a quarantine bucket, and any object that is not treated is not processed will go to our error bucket. Optionally in this module we've also added a dead letter queue and SQS so any messages in the queue that are not processed, we want to actually store them too because we want to investigate what happened and process them at a later stage. And we've also had the ability to uh send notifications through SNS. We believe this is actually very important because if you have a malware in your one of your accounts or your buckets, you wanna get notified immediately. So this is an optional feature, uh, and we'll be showing that in the code right away. Matt, do you mind switching to the demo screen? Happy to. Awesome, so Before I show you the code, I'm just gonna kick off the deployment. So again, I'm using these beautiful scripts that Matt put together. And once again it's all very simply it's just doing a terraform apply uh it's not uh the screen from the Matrix, it's definitely doing a terraform reply if you've already seen it. All right, so the terraform module for malware protection is actually pretty simple. So you can see it is available as a submodule in our transfer family, uh, malware protection, Transfer family server module. And it requires primarily 3 different things. Number one, you have to specify a source bucket or a bucket that needs protection, right? So in this case, it's defined using our S3 ingest bucket block. Um, I am actually referencing a bucket that was created in one of our previous stages. It's the same bucket that Matt sent and created an SFTP file too. The second part of the configuration is our optional features. So this is where we can specify an SNS topic for malware detection, um, and optionally a dead letter queue. So all those configurations we've kept them as true as best practices default just to show that this works out of the box. The main thing you wanna know is that we have a routing config. So the routing config maps guard duty events. So these are the exact same events if you, some of you said that you use guard duty. These are the exact same events that guard duty emits. So for example, if the file was clean, guard duty will put in a no threats found tag to one of our buckets. Um, and you see that this, no, uh, this is being mapped to a clean bucket. This could be any of the buckets that you've created from maybe if you have another module that you use to create S3 buckets, so you can refer them as well for any any, uh, object that was considered malicious we'll have a threats found tag which will move to our quarantine bucket. And any objects that were not processed by guard duty, maybe there was a permission issue, maybe the object type was not supported, was an access denied or failed, they will automatically move to our errors bucket for further processing. And Premier, I just want to make sure that I understand this correctly. This isn't tied specifically to the transfer family service. Like you could use this for really any solution where you need malware scanning and S3, right? Yeah, that's, that's right. So the way we build these modules, they are, they're really independent building blocks or Lego pieces. You can take this module, let's say you, you already have a pipeline that deploys all of the above, right? VPCs, transfer family servers, um, maybe identity providers, maybe you're managing it separately using confirmation. That's absolutely fine, but you want to add malware protection, you can just take this module and deploy it in any of your telephone pipelines. So this will work out of the box. And that's in fact how we build all the other modules too, so you don't have to use everything or you know everything the example for to have a full feature deployment you can really plug and play. yep, it's awesome. Great, thank you. So let's test our malware protection. I don't wanna add anything on the code, so let me go backtrack. All right, stage 2 test. So once again we're using some helper scripts that are just simulating the same kind of flow. The first thing we're gonna do is that we're going to connect to our SFTP server, so I'm gonna start the SFTP session. I'm gonna enter my password for the SFTP server. Now I see that my my claim one file was updated. At the same time, I also added an ICAR file. Does anybody know what an ICAR file is? Perfect. So an ICAR file is a way for you to test malware protection without compromising the system. You actually don't want to inject an actual malware to test if your malware protection is working, you know, that's not a good idea. What if your system gets infected? So ICAR files are a great way to like, uh, test malware protection. So in this case we are parsing cloudWatch logs and we see that guard duty did pick these up as, as appropriately. So our clean file was tagged as no threats found and our malicious file, which is our ICA file, was detected as threats found. Now, I'm just gonna do an S3LS on our clean bucket to see if the files did land in the right place or not. So firstly, if I see in my cleans bucket, Uh, um, yeah, actually that was over here, so I did see that my file was processed properly. My clean file landed in my S3 clean bucket. If I look at my quarantine bucket. I do see that my ICAR file was successfully. Uh, moved to our, uh, quarantine bucket and again all done through even different architecture. I didn't have to make any manual changes on my side. All right. Now, I think it's a good moment for us to talk about. Agents or agent AI. How many of you are using agents or AIA agents are exploring with AI agents today, Whether it's in test? OK, a few of you. So for those who are not using AI agents, this architecture actually might look very familiar, you know, if let's say if you have a some kind of file processing pipeline that uses intelligence, intelligence, what I mean by intelligence is it uses some of our AI services. There were just a lot more steps involved in building something that does like I would say file processing and these steps were basically lambda functions or step functions that are that basically were called different area services. The thing I wanna call out that this architecture is still very relevant. So if you have a very, I would say, a pipeline that's very, uh, I, I would say. Um, very strict in terms of like you doesn't, you know exactly what needs to be processed and what orders this is, this is perfect, the scales really well. But for organizations that are looking at a little more flexibility, this is where agents are really, really powerful. Now, for this use case, we're going to be using an agentic care workflow, um, and we're going to be orchestrating all our agents using Amazon Bedrock agent core. We built all these agents using the strands SDK, which is an open source framework that AWB has developed uh, for you to like build agents, uh, and offers a lot more flexibility, and I'm gonna talk about it just in a second. We have a couple of different agents. I'm going to talk about the agents that are in the middle. So I have an entity detection agent. What it's doing is that its job is to extract text from a PDF file. I have a validation agent that actually is built on the principles of multimodality, so it will take text and will also read the image and then compare if the text matches with the images or not. I have built a summarization agent that summarizes all the findings from the previous agents, and I have a database agent that basically takes all the entities that are detected and pushes them to a dynoEDB table. The brain behind all of this is our supervisor agent, which is coordinating all the tasks between the other sub agents. So the uh supervisor agent kind of decides where which agent to invoke at what time. Now the thing I wanna mention is that this patent is actually not only relevant for insurance customer but it can be applied to any industries, right? Somebody mentioned healthcare, like maybe if you're in, um, like maybe in finance this kind of flow and this kind of architecture works really, really well. It's just that you have to change the prompt a little bit. And speaking of prompts, let me actually show you how to build these agents very quickly in terms of the prompts. So we have our claim files. The first thing I'm doing is that I've actually tasked the entry detection agent, and I'm using simple natural language to build my prompt. I'm asking you to extract this information from this car damage claim and ingest it in a JSON format. I'm asking my validation agent to analyze this car damage image against the reported claim details. I'm asking my summarization agent to summarize all of the findings into this specific format that I'm showing you over here. And lastly, I have my database agent, and I've instructed it in natural language to extract all these entities and push them into a Dynamo DB table. All right, so let, let's talk about how I've built this in code. Matt, do you mind switching to the demo, please? Sure. There we go. Awesome. So the first thing I'm gonna do is kick off the deployment. Now I wanna quickly walk you through the architecture and how we've built these uh modules. So the first thing I wanna mention is that we are. Uh, using one of our modules for agent core, this is a module that it supports out of the box, uh, you know, uh, supported officially, so we're using one of them, but the thing I wanna show you is actually how we build these agents using strands. Anybody using strands, by the way, today, or played around with strands? OK, we got one. We got one. OK, perfect. I see. Great, awesome. So for those who have used strands or this might look very familiar, but for those who haven't used strands, strands actually works on 3 different concepts or 3 different things that it requires. So the first thing that strands require is that the agent needs to have a model, right? So, um, the model is basically the brain for the AI. The second thing that the agent needs is a tool. Now a tool in very simple terms is an action that that you provide to the agent and an action that the AI agent can perform. This action could be a, a function, right? Something that you specify that can do that you do a task, or it could be another sub-agent, right, that this agent, this orchestrated agent can use. So it's basically you're giving the AI hands to, uh, uh, you know, interact with the outside world. Now, I've built a couple of tools for my orchestrator agents. I'm going to show you the exam, an example of one of them. The tool that I've built over here is actually a very simple function that calls our one of our sub-agents, which is the entity extraction agent. And you know for those who are familiar with programming, what I'm doing is pretty simple. The first thing I'm actually asking is to extract the entity extract the ARN off my entity detection agent. In my step two, I'm asking, um, I'm actually ingesting a payload which is my. Bucket name and my object name or the PDF key, right, which is going to be our claims forms that we'll be testing. I'm creating a unique session ID and again I've done this just for best practices. I want to troubleshoot every session in CloudWatch. And in step 4, I'm, actually, this is the place where I'm invoking the entity extraction agent bedrock. And lastly, in my final step, I'm just parsing all the outputs that are that I've uh that have been generated from this agent, um, and then this orchestrated agent can read them and proceed to the next step. Now I've built the other agents in a similar way. So you see that I have like the fodder detection agent specification, uh, I have the database insertion agents. It's basically just calling these agents, but I think I wanna show you over here is. Is this Now For those who have interacted with some kind of text-based or LLM-based, you know, um, service, even chat GPT, this might actually look very familiar. So instead of using complex logic, right, statements written in SQL or whatnot, right, I am instructing this agent in natural language to do the following, right? So I've, I've given it a task. I've said that you're, you are a claims processing workflow agent, leveraging strands. I, I'm saying that your job is to extract entities, validate the damage consistency, insert all this data into a database, and generate a summary. Now, the agent decides, based on this task I've given, Which tools to use and you saw like I, I showed one of the tools the way I've configured it so the agent decide, you know what? OK, I need to extract entities. I know that I have a tool for that, which is my sub agent so I can call that. So that's how we've built this in strands and again. A fun fact is that I've built this in a couple of minutes. So you know we're using Qro CLI. So for those who are using Qro or any other AI powered coding system, this can be done very, very quickly. So all you have to do is just put your business logic, ask it to build something in strands, and you will have your agenttic AI workflow, all right. Now, before I test the, the agents, I'm going to show you what the claim files look like. So I have two sample claim files. On my left, I have a claims record that shows, um, let me come here actually. A car that was damaged in a parking lot, we have a rear bumper damage that happened in the parking lot, and the estimated cost to repair it is $995 if I'm not mistaken. And if you look at the image, this is very consistent with what we see. But on my right hand side, I have a claim form that states that this was actually a minor front bumper scratch that happened in a grocery store parking lot. And I don't know, like, do you think that this is a minor scratch? Doesn't look like a minor scratch to me. Um, yeah, I would say that's a little more than a minor scratch in a parking lot. Yeah, yeah. And by the way, no cars were harmed in making this demo, right? So I just want to call that out sort of in a very, very expensive demo, otherwise. Awesome. So let's test this, Matt. What do you think? Yeah, let's try it out. I'm gonna switch back to the demo. All right. So once again we're using a test script. Same flow, our event-driven architecture is going to kick in. We're going to log into the RSFTP server. You're going to enter a password. And we've uploaded our one of our claim files. And one thing we've did with the script is that we've asked it to parse Cloudwatch logs, so we have like 5 different agents. We thought that the best use of our time would not, wouldn't have been to go to the console, find Cloudwatch logs. Who likes the, the, you know, going to the console, by the way. Anybody, you guys are all like no one likes. I like the console. Don't let's be cool, yeah, let's not be too mean. So you like everybody likes to go code being like the ID. Yeah, that's, that's what I see the team over here. Yeah, cool. That's good, I guess that's the right answer. Perfect. Cool. Maybe. I don't know. All right, so we see that our, um, cloud watch logs picked up different things that are happening in our workflow. So we see the first thing we found that I'm just gonna scroll up just in a bit, we see our entity detection agent picked up all these entities, right? um, our fraud validation agent, um, again took this data and it's now reading the image that I've that I've supplied with it. Our workflow agent is our orchestrator, so it's again it's kind of showing a different orchestration that's going on. And our database agent again took all these entities and now it's meaningfully parsing it and ingesting into a 90DB record. All right. So I see that our claims processing was finished and I'm gonna show you the claim in just in a minute. All right, so I see that my claim was successfully processed, and I'm going to move on to the second claim as well. I'm going to test both of them, and we're going to see them at the same time. OK, so I have uh uploaded this file. Um, I know for those who like the console, let's actually go to the console and see what's happening. OK, so if you create a couple of different buckets, I will be in my clean bucket because all these files were clean, and I can see that my claim was successfully uploaded. I just uploaded my claim too, so it's still being processed, but at the same time, I have these two prefixes. So I have my submitted claims. I see the claim one that was submitted, so this is very consistent with the image and the PDF that I showed to you. And our process claim was the one that was just processed by the AI agent. So I have the same prefix again. So, so if I open this summary, let me just make it slightly bigger. So this claim is claim one. We saw that this was a claim that had, um, you know, that said that it was had a minor rear bumper damage. The estimated cost to repair this claim was $995 and this is what my summarization agent, by the way, did. So it's, it states that our fraud agent said that this is the the fraud agent is 90% confident that the description matches the image. The analysis again all done in natural language which says that this is a car that had a minor bumper damage. It's very consistent with the claim description. There's a visible denting on the rear bumper and there's no apparent damage on the vehicle and the recommendation by this agent to our human agents is that this agent this claim can be approved. So again this is super super powerful, uh, just eliminate a whole bunch of manual steps that human users have to do. Uh, at the same time, let me, let's see the second one. Yeah, let's see. I'm curious about that. Let's see if it processed. OK, so my second claim is also processed I think should be the 1st 1 over here. Alright, yes, legible, OK, perfect. So my second claim again, uh, the, the our claim forms stated that this was a minor front bumper scratch and the estimated cost to repair that was $350 but our AI agent said that this is actually the image does not match the description and the agent is 95% confident, so even more confident in that this is not a scratch, right? So that's exactly what it states. Uh, it says that although the claims suggests that this is a front bumper scratch, but the image shows severe front end damage, and it suggests that this claim should be reviewed for by a by a human just to be sure. So you know, the thing I wanna call out as that is this shows how powerful this workflow can be and how you can apply this in your use cases, and I build this using simple natural language prompts. The power of this is that you can literally take our demo, maybe play around with the prompts, and have something up and running for your own use cases too. The one thing I do want to call out is that I don't think that, you know, our business users will like going to the ID. I know we all of us, we do like the ID or the AWS console maybe, yeah, not even that to advance for someone that needs to review insurance claims, right? So what do you think Matt, we should do for that? Well, um, I think this would be a good time to implement transfer family web apps. OK, take it later. Why don't we do that and I'm gonna grab this, switch back to our PowerPoint. So the last stage, now that we've um created this awesome workflow, where we're, we're automating the file or the claims processing and, and summarizing it, looking for the potential for fraud is we need to make this available to the humans that are gonna do this final review and sign off on, on these claims, right? And so we'll call those our claims reviewers, and we're gonna give them access using a transfer family web app. So, um, show of hands, is anybody using Transfer Family web apps at all or explored it? Awesome. And so, um, anybody here use a service called S3 Access Grants? Yes, obviously if you use web apps you use S3 access grants. Good, good answer. So, um, we're gonna be using a combination of these services today. I'm gonna briefly, for, for those not familiar, walk through how the service works. So when you ran when you deploy a web app, um, you integrate it with another AWS service called IAM Identity Center. And identity Center supports federated authentication from um identity providers OCTA, uh, Atra just to name a few, but really any SAML-based identity provider can integrate with Identity Center, and that's how your end users will authenticate. They'll authenticate through Identity Center and then be redirected back to the web app. But then we need to assign them permissions and authorizations. And that is done through another service called S3 Access Grants. And in order to do this, we first define an S3 bucket location with our access grants. And so this is where we're going to, um, specify a location to assign grants or permissions to. And um then we're going to assign individual grants for our users or groups. And in the table in the lower left, that's the grants that we'll be assigning, and you can see that our claims reviewers group will be given read only access to the, the submitted claims and process claims buckets. And then we'll have a claims admin group. If they need to manage the files in the bucket, um, we'll give them read and write access. So we're gonna set all of this up. Now, the, the one thing I, I want you to be aware of is if you've ever had to configure all of this in the console, especially the access grants and setting up and assigning the permissions, um, you may find that it, it's a lot of clicks, it's a lot of, uh, manual, uh, processes, looking up user IDs based on their display names. So we're gonna automate all of that with Terraform here in just a moment. So let's switch over for beer. And again, I'm gonna go ahead and kick off our. Deployment before we show the code. And we'll go over this. OK, so, um. What we're gonna do today is show you what we're calling the alpha module of uh the transfer family web app for Terraform and this is something that we haven't released yet, uh, but it is in the the source code that we share we're sharing a link with today and it's something that we're hoping to release here very shortly, um. But this, what we expect this will allow you to do is really simplify deploying transfers family web apps and configuring your S3 access grants for your user entitlements. Um, this is made up of, of one main module for the web app and then two submodules, which I'm gonna walk through here quickly. So, first we use our transfer web app module itself. And this is what's going to actually deploy the Transfer family web app. And a web app is actually really easy to deploy. It's fully managed with a couple clicks in the console, you can deploy a a web app in the transfer family uh transfers family console. Um, and really all we need to do is specify the name that we want to assign our web app. And then we need to connect it with an IAM identity center instance and an S3 access grant instance. And usually these are both in the same AWS account. However, if you already have an organization-wide identity center instance, you can use that instead of an account-based instance. So in a couple of lines of code, we have our web app deployed. Now we need to configure our permissions, and we have a submodule first that will define the S3 locations where we're going to um be giving users access for in web apps. So, what this module is, is doing is configuring this in at the S3 access grants instance, and it's automatically creating an IAM role with the associated permissions and trust policy required, and it can optionally create a bucket for us if we want to specify a clean bucket that we're gonna be using. Um, or in this case, we're gonna be using an existing bucket which has those process claims. The other thing that it can do, and this is a requirement when you're configuring transfer family web apps, is we're gonna set the uh course configuration on our S3 bucket. So you need to do this for any S3 bucket that you configure with web apps. And so this will also do that for you automatically as well. Now the last piece is to configure the entitlements or the permissions themselves. And that's where this web app users and groups module comes in. And again, this is a submodule that will be under the, the transfer web apps module, because it's purpose built for web apps. And. We can define both user and group entitlements via the users and groups parameters. In this case, we're doing role-based role-based access control, so we're assigning groups and. First we specify the display name for our group. Now I'll show you in the console in a moment what this actually does is it looks up the the group ID, um, which is a UU ID of that group in identity Center, and it retrieves that and we'll use that for the configuration. So it's simplifying a lot of that process of doing lookups, and then we assign our actual access grants. In other words, the permissions that we are going to give to this group. In this case, for our claims admins, we're gonna assign read-write access to our, uh, S3 bucket location that we defined in the previous step. And we're going to give it access to the entire bucket, so all paths with that wildcard. Now for our claims reviewers, very similar setup, except we're going to have two access grants assigned. First, we're going to give read access to submitted claims in any sub files and folders, and then to process claims as well. So let's check on the status of our deployment. And it looks like we deployed successfully. So, uh, there's nothing to test in the terminal this time. I'm gonna take you to the actual console because this is a human interface. Um, quickly, what I wanna show you is what we just created. So. This is our transfer family web app that was just provisioned. Uh, if we go into our web app, we can see the configuration, including our, um, identity center instance being configured. And if we go to the groups tab, we can see that both of our groups have been assigned. These are the group IDs that I talked about. So these are just IDs that are unique that you would otherwise have to look up manually yourself. So we've really simplified that process with this module. And now I'll jump over to S3 access grants. Actually, I'll just switch over to this tab. And if we go into the details here, we can see. We have our location set up, in this case, our claims bucket with our clean and process claims. And then our grants, so you can see our grant IDs for our claims reviewer and claims admin groups, and the grant scopes and permissions that have been applied. So all of that was done with just a couple of lines of terraform code. So what does it actually look like once you authenticate? Let's go back to web apps. And we will open up our web apps URL. We'll sign in as our claims reviewer here, and I'm gonna make this, I'm not gonna turn up the volume, I'm gonna make this just a little bit larger, maybe too large, but we'll see. I'm gonna remember my password, hopefully. All right, so, now we're in our user-friendly transfer web app interface. And if we look here, we have access to both the processed and submitted claims, read permission. If we go into our process claims, we can open one of our claims folders and download that same summary that Pravier showed us earlier. Um, one other thing to call out here is there is some level of customization you can do with these web apps. So if you need a custom logo or you wanna use your organization's logo, um, you can specify that in the configuration, and that will show up here instead of the transfer web app icon that's right here. And this also was a little small, so I'll make that slightly larger now that it's, we've already gone through it. All right, so, that I think is the end of our architecture. And just to quickly recap what we've done today, so we configured a transfer family server with authentication to an external identity provider. In this case, it was Cognito. Um, we did malware scanning automatically once we received files. We then used agentic AI to process our submitted claims, look and see if there was an inaccuracy issue, and then we made those claims, once processed, available to our claims reviewers. So with that, Previer, where do we go from here? What are our next steps for sure. So I just want to say before we wrap up, all the code that you saw today is powered by our Tran Family Terraform module. And this is available to you for for use, uh, you know, whenever you can. And the step by step example that we covered today is one of many examples that exist in our repository. So it's built typically on use cases. So if you have a specific use case, we show you different components such as connectors. But today we showed you a use case with the Gentech AI. All of these can be deployed in just a couple of minutes. So again, very, very quickly for you to set up, if especially if you're using infrastructures code. We launched this module only just a couple of months ago and we've already had over 10,000 downloads, um, and I just wanna thank all the database solution architects that are basically contributing to this but also folks like you, you know, we've had a lot of external contributions too, um, so if you're one of them, thank you so much. Uh, you can grab this QR code for direct access. This will take you to the, the root, uh, of the, the module which has all the examples and has all the, uh, modules that you can use in your workflows. One thing I wanna call out is that we do maintain a public roadmap for all the things that we're working on. So if you like any of the features that we're working on, you can always do a plus one on them. But if you feel that there's something that we do not support, uh, if you have any suggestions, you can always create a GitHub issue. Uh, my team and I, we review these, uh, almost daily. We wanna get new features for you as soon as possible, and I'm super excited to see what you'll build next with these tools. So with that, I just wanna thank you for your time. I hope you enjoy the rest of Green Me. Thank you. Have a great week, everyone.