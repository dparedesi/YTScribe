---
video_id: kfgt1uJE-E4
video_url: https://www.youtube.com/watch?v=kfgt1uJE-E4
title: AWS re:Invent 2025 - Build agentic workflows on AWS with third-party agents and tools (AIM3311)
author: AWS Events
published_date: 2025-12-04
length_minutes: 56.3
views: 86
description: "Discover how to architect and enhance AI development workflows on AWS with powerful third-party agents and tools from AWS Marketplace using Amazon Bedrock AgentCore Gateway and Runtime. This session walks through real-world use cases for third-party agents and how to efficiently source and deploy AI development tools the moment you need them. Through architecture diagrams and live demonstrations, we'll explore how to import tools via Gateway, establish connections with your IDE and MCP client, a..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

Good evening everyone and thank you for joining us, whether you're here in person or joining us virtually, we're excited to have you. Um, my name is Andrea Kotnikov, and I lead product management for AWS Marketplace deployment Experience, which includes the fast growing, um, category of AI agents and tools, and I have two amazing co-presenters with me today. Salman Taherian is the global AI. Partner, uh, lead, uh, who's working closely with many of the partners who are building the next generation of AI agents and tools, and we have a special guest today, um, Mira Rajaram, um, head of AI core services at Workday planning, uh, who's bringing the practitioner experience of building and operationalizing AI agents. Um, we're excited to take you through today's session, uh, build agentic workflows on AWS with third party agents and tools. Before we dive in, let's do a little warm-up. Please raise your hands if you have built an AI agent. Nice. Let's keep them up. I see a few keep them up, keep them up. Uh, keep them up if you have more than 5 AI agents running in production in the organization. Nice, I see a few hands more than 10. OK, last hands, but still we got a few. Cool, so. Uh, the goal of today's session is to show you how AWS and the partner ecosystem give you a strong foundation for building agentic solution. Um, by the end of the session we want to make sure you get the strong understanding of all the foundation and actually get ready to put your hands on the keyboard and get started building the agentic workflows. So here's the plan for the next 60 minutes. We'll start with the basics, the overview of AWS Agentic stack, which includes the core services, some of the architectural principles, and the recommendations, uh, which we have collected from our customers who are building agentic applications right now. Uh, after that, I'll give a quick overview of AWS Marketplace and how AA agents and tools from third party vendors can help you building agentic applications and specifically how you can use them in the new set of um agentic services on AWS in Agent core runtime and Agent core gateway. After that, I'll pass to Mira to walk us through Workday's example of uh building the planning agent so you'll hear some inside stories of what's going well, what's not going well, of building and operationalizing um AI. Next, Salman will walk us through a hands-on experience of building energetic workflow using third party tools. So you'll see how to get started from zero to ready to go agent, and we will wrap up this presentation with the demo of a pre-built agent which you can get from Marketplace and which you can deploy in a few minutes. So let's get started. Fantastic. Thank you so much, Andre. Hi, Ruan. So agents, when we talk about agents, it's important to acknowledge their agency. They need to be able to act. And with AI there are two additional attributes that are important. They need to be able to take on instructions and form a goal and an objective that they would be able to achieve. And then the second one is that we should be able to reason and plan in order to be able to take advantage of those tools that are available. So, agency is super important and with AI it's able to capture a high level goal and it's able to reason and plan in order to invoke the right tools in order to serve the outcome to the user. And today, we'll see that, you know, only we'll see it, we hear about these experiences that we've had with Agent core and building agents and Miro's gonna be talking about it later on. So it's very much an evolution. We've had bots in the past where for static workflows we've been able to bring automation. Over time that's evolved into cognitive automation, enhanced workflows with ML capabilities and AI capabilities, and generative AI assistance came about where we were forming these prompts and queries, sending them over to a foundation model, getting a response back. But now with agents as I highlighted, they can really do a reasoning, planning, and they can form that collaboration where they can take on an objective more than just a task, fulfill that objective and return a response to us. And over time we're gonna see fully autonomous agenttic systems. So there's a maturity curve shifting from. Low agency to higher agency by the agents and the role of a human from you actually doing the work in the past is shifting over to more of an oversight. Now in AWS there are 3 areas that we're deeply focused on. First one, choice, bringing the latest market leading capabilities to our customers and partners so they can freely innovate, innovate fast, realize ROI and, and value quickly is important, and then move in, move along with the latest capabilities that come within the market landscape. Second one, trust. AI has been probabilistic. It will likely remain probabilistic, so it's important to be able to control, govern, establish guardrails, and compliance for using these agents in production. And the third one is around scale because whilst the key component of these LLMs and some of the tools are fundamental, the end to end solution compromised of multiple components and having enterprise great capabilities that would allow you to quickly innovate, move those into production. And then scale that production as your demand and adoption grows is super important. That's what we bring from AWS. I'm gonna briefly touch upon each one of these. Firstly, around choice. So with choice, the ability to buy or build or partner, depending on the use case, depending on your strategy is extremely important. So at the top you see some of our AWS native capabilities with QuickSuite and third party capabilities that Andre is going to be talking about from Marketplace. In the second layer, you have Amazon, Bedrock, Agent Core, as well as Strands Agents as a framework where you can build any custom agenttic solution of your choice. And this is where you know if your use case in particular does not fit one of those out of the box models we see that the ability to be able to build your custom agent is important. Lastly, bringing partners who have the expertise and experience at within AWS with with the AWS Generative AI Innovation Center, but also our partners professional services is important and that's an area where we can partner in order to build those solutions, take advantage of the lessons learned and the best practices that has been accumulated over many runs and many build scenarios. And in this context, we are providing the capability that would be able to work with any models and any frameworks, and that's the flexibility and a choice that would speak to our value. Now in terms of trust, security, governance, audit, I want to highlight three points here. The first one around transparency. It's extremely important to acknowledge what are the models that are available, how services are developed, what are the intended use cases. In fact, we started this with our AI services where we had AI service cards and we've now extended those into our agentic AI capabilities where we talk about how they've been trained, what type of data it's been used, what are the intended applications, how do they fare against bias, transparency, explainability. It's extremely important when you consider responsible AI as well as providing IP identification for Amazon Nova models. In the second part, it's around having control. Now, this is a shared responsibility model. We're working closely with model providers around how to how to make the models themselves safe, but also we need to have the right guardrails around the models, and this is where bedrock guardrails comes into play in order to be able to help you govern those for specific applications. Make sure that you control the egress ingress as well as the egress of those LLM models. Lastly, when it comes to audit, data lineage, observing what's happening across the agent runs, this is where I'll talk about agent core observability, but also our services such as Cloud trail and Cloud Watch have been there in order to support that. Now, I talked about Agent Core. I wanna, you know, double click on it slightly more. There are 3 key areas with agent core. We talked about agents having agency. That's the first part. It's around having tools and memory to be able to realize what is the session information, what is the short term as well as the long term, so that each time an agent is run, it's able to learn from its experiences, take those information into account as memory, and enhance its output. But also having access to tools, and here I've highlighted two of our native capabilities, the code interpreter, where you can basically sort of generate code but also securely within a sandbox environment, test that code and run the code, as well as a browser tool that would allow your agent to browse and utilize the browser much like a human would do. In the second part, it's around deploying these securely at scale. So runtime and identity, which I'll cover in the next slide, are the key services in this area. Lastly, but not least, it's important to have view on the whole end to an execution of these agents. That's where agent core observability comes into it. So briefly, when it comes to deploying these securely, we have agent core runtime, which would allow you to containerize your code, deploy it into a cloud environment within a customer VPC securely and operate that. So that's extremely important to allow customers leverage the IP of a partner per se. And run it within their own environment and have guarantees around what data can go outside and what data can stay within their own customer VPC environment. Agent core identity in particular addresses one of the key challenges that is access control. Access control for the user who's using the agent because based on your credentials you would have access to certain resources and information and every one of us would have a different level of access but also access control for the agent itself as it tries to access resources across systems. When it comes to tools and memory, we have agent called gateway that would provide access to a list of tools and capabilities that are available for the agent to be able to take advantage of. Agent core memory we would allow you to, as we talked about, improve your run based on taking account of the shared context between different agents across sessions, be it short term or long term. And the last two I talked about these from a browser and code interpreter capabilities, and later on we'll see a demo where we're actually using a third party tool. So these two are the native tools provided by AWS. Lastly, when it comes to agent core observability, we have integrations with standard models such as Otel, and we're bringing that data into cloudWatch and cloud trail so that users can inspect agent runs, perhaps diagnose any kind of root cause of issues, understand why certain patterns and why certain traces were executed, and improve that over time. But that's it, I'm gonna hand over to Andre to talk about Marketplace. There you go, Saint Salman. Um, so when we talk about agents, they're by default consist of multiple components including the LLMs, in many cases the tools, the knowledge bases, and. One of the key challenges that I hear from customers building or experimenting with Agenda AI is not how to build the agent, but it is how to get access to all the tools that they need. Think of the new vendor on boarding, security reviews, legal reviews, payment set up, all that can take from weeks to months, slowing down innovation and adoption, and AWS marketplace removed that friction. Marketplace is an enterprise ready digital catalog of software, services, and data, and now also AI agents and tools for the developers who are building agenda AI. The marketplace brings three key benefits. The first one, the procurement time drops from months to days or minutes if Marketplace is already configured in your organization. You can, your developers can go to marketplace, find the tool they need, and subscribe in a single click, uh, with the full transparency about the pricing, contractual terms, so that's, um, a big advantage for the getting started time. The second benefit is the unified billing and the cost control transparency. Uh, all the spend through AWS Marketplace goes through your AWS bill, so your developers don't need to put their own credit card and do the expense reports for the, for any tools that they're experimenting with. But in addition to that, Marketplace integrates with AWS, uh, cost tools such as budgets and the costs explorer, so you could set up the certain spend goals and have full transparency on who's spending and what they're spending building agenda AI. And finally, that's the actual developer experience. It's a simplicity of deployment. Um, marketplace products natively integrates with AWS services, so in a single click you can deploy an agent on agent core runtime. I'll show you that in a later part of my presentation. So when a developer wants to start with building an AI agent or exploring the tool, evaluating the tool, they can get started in the marketplace with just a few clicks. Making it a one stop shop for AI agents and tools, we launched this category just a few months ago, uh, in July, and it's been growing very fast. Right now we have over 2300 agents, tools, and agentic applications, including professional services from our partners. Um, they fall into different categories. So let me break this down for you. On the right hand of the slides are the API based agents and tools. Those are the remote tools, um, ISC hosted. So for example, Tavili search tool, you'll see it later in Salman's, uh, section of the presentation. Um, from the developer, uh, standpoint, they act as an external targets. So developers go to marketplace, they subscribe in a single click, and they can also in a single click import those tools into agent core gateway as an external targets, and then they can use or reuse those tools in multiple agents for different kind of applications they're building. And what's great is the automation because um agent agent core gateway automatically pulls down all the metadata, the API schema, actions. Also we automate the API key rotation uh and give the full transparency into what's happening with those tools. So, no manual wiring, no schema translation, uh, gateway handles all of that. Um, the second category on the left portion of the slide are container-based agents enforced. Those are containerized agents that are designed specifically to run on agent core, uh, runtime. The key benefit those are run inside Euro VPC on agent core runtime. So you're getting all the benefits of agent core service including the VPC isolation, IM-based identity and resource policies, tool level permissions, and of course observability. So you're getting all the benefits of the partner logic, the partner defined agents who have pre-built, who've done all the prompt engineering, but it runs within your VPC. And the third category on the bottom are agentic applications and also the uh professional services we hear from many of our customers who are saying we need some help to get started. You can go on agent uh on AOS Marketplace and find support from a broad network of the partners. So before we go deeper into building the agents and showing how that works, I want to ground all the theory that Salman and I have covered in a real customer implementation. Workday has been one of the early adopters of Agent Core and other AWS Agentic services, and please welcome on stage Miram, uh, who will share their experience of building a planning agent. OK. Um, thank you, Andre. So I'm Meera Rajaram, and I had. The AI core services for workday planning. So, workday. I think we are perfectly positioned to help our customers manage their money, people, and agents. Now, this is a marketing slide. Um, these are the list of agents I'm allowed to show you, um, because these agents are actually gonna be coming out by end of this year in the hands of customers. There's a whole other spreadsheet of agents that we're working on, and that are going to be following shortly. So, How are we getting all these agents out? Um, Workday has an agent platform team. That's essentially making some of these, you know, common stack consistency code to how we're building agents. So we, we leverage Landgraph, we're using Langsmith for our evals and traces, and we have something like a centralized gateway to the LLM models, right? So this is what we end up leveraging across the board. And the planning agent is no different. We are also leveraging that platform. But let's dive into the planning agent. So, planning agent is one agent that represents 4 personas. To give this complete planning intelligence, right? So the personas here are analyst, modeler, the planner, and the admin persona. And based on the role, like we are essentially offering the skills for those personas. So if we dive a little into the analyst role, like one of the key things an analyst does is explore data. So let's dive into that. So the data exploration skill that we offer essentially allows our end users like these analysts to leverage natural language and conduct ad hoc analysis on vast amounts of data. And trust me, when I say vast, it is vast. So, um, how did this, this go about, you know, for us? How are we successful at it? Well, I'll tell you, it took us multiple tries. Iteration one. Let's get all this data. Let's send it to the LLM and have the LLM do the data analysis for us. OK, we ran into a bunch of issues with that. First, We ran into token limits, um, because it was a lot of data. 2, if we didn't run into token limits, it was still an expensive solution because we were sending a large number of token. And 3, well, LLMs aren't really very accurate with math, so, you know, this was not a good solution for us. OK, um, let's see what else we can do, and the team came up with another solution, so this was iteration 2. Token limits, we know how to solve it. We will chunk the data and send it to the LLM. That's exactly what we did. So, it was a multi-step process. We would chunk the data, send it to the LLM, get the output, and send that output and the next chunk of data to the LLM. And of course this was not a perfect solution again, because One is it introduced latency, um. 2, we did solve for the token limits, but the solution is still expensive. And 3, we're still not solved for accuracy. Why? Because at the heart of it. LLMs aren't great at math, but LLMs are really good at writing code. And so we figured that we would rather get the LLM to write code, Python code that can do the data analysis that we need, and then the LLM can leverage that output and, and be able to explain it, right, in natural language to our end users. So that's really what we went with. Now, why do we need a sandbox code interpreter, and what exactly is this? Well, um, show of hands for how many people have raised kids. OK, OK, we've got, we've got a few folks. Um, well, if you've got a high energy toddler at home, one way to deal with, with that toddler is to maybe create a playpen, put them with all their toys. They can create whatever mess they want to in that playpen, and the beauty is your fancy carpet and walls are not disturbed. So that's one way of doing it, and that's exactly what the sandbox code interpreter is. Think of your code like your toddler. It can go wild, it can create a mess, and you want to put it in a sandbox. So that's exactly what a sandbox code interpreter allows you to do. It is to ensure that your code can run in a safe, secure environment. Now, we evaluated a lot of different options. One is, should Workday build our own sandbox code interpreter? That's not the core of what Workday does, so that's not gonna be a good solution for us. Um, we then started looking at different frameworks and tools. Now, if you've been in this industry long enough, you've seen a lot of demos, and these demos are amazing because they tell you how to build agents in an hour, you can create POCs in an hour. But taking that POC from POC stage to production is a whole other matter, because you have to incorporate all the enterprise controls. You have to think about security. You have to think about like customer data, which is so precious. And so, um, we said, hey, you know what? We need, we need a good solution that's going to help us address it. And Agent Core essentially provided that solution. Like, that was the answer to what we were looking for, because one, it manages security for you. So as far as your code is concerned, that code is running in a very secured, like a virtual computer, right? The data that you have, your customer data is uploaded into that environment. And as soon as you're done, all of that data is wiped clean, and you're now starting with a, with a clean slate. So that was super critical. Second thing that the. You know, agent code code interpreter does, which is, is kind of a differentiator compared to a lot of the other tools, is that it handles stateful multi-step analysis. So for that entire session, it's actually able to do a multi-step analysis process to ensure that you get the most accurate results. And then scalability, we're actually able to send a ton of data to the code interpreter. Um, at least with what we saw last, we were able to upload anywhere between 100 MB file for like inline processing. And of course, we are compressing that file and sending it over, but you could also leverage S3 for it, where you can have up to gigabytes of data. So that's where we landed. Now, what did it take us to get this solution with agent core, code interpreter, in production? It took us 1 software developer, 2 engineering managers, and 3 days to get this from POC to something that can be production ready. And That is, is huge. Like, you know, how many folks can say that you can get something from POC to production in such a short span of time. That's exactly what we got with the agent code, code interpreter. And by the way, we just went live with our solution 2 weeks back, so customers are actually using this in production. Which is, which is amazing. So, let's talk about some of the results, right, with the code interpreter solution. What we ended up seeing is, I mean, yeah, we are adding some latency with code interpreter. Somewhere in the ballpark of 400 to 500 milliseconds. If you look at the 90th percentile, somewhere between 1 to 2 seconds. But what we gained is the token size reduced significantly. So we're almost seeing, like when we have these large payloads, we are seeing up to a 50% reduction in the tokens that we end up sending. And that's significantly reducing the cost for us. 2, the other benefit that we're getting is the accuracy. Like, we ended up getting much better results with this solution than, you know, what we had done previously. So this was huge. And 3, scalability, like, we now didn't have to worry about, are we going to be hitting token limits or any of that. The solution is scalable. And at the end of the day, like, what is it our customers are looking to do? They want to analyze large reports, vast amounts of data, and this is exactly something that's solving for it. With that, I do want to thank the close partnership we have had with the planning agent team, our own workday agent platform team, and AWS folks. Like this was a very strategic partnership that helped us to innovate at a really rapid pace. With that, I will hand it off to Salman. Thank you, Mira. OK, we're gonna be building an agent now, uh, disclaimer number one, we're gonna be seeing some code. Disclaimer number 2, you can definitely use AI to help you with any form of coding. I actually did use AI to generate this code. Third one, I gotta do this in 10 minutes. I'm gonna focus on the bare bones of that capability that I talked about. How do you bring that tool and agency within the agent? Where do you bring the LLM that does the reasoning? How do you bring in the query? And then I'll give some pointers around how you can start off but also strengthen that in order to move such a solution into production. Now in this example I'm gonna be showing how to build an agent using a third party tool from Tevili. So Tevili is an Internet search tool, provides a capability. I'll talk to you about how can you, you know, take that tool, integrate it within the code, and, and run it. First things first, what is the use case? That's important, right? And in this particular case, I am gonna be demonstrating a company research agent for sales team. This happens to be a popular use case that I'm seeing in production. Reason being that many of the sales teams, they focus on creating searching information from the Internet, creating these tailored briefing documents ahead of the customer meetings. So this is one use case that I've seen emerge in production and I thought, you know, like, create a simple bare bone capability to show how these are built. Next one, steps that we need to follow. Step number 1, we need to architect the solution, so we're gonna go over that. Step number 2, we need to set up the tools in the cloud environment. I'll briefly touch upon that as well. And then step number 3, we have to have the code and then run it, right? So step number one, solution components. This is an overly simplified version of solution components involved in an agent. You have the user that will be interacting with an agent executor, and that executor takes on the role of an orchestrator, right? Uh, it sort of determines where's the LLM that I need to sort of take a take advantage of in order to do that reasoning and the planning. What are the tools that have access to me and what does that mean? So in this case, in this example, we have two tools, one to represent information that is coming from the web and the Internet, and the second one is information that is residing within the enterprise environment. In this particular case, I just have a file sitting on an Amazon S3 storage, right? Uh, just to show how the two information can come and be combined, curated, and present a response to the user. Now, with each one of them we need to have some decisions around the technical capabilities. So with the agent executor there's a number of frameworks that are available. Um, Mira talked about land graph. Lang chain is one that is very close to that, comes from, from the, from land chain, uh, provider. So that's a framework we're gonna be using in order to orchestrate these capabilities from a reasoning LLM. We have access to more than 200 models from Amazon Bedrock. In this particular case, we're gonna see an example from anthropic cloud, uh, cloud. And then when it comes to the tooling, the agent core gateway is gonna be presenting us the option of connecting to Tivili's tool in order to search the internet, and the local data search is simply a tool function within Python environment. I may or, I think I've skipped that part for the interest of time. So we're gonna go over the example and I'm gonna cover how to now set up the tools and the cloud environment, and then we'll go into the code piece, right, and I'll walk you through what what each section of the code is doing so you can follow. So the first thing around the choice of the model, you can go to Amazon Bedrock, as I said, you've got access to more than 200 models. In this particular case, um, the use case seems simple enough, so a model like a Claw 3.5 IQ should be able to, you know, provide a decent result. Now within Bedrock, you have the Bedrock evaluation capability where you can actually test models side by side. And those are some of the best practices when you're moving and and executing on a proof of concept to be able to identify what is the optimal model for your particular use case for but here for the sake for the sake of simplicity, I'm going ahead with cloud 3.5 IQ model. You can select the model and then you can generate keys either short term or long term. So in this instance we're generating short-term keys and then we're storing these within the secrets manager as well as taking it and incorporating it into the um code development that we have that I'll show you later on. Second part is around the tooling. So as I talked about, you know, we're using Tevili's tool to search the Internet. You'll be able to access and get keys from Tevili.com. And then again, same practice with those API keys, you can store them securely within your Amazon environment in the secret manager and within Agent Core gateway you can register this. As it so happens within the Asian core Gateway, we have a number of native integrations. So one of the options that is the furthest to the right talks about some of the built-in integrations you'll be selecting that Tevili is one of the options tools available there along with. Some other third party tools you'll be able to select that, select your key, and you'll be gene you'll be. You'll be given And agent core gateway, URL and ARN that you will be using as part of your tool. So that's your cloud environment now set up. Now, let's move on to the code, right? So in the simplest form, you know, having that lang chain that I talked about and setting up the agent executor, it's not that difficult. So if you could just follow, you know, um, the Python code here, you'd be basically pulling in some of the libraries in this case lang chain. And then there are a couple of, you know, lines of code where this agent executor comes and and sort of orchestrate all the capabilities from the LLM, the tools and the prompts you create that tool calling capability and then you form an agent executor. And once you have this object, you can actually call invoke against that so then you can give a user's query in order to invoke this agent and it would operationalize and it would generate a response. Three parts are key, the LLM, the tools, and the prompt. These are the 3 parameters that are provided. So I'm gonna be talking about those 3 parameters in the subsequent slide. So this code highlights all three. The first one is the LLM. So in this case again within land chain, you have native integration with Amazon Bedrock. You can simply use chat bedrock in order to select the model and provide information around, you know, the region and its credentials that you have. So here you see the example of anthropic cloth 3.5 IQ being mentioned there. On the second one, you need to provide prompt. So we talked about agents following an objective. Here I have an objective and it reads such as you're a company research agent. Your goal is to answer questions about companies using available data sources. You have access to two tools, right? Number 1, search local documents, search internal company documents. For information, #2, search the web. Search the web for current company information. Fun fact, with Agent Core, you can even have a separate tool to say you can access, you know, inquire on the list of tools so you can get what are the tools of. Available passes on to this reasoning LLM so that it can actually in the subsequent stages determine out of those tools which ones to use. But in this case again I've kept it simple so I've provided those tools that are relevant to this agent directly within the prompt. And the 3rd 1 you see that we have tools as an array that talks about these two search local documents and search web. I'm gonna skip the search local documents, but again, I want to show you the code for search web so you can see how easy it is to bring all of this capability into a Python code and execute it. First part again, bringing some of those configurations that we have set up in a cloud environment in AWS environment from your AWS region, your S3 buckets are for the, you know, the, the local search that I talked about, and then the agent core URL and tool. So those are the information that we already had from the AWS console. You're gonna be able to take advantage of it. And then the search web that was, you know, mentioned in the array is basically a function, and this function is decorated as a as a tool. So then Python interpreter environment will be able to understand this is one of the tools available for that agent and within that, you know, we capture the name of the tool from the agent core service we're gonna be able to package this as a JSON RPC. Sign the query and then send it over to AWS for invocation. So in this case what you're seeing the payload is packaged up with some of the information around the JSON RPC information also having arguments and argument says query is query. The query parameter is what the user provides, and I'll show you the example, you know, that we have in, in the run, in the recorded run, and then post that you're gonna create this, encapsulate this as a request, AWS request, and basically send this request as a post operation. So that's it. This is when whenever the agent decides to call the search web, it would go through this. It would have the response and the response is being sent to the LM for further decision making as to whether he needs to further call tools or whether he needs to just curate the response and provide an output to the user. That's a decision that is left with the LLM to decide in this case. So now I want to show you how this comes together when you run it and in this particular case we need to have an example of the query. So the query here is how has AWS's approach to sustainability and green computing evolved, particularly focusing on their commitment to run on 100% renewable energy. So that's a query we're gonna be providing it to this agent, and we're going to have the agent run, and I'm going to show you the screenshots of the run in this case. So firstly, once you sort of run this, it would actually try to figure out, hey, do I have the information or do I need to go and use the tools that are available. So the green is the JSON output of what the LLM has determined. It's determined that I need to actually invoke the search web tool that I have available, and it's also formed a query that is different to the query that I have in order to determine what is it that I'm going to be searching the web for. So in this case it says I want to search the web with the information AWS Amazon Web Services, sustainability, renewable energy, 100% commitment progress. So this is the determination of the LLM saying I've seen the query and I'm going to be using this tool and the tool is going to be used with this argument, right? It's formed its own argument. This tool is going to be invoked through, as I said, Asian Core gateway and Tevili's tool and it reverts with information. Now you see a dense piece of text is because it brings back search results from the Internet relating to that argument, relating to that query. And in this particular case that you see there are information around the Amazon Web Services sustainability, renewable energy commitments, URLs so that you can have citations and sort of linkage to the original material as well as text. Now I'm not gonna read over this text, but what I'll highlight is ultimately the LLM processes information combined with the S3 information that it has, and it brings it and provides this raw format in JSON which can be, you know, processed further in order to have a much more human readable text. But in this case, the output that is provided is based on the web search results. I can provide a comprehensive overview of AWS's approach to sustainability. And green computing. Renewable energy commitment. AWS has achieved its 100% renewable energy goal seven years ahead of scheduled target in 2021. So I'm not going to read the whole text, but as you can see, three steps one, we provided the query. Step number 2, is determine which tools to call and which parameters to provide for each one of those tools. Step number 3, curated the responses and provide a response as an outcome. So next steps from here, this is the, you know, the core building blocks of building an agent, but this in itself should not be moved into production. So number one is around security and governance. So with Agent Core runtime, and I didn't cover this within, you know, today's session, you'll be able to containerize this code and push it into Agent Core runtime so it operationalized. So that's step number 1. Step number 2 is, as I've talked about these LLMs, it's important to place guardrails around the application. So having those guardrails implemented with bedrock guardrail is quite important. Step number 2 is in terms of performance and reliability. In many of these applications we see caching to be effective because there's a lot of repeat queries with the same kind of company names or information. So it very much depends on the application context and having reliability in terms of in terms of failing over properly in case a tool cannot be called or the answer does not exist. How do you recover from that? Lastly, Cost control. You need to have mechanisms in place in order to realize how much of LLMs are recalling, what kind of token-based pricing is being generated here per request, what about the tool calls that you have, all of those aspects would help you to strengthen this in order to move it into a production grade environment. With that said, I'm gonna hand over to Andre to talk about deploying a pre-built agent. There we go. Thanks so. So, um, you just saw how to build an agent from scratch, um, pick your LLM, pick your tools, do the prompt engineering, test that it's working, which is great when you want to have flexibility and when you have the granular control, uh, over the tool, but many teams don't want to start from scratch, and sometimes the fastest pass is to start with a production ready pre-built agent that you can get from a third party vendor. Um, I'll be using an, uh, CRM AI sales agent that is built by one of the vendors available on AWS Marketplace, um, uh, that can help you automate the routine CRM tasks. This is the fully packaged solution with defined action schemas with defined. Prompt engineering, uh, which has the recommended LLM already baked in and um it is uh already packaged and optimized to run on Asian core runtime and that means you can deploy it in minutes and I'm gonna show you in just a few slides that I'll give you, show you the demo of how that's gonna be done. And this is the power of pre-built, um, you inherit all the vendors work and they're kind of like testing, making sure that all things and components work together, but it can still run securely in your environment within your VPC. Getting started experience is extremely simple. It is just 3 steps. You first discover the agent or the solution, uh, based on your business needs. Uh, we have the new agent mode on AWS Marketplace where you can describe your business problem or attach to your internal, uh, um, requirements doc or your, uh, plan for what you're trying to build, and we'll give you the best optimal solution for that. The second is subscription flow, which is a single click. This particular tool is free, but it still requires you to accept vendors' terms and conditions. And finally, is the deployment process which is almost as simple as the single click. I'll talk about it a little bit more. So, um. Marketplace has standardized the framework uh for how to package, how to build and package those containers, and they come with vendor authored instructions and all the configuration and require users to provide only, only minimal inputs. In this case, uh, users to deploy the agents need to provide the environment variables, uh, such as the, uh, CRM access credentials. And uh what's important about Marketplace is that it acts as the um supply and chain guard rail. Think of it this way all the applica all the agents that are containerized applications are stored in marketplace private ECR repo. That means that vendors cannot just swap the image when whenever they want. All the images that they provide. It goes through the rigorous scanning process and they are behind the entitlement enforced access so only users who are entitled to use this product can read them and the vendors before pushing the new version have to go through the full publishing flow full the security reviews. Um, once it goes to the agent core runtime site, that's where you get all the benefits of running these agents within your VPC. So you get IM roll configurations where, um, you start by either creating a new IM roll or selecting existing IM roll. If you have one already where you define the minimum required set of permissions for this agent to operate properly, which usually requires access to bedrock, LLMs, access to the tools you're using, uh, or other internal resources that are required for the agents, um, the secure runtime environment was an agent core, um. Gives you the full control. The agents, uh, would never leave the boundaries of your VPC. It's isolated from inbound, uh, requests from the Internet unless you allow that, and you remain full control, um, over the what agent does and how that operates so marketplace. In this case ensures that the trusted delivery of the agent while agent will run time enforces secure uh governance of the agents at the execution step. So just to summarize, here is the um uh flow if you want to get a third party pre-built agent again it's very simple. You subscribe, you find a marketplace, subscribe, you provide the uh environment variables in this case CRM credentials you deploy it on an agent core runtime and you invoke the agent. So let's see that in action. I got a short pre-recorded demo just to put all this together. Uh, we're starting on the marketplace website and we go to the new, uh, search experience. Uh, where you can describe your problem in a natural language. In this case, I'm looking for an AI, um, agent to automate my routine, uh, CRM. Um, actions. And as you can see, I'm typing very slowly. Um, It will sync for a few seconds. You can also upload a document with your requirements, uh, if you need more than precise recommendations here, and you will see a few options with the journey I recommended summary, and we just, um, launched a new version of that experience where you can also create an AI build comparison of different tools so you can compare them across different dimensions. Once you identify the tool that works for you, you go to the product detail page where you can read all the information about how the tool works and also see the deployment instructions, uh, including the, uh, tool, um, or in this case agent schema and the all the inputs that are required from you, uh, to make sure that this agent operates properly. Once you're ready, you move to the purchase step, uh, where you review pricing and terms and conditions. This is, this tool is free, but you still need to hit the subscribe button to make sure you agree with vendor's terms and conditions. Once you're subscribed, you can proceed, proceed to launching the software. And this step there are two options. You can either proceed with CLI if this is your preferred method, and we will provide you um a few pre-built, pre-generated prompts for the CLI. The first step would be creating an IM roll. Let's see. Here it is. So the first step will be creating an iron roll so you can copy that, uh, uh, and execute this commain attaching permission policies and, uh, finally actually invoking the, uh, sorry, deploying the agents with the container image provided by the partner. Or alternatively, uh, you can get the same uh work done directly in the agent core uh console which we'll be using for, uh, for this demo. An agent que console, again, your inputs are fairly simple. Provide a name for the agent, it's uh pre-populated already, but I prefer a different one, let's call it a Salesforce agent. Uh, the ECR containers URI is already pre-populated from the marketplace, the solution that you have just purchased, you select or create a new iron roll. I have one already selected and, uh, I'm almost ready to hit the hosting engine but not yet. I need to provide the environment variables. In this case, I need to provide, um. Uh, 44 keys with respective values, and that's my CRM username. This is my, uh, CRM, uh, passwords and. Uh, the token and the domain again this information never leaves the boundaries of, uh, your VPC. This information is never shared with the vendor, and, uh, you can also store those credentials in the secrets manager and instead of providing environment variables here you can reference them from the agent directly, which is much better for the production and when you need to do the regular rotation. So now let's test this agent and see if that works. I'm asking to give it to me to provide me a summary for the specific account, um. And let's see. We'll take a few more seconds to sync and here we go. So here's an example of an agent that can help automate uh the routine work of your uh sales teams and there are multiple ways how we can plug in this agent into your own individual workflows with all the data never leaving, uh the boundaries of your uh VPC with you having the full control but at the same time with. No, uh, no building, no coding, and no, uh, prompt engineering. The one thing I also want to mention this particular agent built by partners built using strengths framework, um, which already has Bedrock LLMs baked in. So in the previous example you saw how we need to pick an LLM, how we need to configure the LLM access. In this case you don't need to do any of that. It's kind of comes prebuilt. And with that, that's uh the end of our presentation. Uh, we'll be wrapping up in a second. Please join me on stage. Um, we are running a little short on time, but as soon as we wrap up, we'll, uh, be around here for Q&A. Thank you Andre. Despite his slow typing, I'm still impressed, you know, getting an agent into production in 5 minutes, uh, it's pretty impressive. So what we saw today was AW's capabilities, uh, you know, providing pre-built agents, tools, models, infrastructure, and expertise. Generally, the best way to go about this is to identify a use case and start this flywheel cycle, right? Uh, what is the use case, what is the business pain point or opportunity, identify how, whether we should be building or buying or partnering, and then, you know, create this fly flywheel effect that you can improve over time. There are some resources that are available and and uh this recording is gonna be available as well so uh we've got information around some of the sample code with the agent core but also some training material to improve your skills with our agentic AI offerings and lastly I wanna close off with some of the relevant sessions that are also covering similar topics at Reinvent. Thank you so much for being with us. Thank you. Thank you.