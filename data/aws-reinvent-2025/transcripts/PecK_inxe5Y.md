---
video_id: PecK_inxe5Y
video_url: https://www.youtube.com/watch?v=PecK_inxe5Y
is_generated: False
is_translatable: True
summary: "This session, \"It's About Time! Improving Distributed Systems with Amazon Time Sync (CMP409),\" explores the critical role of precise timekeeping in cloud computing and introduces advanced capabilities of the Amazon Time Sync Service. Speakers Josh Levinson (EC2 Product Manager), Julien Ridoux (Principal Software Engineer), and Nikolai Larbalestier (SVP at NASDAQ) discuss how accurate clocks can simplify distributed systems by reducing communication overhead and allowing for better event ordering. They detail how AWS achieves microsecond and even nanosecond-level accuracy using dedicated hardware—atomic clocks, GPS antennas, and the Nitro System—bypassing traditional network jitter issues associated with NTP.\n\nThe presentation highlights a significant customer use case with NASDAQ, who leverages these \"hardware packet timestamps\" to reorder financial transactions with nanosecond precision in the cloud, a critical requirement for moving high-volume, low-latency exchanges to AWS. The speakers also introduce \"ClockBound,\" an open-source daemon that provides applications with a \"window of uncertainty\" (error bound), current time, and clock status in a single API call. This tool enables developers to build consistent distributed systems (like CockroachDB and YugabyteDB) with fewer retries and better conflict resolution. The session concludes with the announcement of a new, cloud-native ClockBound daemon that acts as a standalone time client, further improving accuracy and reliability for EC2 instances."\nkeywords: Amazon Time Sync Service, ClockBound, Distributed Systems, Nitro System, Atomic Clock, NTP, PTP, Hardware Timestamps, NASDAQ, Cloud Computing\n---

Good morning everyone. As builders and as humans, we know clocks are important. Today we're gonna cover why, why they're important and how they're good in the systems and technologies that we build. This is CMP 409. It's about time. Improving distributed systems with Amazon Time Sync. My name is Josh Levinson. I'm a principal product manager for EC2. I'm joined by Julienne Ridou, who's a principal software engineer. And Nikolai Larbellestier, who's a senior vice president of cloud strategy and enterprise architecture. At NASDAQ. Today we're going to go very deep. Into clocks. Making a clock, what is a good clock. Leveraging and using a good clock, and then using good clocks globally at planetary scale on AWS. I want to remind you that this is a 400 level talk. You do not need to understand the concepts we're gonna go through at the start of this presentation. Towards the end, we're gonna share how we simplify things and we've done the heavy lifting for you so you could take advantage of this in your systems or using AWS partners systems to use these uh these definitely tricky concepts in your workloads. I like starting with a quote, and this one from Barbara Lipskovoff, synchronized clocks are interesting because they can be used to improve the performance of a distributed system by reducing communication. We're going to get back to this. How clocks reduce communication and how clocks make systems more simple when you take advantage of them. Starting just with a baseline before we get into what a clock is, is what clocks are used for. I think we all understand clocks as a common scale of measurement. Uh, and historically distributed systems have not been built to trust clocks. Clocks are used for human observability, that's log messages, metrics monitoring, uh UI dashboards. We think of sports, how much time is left in a period, or, or how much time it took for someone to run a race. Auditing another place that I think we're all familiar with clocks uh in industries like financial services, health care, broadcast media, uh, online gaming, making sure events happen uh synchronously at the same time. Then in the last quadrant, I have distributed systems with a question mark. Again, going back to that quote from from Barbara Liskoff, where if you have a a reliable clock, you can simplify your systems, reduce locking, leader election, reduce those communications, and we're gonna answer the the how in this presentation. To start off I like to talk about just what is a clock. Julian, could you share what is a clock? I will try my best. Thank you, Josh. Um, so at the beginning of this journey, we really wanted to start with some of the basics, which is how to build a computer clock. And at this level you really need two things, very simple things, one thing that ticks periodically and one thing that counts this number of cycles. Usually this is a piece of hardware that is combining these two functions, but again, something that ticks, something that counts. If you have this information, you can then build a clock, and that's represented by the equation at the at the bottom of this slide. At a perfect time T, you could read the clock C here by saying that this is the number of cycles that have elapsed, divided by the frequency and some constant. Sounds a little bit esoteric, let me make it simple. Imagine that you have counted 1000 cycles. And you know that your hardware is doing 1 cycle per second. Well, 1000 cycles, 1 2nd each, 1000 seconds have elapsed. You have now built a clock. This is the fundamental component that allows you to build a clock on a computer. All right. And just talk about some hardware. What does that look like? You have a range of options from the left hand side here to the right hand side. It ranges from small form factor crystal oscillators to equipment that you will find in a physics lab, caesium beams, hydrogen masers, and optical clocks soon to be released. In terms of form factor, you range from a few millimeters to things that are pieces of equipment that are the size of a cabinet. Even these ones are a bit more difficult to fit into a computer. And of course the price point changes as well. On the left hand side, it tends to be a better price point for cloud computing. On the right hand side, this can range from hundreds of thousands of dollars if you want to buy this piece of equipment. So we have trade-offs in terms of small form factor or not, in terms of price points, but also in terms of the quality of the hardware you can purchase to build a clock. The ability for hardware to keep accuracy to within one microsecond changes around that scale. On the left hand side, you can hold one microsecond accuracy for about a couple of seconds. On the right hand side, when you go to hydrogen measures and optical clock, you can keep accuracy for years, if not centuries, or what's the promise. So you have a range of hardware that you can choose from to be the clock in the computer. But no matter what decision you make, all oscillators drift. Even the most accurate atomic clocks do drift. Just that it will take them centuries for you to observe that drift at a microsecond level. In general, we tend to have a small form factor, a good price point, and these oscillators, they drift due to manufacturing process or aging. These are mostly considerations that are of importance for physicists. For us, cloud providers building computers, we mostly care about small differences in power supply or temperature variations due to the impact of your workload on your processor, CPUs, and the server as a whole. So, I did lie a little in my previous slide. I gave you an equation that said you can't build a clock if you know the number of cycles and you know how long each cycle is. But the truth is that the frequency of this cycle does change over time, as is shown in this equation. And the net result is that clocks drift, and we are showing you an example in here with this graph. Here we picked an EC2 instance that's been running for about 5 days, and we let the oscillator free running, no correction applied. What you see is that over a period of 5 days, the clock has been drifting by about 75 milliseconds, which is something you would expect. All right, so we have the hardware, we know how to build a clock. We've observed that the hardware does drift over time. What can we do about this? Well, we need to track and correct for that drift. How does that work? Well, computers are no different than us humans. Imagine you have a wristwatch and that watch is not on time. You will ask a friend, Josh, for example, asking what time it is. Josh gives you the answer. You can adjust your watch, you're good to go. Computers are no different. Here we are representing a small example that illustrates the network time protocol that has been in use for decades now. Effectively, you have a clock that's the bottom line on this graph that needs to be adjusted. The top line is a server clock, a reference clock that you will have over the network, and the client clock sends a message asking what time it is. Server resist time, sends a response back. In this late example, the clown can then have all the information it needs to adjust its time, realize that it's running 2 minutes behind, and will correct its clock and it's good to go. But is it? The reality is that the frequency changes all the time. You cannot ask what time is it only once and be done. You have to continuously ask what time is it repeatedly. So this operation repeats and this communication over other networks then sees messages being exchanged with slightly various delays and of course the risk of failure when messages are dropped. All right, we talk about clocks, how to fix them, how to track the drift, track drift and create them. So why is that, uh, why, why is clock synchronization a hard problem still? The short answer is that the network is part of the problem. This communication between a clock to be adjusted and your trusted Fred, who can give you the accurate time, is problematic for two reasons. First, on the left-hand side, we'll show you an example. The the first reason is that the viability and the delays of communications have an impact. Here, the blue line shows you the, the, the noise of the communication delays when a server repeatedly asks what time it is. And the hard problem is to identify this purple line here which is actually the cloud drift from this very, very noisy blue input. That's one key difficulty seeing through the noise, seeing through the viability of the communication delays. The second problem that happens with the network is that the network is not symmetrical. So far I've shown an example where you can adjust your time, you have an exchange, and as you can see on the top graph, you have a nice triangle that is very symmetrical. Imagine now that these two clocks are perfect. Imagine that we have achieved perfect synchronization. On that first triangle graph in here, it's great. Time is being exchanged, two clocks are in agreement, they are perfectly synchronized. Second example, clocks are still perfect, but this time the network is not symmetrical. When the client compares the time from the server to its own time, it will observe a difference. That difference is not that a clock error, we have assumed the clocks are perfect, it's just a network asymmetry. But from a clown point of view, it's ambiguous. It cannot make a difference between the clock error and the network asymmetry. All right. So this is a, a very fast track view about what the problem is. And a few years ago, we really wanted to be good clocks for the purpose of synchronizing our cloud. So we went through this analysis, what clocks are, how they are made, and what the problems are. We knew the problems of tracking cloud based over a non-deterministic network is not something any of our customers wanted to have to worry about. So we put ourselves to work and I'm going to let Josh take us there. So Julian just covered a lot for 9 a.m. on a Wednesday, or if you're watching this recording. And again, as he said, the good news is you don't need to know that we've done the work for you. We started with the Amazon Time Sync Service, which was a a NTP network time protocol endpoint, a link local for every EC2 instance worldwide. We invested in engineering and hardware to support this service globally, and we did this in 2017 for our customers. I want to start by showing you what that looks like. So here's an EC2 instance. Uh, you have the, the nitro system down below, the instance up above, uh, crony D or or your time Damon in your software. And what you have is a NTP packet from our distribution system uh through the nitro system to your server that you can use to correct your clock. The accuracy for our NTP service is under 1 millisecond. It's really in the the 500 to 700 microsecond range. And customers were happy. This is easy, it's hands off. Again, you don't need to understand the science, the technology, the methodology behind doing this, it's handled for you. But customers have asked for more, and when you really want to use clocks in distributed systems, you need to get much better than one millisecond. And we challenge ourselves, how do you get to nanoseconds? Other cloud providers uh and scalers try to do this with software. Uh, and there are different ways that you could go through with uh lucky packets, and very clever algorithms and designs to get close. We found in testing, you really need to invest in hardware, all the way down to the servers to get to nanoseconds. And this is how we did it Here's an animation of what it looks like, but we have dedicated cables from GPS, uh, redundant atomic hardware clocks, and we send a signal, a reference signal, essentially a GPS signal to your EC2 instances. This is not over the EC2 network and the AWS network. It's not over the data plane, it's not over the control plane. This is entirely dedicated for clock synchronization. And I wanna show what that looks like again at the instance, going deeper because this is a a 400 level talk on what the infrastructure looks like. Cause we really took the network out of network time synchronization and everything's hardware based and nitro-based. It's a great story about uh nitro and the uh nitro system. So what does that look like? You now see there's a nitro clock, hardware reference clock. And if you're using NTP your NTP packet, the network time uh protocol, does not go over the network, it's just directly to the nitro card and back. We've also added a PTP hardware clock directly in the nitro system. And the way that that works for you. Is you read the, the PTP hardware clock, the PHC. And you can collect your system, correct the system clock, uh, right there. So as you see, there's no network communication. Uh, we give the, the PTP, the precision time protocol for you because that's what most software and systems are familiar with. But as you can see, there is no back end, there's no PTP distribution within AWS. It's all local to a reference clock. That is how we get in the microsecond and nanosecond range. When you're using the Amazon Time Sync service, this way, uh, NTP is gonna be under 100 microseconds at the software level in your guess, and then PTP is gonna be in the range of around 20 microseconds, again in the software layer at your guess. And then just to to recap, this is built on the nitro system where we've added these reference PTP hardware clocks. What's really cool about that is we add them everywhere. It could be built on Graviton, on Intel, on AMD, on our, our latest GPU servers as well, because again the base is the, the hardware of the nitro system and this large infrastructure investment we're doing worldwide to support this feature. Now if you're paying close attention, you might realize I talked about nanoseconds, but that in the software layer, you're in the microsecond range. And our customers have called us out on this too. Said, hey, if you have these nanosecond reference clocks, how do we get to nanoseconds as well? Uh, Julian, can we share how we go further in getting to the nanoseconds? Thank you, Josh. Yes, we can definitely talk about one of the key applications. So far we went through explaining or describing what is a clock made of, how to correct for it, and the system we built at AWS to have this level of accuracy in here. One of the key applications that our customers were really interested in is the ability to do hardware packet timestamping. This is a feature we've launched earlier this year and has a lot of benefit, first of all from observability. If you count time some packet closer to the wire, you get a more accurate view about the network performance, which is very important if you are running a distributed workload. Some applications also are very interested in hardware packet time stamping because it allows to order messages that you are receiving in your server. That facility is built in in the driver and so you can access it today readily. It provides nanosecond resolution on received time stamps, for instance, for all the network packets that your server is involved with. How do you access this facility? You can access it through the classic socket API that most applications are built over or the DPDK toolkit that allows to bypass kernel infrastructure and get direct access to the packets down to the wire. So with this facility, you get the benefit of a very accurate clock and gain a lot of insight into how your application operates over the network. This sounds a little bit abstract, so I want to take you through the example of the before and the after. Here on this slide, I'm going to illustrate what, how would you do the same things before this feature was released. So we are first looking at the application of time stamping packets, messages you receive at the application layer. Let me start with a simple animation. Here we still have the nitro system that underpins our virtualization layers and the instance that runs on top. A message arrives, passes through the natural system, goes through your instance. The application receives the packet, reads the time from the system clock. You now have a time stamp attached to your message. How much work does it take to do this? Well, on the right hand side you can see, it takes about 4 lines of C code in this example. You open a socket, here I'm using the socket API as an example. You receive the packet. Once you have received that message, you can read time. It's relatively simple. Let me now show you how you would do the same thing by using the hardware packet time stamping facilities. Again, the nitro system is here, but now we have the nitro clock that Josh described prior. Again, let's go through a little animation. The Message arrives. This time the packet is time signed by the natural system and the nitro clock before it reaches your instances. Then the message and the time stamp credit for you is then passed to your application up to the layers. On the right hand side of the slide, a bit of an illustration around how much, how much work does it take. Well, a little bit more. You can see the number of lines of code has increased. You still need to create a socket. You still need to receive a message, but you have to do a little bit more work to process controlled information, extract the time stamp that's been creating for you. Nothing catastrophic, but just a little bit more work. Now, what do you get from uh doing this extra piece of work in here and to access times on the created by the natural system. So here we have a little illustration. We have two instances pinging each other across an availability zone, and we're recording the round trip time of these exchanges. We have 2 graphs, 2 lines on this graph, but they are actually measuring the exact same thing. They are both measuring the round trip time between these two instances. The line at the top in blue is if you measure this round trip time using user space and your application. The first example, the B4. As you can see, we're in the range of 400 microseconds of round trip time between these two instances. The red line is measuring the exact same thing, but this time using the hardware packet time stamping offered by the natural system. And this time we're in the range of about 100 to 150 microseconds of one trip time measure. We measure the same thing, but our yardstick is much more accurate and much precise. The difference between these two lines is around 20 250 microseconds in that example, and this is the amount of time they spent into your operating system, the network stack, and passing this information to the user space. If you look at this example, this represents about 2/3 of the round trip time that is actually the time spent over the network. So you get a much more accurate view of what's going on. The observability of the network performance is becoming much crisper, and you can start identifying and breaking down what are the delays between your application and the network. All right, I'm gonna pass that back to Josh now. Thank you very much. Thanks, Julian. With this technology, we want to give examples of why this matters and and how this is used. I'm very happy to introduce Nikolai, the senior vice president of cloud strategy and enterprise architecture at NASDAQ, to show how they are leveraging precision time with the Amazon Time Sync Service. All right. All right, thank you, Josh. Thank you, Julian. So building on all of that excellent background and work we're gonna walk you through a little bit about just background on who NASDAQ is but how we're using this technology to demonstrate progress towards our goals of running markets and public cloud. So first, a little bit of background. NASDAQ has is a is an innovator. We were the world's first electronic exchange, uh, created in 1971. We, uh, operate more than 30 markets around the world in the US and Nordics that covers stocks, bonds, and, uh, options and other derivatives. We are known as the home of technology stock and technology stocks globally. And uh we host the top 4 companies by market cap, of course, including Amazon. We were proud to have as a listed company. We also provide technology to more than 130. Market operators, so exchanges, clearing houses, regulators around the globe. And, and provide technology to more than 2300 financial institutions that could be for managing surveillance, anti-financial crime, or capital flows, reporting. Number of other things and more than 6000 corporates we provide technology to to support their journey in both private and public markets through the full life cycle of capital markets. We have, we have a listing venue and we have approximately 44,400 public companies listed on NASDAQ exchanges globally. Now we have been an innovator in cloud and we started our cloud journey. Well over 10 years ago now, I think probably close to 15 at this point. But we'd had approached it very carefully and very thoughtfully. Uh, and so what you see here is a kind of representation of We started from the from the edge out. We started with our data repositories, data warehousing, so we had our first data warehouse in. 2012, 2013-ish. Created in natively in AWS uh using Redshift. We've continued to evolve that we now store about. 60 petabytes of data in that data warehouse and that underpins a number of key functions, mostly back office in this case that support the operation of our exchanges including billing and regulatory reporting, but we didn't stop there we continued our journey into more and more real-time processing. Started with things like. Our market surveillance, taking streaming, uh, real-time market information to perform that. Market data distribution is now available through. The public cloud, you can subscribe to NASDAQ products and a number of alternative data sets. And of course in 2022 we announced and we launched our first options exchange on AWS Outpost NASDAQ MRX and we've continued that cadence and we now have uh. Uh, 6 plus market systems running, uh, on outpost, so we continue that journey. In addition, a number of our critical surrounding systems that provide real-time insight to market participants on the, the trading activity that's occurring on the market. Those are hosted natively in the public cloud and we take advantage of the scalability and the performance, the right sizing. To process the large volumes of messages we handle and, and the peaks managed capacity. So of course we do have some specific challenges in moving to the cloud. One of those is that in particular in the US there's a very specific Geography in which trading occurs, and that's predominantly in northern New Jersey for equities and equity options. So that provides some constraints on where you can deploy the compute because there's latency uh dependencies between those exchanges uh that are all hosted. In that general area. We also have ultra low latency transactions and uh by that I mean something on the order of 20 microseconds measured as what we call order to act so that's the time an order comes into our system, gets processed, booked in the match, potentially trades and returned to the customer. And coming with that, we also have large volumes, um, well north of 100 billion messages a day currently. Uh, with, uh, any, any specific exchange system processing up to 2 to 3 million messages a second. We also have high resiliency and uptime expectations. The capital markets depend on, on us. We are a primary listing venue for a number of, uh, a number of public companies as mentioned earlier. And us being there to provide that liquidity, provide that venue for, for people to trade and, and do, uh, generate price discovery is a key function of how we keep our, our capital markets efficient and operating well. And of course we do have regulatory oversight of what we do, where we run things, how we run things, and what third parties we're interacting with. So we put all these together. And I think you start to see the picture of why we've been very intentional over the past 10 to 15 years continuing to move. Progressively to more and more real time and into the match. OK, so now we're running these exchanges. And many of our support on outposts and many of the supporting systems in public region, but we don't wanna stop there. So this is where Amazon Time Sync comes in and I wanted to talk through some of the, uh, some of the prototyping and experimentation we've been doing to demonstrate some of the capabilities that will enable us to deploy markets into the public cloud. But first, a short primer on an exchange system. So very high level architecture view. Of an exchange trading system. There's really 3 primary components. Before I go there, I should mention that we operate 3 equities exchanges and 6 options exchanges in the US. And as I mentioned before, more than 100 billion messages a day. With a single exchange out of those. 6 options exchanges running north of 30 billion messages a day. And as mentioned, the median latency is about 20 microseconds for that. So, If we look at The components of the system here. We have of course at the top left as you can see the matching engine and that is where the primary logic to book orders and trades and match buyers and sellers and generate, uh, generate trades occurs. Going one level down, we have what we refer to as order ports, and those are the primary point of interface for customers who are sending order flow to our exchange. And act as a point of validation. Uh, and in some cases, pre-trade risk and other, other functions occur, occur there. Next to that on the right we have our what we call our public market data feeds. So these are, this is a published view of what's happening in the matching engine to the market, uh, NASDAQ I, for those of you familiar with it. And then of course uh at the bottom, but, but maybe the driver for the whole thing are the clients that provide the order flow to the order ports to the matching engine through their inbound inbound transactions for round trip through the system. So, With that background. An exchange trading system on-prem is a traditional model. It's a model we run today and most other exchanges in the world. You wanna provide a deterministic and statistically fair market to participants. And so how do we accomplish that? Well, given that it's an on-premise data center, there's cer certain physical characteristics we can rely upon. That is the lengths of fibers in the data center. The number of switch hops between components of the system and customers. And those drive what see what you see here as D1 and D2. The primary sources of network latency in the system. And then D3 encompassing both the network components and the processing components, D3 being one half of the round trip. So you can think that, think of that as about roughly 10 microseconds. So because of those physical characteristics and the sequential processing of the matching engine. You result in an orderly processing of orders that are processed as they're submitted by customers and returned. To customers and public market data feed in a consistent and repeatable. Time period. So, that doesn't work so well when you're talking about public cloud obviously. Uh, we definitely do not have fixed cable lengths. We definitely often don't have any idea really how many switch hops sit between nodes. And there can be various other things happening in the network. There can be other traffic that's, that's, uh, delaying packets or just the placement of compute nodes can be on, you know, uh, it can be unpredictably, uh, from with even within the data center can be on one side or the other or it could be in across AZs or it could even be across regions potentially. So what do you do about that? Given that we, you know, can focus on these, these three kind of key time periods here, of D1, D2, and D3. This is where the time stamps come in. So using hardware time stamps to Record the time of each transaction at each point through the network. We can then Use this high precision distributed time source. To understand when orders came into the system or or were sent by customers. And hence then when we should process them to ensure the sequential processing and sequential distribution of those messages. So if you look here at the kind of key components that we, that we have that are relevant, right, so We have our RVs or reordering buffer, uh, at the top left sitting in front of the matching engine, and it's important to note that this, this POC is really focusing. And what we're discussing here is focusing on the ordering of inbound transactions. Now there's obviously a similar problem domain for. Or a network flow going the other direction. Similar techniques apply, and I think you can use your, your imagination to see how that would expand, but that's not, that's not the specific focus of this POC that we did. But using that reordering buffer and the hardware time stamps that are provided uh through Nitro and through the uh time sync service, we're now able to, to determine. For example, T1 being the time of an order being received at the order port from client 1, T2 being the time an order was received at the order point from port from client two. And T3 being the, the time of an order received from client 3. Given that those are hardware timestamps with, with nanosecond precision, we can use that information to define a time-based boundary. And using the reordering buffer that's that that understands those time stamps. To put them back in the or ensure that they're in the order that they originated from. Now this is, this is obviously a specific challenge. And uh we'll talk just a little bit more about what that means. So how, how they're used, right? So if you think back to the prior slide with, excuse me, with uh the T1, T2, T3. In this case we have on the inbound side order T1. Flushed at time T1. Or received I should say at at time T1 gets flushed at time T 11. So we have T1 and at time T3 we have order T3. And at T4, we have order T2. As a result of the reordering on the outbound side, we see it as order T1, order T2, and order T3 reordered to their originating timestamp. So this is a clear and we have a we have a patent for this technique uh to use time for this. This is something we've been working on for a number of years and we see as foundational to continue to evolve our posture towards public cloud for these kinds of very sensitive low latency systems. So what do we do this with? So if I go back to the prior slides, so just thinking about the, the. Prototype that we built with this, so we're running on uh On AWS public instances. Using those were M7I 2 XLs. Using proximity placement groups to ensure that the compute nodes were placed close in relation to each other within, within a, uh A few racks were within the same rack. And then using these hardware time stamps to reorder those packets. So what we have here, if you look at this, this example and you take these time stamps and we in in our testing example we use two time boundaries to demonstrate the impact of this. One was a 5 microsecond boundary and at 5 microseconds, as you might imagine, you're not successful in reordering all that many packets because that's. That's probably more if you will take many packets much longer than that to arrive at the matching engine, so they fall outside the reordering boundary, but that's about, let's say roughly that was about 25% of packets got reordered successfully in, in that time boundary when we move it up to 50 microseconds. We see Depending on the message rate up to 84% or higher, right. So we continue to iterate on that boundary, work on how we can, we can continue to evolve the proximity placement functions so that we can get greater granularity of where to place them. And uh leveraging the Amazon time sync service to accomplish that. So if we look forward with, with this foundation, and as I mentioned earlier, uh this is a key part of our innovation and working with, with AWS to. Solve problems that are hard for public cloud as we believe that many of these problems will be solved or will get migrated to the cloud in the future, we want to be part of that solution. So this is about for us a developing scalable trading systems, continuing to ensure that our systems are the most scalable and performance. Systems wherever they are deployed if that's in public cloud or on premise. We know there's a lot of innovation happening in public cloud and uh some things like the, the explosion of of AI and and how we're gonna apply that and certainly, you know, bringing your data to public cloud is a key part of that strategy. Enhancing our security and reliability. So as mentioned before. We have High expectations both for from ourselves and from our clients and from the regulators in terms of being available. And ensuring that the capital markets continue to, to flow. And public cloud has significant advantages in both of those, uh, in that, you know, you do have to understand the resiliency considerations and make sure that you're building with those in mind they're not the same as on-prem but built. Built responsibly and in the right way, they're much more resilient and secure. And of course part of the, the, the final part point there is that. Leveraging our strategic market expertise, our, you know, operation of 30 exchanges globally and as a supplier to 130 marketplaces around the world. We are a leader. We need to think strategically about this and how we apply technology. To this. So, thank you. Thanks for the time to Josh and Julian for the time sync work and uh. We look forward to future innovation here. Thanks. Thanks, Nikolai. I love that example in technical depth, and we get really deep into the hardware world and hardware timestamps. But most people are not building a financial exchange like NASDAQ. And we wanna get into how you leverage really good clocks in your systems globally and at scale and back into uh the software world. I'm going back to this quote from, from Barbara Liskoff on. Distributed systems and reducing communication. Cause we now have a very good clock in the nanosecond range, where in the software world you're in in microsecond range. And we have the hardware packet time stamping. This allows your applications to be better. You can measure your network latency. You have monitoring and observability in the micro nanosecond scale and consistent point in time snapshots. But we still haven't talked about distributed systems, and distributed systems are all about ordering. Uh, distributed systems have multiple clocks. That's why it's a distributed system. So in, in the first example on the left, if you have a system with 1 clock and 3 events. It's relatively easy to know what event comes first. But as Nikolai pointed out, when you have multiple servers. They each have their own clock, and each clock is a little different. And as that system scales, it gets more complex. In the past It's been difficult to rely on the clocks because they were not good enough to compare. So things like distributed locks, a leader election, consensus algorithms built by decades of very smart computer scientists, Turing award winners, uh. Have been done to do ordering on a complex system. But these algorithms put a limit on the performance, and as the system grows larger, uh, so does that complexity. In order to use a clock, we recommend, you know, changing the way you read a clock. Uh, I look at, uh, you know, Julian's example, asking a friend for a time. You ask what time is it? And we think as, as humans, as people, it's, it's 2 p.m. But no clock is perfect. And each clock has an uncertainty, so if you're using our precision clocks in the software layer, when you say what time is it, you need to think about, oh, it's 2:00 p.m. plus or minus 20 microseconds. And perfect time must be within that uncertainty window. And, and you have to factor in a margin of error in your software application. I'm going to go a little deeper into why. And then Julian will show how we simplify that so that you could do it without needing to understand these details, but I think it's important to share these really deep details. Uh, here's a, a, a simplified example of a bank account. Let's say you're starting with a balance of 0 and there's two places where there are transactions, maybe a deposit of $10 at the bank. Uh, on node two that could be a, a direct deposit electronically, uh, and then a withdrawal. Looking at those confidence intervals, you know that event 1, that first deposit occurred before the other two with absolute certainty. But processing the other two is really important. If you get the order wrong, you might have an overdraft. You might not have enough money, funds in your account to withdraw. So Julian, knowing this, how do, do we, how do customers know what these intervals are? Well, that's actually a fairly complex question, and we thought we would start by an example, right? We know that no clock is perfect, and so we need to estimate the error of this clock to build this window of uncertainty. So let's start with this example here. If you were to start an EC2 instance with a default configuration, you could inspect your time synchronization demon to look at the sources it's using to synchronize your own clock. I'm going to start from the bottom of this output in here, one row at a time. The bottom line represents a source that is over the internet, NTP servers on the internet, for example, time.aws.com will be a source of time here. The second rule from the bottom up is accessing the local NTP servers offered by the nitro system that described before. And last, the row at the top represents the PP hardware clock, which is our enhanced Amazon timeing service in here that is provided again by the natural system. What matters in this output, lots of numbers. What matters in there is the column on the right hand side. That is what the time synchronization Danon believes is the size of the window of uncertainty that Josh was describing. Again, from the bottom, if you use an NTP source over the internet, in this example, 395 microseconds is the width of that window. If you use the local NTP sources, it shrinks to 90 microseconds, much better, much more accurate. And lastly, if you use a PTP hardware clock, the tandem on here announces 32 nanoseconds as the size of the window, something quite small. But the reality is that there's a little bit more to it, and we, uh, Josh, myself and the team behind the Amazon Timeing Service knows that we have engineered the system from the ground up. It turns out that there's a little bit more to know behind this number than just what is printed out in this output, and I want to give you a bit more information and details about what that is. But the reality is that getting this information is extremely complex, and we cannot ask any of our customers to know the details behind the engineering and the set up of the infrastructure that we have put in place. So to solve that question, to answer that question simply, since 2021 we have released a software infrastructure, software demo called Clockbound. It's open source and it's available on GitHub at the URL printed in here on the slide. The key information about this software is that it gives you the answers in one go. In a single operation, you get three pieces of information. First, you get the current time. It's Monday at 2:00 p.m. Second, you get the the the size of that window plus minus 20 microseconds. And last, because it's important to every single application, you get a status about the validity of the clock. Is it initialized? Is it free running? Is it synchronized? So in in one call you get these three pieces of information which gives you a much richer information about time and the clock status and the quality of time upon which you can build your own business logic. The clock-bound demand has been supporting PDP hardware, uh, clock sources and NTP sources from the get-go. It scales to the most demanding workload. You can ask this question millions of times per second to make sure that you can match workloads such as the one that Nikolai was describing before. It is a piece of software that is written in RST, something we like in Amazon for performance and memory safety. We really care about this concept. It's available in RSS as a cloud and also for your C application if you want to back port it. Now, I want to give you a bit of an intuition about the calculation behind the scene. Again, you do not need to know these details. Clockbo solved this problem for you. However, it's nice to have a bit of an intuition about what this is all about. If I were to summarize what the clock error bound is in one sentence, this would be the tightest bound on the worst case clock error that is due to either oscillator drift and communication delays. And this illustration on the right hand side gives you a sense for how errors accumulate. At the bottom of the stack, we have the Amazon Time Sync Service with all the infrastructure that George described before GNSS antennas, atomic clock. We get into the nanosecond range of accuracy in the hardware. But of course this information has to be passed up the stack through layers of operating system and your application, and this is where some little errors do accumulate. When you target nanosecond and microsecond level, everything does matter. And so as you move up the stack, you move from nanosecond accuracy up to microsecond accuracy, and this is where a clockbo shines and gives you the answer. Now, again, giving you some intuitions about this. Let's imagine that you have started an EC2 instance that you are running the clock bound and the client. You can inspect it and ask what is the value and the size of this uncertainty window at any point in time. So in this example in here, the white dotted line represents a hypothetical perfect synchronization, the perfect clock. And as you can see over time, the size of the window grows and shrinks. It grows linearly and shrinks when the clock is updated on every tick and grows again. At any point in time you can ask what is the size of the window, which is represented here by this interval between the earliest and the latest borders, boundaries of that uncertainty window. So with clockdown, the result is that all this complexity is handled for you, right? And then, and you will get this result and be able to access it as from here in this diagram. Now, with Clockbound, we now have a tool that allows applications and customer applications to leverage their local time with confidence. And some applications are particularly interested in this new functionality, and Josh is going to tell us all about it. Thanks, Julian. Now's the fun part. Getting into Who is using this and how they're using it again doing all of the difficult communications for you we have the primitive of the precision time, the hardware, the hardware time stamps, and then clock bound doing that work to reduce the communications that we talked about uh in the quote at the start of the presentation. The first example I want to share is Uigabyte DB. Uigabyte's an open source database using our open source clock-bound software. And they have a system called a hybrid logical clock. They do some of the uh ordering, you know, logically as a traditional database would combined with our precision clocks. And when you're leveraging the Amazon Time Sync service in Clockbound, the first thing that they saw was reduced latency for their customers by 3 times. They also saw increased throughput in the transactions, doubles, and then the, the, the big number I, I wanna share and, and, and focus on is incredible, 1000 times fewer retries. A retry would typically occur when you have two transactions, let's say a read and a write, at the same time, and there's a conflict. And these retries, it's additional communication it's load on, on their network communications it's load on their servers that no longer exist. The result is a, a better experience for their customers in consistency, you know, isolation, but also just end uh user experience in their database. I have a QR code there of a blog. I co-wrote with uh Kartiker, co-founder and co-CEO of Ugayte, if you wanna learn more about uh their hybrid logical clocks and see some examples of this. At AWS we are also using our precision clocks at scale. We're doing that with Aurora DSQL and with Dynamo DB global tables. These are both multi-region global planetary scale databases. The reason why is shown in this diagram here. So these are network latencies in milliseconds across 3 regions in the US. So if you look at US West 2 to USD 1, you're gonna have a latency of around 70 milliseconds. And again, we're talking about clocks in microseconds in nanosecond range. So using our precision clocks, you know, faster than sending a packet over the network, the order of your transactions. And this is what it looks like with the Rory D sequel. This is the same example, the same diagram I showed you earlier, uh, with the bank accounts getting a little deeper and more complicated, and this time we're looking at two nodes from two different regions for Aurora DSQL and getting this right is critical for consistency and isolation in a multi-region database. Using clockbound's uncertainty window. You have event 1 with a T start and T commit, which is guaranteed to be in the pass, compared to event 2 or event 3. Now event 2 and 3, as you can see, have some overlap, and they need to be processed carefully. One thing you can do is delay an event, so delay the start of event 3, so there's no retry. Uh, if let's say that was a read and event 2 was a write. But again, it depends on your database or system design, uh, the type of events that are occurring, and what you want for your customers. But this is not a database talk. I'm not gonna go further into that. It could take hours. Here are 3 sessions going deep into the architecture of Aurora DSQL and Aurora Dynamo DB. Uh, these are all later this week and for people listening in on the recording, these are also all recorded sessions that you can find online. Now we've covered a lot, we've gone very deep into the science of time. We have NASDAQ's example on leveraging hardware time stamps for in-region uh financial exchanges. We've talked about thinking and changing the way that you think about time, uh, and measuring not just a specific uh time but an interval in confidence. AWS partners and AWS databases leveraging this. But I promised you. At the beginning of this presentation, that while it's good to understand this, you don't need to go that deep to put this to you, to work in your systems. So we have one more thing. To share with everyone today. Because customers ask us these primitives are incredible and very powerful, but how do we make it easier to use? How do we build our software and systems and have this, this work done for us? Julian, do we have an answer? Well, that's the part where I'm probably going to get a bit more excited. Um, so as we described, we have built a very accurate time synchronization service, uh, at Amazon. We support many workloads. We support innovation. We have examples from NASDAQ, innovation in the new generation of databases. The point of this is that our customers put their trust in us and so we have to continue to keep that trust and continue to improve the customer experience over time. And that's why I'm very happy to announce that we have just released a new EST demon. The thing is, it's under the same software stack, the same clog-bound software that we have released 4 years, and it's still accessible on GitHub, but if you are aware, the QR code will take you there as well. There are a lot of improvements we have made in there. A lot of work has been done in there to continue to improve and deliver a higher accuracy and a higher reliability for our customers. One of the key changes and the key innovation is that clockbound now includes this time demand, which is a stand-alone time client and demand. It does not rely on third party software anymore. It will synchronize your operating system clock for you as well as continue to give you this information for your application to know what time it is, the size of the window of uncertainty, and the status of the clock all in one go. It's important for many reasons. Nikolai mentioned regulatory reason there, but also just simple observability for your distributed system. We have built-in metrics. Observability is a first-class citizen and must be absolutely a first-class citizen. Again, it's the same build on the same software stack in Rust that is provide memory safety and high performance, and performance is achieved through different means. Here we have a shared memory segment that allows for inter-process communication that allows us to provide this information at a very high rate. So this is very exciting, and there are 2 more things that I want to share with you before going into more details in the following slide. The first one is that we have redesigned the system to be cloud first design. Time synchronization software has existed for decades before the cloud existed. Here we make a massive change in paradigm. Most of the historical systems use a feedback loop system that is not well suited to cloud applications. We've changed that around. We now have a system that feeds forward information in the system which is much more friendly to cloud applications, and we'll talk a bit more about this in a second. And last, if you're like us, you are a time nerd in the room or watching this online. I just want you to know that the new software stack ships with a simulator that allows you to test the software, see it for yourself, find what you like in it, potentially report some issues or things you would like to see improved, and we will welcome your feedback, of course, as you try it. So let me go back quickly into the last components of the talk in here. First, we talk about cloud native synchronization. So you know that at EberDres, availability is a top priority through all the layers of virtualization. We have made some innovation in here and contributed a new feature to the Linux kernel called VM clock for virtual machine clock. And this feature combined with our new clock bond it release allows the direct transfer of time from the nitro clock into your instance. It's it's an improvement over any kind of maintenance event to the point that you now have your workload running smoothly at any point in time. Great. Let's have a quick look at how this looks as well in terms of performance. Um, Similar to the earlier slides, top graph in here shows the drift of an oscillator in a band of like thousands of microseconds. How well are we doing? Well, the result is the graph below. Given this input, we managed to track and correct for the drift to within the band of plus or minus 1 microsecond over this period of multiple hours. This is how good the system is working today. So this is for the internal of the system, but we mentioned that what matters to the customer application is the window of uncertainty. So here we have a little application, a little demonstration where we can see in real time that we are constantly querying the clock-bound demand to plot the size of the window of uncertainty. As you can see in the previous graph, it has this sawtooth behavior. It grows and shrinks as the clock gets updated. But what matters in here is that the results are in a band of somewhere between 20 and 26 microseconds. That's the size of the window of uncertainty you can achieve inside your application by using this new application. And now I see that we're almost out of time and it's now a good opportunity for Josh to conclude and take us there. Thanks everyone. Now it's time for you to build and take advantage of the Amazon Time Sync service and the precision time capabilities that we have. Again, we've built this into Amazon EC2 instances. You could use AWS databases, AWS services built to take advantage using NASDAQ, AWS partners like Ugayte who are leveraging time for you. Uh, again, Amazon Time Sync and and Clockbound do the work for you. And I can't wait to see what you build using this. Thank you.