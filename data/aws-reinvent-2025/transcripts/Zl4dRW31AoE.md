---
video_id: Zl4dRW31AoE
video_url: https://www.youtube.com/watch?v=Zl4dRW31AoE
is_generated: False
is_translatable: True
---

Uh hello everyone. Uh, nice to see you all here. Hope everyone got to Vegas safely and, uh, ready for a great reinvent. Uh, my name is, uh, Prashant Gannapati. I'm a senior solutions architect with AWS. I've been with AWS for about 5 years now. I just completed 5 years, but I've been in the solution architecting role for about 20 years now. And uh my claim to fame is I was doing AIML before it was cool like uh 3 years ago so uh helping customers in AWS with AIML technology since I joined in 2020, uh, but I'm excited to be here today and uh share Slack's journey. Uh, of their developer experience group and their internal developers, how they used generative AI and then eventually agents over the last couple of years and, uh, what kind of success they saw, and that's what we'll be sharing today, uh, with me is Shrivani and Mani, uh, from AWS and Slack, and I'll let them introduce themselves. Thank you, Prashant. Hello everyone. I'm Shivani Bitti. I'm a staff software engineer on Dicpi AI team at Slack. Um, I've been with Slack for about 7 years, and I've been in on DevicP AI team for about 3 years. Um, so today I'm very excited to share some of our learnings, um, and journey in developing AI tooling. Awesome. Well, thanks, Shivani. Uh, Mani here. Um, I lead the strategic ISV accounts, uh, the Gen AI, the cool stuff. Uh, I've been working with AWS for 5 years and like this entire journey I've worked with some of our largest ISP providers and our product team. So basically helping customers put their ideas into actual work and then helping our product teams develop their roadmap. So consider me as a bridge between the customers and our product teams, and today here to share about the all the good work that we did with Slack, um, that's what the session is going to be all about, um. So before we get started, quick show of hands, how many of you checked your slack before coming in? Exactly, uh, Slack is where I would say the work truly happens, you know, we get into conversations. This is where ideas, you know, turn out, they become decisions. Decisions become actions in turn. Um, and every idea that turns into action, the teams are talking, building in Slack, uh, every single day, and because of that, Slack can never slow down. They have to be fast. They have to be reliable, and they have to be secure. All of this at a scale now as Slack continues to grow across different enterprises across the world. Um, it, the innovation becomes absolutely critical, and that's what I would say AWS comes in, you know, AWS came in and helped Slack, um, develop and build things faster. It helped, it unlocked the new levels of innovation. Uh, so in this session we'll talk about how Bedrock kind of became the foundation that powers and governs this next chapter for Slack. And I love the quote from Andy Jasse which says that together AWS and Slack are giving developer teams the ability to collaborate and innovate faster. Now this is quick and easy what we're gonna cover today. First I'll share a little bit about the Slack's developer experience AI journey. That's the Slack team, uh, using Bedrock. Um, then we'll talk about how Slack rolled out the code assistant tools, uh, follow it up with the real impact because I think that's important on, on the developer community. And then we'll explore how Slack is now moving into agents and strands, um, and finally we close, uh, what's the road ahead, what does it look like? So by the end of this session you'll see not just what Slack built, why they built it, and how, how they built it. I promise no pop quiz at the end, but hopefully you'll get some few good ideas that you can take back and implement in your work life. Now this slide is really about the people behind uh the Slack developer experience team. um, the team that build this um integration, it exists uh to make the life of Slack engineers better. They bring in AI closer to developers and they remove the friction from everyday work while we use Slack for our day to day work. How do we make the developers much more, um. Agile and much more productive, so this journey started by Slack, uh, developer team building something called Buddybot to help build documentation, you know, to help these developer answer these questions more quickly. And today this team touches everything. They build, they touch the build and release tools. They have the testing infrastructure. They, um, develop our tools. They develop these tools at scale. And they're around about, I think like 70 to 80 people. They are powering the entire Slack engineering team and then, you know, even further to the Salesforce organization. Uh, what I would like to highlight is what's powerful about this team is that their approach, they start internally first, so they kind of build something. Put it for a POC or internal usage. They roll it out at a smaller engineering team, and once they are able to prove it successful, they roll it out to a bigger audience. So this team is a huge reason for Slack to move fast and to move things very quickly without breaking it. Now before I jump into Slack's journey, you know, between Shuwani and Prashant, they're gonna talk about how they implemented. I quickly wanted to level set on the AWS stack that exists, um, that, that, that the team used to build this entire, uh, solution. So at the foundation level we have our Amazon Sagemaker and AI compute um that basically developer teams can use to build, train and deploy custom models, and they manage their own data and MLOs. Above this layer is our Amazon Bedrock, which is a fully managed foundation model layer. Now this is where if the teams want to move faster, they want to experiment. Bedrock gives you a choice of leading model providers. It gives you. In guard rails, it has knowledge base for rack retrieval augmented generation, and it has multiple flexible hosting options pay as you go. You can probably pay up front and all of this in the most secure way that you can scale very quickly. We also had launched Agent Corp, which handles a lot of you know plumbing in the end, like think about runtime, identity, memory, observability. All of this can be done through Agent Core and while the team focuses on the agent tech work. Now, on top of these are our SDK agents, which is uh more like frameworks, which we will talk about today, the strands agents, um, and we also have our, um, First party model called Nova Act and on top of all of this finally is the layer of applications like Hiro and Quick Suite which is like if you really want to plug it into your application you use these applications so that's like a complete stack and we would be referring to some of these wire. We talk through the presentation. So in simple terms, Sagemaker is where you build models. Bedrock is where you can scale safely. Uh, strands is where you bring agents to life, and your top layer applications is where you, you deliver. So now with this in the mind, let's move on to see how Slack, uh, build the story. One of my favorite slides, uh, because it shows how Slack actually build this step by step. Now they, if you see they started way back in Q2 of 2023, almost like two years back with Sagemaker, um, at this time it was all about learning. Jennea was picking up, you know, everybody was so excited, um, so they started experimenting, prototyping, and this is, uh, probably the reason one thing that they decided with Sagemaker was Slack has a, um, really strict requirement of being fed and compliant, and Sagemaker gave him that option. And when it came to Q3 2023, they all started with an internal hackathon, and this is where things got real, and our teams experimented, they built prototypes, and they even created things like huddle summaries. I don't know if you have seen that in Slack that also a part of this hackathon. Now this is a phase which is all about, you know, proving the art of possible, what all you can do it. And then Q1 2024, Slack moved to Amazon Bedrock because Bedrock was now FedRAM compliant. Um, all the latest anthropic models are available on Bedrock and infrastructure got easier. They really didn't have to take care of infrastructure with Bedrock. It was like everything. You just pull in a feature and it's all taken care of. All the underfringed heavy lifting was done by them, and they nearly saved 98% of the costs when they moved to Bedrock. Now as um they got familiar with Bedrock, you know, came Q2 of 2024, um, they came up with their first bot called Buddybot, and as I mentioned, this is more like. Helping with documentation, helping, you know, developers find things more easily, and, uh, they, they, the good part is while they were building this, they started using knowledge bases, so nobody had to manage their vector stores anymore. Developers just got better on embeddings. They got better knowledge bases. They got faster answers. Now as the build Buddybot by Q1 2025, the developers being developers, they started asking for coding assistance. They said, Can we go further? Can we build coding assistance? And that is where, um, anthropic, uh, they started experiment with cursor and clot code. And since anthropic models. Anyway available and they were the foundation of all the things that they built. It was very easy for them to use clot code and cursor, and they were adopted, um, right on bedrock. And then finally in Q2 of not finally, but in Q2 of 2025, Slack took a critical step. But obvious to build agents um because Agent Tech was the new way to access data through MCP servers, um, they didn't rush into A2A because they just wanted to spend time learning that as I said Slack didn't get into Analysis Palaceis they wanted to take it slow. Um, they built their first MCP server and um they got into the foundation of building agents and then in Q3 2025 Slack introduced strands. And the escalation bot, which Shivan is going to talk in more details, it's how they move from buddy bot to escalation bot using strands and agents. Strands is an open source multi model agnostic and flexible framework that we have. And if you're on an agentic journey, you know, we would love for you to use that, and we'll talk more about it. So I would just say the biggest takeaway here is Slack did not get stuck in the hole. Analysis paralysis thing they kept experimenting they kept shipping and they kept learning from it. So today they have scaled uh from processing a few 100,000 tokens per minute to millions of tokens per minute. It's just like basically AI now instead of jogging, it's sprinting, uh, all thanks to Anthropic's 1 million context window. They just moved from, you know, a few tokens to a million tokens in a minute. Now the question is why did Slack ultimately standardize on Bedrock? like why Bedrock? I think first thing that they loved is that it's a unified platform across AWS, one place to build scale and govern everything. Second, I think it is very important and it's job zero for us is the built-in security that we have within Bedrock, the guard rails that we have, and the govern governance that we have in place. So guard rails, security, compliance, it was all built in, um, as a part of Bedrock. And third was the massive scalability. Slack isn't running one AI use cases. It is running multiple UI cases, AI cases across multiple different organizations with thousands, hundreds, and thousands of employees, and all of this happened without Slack needing, needing to worry about the infrastructure. And I, I think that's, that's the move from Sagemaker to Bedrock really helped them. Now Bedrock let them focus truly what matters, building an amazing developer experience and user experiences around it. Now let's bring this home like before I hand it over to Shivani in terms of impact like with an AI assisted coding using cursor and clot code uh on Amazon Bedrock, Slack completely changed how fast ideas turned and were shipped into the actual features. Quick show of hands, how many of you think the productivity would have gone up by 20%? Nobody thinks. OK, there's one, maybe 50%. Is that too big of a number? Oh wow, I have 50% more than OK, so here's what we know for sure that it, uh, accelerated the developer productivity. It empowered the teams, uh, to innovate faster, and it, uh, massively reduced the prototyping time. But instead of guessing the numbers, um, I'm going to hand it over to Shivani who is going to show you the actual numbers and the real results behind this infrastructure. Thank you, Mo. I think one of the measuring impact. How many of you here have some kind of metrics to measure AI impact on developer productivity? Oh, quite a, quite a few. I think we're all talking about AI, but I think one of the hardest problem, hardest question to answer is, is it actually helping? I think in order to answer that, we need to know what to measure, how to measure. So we initially started with like two foundational metrics, AI adoption, which means basically if engineers are adopting AI tooling, it's actually a first sign of like it's helping them relieve pain in one of their workflows. The other, the other part of it is impact on developer experience metrics like Dora and space metrics. To measure these metrics, we needed data from multiple data sources, so we used open telemetry, hotel metrics, plumbed into all of our AI tooling to get the usage metrics and some of the AI tool calls and all of that. We also measured metrics from GitHub, our source code, like pull requests and commits, which are co-authored by AI and also have some kind of AI signature. So these metrics helped us get like not a perfect metric, but overall a good estimate of how developers are impacted using AI. Let's talk about the overall developer impact here. With all of the tools we have rolled out, we have seen consistent like week over week adoption rates increase, and we've also seen consistent usage of these tools month over month for a few months now. Currently, 99% of our developers are using some kind of AI assistance. Which is huge. Once we got the adoption numbers we're looking for, then we started looking into developer productivity metrics. We started looking at PR throughput. And uh observe that some of the major repositories we have, we're seeing about 25% consistent month over month increase in the PR throughput. There are also other metrics. But this has been consistent. The, the other metrics we are, we are looking into is AI bot assistance, which Man just talked about. It's a bot we rolled out to help our engineers with knowledge search and also help them with the escalations. So escalations at Slack happen in Slack. When users have questions, they come into Escal channels and escalate it to the to the appropriate teams. This was causing a lot of on-call fatigue for our engineers, so we rolled out AI assistance to help engineers ease the on-call pain. And currently our AI Assistance Bot is helping over 5000 escalation requests per month. And the final metric, and the most important metric is the qualitative metric, which is the direct developer feedback. And the feedback we've been receiving confirms that these tools are actually helping developers. I think with all of those. With all of those tools. Of course, AI is not perfect. We are also seeing downsides of increase in peer review time. As AI is helping engineers write more code, we're also seeing that the surface area for review is increasing and it's causing more load for developers. We're actively working on this area to reduce the review time with AI assistance. And we are hoping we'll we'll implement AI to ease developer pain across all of the developer developer cycle, the broad cycle. Learnings and experiences. We have seen the metrics, 99% adoption and 25% PR throughput. But the path to get here, it wasn't a straight line. Like many of you, we started our AI journey with experimentation, with pure experimentation, like 3 years ago, like money just talked about. We built our initial capabilities using Sagemaker and Drag. This gave us maximum control, but it also came with a huge hidden cost of infrastructure maintenance. Our breakthrough came in when we adopted AWS Bedrock. This wasn't a technology change. It also is a philosophy shift for us. Bedrock instantly simplified our infrastructure, handling all of the LLM scale and infrastructure maintenance for us. This change immediately addressed. Are critical success factors for adoption. Like security Bedrock allowed us to keep everything within our AWS accounts secure. And also adoption. So making LLMs available through a proxy API, we were able to provide LLM access to all of our developers to experiment. Using AI. And the other one, the other one. I It's not working. Oops. Come. OK. It's working, I think. His lives are not changing It's not working, the lights. The other advantage we also got with switching to bedrock is observability. So the inbuilt native observability of bedrock of cloud watch logs and metrics and alerts helped us get into some insights into our LLM usage. Thank you. Through our journey, one of the main challenges we faced was experimental experimentation fatigue with different LLMs and tools. The air landscape is changing so fast. And we are struggling to keep up. We realize that constantly rolling out new competing internal features. Was only causing confusion to developers and also caused maintenance overhead for us. So to combat that, we doubled down on a high impact tech stack, Amazon bedrock, and anthropic models and tooling. We drove adoption by integrating tools like cloud code and cursor with Bedrock. In fact, I think we were one of the first teams, uh, first company to roll out clod code, um, early in Q2 of this year. The goal here was to create seamless experience. That maximizes throughput and reduces the decision fatigue for our developers. Now I'm going to hand off to my colleague Prashant to talk about agentic frameworks. Thank you, Shivani. Uh, those were some insightful and, uh, learnings and some impressive statistics, right, from Shriwai, um, as you all saw and heard from Shrivani and Money, Slack started their journey in AI developer tools and using Amazon Bedrock and saw some impressive early success. Uh, but I'm here to talk about the next stage in the agentic journey, right? This is the year of agents. Gotta hear why and what they're doing with agents. So quick show of hands, how many of you here are exploring agents in your organization? Yeah, quite a few. And how many of you have agents running in production? All right. Some of you maybe we should talk to later as well and learn from you, right? Um. So The key questions are, you know, why agents, right? Besides the obvious answer, because everybody else is doing it and we have to do it, but for Slack there were some key questions, right? So as they were using these coding assistant tools and using the LLMs through APIs. A lot of their actions were ad hoc workflows, right? So they would, for example, there's an issue going on and they would take the logs and dump it into cloud code and say what's going on and cloud code will answer it, right? But that's an ad hoc workflow. Now imagine on call engineers are getting lots of these requests. They want automated run books to be running. Right, so they want to take that to the next step where agents are processing what the ask is choosing the right tools, making those decisions, and then, uh, doing the, uh, you know, analysis and remediation, right? So moving from ad hoc to automated workflows was one of the reasons and they were like we're already doing this, we should just extend it. Um, the second part is, you know, with just LLM calls and doing ad hoc flows, uh, it's, it's not doing any complicated reasoning, right? It's doing LLM retrieval and then maybe some post processing, but it's not doing any complicated reasoning there. It's not planning. It's not adapting. That's another reason to go towards agents, especially in their environment where they are, you know, fixing things on the fly and there are issues happening and they have to react to it. Um, also, Slack has built a lot of tools and data sources on top of AWS services which they use effectively for their data pipeline, for their CICD build, for collecting logs, all of this, right? So in order to take advantage of all of this in a, in a dynamic fashion. They would need to build some sort of standardized access to this, right? So agents would work with something like MCP model context protocol. If you haven't heard of it, you'll most probably hear about it in this week. So using a standardized protocol to build out connections and being able to dynamically use these tools and data sources, that's another reason to build agents. So these were the key reasons why they were like, we're already doing this in an ad hoc fashion. We should standardize this and automate it. That was the key reason for building agents. Like the fancy font. I want to show it for the agents, you know. So, um. How, right? So they were heavily using cloud code already, right? So, uh, one of the key features about cloud code was they were adding a lot of agenttic capabilities, especially the later half of this year, uh, features like cloud code subagents and, uh, planning capabilities, and now skills has come out, right? So they're adding agenttic capabilities and they built an SDK, right? So it's an agent with an SDK which you can use effectively as subagents for various tasks. So instead of building, you know, agents from scratch which can do specialized tasks, which is very complex and it's hard to perfect, right, in production, they started using clouded code sub agents for a lot of these specialized tasks that they're running into. That was the first part they did. Um, the second part, like I said, to access the variety of tools that they have, uh, and the data sources, they started building out their own MCP servers, and they also learned, uh, some of that from us. Like we built an MCP server for EKS. They learned how to use that for some of their use cases, things like that. So being able to standardly access, uh, you know, access all of these tools and, uh. Data sources without having to, you know, think about, oh, I have to use this API or I have to use that API, you know, they want to standardize it. That was the idea and finally they were looking at various agentic frameworks for integration and that's where Amazon came in and we'll talk about strands as we go along, but that's how they started exploring agents. And then finally, um. You know what are they doing right now, right? So one is instead of taking a giant leap and building a super agent, they're taking existing workflows and which they built with LLM integrations and they're enhancing it to add more capabilities using these agentic workflows and then also exploring new use cases in the DevOps environments as well as. Incident management and things like that, right, so that's what they're doing. So these three steps you can see it's small steps, but they're making progress and they're putting things into production, right, which, uh, which is a very key step, right? A lot of times we see customers getting stuck in analysis paralysis, and we want, and they're kind of moving forward with these small steps. So the key question right when I was talking to Shivani, uh, some time back, right, cloud code and sub agents are so powerful and you can create automations with it pretty easily. It's SDK based. So why look beyond something like that which is so powerful and it's meeting most of their needs today, right? That's a key question to ask. We should not just adopt a new technology just because it's there. It should serve. Purpose, right? So we are kind of discussed that and this was a key reason, right? So one is it's great, right? It's a great tool, but it can get expansive, right? It's a, it's got its own system prompt you can, you know, prompt it with your own user instructions, but it can get expensive, it can get expensive and it can be less predictable depending on what you ask it to do. That's one thing. Uh, also, you know, for Slack and for everybody, it should be that you should be model agnostic, right? Today it's anthropic, tomorrow it could be something else. We are so early in this journey, we don't know what's going to come out, so don't get, you know, uh, locked in into one technology. Uh, so model agnostic is another part of it. Uh, right now people are in exploratory phases, so cost is not so much of a consideration, but as production, as you roll this out into production and the usage goes up, the cost will become a big, uh, big factor, right? And so you may want to say, oh, for this specialized task, why am I spending so much money? I want to use a, uh, you know, a cheaper LLM and kind of pointed to that. So if you get, uh, you know, just uh stuck with cloud code, you may not be able to do that. The other part is, uh, one of the ideas which we discussed whichrivani will talk about is the idea of the orchestrator agent, right? So clouded by itself and cloud code has the ability, uh, to have its own orchestrator, the planning, the thinking capability, and then it can direct its sub agents. But now you're all into cloud code, right? So what if you abstract it out, right? What if you abstract. The orchestrator away from cloud code and use what is really good at, which is the sub agents right doing specific tasks. So once you do that now this orchestrator agent which you have built from scratch using an open source technology, you are able to then uh point to cloud code subagents today, but you can also point them something else tomorrow, right? That's the key. So abstract it out. And control what you're accessing when you're accessing keep that within your framework that was another reason. So finally by doing all of this, uh, you can create an agnostic agentic framework which will future proof your production deployments, right? That was the key. So we went through this discussion and then we kind of went into the journey of, uh, the agentic world. So that's where strands comes in, right? Strands is, I'll talk about strands and what it is, but before we dive into strands, I wanted to discuss why Amazon builds strands, right, and open sourced it. So while the potential of agents is exciting, right, everybody's wanting to build agents, and I've tried to build agents. Uh, we have all been trying to build agents. At some point you realize it's more complex than you think it is, right? Everybody has gone through that raise of hands if you think it's resulted in being more complex than you thought it was, right? OK. Many of you have not raised hands, so I guess it's simpler than we think it is. So, um, So a key challenges that developers face, right, with reliable agents. One, there could be a steep learning curve in building agents, right? While we worked with some of our customers, we saw that they could make simple use cases work, but as it got complex and the fact that new features and technologies are coming out every week, it's very hard to keep up and. Decide, OK, this is good enough for me or should I wait for that thing to come out, right? That, that was happening a lot, um, enterprise and production readiness, right? So as you build stuff, it will, most of them will work great in POC and demos, right? But as soon as you take it to production there are a whole other set of criteria that you have to meet and that takes a much longer cycle and we saw that, um. Uh, complex orchestration logic, right? So again, a single agent or an orchestrator calling one or two agents with a couple of tools works great as long as soon as it goes to thousands of agents. You as a company, if your goal is not to just build agents, it becomes much more complex, and we'll talk about some multi-agent patterns, right? It does get complex in that stage and again for production it can be challenging. Also lack of visibility, lack of controls, not enough flexibility. These are some common challenges in most distributed systems, and you see that with agents as well, right? So there's a really early stage in this technology and so a lot of this, uh, you wanna keep it in a framework which is open source. That's why we open source strands, right? So strands is open source. Uh, initially we released a Python SDK. You may hear some announcements in reinvent. Uh, but initially we released our Python SDK for building agents with just a few lines of code, and I'll show you some examples as we go along. It's simple to use, eliminates the need for complex agent orchestration, and it's a. Code for a solution, right? It's, uh built keeping, uh, builders in mind, developers in mind, right? They can define a prompt, uh, select a list of tools, and then, uh, select the LLM and then let it go, right? That's how easy it is. And by open sourcing we aim to provide developers with powerful flexible tools to build agents in the rapidly evolving agenttic landscape. So now that I've introduced stands, what are its key features, right? Um, one is, like I said, model and deployment choice, right? It's open source and while the default LLM is Bedrock, you can choose any third party custom providers and your list of them and we keep adding more, um, to be used as the LLM as part of the agent. So we're not restricting you from choosing the LLM of your choice, right? And also be able to deploy it anywhere, any production agentic framework, right? We're not restricting that either. Um, highly flexible, it's got built-in guard rails. It connects to AWS's guard rail features, but other, uh, uh, other guard rail features externally as well. It's got built-in native observability and monitoring hotel metrics that you can stream out, uh, if you use agent core, it connects into agent core automatically with these metrics, so very easy to get visibility and traces of these complex agenttic flows that happen, right? Uh, 3, the MCP integration, right, model context protocol that's sort of become the industry standard as we move forward to connecting to data sources and tools. Uh, we provide that as well as there are a lot of in-built tools in strands itself that you can use for a lot of tasks. Add custom tools. You can see on the integrations we have integration with Mem Zero, Raga, Stavili, Temporal. There's a temporal and uh AWS open source session happening somewhere else in reinvent. You should try to attend that, uh, so integrating a lot of these third party services out there, uh, to make it highly flexible, right. So these are some of the broad capabilities that SANs has. Um, before we go deeper into how Slack used strands to, uh, you know, improve their, uh, agenttic workflows or add agenttic workflows, I did want to talk about the multi-agent patterns that exist within, uh, strands. Uh, you can see 4 of them. I'm gonna talk about the, uh, starting a swarm and the 32 to the right of them, and then I'll come back to agent as tools, right? So swarm is, as you can imagine. Collaboration between multiple agents so there are communication patterns, shared memory systems, coordination mechanisms, and a host of agents are talking to each other to solve these complex problems. A graph, as again the name suggests, is you have the agents which are nodes and then the way they communicate with other agents are the edges and you define explicitly how they communicate, right? So you can build out a graph workflow pattern. And then the final is the workflow, which is essentially a structured way of defining how one agent will do one task and pass it on to the next agent and so on and so forth, right? So these are, uh, three patterns that we have. The 4th 1 on the left, agent as tools, is the most interesting to us right now because that's what Slack is using. Uh, you heard me talk about the orchestrator agent. So we're using the orchestrator agent in agent as tools which handles the user interaction and it decides which specialized tool to call, and these tools could be other agents. That's why agents as tools. And so the specialized agents perform those tasks and then. Hand back the answer to the orchestration tool to orchestration agent to decide how to answer back to the user, right? So as Shivani will describe later on, you look at how they use strands as the orchestrator agent and our specialized agents in cloud code sub agents to do the specialized tasks. So, um, Before we get into the architecture discussion that Shrivani will talk about, uh, I did want to also, uh, you know, talk to you about what strands agent is, right? It's a very simple concept, um, strands is what it calls an agentic loop, right? that forms the core of its functionality. Uh, it receives a prompt and a context along with a description of the available tools. Then the model reasons about the tasks and decides whether to respond directly. Suppose it can respond directly, it doesn't need the tools to respond. Otherwise it'll plan a series of steps, reflect on previous actions, select the tools that it needs, and do one of these steps. Once it gets back the response from the tool or whatever it asks the task to do, it decides whether the task is actually complete, or else it'll repeat the cycle again until the task is complete. That's the basic nature of strands, uh, uh, how you create a strands agent and the multi-agent pattern. So before I hand it back to Shivani, I wanted to leave you with a couple of just simple examples of creating strands agents, right, with uh model choice and then I'll show you the tools example so you can see over here with a few lines of code we're able to create an agent which uses the default choice which is uh bedrock in this case, uh, and, uh, uh, a Nova model to create the, uh, agent and, uh, ask that question and then similarly. You can attach tools. You can see there are a whole host of tool categories that is available and then from these tools, uh, you can attach these tools to the agent and create a simple agent where in this case is using the HTTP request tool to ask a specific question, right? So again these are simple examples, but you can see to get started. A very few lines of code are needed and I, I wanted to highlight that as to how easy it is to get started with strands. So hopefully you got a good understanding of, you know, why, uh, Slack went towards agentic, uh, workflows, uh, why they selected strands as an exploratory, uh, area to move forward with and what it is solving and also learn a little bit about strands. That was my, uh, part of the, uh, uh, presentation, and now I'll hand it back to Shriwani. To take it home, uh, with the technical deep dive starting with the buddybot, how they enhanced it with strands, and then, uh, we'll get to Q&A. Thanks. Talk about the evolution of our buddybot technical deep dive, um, and to using strands, uh, from like the version 0 which we started. Our story starts with the fundamental pain point of engineers spending a lot of time on scals. The initial bodybot architecture which was shown above was designed to handle basic escalations. By using knowledge that is spread across different data sources. So as you see, like the data sources we are calling here is Slack data, Slack messages and files, and also some of the technique, some of the data is also scattered across our GitHub repositories, like technical designs documentation. So as you see here, like the first thing we did is like we did a hybrid search. So we gathered the right relevant information across all of these data sources. And then we in turn re-rank those data to get the more accurate or more relevant data across the knowledge sources. And then we provide the top most relevant documents to the LLM with the user query to provide a more accurate answer back to the escalation. So this was our first design which was working great. But we ran into challenges with the initial design, with respect to maintaining the conversational history and also being able to execute external actions. So then we evolved into what you see here, the newer version of our body. Which is showing a powerful agent that we have built or we're exploring. Which begins when a user sends a message, our back end receives an event. And we start a temporal workflow orchestra orchestration around that, which basically provides durability as well as maintains the conversational state across the entire escalation until it is resolved. Which releases the ease of pain for maintaining the conversations for all of the escals in our applications. The temporal workflow, which then calls the main strands orchestrator. The strands orchestrator agent which we built using anthropic as well, anthropic cloud model as well, which then decides which sub-agents to call. And the sub-agents have access to the MCP servers to interact with our internal services. All of the sub-agents that you see here, we've built using cloud SDKs. So the subagents are clouds, the orchestration agent is, so the, the orchestration agent is strands and the sub-agents are cloud code. The reason we chose Strand's agent here as orchestrator is to explore different LLMs, not just use cloud code subagents, as we're actively exploring other competing models. And once all of the subagents are finished running, the main agent, the orchestration agent, gets the context or response back, which then synthesizes its processes, and validates the response before sending it back to the Slack channel. So as you see the flow here. So we're going to see a live demo of like. How it happens at Slack. So when a user sends a question or an escalation in Slack channel. Our back end gets an event. And then our back end then spins off a temporal workflow which you see here. Which has the context of all the Slack conversations that happen or escalations that happen in that Slack thread. It then kicks off our orchestrator agent. Which is written using strands. And once the orchestrator agent Receives the request, it actually sponsor sub-agents through a, through the tool call, which um Prashant already just talked about. As you see here, it's actually calling the triage agent and KB agent. Those are all sub-agents we built using clot code. And the good thing with the The temporal is, it also provides visibility into all of the calls and traceability as well. So once the main orchestrator agent processes everything, it sends a response back to Slack channel. And when a user asks a follow-up question, all of the context of the previous conversation is actually maintained by temporal, which eases the pain on the application itself, uh, to maintain the conversation history and the states. So as you see, temporal resumes the workflow whenever user sends a continues a conversation in Slack thread. It just resumes the same workflow. So once it finishes, the same flow, like the response is sent back to the customer. So this Workflow simplified, this arch this architecture simplified our code quite a bit, as we didn't have to maintain the conversational history and durability of retries. Which is all provided by Temporal, and with the strands, we were able to experiment with different sub-agents, and we were not, had to stuck with just Clark code. So as you see, as you saw our overview of the technical architecture, we upgraded our um our buddy bot from a simple search bot into a powerful agent. And some of the things we considered while building it is reliability and efficiency. First, we built a stable foundation. We used temporal for reliability. As the bot never forgets the conversation, even during failures. Even when the backend dies, Temporal maintains the state in a database, so it resumes where it left off. Temporal also supports like retries, automated retries, so we didn't have to retry tool call failures and all of that um in our application, which eased or simplified our code quite a bit. And next we solved a crucial security challenge. We created remote MCP servers with OA service, which integrates to our Uber proxy, which is a networking system. This ensures the bot can safely access sensitive internal systems like GitHub with the right permissions. Finally, we were focused on making bot faster, so we ran all of these sub-agents in parallel. And we also optimize the token usage management, so the strand subagent. Before uh sending it to the LLM to summarize and like confirm uh the response, it summarized each of the response from the sub-agents to reduce the token management across when sending it to uh very expensive LLMs. And also strands provided us extensibility for any future being LLM agnostic. So the road ahead, we've stabilized the architecture and solved the security problems and optimized performance, but this is just the foundation. Our vision is much bigger than single escalation bot. We're on a road to establishing fully automated agentic workflows across the entire development cycle. We would like to experiment strands use cases beyond escalation. And we would like to integrate more internal tools via to make our bots or agents more powerful. We also are exploring ancient core. And we would like to have native integration with temporal and strands agent for smoother execution and also more granular retries. Our long term goal is simple but ambitious, a fully automated agentic workflows across the entire development cycle. Here are some of the links for you all to get started, Building agents. Yeah, I can. Yeah, thank you again and uh, yeah, I'll give, keep it here a minute and then we can take Q&A after that. So if you want to um get the QR codes, I'm sure you'll run into a lot of these links throughout reinvent. Please don't forget to fill out the survey in your mobile app, uh, and then we will just, uh, move to Q&A. Yeah, and, and all of these sessions that you see, if you go back on the QR code, if you think any of these topics are interested, we have multiple sessions going on across the next 2 or 3 days. So please feel free to look around the different sessions, the keynotes, uh, Swami will be making an AML keynote. Definitely watch that. A lot of new announcements coming in across Asian coast, rends, and Bedrock. Yeah, thank you.