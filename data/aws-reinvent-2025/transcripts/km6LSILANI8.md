---
video_id: km6LSILANI8
video_url: https://www.youtube.com/watch?v=km6LSILANI8
is_generated: False
is_translatable: True
---

Hi, um, my name's Morgan. I'm a services engineer at Another Axiom where I'm the tech lead for Mothership, which is another Axiom's, uh, back end as a service platform. Today we're gonna be talking about the journey that Another Axiom has taken migrating the back ends for our popular VR games to AWS. Uh, first, we're gonna start off by talking about who we are as another axiom. Then we're gonna talk about the state of the world before we started this project. We're going to briefly cover things that we built on AWS from an architecture and implementation standpoint, and then we're gonna cover load testing, cost optimization, and our eventual migration to Graviton. Finally, we're gonna cover where we are today, what we've learned, and where we're going soon. Um, first off, if you're not familiar with another axiom, we're a leading VR game studio with a focus on social, plausible, dagegetic worlds and innovative interaction and locomotion systems. We believe in the power of VR to bring people together and do everything we can to create plausible alternative spaces. For you to inhabit with your friends. Our flagship titles are gorilla Tag, which is a social sandbox game created around playing, uh, playground games like tag as gorillas, and Orion Drift, which is a massively multiplayer sci-fi social sports game. To give you an idea of the scale of the games we're talking about here, gorilla Tag is the most popular game on the MetaQuest store. At the last time we measured, we had about 17 million Lifetime Unique players. At the time of writing this presentation, our all-time peak concurrent was about 100,000 CCU. Orion Drift is also very successful with peaks of tens of thousands of concurrent users. The VR market has some challenges that sound familiar if you're familiar with the general gaming landscape and some that are a little bit unique. The two big features to think about when capacity planning for VR are seasonality and virality. VR is extremely seasonal. At peak times during the winter holidays, it's possible to have an order of magnitude more peak concurrent players than at the lowest point of summer. That, that kind of seasonality is pretty common across the entire like video game segment, but we find that in VR that difference is magnified. VR is also extremely social and viral. If the broader VR creator community decides to run a big game meetup, you can end up with these massive semi-unpredictable load spikes. It's a great problem to have, but your infrastructure has to be able to scale to take these sudden hockey stick moments. This leads to the whole reason we're talking today. When I started at another Axiom, everything had been built on a popular third party platform as a service product. Guerrilla Tag was bootstrapped from a single developer, so it didn't make sense for that person to go and build their own back end. As Guerrilla Tag took off, we quickly outgrew the off the shelf solution. We're big for the VR market, but some of these off the shelf back end as a service platforms are dealing with absolutely massive gains, particularly on mobile. Our needs don't always line up with what their average game customer needs either, so we spend a lot of time extending that platform and bending it to our needs. Plus, as kind of a smaller fish in a big pond, we had a very limited ability to get support and even less ability to request features or give feedback. As an example, another Axiom's games primarily target the MetaQuest ecosystem. For a large customer, our platform vendor might have been able to get meta integration onto their roadmap. Instead, they gave us some guidance on how to build our own, our own pieces and. Kind of helped watch us as we bolted them on. They left us with a bunch of dev work to do, new services to support, and a new security service area that we had to deal with all on our own. Once you start building a couple pieces of that size and scale, you start to wonder how much it makes sense to keep a dependency on the rest of that platform. The final straw came during one of the hockey stick viral events I alluded to earlier. We had advance warning of a large creator-driven community event, and we'd warned our partners well in advance, but we still managed to fall over. At that point, we knew we needed to move to an in-house, in-house solution so we could take support into our own hands. For those who may not be familiar with what a game backend platform provides, here's a short list of some of the most important features Grilla Tag needed in order to run as a live service game identity and authentication. We need to know who you are and that you're actually allowed to play. Do you own the game? Have you been banned, etc. Moderation goes hand in hand with identity. Players can submit reports of other players behaving badly, and our support team can sanction bad actors. We also do real-time voice moderation with an AI product. Analytics. We need to measure what players are doing in-game and use that data to make the best experiences we can for our users. Per user storage, players need to save things like what they're wearing or what their score is or other data, and that data needs to follow them across their devices. Per title configuration. Grilla Teg and Orion Drift need to be able to remotely host configuration so that they can tweak things on the back end without releasing client patches. And then commerce and IAP, we need to sell things. We, we need to pay for development. We need to make money. Most games need these as sort of table stakes, and while we could have just built them piecemeal as Orion Drift or Grilla Tech needed them, we decided that we would much rather build a central generic platform that we offered to the game teams as a fully managed product offering. So with that in mind, we started working on what I call mothership. Mothership has 3 pillars, a back-end service app, a developer dashboard, and a game SDK. Today we're primarily going to focus on the Mothership app. In Mothership, we have a few models for hosting game tenants, but they all revolve around the concept of a vertical. Another word you might use would be sell. Multiple smaller game tenants might share a vertical, but we can also spin out larger customers to their own dedicated verticals. This gives smaller games and incubation titles great cost savings, but allows us to scale our largest titles as far as they want to go. Each vertical is a network isolated clone of the mothership's stack, which itself is an OJS app hosted in ECS along with some storage. The technology we chose here was primarily driven by the constraints of our dev team size. For the majority of Mothership's life cycle, we've been a dev team of 1 to 3, and so we needed to rely on as much managed product as we could in order to keep our development velocity going. One interesting thing about the Mothership app itself is that it's a monolith, but it's deployed like a microservice. We shard compute and database by feature area. So for example, the A compute and database scales independently of the user storage computing databases. However, rather than relying on intra-service communication via an API like you would with a traditional microservices deployment, each instance. Is running the same code. That means that if the off database needs some data from the moderation database, that can all happen within a single process and every node has access to all of the charted data at rest in all the various databases. We believe that keeping the contracts in code makes them easier to reason about and having a single version of the app deployed at all times simplifies operations and on-calls for our small team. The most important piece of the mothership infrastructure outside of the app itself is our CICD pipeline. This is not meant to be readable. This is just kind of for scale about kind of what we're doing here. Uh, Mothership is a fully continuous integration, fully continuous deployment product. We utilize trunk-based GitOps, meaning that when a pull request merges to Maine, we have a GitHub action, build a new docker image and push it to ECR, which starts our pipeline. This keeps changes small, isolated, and easy to roll back. Of course, we're not just merging domain and hoping for the best. We use code pipeline to serialize our changes to one vertical at a time. Code pipeline also gives us robust control over the CI process. For example, if we have a risky change or a change that can't be fully validated without working with other services in the cloud, we can pause the pipeline at a given point in time and disable transitions out of our pre-prod stages. Code pipeline also makes disaster recovery with tasks like rollback very simple. To manage actually deploying new versions of the Mothership app, we use CodeDeploy blue-green deployments in a canary strategy. When a new version of the app makes its way to a specific ECS cluster from our code pipeline, CodeDeploy spins up a new set of tasks with a new version of the, the code, runs an integration test. Suite which we run through step functions and if that passes, routes a fraction of traffic to the new app. We've set up some key reliability metrics with cloud watch alarms and if any of those trip at any point during the canary process, the deployment is stopped, rolled back, and the pipeline stops and alerts us so we can address the issues. I want to talk a little bit in depth about our integration test suite because I think it's a really critical piece of reliability engineering that often gets overlooked. Our integration test suite is a Python locust test project. Locust is traditionally a load testing framework, but we like how easy it is to model realistic user workflows in its test runs. The other thing that's pretty unique about this integration suite is that it's actually driving the same SDK that our games use. Our game SDK is a C++ SDK, but we have projections up into Python, C, all sorts of languages, so that higher level languages can take advantage of the same tools that the games are using. This is pretty unique and it allows us to do some really interesting things. Like for example, we can run the test suite against every known version of the SDK that a game in the wild is using, so we can be confident before a change rolls out to a vertical that a customer is on that that change has been validated against the same SDK that that game is on. This piece took some real work to build, and it takes some real care and feeding to uh to to keep current, but it's paid for itself many, many times over with the peace of mind that it gives us. About a year into development, we worked with a load testing partner to fully load test Mothership. We targeted 100,000 ccCU per vertical because that's what grew and scale Mothership to hit that goal. On a previous slide, we showed um RDS proxy as part of the architecture diagram, but I want to note that at the time we ran these load tests, that was not yet a product we were consuming uh due to an incompatibility with another tool in the, in the services. Our main bottleneck through all of that load testing process was the cost of opening new database connections. That's really IO intensive on the app side and pretty CPU intensive on the database side. Um, if we were scaling like a hockey stick, uh, we'd open a ton of database connections in fast succession, and then that would start to dominate the database CPU which would then start to slow down IO in the, in the app, which would then cause the event queue in the app to start to back up, and we'd have this, this kind of cycle of death. As we were kind of tuning and tweaking this, we learned that the, we, we discovered that the best tuning for the client side uh connection pooling that we were doing was to use large instances with many cores that we shard across those cores, um, that allowed us to, you know, it, it gave us two big wins there. Uh, the first was that the CPUs were more powerful and could tank more traffic before they started to fall over. And because we were provisioning larger instances more slowly, the database could keep up better with those incoming connections. Um, the other thing we did was we provisioned larger database instances to keep them pre-warmed to avoid these potential spikes. Configured like that, this was our ECS run rate. This was from mid-summer, which is the lowest point of the year for VR gaming in general. And we were looking at about $1000 a day of purely compute spend, and that seems pretty expensive. In June, we started a broad effort to understand and reduce our cloud spending. Um, but in kind of parallel to that, we happened to get rid of the, the piece that was keeping us from using RDS proxy. So that was now back into the infrastructure. When that happened, we realized we could scale down all of our kind of pre-warmed large provisioned instances and move almost exclusively to uh serverless Aurora compute. Um, in places where we were already on Aurora service list, we were able to reduce our minimums across the board, although specific numbers vary by feature. This didn't address our steady-state compute costs though, because we never went back and examined, you know, those provisions. Cost Explorer showed that we'd see a small cost improvement by switching to Graviton. At first it seemed a little inconsequential. Depending on the view, we were showing between 4 and 6% savings. That seemed low because we'd heard stories of teams realizing much larger savings. We reached out to our account team who put us in touch with some experts, and the experts projected a much larger savings of about 20 to 30%. We decided that that was worth exploring, and so we did. Like many teams, we had a sort of vague idea that running on ARM was probably cheaper or faster, but it wasn't a priority for us. Maybe it would be difficult. After all, sometimes porting games between different consoles running the same architecture is hard. Porting between different architectures may be similar. Now that we were serious about cutting costs, we needed to take a closer look. At the time, nothing in another axioms back end ran on ARM. We were worried that there might be like native node packages, potentially even transitive includes that we didn't plan for that might not have good arm support. We didn't know if there'd be impacts to CI tooling or monitoring, particularly the image build. And we were looking at other. Broader cost-cutting measures and wondered if we were doing too much too fast. Plus, frankly, like we had a bunch of different estimates coming in, and we didn't know if the effort was going to be worth it in the end. Um, with those concerns in mind, we scheduled a few meetings to start to de-risk, a potential graviton migration just to see what it would get us. Um, it turned out that migrating to Graviton was a lot simpler than we anticipated. In the time it took us, the, the time it took us to finish scheduling meetings, uh, I had a working proof of concept done on my PC. By the end of that day, we were essentially ready to go to Prague. It turns out node apps are fairly simple to migrate to ARM. ARM is popular enough that the majority of common MPM packages just work out of the box. We didn't run into any cases where we had some native dependency that just wouldn't work. Uh, Docker Buildex makes cross arch images, basically trivial. In about an hour, most of which I spent doing research, I had local cross arch builds, and with a little bit more work, I had a local Docker composed stack working cross arch using QEmu. Um, I was able to validate startup, run some tests locally, and it all just worked. Finally, as we've already discussed, Mothership has an extensive CI tooling and test coverage. So I knew when it was time to go live, we'd have our bases covered. With some local proof of life and a bit of a tailwind, I started the process of getting us officially migrated to ARM. Because we have a fairly CICD pipeline, we knew we'd be able to build a ton of confidence in the migration before it impacted players. Here's how we took this change to prod. First, I went into code pipeline and I disabled two transitions, the transition from our staging pre-prod stage to our first production vertical, which we used for testing and incubation. Then I disabled the transition from that vertical to the first vertical running an actual game, in this case, gorilla tag. That ensured I had a safe setup where we could mess around in staging all we wanted and never impact a paying customer. Then I built a multi-arch image of the Mothership app on my personal desktop and pushed it directly to ECR with a custom tag for testing. I manually created a task provision that targeted the arm processor family and deployed that to a staging ECS cluster. Essentially, I was just looking for app startup and health checks. Without working, I merged the changes to GitHub that updated our builder action to produce multi-arch builds, as well as the necessary task definition changes to use the smaller to use smaller arm instances, and we watched it roll out through our normal CI process. When it got to staging, the new ARM instances started up, ran the integration tests, and performed canary traffic routing like normal. The next day, once I was satisfied that our metrics looked good in staging, I enabled the I enabled the transition to our first prod vertical. Again, this vertical is mainly used for testing and incubation purposes, so it was pretty safe to do. And again, the task started successfully, the integration tests passed, and the traffic routing looked fine. At that point, we had a high degree of confidence in the process, so I enabled the final transition and we began rolling changes out to the game team verticals. There was 0 downtime and zero noticed impact of players. Here's our run rate when we made the switch. This pricing trend has held to date. We went from about a $1000 a day spend to about $250 a day, which is just massive. To use some round numbers, that's roughly a 60% compute cost savings, and we saw around a 30% reduction in our total cloud spend with just this change. This is, I want to note, not opting into any compute savings plans or any other pricing agreements. This is not using spot instances. This is pure on-demand usage. Um, we've seen other side benefits as well. One kind of confounding change here that we made was that we went from large, uh, like multi-core instances down to two VCPU instances at the same time because we no longer had that, that database, uh, proxy dependency. So, um, you know, the, the, the, the instances themselves are cheaper and because we can scale much more granularly with these smaller instance sizes, we're allocating about 25% fewer VCPUs. We believe that that's due to both being able to allocate much more granularly and also because there seems to be some performance benefit just moving to ARM, um. The other kind of side benefit we got is that on the large instances, if we were kind of hovering right around an auto scale boundary, we'd get this fluttering effect where a large instance would come on and off and on and off and on and offline as we kind of were right around a scale boundary. That could lead to some instability in our system because we can now scale so much more granularly, we're much more stable across all traffic levels. So what did we learn? First off, it's important to update your assumptions as things change. We ran in a configuration with a ton of unnecessarily expensive compute for much longer than we needed because we never revisited our priors about how the system worked, you know, and how the database connections worked. It's important that as you make any change to a larger system, that you're thinking about the rest of the system and what the knock-on impacts could be. Secondly, some changes that sound big are actually pretty small. It sounds like a huge shift to change of processor architectures, but in our case it was trivial. It's worth going beyond blog posts, marketing and gut level estimation and getting your hands dirty, even a little, just to understand the work involved in any change. Um, the next two points are linked. Cost Explorer tends to way underestimate your savings and work with your account team to find experts. When we started trying to cut costs, the most valuable step we took was bringing in our account team and sharing our plans and concerns with them. They were able to get us in touch with experts from Graviton, RDS, Elastic Cash, and a few other services to find places where we were being inefficient. They were able to walk us through potential savings and help us wrap our heads around the effort required. They saved tons of research and it helped us focus on the tweaks that would really move the needle. The last and most important thing, um, I think is any investment in your CI, CD, and test infrastructure pays huge dividends. I've been very fortunate at another axiom that I've had complete support from our engineering leadership to make large investments in our tooling here. And now we have incredible confidence that when we deploy code, it'll land in a working state, that it won't break any live game clients, and that we always know what is where and how it got there. It's Mothership's secret weapon for stability. So where does that take us going forward? Uh, Ryan Drift and all future games are 100% on Mothership. Grilla Tag is using Mothership for tons of critical workloads, including authentication and analytics, but there are still a few holdout pieces we're working on migrating. We're exploring different hosting models, including a larger emphasis on multi-tenant verticals to enable low-cost rapid prototyping. We're exploring new platforms, new opportunities, new services we can offer, and we're working with our game teams to really evolve the platform. Um, the last thing just as kind of a call to action, come try our games. These, if you have a headset somewhere, uh, you know, these links will take you to the meta product details pages for our games. Highly encourage you to check them out. Uh, I think they're a lot of fun and I hope you will too. Um, and, and that's all I have for you today. So thank you so much. Um, if anybody has any questions or wants to go deeper on anything, um, I'll be available over there, um, but otherwise, thank you so much.