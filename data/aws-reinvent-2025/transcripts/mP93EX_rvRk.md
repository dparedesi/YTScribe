---
video_id: mP93EX_rvRk
video_url: https://www.youtube.com/watch?v=mP93EX_rvRk
is_generated: False
is_translatable: True
---

Thanks for coming and thanks for your interest in this session. I'm Sebastian from Volkswagen Group Services in Germany and um I'm responsible for the department IT Service strategy and innovation and our mission is to turn technology into value and innovation for the whole Volkswagen Group. And my name's Kim Robbins, I'm a senior AI gene generative AI strategist from AWS and with me is uh Liam. Hello, my name's Liam, I'm a data scientist and I'm part of the AWS Generative AI Innovation Centre. Now Sebastian, take us away and, and teach us a little bit about Volkswagen. Yeah. So let's start with the scale of what we are supporting. In the previous nine months of this year, the Volkswagen Group delivered over 6.6 million passenger cars worldwide, and that's not just a big business, it's a global ecosystem where marketing is absolutely mission critical. No, we are in the middle of a massive transformation, as you can see, over 1 million of those vehicles were electrified, with sales up more than 42%, and this explosive growth in a new product category demands marketing that can pivot and scale instantly across every market. So now look closer to the world map, at the world map. Our challenge is to ensure global brand consistency while facing widely divigan market realities. On one hand, we are seeing strong acceleration in emerging markets. Look at South America with sales up more than 15% and the Middle East and Africa with sales up more than 10%. These are high opportunity regions where we need to deploy fast, high quality content to capture and establish brand loyalty faster than our competitors. On the other hand, We must address critical challenges in our established markets, specifically North America with a reduction of 8% and China with a reduction of 4%, and these declines require the exact opposite of generic content. They demand hyperlocal data-driven marketing interventions to stabilize sales and adapt quickly to local pressures. The problem is bigger than just global scale. It's about brand identities. So the group is built on 10 powerful brands organized into 3 distinct groups. Core progressive and sport luxury. So can any of you list all the 10 brands we have here for passenger cars? Don't be shy. OK, I will give you the solution. So the core brands with Volkswagen commercial vehicles, Skoda, Seat, and Cupra. These brands are built on trust and utility. The progressive group with Audi, a Lamborghini, Bentley, and Ducati. This group is about innovation and emotion. And Sport luxury with Porsche is a pinnacle of aspiration and performance and. This segmentation here is non-negotiable. Every brand has its own unique sacred DNA. This is the segmentation challenge for our Gen AI. We couldn't use a single large generic model. Our technology had to be engineered to respect this hierarchy. Ensuring that the Bentley marketing line never ever sounds like a Seat line, and the Porsche story retains its exclusive high performance edge in every single market. So don't be afraid. I will not explain this in depth, um, but to understand, to understand the scale of the solution. You have to understand the old problem here. And this is the traditional content supply chain. It's a linear manual process that takes us from concept to campaign. The most critical point here is the world premiere deadline. This is a non-negotiable milestone for a major product launch. Every step in this manual chain, the planning, the expensive photo shoots, the review cycles used to take weeks and sometimes months. All pushing against that fixed launch date. In a competitive yearly, nearly 10 million car business, this time lag is financially unacceptable. Our primary goal was total acceleration of this slow process here. But the manual process didn't just hurt speed. It introduced massive risk and complexity. Our struggle came down to the following constraints. The first constraint high creative demands. So all this risk and compliance review has to happen while our teams are under pressure to deliver massive volumes, massive volumes of unique hyperlocalized content. The demand for creative volume was simply exceeding human capacity. The second constraint, it's confidentiality, or we call it the risk. Look at asset production. We often rely on pre-production cars, prototypes under camouflage to create launch assets before the car is officially revealed. And this is a huge confidentiality risk. We needed a digital workspace that could accelerate content creation while guaranteeing zero leakage of these highly sensitive um unreleased vehicles. And the third one Legal and compliance, that's a bottleneck, that's a real bottleneck, or this is the real killer, the real killer of speed and the greatest source of manual effort because we use these pre-production assets, we must then manually verify compliance and ask ourselves, is this content legal? Is the feature claim compliant in that specific country, manually checking every single asset for brands, legal, and regional compliance. Across 10 brands and 7 regions, around about 200 countries created an agonizing bottleneck. In the review and approval stage. So to give you an idea of this complexity, think about marketing a trunk feature. Of Volkswa Tuareg. It's a mid-size luxury crossover SUV. In Sweden, local law requires a dog to be transported in a safety harness or a transport box. If our German marketing team uses an image showing a dog loose in the trunk, that content is legally noncompliant in Sweden. Trying to manually catch thousands of micro regulations like this is simply impossible at scale. Another example is when, when we are launching the all new Volkswagen 87 GTX, its interior design is one of its most attractive selling points. The clean sporty cockpit and the massive redesigned infotainment screen set a new benchmark in our electric portfolio. And this striking design and powerful digital interface are exactly what our global marketing teams need to highlight. But these features also bring a critical challenge to our content production. No, no, no We must avoid a key compliance risk. Driver distraction law. Or driver distraction laws in many major markets, including Germany, drivers cannot perform complex tasks on in car screens while driving. Our marketing must always show safe legal behavior. Any detailed interaction with the ID 7's systems should appear only when the vehicle is stationary or involves a quick glance-based adjustment, so our safe operation compliance filter checks every asset to ensure this standard is met. So to conclude, The fundamental problem was this a slow, risky, manual process governing content that was confidential, legally complex, and required immediate scale. We needed a system. That could generate content with the compliance of a computer and the creativity of a human. The solution required a new technological foundation to enforce consistency and compliance by design. That's why we turned to AWS. And began building our custom Gen AI platform solution, so. I think now the time has come. Now that we all understood. The problem, let's dive deep into the solution and the technical, um, the technical core of our joint solution. So Kim, the stage is yours. Thank you. Alright, so now it's time to, to pop the hood and to have a quick recap, Volkswagen needs to move faster and really streamline their content supply chain and quick get quicker go to market. And so we worked back backwards from this problem and really landed on two core capabilities we wanted to enable Volkswagen with. So the first part was image generation, so to empower creative teams to accelerate content production, and the second part was image evaluation, so to automate the compliance and brand checks at scale. So how does this actually work? Let's look at what the solution looks like. So this is the full picture, end to end image generation and evaluation pipeline. What used to take weeks, now takes just minutes. There are 3 stages to walk through, so we start with the prompt. This is where the creative team enters a prompt, for example, generate an image of a Volkswagen Tiguan. This is where the creative teams can express their intent, and however, before it hits the models, we use a large language model to enhance the prompt. This adds details and optimizes for better output quality. The second part is the prompt goes to a custom model hosted on Amazon Sage Maker. The key here is that we used a fine-tuned model to understand Volkswagen's vehicles, styling and brand aesthetics. And finally out comes a generated image. And then the last piece in this evaluation, we built two parallel evaluation paths using Amazon Bedrock. First we have the component level evaluation, segmentation models, breaks breaks down the image and compares it against reference images. And the second part is the brand guidelines evaluation. Here we check that the generated images against Volkswagen brands documentation, and this can vary brand to brand across the 10 different car brands. Both are powered by Frontier Vision language models. And we're going to take you through each of these stages, and let's start with the generation. So to understand how we fine tune this for Volkswagen, let's quickly walk through how diffusion models actually work. So first we start with random noise and the diffusion model iteratively denoises step by step guided by the text in the prompt. Each step removes a little bit of noise and refines the image until you get a coherent output. In this image you can see that the output of a base model trained on the entire internet. The problem is that base models produce generic images. They have no knowledge of unreleased vehicles. Volkswagen can't use this for cars that haven't been announced yet, and that's where the confidentiality issue comes up and becomes a deal breaker for a brand like Volkswagen. So how do we solve this? This is where fine tuning is the solution. We take the base diffusion model and train it further on Volkswagen's own image library, um, for example, official vehicle photography, product shots, approved brand assets, we can even take images from their digital twin. The model now understands Volkswagens uh specifically, not just what a car is, but exactly what a Volkswagen Atlas is. The exact grill design, the badge placement, body lines, the details that really matter for brand accuracy. And this solves the confidentiality problem because the model can now learn the vehicles that haven't been released yet or images that don't exist anywhere on the internet, training on internal proprietary data. The result is that we end up from a single prompt of Volkswagen Atlas by the Golden Gate Bridge, and the output is a brand accurate atlas with correct proportions, the right styling, and the location that resonates with the North American audience without a photographer and a studio and no six week production timeline. This is the power of fine tuning, however, the magic is in exactly how you do it. Please take us through that Liam. Thank you very much. So exactly as Kim said, before we go through fine-tuning, it's important to understand how these image generation models actually work. Because it almost feels like magic. You just describe what you want to the image generation model and then out pops an image that looks really good. But there's a lot that goes on between those two stages. So I'm going to take you through how we can train a diffusion model, which is one of the most popular frameworks or architectures for an image generation model. As always with machine learning models, we're going to start off with data. At the top left of the slide you can see we have a generic image of a car parked on the sidewalk. And you can see as it progresses along to the right, it seems to get blurrier, noisier, and it's generally harder to make out the details. And we're doing this on purpose. We're actually injecting noise into the image to make these image these details harder to find. Until at the end, it's pretty much just noise, it's like TV static. And the challenge is, can an AI model take that noise and get back to the original image, which almost feels like an impossible task, it's like getting an image out of the dust really. So this return leg is where the AI actually is. We start off with the noise and we need to be able to remove the noise from the image to get a high quality final output. And to do this, we use a transformer. And all of you would have been using transformers, whether you realize it or not, they're the backbone of large language models. But large language models, and these are images, so there's a bit of a difference there. With language you have words and these words roughly translate into tokens, but with images it's a bit less clear. So for this you have to take patches of the image. Instead we chop up the image uh which is this noisy image at the beginning into these small segments and each one of those you can almost think of like a token. And we also need to provide the model context of what it's generating. So next time you ask it to generate that it knows what to generate. So in this case we provide the label, a car parked on a on the sidewalk. In the latest image generation models, this stage is actually done at a higher order representation in the latent space, but the intuition is really the same. And then you can see using this transformer we're able to remove the noise step by step. So we actually don't go in one step, the transformer is predicting how much noise and how to remove the noise from the image and it iteratively goes through until we get an output that looks very similar to what we started off with. And this is how you would train a diffusion model. However, as Kim mentions, base models are just trained on pretty much the entire internet. And they don't have that deep level understanding of what it means to be a Volkswagen Tiguan or a Volkswagen Golf. So we need to add some domain specific knowledge into this model and actually teach it how to generate these images. So that's where Dream Booth comes in. There's many different options for fine tuning image generation models. A personal favorite of mine is called Dream Booth, um, and particularly because you can have very few images, so in this dataset here, we have a data set of around 3 to 5 images. If you have more, that's great. And these are photographic real images of the car. We also have this text prompt you can see next to it, that says a VW T1 car. I think the keen-eyed amongst you would have noticed there's some square brackets there and that is actually by design. So that's to make the VW Tiguan token a special token, meaning when the model sees this token in the future, it knows to generate what it learned during this fine tuning process. And then from there the model attempts to generate its best guess of what the car actually looks like and then we can compare back using this reconstruction loss, comparing the generated with those real images. Now anyone here who's uh has any experience of training machine learning models knows about the risk of overfitting. We don't want the car, we we don't want the model to only produce exact versions of what it's seen in that data set. So, this is what sets Dream booth apart. is its ability to use prior preservation. The idea is that we can make sure the model generates product accurate images of the Volkswagen Tiguan and it can still generate generic images of cars, which almost sounds a bit funny, but the idea is that we keep that flexibility in the model so it's not too overtrained um on that dataset. So in this process we generate a lot of generic images of cars you can see in the bottom left. And then we using the same process, the model will generate those images and compare back to that dataset. So it's actually kind of a joint training objective between training on the Volkswagen Tigu One and generic cars, meaning you get a fine-tuned model. That's very flexible. So that's the algorithm of how you can fine tune image generation models, but there's still some remaining questions of exactly how do we update these weights of the model. So back when models were a bit smaller, a typical setup would be to use full fine tuning. This means updating all of the weights of the model directly. In this diagram, you can see on the right here. In blue we have the pre-trained weights, these are the base weights you get out of the box from any image generation model you have. And in this fine tuning process, we calculate the weight update matrix. So this is all the small changes we need to make to those weights based on that fine tuning. However, Very very similar to large language models, the size of image generation models has also got huge, so for reference last week, the new Flux models in the open source range um has over 32 billion weights, which is actually very similar to what you see in large language models. So this is why looking into parameter efficient fine tuning is very important here. You may have come across Laura before, which actually means low rank adaptation, which is probably the most prominent methods within this family. If you haven't before, the idea is you can reduce the number of trainable parameters by around 10,000 times and GPU memory requirements by around 3 times. How it works is you'll notice on the diagram on the right that larger weight update matrix has been split into two smaller matrices. This is a bit of a maths trick going on that instead of trying to learn one large matrix, we actually learn two smaller ones that can be multiplied together to create that larger weight update matrix, meaning far fewer parameters to train. Great, so that's everything about fine tuning. I'm now gonna pass on to Kim to go through how can we get the most from these image generation models at inference time. Thank you very much, so fine tuning gets us a a model that knows exactly what a Volkswagen car looks like, but there's another challenge, the people that are using this system aren't necessarily prompt engineers. They're marketers, designers, regional teams, and they need to generate images quickly without learning specialized syntax. The prompt engineering for image generation is not always an intuitive skill. So research shows that to have an effective prompt you need to have style modifiers and descriptors, things like cinematic lighting, rule of thirds, shallow depths of field, and most users don't necessarily know these terms and they shouldn't have to, to use a product that's effective. And so our solution was to automate and automate uh prompt optimization using a large language model. So in this case we use Amazon Nova Lite to sit between the user and the image generator. So the user writes something simple and natural language prompts, something like generate an image of a Volkswagen Tiguan. And Nova Light enriches it, adds those style mo mo modifiers, technical details, composition guidance, and really optimizes the prompt, to get what uh to get a really high quality prompt that gives a good image out of the image generation. Model, this removes the skill barrier and marketers get a high quality output without necessarily having deep prompt expertise. This also means much higher consistency for Volkswagen, across all teams and regions and any users and also results in less trial and error. So let me show you what this looks like in practice. On the left we can see like a before image where we've got a, a basic simple prompt like a cartoon cat climbing a tree, and this is how most normal users ride a prompt, and the output is fine I guess. However, it's just a cat and there is not, it's a little bit flat, a little bit basic, and it's really lacking some of that character. And then on the right we have the after where the prompt's really enhanced, so it's the same intent, but Nova Light has really enhanced the prompt and added sort of style modifiers like brightly colored, large expressive eyes, it's added some motion into the prompt, and this really results in a dramatically better, more vibrant, engaging and polished uh output image. This vocabulary gap is is what we're really trying to solve, so the user didn't need to know all of these terms, and the large language model already understands the basic intent and translates that into the prompt that is best for the model. And now if you imagine this applied to the Volkswagen uh vehicle imagery at at real scale, uh the system enriches like a basic prompt, like a Tiguan in the mountains and takes that and adds lighting and composition and mood and also the appropriate brand uh styling that Volkswagen needs for their, their content generation. Meaning professional quality results every single time. Now going a little bit further, so far we've talked about training on, on photos and existing uh images, but Volkswagen is really pushing this further with uh their partner SolidMta and Univus. The shift that they've gone through is to not to think a little bit outside the box and move away from just having photos but also 3D assets. To do this they've uh built a a pipeline to go from a CAD image to a a unreal engine and then using Nvidia Omniverse to, to make a digital twin of their cars, and why would they do that, so this makes a a means that they get a perfect, Accurately, uh, exact with exact geo geometry from the source of truth, the CAD drawing in this case, and full control on any angle, lighting and environment when they have this digital twin. That means that they have infinite variations without any photoshoots. It also means they can take pre-production vehicles and train before it's even rolled off the factory line in its first prototype. And now the workflow for how this works is they take the, once they've got to the point of having Unreal Engine and and Nvidia Omniverse uh digital twin, they can generate thousands and thousands of perfect images and then take that to the fine-tuned diffusion model. And we really uh from this results like a much more cost effective way of generating those, those uh assets to train the model in a more effective way. Now Liam, would you like to test the audience a little bit on, on what images are generated and not generated? Let's do that. So we, we've spoken a lot about how to use these image generation models and how to fine tune them, so we're gonna let you guys be the judge of how well we've done. Let's let's hold back for a second. So I'm gonna ask you guys to guess which one of these are generated and you're gonna put your hands up for which one. Hands up if you think the image on the left is the generated one. OK, I think that might be around 30-40%. 0, maybe, probably maybe just below half. How about hands up for the one on the right? Ooh, that's more. I think you guys are experts here. You're absolutely right. The majority, I think wisdom of the crowd wins here. The image on the right is a fine-tuned image. But I think you can tell the details, the fine-grained details actually make it very difficult to tell the two apart. And I think having this so large definitely helps, but I don't know if, if you weren't expecting to see a fine-tuned image, you might not realize it at all. Now it's not just about generating that one perfect angle. These models can generate all sorts of angles, so in this example we've generated the rear of the car in a different color in a totally different setting. And you can see it's still preserving a lot of that product accuracy as well. Let's do another one. Hands up if you think the image on the left is the generated one. OK, I think that's a roughly 1. Hands up if you think the one on the right is the generator one. OK, I think that's everyone else, so probably roughly fifty-fifty. Um, I'm afraid I lied to you and they're actually both generated, um. So I probably should have given you a 3rd option, but I hope that kind of shows how it is actually quite difficult to tell. So it's not just about generating content faster, it's actually also about new possibilities. So traditionally for each one of these marketing shoots you'd have to take the physical car on location, make sure it's the right time of year, the right lighting, also have lots of staff there as well. But with these models we can describe what we want. So in this case, we've got an image on the left showing the autumn or fall season. That you can think how this can be used for this kind of more hyper personalized marketing, depending on the time of year and where that user's uh accessing your content from. And another more interactive one on the right showing it speeding through a desert, maybe a little bit less brand compliance but um I think it's cool to see how these models can even model the how the sand interacts with the tires, so I think it's a really nice example. And here's a couple more examples that we can even generate at nights. And an interesting um observation we had is we didn't even train images on, on the nights in the training set for the for the fine tuning and it could still actually model the lighting very accurately. And we have another one in the cityscape, so again, kind of showing how we can have a more personalized marketing experience at scale and you could also AB test these across maybe 50 different examples for each one just because it's now takes seconds to generate these images rather than weeks or months. Another interesting output of this work is localization. So Volkswagen Group sell vehicles in over 150 countries worldwide. And you can imagine the challenge of managing this marketing campaign across that many companies, or that many countries even. So you can see in this example here we've generated an image of um the Volkswagen Tiguan in my hometown in London. um and you can see it's got the classic uh double-decker buses behind, you can see Big Ben and Westminster, so all very recognizable. And this is a real game changer because this means you can generate location specific content for anywhere in the world at speed. Great. So that's all the nice images, um, and now we're gonna pass on to Kim to show how can we detect even the smallest details are perfectly accurate. Thank you, so, it's easy enough to generate really beautiful images, but beautiful isn't enough, we really want to be able to, Generate really accurate images. So if you zoom in you'll see one of the problems in this image, we have uh the image on the left and then you can see that on with the reference compared to the generated image that the wheel is actually different. And that they don't match, the spoke pattern is totally different, the design is wrong, and to a consumer this might seem a little minor, however, to Volkswagen that's a huge deal breaker. But why exactly does accuracy matter so much? So these images are advertising a real product to real consumers, and if someone sees this image and walks into a dealership, and then the car looks different, that's a, a really big problem. The trust, the brand integrity, and in some cases, legal liability are on the line. And the scale makes this even harder with multiple vehicle models, many different trim levels, different regions, and certain configurations are only available in certain regions. So we're not really talking about just one car when we talk about a model, we're talking about thousands of thousands of different configurations, um, and different configurations being available in different regions. Our target was to get to an accuracy level of a minimum of 95%, and to do this, every single component needs to be verifiable against the real product, and we need to do this automatically and at scale. So how do we actually evaluate these images? Let's get into that. Liam, let's dive a little bit deeper into how, how we break that down. Sure. So in the field of computer vision, traditionally for evaluating images you might use metrics and these metrics give you a single number describing whether that image is accurate to the reference or not. However, these metrics are usually based on the pixel values and they don't really understand what the overall context of the image is. And not to mention that these metrics don't give us any explainability. So this is where we turn to using vision language models which have a strong vision reasoning capability and inherently have a very good ability to explain why because they're language models. However, even the best vision language models can't process all the minor details in a whole car at once because a car is probably one of the most complex piece of machinery you'll ever buy in your lifetime. So Instead of taking this whole reference image and just passing it through to to compare it to the images we've generated, we instead use an image segmentation model. So you can see here, you see these bounding boxes, this is actually we're stripping apart all the different components and treating them individually. So just like how Volkswagen might may manufacture the car, starting at each individual component, making sure it's all perfect and bringing it all together, we're actually taking a very similar approach and pulling apart, checking that all the small details are accurate, which will give us a high level understanding. So we do the exact same sort of generated image as well. So you can see here we have a generated image and we're pulling it apart into its components and you can kind of see where this is going. We have the components of the reference image which is real and the components of the generated image um and we can put those side by side. For the specific models we used here, we experimented with a few different options. Uh, we found the open source Florence models were very effective and we were able to host these on the SageMaker endpoints. Um, I would suggest you guys look at um Meta's latest release with SAM Free. I think this would be very powerful in this area as well. So I'm gonna pass it back over to Kim to go over an end to end example of exactly this. Thank you, so we start with the reference image that we have on the left. And then we have the AI generated image on the right. And then we go a little bit deeper. And so to show what that looks like, uh, here you can see we've got two different examples. So on the left side we have component level comparisons, so for example the isolated headlights and also the grill. This is where the details really matter, and we take not just one reference, we take multiple references to compare it to. Additionally we also take the more sectional level comparisons, so for example, the right side of a full vehicle or the profile of the door and the side panel, and the reason that we do this is we need to make sure the body lines, the proportions, where the mirror is and where the door handles are, that they're all looking accurate compared to a reference image. Again we're taking multiple reference images per component, and this also allows us to provide different lighting sources and really build a robust, uh, robust comparison rather than relying on just a single shot. Um, this also unlocks configuration flexibility, so when we think of, for example, in North America there might be, um, 5 different wheels that you can pick on a certain model, and we can really check that those particular wheels are ones that are available and and generated in the image, whereas in another region such as Germany, it might be a different set of configurations that are not available, and so we really wanna make sure that the images that are generated are accurate for the region. So now that we have our pairs, how do we actually score them? And this is where it comes down to uh a large language model as a judge. And here we're using uh Claude Sonnet 4.5 or you could also um use the recently released um Opus 4.5. Um these models are multimodal and that means that it can both see and reason about images. So we pass in the generated image and the reference images and we ask it exactly how accurate it is. Here we've picked just two metrics, but we actually evaluate it across between 8 and 10 metrics per component. Um, but we don't just get like a single score, so here you can see that for example the housing and trim scored a 5 out of 5, nearly identical, integrates well into the body lines. Um, however, for internal structure it only scored a 4 out of 5. So there was slightly more detailed in the generated image than the real component and the model provides this language reasoning for it, a human to go, uh, check, but this allows that process to be much quicker. Here's another example, so we have the same approach, but now it looks at the overall image and at, in the first take it, it looks great, it's the same age that we showed before, iconic London setting, um, authenticity, it scores a 5, it's natural, it's not staged, but if you look closer, the license plate is actually incorrect and it's for the wrong region. So on the number plate um is actually the German registration, and we get the proper reasoning why that is and that's why the license plate section scores a 2 out of 5. And this is the regional compliance in action. So, while you might see this great image and in on the first take to the naked eye or human validating it, they might miss that tiny detail, but the model never misses those small details and is able to, Uh, look at many, many different components of the image because we've done that segmentation earlier. Now Sebastian, would you jump in and talk a little bit more about the brand fit of Volkswagen? Thank you very much guys. OK, so far we have seen how we generate images and evaluate the technical details of um the car itself. But a brand is more than just a car. It is also the environment, the scenery, the weather, and the overall mood. So for every single Volkswagen brand, we have strict guidelines on how these emotions should be staged. On this slide, You see a simple example of one image that doesn't fit the brand story and one that does. Let me show you how we turn those brand rules into something an AI system can understand. So From this unstructured text on the left, the model generates clear evaluation criteria that we can use to judge AI generated images. In other words, we translate human brain language into machine readable rules. And all of these rules live in a dedicated portal, including detailed guidance on how to stage each each brand. We take the text from these brand guideline pages and feed it into Amazon Bedrock Nova Pro. And these generated criteria are then combined, are then combined with the image we want to evaluate. Both are sent to Amazon Bedrock with Cloud Sonnet, which performs a full brand compliance analysis. And the result is a set of indicators. You can see this there, such as um yeah, the overall brands adherence, the color representation, lighting, authenticity, and even imperfection. And this gives us a consistent objective view of Brain fit that we can apply at scale across thousands, thousands of images. Now, let's have a look at a short demo. Of this process, we start with a very, very simple human request. I want a red Tiguan in Paris. That's all the marketer needs to type. Our system then builds a perfected brand safe prompt on top of that input. And this becomes the basis of the entire image generation process here. And Here you can see the generated image that comes out of this process. From a simple request, we now have a high quality visual that follows our product and brand requirements. The next step is to automatically evaluate this image against the brand criteria. That we created earlier in this process. So In this case, In this case, the image meets all our environment guidelines from scenery and lighting to overall mood. And here you can see the detailed report for each individual indicator, so. So that our team understands exactly why an image is compliant. Now, Let's look under the hood and see which Amazon Web Services we use to make all of this possible. So. Thank you, so generating an image is one thing, but evaluating if they meet Volkswagen brand standards at scale, that's a real challenge. So we really wanted to open the door to continuous learning, and the solution we came up with was to fine-tune Amazon Nova Pro, to teach the model how to classify on-brand versus off-brand images and specific to Volkswagen Group standards. Now here's what this looks like at scale, compliant versus non-compliant images. On the top row we have all the compliant images, realistic settings, natural composition, urban streets, mountain roads, tree lined avenues. These feel like Volkswagen, grounded, authentic, aspirational, but also believable. Underneath we have the less compliant or the non-compliant versions, for example, Northern Lights on a beach, visually striking, but a little off brand. We also have uh a cosmic Galaxy background, doesn't quite feel like Volkswagen. These might look cool but they don't meet the brand guidelines, and the brand compliance isn't just about the car, it's about the context and really where is the vehicle placed. And the system catches these automatically before they ever reach the market. So how do we do that, so one new feature that we released recently is uh on SageMaker AI is Nova Recipes. And we really um open this up so you can choose things like supervised fine tuning, proximal policy optimization, or direct preference optimization. How it works in practice is really 3 steps. Firstly, you specify your data, you prepare the training and validation data in your environment of choice, and then you save it to your S3 bucket. The second part is you select the recipe. This is a preconfigured infrastructure recommendation. That includes uh what you need for sage maker, Hyperpod or sage maker training jobs, and the recipe tells you exactly how many instances you need. And then finally, you run the recipe. These are pre-tested hyperparameters, so that means there's no trial and error. This avoids multiple runs to find the right configuration. The bottom line, identify your model, select the recipe and run, it's that simple. Now Liam, can you take us a little bit deeper into this? Absolutely. So we're going to go in depth on the supervised fine tuning recipe. If you've never come across supervised fine tuning, it's a way of fine tuning a model. Um, the idea is you have realistic inputs in your dataset and for each one of those inputs you have a corresponding output you would like the model to generate. And from that it would be able to learn what the ideal outputs are for the input. In this case for evaluating brand guidelines, we have an image and we want it to generate a output for the evaluation that is um very accurate. So. How this works is we first sample one of these images, it could be generated, it could be a real image. And then at stage 2, we need that evaluation in our training data sets. So we actually ask a Volkswagen marketing expert to tell us for each image why is it valid or why is it not, so it's a kind of human in the loop approach. And then once we have enough examples we can then run the recipe and it's just a simple case of putting the data in an S3 bucket, running the recipe and you get a nice fine-tuned model you can inference with. However, Anyone that's tried to collate a data set for fine tuning before knows that that stage is very slow in the middle. Getting a real person to go over thousands of examples takes hours of people's time and you can imagine how poorly that scales across all of Volkswagen's different brands. So this is where we turn to synthetic data. Instead, We generate both the image and the evaluation because we can use those guidelines to purposefully generate good and bad images and therefore we know which images are good and bad. So from there we can have a good data set for compliant images, non-compliant images and run that fine tuning job. I'm gonna quickly run through how this works, so starting off with generating these images. As I mentioned, using those Volkswagen brand guidelines, we can use a large language model to summarize them and then to create 1000 unique compliance prompts for image generation, 1000 non-compliant prompts for image generation. And how this looks You can see at the top is an example of the prompt that's generated and an output. And that looks like a pretty typical automotive marketing piece. At the top you can see it's like a mountain road, all very compliant. Whereas at the bottom you can see it's asked it to generate the Volkswagen Tiguan on Mars in bright purple, so not very brand compliant, not very realistic. And because we know which ones are good and which ones are bad, it's just a case of generating that corresponding criteria that we can fine tune with. So to create a nice output we give it the original image for context. We give it an indication saying this image should be rated poor in this case in our rating scheme. As well as the image generation prompt for extra added context. And you can see on the outputs we now have our generated evaluation. So we have the images and the evaluation now which is great. So that that process that I described earlier, that would have taken many, many hours, means that now we can generate a full fine-tuned data set in just hours. So here's another example here of the corresponding compliance image as well as the evaluation and the non-compliant as well as that evaluation as well. So now we have our data, it's just a case of running the recipe. So with Nova model customization, the training is all managed for you, all of the infrastructure, all of the scaling, that's all there. And training only takes around 2 hours for our for our case at least. And then once you've trained your model, that's where I think the real unique power of Nova Model bus customization comes in is because you can simply use these models like they were any other model on Bedrock. So if you're using a Laura recipe as technique described earlier, you can use that with on-demand inference, making it really, really easy and no need to worry about hosting these custom models with big GPU instances. And as a result, we saw a very significant improvement in the model's ability to identify on brand images. So what we've really done here is we've founded this decision line between compliance and non-compliant images. But really it's kind of more than that and we kind of like to think of this as kind of one of the tools in this kind of new age of marketing. If you're generating thousands of unique images, you need a way to be able to have governance over those thousands of images at scale. So prior to what we've talked about today, we found the generation and evaluation of bottlenecks. But now they can both scale together with the techniques we've described. Finally, I'm gonna pass on to Sebastian to wrap up and put all the puzzle pieces together. Thank you very much. So then let me quickly uh wrap this up today you have seen how we use Gen AI to generate brand safe images, evaluate them against complex guidelines, and integrate this into our content supply chain. So together with AWS we turned Gen AI into a production platform that works at Volkswagen scale. Looking ahead Compliance will not stay where it is today. Our roadmap extends from pure brand legal checks towards political compliance, cultural sensitivity, and social balance in our content. And for sure One more thing, Video generation. So as a sneak preview, we are already going beyond still images together with our virtual reality experts. We can take the technical CAD files of a vehicle and turn them into full film sequences without a physical car. And video generation is still early. However, there is additional potential as the models improve in this domain. So no, to bring this all together, this is our Volkswagen Group service Gen AI platform. At the bottom, we connect our core ERP CRM, HR and PLM systems. On AWS. And on top we provide shared services, shared capabilities. Like we saw today, so image and video generation and additional documents processing, smart ticket handling, and AI coding for both personal productivity and expert services with every new use case like you see this here on the right, the service finder. BPM 2.0 and the Volkswah Group Service Coda, we gain additional capabilities that can be reused across the entire organization. And the entire Volkswagen Group. So What did we actually achieve with this platform? First, we created massive time savings in content production and evaluation. Second, we gained real confidence in brand compliance because the checks are now built into the process. And third, we integrated multiple projects into one shared platform instead of running isolated solutions, and overall this leads us to a much shorter time to market for our campaigns. So By combining our domain expertise with AWS, we built a G AI platform that makes our marketing faster, smarter, and safer. Thank you very much.