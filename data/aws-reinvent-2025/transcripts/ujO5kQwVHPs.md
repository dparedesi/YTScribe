---
video_id: ujO5kQwVHPs
video_url: https://www.youtube.com/watch?v=ujO5kQwVHPs
is_generated: False
is_translatable: True
---

Hello, everyone. Uh, my name is Husain Shell. I'm the, uh, CTO for AWS Energy and Utilities. Uh, thank you so much for joining us today and for being here and when you can be anywhere else, uh, in Vegas. Um, I am extremely proud to, uh, to present my, my colleague here and my, uh, my partner in crime and everything we've been doing around HBC and the energy. Um, space, uh, with many of our customers, uh, Michael Gual, the head of global HBC engineering and operations for Shell, is here with us to talk about, uh, what we have been doing for the last 4 or 5 year journey around not only high performance computing but also around generative AI co-innovation, and some of the work that we're doing around data and, and, and, uh, analytics. Um, I'm really excited, uh, for, uh, what's coming. Uh, fortunately for you he's gonna do most of the talking, but I'm here to answer any questions needed, uh, and without further ado, Michael, please, thanks, Husain. It's, uh, it's, it's great to be here. Thanks to everyone making the trek. I, I thought it was wise to try and walk here and maybe that wasn't as wise of a decision, uh, after the keynote, but, uh, thank, thanks for making the journey over here. Um, really excited to share the, the journey today. It's, it's been actually longer than 5 years and. And had some struggles and we'll talk about those in, in, in, in, in, in the talk today. But, uh, first and, and most importantly, um, uh, a long slide about, uh, cautionary notes. Yes, you're welcome to take pictures of it, Claus, but, but, uh, yeah, don't, don't take financial advice. We're in Vegas, you know, um, what, what are we gonna talk about today? I'll, I'll, I'll, I'll give you a bit of context of about Shell and, and what we do beyond the gas stations and. And, uh, what you see on, on the F1 track, but I'll give you a bit of context about Shell. Uh, we'll, we'll talk about our HPC, you know, our upstream challenges, and then how that translates to HPC, our journey with AWS, which, which started in 2017 on HPC, and we'll, we'll go through that. Uh, I'll show you what it actually looks like, um, in, in an architecture. We'll talk about what's next, and, and we probably will have some time for, for Q&A, uh, at the end as well, and the hard questions can go to Hussein. So, um. So first, uh, a, a bit about Shell and, and, and just setting, um, you know, again what do we do besides the gas stations, uh, so over, you know, 93,000 people across 70 countries, um, really touching all places of, of energy, uh. What we'll talk a lot about today is our upstream. So some of the big metrics there are 66 million tons of LNG in 2022 and over 22,800 barrels of oil a day equivalent production, but, but. Um Clearly, a, a, a lot of different areas and fund numbers and then what does that look like from a customer sector, um, again servicing integrated energy solutions across all, all aspects of energy, um, including things like data centers, uh, of, of course, which is a, a huge use of power these days, um. But again, hopefully, uh, all of you use gel products on your uh journeys to Vegas. Um, but let's, let's dive into Upstream and, and talk about who really uses our HPC and first, let's talk about what are we doing technology-wise and. And what are we trying to change? We're, we're a company where for decades we've done a lot of things proprietary, a lot of proprietary kit, um, a lot of build it ourselves, and what we're, what we're really finding is to unlock future ways of working integration is gonna be one of the keys. So that's where we come into strengthening our core workflows. How do we simplify our workflows? How do we take a whole bunch of. Different siloed applications and integrate them. How do we, how do we, you know, embed AI into these deeply technical workflows? And then on the right, where do we then choose to, to invest where there is still differentiation here, but there's also a lot of market standard. So how do we then pick the right targeted areas to invest and. And, and then that's where we, we know, we invest strategically there and we try and simplify the, the overall user experience. It becomes a pretty obvious sort of conclusion that doing this in the cloud and driving this integration in the cloud is a natural place to do it. And that's where, you know, if you, if you, if you look at our partnership with, with AWS initiated in 2017. Um, Shell started, uh, what was once upon a time SDU, which became OSDU, uh, for those familiar with it, which is, uh, which is now, uh, also a managed offering within, uh, AWS called EDI, um, which I'm sure you're saying we can talk about that when we, we talk about the whole stack, uh, but, but Ato Bissentel, they're, they're a key, uh, strategic partner for us in, in our hyperscalar strategy. And then in HPC we've been live since 2022 and that's embedded across our friends in the energy vertical, uh, uh, an embedded Proserve team within my HPC, uh, engineering team and, and of course our, our own developers and, and users but it's, um, it's been a real journey and this is what we'll, we'll talk about in a lot more detail, uh, shortly here but. Now let's talk about a bit about HPC. Who uses HPC? Why do we need HPC? Why do we need more HPC? Uh, we're, we're an industry that is, is famous for having machines in the top 20 list. Um, there is a lot of compute and very simply, why do we need a lot of computes to do a lot of physics? What are we, what are we doing? Seismic, for those not familiar with seismic, we send sound waves miles and miles beneath the, beneath the typically sea floor. Um, we listen to those waves up to 40 kilometers away. That generates petabyte scale data. That data then needs to be processed with a lot of compute. And generally speaking, The algorithms we use are decades old, but we've never been able to afford the compute. And so because of that, as we get more and more computation power at affordable rates, we just throw more and more physics at it. We take the assumptions out. We put more and more parameters in it. So as we get into more complex areas to find energy, as we get into basins that we've been in, and we want to image better, we need more and more geo more and more physics, geophysics, and as we scale those algorithms, we need exponentially more compute. So, the, the, generally speaking, we give a geophysicist, uh, a, a new, new piece of hardware. It is usually taxed immediately and, uh, until the next one comes out and, and similar with storage. The only type of storage we usually have is full, um, but, uh, but that's, so that's what's driving what we need and why we need so much compute. But now let's put HPC in the business context on, on that as well of. All of this imaging that we do is in the critical path for us, for us as a company to ultimately drive revenue and to drive some of those fancy numbers that I got to show at the very start. You need to have an image of what the subsurface looks like and, and so it's in our critical path and the good news is it's an embarrassingly parallel workload so if you throw more capacity at it, it will generally go faster. Um, scale is a nuance that you know that there are always nuances and bottlenecks, but generally speaking it's a very, uh, parallellyzed workload and that more capacity equals faster results, um. And then what do we want to do and why do we want variable capacity? We want to be able to make decisions on capacity related to our business decisions. Otherwise, what do we historically do? We guess you guess you buy a system and you hope it's enough, um, and it's a long procurement cycle and, and it's one that you're then stuck with that decision for some number of years because the amount of investment we wanted to be able to enable a value of information and be able to, to enable being able to to make capacity decisions based on our business decisions with far more immediacy and, and then. The waiting game of HPC for those that have deployed systems and we still manage them on premise as well. It is becoming increasingly difficult to get gear in in a data center whether it's the AI boom, whether it's making sure you have enough power, um, whether it's just general supply chain challenges it is quite the challenge and uh I'm very happy that AWS does a great job of it for us and uh that that that you guys get to worry about a lot of what uh we've we've historically had to deal with so. More, more complex geophysics is why we need a lot of our HPC and then why we want the cloud really is around giving us that variability, giving us that, that flexibility and really being able to say when we need something that we're able to do it. We had an example this year where we, we took an innovation, uh, strategy all the way to the top of the house and we said, all right, we wanna go enable this, we wanna go, we wanna go chase one of those targeted differentiation areas. And within weeks we had that capacity online full and the researchers active. That's something that historically as an HPC manager when someone comes to me and says I want more and here's some money, it's like great, where's the rest of the money and where's it for the next 3 years or 5 years? It's this is all right, let's go. It's on and and and and off we go. So that's where we are now. Let's talk about and the challenges, let's talk about how we got there and, and this is, um. This has been a long journey, and, and the good news is it's, it's. It started really slow, I'll be very honest, but, uh, it has gotten a lot faster. So if, if I use the Amaziian phrase of a, of a flywheel, so when we started this, this was very much a staircase journey and going up the staircase was very slow and very challenging and, and at times didn't even know if the next stair was there or if I was just looking at a wall, um, but, but you know, we've, we've gotten to a flywheel. So how did, how did it start and what did it look like in 207 to 2022? And I think. It, it's, it's a story that's half, half technical and probably a, a, uh, you know, half emotional as well. In 2017 when you started to talk about HPC in the cloud, people looked at you funny and said, what are you doing? Um, why are you doing that? That, no, that doesn't work, um. And so from 2017 to 2022 we tried a lot of different POCs. We tried a lot of different things to say, oh, that, that spot looks interesting, that makes it affordable. Oh, I don't have a, you know, I can't take the interruptions. OK, fine. Well, let's, let's try something else. Let's try and let's try and use a whole bunch of heterogeneous skis. Oh, that didn't work well, and, and we just tried. Item after item that didn't really work, um, we, we never really hit the scale we never hit the mark. A lot of what we're doing and, and this comes after sort of reflecting on it is we were just trying to do the exact same thing we did on premise. OK, I have 1000 nodes. Well, therefore I need 100 nodes in AWS. Well, your nodes don't look the same now I need 200 because I now they look, you know, they're, they're, they're only half as powerful, so I need 200 and then. You just add so much complexity and because you're forcing yourself to do the same thing. The biggest pivotal moment that we had. Was, was the 10X Challenge. And so in, in 2021, which is, you know, early 2022 before we went in production. We, we, we, we had, and, and the person at the office in the audience here today is, is, let's, let's put the 10X challenge out there, which is, if we could get 10x wall clock improvement time, we will change the way we think about compute, we will change the way we do our business, we will change everything. Because think about life. If something takes you 10 hours, and you can do it in an hour, you will do it differently. So when you do something 10X, you fundamentally change your behavior around it. So the 10X challenge became, all right, AWS forget about trying to replicate what we're doing on-premise. 10x wall clock time. And the other thing we're gonna do is we're gonna separate technical and commercial. First, prove it technically. Can we do it? How fast can the car go? And if the car can go really fast, that's amazing. We will talk about commercials as well, because if it's really expensive or too expensive, we may never do it. If it's really expensive, we might do it once a year. And if it becomes affordable, we'll do it all the time. And, and so that became the challenge. A few other stars aligned when we started looking at it in that metric. One is our key algorithm, which is full waveform inversion, which is very common in our industry, went from CPU to GPU. Nvidia was, you know, this was in, in 2022. The P4DE and the A180 gigs had just come out. And, and now, because of the uh Nvidia's reference architecture, the nodes looked very similar to what we were putting on premise as well. So now we had some homogeneity. We didn't have this, well, here's my CPU nodes don't look like your CPU nodes. So that started to work. The other thing is, we always used to think because of the petabyte scale data, that you had to move everything, and that the data was the gravity or the anchor that was keeping us on premise. What we started to think about is, well, maybe it doesn't have to be. What if compute is a magnet? Rather than, and and what if compute can actually pull your workflows and enable different ways? And I'll show what that looks like on on the next slide, but that's one of the other things of how do we deconstruct our workflow to do what we wanna do. And again, let's not just replicate what we do on on premise. Let's think about our workflow differently and how do we optimize for for what we want. So that's. And, and then the other really trivial thing that we realized is, uh, 10 gig direct connects in the US became very affordable. Um, that if you look at the scale of, of cost of compute and how much a 10 gig or 100 gig direct connect costs, it's like, oh wow, that, that's, that's a very investable, uh, item that uh, that, uh, we, we can manage. So, in 2022, we, we went live in production. And it was amazing with, you know, we went live on December 23rd. I was very happy because the AWS almost ruined my Christmas, but you didn't. But, uh, and, and the person that filled the queue sitting here in the front row as well, they came on on the 23rd. They came online and within, within minutes they were full and, uh, they stayed full the entire duration we had them. Um, it, it was, it was really impressive. It wasn't great though, I'll be very honest. The challenge that we had with P4DEs was getting them. We had a bit of base capacity, but it was a rare commodity, and we had lots of clever ways to try and get them or find them or just call Hussein and yell, uh, but uh. It was, uh, it, it wasn't great, um, it, it definitely wasn't meeting our 10x. Did we accelerate projects? Absolutely, um, but I would say we were more in the 3X range that, that we were accelerating, we were, we were doing some great things, um, but we knew we could do more. And that's where this year when we, we, we, we made the move to, to 1 to P5VNs and, and, and Nvidia's H200s, um, which for our workload were fantastic because that, that HBM, uh, really, um, helped us on our workloads, uh, but we also then started using EC2 capacity blocks and for us that was and remains to be a, a game changer as far as predictability. We can now. Knowing our projects, we have weeks of, of, of visibility of what we need in weeks. It's not like we, we wake up in the morning and, and then freak out and need compute that day. We, we, we have some visibility of when these projects are coming and so now we book them. And I, we have a dashboard and I know when the capacity is coming. I know when it's going away, but that predictability of burst has been a true game changer for us and that what the the simple, you know, the simple the way I describe it now is what I can have an engineer do in one click or you know. Half a dozen clicks is what historically would have taken me 69 months, maybe 12 to 18 months if you're thinking about I have to RFP, I have to buy the machine, I have to commission it, I have to bring it on, and then I have to keep it for any number of years as well, where now I click the button. And it's there. The scary thing is, it's just as easy as buying things on Amazon, so I have to keep my engineers in check because he's gotten a little, he's gotten very used to spending some very large numbers on on on online, but uh it, it is really remarkable to see the power of a capacity block, and I'll I'll I'll talk later about what that's done to our engineers and, and. And our actual HPC users, which has been a really fun thing, but that's, that's been the journey and now into the flywheel. So now if I look at when we went live with H200s earlier this year to what we've even done this year has been such an increased pace of, of innovation of optimization. That what we're doing now is we're bringing many, many more use cases. We're bringing R&D use cases. We're bringing production use cases. We're bringing some of our AI use cases, some of our G AI use cases that what we're able to spin up and react to as far as business needs. It is amazing. Are we at 10x? No, we're not at 10x. I will say we're sustainably at 3 to 5x now, so we struggled to get up, you know, 1 to 3x on, on a P4DE. We're, we're, we're definitely in the 3 to 5x range now, um, but, you know, it's still more, more to come there, um. But that's, that's part of the journey and again I, I am confident that that we will get there but then we'll just keep going more. I mean we will keep pushing and, and, and the other thing. To me is price performance is still number one. What I describe to people as my number one job is to deliver price performance cycles to our, to our HPC users. And so ensuring that we get the best performance for the best price and the availability are all paramount to what we did do. And, and to give an an example of, well, I'll I'll I'll save that, uh, I'll save that, uh, stat for later, but this is what it looks like now under the hood. So Pettabyte scale data comes in. Um, we bring it in, we bring it in on premise. The first thing we generally do is depopulate it and, and you start to shrink the model. So now, This is where our, our, our CPUs still exist and, and, uh, and yeah, yes, we're, you know, we, we, we do look at AWS for this, but this is, you know, this is a nut we've not been able to crack yet but it, it's one that, you know, we, we, we, we, we have value in that so we, we do it there but we bring it in and um. What we do is we do a lot of, a lot of the CPU intensive stuff on, on premise, but ultimately for full waveform inversion FWI that input file goes from petabyte scale to about 10 terabytes. That 10 terabytes then goes on the direct connect up to AWS. Then we use a tremendous number of, you know, today P5ENs or, you know, P series instances to run that. That that's worked tremendously well. We, we've just moved to, uh, PCS parallel compute service as well, so that's SLRM-based. The, the look and the feel is, is very similar, um, for us, for engineers. I'll, I'll show you what it looks like from a user perspective on, on, on the next slide. But this works really, really well, um. The output then is a terabyte. Yes, there is some egress cost. I tell AWS I'm always annoyed about it, but it's, it's like a ticket fee. Uh, but, but again, in the grand scheme, it, it, it is worth the, it is worth the overall, uh, outcome. And then that comes back, and then if we need to do any remaining CPU, um, it, it happens on-premise. So the data gravity. Is still there, but what we're actually doing is enabling far more workloads. And then at the end what we're saying I'll show you is how we actually integrate this into our overall stack, but this is how it works. And then from a capacity perspective we have some dedicated nodes, we have capacity blocks, and what we're also starting to explore what we can do with spot, uh, as, as we start to see a bit more spot appear in the P series instances, um, but that's, that's how we actually make it work and how. How the the the hybrid solution has been working tremendously well for us. This is the FWI use case in in in other use cases like where we're doing some fine tuning. Of a large language model, it just happens in the cloud. There's no, we don't, we don't, because you don't have the CPU stuff on either side, we can just have it happen natively in the cloud. Some of our innovation and R&D workloads just happen in that yellow box in the middle, um, it natively. So It's probably too small, but that's OK. I'll, uh, I'll I'll I'll uh I'll I'll I'll I'll I'll I'll describe the important part. So on the left is our user environment, that's our virtual workstation. So that's our, our users logging into their Linux or or Windows virtual machines. And, and the key thing there is that's the same interface they have for on-premise, that's the same interface they have for on the cloud. So that is their virtual workstation today that sits on, on, on, on premise. Then on, on, on the AWS side, so what you are seeing here is parallel cluster service PCS at at at work here which is uh AWS is managed slurm offering but. Through that API gateway and, and I'll call it out because you're probably staring at the, is there a hamster on that diagram? Yes, there is a hamster on this diagram. Our optimization team has, has, has, has a thing with rodents that the team themselves is called the rats. Um, that is hamster. We do have a tool called mice. Um, the person that is on support, uh, every week is called the rat in the hole. So it's, yes, it's a hamster. Yes, it's a tool. It's, it's, it's, it's a really important API to make all this work. But, but ultimately. You know, you have the job submission happening with the API calls, you know, with with hamster in there, but ultimately submitting it to SLRM just like we would on premise. And of course it is not a single pane of gas, a single pane of glass. Uh, the user does know they're running in the cloud, and the biggest reason they know that is because they have to make sure their data is there. So we have jobs that they can orchestrate that move their data there, um, but, but that moves there. But some of the other details that this slide doesn't really do justice to, but. How we've been able to really leverage the cloud and find capacity and leverage the scale. This is an architecture that runs in multiple regions so I can say this as of, you know, this morning we're running in 3 different regions. We don't run cross region yet. That is something that we are, are actively working with and talking to to AWS about how could we do that today. We run in multiple regions, but they all run independent of each other. We also run in multiple AZs. So our the the joy of our workload not being tightly coupled is we can run the same workload across multiple AZs in a region. And so it just is another way that we can get access to more capacity. And then underneath it for data we have FSX, but what we realized again is we had to think about FSX differently because if we used luster on the cloud like we do on premise, it would cost more than our GPUs. And so we had to get, and that's just a nature of how we use luster on premise and how we, we don't have tiering like the cloud does. We just have really, really hot storage, which is luster for us, and then we have really, really cold, which is tapes. We don't have an AWS where we can have all this hierarchy. So what we then do in, in AWS is we have S3 backed FSX. The way that our, our job works is you run, you run a whole bunch of computation, you get the model out, it, it, it sits on FSX on luster. But then you go and you update the model again and then that's it. But then the, the older version of the model can go to S3. So as long as just the most recent version is, is hydrated on FSX, we can keep that FSX to be a small, small fraction of the overall project. And then again, if you remember how I described the, the workflow. We just delete it at the end. We treat all of this as ephemeral. Our input data that goes up, we don't bring it back. We have a copy of that on premise. It's, it's, we just throw it away, um, well, delete it, uh, safely and securely, uh. But, uh, but, but, but this architecture for us has really enabled us to be mobile in the cloud and then now with capacity blocks it's enabled us to know, OK, on this day I'm moving to this region and then with PCS we can spin up a cluster in minutes, maybe, you know, hour max, um. And, and, and it has just enabled us to, to go on that flywheel and really, really make that flywheel spin. Um, it's, it's fantastic to see. So this morning, you know, Matt announced GB 300s. OK, I can go ask my data scientist, do you wanna, do you want to test the GB 300 tomorrow? We can do it. That pace of what we're able to do and trial and look at benchmarking and look at what you know what what works, what doesn't work, where do we need to go invest time with Nvidia and and to leverage the latest chipset those are things that just the pace that we can do because how do we do that in in in old HPC days? OK. New chip comes out. Let me call Nvidia. Let me get a white box in. Do I have the right OS on it? Is it, is it, you know, is it compatible? How do I get it on my network? All of these other steps. Now, I can just get a capacity blocker I can get it on demand or pull it up on spot. So, again, the, the pace of, of what we're able to do and, um, yeah, so the biggest thing that I, I, I want this diagram to show in the future is multi-region. That how can we really play a multi-region and chase some capacity and, and, and, you know, can we leverage spot and that's, you know, in more than than we can now, um. Maybe one other, one other plug for, for PCS and I can recognize some people saw me talk about PCS yesterday, but having PCS manage our SLRM compared to parallel cluster, one of the things that we're now able to do as well is make updates to our clusters seamlessly. Before with parallel cluster you make a change, you then need to take the cluster down, um, where now if we wanna update it, so for example, if we wanna add an instance to one of these clusters, we can add it. And then automatically um the the the slur manage service will will update that and that's seamless to the user where before you wanna add an instance you wanna do anything else you you you'd have to redeploy the cluster and take it down so um really really uh great solution so now. I have a very biased view of the slide, but what we've been talking about is the the, the bottom, which is obviously the foundation that holds it all up, the most important piece. But, but why are we doing all this in, in, in, in, in honesty is, it's to deliver back to that upstream challenge I talked about at the very front. It's how do we deliver an integrated stack? How do we deliver. Competitive differentiation, how do we deliver market standards? So if you look at what we're providing, and now again this is where it's not the data gravity, this is the HPC is the enabler, and now the conversation is where do we need it? What workflows need it to be enabled, and so then you go up the stack and. And again drive and integrate experience but baby Hussein, I'll let you chip in here on what, what you, what we're doing with you and what you're doing across the, the vertical as well to help build this whole ecosystem. Yeah, no, thank you, Michael, and, and so I just wanna bring everyone back up about 10-15,000 feet of why we need to look at the forest, not just the trees, because it's important to make sure the foundation and the infrastructure and the business value is driven by, um, the different levels of the stack but not. Uh, forget about what the rest of the stack looks like and why it's needed, whether it's on the data, it's on the end user experience, it's on the innovation, and it's also on, um, you know, generative AI and the AI capabilities that are starting to get more and more embedded within these workflows and so for us this represents the big picture. And we are lucky that Shell has been a great partner of ours that keeps reminding us that it's about the big picture. What else we need to do. Don't just think about the compute. Don't just think about the data. It's all of the above. We still have a lot of work to do around the end user enablement. You saw the slide before where data comes from on premise. To the cloud then goes back, uh, we can reduce a lot of that cost and a lot of that friction when we start enabling the rest of the pieces that are on the top, for example, on the application access to be also present in the cloud at the price performance and at the need that Shell has, as well as many of our other operators that are in the room that we're partnering with as well, uh, out in the world. For me this is, this is a um this is an all end to end journey that takes multiple years to achieve. Because you have teams that are completely disconnected, to be honest with you. That are working on pieces of this we have you heard from Michael some of the R&D team is doing a lot of AI and generative AI and, and, and models, um, simulation, etc. on AWS using the same infrastructure that Michael has access to in HPC. How do we make that. Experience seamless. We have teams that are working on data management and ingestion and deployment and integration with applications. How do we make sure those data sets also impact the results that come out of HPC or go into R&D and also some of the key, I think, enablement that people usually just forget because they, they just want to do their day to day business is that that co-innovation piece. I think one of the best opportunities that I've worked on in my career was the co-innovation we've done with Shell, with Oxy, and others where we've created a whole new way of looking at HPC orchestration and workflow enablement in the cloud without having to repeat the same mistakes that Michael mentioned of doing it as we did it for decades on premise, and that's with uh HPC orchestrator. It's a low code no code. Tool that we literally came up with together as an idea we put it into uh action and we created something that is now tested almost in every operator that I talked to because it makes HBC a lot seamless in the cloud and and removes that barrier of complexity and adoption that Michael talked about the first few years you just spin it up you go in you build your workload. And you build templates of what you're trying to run and it could be HBC, it could be AI, it could be any kind of stimulation. That's one area and then there's also some work that we're doing around agents and uh in the Gen AI space that we can talk about maybe next time, uh, but for me I'm excited because it's a much bigger journey. And a much bigger purpose that we're we're driving together with Shell that also benefits many of the other operators and customers that we have as well as the partner ecosystem enablement that we put in this as well. Back to you, Michael. Yeah, no, thank, thanks. I, I, I, I, the one I'll, I'll, I'll pick in, and, and you mentioned it, Ray around HPC orchestrator. I think it's a great example of. To leverage the cloud you really have to think a bit differently and, and, and, and use the tools that are there rather than simply use the tools that we're used to using and so if I look at the templates that that are an HPC orchestrator, um, those templates do some really smart things like driving asynchronicity and, and, and using storage in smarter ways. Those are things that they, they help speed they help price, they help throughput. So thinking about it from a cloud. Negative perspective instead of redesigning and the fact that then there's a template as well is it's an area where there's a lot of things in our industry where we compete, but there's also lots of areas where there's there's great room for collaboration and so something like a template on how to best use a cloud cloud native technology is a great area for us to collaborate and then we can just keep our code that we consider differentiated um to ourselves, but we can all use the same template so I think it's. It's a great example of where, you know, I appreciate what AWS is doing here and helping drive us as an industry, um, to, to deliver this, this vision as well. So, um, let's talk about some of the fun stuff now and, and, and so what? So what we have some nodes, so what we have, we have, uh, we have a bunch of computers, um, they, they're great. You guys do a great job of running them, you know, Matt, Matt was very proud of, uh, you guys' uh, Nvidia reliability this morning at the keynote and, uh. So, so what is that burst done? So unfortunately we're not at 10x, but we are at 3 to 5, and, and we, we were looking at the numbers and thought, all right, how much have we accelerated wall clock time these projects since 2022? 2.5 years. We've saved 2.5 years of wall clock time because of what we've been able to accelerate with burst capacity. That that that matters that flexibility matters. It matters to our business. It matters to how we think about HBC. It it it it gives us variability and capability that frankly we've never had before, um, but to me I, I look at it now and I've been part of this journey in some fashion or form since. 2017 and I look at now what we're doing this year and this is where we wanted it to be. I still wanna go. There's still more to do. There's always more to do. It can always be better, but we're doing what we dreamed of. We're doing what that PRFAQ said that we wanted to do in 2017. We're starting to do some of that now. We may have had the date wrong when we wrote that PRFAQ. The first time, but, but it's, it's, it's there, um, and I think it's the beginning of now you kind of covered a lot of the basics and the foundations to allow you to do a lot more faster next, and, and, and, and we'll come to, we've changed the mindsets and I'll come to that as well, but the deployment, like I said, I, I think differently about how to manage HPC demand internally now. If I look at what standard lead times are now and what it would take to bring in a machine the scale of of the of what we use, it would take a long time, it would take a lot of planning, it would take a lot of capital. And now I can, I can do that in clicks. And, and that is a huge difference that we can now focus on different areas of our business. We can react when we need to react and, and, and really focus where we, where we as a company make more, you know, make the most of our investment of time and, and, and, and effort, uh, innovation, it's um. It was really interesting, and this comes into the changing mindset when we started to talk to our researchers earlier this year and said, hey, we're moving your GPUSs to the cloud. A few of them were not very happy with us. Like, Well, no, no, no, I get I, I like my on-prem node, it's like, it'll be OK. It'll be OK. And, and now we can't take them away from them. They just, because now they say, oh well, can I get a big memory, can I get a bigger memory node? Can I get less cores? Can I get more cores? The heterogeneity and the innovation that we see. Happening. OK, can I, can I pull this agent up? Can I use this service? It is just such a difference where before historically it would be OK, you want that? Well, yeah, we can get an engineer to spend a few days on it and we can duct tape some stuff together for you, um, but it is, it is just enabled such, such a different dialogue with with with our innovators and it and it's changed the mindset and, and this is actually. It, it, it's, it's, it's, it's both a blessing and a curse as far as changing mindset because when you have a really big system, the good news is you just try and fill the system. And, and you don't really scrutinize at times, is that, does it need to run? Is it the most important thing you need to run? What, you know, what, what's the priority? The, the challenge now is not, is the system full, it's how big do you want the system to be. And and really then then the fun question I get asked to do is, OK, and how much do you want to spend? And And it is a very different conversation that we're having now around what's the value and, and, and the other really fun conversation we get to have now is, what do you want it to go faster? How fast? And, and, and, and, and that's where the 5X comes in and, and, and, and, and you know, hopefully 10DX, but where do you want to accelerate, um. So it's, it's, it's just been, uh, you know, like I said, it's, we're, we're, we're doing what we, we had, we had, we had hoped for, um, when we set out and then, you know, the, the, the lessons learned and, and, and. It it comes back to vision. This was a both a technical and, and like I said at the beginning, an emotional journey. And, and one of the biggest things that unlocked it was finding how are we gonna do things differently. So to me it's for all those out there, what's your 10x challenge? Maybe it is 10x, by all means take it. AWS has heard it before and they're probably tired of hearing about it, uh, but, but it's what's going to make a material difference if you just say I have 100 nodes and I want 100 nodes there. I can tell you that you're gonna find a reason to not like the cloud, um, because you'll do something with your 100 nodes that you prefer versus their 100 nodes, and, and it's probably not gonna be a great setup. But what's your TEDx? Um, for us, velocity and, and, and what I would encourage is I would, I would continue to separate commercials and technical. And, and, and, and AWS loves technical problems, um, and, and, and the teams dive deep and it enables our engineers to go do some really, really fun and hard things and then we, we then have to figure of course the commercials are always a consideration that that is, that is a given, um, but it, but it helped and then I, I think the other thing that we learned a lot is. De-risking the most important things first and trying to do it in an expedited fashion and and and actually do it in an expedited fashion when everyone's in the same room. So there's a few times and and Amazon has a program called EBA, what's it stand for? Experience based acceleration acceleration, but essentially it's one of the things that we did is we said we want to move and be able to run in multiple regions. We brought a bunch of. Engineers together, we brought some AWS people together. We didn't have it working in 5 days, but we fundamentally de-risked the most important things, and that's actually one of the most important things was making hamster work, um, which you which you heard about earlier. But, but it's then we had the confidence. It was not implemented, but we knew we had de-risked the most important things of how do we get on-premise to communicate to multiple regions, how do we orchestrate all those things, and it was OK, we, at the end of the week ran a dummy job. But that thing gave us the confidence that, OK, off we go. But having all the right people in the room, and that's where the partnership has been really, really great with AWS to me is if, if, if you ask and poll the the the response is always there, um. And and then the other one, and this is just to me, day in the life of HPC is relentless optimization. It's never good enough, it's always needs to be faster. There's always a bottleneck. It's always too expensive. Uh, um, but, uh, it's, it's, it's relentless optimization. So what, what's, what's next for us? Um, We've, we've talked about it, but integration of workflows and, and new workflows and, and, and again we're already doing that today. It's, we wanna fine tune an LLM. Great, let's go. What I can also tell you is we were training that LLM on H-200s a few weeks ago and now we're training it on Blackwell's. Why? Because we know the workload drives it, we get the price performance, we're there. That rapid response to needs is, is, is, is, is, is very, very real, integration modulation we talked about energy, uh, orchestrator, but to me it's also beyond that it's getting it into EDI where our data sits. It's getting uh agentic for energy. It's, it's getting all of that integrated workflow across both our stuff we make across our. Vendor and partner ecosystem, it's gluing all that together. And again, it's it's it's changing the paradigm of, well, no, no, we can't go to the cloud because our data is not there. To me is if we give an environment that's integrative and the carrot is there, our users are going to come because it's going to be the obvious place. It's not going to be if and or it's going to be, well, I have to be there. Relentless optimization talked about it, you know, again that is, that is what we have done day in day out. There's always something that we're looking at and saying, could we do this better? And so today for us in in HPC that's multi-region that's, that's our next big challenge that we want to go de-risk. And, and then price performance. I, uh, on, on, on my long walk over here did run into your, your accelerator, uh, lead, and we sort it's like, so, so when do you want to test B-300s? When do you want, when now that they're out and, and, and, and so it's looking at that price performance. It's looking at what works for our workloads and, and how do we then deliver that in, in the most meaningful way. So that's, that's what's next for us and, and with that, just one as, as a closing comment, just a ton of appreciation to, to AWS for, for it's been a long journey, um, like I said, the staircase seemed like walls at times, um, but, uh, we're, you know, really, really appreciative of, of the journey and, and, and. You know, for, for me personally, a huge appreciation to all the, both the sponsors and engineers that I have on, on, on, on the shell side. I, I'm incredibly privileged to be able to, to be up here and tell this story, but it, it's through the hard work of all the people that, that have made this work that are able to do it. So thank you to everyone and thank you AWS. Of course, thank you for having us. Right.