---
video_id: mOAd8grR1BU
video_url: https://www.youtube.com/watch?v=mOAd8grR1BU
is_generated: False
is_translatable: True
summary: |
  Datadog’s Kunal Batra and AWS’s Dwane Lightfoot show how to build, deploy, and observe generative AI agents with Strands Agents, Amazon Bedrock Agent Core, and Datadog. Kunal notes that agents amplify existing complexity with non-deterministic behavior, autonomous tool calls, evolving frameworks, token-driven costs, and shared responsibility across models and orchestration. The biggest risks Datadog sees are reliability, troubleshooting, cost overruns, and security or guardrail gaps. Dwane demonstrates a practical agent that gathers AWS news and emails a ranked newsletter, reflecting his own need to keep pace with fast-changing content.
  He traces the path from manual RSS scripts to ChatGPT summaries to today’s tool-calling agents, emphasizing that core software skills still matter because agents sit atop familiar compute, storage, auth, and API layers. An agentic system combines the foundation model, knowledge base, guardrails, tools/APIs, memory, and multi-agent workflows plus logging and evaluation of token use. Strands Agents is the model-agnostic framework (Python/TypeScript, MCP and agent-to-agent support); Bedrock Agent Core supplies managed runtime, isolated internet-enabled sessions with identity, scaling, policy, and governance. Packaging an agent or MCP server as a Docker image and defining an entry point is enough to deploy, while Agent Core memory offers short-term context and long-term strategies (semantic, preference, episodic, custom) that automatically extract and persist key details.
  Dwane’s architecture uses a Strands orchestrator plus an analysis agent-as-a-tool, limited to purpose-built actions (fetch news, extract links, schedule events) and constrained browsing sources to cut cost and mistakes. Distinct “browse” and “publish” workflows are steered by system prompts; EventBridge scheduling enables autonomy; Cognito provides sign-in. In the demo the agent greets by name via memory, fetches current AWS headlines, decides whether to call the analysis agent, produces a TL;DR with ranked topics, emails the newsletter, stores links to avoid duplicates, and schedules its own daily run by creating an EventBridge rule with its runtime ARN and prompt.
  Kunal maps this to the AWS Well-Architected generative AI lens: collect metrics and user feedback, enforce guardrails against prompt injection or PII leakage, monitor tool/API success and latency, track cost per workflow/user/model, and report inference efficiency for sustainable scaling. Strands/Agent Core emit telemetry that can be pointed to Datadog, streaming traces, metrics, logs, and token data into Datadog’s LLM observability UI. He shows dashboards that list agents, error rates, and monitors; per-trace reasoning paths with tool calls, costs, and latency; automatic PII stripping; topic clustering; and built-in or custom evaluations (failure to answer, hallucination, goal completeness, brand voice, toxicity, prompt injection, tool argument correctness). Experiments compare prompts or models against baseline accuracy, satisfaction, duration, error rate, and token cost, with datasets built from real traces, and monitors trigger alerts or automation when thresholds are crossed.
  Core advice: start small but bake in observability and evals early; limit tools and scope to reduce confusion and spend; use managed runtime and memory to standardize deployment and context; monitor tokens, latency, errors, and safety continuously; and iterate with experiments before scaling multi-agent systems. Code, docs, and links are provided for further exploration.
keywords: Bedrock Agent Core, Strands Agents, Datadog observability, generative AI lens, agent memory
---

Hello, hi everyone. So in this session we're gonna be talking about building observable agents with AWS strands, Amazon Bedrock Agent Core, and Data Dog. My name is Kunal Batra. I am a senior technical advocate at Dataog and I'm joined by Duan Lightfoot over here who's a senior developer advocate with AWS. Now, quick story, Duan and I actually worked together a couple of years back on the AWS developer relations team, and when I had the opportunity to present this talk, uh, I couldn't think of a better, uh, co-presenter. So you guys are in a real treat, um, in this session. So let's get into the agenda and what we're gonna be covering uh today. So the first thing I wanna talk about uh is why observability matters. We'll talk about it at a high level and then drill down uh into some of the challenges we see at Data Dog when it comes to uh pushing these agents into production. From there we're gonna transition it to Duan who's gonna walk us through building and deploying uh these agents with AWS strands and uh Bedrock uh Agent core. From there once we have an agent in production, how do we make sure that this agent is running reliably, securely and efficiently, uh, at scale and so we're gonna take a look at the well architected framework, specifically the generative AI lens for some best practices. And then we're gonna see how can we operationalize these best practices with Data Dog, followed by some key takeaways and resources. So let's get into it. Why does observability matter? And before we start talking about agents, I just wanna talk about today's world, uh, really quickly and essentially, uh, modern architectures have caused an explosion in complexity. There's a diversity of technologies in use, multiple clouds, open source frameworks, different SAS providers. Essentially each of our stacks is a mosaic of these stitched services. And then compute is also increasing, especially when it comes to ephemeral commute compute. We have serverless functions that can last only milliseconds. We have containers that can be spun up just for seconds, uh, and on top of this, the rate of change is increasing rapidly. Now when we add AI agents into the mix, this just multiplies the complexity. Now agents are compound systems. They have, uh, vector stores, models, um, evals, orchestration. A lot is happening, uh, behind the scenes when it comes to agents, and they can operate with real autonomy, uh, decide when to make the tool calls in an ever changing, uh, environment, and this is exciting. Uh, but it's also unpredictable. These agents are essentially non-deterministic, giving it the same inputs. You're never sure what the output is gonna be. They're also evolving continuously, not just the models but also the underlying frameworks, uh, and then accountability becomes shared so when something goes wrong, is it the model, is it the framework, is it the orchestration, is it the tool call, uh, it gets hard to start troubleshooting these agents. So at Data Dog we've seen a lot of challenges when you start running these agents at scale. The first one is reliability, right? We don't want our agents to hallucinate we wanna make sure that uh they're operating um and with quality and hitting the goal state that uh they're meant to. We spoke about complexity. Uh, this is hard to troubleshoot, right? If something goes wrong, there are a lot of different components when it comes to agents. And now when it comes to cost every model interaction consumes tokens, right? And if something goes wrong, these costs can get out of scale, uh, pretty quickly. And then finally security and safety we wanna make sure that these agents are running in a secure fashion uh and we have enforced uh guard rails. So these are some of the challenges, uh, and then before we go into addressing these challenges we have Dean who's gonna come up and showcase how to build and deploy these agents and, and then we can see how we can address some of these challenges. Dean, thanks, Kar. So I started at AWS about 4 years ago. And one of the challenges I face. Was the amount of information that I needed to learn to do my role to get up to speed with AWS. To stay up to date with what's going on in the industry, and the information just kept going and going. So, during that time, I had some helpful friends like Kna to kinda help me get up to speed of AWS. But after about 5 days once, I felt overwhelmed and tried to figure it out. So the question I have is how do we stay up to date with so much information? How many of you all can relate? All of the changes in the industry, AI, a new models everyday, a new agency framework, a new approach, something new is happening. And so, when I thought about this and how to solve this at AWS after about my first month. I'll put my developer hat on and I said, OK, what I could do is take the standard approach. All the resources that I know and access, let me curate this information and create a newsletter that I will send out to myself, send out to my team, and just write code. But the problem with this is that, of course I can automate. Getting the feed. I could do some basic keyword search. I could schedule the execution using lambda and event bridge, and I could scan and kinda update this, but there's a lot of manual labor. There's also a lot of conditional statements I have to kind of build in. So the problem I ran into over and over is that I didn't have intelligent categorization. There was no context aware summaries, right. I could use AI but it had to be based off of the information I provided, off of my conditional statements. And then priority ranking. What's the most important thing that I need to know about right now that's gonna affect me? How do I put that up front? How do I get gain actionable insights and adaptive filtering? So we fast forward. 2 January 2023. Chat GTP was announced in November 2022. I spoke at reinvent, came back, heard about it. I was like, let me try this out. I hopped in, started using Generative AI. Then I was like, wow, I could summarize blogs. I could write code. It kinda worked, not really, but you know, it kinda worked. But the problem I ran into once I started using the APIs was that the contest windows were so small, it wouldn't fit a whole blog. And so what I had to figure out was how can I take a blog and use AI to extract the information and summarize. So what I had to do was Take each chunk of the blog, summarize that chunk, and then once I have all these summaries of the entire blog, concatenate it and create one summary. This is before rag, right? So come June 2023, we start hearing about rag. Now everybody wants to take all their data in their companies and start using it, gain knowledgeable insights, and help their teams, right? How many of you, your organization started building chatbots and started leverageaging Rag? I think we all try to figure out how to do this. Now, The end of 2023, 2024 is when we had function calling and tool calling. Now we start hearing about the terms, agents, the excitement, the what if, what's possible. And then we start having agentic frameworks, because once you start building an agent, now that agent needs memory. Now it needs context, now it needs to call tools and APIs. Now we have MCP, we need au authentication, authorization. We need agents to talk to agents. We need agents to talk to tools, machine to machine communication. How do we manage that? How do we observe that? How many of you are software developers, architects, network engineers? OK, one thing I wanna call out before I dive into any code, is that all of that information is important when you build agents. Because at the end of the day, it's still software. You still need compute, you still need databases, you still need storage. You still need OA, you need all of these things, and so when you build these applications, this is under the hood, what's the most important, and you add intelligence into that. And so, when we look at an agentic system, let's actually start from the beginning. You have a workflow, an LLM, large language model, where it accepts an input a prompt, you generate a response. We can have a human in the loop of the agent, but then from there you have the agentic components, like the foundation model, the knowledge basis. The guardrails, letting the agent know you should not do that. Like, guarding the inputs and the outputs. The API and the tool that it needs to cost, the memory for that short term and long term conversations, of the agents that it needs to communicate with in the workflows. Maybe you wanna have a multi-agent system, right? How do you manage this all? And then from there, once you have that system, how do you continuously evaluate it? How do you get the logs, the metrics? How do you test, evaluate it to say how well it's performing? How do you monitor how many tokens is being used to determine if you can use a smaller model, improve your prompts, your system prompts. How do you monitor and evaluate that? And that's what Daddy Dog is gonna talk about today. And so once you have all of this, the entire system in place, now we're trying to figure out how can we gain actionable insights so we can optimize our agents. And this is what it, when you put all of this together, this is what we're building. And if you look at any software application, all of these building blocks are a part of it, and you'll see how we expose that with Agent Core. Now, the agent gentic framework that I'll be using today is strands agents. It's an open source Python and TypeScript SDK that allows you to build an agent and a few lines of code. It's model agnostic. So if you're using OpenAI, great. If you're using anthropic, great. If you're using Amazon Bear Rock, great. All of that is supported. It supports MCP and A2A. If you're using multi-agent. Whether it's sworn or agents as tools, all of that is supported with insurance agents. Now, we have our agent. There's a problem. Because how do we take that agent from prototype and put it in production? How many of you have agents in production today? A few people. How many of you have agents in a dev environment or running on your laptop? A few people, right? It works. It works on your laptop. It works in your dev environment. But then you soon soon realize, how do you get an isolated compute, secure, isolated environment to store that agent in a compute one time that allows you to have isolated sessions. Has access to the internet, has security baked in, so you can provide connectivity to the agent either via something like IM. Incognito. OO or connect to a third party provider. How do you actually do that, right? And then from there, you need to be able to scale it. It needs memory, short term and long term. All of these are challenges. We talked about the security and guardrails that need to be in place, and then governance. Like how do you provide policy and evaluations for these agents. Doing this in production is not easy. And then, if you're starting with a multi-agent solution, and you haven't started with observability. That's gonna be even harder of a problem to solve if you already have it in place. So if you're building out agents now, the best thing to do is start with an MVP, which is a small use case. And in the beginning, build observability from that point, along with evaluations. So that way as you scale your agent, You have reservability from the beginning and you can evaluate the performance of your agent. And that's what Kalho is gonna talk about here in a moment. Now, we talked about how we are building an agent. Let's talk about actually deploying that agent with Agent Core. Breaking down Agent Core has multiple primitives. It is a fully managed service that provides you with runtime, memory, gateway for tools, browser to be able to search the web, and a code interpreter, along with observability, identity. Not just so you can access your agent, but also control what the agent. Can access machine to machine communication. All of that is baked in. And then policy around the tools it can use. I'll quickly go over some of the primitives here. When we look at Agent Core runtime, and I'll go into this further in another slide, but Agent Core runtime supports any framework in any model. Right, so if you're using lane train, great, or lane graph. If you're using crew AI great. The cloud agents SDK. Or strands agents, all of those are supported, or any model as well are supported in the runtime. The thing about this runtime is that it supports workloads up to 8 hours, right? If you're building an agent, often, let's say you're running deep research or some other or some other application, that agent is gonna run for 15 minutes, it can run for 1 hour, you can have a workload that runs for 8 hours. You can do that in run time. And all of the sessions are secure and isolated. Now, memory. I'm gonna show you a slide here on memory, but the cool thing about this is that you need to manage those conversations along with your state of the agent interactions. Memory supports that short-term and long-term memory, and I'll show you the importance of that here in a moment. Going deeper in the run time. In a few lines of code, you could take your agent, if you're building it today and take that framework and deploy it to AWS in a runtime. You can do that either in the Docker file or upload a zip file, and from there it goes in the ECR S3 bucket and then you launch it into the runtime. And so if we look at this in code, we have our agentic framework that we're using, it can be any framework. We import the agent cops. SDK with the Bad Rock Agent Core app, we instantiate the app and then from there we add an entry point. That entry point allows you to invoke this agent. Now, the thing about runtime, something I didn't call out before, you can launch any framework into runtime. Another thing that supports is MCP. So if you're running MCP servers, you can launch those and use those MCP servers inside of runtime. Once you have your code and everything ready to deploy, from there, a few lines of code, you configure the agent, that creates a Docker file, and then you launch that agent into agent core. A few lines of code, this is about 4 lines of code, 2 lines or 3 lines inside of your app, and then about 3 lines in the terminal. It also supports. Infrastructure is code, so if you, if you already use the CDK you can deploy with CDK as well. Now here's one of the features I get a lot of questions about, and that's agent core memory. Because when you're building your agent, You're gonna need to manage those conversations, right? So the chat messages, the system prompt, the user assistant messages along with tool calls in the session state. All of that goes into the short-term memory. Now, once you have your short-term memory, you can actually create something called a strategy. And these strategies can be semantic search, user prefaces. A summary, episodic as well as custom strategies. What this means is that we will automatically extract from those short term messages or the short term memory and populate the long-term memory. So this way in one single session, you can use the short term memory, but if you go into another session. Or you have other agents that need to use that long-term memory, you could actually use the retrieve memory records and add that context into the conversation so they can remember things like your name, your birthday, the type of applications you use, and etc. from that long-term memory. And if you have questions about this, feel free to see me when we open up for questions. And again, a few lines of code, we're using the agent corps at SDK. And we're creating a memory. Once we create the memory here, we give it a name, but we can also add a strategy here as well. And then from there, a few lines of code, we can start adding events to memory, and that gets populated. Now before we hop in the demo, here's what our architecture actually looks like at a high level. So we have a strange agent. This is actually a multi-agent solution. One single strange agent, that's our orchestrator. If you look at the pink, that's actually gonna be our second agent, that's an agent as a tool. That's gonna be deep analysis. So when I built this newsletter agent, I gave it several tools, like to look up the current date, to fetch the latest news, the news. Here's the thing about that tool. This is a purpose-built tools. When you build an agent, I like to build agents that are specialized. Give them the tools they need so they can complete the complete the task at hand. I don't give them a whole bunch of tools, because an agent can reason plan, reason and plan and take actions. So why complicate? Why add more to cost? Why give it a longer time to actually reason on what tools to use? I give it the tools it needs. And so also I control what it can actually access. Of course I can give it access to the entire internet, but it doesn't need to search the whole entire internet. I know the sites I want it to go to. Another thing about this, I have two versions. One focuses on the AWS news, but this agent actually has the ability to add URLs to itself, right? And you'll see in the demo how some of the memory features I use to make this agent be able to be more autonomous. From there we extract links. Yes, I could use the agent to do some red jacks, but if I know the tools and what needs to happen, why not use my software programming skills and help the agent be more effective and just use the tool and then use code to do everything else, right? And then from there, we published a newsletter. I have two modes here. I have a browse mode so I can just talk to the agent, then I have a publish mode. The agent in the system prompt, which is very important, to guide the agent on how you want it to perform, in the browse mode, I talk to the agent, answer questions, and it knows not to publish. But once we get to the point where we want to publish, now the agent will focus on that workflow. So there are two different workflows. And then finally, the agent has the event, the, the ability to manage his own schedule events. So I can talk to the agent. And tell it to schedule a daily update or some other update and it can manage itself. So let's actually hop into the demo before we turn it back over to Canal. Alright, so the first thing we're gonna log in, we're using cognito to sign in. The first thing I'm gonna ask the agent is, what's the AWS news today? It's gonna look up the current time, it's gonna fetch the news. Now notice it has my name. It retrieved that from memory, right? It knows my name because it learned my name. Now it looks up the latest news. Now I know you may think this this is a chatbot, but the thing about this, an agent has some type of input front end. This allows me to talk and train the agent. Right now I'm telling them to send me a newsletter. So now it's using that published newsletter workflow. Look up the current time, fetch the news, and now it's gonna extract information from all of the resources that it looked up. So for each URL that is searching, it's extracting information and determining if it needs to use another agent. In this example, it decided not to use the other agent because we didn't need deeper analysis, but the agent is able to reason and determine that. So now it's gonna keep extracting, and then it's going to send a newsletter. Now it's publishing. And you can see they published a newsletter. And you can watch this at the end, it says process the article URLs for memory tracking. I built a custom strategy so we can remember every newsletter we sent. All the URLs have been processed so that way it can just look up in memory rather than doing a tool call. And I can determine if I'm getting duplicates or not, right? The agent will know not to send a duplicate about a topic. And you can see who's the recipient and it has a message ID. So this verifies that it's been sent, but we'll hop into my email and then we'll take a look. So we'll sync my email and we can see the newsletter being populated, and you can see we had a too long didn't read. That kind of summarized everything that I care about. It learned that, it understands that, and now all of these are ranked in the topics important. The cool thing about this, I built this newsletter because I wanna know about the topics I care about, but I also wanna know about GitHub repos and everything related to that topic. So I sign out, we sign back in. I ask it, when did I send the last newsletter? And notice here, based on my memory. It knows that it sent the newsletter. So all of this, we're in a new session, it has the memory. Record of the newsletter that it's already sent. Along with the message ID. So all of this came from memory across sessions. It was able to maintain that. Now the next question we're gonna ask is to actually schedule a newsletter to be sent daily at 6 a.m. Eastern Standard Time. The agent has to look up his ID. It has to look up actually who it is, so it actually entered the arm when it actually schedules the newsletter. So it has the ability to look up itself and then it's creating the JSO to schedule the event inside of EventBridge. And notice here, it tells us everything about the memory. We hop in the event bridge. We can look at the schedules And you can see the agent was able to schedule itself. And there's the target. You can see the run time, the actual agent arm, when this was scheduled. And then it generated its own prompt that will invoke itself. So now this agent will run daily autonomously, and I can log in and check the status of it, or I can hop into the general observability with Data Dog and observe how this agent is working in production. And we'll assign it back over to you, Kal, because he's gonna talk about Gen AIs over Billy and Stran's agents. Thanks. So now that we've seen how to build and deploy an agent in production, how do we make sure this is running securely, efficiently and reliable, uh, in a reliable fashion? Um, as we can take a look at for best practices, the AWS well architected framework. So now this is a 200 level session so I'm just gonna have a little bit of a background on the well architected framework just in case you're not aware, um. Now this actually started a year before what's shown on the timeline back in 2011 when AWS noticed that whenever there were certain outages, there are certain customers who are not being affected by these outages. And this led to conversations and building up a series of best practices internally, uh, and then that got distilled into these pillars which was released externally in 2015, uh, and then a couple of years after that, uh, AWS released something called Lenses which is, uh. Essentially the best practices but for a narrow domain, right? So in this case, um we're gonna be looking at these um for the generative AI domain which I believe was just released earlier this year. Now these are the pillars of the well architected framework. If you're not familiar with this, I highly recommend uh attending one of the modernization sessions or catching one of those after the event, um, a wealth of information and so we're gonna take a look at the generative AI lens of each of these pillars. So in that small font on the top right you can see the pillar and then what how this uh reflects for um best practices for building agents right? so this is for operational excellence and we wanna be able to collect metrics, user feedback, uh, and functional performance data, um, essentially the metrics logs uh and traces um in uh in a platform. We also want to be sure that we can implement guard rails, right, so we wanna observe the incoming logs, uh, the inputs and outputs, and make sure there's no policy violations. We wanna be able to detect prompt injection, jailbreak attempts, or any sort of PII uh exposure. We also wanna be able to monitor success and latency of API tool calls. And then track cost per workflow per user and per model call. And then finally measure and report inference efficiencies to guide sustainable scaling. So now let's operationalize. Let's see how we operationalize this with Data Dog. Now before we dump, uh, sorry, go into the demo, I just wanna showcase how to integrate with strands in this case. Now with AWS strands and Bedrock agent core we get out of the box telemetry, which is really interesting, right? So with a couple of lines of code you can see that we are setting, uh, right after our import statements we are setting, uh, the ingestion URL where we want AWS strands to send all the telemetry, right? So these are gonna be the metrics, traces, logs, and that's gonna come right into Data Dog and so this is a simple strands agent. That's gonna send all the telemetry and data dog and then from there we can kind of operationalize those best practices so now I'm gonna switch over to the demo. And then we can kind of see how this works in action. OK, so I'm on my laptop over here and this is the Data dog dashboard. I'm not sure if anyone here is familiar with the Data dog dashboard, but on the left hand side of the screen on the bottom portion where you see the errors, metrics, logs, these are the primitives, right? So this is what Data Dog is ingesting um into the platform. Now that middle section over there, these are all the products and features of Data Dog. It's gathering insights from those primitives, um, and then from there it's gonna be able to tell you, uh, give you those insights for your application infrastructure, uh, database, whatever you're looking at and then the top panel over here, top portion of that left side panel are just the most recently, uh, used pieces of functionality or things that we've deemed are that you might want quick access to. Now we're looking at the AI observability portion over here so you can see that's highlighted in the left hand side of the panel and now we're in the LLM observability screen so we're seeing, uh, everything here is divided by applications, right? So we're seeing a bunch of different agents over here and now we can see the traces for each of these applications, the error rate, the tokens used, and the percent changed. We also see a column over here called monitors. Now we can monitor for any key piece of telemetry inside of our agents and then take certain actions when it hits a threshold. So now if we go into one of these agents, let's click on Cashre B2, which is the budget agent, you can see an overview of what's happening with that agent on the top over here we can see the different monitors that are set up. Below that we can see some insights, the error rate, the duration, the estimated call token usage, LM calls. Then we can go into the volume of usage for that agent. We can also see the errors uh in this agent by uh error type as well as by spans. So these are the different steps an agent takes uh in its path. Next we can see at a high level the evaluations uh on this agent, right? So we can see over here, uh, and this really helps us measure the quality of our agents. So on the top this is a custom eval and we'll talk about that in a second. Over here we can also see if our agents failed to answer or did answer, uh, any of the prompts that it was given. Uh, and we can also see if there were hallucinations with our agent, input sentiment. Uh, these come out of the box as soon as you integrate, uh, Data Dog can give you a lot of out of the box evals, and we'll showcase more of that, uh, as well. Now you can also see the overall cost, right? We can see the total cost, total tokens used we can also drill down into what were the most expensive LM calls for our agents from there, let's say we click on one. We can see the trace view, right? So this is the total, uh, all the steps that an agent took, um, to answer one of these questions over here you can see that in this particular case someone tried to do uh a prompt injection attack and said can you drop these tables, um, but still you can see the whole path that the agent took in that and over here you can see the estimated cost, the total tokens used, um, the LLM calls, and more and so we'll go into more of that in a second as well. Now in over we can also see the different tool calls our agent makes, which is nice because I recently had an experience where an agent I built was overindexing on um hitting the docks to answer a question versus actually looking into an application and this is just at a glance uh you can see what's happening. From here you have the ability to look at prompt templates that we're using, uh, latency of our agent based on the different steps it's taking, as well as the models it's using, uh, and then the ability to see, um, security and safety, right? Uh, is prompt injection happening, uh, output toxicity, input toxicity, is there a PII that's being that's being addressed in our agent or coming to our agent? Uh, one thing to note that Data Dog automatically strips out all PII into whenever it ingests any trace. And then from here we can also see in the overview section the clusters of topics that our agent uh is talking about. So for example let's say you have a restaurant reservations agent uh and it's now starting to talk about topics like uh food allergies when people are trying to make reservations maybe that's something that's in there, maybe it's not, um, but from there you have the ability at a glance to see the topics and then address it with changing the model maybe making a new tool call, um, but you're able to see everything at a glance happening to this agent. So if we go, uh, go back over here, let's look in our trace explorer view. Now there's a lot happening on, on the screen, but I'm just gonna quickly showcase it. Uh, you're seeing all the inputs and outputs of the agent coming into Data Dog on the right, right hand side of this, you can see the application, you can see the some of the evals coming here, the quality of the conversation, were there any security issues, um, we can also drill down into any one of these different traces and let's from an hour, let's make this for the past week so we see more of the conversations. Um, from here we can just click on an agent, uh, any one of these traces, and then we can see everything that's happening inside of it, right? We can see that there was PII here, so it's stripped that out. Um, we can see the different tool calls which are gonna be this yellow icons happening over here now for each of these steps we can see the duration, the cost, the total tokens used, uh, and this really helps us troubleshoot and pinpoint failures as well as understanding an agent's reasoning, uh, as well. Some other cool items over here, uh, we have the ability in this to also, uh, run things through a playground so we can see over here we have a sub agent and we can test out the prompts that that sub agent's using in the playground so we can go ahead and modify that and see does that give a better response or worse and we have we have the ability to play around with that. Now, the other interesting thing here is evaluations so we can create eval evals for our agents, and this ties into, if you remember from the beginning of the talk, one of the challenges which is quality of our agents. Now we have a bunch of different evals that come out of the box failure to answer, goal completeness, hallucination, input toxicity, output toxicity, are there prompt injections, tool argument correctness. The way this operates uh is we have an LLM as a judge functionality. You can choose the LLM, uh, and then from there it will start classifying all of these traces uh uh in one of these evals. You also have the ability to create a custom eval of your own, so we see a lot of companies who want to make sure that their agent responds back in the brand voice of their agent, and that's something that you can go ahead and create an eval, uh, for over here as well. Now, outside of that, we also have the ability for uh creating an experiment. So when you are developing your agent, you have the options obviously to pick which model you wanna use, which prompts you wanna use, uh, and then with the experimentation functionality you can see which one works best for your particular use case. Uh, in this case you can see over here if I zoom in a little, these are the inputs so you can have the inputs of different models, inputs of the different prompts, and then from here you can see the outputs, uh, which are gonna be the information accuracy, user satisfaction, brand voice consistency, duration, error rate, token cost. Now what we recommend uh at Data Dog is you wanna choose the most capable model first create some sort of performance baseline, and then from there you can go ahead and experiment with different models, different props, and then see how that is affecting your performance baseline um with whatever the whatever you're measuring over here. So let's go back to the PowerPoint presentation. Now some key uh takeaways here. The first is troubleshooting, right? So you have the ability to look at all of the steps uh agent takes, uh, in its reasoning path with that trace view of that agent, right? In each of those steps we can call it spans and then you can see the overall cost duration, um, anything there to help you really pinpoint any failures or if anything that you wanna optimize. You have the ability to uh monitor any key piece of telemetry for your agent right as soon as it hits a certain threshold then from there you have the ability to be alerted alerted for that uh or um automate some sort of action or work flow to remediate that. We have uh evals as well, so a bunch of out of the box evals that work with an LLM as a judge functionality. You also have the ability to create a custom eval too. Now we wanna make sure that each of the inputs and outputs that our agent takes is monitored. We don't want any data leakage. We don't want any PII exposure, uh, and we wanna make sure that we limit, uh, the amount of prompt injections coming in. So this really helps you, uh, with that as well. For the experiments like we just showcased, uh, you have the ability to, uh, change the different models, change the prompts, uh, and then see how that affects certain pieces of outputs and then just really quickly I think I just forgot to show something, so let me just switch back to the demo, uh. If we go back to the traces, we can create a data set for our experiments, so we can just choose in real life all the different traces that we just saw and we can add that to a data set um to use to experiment on. And that concludes our talk and so Joan and I put together some resources, uh, source code, documentation, um, anything you might need and find valuable from this talk we wanted to put it on a separate site, uh, and so I'm gonna leave that up and then if there are any questions feel free to ask. Thank you everyone for coming.
