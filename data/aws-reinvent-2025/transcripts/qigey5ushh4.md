---
video_id: qigey5ushh4
video_url: https://www.youtube.com/watch?v=qigey5ushh4
is_generated: False
is_translatable: True
---

Good afternoon everyone. Um, my name is, uh, Rohit Singh. I'm a principal solution architect at Financial Services AWS. I'm, uh, delighted to be joined by Jason West, Group director Real Time, and Ed Johnson, Real-time architect, and we'll be talking about the EEC transformative journey, how they are evolving market intelligence at massive scale. Yeah. So by massive um. I mean they're, they're doing they're working with some really big numbers, um. We'll start the agenda with a quick look at the capital market ecosystem, the challenges it faces, a system which is always running, uh, and we will soon see exchanges running 24/7 that would further explore the volume of market data that that that is produced. Um, then Jason will take stage and will share the LSE, uh, partnership journey with, with AWS, um, how, you know, the key, um, uh, business drivers, strategic decisions, and impressive results they've achieved working with AWS, and then of course Ed will take stage to dive deep into the metrics, uh, uh, signals platform, um. The technical backbone that enables sophisticated market intelligence at at at scale. Finally we'll wrap up with the key outcomes and takeaways, uh, things that you can apply with your own organizations. Uh, it's going to be really exciting journey through innovation at scale. Uh, By the end of the session, uh, you will learn how ESEC have designed a serverless solution that operates at scale. Uh, we are talking about some really big numbers. Uh, we're talking about 274 billion messages processed and delivered globally. We are talking about 75 terabytes of historical tick data, and then we're also talking about 90 million plus market instruments. So focus of the session, of course, like I said, metric signals platform. Ed will walk us through how ESEC designed the surless solution to. Work at extreme scales, uh, no more headaches with traditional infrastructure. No more managing the scaling of that. Uh, this solution scales seamlessly, um, and, uh, cutting infrastructure costs by up to 5 times. So a solution that also offers flexibility as the market conditions change as we're seeing right now and how it leverages Gen AI using Amazon Bedrock to provide market insights. So we are dealing with hundreds of exchanges, uh, each producing massive amount of data around the clock, and then we have aggregators like London Stock Exchange Group. They have the challenging task of getting all this data, processing it, and distributing it around the globe. And then we have the consumers, the banks and hedge funds who then need this data to make trading decisions. Just think about the scale 575 distinct sources, each with their own format, protocol, and timing requirements. And this in the grand scheme of things this represents trillions of dollars of market trading every day. So when, when the, when the system runs smoothly, the global markets operate efficiently, any delays or issues with the data can result in lost opportunities worth millions. So the stakes are absolutely high and the performance requirements are very stringent. Um, each layer, uh, each layer represents its own set of challenges, so producer needs ultra low latency and high throughput while managing, uh, geographical constraints and um. Uh, regulatory requirements, uh, especially across regions, and then we have, um, aggregators, uh, like EEG. They have to process, uh, they have to collect all this diverse data, process it, normalize it, and distribute it globally, uh, to clients who rely obviously for making critical trading decisions, and then consumers require secure access and the ability to then process, uh, analyze data at petabyte scale. All to be delivered in a cost effective manner and plus there's also a need for more flexibility as we've seen that the market conditions can always change so this issue grows significantly as the market data volumes are growing year on year and traditional infrastructure just can't keep up with that. So just double clicking on the challenges by the data market producers, uh, market producers, the complexity scales. Rapidly and the challenges multiply exponentially. Um they must cope with the market volatility, which can spike the market rate of volumes by 10 to 50 times, uh, especially during major market events. On top of that, uh, everything has to be, uh, cost efficient and should be reliably delivered to all their clients. AWS service like services like S3. Um, and, um, Private Link can help with that. So S3 provides petabyte scale storage, and it has integration with other AWS services so you can perform analytics, um, while the data sits on S3, and Private Link offers, um, high performance, secure connectivity for your clients globally from anywhere now. So with now that I've given you a bit of overview of incredible incredible complexity of uh capital market ecosystem, I'll now invite Jason uh to share uh the key business challenges that drove this transformation and explain why ESEG chose AWS as their technology partner, the business uh. Uh, context Jason will provide is extremely crucial because this was not about technology modernization. It's about, it was about providing new business capabilities, improving client experience, and positioning LSEC for continued leadership in the market, uh, aggregation and distribution business. Over to you, Jason. Thank you Rohit, I appreciate it. So I'm Jason West, I look after the real-time business at the London Stock Exchange Group. Who are we? What are we? So for those of you who don't know, um, we're one of the leading global financial market infrastructure companies in the world, delivering data to 4,444,000 clients around the world in 170 countries. We've got 26,000 employees around the world, and we've got 300 years of innovation. Our best innovation was in 1850 when we sent the first message from the London Stock Exchange by carrier pigeon. Now we're all about Gen AI and cool stuff. So we're progressing as our clients are progressing, as our technology partners are progressing. So This is the makeup of the London Stock Exchange Group today. So there's multiple divisions in here. Starting the top left, LSEG data and analytics. That's where I sit, that's where the real-time business sits. To the right of that you've got LSEG FX. So that provides FX markets and trading direct to brokers and via our workforce, uh our workspace, uh, platform. I'll say post trade. So this is all about our clearing, trade clearing. Risk intelligence, some of you might have seen that chat yesterday. These are the guys that keep us safe, they keep security, they make sure that our clients can work within proper regulatory frameworks and the data is good. Our FTSE Russell business, we're all about indexes. Um, huge business for us, very, very well liked and supported around the world. I think these guys are running 44,000 clients today. And then ultimately, we're a stock exchange as well. So from ingestion, execution, trade input to clearing. As a company, we cover all these things. So if we drill down to the real-time business in numbers, so we're present in 170 countries today. Um We transmit 274 billion messages a day around the world. So this is, this is ingesting from 575 venues and exchanges around the world, some of them are cloud-based, some of them are normal venues and exchanges. We ingest that data in 4 milliseconds, we normalize that data and put it onto our network. So that allows us to put the strike price, the up, the high price, the low price, whatever the venue, in exactly the same place for our clients. Since 1996 we've been storing every trade, every tick from every single venue. The only company to do that. It's currently at 75 petabytes now for us. 2 or 3 years ago, we made the decision that this needs to be hosted in cloud, so that data is now off our premises and it sits within AWS and we're writing probably 5 to 10 terabytes of data to that every single day, so we need the elasticity to grow that. As Rohit mentioned, 90+ million single instruments from all these venues are transmitted around the world daily. We're taking on or upgrading 150 to 200 new venues around the world, that's part of our aggregation within data and analytics. And we've got 34 collection and distribution Pops around the world. That's where we collect our data from the venues and the exchanges, normalize it and then distribute it onto our network. So the unprecedented scale of modern financial markets, I think over the last couple of years we've seen some pretty big trends. COVID was one. The markets sort of flatlined a bit. Ukraine and Russia conflict saw a 40% increase in data rates. That did not go back down and it hasn't gone back down. If we look to the US legislation and tariff wars, that pushed us up to nearly 20 million messages a second on our backbone. We've got the capacity for it, we've got 20 to 25% scale within our environment where we collect and distribute that data. But this is just getting bigger, right, we're right into uh our tick history store constantly. We're right into our PAP store, plus we've got to get these messages around the world to our clients. Low latency, high throughput, no jitter, delivered on time. You know we're being really really relied on to do this. So We expect this to grow. There's more and more venues coming online. There's more and more of our clients now working with Gen AI data scientists, data engineers. All analyzing, sucking up, and hoovering this data. Trying to find alpha, trying to trade smarter, trying to beat other people into the markets. So we had to come up with a plan to deal with this. So we, we have scaled on our core network and infrastructure and backbone, plus we're storing all this stuff. The reason we store it is our clients don't want to. If you have a look at infrastructure costs today for all of those that are you in that game, you don't want to throw horsepower at this problem, you need to run at scale, both up and down. So addressing the business challenge, so legacy on-premise infrastructure is a problem for most people. You've either got to replace it every 3 to 5 years when it's advertised down, or you've got to ship more of it into your data centers, your colos are costing you more money. How do you deal with this? So at LSEG we took the decision for our historical data to be moved out, uh, and we shut down that environment which is great for us. That moves us from a CapE model to an opex model. Easier to maintain. Exponential data growth, we see it daily, it's just growing and growing and growing, we don't know where it's gonna be, we're predicting that we're at 20 million messages a second today, by 2029 that's gonna be 50 million messages a day. It's going to be a hard case to cope with, right? Uh, customer demand for real-time insights. So in a previous role, uh, I worked at a tier one investment bank. Our head of the FX desk came to me one day and said, I want to be in this venue. I was like, great, give me half a million dollars in 9 months and I'll build it. That's not good enough these days, you've got to be able to get your clients, your partners, and your desks into the market as fast as possible. Cost pressures in competitive markets, everyone's looking at margins. I'll be honest, I think our data's the best data in the world, but I do have competitive uh issues with that. So I've gotta, I've gotta make it cost effective for me and for our clients. And a need for innovation velocity. We have developers that code and execute during the day, 5 to 10 times a day they're doing releases. We've got to make those right for those markets. So the transformation journey, so historical data migration, tick history PCA. We originally stored all this stuff as CSV files. Huge amounts of data, we've now changed that to parquet format, it's easier to interrogate, it stores a lot smaller. Helps us in our costs, allows our clients to get to that data quicker. Real-time platform development. So in our 34 locations globally, we have to be able to scale when the markets move, and we do. We have a very good capacity forecasting team. We've got 20 to 25% headroom in all of these places. But even as the guy that runs real time today, I've got to look at the next generation. How am I gonna be able to scale 30, 40, 50% when it's required? Matrix signal platform and large scale ETL. So this is the brainy guy, Ed, he'll be covering this stuff shortly. So if we look at cloud native data feeds across the latency spectrum, the data room feeds business with an LSEG delivers this front to back, so ultra low latency all the way up to quant data solutions, price and reference data sets, historical this side, low latency this side. As you can see along the bottom, these are some of the AWS products we're using today to help us facilitate our move uh as we grow into the markets. So the business outcome, geographic expansion is huge for us. Our clients expect us to meet them wherever they are. The push for us now to deliver in some countries around the world, especially in regulated countries where we have to live the data has to live and reside in that country and it has to be operated in that country. We work with multiple partners. We work with regulators, we work with central banks, and we work with cloud providers as well. Faster on boarding For some of our products, it used to take us weeks to onboard. Now we can get a client up and running in 2 minutes with a with an email for our real-time optimized product. That comes out of AWS. It's 3 updates a second Trade safe. People using low latency trading won't particularly use that, but for wealth funds, hedge funds, they're happy to use that type of data. On demand capacity for scale. So when the markets moved in April this year, We had lots of clients phone us up and say, can you please conflate this data? Our applications are choking. We can't deal with it. So within our distribution Pops for each one of those clients, we reduced the update rate from full tick to maybe 4 updates a 2nd, 5 updates a second. When the market stabilized, we switched it back up to full tick. That gives us that ability to do that within our, our public and private clouds. And accelerated innovation cycles. I think we've all seen the craze of Gen AI, faster chips, Nvidia. Everyone's striving to get in these markets quicker, do smarter things with data. We've got to be smart as well to help our clients do what they need to do. So we work tirelessly with our technical uh technical teams, our architects and our developers to make sure we're ahead of the market. So what I'd like to do now for the main event is hand over to Ed Johnson, one of our real-time architects. Thank you. Thanks, Jason. Thank you, Jason. I'm Ed. I'm a real-time architect. Today I'm gonna be taking everyone in the room through um a deep dive of the real-time matrix signals platform. Both from an architecture perspective and from some of the technology lessons learnt that we've faced, that's gonna hopefully help others to er to to learn some lessons and and use our data at scale. So, as a, as an initial start, it helps us to understand a little bit about what LSEG's real-time platform is from a technical perspective, um, before we kind of get into, into some of the specifics around our, our matrix signals environment. For the non-market data experts in the room, and there will be a few of you I'm sure, LSAG Real Time is a huge globally distributed. Stateful, first in, first out, message bus. That message bus sits not only across one stream of data but across every single market instrument that we transmit in in the world. So uh that's just not, not just 11 stream, but 90 million of them. There's an example on the er on the slide there, you can see that's for, er that's our, our Rick, our, our instrument code for Nvidia, which is er as many of you will know, one of the most liquid er and most traded instruments in in the United States market. And that represents one of our, uh one of our streams of data. But we have a first in, first out, low latencyQ um that that delivers that data from our collections estate through our core infrastructure and out to to our distribution locations to our customers in the fastest way possible. As part of that service, we also add value add and analytics data in the stream, um, to make that data available natively as well to customers. And we're also normalizing that data, so putting it wherever, wherever the venue in the world is that that has provided that, that price, um, we'll put it into our LSEG format, er, and it's then the same API and the same data structure no matter where in the world you're receiving that data from. The challenge for us today and what we're gonna deep dive into is being able to understand how these 90 million streams overlap, detect hotspots at scale, work out where in our infrastructures they, they overlap, and respond to that in the market in a much, much faster, more optimized way. As part of this transformation as well, we've also delivered over 5 times cost reduction in the cost of ownership of detecting these signals in the market. Brief analogy for everybody, er just to take it out of the technical realm. It's quite straightforward if you, if you think of Elseg real time and our feed as, as a river of, of data. Imagine each stream kind of sits within that river. All 575 global stock exchanges, trading venues and, and other contributors feed almost as tributaries into that river of data. And our customers sit alongside on the banks, if you like, observing that stream as it as it comes past. At the bottom of the river, and Jason mentioned it earlier, we have a, a, a basically a, what you could think of as a lake, which is our, uh, tick history product, and that provides the captured version of that stream of data right back to 1996 with the 75 petabytes plus of data in that, in that S3 format in, in uh in parquet. So it might be useful for for some in the room and we'll come back to that analogy as we discuss. So Elseg real time is is the river and our tick history product is the lake at the end of it. So I mentioned earlier that this estate was globally distributed. So this is an overview of our um collections core and distribution infrastructure, both in LSEG's private cloud environments that that sit in um in our, in our data centers around the world, um, but also in some AWS distribution locations where we serve the real-time optimized product from. So our global er estate is is across many, many sites and all of that is connected via our um our world leading er software defined network. This collections and distribution infrastructure serves content from pretty much any source to pretty much any destination. So you can consume data from the United States, in Asia and vice versa. Um, and there's a global hardware footprint that we used to have, um, that we, er, that we used to detect signals, hotspots and insights across this environment that allowed us to respond to market volatility. Now the cost of that er that signals detection infrastructure that was designed for for capacity management and other types of of er of hotspot detection. Was ballooning um and has continued to increase as we see the increasing data rates that Jason was talking about earlier in, in the market. So we, we needed to find a way to futureproof that solution, leverage some more modern tooling, um, and transform our, our product offering as a, as a part of that as well. This hotspot detection is so important for us as an organization. Delivering data in a low latency, low jitter, accurate, timely fashion is our business, it's what we do. And it's really important that we detect that before it becomes a problem. So returning to the streams of data I was describing there. It's helpful to distill the problem statements slightly so we can deep dive into the architecture in a moment. First thing, we're going to be forecasting uh and uh optimizing for capacity management, so. That is detecting hotspots, implementing uh performance optimisations where the components of this environment, where the where the um where the infrastructure receives kind of more of these data streams. So each bit of our infrastructure serves some of them, so working out where the hotspots are in that is super, super important. We wanted to mitigate er to mitigate increasing infrastructure costs. So reducing the cost of ownership of our on-premise packet capture based signal detection um systems er was, was a key er a key deliverable as part of this. And we wanted to enable our operational teams and our leaders to accelerate the time to accelerate the speed at which they could make decisions. As we saw the markets scale, and we saw them scale very quickly, er, our signals detection environment was slower to respond in terms of signals, er, and we needed a way to, to speed that up and provide the insights to our operational teams and our leadership teams, um, enabling us to, to be able to respond quicker in a, in, in times of extreme market volatility. So let's take one of those streams, um, uh, Nvidia here's the example, we're carrying that one on from earlier, um, and there's an example here of what, what that data actually looks like from our, um, from a normalized data format. So you can see there's uh 7 messages on the screen just as examples that are mocked up, and each of these is a is is what we would call a tick in in the um. In in the in the parlance. So each tick here contains an update to a stock price. So that's fundamentally what the um what the data's showing. And there's some trades as well you can see, so trade prices and volumes as well um that will send as part of that data stream. This is just for a one second period for, for this instrument, and this is just a mocked up example. We'll have many instruments in our environment that may may have a a set of data that is sending these ticks over 10,000 times in a single second. If you scale that across the 90 million instruments we have, you can immediately see where the problem of, of scale comes in. Uh, and the amount of of work we have to do to, to, uh, make sure that this environment is super performant and super optimized. In addition to the data you see on the screen, there's also value add data and analytics that we er will package up with the feed. er and it's all sitting in the same format no matter the trading venue, as I mentioned earlier. One thing to note from a kind of market data perspective, this is only meaningful if this data is ordered, so it has to arrive in the right order, otherwise the the stock price is is is incorrect. um, so you've gotta make sure that it arrives in the in the right way. That's where the first in, first out came from. So let's dive into some some architecture then. So you see our er collections core core and distribution infrastructure that that forms our our er low latency uh on-premise environment for full tick data. Uh, the LSEG Tick History product is hosted in, uh, in Amazon S3 with the 75 petabytes of data. So the theory, uh, and, and the kind of the change that we're making as part of this architecture is being able to turn stuff that we see in the river of data in the LSEG real-time feed in, uh, into the insights we've got in tech history, and then we'll take the, the, uh, analysis, go and perform the analysis in our tech history, optimized data set, uh, in, in the cloud environment. And then reflect that back into our on-premise environments and across our our global distribution infrastructure to be able to then respond quickly to hotspots. So the solution then is taking in day captures from tick history and being able to aggregate that data into a highly optimized time series format and then take it, take out from that er the parameters and er and insights that we need to detect hotspots in the environment. So there's a bit more detail on the architecture here, um, and we started off as part of this without any of the glue components and without any of the open search stuff um to to demonstrate actually what are we going to achieve as part of this build. So I had to uh to to look at the data with Amazon Athena and we chose that as our first as our first base um because it's a a super low cost um light touch solution where we could go and inspect the data that sits in that tick history data set natively in parquet with a completely zero copy. So to get this sort of uh this approach rolling, demonstrating that the insight was there in this dataset and that we can reflect this uh the signals straight back into the environment quickly enough was super valuable, and we did that actually in a, in a few months as part of a proof of concept with no um deployment of, of infrastructure. From the proof of concept with Athena then we added the glue layer on top. That glue ETL layer is like a super optimized preaggregation, which I'll talk to in a, in a moment in some more detail. And that pre-aggregation step is, is the generation of all of the time series that we, we, um, we can derive along with some other parameters and reference information er that sits on the back of the er tick by tick view that sits in the, in the S3 bucket above. We then index and embed that data as part of our er the Amazon er OpenSearch pipelines, which then makes that available to um internal use cases and then also other parts of the organization. This is all then hung together with a series of lambda functions and event bridge and other things that er many in the room will be, be very familiar with. The signals from this come back into our environment and we use it to then respond quickly er to hotspots. So returning to that feed of of data that we started with in uh in the discussion. So the tick by tick data sits in the Amazon S3 LSEG tick history uh product set. Uh, and we're querying that natively with Amazon S3 access points, so we're not needing to, to, to copy that data. Now this isn't just stuff that we do internally. We provide this data in a zero copy fashion, uh, via S3 access points as well, uh, with our S3 direct product from Tig History. So we're taking that tick by tick view and then generating that optimized time series version of the data. So, uh each of, each of the instruments we have in our environment is aggregated into a uh 1 2nd time series with 100 millisecond burst detection across every single market instrument. So we're, we're pre-aggregating into that, uh, data set. From that point on, we're able to then use a a flexible modeling layer with Animas and Athena, which actually can be multi-tenanted as well, um, that, that we can then provide those kind of billions of data points in that optimized data set into insights and signals that we can model very flexibly with that, um, with that end query layer. So to take through an example, so there's 7 ticks mocked up on the top. Those 7 ticks would feed through into the total messages of, of, of a 7 parameter in that for that second, and then it might be we'd calculate an instrument rank as part of a model that would say go and find me the top um instruments that are being consumed in this part of Asia, for example. So I'm going to take us through 3 specific dimensions of this architecture that bring it into a little bit more detail and hopefully provide some lessons learned. First dimension is transforming trillions of ticks, you see that tick by tick store in the tick history data set into billions of optimized data points. And we're using S3 access points and GlueServus to do that. This is zero copy, and the reason we then introduced glue slightly later was so we had more and more dials to be able to performance tune the ETL phase, um, to be able to uh to get the the the most bang for the buck if you like in terms of uh performance data ingestion and aggregation. There's flexible scaling glue as well, so we scale up and down depending on whether it's weekend or in day or a busy part of the market, US market open. So that helps us with our, our cost optimization. Um, and we're also joining this information at this stage with reference information and other parameters that helps us support and aggregate against other parts of the environment. As a part of this we've also then delivered a zero storage cost of ownership because we're no longer having to, to maintain any storage costs as part of the solution. Second dimension, uh, we're creating the model from those billions of optimized data points, uh, to, uh, kind of very specific signals that are responding to the questions that we want to ask of the data. So we've retained this flexible backbone that we kind of saw initially as part of the proof of concept, because we didn't really know in advance the set of questions that we'd want to ask of this data set. So it, it's a really neat way to be able to actually futureproof the solution and provide a capability to to future use cases as well as the ones that we know about today. This Athena layer automatically detects when new data's available and we'll run all of the models against that new data set. Um and then we, we're asking any question of the data with with that. So some examples of the questions that we'd ask with this layer of uh of aggregation. So it might be we'd say pinpoint all 100 milliseconds hotspots in the US region at at at market open or go and find me the most liquid instruments uh for today compared to yesterday, or analyze kind of compute requirements and performance requirements for a given instrument watchlist for one of our, our, our, our tier one bank customers. We structure that with Trino, er Athena er SQL queries, but all of those are multi-tenanted, so they're owned by the end users of this, this capacity and hotspot detection data. We're not having to build a very specific pipeline for each individual use case. Maintains a super low cost as well um and we we don't really have to pay for this environment either. The third dimension then is something that you might not, may not have seen linked to Athena um before, but we're taking the outputs of Athena in parquet format uh and auto-indexing them straight away into OpenSearch. Now this is beneficial for us for two very specific reasons. It gives us API-based access to that data and uh means we're means that we're able to aggregate in a very specific, um. Aggregate in a very specific way and then provide that data as an API natively no matter on on the use case, um, so we're indexing that data so we can provide quick insights and serve that via an API. In addition to that, we're also then making that data available in Bedrock via um for for knowledge-based use cases um so that we can er get the embedding results of, of the data and actually detect er hotspots but also correlate those hotspots in a a Gen AI native way that's futureproofed for uh for the technologies uh that are available in Bedrock. So by making this this available for any use case, um we've we've actually improved the, the data set that's available for capacity across the organization. So it's now API based integration or Gen AI based integrations and er those hotspots and the volatility detection we've made available to a much wider audience within the within the firm. So it might be then we can start to do fin-ops on it and do other things as well. As a final bit of er explanation for this, and you saw on, on the on the on the first slides around volatility, the unexpected news in the market drives volatility for us. Um, so when we see that. Having a, a quick and easy way to correlate that volatility information with market leading news that we provide for financial services customers from, from Reuters is super valuable. So we've added a news correlation layer as part of this solution, so not, we're not only saying there was a hotspot, but we're able to also explain why there was a hotspot. That's something that is only available from a Gen AI type summarization er and RAG perspective. Finally then, multi-tenancy and being able to to to do this at scale, you might just be wondering how we're doing that with Athena. Very straightforward, we're just putting it through Fargate and and making those authenticated APIs available but in a safe structured way to different users within the organization. So that's helping us with that part as well. So let's take a signal detection from end to end. So we've got that tick by tick data which ultimately is in our uh AWS Glue servalus aggregation job. And that's pre-aggregated, we've got that data available then for every market instrument to to to be able to detect and ask any question we want of that data. So there's an example instrument there that might have uh 5000 messages in a in a given second, of which 4000 of those will have appeared in a 100 millisecond burst within that second. From that glue server this job, er we'll then auto run our hotspot detection model as part of the Amazon Athena SQL um approach. So we'll detect unusual bursts in the data flow, um, and in this case we're saying it, er, we want to look for hotspots, um, where there's a proportion of burst messages that are, are significant, er above the total available. From that point, we'll then auto-index the output of all of that, so find all the hotspots and then auto-index all the hotspots, um, to trigger burst-based monitoring, um, uh, via an API. So this is, so it's a very quick model of detection on new data, followed by auto-indexing. Um, which is super straightforward as well, and that indexing is then made available via an API, um, based view which we then kick off, um, to, to take infrastructure action, uh, specific monitoring actions, or burst remediation as part of that environment. So from end to end, we're able to to aggregate this data from, from tick by tick view in a really efficient, super cost effective way without copying any data around our environment. Um, but we're doing this at at a scale where we're touching all 90 million instruments in the environment, which is something that asking this question of every single one individually, we would never have been able to do in the past. So returning to the end to end architecture then, uh we've got uh a. Ability to take the tick by tick data. Aggregate and analyze that data. Uh, with glue, so there's a, a sort of a, a, a, a, a cost to that, but it's, uh, it's managed, and then we model that data through Amazon Atheno as part of a, a flexible querying layer, er, and then we'll index and embed that as part of our Amazon Bedrock and Fargate, um, delivery mechanisms. So the purpose of this has been to be able to deliver real-time pattern and anomaly detection at scale and We're doing this as soon as the data's available for us in that S3 bucket. The scale we're talking about, it's trillions of messages um into key insights, and from a storage perspective, uh, infrastructure cost perspective, and a time to signal perspective, I'm gonna deep dive into these slightly. So we were maintaining on-premise um hardware for packet capture across this signals environment uh which was, which was very, very expensive to maintain and support. We've reduced the storage costs from a uh to basically negligible uh over, over that estate to, to effectively a 97% reduction um using the data that's available in in S3 already. The cost of infrastructure we see reducing the cost of that and using scalable glue jobs and some of the enterprise capabilities that exist in our AWS environment means that we've delivered about an 82% reduction in infrastructure costs as well. And on top of that, uh, the time to signal we've reduced from 3 days in, in the worst case if we're looking for specific data points, right down to a 30 to 45 minute window, um, which is, uh, completely revolutionary for how quickly we can respond to hotspots for our customers. This is all a servers first architecture, so we're not having to maintain er OS images and er patching and all those other things that that's expensive operationally. So from an operational cost perspective and by leveraging the cloud data um approach here, we've been able to completely reduce our our our operational expenses too. We're very optimistic that this can scale as well, um, as we see it at the the number of messages in the market approach 50 million messages per second by 2050 or 2030 rather. Apologies for the floating tea on that, on that bubble, um, so. Uh, we've been enabled by this AWS technology, um, to address a, a kind of a wider range of organizational challenges than just capacity management. Um, and this kind of sums up the overall thing we've done. We would have been able to, to, to address capacity management by existing in, uh, by investing in the existing approach. What we've actually done in this case though is add a load of other benefits on uh by just changing the architecture and using some serverless tooling, and we've been able to make the most of the many innovations that have happened in the big data space over the last 10 years, that previously we not, have not been accessible to us from a kind of low latency platform perspective. So we've improved capacity management, but I'm gonna just touch on some specifics around the other things we've been able to do. So as I mentioned earlier, we're leveraging Bedrock to enrich the hotspot detection with specific market news and insights um from Reuters news data that explains why there's there's hotspots in addition to the fact that there is some. We've ensured open API based access to data so that we're not having specific kind of domain experts guarding of capacity and hotspot detection information and making that open across the organization and enabling many, many different use cases to to to respond to hotspots across the environment. We've been able to plot the end to end data path for the first time consistently, and the reason that's been hard is we have 90 million distinct streams of data as I talked about working out where those are flowing from, and two is a is a very large, uh big data problem for us. So being able to plot that path from top to bottom is super helpful, and we've been able to do that across every market instrument for the first time. Enabling FinOs for market data to consumers, um, that's really important. So, uh, how we better optimize our environment to, to, to, um, to put our, our biggest infrastructure in locations where there's the most volatility is super valuable. And then we've been able to reduce the time for new customers on boardings, um, which is er very important as part of our er our product experience because we're able to deliver more accurate, more targeted estimations of bandwidth, capacity requirements, um, last line connectivity, er, in a, in a, in a few um few minutes versus the, the multiple days it used to take us in the past to go and er get that data out of the old signal systems. It's been a good journey, we've learnt a lot, er, proof of concept infrastructure, er and proving this to the organization meant that, er, Jason and and others had the confidence to, to fund it to delivery. And we're now in a position where we, we can, er, scale across the environment to be able to detect signals and hotspots, er, into the future as we see market data volumes increase. Thanks for listening to the deep dive and I'm gonna hand back to Jason who's gonna conclude and pass on some next steps. Cheers, Ed, thank you very much. I appreciate it. Um, so as Ed mentioned there, the POC, if he'd asked me to do the POC on real live infrastructure and data center, it would've cost me a fortune. I think it cost me 20 quid, so it was great, and I'm joking, it wasn't that cheap. Um, but conclusions and next steps. So I'll's AWS transformation journey, um. Our tick history data, 75 petabytes and growing by terabytes a day, now sits in a scalable environment that's not on my premise, doesn't cost me Capex, and we can scale when we need it. It also gives our clients a direct access point to that data. They don't have to come into my environment, they can use their own S3 and connect to it. So that gives all of our client base. Single point of access to basically all of our data without them having to store it. Real-time platform development, so real-time optimized is one of our products that comes out of AWS, it's trades safe, it's 3 updates per second. We have about 500 clients using that today because they don't need full tech. They're comfortable running their workloads or they're taking that data directly into their premises. RTMDS, which is our managed service offering. Last year we started deploying this in public cloud as well. So we're managing the infrastructure and the service for our clients while they look after their apps and their workloads. And then with the release of private link connectivity last year by AWS, I have 6 regions globally. I can now interconnect to any AWS region using their backbone, their infrastructure, without my clients or myself having to get involved in procuring telco lines. So that helps us get to the market a lot quicker. Matrix signals platform. Great, you know, invention by Ed and some other smart guys in our organization, this allows us to focus on capacity management and scaling. We know where the hotspots are, we know where we need to invest and we know where we need to scale up. So hopefully we'll continue to build and it'll exist for the lifetime. And then advanced advanced optimisations. So like I alluded to, we're a data company, we're not a telco provider. Wherever I can get my data to my clients quickly, fairly cheaply and easily without me having to do it, I'm gonna take that option. So the release of Cross Region Private Link last year to the 34 AWS regions is great for me and my clients on AWS. Um, works for us, enables us to get our data out quicker and faster. Um, everyone's happy. So this is some of the transformations we've done in partnership with AWS, um. And yeah, we'll continue to work with these guys and see how we get on. So I think I'm handing over to Rohit now who'll give us the last summary. Cheers Robert. Cheers, thank you. So thank you Jason and Ed. Um, uh, what an incredible journey and um the technical architecture that Ed took us through is truly impressive, especially the, the fact that it's zero data copy. And um it's completely serverless uh and cloud nated by design and this is one of the examples where you build a solution early on like the ticketry solution now they're building on top of it, coming up with new use cases, and I think the innovation will carry on. So just coming on to a bit on the Gen AI side of the of the equation, so metric solution, um, is, is, uh, kind of demonstrating the, the Gen AI potential for financial services, one of the examples. So having an AI system that can correlate. Uh, you know, market, uh, events with, uh, Reuters news, so ability and G AI's ability to kind of relate events, um, and market movements to when, uh, you know, news, uh, events happen, and then these are, we're talking about some, some, you know, major, major events, and this is not something you can operate a, a business analyst can, can do, or, or spot, um, you know, at, at this pace and at that scale of data. And then also uh ability to kind of recognize patterns when we have 90 million plus instruments simultaneously, the ability to, um, you know, finding connections and hotspots and trends which are not possible for you know a human to identify. And then obviously the metrics platform is providing operational actionable operational intelligence to for the team to kind of prepare themselves when those events happen in a in a much quicker fashion. So this is one of those problems that is truly solved by Gen AI when you're transforming a very large amount of data and into actionable intelligence, um, and especially at this scale and this speed, which is unimaginable and just a few years ago if you were to kind of use another system to do it. So I mean the LSEC team are using G AI. It's just not nice to have. They're actually using it to fundamentally transform the way market intelligence works. So these are some of the resources, AWS services and features that will be relevant to our capital market customers. So some of the things our LEC team is using. So things like, uh, you know, low latency outposts, things like, uh, the, the net microsecond, uh, uh, time stamping ability on EC2, and these solutions are continuing to coming out. Uh, we also included a couple of links to the, um, LSEC solutions which are featured in today's session. So, uh, do try and, um, I'll probably pause here for a second if you want to take a, take a picture of those, uh, QR codes. So with that I would like to thank you for uh attending this session, uh, EE's incredible journey, uh, with, you know, uh, of what exactly they've managed to achieve, uh, at this massive scale, uh, an example of any combined visionary leadership, uh, the right, uh, cloud platform. And a collaborative partnership approach. So the aid of services which Shellse team are using are available ready for, um, you know, customers to try and build the next generation financial solutions, um, you know, the genre, the, the organization which are embracing the ability to start using these, uh, services, these innovation will be the market leader of the future. And uh we'll be we'll be around later on to answer any questions, any specific discussion you wanna have about a specific challenge or opportunity um with that I will thank you for your time and please do provide some feedback for the session. Thank you very much.