---
video_id: Ce9lJzpHBJ0
video_url: https://www.youtube.com/watch?v=Ce9lJzpHBJ0
title: AWS re:Invent 2025 - What's new with AWS Lambda (CNS376)
author: AWS Events
published_date: 2025-12-04
length_minutes: 18.93
views: 681
description: "Discover the latest innovations in AWS Lambda that go beyond traditional event-driven applications. In this lightning talk, we'll showcase the game-changing capabilities that we just launched at re:Invent. Learn how these features can accelerate your development process and scale your applications effortlessly. Whether you're building proof-of-concept or production-ready solutions, see how Lambda's simplicity and power can help you deliver faster results for web applications, AI solutions, and e..."
keywords: AWS reInvent 2025
is_generated: False
is_translatable: True
---

I lead product management for AWS Lambda in my spare time. In my main time I'm here blocking you all from lunch. So let's get right to it. Uh, you know, my, uh, Serverless has been around for a decade now, and, uh, in this time, usually if a technology manages to stick around, there's always biases that build up and, uh, about what the technology can do, cannot do, and so on. And 2025 in particular has been a transformational year for lambda, for serverless because. We've built a bunch of new capabilities that enable entirely new classes of workloads for you to service, you know, with server less, and, uh, let us get to it, you know, I just have 20 short minutes with you. I want to cover as much as possible. Uh, before we get into the launches, you know, a key differentiation of like, you know, why should lambda exist in this world, you know, a lot of customers ask us that, uh, believe it or not, it's been 10 years, but still, uh, the, the main, uh, aspiration that we have to give to customers and to builders is speed. Now speed not just in code generation, you know, that generative AI can take care of it for you, but also the, the whole shipping cycle, uh, code generation just one piece, but then you have to ship faster and if you ship faster you're able to build engaging applications, you know, collect more iterations from your users, feedback from our users, iterate on the application, and so on. We try to make sure that an application is well architected by default, so we try to do this by enabling or taking care of all of the eliteities for you, the scalability, availability, reliability, and many more such more elites. And, uh, again, the primary benefit is there is no infrastructure to manage. It reduces a crap ton of operations for you, uh, that enables you to get again more speed. So I've bucketed the launches into 3, sort of cohorts. First is the new primitives. This allows you to do, you know, entirely new classes of workloads, uh, that what you could not do before. Uh, we're delighted to introduce Lambda's managed instances for you. So the mental model here was, you know, customers tell us that lambda or servers really shines when you have needlepoint workloads, you know, when you're, uh, 700, 800 xing your traffic, you know, from zero, and then your scale to 0 is again important to you. In those use cases, lambda was really fantastic. But when your application achieves, you know, a really good level of adoption, scale to zero is no longer important. There's always some traffic, some users to surf. At that time, I'm calling it steady-state traffic. So for steady-state traffic, what builders wanted to do was to optimize. Optimize performance, you know, by way of adding, let's say more network or memory or compute specific instances so managed instances allow you to do that. You can choose where to run your lambda function, for instance. Uh, they wanted to still benefit from fully surveillance operations with the same developer experience, so, uh, you have all of the same event source integrations, you know, patching, routing, scaling, you know, we do all of that for you, you know, still with the manage instances. And optimize costs. So, uh, you know if you know lambda today, if you use lambda today, you know it is single concurrent one execution environment serves only one request, uh, that was inefficient for some classes of workloads. With this we're also adding the concept of multi-concurrency, so multiple requests can hit the same execution environment to be serviced. And when you comply or when you bake in EC2's pricing incentives like savings plan, reserve instances, your savings really, really magnify. So that's the key benefit here. Uh, I do realize that, you know, adding, uh, servers to surveillance is a bit of a mental model shift, so we try to, you know, make it as intuitive as possible. The only additional thing that you have to do is to create this concept called a capacity provider. Uh, there's APIs for it too, but, uh, and console obviously, but, uh, in the, in the capacity provider you give it your preferences on what compute instances or memory instances or what the scaling profile is that you want, or optionally you can just let lambda choose for you and we'll try to continuously benefit or you know improve your price performance of the workload right off the bat. So, uh, thinking a little bit about the use cases, we covered the steady state applications, you know, that is just seamlessly handled now. So the entire, entire gamut of applications, you know, the, the functionality of your app that requires, you know, needlepoint bursts or needlepoint traffic management that you can leave on lambda today, uh, if you want the functionality that achieves steady state, you know, real popularity, there's always users to serve, you know, you can move them over to, uh, manage instances. Uh, the performance critical apps, you know, the real SLA bound apps for which you need specialized instances, you know, they get opened up. Uh, the variety of applications open up, you know, from media data processing, you know, web applications, event-driven applications. You can migrate a whole lot more to servers now, and, uh, at times for those of you who have to struggle with regulatory requirements, struggle is a chosen word. Um, you know, you, uh, have some preferences like, hey, I can only use compute in local, uh, zone X or in availability zone Y and so on. With managed instances, the mental modelers will be able to, uh, help you there. The other primitive is lambda's tenants isolation feature. So, especially if you're a SAS builder, and SAS builder is an analogy, but wherever you need isolation between requests from different tenants that go to the same execution environment. What happens is, you know, builders have to do a bunch of custom tooling and processes which again introduces pain in the CICD pipelines and, you know, shipping software cycles because what happens is that's if you're using a global variable or if you are storing something to disk of that execution environment. If you try to reuse that execution environment for the next tenant, and you should reuse because that is what improves your utilization and which will drive down your costs, you're able to, you know, increase your margins, uh. So if you try to do that, you have to clean up the execution environment which reduces your velocity. So with tenant isolation, the only additional thing you have to do to use it is pass in the unique identification of that tenant. Sometimes it's a tenant ID, sometimes it's a JWT token that you use, pass in anything to us, and we guarantee that we'll give you a clean, isolated execution environment for that tenant. So it again, improves your CICD cycles, helps you to achieve isolation without any custom tooling whatsoever. Next, we'll switch to how we try to make you well architected by default. So new runtime. Run times is a key component of our value prop of lambdas. That we offer to customers, uh, it's basically the programming languages, and, uh, you know, I started my career as a developer. Uh, I like to use the latest features, and the latest technologies. The performance fixes are baked in security fixes are baked in, so new run times are are, you know, highly in demand. We provide them to you uh with full management uh throughout the life cycle of that run time. So over the last couple of weeks, we've announced Python 314, Java 25, Note GS 24, all available with, you know, patching and performance optimizations baked in. Uh, Again, what it does, it helps to improve developer productivity. You're able to ship safe software faster, uh, and, you know, our mental model here is within 90 days of the runtime being available in the community, we want to make it available to you, uh, for use in lambda. The other thing we do is uh upgrading the runtime. Now, upgrading with patches, you know, let's say log 4G vulnerability happens, it needs a runtime patch, we do it all for you. Uh, but the effort to upgrade runtime is high, particularly if, you know, you're one of those who has like, you know, hundreds of thousands of functions. Upgrading is quite cumbersome. So that is where we found one of the, at least in my mind, one of the great use cases for generative AI. Uh, all of that technical debt that you take by not upgrading, you know, same day, uh, turns out G AI is pretty good at it. So in Matt's keynote today or yesterday, you know, we announced this new capability called AWS Transform Custom. This provides you, you know, G AI based upgrades. They're incredibly easy to use. They are baked in. You can bake them into your pipelines, into your dev or software release cycles seamlessly. And uh in previews, we saw that it reduces your tech debt by up to 85%. So 85% of functions you could just accept as is what the Gen AI recommendation was. So go ahead, use it if you would like to run, uh, upgrade your runtime. Uh, Snapstart, so, uh, Servalus introduced the term cold starts in the dictionary, uh, and it happens when you initialize your function with your dependencies. Uh, we announced Snapstart for Java a couple years back. We've also announced it for Python and.NET now. So what this does is it takes the snapshot of that entire execution environment. When you start the function creation process or the function deployment process rather and on subsequent invokes we restore that snapshot, so your initialization time is really reduced. What we see is like the cold start times really benefit which leads to a lot of happy end users of your applications. There's no code changes required and no custom tooling for you to build. It's just a checkbox that you enable. And then, uh, you know, we are all good engineers. We want to test our applications, uh, in spite of the, you know, removal of testers from the software cycle in, in, in a way, uh, so testing resilience is hard, uh, you know, AWS has a service called fault injection Service or FIS. What it allows you to do is that it allows you to, uh, you know, specify your conditions for testing, increase latency, increase, uh, you know, like make sure downstream piece like like some storage or database is not available for a while, and so on, and then it helps you to see how your application reacts to those stress conditions. So this integration with FIS helps you to do exactly that, uh, eventually what it leads to is like, uh, you know, you, uh, see how your application reacts to real production scenarios, helps you to plan and prevent outages better and, you know, have more confidence as you go to bed every night as a developer again, no custom tooling required. This is a picture of a type of uh uh support that FIS adds to you. You can test for delays here. This picture shows you the graphs of the state of the world, uh, with the delay that you see in your application that you would see in production otherwise. And then, uh, you know, there's no surveillance without event driven patterns, so we have to do something there, um, with serverless, one of the key things that developers tell us is that, you know, they don't have access to the underlying hardware, so observability is hard, uh, and it needs to be better, especially as we poll the event sources like, you know, SQS or, you know, Kafka triggers on the customers' behalf, um, and lack of visibility into this polling mechanism was a hindrance. Because what they had to do was they had to piece together the metrics that, uh, you know, the Kafka or the SQS event source provides with what the lambda function provides. Um, so what we did was we enabled the additional cloud wash metrics. So you see the polar count and lag and throttles right there. Uh, this helps you again to detect your issues instantly, uh, you know, increase your or improve rather your time to resolution, and again, no custom tooling required. Here's a picture of some of the additional metrics that you now get. Schema registry, so customers that use Kafka in particular try to use the Avro format. Avro supports schema evolution, and you know it's just easier to deserial serialize, but we didn't support Avro format before, so the pain was, you know, builders had to manually add boilerplate code to every function that they want to use to process their Kafka events. Uh, what we did, uh, a couple months back is added, uh, this capability, capability to auto serialize and deserialize, uh, you know, avro events. This also gives you ability to specify your, uh, schemas and, you know, evolve your schemas. Leading to less code and with less code, as you know, it's less errors, uh, add scheme evolution support and you know again, no custom tooling required. Provision mode, the faster we try to scale up for customers, the faster they want to go further. So for some of the customers, they wanted even faster scaling that required them to have specific control over the concurrency that they specify for the polling mechanism. So this feature provision mode for SQS enables them to pre-warm capacity to handle any spike instantly. We released provision mode for Kafka last year at treatment, so this is a follow up for SQS. Again, it helps to eliminate uh delays as your spikes happen, helps you to meet SLAs, optimize costs, and no custom tooling required again. Now, accelerating developers, this is again to ship, uh, to help the developers to ship software faster, not just generate code faster. And when we talk to developers, you know, what they say is like they go to the console just to test the feature, you know, do basic Hello world capabilities, but after that they try to develop in their local machines IDE. Oftentimes it's VS code, and then they try to test locally. That is what drives the faster accelerated ship cycles or the development cycles in the in this phase. And then before deploying the production they want to test in the cloud. So these 3 integration points or these 3, you know, deflection points is what we, uh, capitalized on and we converted them into 3 features that, you know, help you to accelerate your ship cycles. So the first is seamless code, seamless console to ID transition. What it allows you to do is like as you build the, you know, the, the bare bones application on the console with a single click, uh, we package in all of the dependencies for you and it starts to light up on your local IDE. You can start immediately coding on your local ID without doing any additional manual work or downloading or so on. Second is local testing. So LocalStack is an AWS partner. They assist with emulating and simulating a bunch of AWS services, and an application is not just the function, but it's comprising of, you know, storage and database and networking and so on. So what LocalStack allows you to do is to, you know, do all of this locally in your ID. So with LocalStack, uh, we're deeply integrated now. Uh, it helps you to develop offline fully on your local machine, uh, test on your local machine. Leading to faster iterations of your business logic. Without any additional custom tooling that you need. And finally at this phase, you know, you're at the at the place where most developers say, hey, it works on my machine, you know, it sucks to be you, it doesn't work on your machine, but like you know you want to be good people and you want to like, you know, test in the cloud before to get a feel of what the real world looks like before you deploy. So remote debugging enables you to do exactly that with like two clicks. You can simply enable your checkpoints or breakpoints, analyze your variables and source code the way you do locally while the code runs in production. So additional test cases that you can cover here are, you know, like your IAM policies or permissions, uh, your roles, your database connectivity, so your VPC and network packs and so on. Without any additional custom to them. And uh with the MCP server that we released in Q2, our idea was to uh enable best practices to be baked in because customers tell us like you know generative AI assists in accelerating coding, but that is that code is not production ready, uh, you know, there's not much by way of input validations or error handling or, you know, status codes and so on so our MCP server bakes those best practices in, in, in it so your coding assistant. It can actually generate better quality code. It's more consistent quality that, you know, helps in reducing your code review cycles as well, overall accelerating your velocity. Uh, and we have support for web apps, and this is just a key picture that shows you, like, you know, how we, uh, list and deploy web apps, and then we also added support for event-driven applications here so the MCP server can now help you to, uh, set up your Kafka triggers and, you know, complete your event-driven architectures as well. So there's some other sessions on servers and there's some not here if you're interested here, um, you know, like some, some capabilities like durable functions that was announced earlier this morning because this session was within 4 hours. I couldn't, I didn't have the permission to cover it, but tomorrow's session we will cover that in great detail as well. Please join us and our roadmap is now on GitHub. Uh, we welcome all participation there. Any future requests you have or, you know, any courses you want to give us, we might delete those posts, but you know, we will, uh, read them at least, uh, to delete. You also have to read, uh, but please engage in discussions there. You know, we're fairly customer obsessed, deeply customer obsessed rather, and we'd love to, uh, get your input into what we should build next and. Uh, if you want to get involved in any private previews, developer previews, you know, you, we can have the conversation there. Thank you very much for joining me today. Uh, I know I spoke like a train. Uh, it was really 20 minutes compressed. A bunch of stuff that I could include I had to delete, but please join us in the other sessions. Uh, we would love to have you there. And, uh, like I said, I was holding you from lunch or for lunch, from lunch, uh, stick around or get lunch, but you know, it was great to connect with you today. OK? Thank you so much.