---
video_id: L5PvkwkpeAM
video_url: https://www.youtube.com/watch?v=L5PvkwkpeAM
is_generated: False
is_translatable: True
summary: This comprehensive session delves into AWS's incident management methodology, covering the entire lifecycle from detection through resolution and post-incident analysis, presented by Giorgio and Anthony from AWS Enterprise Support and AWS Event Management teams who collectively possess over twenty years of incident response experience. The presentation begins by establishing the foundation of event detection through two primary categories service-driven metrics including alarms, subsystem monitoring, synthetic canaries that emulate customer experiences, and aggregate alarms that trigger when multiple services fail simultaneously, alongside customer-driven metrics that monitor traffic anomalies and analyze support case trends to identify potential issues before internal systems detect them. The speakers introduce the AWS Dashboard, a centralized monitoring tool created in 2008 that provides visibility into all AWS services and regions with three to five key performance metrics per service, emphasizing that this single-pane-of-glass view remains critical for managing AWS's massive scale. Event engagement occurs through two pathways single-service events that engage only affected service teams and AWS support when customers are impacted, and multi-service events triggered by aggregate alarms that engage AWS Incident Response teams, support teams, all impacted services, and foundational "usual suspect" services like authentication, DNS, and networking from the outset. The coordination phase employs parallel work streams including the tech call focused on rapid mitigation with hundreds of participants coordinated by AWS Air teams who own the process and tooling rather than merely facilitating, and the support call managed by AWS Event Management that handles customer communication, recovery guidance, account team engagement, and ingestion of customer feedback to verify mitigations are working. The mitigation philosophy prioritizes shifting away from failures through techniques like removing problematic instances from fleets, isolating failing availability zones, rolling back deployments without confirming correlation to reduce customer impact immediately, scaling resources to compensate for increased consumption, and only as last resorts restarting components or rolling forward with new configurations. Post-incident analysis through Correction of Errors documents includes comprehensive impact summaries, timeline narratives, root cause analysis using the five whys methodology reimagined as a tree rather than a linear chain to identify multiple contributing factors simultaneously, lessons learned that flow directly from root causes, and action items categorized as short-term immediate recurrence prevention, mid-term sustainable solutions implemented within weeks, and long-term systemic changes that may involve building entirely new services. The speakers emphasize creating safe spaces for retrospectives where decisions are assumed to have been made with best intent, examining context rather than assigning blame, being self-critical about detection speed and observability gaps, analyzing dependencies to ensure they worked as promised and were implemented correctly, and validating that internal metrics accurately reflected customer experience throughout the incident. Learning and scaling mechanisms include team-level COE reviews, organization-wide reviews, AWS-wide Wednesday operational metrics meetings attended by thousands including engineers, product managers, and leaders where patterns are identified and solutions shared across all of AWS, with scaling achieved through team education via operational tenants, distributed changes like Trusted Advisor recommendations, and centralized changes like AWS Certificate Manager and the migration of STS from global to regional endpoints that required no customer action. The presentation concludes with key takeaways engage early and often with leadership involvement, communicate fast during events with iterative depth rather than delayed comprehensive updates, mitigate first before deep diving root causes since incident response is not the time for investigation, ensure incidents become powerful organizational lessons rather than isolated team learnings, reflect with empathy in safe spaces, and establish simple memorable tenants that withstand time and serve as tiebreakers for difficult decisions across teams.
keywords: incident management, AWS operations, root cause analysis, post-incident review, operational excellence
---

Hey, so, hello and welcome everyone to Reinvent 2025. Thanks for being here, especially considering this is a 9:00 a.m. session. I hope jet lag is treating you all well. Um, we are here today to talk about the incident, and it's a really good thing that we haven't had one in a while, right? So, let's start off with a bit of background on who we are and why we are here today. I'm muted. OK, I'm Giorgio. I'm a principal in AWS Enterprise Support that is the organization that supports our customers in their day to day improvement of operations and resilience, and I'm Anthony. I'm a principal engineer in the AWS event management team. We're part of the AWS health organization. And together we have more than 20 years of combined experience in all things incident at AWS from detection to resolution and implementation of action items, and we are here today to share some lessons we have learned over time, tips and tricks, and discuss how we do things here at Amazon. OK. So with a show of hands, how many folks here have been involved either in an incident or after an incident? OK, looks like most of us, well, you're in the right spot, um, so today we're gonna go through a couple different things. We're gonna talk about event management at AWS, specifically how we do it, uh, how we detect, uh, how we engage, and then we're gonna focus the discussion around the retrospective phase, um, which is where we'll focus on specifically how we do that at scale, and we'll start with how we, uh, trigger and manage events and then zoom into that. But I wanna key in on the fact that. The post incident is not just the creation of a document, right? It's really deep diving and truly understanding the root cause, um, and so we'll talk about the five whys and a couple of different ways to get to that. And then to wrap up we'll talk about specifically how to scale it across a very large organization. OK, so we're gonna start with detection. So all events start with detection and they end at least in the short term with mitigation. So for detection, we have two broad categories of events. Uh, the first is service-driven. And service-driven, uh, metrics include things like alarms on the service, right? So EC2, uh, launch instances, um, launch latency, etc. But we also have metrics and alarms on the subsystem layer. Uh, and so those might include other subsystems in the path for the whole plane, uh, or, or in the data plan and the creation of the instances. But we also supplement that with synthetic monitoring, uh, we call them canaries, and these are intended to emulate the customer experience end to end. And so you can think of, um, launching an EC2 instance. Certainly we're monitoring whether the, the launch was successful, but we also want to know whether that instance is available, whether it can be, uh, contacted and, um, responding to DNS requests. And so those are the two main categories of metrics and and how we detect issues, but the third one is maybe a little bit unique to AWS, and these are what we call aggregate alarms. And so these are alarms that trigger when multiple services are are in an alarm at the same time or multiple metrics. Um, those are a little bit different in that they engage, uh, a full incident response, um, and we'll talk about that here in just a second. And the second category of metrics that we use for detection are what we call customer-driven. So those are of two types. The first one is we monitor for traffic anomalies. We have models that allow us to predict the amount of traffic that should be hitting a service at a certain time of the day in a certain region. And if the traffic is not there, then we trigger alarms that start investigations on why the traffic cannot reach the service. And related, we track impact reports customers. So this goes from um noise on social media to reports on social media to support cases raised by customers, and we analyze those trends and if there is an uptick on cases about a specific service, then it might be a sign that we need to engage and start investigating a potential issue that the first category of metrics is not good. OK, so let's talk about detection a little bit. So I wanna show you the AWS dashboard, and so this dashboard, legend has it, was built in 2008, 2009 time, uh, by an engineer from Dublin who was visiting Seattle and had identified the problem that as we scale, we need a centralized view into the health of AWS services and AWS regions. Um, they also wanted to build a deep dive by partition as well. And so what we have here is we have a view of all of that. And we can drill into each service and each region. And each service on boards 3 to 5 key performance metrics that indicate their, their services, uh, overall health. And ultimately over the years we've continued to develop and own this the same tool and not much has changed, but the goal remains the same, right? We need that centralized view into AWS's health. So after detection, we have engagement. And so engagement, we have two categories of events here as well. We have single service events that are triggered by service alarms. We'll start with that. Um, these are, these are alarms that engage the, the service team, and generally only the service team for what's in alarm. Um, they can engage others as required, and ultimately they can engage AWS support and AWS support is in the loop if customers are impacted by an issue. Secondarily, we have multi-service events. And so these are the ones that are triggered by the aggregate alarms that I was mentioning before. Uh, and so these ones are engagement at scale. And so they trigger, uh, engagements of a couple of different teams. So the first one is AWS incident Response, or as we call AIR. Additionally, AWS support is involved, uh, and engaged at the onset of the event. All impacted services from the creation of the event all the way down to, uh, the full life cycle of the event. So not just what's an alarm at the creation, but, um, throughout the entire time and at the onset of these we also engage what we like to call the usual suspects and so these will be, um, you know, foundational or core services that are low in the stack that are that are often a root cause of multi-service, you know, events. So these are things like, uh, authentication. Um, DNS, networking, etc. So we'll engage Route 53, IM, um, and our, our, uh, networking teams. So after engagements, we have the coordination phase and you can imagine coordinating incident response at at AWS scale is quite a challenge. Uh, we do this by breaking down into work streams, each one coordinated by a related call. Um, the first one is the tech call. Um, it involves engineers and leaders of the affected services, and it's focused on delivering the fastest possible mitigation and the resolution for customers. And then secondarily we have the support call, and the support call focuses on communicating with impacted customers, but also giving customers, uh, advice and, and how to recover faster. There's oftentimes scenarios where we can provide information to get you out of pain earlier. Uh, the, the participants on that call include support and service leadership. So the way we think about the tech call is that we want to run a large scale parallel investigation instead of serious. So the way we'll do this is we will engage all of the impacted teams and ask them to review their services, their metrics, and come up with impact statements. This might be slightly different from what you are generally used to see, that is a sequential investigation where each possible root cause is removed. In order. So that's, that's a bit deeper into the tech co. So Anthony mentioned it's supported by a team called AWS Incident Response, uh, AWS Air for France. Um, this is not the classic operation team. They, they are not just here to operate the courses and coordinate the incident response. They own the process end to end. They own the mental models. They own the foundations of our incident response, and they own all of the related tooling. So they are quite active in the process. They are not here just for coordination. As we say, this goal is focused on one thing, that is mitigation, and after mitigation resolution. Um, large events are supported by what we call a call leader. Now, um, the industry standard term for call leaders is probably something like incident commander, and this is the person, extremely senior person that is in the call and takes decisions where we face scenarios that we have not experienced before. It's a really small number of individuals. It's single digit across the entire of AWS, and they are here to practice or just to take those split-second decisions that help with incident integration. Um, the code is paired to a ticket, and we have quite a strong mindset about what is discussed in the call. Imagine this call we often run in, in the hundreds of attendees. So having um a mindset, a clear definition of what needs to be said in the call and what can be written in the ticket instead and what must be written in the ticket because we want to study it for long term is really key. And we support this to a strong etiquette, so it's quite clear on who's supposed to talk. It's clear who has the microphone at any certain point in time. I wish we were a bit better with muting people because it's not a nerd who have dogs barking in the background, helicopters taking off, or similar. And this call is the one and main coordination bridge for the van. But when a service team needs some time to maybe look deeper into their functionalities and involve different engineers, they are going to fork to a separate one and still keep at least one engineer that has a bridge between the two. So the tech core remains the core of incident response that can be forked out as appropriate and as required from time to time. So I mentioned the tactics that we use for long-term storage of observation. I want to share with you a couple of examples of those observations that might make this more tangible. So in the first example, we have a cat or an engineer um pasting an error log. Now, you might think this is quite redundant since the error log is stored somewhere else and it's got a time stamp already, but putting it here makes it immediately visible to the hundreds of resolvers that are involved. And also gives us an indirect piece of information. We are going to know later in the postmortem review that at this point in time we were aware of this error, right? The fact that it was written in a log somewhere is not the entire story. We want to remember that at this time we were looking at this service, we were looking at this error condition. Then we have another engineer that is reporting having started a rollback. Now, the our pipeline tooling obviously tracks time stamps and start of rollback, but again, it's it's shared here for immediate visibility to everyone. And the other thing that they are doing, they share the link to the pipeline for reviewing the deployment state. Again, this is one of those measures that help keeping the traffic on the tech call low because you will not hear anyone asking about the deployment status, the rollback status. They're just going to click on that link and see for themselves. Here is a case where we share metrics and here we, we see the engineer reporting 100% recovery on their fleet, and they do this by sharing a metric that shows the steady state, shows when the anomaly started, shows when the anomaly crossed the alarming threshold, and then they're back to normal. And again, this is all data that is gonna be stored somewhere else, but we put it here for easy reference, especially in the post-event review phase. And finally, another, uh, it's a quite common type of observation here, we have an engineer that is just looking around while trying to figure out the root cause and spots a potential correlation with a metric. Now, this might be a red herring, but sharing it here for everyone to see might have just triggering someone else into looking at their service and figuring out that that correlation was effective, relevant, induced and related to the root cause of the event. OK, let's take a look at the support call. And so the support call owns customer communication. Uh, it is run by AWS Event Management. We're most frequently the ones that are communicating to you for operational issues that you see on the personal health dashboard, um, and the, the service help dashboard. Um, additionally, we proactively engage with account teams, right? So we want to engage the account teams. We want to make sure that they're prepared to respond and check in with customers early. We also place a heavy focus on recovery guidance and best practices wherever possible. For some events there's there's uh actions that customers can take that can get them out of pain earlier and we wanna provide that information as soon as we have it. Additionally, we're gonna use the, the data from the case trends, uh, the customer impact reports, and the sentiment as an additional additional data point to verify that our mitigations are working as we expect and that we understand the customer experience correctly. In order to provide detailed and real-time updates, it's super important that we're in contact with the tech call that Georgia was, was just discussing and so we need that real-time update, right, to be able to send communications to you, the customers, and so, um, we have a two-way communication that's occurring the entire time, uh, where we're providing information about the customer experience of the technical call and we're getting information back as to what's happening in real time, where we're at in mitigation, recovery efforts, etc. I wanna show you a, uh, uh, visual here of, of command center. And so command center is our all in one support tool. And this is a command center notice. And so we use command center notices to track and centralize and share information as it pertains to an event. Um, in this example what you see on the left hand side is you see some metadata about the incident and so this will include the, the service or services and, uh, that are impacted, um, you'll see the start time and the, on the bottom left you will see the customer contacts, uh, as, as we, we have them and then we're, and we're relating them. In the middle, uh, at the very bottom here you'll see the internal summary, and so this is where we're keeping that up to date so that our, our, our customer facing teams have access to that, that real time, uh, information as to what's happening. Additionally, you'll see at the top here there's a messaging tab, and that messaging tab is where communications that have been sent to customers are visible to those account teams as well. Finally we have the sentiments tab here at the top and that's where we're tracking and reporting on customer feedback. So communication is key on this on the support call so it's important to communicate to the right audience. There's two different audiences that are our main stakeholders here, and those are internal stakeholders and external customers. And so external customers get regular updates. They get regular recommendations, um, and we're communicating to them specificity about what's impacted, right, and clarity over time. In addition, we're providing live status updates to our field teams, um, the customer facing account teams, we wanna empower them and ensure that they have the latest information that they're not caught off guard, but ultimately we also wanna ingest customer inquiries, right? And so we can respond by creating FAQs, uh, or bolstering the FAQs that we already have. So let's take a deep dive into communication, so the external communication specifically. So communication is a matter of balance, um, and we bias towards speed, right? We wanna communicate as soon as we know that something is occurring. We wanna give that heads up. Secondarily though, we have accuracy. So accuracy we define here as, you know, we wanna target customers that are actually impacted. We want the communications to be relevant and to be actionable. We also want them to have as much detail as possible, right? So effective resources, etc. Finally we have depth of our communications and so this is where clarity is incremental and it comes with time. Our first communication often is, is, uh, very generic in the sense of we're investigating, you know, increased error rates and latencies, but it's because we wanna give you that communication first and we do bias towards speed. Um, but over time we, we, we update those communications and we provide details with specificity as to what exactly is happening, what we're seeing about it, what we're gonna be doing next, any workarounds that are available, and time that we think those, uh, those steps will take. OK, so here you see probably what's the most common quote from our tech call, and it's the call leader asking about uh a rollback. So I, I want to discuss now a bunch of measures that we take for mitigation. Remember, when we started the event management process and when we set the incident response, our one focus is mitigating the impact for our customers. We want the service to recover as soon as possible. Sometimes through side measures and without tackling the real root cause, we want our customers to go back to their normal operations so we gain some time for a deeper investigation. Now, what's probably our most common type of response is shifting away from the failure. Now. This is about removing the component that is failing when we can identify it, and this goes from simple things like imagine a large fleet of instances where we identify that only a bunch of them are responding with increased latency, and we have an end check that automatically takes them out of the fleet. Those instances remain broken, so we are not really resolving the root cause, but the customer impact just goes away. Your customers recover and you get some more time to conduct a deeper investigation. Or also more complex cases, we might be investigating again an increase in latency in a service, and our metrics showed that that increase in latency is happening only in a specific availability zone. So the first measure here, remove that availability zone from service, let traffic hit only the remaining two, and allow customers to recover. Once they are recovered, you have more time and just under less, way less pressure to resolve. Um, similarly, rollbacks. So when we start the incident management, the first thing we look at is if there is any deployment that is ongoing or that started at around the time the event itself started. We do not only look at the component that is affected, we sometimes look at all of AWS. And again, here, if we suspect some correlation, we don't waste time to confirm that correlation. We just bring everything back to the previous known and stable state. to mitigate again as soon as we can. And third, we are in the cloud, and there is a large category of events that result in increased resource usage. So you might, because of your increased latency, might be increased CPU usage. While you work to understand why that fleet, that type of request is using today more CPU power than it was using yesterday, you can just scale the fleet up and provide more resources so that the API latency goes back to normal, and at that point you can investigate. And then there are a couple of measures that are part of the toolkit. We generally prefer not to take them. The first one is the turn it off and on again kind of thing. So um software carries state. There are caches. There are buffers, and sometimes restarting a component can help with recovery. It's quite a tricky kind of measure to take because sometimes with the restart you're also going to lose the state that led to the error condition, so. Whenever we have to do this, we try to isolate some nodes on the side that we can use for the investigation and we restart everything else. And finally, Um, changes are challenging, especially when not completely tested, but Some events are clearly solved by a small change in configuration or by rolling out a new software version that we were testing to improve that specific component. When it's really our last option, that's what we will do. And if we are. If we have extreme confidence that that change in configuration is going to help the problem, if we have extreme confidence that there is a new software version that is stable, can be trusted, and solves the issue, we will roll forward. So Once you have mitigated the issue, there is the um deeper resolution phase. And one of the first questions you ask is um about the risk of recurrence, right? So, Before disengaging, there are a few things that we want to do, and the first one. Is when Anthony mentioned multiple times how we also use data coming from our customers to detect events and we use data coming from our customers throughout the event process to improve our communication and overall to make sure that our internal metrics are telling us the right story. Uh, we do exactly the same with mitigation. Once we are confident that we have mitigated the issue and our metrics are back in the clear, we just spend that additional 5, 10 minutes to confirm with some customers that they are seeing the same. Hopefully I will, but if they want, they can just go back to incident management mode. Um, second one is the risk of recurrence. So if you made a small configuration change or you changed something at runtime, there might be a pipeline, um, a recurring process or a schedule change that is gonna undo it. You want to make sure that whatever temporary patch you have implemented, it's there to stay. And additionally, this is, we do it at the AWS, but I guess everyone else does the two components. Um, software components, SDKs, library are used across services. So if we suspect a specific library or a specific piece of code to be the culprit, we want to go check other, other services that are using the same, and we want to validate that they are not just on the brink of facing that same issue as well. And finally, this is the point where we start forming root cause hypothesis. So they don't have to be detailed, something that you do in 1520 minutes, but we want to have an idea on what services are, where we want to conduct a deeper dive in the following hours to days. So Ready to close the incident bridge, the attack call in the case of EWS just a small list of things that we always make sure we do before doing, before closing it. So, we obviously want to validate that all of the short term fixes critical to preventing recurrence are complete. Um, we do not consider the incident resolved until we have this done and validated. And second, uh, we talked about the tech tickets and, and we make sure that we are preserving all of the data and logs that we might need later. And this ranges from logs that might be flushed out in systems every 24 hours to screenshots that live in the laptop of a developer that was involved in the incident resolution. Here is where we check that everything that we might need later is stored essentially forever in the tech ticket. Um, this is also when we start assigning the postmortems. So, um, every team where we see an opportunity for improvement, maybe we page a team and their uncle joined 5 minutes late, or there was another team that wasn't really sure on their, on their automation to remove traffic from our availability zone, we, you will have. A larger amount of engineers taking notes and all of this stuff during the event uh resolution phase, and at the end we just come together and assign those postmortems to everyone that was involved and where we see opportunities for improvement. Um, it's not a blaming process. This is just where we look at how we responded, we start being self-critical and just ask teams to deeper investigate if they could have done anything better. And finally, well, quite obvious, but we want to make sure that there is a common understanding of what are the next steps. We are gonna move out of the 24/7 operation mode. We are going to disengage from the incident. Uh, if someone was paged overnight, go back to bed, but there are 2 or 3 days where work is still going to be intense. We want to make sure that there is coordination and a plan for what everyone needs to do. OK, let's take a look at phase 2, reflecting and planning. So, after an incident, there's gonna be an author or someone responsible for deep diving and performing that post-incident analysis, right? Uh, and it's critical to ensure that you're examining it through an effective lens. So how can you do that? Well, first and foremost, you need to create a safe space, so that you can understand the context of decisions that were made both before, when the, when the software was designed, during an incident, but also, uh, after the incident, in hindsight. Also assume that decisions were made with the best intent, right? Uh, everyone's doing their best, um, and you want again, you want it to be a safe space so that folks feel comfortable challenging, uh, they feel comfortable criticizing but constructively. Um, Sometimes it's also important to do multiple retrospectives, so don't assume that that one can cover it all, right? Uh, it's important to deep dive any lesson or any failure that that you want to prevent recurrence of, um, and so it's not uncommon for that to happen with us, right, if there's, uh, uh, a multi-service event and there's different learnings that we wanna take away from different teams and, and share broadly, we'll do so, and each team will be responsible for kind of deep diving that, um, every, every failure, every, everything that we want to prevent recurrence of. OK, so at AWS we call them COEs, uh, so they, that stands for correction of errors, um, and this is our post incident analysis, um, mechanism that we have. So what is a COE? So a COE is an impact summary, and that's what it starts with, and it, it goes over specifically the timeline of the incident, um, what exactly was the customer experience? Uh, it should, it should stand on its own, be multiple paragraphs generally in a narrative format, and again talk about the customer experience and the life cycle of the event. You should hit on all the key milestones of an incident. It also goes into the root causes. Uh, we do that through the five whys, right? The five whys allow you to really deep dive and get to the true root cause, and we'll go over that in a little bit more detail shortly, um, but it's super critical that you're, you're actually getting to the true root causes in order to ensure that you're taking the right lessons and, uh, implementing the right actions. So what comes next are those learnings, right? These are the lessons they should directly come from and flow from those five whys and those root causes. From those lessons, each of them will generally have an action item, right? Uh, the lessons are usually things that we didn't know beforehand that we want to ensure that we're capturing for posterity and, and taking actions to either educate, um, or, or resolve their technical issue. So there are a couple of things we are really big on when doing COEs. And the first one is, we try to be extremely self-critical about detection. Um, we just ask ourselves whether we detected the event fast enough and we are happy with our response time. Um, the chances that the answer to this question is no. Um, until detection gets really sort of immediate and you have an automated mitigation, there is always gonna be an opportunity for improvement. So, if the answer is no, and we believe that there are opportunities for detecting faster, we will create action items for this just as we do for the root cause of the event itself. Additionally, um, now we mentioned a few times that we use our internal metrics, we use customers to confirm that those metrics are telling us the right story, but we also want to validate this. We want to make sure that throughout the event we had the right observability. So our, we want to validate that our metrics were giving us the real status and that were reflective of the customer experience. And finally, this is when we started zooming out a bit, so we do not focus on this very specific event, this error condition, but look a bit broadly and try to figure out if we are confident with our ability of detecting similar event patterns and similar incidents in the future. The second part where we are big on is dependencies. Now, um, whether them being either internal or external, so using a piece of software from another team or from a third party, dependencies are really useful as they help, uh, avoiding repeated work. You might just use a component built by someone else and focus on what's the core of your software component. When an event is caused by a dependency, it might be really tempting to just tag it as such and completely disengage from the post-incident review. We do not really allow our teams to do so, and we want them to still analyze what happened. The first question, and probably the most obvious, is we validate that the dependency promise that worked as promised, so every dependency is going to come with either an availability problem or RPO RTO or those sorts of metrics. We want to make sure that they delivered on what was sold to us internally sold. The second one is validating the implementation. So a dependency is not the world story. How you implement it in your software might affect whether an outage on that dependency impacts customers or not. Then we look at the failure mode and check if it's something we should have expected, something we should have planned for, and didn't, or whether it's something completely new that there was no way to plan for and we need to shift to immediate contingency. And overall we use those, we use those three questions to find opportunities for reducing the blast radius. Now some dependencies are just not critical, and think about a web page that is created through uh involving multiple components in the API. You might just gracefully degrade the page experience and not load the component from that dependence in case it's not available. It is. You know, a degraded experience as the words say, but it's going to be much better than timing out on the entire page. Or similarly, you might be able to cache results, so be able to still serve a request that you have already previously served. And again, it's not going to be 100% availability, but it's certainly better than 0% availability. OK. That's a wrong section. So, Anthony mentioned the five whys and the five whys are a really um Appealing topic, and I think they are one of the most misunderstood concepts in the industry. So they are often explained as a sequence of questions where you identify one root cause and then you ask yourself what caused the root cause and then what caused the second layer and so on. And the first thing is you shouldn't really stop at 5.5 is a number to give an idea that it's more than 2 and less than 100. But you should really keep going until you find something that is meaningful, until you find a measure an opportunity for improvement that if taken helps with a large range of root causes. And the second one, the way they are often explained is as a chain and This is really not the best way to think about this, looking at them as a tree and acknowledging that events are rarely caused by a single root caused by a single trigger, but they are often a contribution of multiple factors is key, so. To make this more tangible, let's look at an example. So we started from the issue that is API calls failing, probably the most common failure. API third party internal was rolling out. And We tracked down those errors to one host in the fleet that was returning 500 errors. So we ask the next why. It's why was disaster turning 500 errors, and we find some IO issue errors in its logs. And then we look into those IO errors and find a problem with the underlying EBS volume. We look at the underlying EBS volume and find a problem with a portion of an availability zone. Now this is really effective. You are going really deep and they found. A 4th layer root cause that explains the event. Um, but it's only covering one of the various potential root causes. So, let's look at a better option. Uh, we start with the same question, but not one thing, and this is gonna become apparent, um, soon. We added some data. So now we are specifically talking about 3% of API errors over a time of 45 minutes. Um, the first question, the first answer is the same as before. So we have 1 out of 100 that is returned 500 errors. And we go down the same branch as before. It is gonna point out to a temporary BS failure in the variability zone. In parallel though, we go look at our health checks because the reality is that if health checks were working and removed that failing ghost from the fleet, our service would have kept working regardless of the underlying disruption to the storage layer and figure out that there was. A bug, uh, um, gap in our service templates that was not in that was incorrectly implementing the TED check. Then Find the problem with those 45 minutes, right? 45 minutes of impact due to a single failure are quite significant. Start looking into that and we find out that the engineer was not engaged for the 1st 30. And again, this is An immediately visible opportunity for improving the detection time, right? If this detection was 15 minutes, the entire event would have been 20 minutes, even without changing anything on the resolution. And finally, I was talking about numbers. I don't know how many of you picked it up, but we are saying that a 1% loss of capacity caused a 3% error rate, and there is clearly a disconnection there. Um, we go look into that and find a problem with the load balancing algorithm. Now, this connection is quite a painful algorithm to use because when a service is erroring out, errors are generally served faster than actual responses. So a single failing node attracts more traffic than it does when it's effectively working. And you see how by doing this, We have not arrived to a single action that in the previous case was basically wait for AWS to solve the EBS problem, but for nearly completely independent root causes and tackling 12 or 3 of them is going to build a layer of resilience across towards that failure condition, so. There are a couple of these, when you think about the five whys, do not imagine a chain, but rather think about a tree. It is gonna bring a range of different issues and there's a range of different uh preventive actions. Um, so, when we are done with this and with the internal part, we start looking at the customer facing root cause analysis and not go too deep into this one. But one thing we try to do and we recommend everyone does is when you write a root cause analysis for your customers, start from their point of view and not from your internal components. So we try to open a root cause analysis by explaining the impact to customers and you know, when we, we need to go deeper into the sequence of events, we try to focus on the functions that customers are familiar with and not with our internal services that provide those functions. And this is overall about removing unnecessary complexities, right? Spending one pager to describe an internal service where the only relevant thing you have to say is that 20% of us in that service were misbehaving and were not detected. So it's just going to drift attention and not be productive. And while writing the root cause analysis, there is a really important balance to take that is between quality and speed. Um Anthony mentioned that how in event communications, speed is our primary metric. We want to be out with a notification that something is wrong as soon as we can, and through iteration, we are gonna explain what that something is. We're gonna share details on the service, on the region, on the type of failure. Now, RCAs are a document that is delivered once, so we do not get this opportunity to iterate, but we are also aware that our customers are waiting for it. They are waiting for our SCA to Understand what happened, but more importantly, to read what we are doing about it and by when we are planning to do such things. Um One thing that might be not immediately visible is that RCAs are not only a way to explain a technical failure, but are also a way to regain the trust of your customers. So if you're writing one, it's because you failed, and customers are gonna inspect you. They are gonna inspect your response. They are gonna inspect your long-term plans, and they're gonna use that to decide whether to keep using your service or not. So seeing RCAs as an opportunity to earn customer trust or. To regain their confidence is quite key to delivering high quality documents. OK. So After this part is done and we have an eye level. Prompt to customers, we start looking into the detail of our action items now. We categorize action items based on how long it takes to implement them and how stable they are. The first type that really doesn't happen at this stage, the first type is the short term ones that we discussed, uh, in terms of things that happened before we close the incident bridge. So they happen in hours. We work 24/7 until they are um implemented, and they are focused on preventing the immediate recurrence. Now, short term action items. Are effective but might not be particularly sustainable. Like you can imagine this as restarting a software component once a day to clear the buffers and caches or to over engaging engineers every time a matrix starts spiking spiking in the wrong direction. It's fine to keep them for a few days. It's not fine to keep them for forever. Um, the second category is the midterm ones. This is when you start building a self-sustaining solution that allows your teams to go back to the priorities they were working on before that. And they are generally implemented in days to weeks, and more rarely months, and those tend to be the promise that we put in the customer facing root cause analysis. Then we have a 3rd type that is really long-term action items, and this is when we think about systemic change or when we start reinventing. And this type of actions might be building a completely new service that solves a problem better than the previous version or or that solves a problem that no service is solving. Um, this is a real quote. Quite fun because it shows how while building these action items, you should not just focus on incremental improvement. And once again, it's, it's a matter of trade-offs and balances. There are 4 forces that are really relevant when looking at long-term plans and preventive actions. So the first one is. The trade-off between incremental change versus innovation. So incremental change, you can see it as something that is rolled out slowly. And that is always retrocompatible and doesn't require any action from the customers, from the users of API, or from downstream dependencies. It's easy to implement, but it's slow and easy to use for the end customers. Innovation is building new solutions. New solutions are. Um Categorize two things. So the first one is Uh, they are new, so it's gonna be a new service. Customs are gonna have to move to, and this doesn't really discount you from fixing the previous version because there is gonna be a ramp up out of a ramp down from the old one and a ramp up to the new one that is gonna take quite a significant amount of time. And then they might be completely incompatible. Might be a new service that behaves in a completely different way that doesn't have the full functionality of the previous one. And the second one is related to timing. So, after an event, it's really common to start focusing on what seem to be perfect solutions. So, you're gonna completely rebuild a component or you're gonna deprecate an API or stop doing something. Um, the problem is It's really key to have realistic, realistic completion dates. Um, when internal and external customers have been impacted by a failure on independency, they are going to expect some significant actions and some significant prevention happening in weeks to months maximum. So. Here is when you take a decision between promising a perfect solution, maybe 2 years out, that doesn't really solve the immediate problem or promising something in the middle that you can deliver quicker and after you have done that, then you can go to incredible. Um, OK, so at this stage you have your action items defined, promised to the customers, and you have internal resourcing and planning to carry them on. It's really common since all of this phase happened in days after the event. The RCA needs to be out in 7 days maximum, and by that point you are making a promise. And then you go into an implementation phase that might last 345 weeks. And as you go, as you do that, you might find better opportunities. So, you might decide that the plan you promised to your customers is not really the best. We are generally fine with changing those plans, as long as we don't change the goal and the promise. So if you have a different way to implement a similar solution that prevents the same failure condition, we will do that. Um, while doing this, it's extremely important to stick to the, um, estimated completion dates that you have promised to your customers. Now, They are really not set in stone. It's fine to push them out by 1 or 2 days if you need them, if you need them to validate a solution, but it's important to not push them out by months. So I was trying to come up with a number before this talk to say being 20% slower is fine. Being 2,000% slower is maybe not. I didn't find this number, but. I have the feeling it's going to be quite low. And finally, uh, this is drilled into our mindset, but, uh, preventive action is not completed the moment you implement it. It's completed the moment you test it and validate that the same underlying conditions do not lead to the same failure. Again, this is key in communication, but also in terms of thinking. Um, the job doesn't stop at implementation, stops after you have confirmed that what's implemented was right and works at scale. OK, let's look at phase 3. So phase 3 is learning and scaling. So, the most important COEs get to phase 3. Here's a quote here and it's, it's worth acknowledging that as privileged male technical folks, Malcolm X was probably not thinking of us when writing these words, uh, but their words are inspirational nonetheless, right? And so it's, it's super important, uh, to, to, to take every opportunity to learn a lesson and improve your, your performance the next time and it's the mental model that we have here at AWS and it's part of our operational excellence, uh, and, and it's, it's critical in the, the process here for COES. OK, so let's look at learning. So there's a couple different phases of learning, uh, so we review all COEs within a team, uh, every, every service or or author that writes the COE, they're gonna review that with their individual team. Additionally, they'll review that with their wider organization. Um, but we also have the AWS operational metrics meetings, uh, and so these are meetings that occur on Wednesdays. Uh, everyone is, is involved. Everyone is invited in all of AWS that includes product managers, that includes, uh, engineers from, from leaders, everyone, um, and it's, it's super important that they're. All engaged as well and this is part of our culture that allows us to scale these lessons across not only one organization or one team but all of AWS and so it's super important uh it's it's a critical critical mechanism to ensure that these are learned once and not multiple times. Um, the other key here is looking for patterns, right? And looking for patterns and ensuring that you're leveraging solutions across the organization. Again, a safe space is super important, right? And so there's some ground rules there. Uh, open discussion is absolutely welcome, but everyone should feel comfortable speaking up. Everyone should feel comfortable, um, uh, challenging action items, asking whether they're the right things, you know, uh, and, and challenging the five whys and how we got to the solutions that we did. You'd be surprised, but, uh, for a meeting with thousands of folks, it is, it is an incredible amount of value that we get out of that meeting. OK, so let's look at the scaling aspect of it. So I kind of alluded to it there, but scaling goes through 33 phases, right? So the first one is team education. And so in order of, of effective, of increased effectiveness, right? So the first one is education. Um, they're the best intention. It's not necessarily mechanistic, and they don't necessarily scale to other teams and other organizations. Then you have a distributed change and a distributed change is one where everyone has to go do something and so it can be expensive, um, but ultimately it can it can scale. And then finally we have a centralized change. And so a centralized change is where you go and you make a centralized change and so everyone gets that by default, right? They consume that. These are changes to the SDKs, default behaviors, etc. So one example of a team education is not only just reviewing the COEs within your team and within your organization but tenants and so tenants are, uh, are core, you know, foundation foundational principles that we use to help guide us, um, and so here's an example of some of our operational tenants. I'm not gonna talk about all of them, but the first one obviously failures must not impact multiple regions, um, it's certainly a lesson, uh, uh, that everyone at AWS is, is very well aware of, um. And additionally there's detecting failures before your customers do. And so this is an obvious one where in that operational metrics meeting if uh if we determine that customers were the root cause or the catalyst rather of identifying an issue, um, it's, it's a, it's an easy COE to write in the sense that it was a failure and we have to go figure out why there's improvements to monitoring, there's improvements to metrics and alarming that we need to go, uh, take. And so for an example of of distributed change, distributed change here is trusted advisor. And so trusted advisor is a service that we built to scale our learnings directly with customers, right? And so it monitors your infrastructure, categorizes recommendations based on severity. Um, on the left here you can see, uh, an example of four critical recommendations. I think Giorgio, you might have a couple unread notifications here in the AWS Health councille on the top right there, um. Well, they're not only categorized by priority, but they're also related to performance, cost optimization, etc. The key here is that we have identified lessons, right, and we wanna scale those lessons to customers and that's the, the distributed change model here but ultimately customers have to prioritize them, um, and so it's not necessarily the most effective, uh, but it's certainly, uh, well worth doing. Thank you. And just to clarify, that's a non-production account. Um, I was looking for example of bad trust advisor pages and so to go dig in an account that I rarely use. Um, so the third type, the centralized change and invention that we have mentioned multiple times. Now, you might not know that, but most of the services you use today in AWS are coming from our experience of a world without them, and they are solutions we build to actual problems we are facing. A really common one, in case you were managing DNS 15 years ago, or, fun fact, um, Route 53 is turning 15 in like 3 days. Um, you knew how hard it was not only for, for building a scalable and resilient data plan, but also for managing control at an organizational scale. So building the right policies for only the right individuals being able to modify a certain DNS z on or some records was just not easy. And that's why we built Amazon Road 53. Similarly, um, load balancers used to be physical. They used to be physical appliances that you were packing, that you were managing and updating, and quite painful overall. And that's one of the reasons why we built elastic load balancer. We built it for us in the first place and to solve a problem we were having. We wanted. A really scalable virtual balancer that could be provisioned on demand. Uh, we just wanted to stop wheeling in servers every time we needed to launch a new server, service, and that's how it came to be. And similarly, uh, I don't know, but if you were to manage a certificate rotation in a large organization, you'll know that doing this through spreadsheets and meetings, it's not the best possible idea. And AWS certificate manager today makes it so simple you basically forget about certificates and you can also afford to rotate them way more often than we were used to before. Um There is a little bit of a case study that I want to show and now all of those things come together and was a case study and now we sometimes get the timing wrong. But you should have heard about identity and access management, AWSIAM. So it's a globally consistent service that you use uh in the build phase or while deploying a service to define permissions, access policies, and define what users and roles can do. Um, next to AM there is a security token service, um, AKA AWSSTS. So this service is regional instead, it's stateless, and it is at run time to generate the temporary credentials that they are then are used to, um, connect and make requests with WS services. Now this service is sort of in the critical part because it's not in the middle of every request, but temporary credentials are really short-lived. So you need this SDS to be extremely resilient. Um As many things STS was at the beginning only in a single region. So when we started launching multiple regions across the globe for a few years, you would have had to make a request to USS to get credentials and then be allowed to use services in the regions you had. By 2015. We figured out this was a problem and launched the regional ST assistances. Uh, this allowed customers to isolate their workloads and make sure that a failure in any in any region was not spreading to others. This was when we changed the best practice. So this is when we start talking about team education. We immediately documented the fact that using regional was better and a more resilient option, um, but left it pretty much at there. And by 2022, we figured out the traffic was still significant on the legacy endpoint and that there was quite a significant availability risk for our customers. So we changed the defaults and we started with active campaigns for customers to move off. So this is when you get a personal dashboard notification that has to stop using a certain thing or implement a change by a date. Um, unfortunately, by 2024, traffic on the global endpoint was still really substantial, and this is when we figured out that, uh, giving the right guidance or the distributed kind of change were not effective, and we had to take a measure that would allow us to solve this quickly and for everyone and without customer action. That's why in April this year, we implemented a change that essentially uh naming here gets really confusing because the global endpoint is now served for region that's enabled by default by the region itself. So customers don't change anything, use the same endpoint as before, but the request instead of getting to USS one gets to the same region and removes this dependency. And to, to give you an idea. On how this worked. So here we have a graph you see goes from early February to mid-April. Um, it's normalized. The 100% on the Y is the starting point where we consider the starting point 100% of affected accounts. And this is how traffic changes on the global endpoint as we roll out the change. So this shows a deployment that starts slowly and tests just to make sure we are deploying the right thing. And then towards the end you see the deployment going. At increasing speed and beating our regions, and traffic, cross-region traffic drops to nearly zero. OK, so there's a couple of takeaways that we want you to, uh, remember leaving this talk. So first and foremost engage early engage often ensure that leaders are engaged, uh, everyone wants to be involved and they should welcome engagement and make sure that you have the right folks at the right time. The second one is on fast communications during the event. Make sure you send out a notification that tells your customers that something is potentially going wrong and think about iterating on depth later. Mitigate first and root cause later so we're all engineers. We want to deep dive and figure out what the root cause is, right? But during an incident is not the time to figure out the root cause. mitigate customer impact, uh, and root cause has its time and its place afterwards. Finally, incidents are really powerful lessons. The problem is that they are extremely expensive and come with customer impact. You want to make sure that you learn once as an organization, so don't limit the learnings to the team that was affected or the team that was the cause of the incident, but make sure that whatever they learn spread across all of the engineers in the team. And then ensure that when you reflect, you reflect with empathy, create a safe space where folks are, are welcome to ask questions and feel confident in doing so. And finally, whenever you need a really simple set of rules that are easy to remember, that are easy to teach to new wires, and that can be used as a tiebreaker when there are hard decisions to take, just define tenets. You have seen ours something similar can be done, and tenants are supposed to be really short sentences that Withstand the test of time and can help driving decisions. You will find them extremely useful when two teams that maybe are not closely working together need to collaborate to launch a service. That set of 5 tenets, 10 tenets is going to help them just building a shared roadmap and shared outcomes that are relevant for the company. All right, well, Georgia, Georgio and I both wanna thank you all for coming. Uh, feel free to take a picture of this slide. We'll be available outside the room, uh, after the, the, uh, presentation here. We're obviously passionate about this topic and so we're happy to, to discuss. Um, please complete the session in the sorry, the session survey in the mobile app. Uh, we love your feedback and we want to improve next time.