---
video_id: e09mW8e8G-s
video_url: https://www.youtube.com/watch?v=e09mW8e8G-s
is_generated: False
is_translatable: True
summary: |
  Scott Perry and Sada Kashyap used a live code talk to show how the Neuron Kernel Interface (Nikki) unlocks the full performance of AWS Trainium and Inferentia chips by letting engineers write hardware-aware kernels instead of relying solely on the general-purpose compiler. After a quick tour of the hardware—Neuron cores with tensor, vector, scalar, and general-purpose engines; on-chip SRAM and HBM; and the roofline model for understanding memory- versus compute-bound workloads—they positioned Nikki as a Python DSL (akin to Triton) integrated with PyTorch, JAX, and NumPy. The goal is to push arithmetic intensity to the compute-bound side by pipelining, minimizing data movement, maximizing throughput, and overlapping collectives.
  The baseline benchmark used Hugging Face’s Qwen-3 model with the default PyTorch attention on a trn1n instance: ~0.35 prompts per second and ~3 seconds per request. Sada inspected the model’s eager attention implementation and swapped it for a Nikki flash-attention kernel via a small adapter that reshaped QKV tensors into the format the kernel expects, invoked the Nikki API, then reshaped outputs back to the model’s layout. After recompiling with the Neuron compiler, throughput jumped to ~2.99 prompts per second and latency fell to roughly a quarter-second—a 6–8x gain—while the Neuron profiler confirmed the acceleration.
  The presenters dissected why the Nikki kernel is faster: it explicitly manages DMAs, packet sizes, and tiling to keep data close to compute; it uses efficient memory layouts; and it fuses operations and pipelines work across the Neuron engines to keep them busy in parallel. Because Nikki lives near the hardware, teams can surgically target bottlenecks (starting with attention but extending to MLPs, projections, etc.) and iterate like “onion peeling” as each optimization shifts the next bottleneck.
  They announced the open-source Nikki Library, a curated set of pre-optimized kernels maintained by the Annapurna team. Initial coverage focuses on dense transformer components, with mixture-of-experts and other kernels coming soon. Developers can drop these kernels into existing models with only minor reshaping code, gaining production-grade performance without writing kernels from scratch. The team recommended profiling first to find hotspots, then integrating Nikki selectively to maximize impact.
  Scott closed by reminding attendees that accelerators deliver their promise only when the software stack is tuned for the hardware. Nikki gives performance engineers the missing lever to go beyond high-level graph compilation and explicitly control memory movement and compute scheduling on Trainium/Inferentia. With open-source kernels, detailed profiling, and modest code changes, they showed that dramatic speedups on real models are achievable today.
keywords: Nikki, Neuron SDK, Trainium, flash attention, kernel optimization
---

everybody. Welcome to this afternoon's session. This is AIM 414. How to optimize your LLM with Nikki. My name's Scott Perry. I'm a solutions architect on the Annapurna Labs team at AWS where we work directly with with customers on our AIML accelerators, uh, ranium and inferentia. And joining me today, my colleague. Hey everyone, this is Sadda. I'm also a solutions architect at AWS Trainium. Uh, I'm primarily focused on AML performance on neuron, and we both are super excited to have you here today for this session. Yeah, thanks for coming out. So this is a code talk. So we're going to spend most of the session digging into, uh, you know, a terminal editor and code editor running some scripts and, and doing all of this live. So hopefully it goes well. Um, but before we do that, we do have to just go through a few background slides just to give everybody an idea of what Neuron and Nikki is all about. So bear with us just for a few slides, and then we'll get right into it. So to give you some context, you know, for many years, customers have been doing machine learning workloads on AWS and some of the early feedback that we got from customers is that they wanted some choice, and they wanted to see improved price performance for running their ML workloads on AWS. So back in about 2019, we actually invested in, in designing a purpose-built machine learning accelerator, um, that we would offer via EC2 instances in AWS. And that first generation was called AWS inferentia. So this is inferentia 1, if you're used to, uh, the EC2 instances. And this was a smaller accelerator designed for the deep learning models of the time. This was kind of like Bert and YOLO and that sort of thing. Since then, we've actually released the second generation, Inferentia 2, and maybe more interestingly, we've also released 3 versions of the AWS ranium chip with the 3rd generation launching just yesterday. So ADWSranium is actually a more powerful chip capable of being used for distributed training as well as inference. So at the heart of the ranium and, and in accelerator chips are the neuron cores. So in a given instance, you might have multiple chips, and within each chip, you can actually have multiple neuron cores. So today we're going to be working with a TRN 2 instance. It actually has 8 physical cores within the chip. We're going to be using one of them for the examples here today, but Within that core, this is a high-level schematic of what's actually involved. So what's really interesting about this architecture is, as I mentioned, it's purpose-built for machine learning, uh, workloads. So if you see, we have 4 specific compute engines within the neuron core, starting with the tensor engine. this is focused on things like matrix multiplications and transposes. We have the vector engine that you can use. Um, for max pool, average pool type layers, there's the scalar engine for things like activations, and we also have a general-purpose compute engine with general-purpose cores that can be reprogrammed for maybe layers that, that aren't covered in the other three engines. So here we have 4 compute engines that you can actually use in parallel on this chip to maximize compute utilization. Also worth noting here is we have a couple on-chip SRAMs. You see the S buff and the PSOM. So these are obviously on chip, so they're very close to the compute offering high bandwidth, but maybe lower capacity. We have a DMA or a set of DMA engines for moving data, and we also have high bandwidth memory as part of the device. And because these are offered via EC2, we also have access to the host memory. In this case, it would be DRAM associated with the CPU. So I'll call it, you know, there's 3 levels of the memory hierarchy that we care about, right? There's that on-chip SRAM. This is the lower capacity, but very high bandwidth memory that we would love all of our data to live on all the time if it could. We also at the bottom of the, the tiers have the host memory, which is high capacity but lower bandwidth. And in between, we have, you know, a bit of a compromise, which is the HPM moderate capacity and, and pretty good bandwidth. And one of the major struggles when you're trying to optimize your machine learning workloads is, you know, how do you place data, get it as close to the compute exactly when you need it, and how do you juggle moving between the tiers otherwise. And when we talk about optimizing machine learning workloads, one way of kind of representing a given workload is the roofline model. Has anybody seen a roofline model before? OK, just take a, a quick refresher then. So as, as part of the roofline model, basically for any given algorithm, any given machine learning, say model, there's going to be an arithmetic intensity, uh, intrinsic to that, that model, like an algorithmic arithmetic intensity. So you can basically, whatever the mathematical operations are for that given model, it's gonna consume and there's going to be a certain amount of operations that you have to do per byte of memory that's read. But for a given accelerator chip, you're going to have a finite memory bandwidth, and you're going to have a finite, you know, maximum compute throughput that you can achieve. And if it turns out that your achieved arithmetic intensity for a given workload falls on the left-hand side, meaning that there's Low ops per byte, you're going to be memory bound. And this, we don't really want to be there because if we're in that case, it means that we're limited by the, how fast we can read from memory for the workload that we're actually operating. What we would like to do is try to push things to the right-hand side of the graph and be compute-bound, meaning that we're doing multiple, many, many ops per byte of memory red so we can take full advantage of the compute accelerators. Um, so the difference between algorithmic kind of theoretical, uh, arithmetic intensity and achieved is basically, you know, the implementation, right? So how can we improve performance and, and kind of shift the. The graph to the right So we can do things like pipelining operations. So if you have a, a major workload, sometimes you can chunk it up into smaller bits and actually run multiple parts of the workload at any given time and keep the engines a little busier than they would be otherwise. Um, there's things like minimizing data movement. So if you're doing a lot of DMAs, moving a lot of data unnecessarily at the wrong time, that's also going to reduce your performance. So we want to try to avoid that. We want to maximize data throughput. So in some cases, you might be sending small messages or moving small bits of data around. Is there some way maybe we can coalesce that into larger chunks and improve data throughput? And if we're in a distributed setting, sorry, um, we might have multiple ranium chips that we need to communicate across sometimes. So say synchronizing gradients during training, uh, and when that happens, it would be nice if we could overlap the collectives with some type of computer data movement so we're not just sitting there waiting for the, the collectives to take place. OK, so, you know, this is why we, and maybe how we would try to improve performance, but what does it mean for tranium and, and neuron? Like how do we get there? So to work with the tranium and the inferential chips, we have the neuron SDK and this is the software development kit that includes all of the various layers of the stack. So we have a compiler, um, runtime, driver, and user tools that allow customers to work with training and differentia. And, you know, we, we typically look at, um, our, our customers as 3 personas. We have ML developers, data scientists, and performance engineers, and lots of folks kind of fall between the lines. But for today, we're focused more on the performance engineer persona. And within the neuron SDK we have a, a certain set of tools that are going to be applicable to performance engineers. Uh, so the one that we want to focus the most on today is called the neuron kernel Interface, NICI. Has anybody worked with Nikki before? Heard about it? OK. Awesome. So what is Nikki? So Nikki is essentially a Python-based domain-specific language for writing kernels for training and inferentia. So before we had Nikki. Basically, you could write your machine learning model maybe in Pytorch and jacks, run it through the neuron compiler, which takes the compute graphs from your model, optimizes them to run on tranium or inferentia, and then you could run your model that way. But you're putting a lot of faith in that the general purpose kind of neuron compiler was going to perfectly optimize your model. And as we see with our accelerators and other accelerators, oftentimes customers need a lower level access. They want to be able to, to really fine tune the model to take full advantage of the underlying hardware. And that's what Nikki offers you. So if you're familiar with OpenAI Triton, uh, this is not quite Triton, it's, it's a little bit different, but this is our kind of flavor of, of writing low-level kernels for our accelerators. So it integrates directly with PyTorch Jacks and NumPy. It directly emits neuron ISA instructions, so the actual hardware instructions that exist on the chips get emitted from this, this platform. And we have a couple of name spaces within NICI. We have the Niki Lang, which is higher-level constructs to help you out, helper functions, that kind of thing. But we also have direct neuron ISA instructions as part of the Nikki Isa name space. And on the left-hand side, you do see a kernel. We're going to actually hop into that in a minute here and go through line by line and show you what it's like to, to build a basic kernel, uh, before we get into the actual benchmarking and whatnot. OK, so for today's session, what are we looking to achieve here in record time? We're going to start off, we want to start by benchmarking and profiling an LLM. In this case, it's going to be Queen 3. so we're going to run that using hugging face transformers and the neuron SDK. So it's going to compile the, the basic Quin implementation to run on tranium as is, using naive attention. That's just going to be our baseline. So we'll look at the performance and look at a basic profile. Then Sadaf's going to walk us through an actual NICKI kernel implementation of attention. He's going to add that into the Quin 3 model that we had previously run, and we're going to redo the benchmarking and the profiling to see what the, the performance gains hopefully are at the end of the session. Uh, and then we'll, we'll obviously give you guys some time for questions at the end. And the technology stack we're using specifically here today. So we're using AWS Tranium 2, the 2nd generation of our chip. On a smaller instance, we're going to be using one of the neuron cores out of 8 that exist on the chip today, just to kind of keep things a little bit more consumable. Uh, we're going to be using the neuron STK to both compile and run our models. In our case today, we're using Quinn 30.6B, an embedding model, and this is available via the Hugging Face Transformers library. OK, so with that, why don't we just take a quick look at that example kernel that I put on the slide in a code editor. We'll go through it, and then we'll get right into the benchmarking and show you that vanilla implementation. OK. Awesome. OK, so here we can see a very basic kernel. In this case, it's called Nikki Tensor Add kernel. This is just a basic kernel that takes two similarly shaped tensors, and it does element-wise addition of them. I, the first thing I want you to notice here is this just, it's just defined as a basic Python function, right? So, the only thing that's really special about this function is that we decorate it with Niki.jt, just to let the neuron compiler know that this is basically intended to be a neuro, a NIKI kernel that we want to compile. And there's a specific constraint around the, the, the NICI kernels today. So, the inputs and the outputs, you can see the A input, B input, these are the two tensors that will be added, as well as the output. These need to exist on HBM, right? So we talked about the three kind of tiers of memory that we're going to be focused on. Um, the input and outputs actually have to exist on the, on device memory, but not on the on-chip SRAM. And the framework is going to handle that for, for us. So we'll be using Pytorch today. So if we just run through kind of line by line what this is doing, we pass in two inputs that we want to add together. There's a basic check to make sure they're the same shape, otherwise, the element-wise addition won't work. And here we're going to see there's a similar flow that exists in all NICI kernels. So the first step is we need to allocate some space on the on-chip S ramps. So here we're allocating some space for both the A input tensor and the B input tensor using this S buff.view. And Sbuff is one of our on-chip SRAMs. And the next step is we actually use an ISA instruction. This is a DMA copy where we specify that we want to copy from the A input and B input HBM tensors to the, the on-chip tensors, OK? And similarly, we actually do something similar. We allocate some space for the result. In this case, we call it C tile. And again, this is on SPO. We use SPuff.vie to allocate the space for the tensor, and then we use the ISA tensor tensor. Uh, instruction to actually do the addition for us. So here you can specify that this tensor tensor op is using the addition operator. The two inputs are the two tiles that we had basically passed in and copied to SRAM, and then we're opening the results to the destination, which is the C tile. And then lastly, because we need to get the outputs back to HPM to return back to the, the framework, um, we use HPM view to allocate an HPM hosted tensor, and then again, we use a DMA copy to get the results back to HPM. Before we return the result. So again, this is a bit of a trivial kernel just to kind of give you the high-level flow, but you can see that this is, you know, fairly low level for if, if you've been used to working with Pytorch and whatnot. But, uh, you know, if you're really looking to squeeze the, the best bang for buck out of your model and make the best use of the hardware, um, there's going to be cases where you might need to actually get this deep into the, into the space. OK, with that, let's take a quick look at the benchmarking script here. Uh, so I didn't want to just blindly run a script, so I am going to run through the code a little bit with you here just to show you what we're running. But this is just a basic Python script. It uses hugging face transformers and the Quin 3 model. And essentially we, we load the, uh, Quin 3 model on CPU first. We're going to compile it for neuron, and then we're going to run a bunch of inferences and kind of time the, the latency and measure the throughput that way. And we'll also do a sanity check against CPU to make sure that the, the, the model results are the same as CPU. OK, so if we skip over the imports, um, there's a little bit of environment setup that we use for a neuron for the runtime. In this case, we want to specify that we're just using a single core, for example. We're also, because we're gonna be doing some profiling, we enable some, uh, profiling enable enabling environment variables here just to make sure that we capture the metrics that we need. OK, so we do wrap the, the model in a basic class here. This is just to make it a little bit easier to get the last hidden state from this embedding model, just to kind of make things a little bit easier for the demo today, um, but the class, you can see, it doesn't really do anything other than do a forward pass with the outputting only the last hidden state. Uh, we also have a helper function that encodes some example text and returns a repeating, uh, tensor based on the batch size that we specify. So it's just gonna be the same text kind of repeated over and over based on the batch size that we need just to help us out. And we have a few arguments that we use to control the behavior of the benchmark script. Obviously, like controlling things like batch size and max length. It's nice to, to be able to tweak that as you're doing the executions. But we also have a NICI flag here that we use to specify whether or not the benchmarking script should use the NICKI implementation that said AF is going to add for us a little bit later. OK, so here you can see this is where we actually load the model for the first time on the CPU. Uh, we do truncate the model, the 4 layers here just to speed things up and get this done in 45 minutes here today. Um, but essentially the performance for, you know, a handful of layers should be similar to the, um, performance for the full model, just, you just have to scale it up accordingly. OK, and if we skip down a little bit here. So this is where we actually get into the meat of it. So if we've previously run the script for this configuration, we're gonna cache the, the neuron compiled model on disk, just to save time if in case we want to run it again. But if it isn't compiled, we use this torch neuron X. trace call. And this is what actually triggers the neuron compiler to take the code. In this case, it's just the vanilla implementation of Queen 3 with naive attention. It's going to pass that into the neuron compiler. The neuron compiler will extract the graphs, compile them to run on neuron using the example inputs that we've provided, and it returns a, um, a Pytorch loadable model that we can then save to disk, and we cash it out to disk. OK, so if we haven't compiled it, we compile it, and if it's already on disk, we just load it in. Uh, we do a quick warm-up inference, we run through an iteration of 5 inferences, measuring the, the inference time, and then we calculate the accuracy compared to CPU and I'll put the results. OK, so it should be pretty straightforward if you've worked with hugging face before, but again, I just wanted to show you the code so it wasn't, uh, a mystery. And why don't we actually run this and get into the, the heart of it here. OK, so if you open up the terminal, here, I'm logged into a TRN 2 3XL instance, and we have a neuron SDK environment already available to us, right? So we can run commands like neuron LS for example. To get a quick breakdown of what's available on this instance type. Um, but let's just run the benchmarking script here, Python. OK, and what we should see is that it detects that the previous CPU outputs are saved on disk to save time and it loads those and it also loads the pre-compiled neuron model for us again, just to save a couple of minutes. And within a few seconds, we should start to see this executing the model on the neuron core. So again, this is Quin 3 0.6B. It's a four-layer version of that model using a 16K sequence length. OK. And it's just about done running, and now we get the accuracy just to prove it to you. So the MSC loss and both the coast and the cosine similarity are what we would expect. So the outputs of this neuron compiled model that didn't run on CPU, it actually ran on the accelerator. They 100% match the CPU outputs, which is great. But on the performance side, you can see that this is not, not a very performant model out of the box, right? We're only getting 0.35 prompts a second. With latency around 3 seconds. So it's, I think there's, there's definitely room for improvement there. And one of the areas where we often look to, to improve performance for transformer models is the attention block. So we will, we'll take a look at that shortly. But before we do that, I did want to show you that we can actually look at the profiling results that we captured here as well. So sometimes what you'll see is that, you know, maybe you're actually getting great device time, like the model's actually running well on tranium, but there's something else going on at the Pytorch layer. That's, that's, that's slowing things down. So if we get a system level view, a system profile here, we'll be able to see did this model actually run kind of best we could expect for right now before we add Nikki. So as part of the neuron STK we do have neuron profile, which is the profiling tool, um. There's different interfaces for this. Today we're just gonna output the system level profile in a Perphetto compatible format so that we can quickly load it up in Perphetto, uh, which is an open source visualization tool that you're probably familiar with. So we'll do a neuron profile view. We specify the directory that contains the profiling data and we'll tell it to use. Output format for photo. OK. And now if we look in the directory, it's created the system profile for us, so we'll just quickly download that. And let's hop into the browser, and this is Prophetto. Has anybody used Prophetto before for any kind of, OK, awesome. So you're familiar. Yeah, so we'll just open up the trace file. And I'll take a quick look here. Awesome. So if you're not familiar, this is just showing the execution timeline starting on the left, kind of scrolling to the right. You can see the first thing that happened, there's NRT load. This is actually the, the neuron runtime loading the weights of the model into HBM. And then beyond that, you see all these NRT execute calls. And what's nice is we can actually see exactly what's happening within the neuron runtime. So here it's executing the model. You can see there's basically 7 executions in the model here. So our benchmarking loop was 5, but we also did 1 execution for the accuracy check and 1 is a quick warm-up. Um, so that's why there's 7. But if we click on any of these executions, you can actually see some metadata associated with the execution. Which for this model may not that interesting, but for a larger model or maybe an execution where you had multiple graphs executing on different cores, this would be very helpful. So here we can see the exact name of the graph, which is the compiled version of the model for neuron, and we can see, you know, which neuron core it's running on. Those sorts of things. And on the left-hand side you can see, you know, the duration of the execution is, is pretty much what we saw from our benchmarking script. It's just under 3 seconds execution, um, latency, which is, you know, I'd say not great. So hopefully we can improve on that. Um, but yeah, with that being said, this is the baseline performance. So we did take that original Quin 3 model from hugging face, we compiled it to run on our own, we executed it on trainium and got those baseline results. And now let's see if CEDAF can improve on that using Nikki. Sure. Thanks, Scott. Uh, thanks for setting up the stage. Hey everyone, this is Sadhaf again. How many of you have lunch already? Oh, man, I'm so sorry. Not a good time to talk about the code after lunch. But I promise you we will try to make, uh, try to make it as exciting and promising as possible, uh, just bear with us uh, what we're trying to do here essentially is, um, uh, as we have seen performance for this, uh, Quin 3 model, uh, uh, we got the performance numbers and we got the timeline as well, and we'll try to see if Nikki, which is neuron kernel interface as Scott mentioned. Can help us to improve this performance. So in this part of the talk, we are going to focus on 3 major objectives. Number 1, I would like to show you how quick and convenient it is to integrate NIKI kernels in our existing model codes. 2, what are the performance impacts of the same? And number 3, which is going to be more like food for thought, like, from the extensibility and applications point of view, is there a way we can use those NICI kernels in our respective workloads when we go back home? So these are the three major objectives that we are going to, uh, uh, uh, address in the, uh, in the, this part of the talk. Are you with me? We're going to do a little bit of code, as I promised, I'll try not to make it very boring for you, but we'll, we'll do our best. We good? Awesome. All right, so there are a couple of things that I think, uh, it's worth, uh, noting from the profiling that we have done so far. Uh, we see like for each iteration, it's taking approximately 2 seconds, 812 milliseconds on a rough, uh, like basis, we can just say it's 3 seconds. Are you able to hear me there? Awesome. That is one. Uh, second, as Scott was trying to show, uh, the throughput that we are seeing here is 0.35 prompts per second. So, technically, it means that it takes approximately 3 seconds to just process one prompt. We're good? So, uh, when we are talking about Nikki and as Scott mentioned like how does it help us to, to improve the performance and the reason it can do that because it's, it works at the very low level which is very close to the hardware and it takes care of all the optimizations that are available on the table so that we can get the maximum performance from the underlying hardware which is cranium in this case. So, uh, we are very proud and excited about, uh, that, uh, yesterday, uh, we have launched, uh, Nikki library which Scott is going to talk about in a minute, and we are exposing some of the, uh, kernels, a predefined, pre-implemented kernels which can be plugged into your models just like we're going to say in this Quin3 model, and we can just take advantage of the performance on training. So let's have a look at that. Uh, people who have, uh, worked with hugging Face Transformers library before, uh, we would know for each, uh, model directory we have different model specific files. So in the, in case of Quin 3 we have configuration file here, we have modeling file, modular file, and so on. Uh, the very first file that I'm super interested in right now is this modeling file. Let me try to make the screen a little bit better. Uh, sorry, there's a question? Uh, we'll talk about that, yes. So, uh, the reason I'm interested in this modeling Qin 3 file because this, this file essentially which is given by Hugging Face Transformers library tells us the complete implementation of Quin 3 model, right? If I want to understand what the RMS norm looked like for Quin 3, it is here. If I want to understand what MLP looks like for Queen 3, it is here. And how the rotary embedding looks, uh, looks like or how is it implemented. Everything is here. And one part that we are super interested today is attention. Everybody heard about transformer-based models? Everybody heard about attention, right? So that's what we are going to, uh, see today we have this eager attention, uh, forward method that has been, uh, provided by the Hugging Face Transformers library itself for this model and what we are going to do is we're trying to see if there's a way we can replace this with Nikki. Attention, right? And we will see what is the performance implication of the same. So that is one module that we are going to be, uh, super interested in today. What I'm going to do. I, let me first. I'm going to take this attention, eager attention forward here and. I'm just going to paste it. Uh, everybody can see the code, right? Do I need to maximize it? Are we good? Awesome. So I just copy pasted that eager attention and let's give it a meaningful name. Let's call it like Nikki Attention forward. Right? And I'll not touch the input arguments, but I'm going to, of course, get rid of this implementation because we are going to write our own implementation here. And, uh, we will return the attention output and because it's inference, we don't need to return the weeds. So we're good. Step number one of my objective is to showcase how easy and convenient it is to to integrate NIKI kernels in our existing models. That's what we are going to look at right now. So we have this attentionforward.pi file here and what it has, we can go into the detail in a minute, but what it has is it has the implementation, the Nikki implementation for different flavors of attention, right? So as we know, like we have SDP attention, we have sliding window attention, we have flash attention and so many other variations of that. It has the implementation for majority of those, uh, profound implement, uh, uh, attention mechanisms. And we are going to use one of them, uh, in this case, we are going to leverage flash attention. Uh, for Quintree. Um, so what this file does, what this implementation file does is on top of that, it provides me, uh, kind of an adapter or you can just think of like an interface function and what it does is it will take all the inputs from me. It will try to make sure basic hygiene like I'm giving the right shapes, formats, data types, and then it will invoke the right kernel for us. We definitely are going to look into that kernel as well, but before that, let's invoke this kernel from our, from our modeling. 3.5. All right, so, uh. My attention output will be coming from here, and I'm going to call this one. But as we were looking at it, and we can go through this, uh, provided documentation as well. Uh, because it's, it's a Nikki implementation of the kernel, it expects us to provide the QKB tensors in a specific format or specific shapes, right? Because very close to the hardware, it expects us to do that, that little bit of, you know, massaging, reshaping before, and then give those tensors back to Nikki so that it can perform its operations. So for that we need to make a little bit of reshaping for our QKV tensors right now. So for query, so first thing, what I'm going to do is, we know, uh. If I expose the shape of my query or key or value, in this case, I will get the bed size. Number of heads, sequence length, and head dimension. That's what it's going to give it to me. Uh, this kernel, the, uh, attention kernel, it expects the query. I'm going to write it as a comment here so that we keep the track of it. So it says, OK, you are in this format right now, but I expect you to give me the query in this format. Just a little bit of reshaping, permute, and all these classical number operations that we are going to perform here, OK? So what we are going to do is from BHSD to B times HDS, essentially we are contracting two dimensions to one dimension and then, uh, changing the sequence of SD and DS. That's it. So what we can do is, first of all, we can do a permute to, to switch SND or swap, swap, um, the sequence length and. The head dimensions. So we'll make it first BHDS and then I think it will be easy from there to make it be time H DH DS. So let's do that. So query equals to. Query.clo. Dot permute. And the reason we are doing it because we need to change the shapes, uh, BH, the sequence of the shapes BHSD to BHDS. So 0 will remain 01 will remain 12 will become 3, and 3 will become 2. So far we're good. So we have achieved BHDS and then we will reshape it. To B times HDS as simple as that, and we will ask to give us contiguous memory location so that it's optimized. Fair enough. We just reshaped our query tensors. That's it. Now, similarly, we can do the same for key tensors as well. So our keys exactly in the same shapes. So we just need to make the change to my. 10 search names here, key states, and everything remains the same. For values, it's even simpler. For values, it says. Give me B times HSD. So we don't need to do the permute here. We can simply just reshape. And then Since we don't have to promote, we can keep the S&D at their original positions. Fair enough. And once we have this, uh, it means I am ready to provide these sensors to this adapter kernel, and I'm going to call this so we can provide our query. Key states. Value states. And then Scaling. Yeah And yeah, just change the variable names. Thank you. So that it's not key, it's values. Fair enough. So what we have done so far, we have changed our QKV shapes. That's it. Did not make any changes to the tensor values, did not make any other changes, just reshape them in the format that this kernel expects. Once we do that, we would be able to call this attention, uh, through Nikki passing out our parameters and we'll get the attention output. Once we get the attention output, the, the beauty of it that that's going to give us, uh, attention output in this format, and we would like to have it in. The final outcome should be in this format. BSHD. So we need to do the exactly same magic here as well. So we can do. So, first of all, we are going to reshape it so that it becomes BHSD. And then we just need to swap S and H, right? So right now, we have HS HS, then we're gonna make it SH. So we'll just Call our favorite function, which is transpose. And then we will transpose the 1st index with the 2nd index. Contiguous. All right. Awesome. And then we return the attention output. If we look at the implementation of this, we have just swapped query key values, not swapped, which reshaped. And Once we have reshaped them, uh, we have called the attention kernel, getting the output, again, reshaping it in the expected format that the, uh, function output, uh, requires, and then. Using it. Another thing that we have to do is Uh, we have to switch. Because there is a place where we are calling the attention here. What I have done here is I have already added this, like if my uh config has the Nikki as an attention implementation, call Nikki attention forward. So we are switching from eager attention to Nikki attention so that we are 100% sure that we are calling the right attention. Fair enough. So, let's quickly have a look. I'm going to remove the previous profiles. As we have seen when Scott was running, he was able to get the profiles. I'm going to delete it so that we don't have any cache data or something. And then we are trying, we will try to make it run as well. So let's. Have our terminal open. And this time, I'm going to call the same script which Scott has run, but this time, I'm going to pass this argument. Right? So, hope it runs. First time. Let's do it. Uh, what, what is going to happen now is. Because it has found a new attention, it's going to submit the model again for the neuron compiler. Hey, I got this new attention. Just compile it for me so that I can run on ranium, right? So this compilation will take maybe a couple of minutes. Meanwhile, we are, it's taking a couple of minutes. Let's have a quick look at the actual attention kernel that I promised to you. So I'm again going to the attention forward by. And here, if we look at our adapter kernel, I will save all these details for you, you can check it out later. But what essentially I can see, it's calling this attention kernel implementation without swapping. I am more interested to find out where's the definition of this kernel. So here it is. Line number 657 and just to warn you it's pretty decent implementation it's like 1000 lines of code so I don't want you to bore with that but what I want to, uh, share with you is that because at the end of the day, attention is nothing. But a function of QKVs essentially, right? We are doing QK transpose Soft max, and then multiplying this with V, right? So this is exactly what it is doing, but it is doing in a very hardware aware fashion. What it means, it means with Nikki, we have this amazing capabilities that we can take the maximum advantage of, of the underlying hardware which is stranium. So what it does is it employs various memory as well as compute optimization techniques. And when I'm saying memory optimization techniques, it, it uses smart DMEs just to ensure like we are efficiently using the DMA engines. We are efficiently using the DMA packet sizes as well. It uses the intelligent tiling concepts. It uses the, uh, efficient memory layout and allocations. When it comes to compute, what it's going to do is it takes the advantage of pipelining so that, you know, as Scott was mentioning in one of the slides where we have different compute engines, it makes sure that it is trying to keep all the compute engines as much busy as possible in parallel so that we can utilize at at like whatever the available compute that we have on the table, we can take the best out of that. It can also, it also uses the concept of of fusion, like when we are fusing or kind of, uh, combining multiple operations together so that we don't have to do a lot of DMAs, a lot of memory accesses, we can keep our compute engines more and more busy. So it employs all these mechanisms for uh to get the best performance from ranium and once you once you uh employ all these uh uh uh mechanisms what you get is a super optimized. Uh, uh, implementation of your model or a specific module that you're talking about that, right? So after a couple of minutes we see the compiler status is passed. Now it's going to run the inference with this new attention implementation. Remember it's not running the naive attention anymore it is running. The Nikki attention this time. Fair enough. And this time, the performance that we see is 2.99 prompts per second. Can anybody remind me what was the performance that we have seen with the knife attention? Exactly, 0.352, almost 3 prompts per second. So we are talking about like approximately 6 to 8 times better performance. Not 6 to 8%, 6 to 8X performance boost here. Isn't it amazing, right? So let's have a look at the profile as well. So just like Scott has, uh, generated the profile, we are going to generate the profile again. We are going to download this system profile. And this time we're going to save it with, let's say, Nikki, We go to the Prophetto again. Remember this time as well, Scott was mentioning 2 seconds, 812 milliseconds, approximately 3 seconds, right? Now, let's. Open this Nikki trace this time, which we have just God And let's expand it a little bit. Hmm. And let's see the time. Are we ready for that? Now, if we go there, the duration from almost 3 seconds to. Almost a quarter of a second. Right? So. The point I'm trying to drive over here is that with this performance. With this Nikki Kernel, we are getting almost 6 to 8x performance boost right in front of us. And that was, that was my second objective. First was to see, to showcase how easy it is to integrate a Nikki Kernel. That's what we have done and what is the, uh, the performance implications or performance boost that we get out of it. The third objective that I was trying to drive is a food for thought. Think about it. It is just a small model with just one attention, uh, module or attention part of the model that we have replaced with Nikki. Now this model has much more than that. It has MLPL also, which is super compute bound. It has, uh, QKV projections, output projections, and so on. And Scott will drive us in a, in a moment. He'll help us to understand like we have some of these kernels exposed to the outside world first time ever. These are the production scale kernel that we can use in our own models and leverage this amazing performance boost. So again, uh. Uh, going back to, uh, the performance gain. From throughput for the naive attention, we got this 1-story building to almost 10-story building now and from, from the latency point of view, which we want to be as small as possible, we have come down from there to here. Yeah, sorry, please the forum. Do we have the profile of what? Uh, I don't think we do profiling based on the power consumption as of today. Yeah So with that, I'll leave the floor for Scott. He'll drive us home from here. Thank you. Awesome. Thank you, Sada. So as you saw guys, we're able to go from not so great performance to pretty impressive performance gains basically just by adding a few lines of code and taking advantage of a kernel that's been, uh, built essentially by the Annapurna team. What we're really excited to talk about though is just yesterday we launched Nikki Library. So this is an open source project. It contains a number of pre-optimized Nikki kernels that are written by our team. You know, curated, tested, and maintained by our team and provided on GitHub, uh, so starting off there's a number of kernels available there today. You can grab the QR code, um, and, and take a look. So there's a lot of, you know, dense model support kind of in the beginning now we're gonna add additional models around or additional kernels around mixture of experts and, and other use cases and workloads as well, uh, but this is super exciting because up until now, um. You know, Nikki's still in beta, but it gives customers this access to the, the lower level compute engines that you just didn't have before, right? So you saw the naive, you know, implementation of Queen when we compiled it. It did run, it gave accurate results, but the performance was really lackluster out of the box, um, but with Nikki, we were able to drastically close the gap there and, and, and really drive amazing performance with this, this model. Um, and again, like we focused on attention today, but there's a lot of other aspects of different models, different layers that you might be able to apply Nikki to. And it's a little bit like onion peeling, right? Like you're going to get into a workload, you'll do a device level profile, figure out where the bottlenecks are. Maybe you tackle some of those with Nikki, it moves the bottleneck somewhere else, right? And at some point, you know, you can, you almost chase it forever, uh, but at least with Nikki, it gives, it puts the capabilities to the customer's hands that you can actually do this performance engineering that, that just wasn't possible from the Pytorch level before. So awesome. With that, I want to thank everybody for coming out today. Um, really appreciate you coming in for an afternoon session to learn about ranium and, and Nikki. Um, we have a number of other neuron and ranium sessions today and tomorrow if you're interested in checking them out, and we're open for questions. Thank you. All right. Thanks, folks.
