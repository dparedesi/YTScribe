---
video_id: O_fCw4zH88U
video_url: https://www.youtube.com/watch?v=O_fCw4zH88U
is_generated: False
is_translatable: True
---

Las Vegas, welcome to our session Build and optimize edge architectures for resiliency with AI. It's a 400 level code talk, and if this is your first code talk. It's also my first code talk. We're super excited to, uh, to take advantage of this format. Previously we've had things like chalk talks where we've had a chance to whiteboard, hey, this is what we think this could look like, or a workshop. Hey, um, come follow along with me as we build. But now we have an opportunity to code in front of you. This is live. Anything could happen, but we think it's a really exciting format to, uh, to learn and build together, um, and because we have a, a nice, uh, small and intimate room setting. We highly encourage questions. So if you wanna see us build uh or tweak something different, we'd love to hear your feedback. But by way of introduction, I'm Robbie Belson. I'm a developer advocate based in New York City. I focus on hybrid and edge services and specifically its application with generative en agentic AI and, uh, my co-pilot, uh. For this afternoon is the moment Jesus Federico. You wanna introduce yourself? Sure. Hi everyone. My name is Jesus Ferico. I'm a principal solution architect, uh, with Focus on Generative AI. I used to work for the Telco Vertical. Now I'm focused on fully on Gen AI. Fun fact, we used to, uh, both work together in the, in the telco vertical. So we've been. Uh, working hard on related topics here for well over a year and excited to share not only what we've developed here but what we've been learning from customers and some of their feedback and how it applies to some of these best practices. So before we start coding, since we have an hour together, I think the theory is very important what we're doing, why, uh, and why we're doing it, and I think it's always helpful to start with a level setting exercise because I find. You know, even as we go, uh, turn back the clock to 209 reinvent 2019, what, six years ago when we first announced a lot of these AWS hybrid and edge services outposts, local zones, wavelength zones, all part of this cloud continuum, we wanted to create a consistent. Set of AWS services, the same set of APIs operational, uh, consistency, um, and the same services you know and love exactly where developers and builders need it most. So just by way of hands, um, how many of you have had a chance to experiment with a local zone or an outpost or a wavelength zone? And I, I, so I, I think I said 2 or 3 hands. That's, that is super consistent across all the 6 reinvents that I've had a chance to build with. We often find that these sessions folks come who are new to all of these services. So we always figured, you know, even though it's a 400 level chalk talk, we wanna start, uh, our code talk rather, we wanna start with the basics and recognize that many of you have yet to build across that cloud continuum. Many of you, uh, have built on our AWS regions. But if you haven't had a chance to build with some of these other services, the, the fundamental question we asked was if how could we bring these cloud services you know and love closer to where that data is being produced and consumed or in a city near you or within the customer premise so just to define our terms as we go from the left to the right, you take that broad and deep set of services in the region and you can extend it to metro areas where maybe no such AWS region exists today and that's what we'll call an AWS, uh, local zone we can further. Extend those cloud services you know and love to the customer premise 42U rack mountable uh compute that we bring to the customer premise we call that an AWS outpost and we have a number of other services here that service the customer premise, um, such as EKS hybrid nodes where virtually any physical or virtual machine can connect back to a remote Kubernetes control plane managed by us and EKS hybrid nodes and then of course a number of services. To support far edge devices, we're gonna focus today most on local zones and outposts. We see a lot of customers. So Jesus mentioned we both supported, uh, telecommunications. It's very common for a telecommunications customer to think about using local zones and outposts to help build out their network. You might have a media and entertainment company using local zones to deliver low latency, so to service a new metro to get closer to those customers. Similarly, you might see that for gaming. But it could also be other regulated industries that are using local zones or outposts for data residency requirements. All that data must be processed in local X. And if an, uh, and if you bring an outpost into that premise, into your customer premise, where it happens to be within that sovereign boundary, well, now you've fulfilled that requirement. So there's a number of different use cases. It could be latency. It could be for migration. It could be for data residency. There's a number of different use cases, but let's jump ahead now that we've defined our terms. Yeah, also, if you're interested in, uh, in more how to deploy agentic solution on the edge, after this session will be another session 302. Um, it's gonna be around deploying agentic solution on edge. You can just come with us. We're, we're, we're supporting the session, so you can come right along with us. We'll, uh, we'll, we'll, we'll, uh, run on over there, so. Again, so we've we've defined our terms of the services that we're going to be working with today, regions, local zones, and outposts. And we wanna talk about resilience. Um, of course, one of the famous quotes, everything fails all the time. How do we care for resilience? And we define resilience as you see here is the ability of a workload to recover from infrastructure. Or service disruptions and you can think about the mental model for resilience in 3 different ways and again this has nothing specifically to do with hybrid and edge services. It's far broader for any region design you're gonna think about high availability again the resistance to common failures through design for a primary site. So in the case of hybrid and Edge what we're gonna show you and one of the key tenets is if you're using an AWS outpost, it's very likely that you're gonna wanna have multiple compute racks to have high availability. Similarly. For local zones, let's say you're deploying in the Northeast for a low latency, you're gonna want to use two local zones either in that same city if they're available or nearby cities too, just like you would in the region with two availability zones, the US East 1A and 1B for instance. But then you can also think about disaster recovery. So how do you think about backup, um, and manage recovery objectives, and we're gonna show you how you can because there, there's a lot to grasp here and each of these concepts of high availability and disaster recovery. Have implications to compute and say Amazon EC2 or storage and RDS databases and beyond networking so the cognitive load to have say one individual be an expert in all of these design primitives for all of the services of AWS as it applies to hybrid and Edge, that's a lot. But what if we could use agentic AI and generative AI more broadly. To leverage that expertise and design systems that help make prescriptive recommendations to create that resiliency, that's exactly what we're gonna do. And so if we do a good job today in the spirit of a code talk, if the if the code that we purport to build delivers what we hope, we hope it covers these areas. And so we talked about the mental model of, uh, resiliency as a whole. I tried to define as I think about resilience for hybrid and edge environments just starting from. The most, the simplest of primitives and and extracting out from there. So I would say when I think about resilience, we probably wanna start with site level resilience that there's a physical discrete location where that compute rack in an AWS outpost or that local zone lives, and I better hope that we have some geodiversity. I would hope that there's um the the networking is resilient, that there are different paths to connect point A to point B if that uh transfer is important to that application workflow. Compute resilience is also gonna be important that if I have, let's just say a fundamental unit of compute, if my application requires 3 units of compute, whatever that might be, an instance type, a load balancer, I better have more than 3 units of compute in that discrete environment. And of course you can extrapolate from there. So site level resilience, compute resilience, network resilience, data is important too. So when we start again to start to think about high availability and disaster recovery, I wanna have data resilience, so things like clustering, replication, and then more broadly like operational resilience, so not any individual primitive, but how everything works together as you start to think about auto scaling, health checking, monitoring. So if we do a good job today. This, these prescriptive recommendations for resiliency developed by an AI agent should be able to come up with um uh recommendations that match those buckets. And here's how we think we're gonna do it. And you could of course implement this differently, but here was our thought. Instead of, or to complement the experts you may already have in each of those domains, what if those experts could be. Represented as MCP servers. Those MCP servers were then tools provided to an agent complemented by the logical reasoning that an LLM, an Amazon Bedrock might have. It could be an anthropic model. It could be an open weights model. You, uh, totally up to you. And there was this agentic loop. I will talk about more that you have a goal and you're gonna keep using those tools in your tool belt, so in this case all the documentation for best practices that we at AWS have plus specific knowledge of how hybrid and edge works, so how, what changes in the world of outpost and local zones that we'll talk about. I gather a bunch of data. Have I achieved that goal? Yes, no? Oh, not yet. I need more information to, uh, achieve that goal. I'll continue that loop until I'm confident that I've achieved that user's result. So if we just quickly go back to that previous, uh, uh, slide for a second, we're gonna use all the resiliency primitives that we talked about before. So it's not gonna just operate in isolation here, it's gonna look specifically again the API. So it's not just knowledge of, hey, what are general best practices. I wanna know my specific environment. What's real, what isn't, how can I make improvements? So I'm gonna use an MCP server that talks to. My AWS uh various AWS APIs to liaise with VPC, EC2, uh, EBS, auto scaling, and the list goes on because now I can look at my entire environment, reconcile it with what those best practices exist, and suggest tailored recommendations to me. So let's jump ahead. We're gonna start talking about what we are gonna build here today from static to. Something more advanced. Yes you wanna talk a little bit more about what this looks like? Sure. So, so, basically, uh, our idea is to start with some configuration static file. Normally how, uh, would, would do this before it's just gather information around our environment, uh, see the resources that we have in terms of locals on outposts, and then, uh, create a kind of first report, uh, it says like kind of static report if you think is think. From having all the resources and say, hey, now I need to build uh uh best practice for my infrastructure. Then you are gonna put some NCP servers, some tools to automatically er request and fetch all these resources and based on the knowledge of the AWS MCP documentation part, we combine that to create um a new, um, a new report that is more uh comprehensive, right? And uh we will be iterating of the agent that we are gonna be building. We are gonna be using strands agents for those who doesn't know strands. We will talk a little bit around it, um, and we will talk around several how we can use multi-pattern strategy with the strands using, for example, agent as tool or swarm or uh workflows. So our goal today, uh, again, this is a kind of a high-level step we're gonna be creating. Uh, static Python file for discovery, uh, then we're gonna be creating a report based on this information. Then using Quiro CLI, um, we are gonna be integrating some MCPs. The first one is a tool to uh get information around the resources automatically. Also be using uh MCP to get all the documentation of AWS, uh, best practice around resilience, and we will be switching this from static, uh, from Kiro CLI. That is prompting into a strands agent and we'll be building a couple of strands agents and maybe quickly just a show of hands how many of you over the course of the week have had a chance either to be hands on or attend a session covering strands or or Kiro. OK, so good, good, that, that I was expecting, so we're maybe at 60-70% of the room. So lucky for you today we're gonna apply all of those concepts that you started to see specifically to this hybrid and edge track of how you, it's not just so there are gonna be plenty of other sessions that show you, hey, on an outpost or locals on those same telecommunications or media and entertainment or gaming companies may be running generally. Of AI models at the edge for latency or data residency reasons, we're flipping that model on its head today and saying, OK, what you could also do is for your own edifice use uh generative AI tools in the cloud to inform how you use those hybrid and edge infrastructure services in the first place. Let's move on. So, um, first let's talk about a little bit around our environment. Again, we are gonna be using outpost. We are using a specific account. It has 5 outposts, 4 of them are active, 1 is in provisioning. We'll be running this in the Frankfurt region where we have all the resources. The first thing that we are gonna be using is Kiiro CLI. Um, this Kiro CLI implements behind the scenes what we see as the Agentic loop, alright? It has the capacity of reason. It has the capacity to connect to the resources, but, uh, the first, uh, thing that we're gonna be using is Kiro CLI to create this kind of static files, the reports, and then move forward to create, uh, the trans agent. So, um, for those that used to how many of you used before Q uh CLI Q developer CLI? Alright, Kiro CLI is basically the evolution. It's the integration of uh of Q developer CLI. Now it's uh Kiro CLI. It's part of integrating of agentic development tool. So, let's move into our first step. Let me see. Let's create something here. Can, can you switch the, yep. Uh, demo. Yeah, so hopefully we are using the internet. It doesn't fail. So let's start talking with Kier of CLI and can everyone see is the screen large enough? Good. OK, great. Alright, uh, for those that haven't used this before, uh, here we have, we are using an agent that is default. What we are seeing here is how the context is being used, uh, so we can check. More here around the context. Basically, it has 3% of being of some of the tools. If you want to see the tools that it has already, it has, for example, one tool that is called AWS. One is the thinking report, write, read files and so on. So those tools already consume part of our context window because it's available directly in the curiosity way. So, I'm gonna be I have some prompts here, so, sorry, uh. Just to show you, so I'm gonna be saying that uh I'm presenting at Rainband. I'm showing live how curiosity Li works and for all my requests, save all the generated access and script to a specific folder, so it knows where to do the file, so we have some kind of consistency. Again, we are gonna be using a specific account that has uh some of the outputs that we already mentioned, so this is the profile that we have. So, here, um, now, er, Quito CLI is validating and creating the folder. It's already there, so I'm ready to set up all the demos. What are the first thing that you want to build? So, as we explained, let's let's move forward and and create a static file. I have my prompt here. So let's see. So I'm gonna be creating a Python file that is called Edge Discovery that use BO 3, the SDK library for uh Python to discover all the edge resources in this particular region, and the script must contain and discover things like availability zones, uh, local zones, outputs. I want to see which are the instance types are available on the outpost. And the script itself needs to manage all this information around error handling, throttling, and I want this an output as a Jason file. So let's, let's start doing this and then let's analyze what it creates so we can like the uh a little bit and key point to uh to think about right now, this is the, the. One, so if we're gonna think about again resiliency and reliability in a hybrid and edge environment, step one, even before we think about creating an agent, yes, we're using an agentic, uh, CLI tool to do so, but we're just trying to take inventory of what exists before we create prescriptive recommendations. And so that's exactly what you're gonna see here, so. It already created my file. Let's let's check the file. First, let's try to run it. If, if something happens, it can automatically fit it, but let's see if it runs and we can dive deep a little bit of the code that it generated, alright? Um. All right, so. Let's see. Well, it's running. So we, we set to create a file that is a static. It discovered the edge infrastructure. Basically, if we dive in a little bit on, on the function it created, it goes to both of three, it, uh, using the profile that we set and the region, and it creates, uh, and it fetch all the information around the availability zone. It has all the information around the zone ID state. Uh, if it's locals on, whether it's, uh, where it's the parent AC or the network border group that is attached for the pos it's doing basically the same. It's listing, it's both the three, SDK uh functions to list the outputs and basically it's getting the information around the availability zones, the instant type available, and so on. Also, we want to see from the output perspective, there is a contest around that is called slotting. So the Opposite itself is a Mac service. It's a hardware that a 42U hardware that you have compute nodes and you need to specify what are the shapings or or the type of easy to instance that you want to allow uh have available on that configuration. So, we want to see in this particular outpost that we have what are the instance types available, uh, and also if we have some placement group, what are the placement groups that we have, and let's see the results. It ran, all right? Uh, this is the information that we, we got, all right? Let's dive deep a little bit on this. It's basically saying the region, the availability zones and all the information that we just, uh, checked in the uh. In the Python script. Now, let's move forward and say that I want to modify this script. I want to, uh, this script to be stored in a JSON file, all right? It's gonna be a simple modification, basically, we expect that it will change a little bit the last part, all right, so now we are uh the output will be saved here in the edgediscovery. JSON file, and if we run this, uh, it will generate my file, my JSON file with the information we already saw. While it's generating any questions, uh, about what we're doing, why are we're doing it, it's sort of the sequencing here again, this is just the, we're creating a static snapshot, a point in time view of just, hey, what's out there. Once we have that, of course, then we'll be able to create recommendations and one very key insight, uh, especially for those who are seeing outposts in local zones for the first time, these are connected. Edge offerings. The local zone in a given metro or an outpost sitting in your customer premise always has a parent region. It's always, it's connected to, uh, an availability zone in the region that creates, of course, high availability, but it creates that connectedness that allows you to run API operations. And why that's important is if you, you see how all those outposts has an anchor availability zone, every local zone is anchored to a region. You're starting to see how that's gonna create high availability best practices. You're gonna want outposts anchored to different availability zones. You're gonna wanna have local zones with different availability zone IDs. So those same best practices continue to apply. Let, let's, let's ask even Kiro CLI to create a, based on the information we have, let's create a first static report. Basically we are saying like. An HTML report based on AWS best practice for the edge and considering the resources that we already discovered, we haven't told Kiro CLI what those best practices are, so it's gotta figure it out. Yeah, it, it will be using the knowledge that, uh, it was, uh, until the model that it's using that is auto model, and we'll dive deep a little bit more on that, but it's using the knowledge base that the model itself has onto the cut point time that it was trained, right, alright. So, if we want to use, and we will be using like, if we want to use the up to-date documentation, the latest documentation, we need to find a way to fetch that documentation and we'll be using an MCP tool that provides access to the best practice using uh AWS MCP documentation, all right, for, for this one. Any questions so far? the the No, we'll be showing this right now. It's just the knowledge that this large language model that's being used by default by Quiro natively knows, and the only reason it knows anything about your environment is we specifically say, hey, edge Discovery.json, which we happen to know captured that point of time snapshot. All right. So here we've got a few. We're we have our first, uh, this is just generated. We are seeing we're seeing it for the first time for the first time as you. So we have here, uh, our information. We are 3 availability zones in the US central one. We have 2 local suns enabled, we have 3 wavelength zones, and we have 5 outputs. 4 of them are active. One is in provisional state. And we have here all the details of the availability zones, the local zones, and the outpost. If you see the outpost itself, as Robo I mentioned. It is tied to a parent AC because when we, uh, request the outputs we uh we do something that is called the anchor VPC and we create a subnet in specific AC that we want to put the control plane go through through that, uh, AC. So we have here 4 outposts, one of the two of them in the one A, the one that is provisioning is not, it doesn't have any information yet, and the other ones are expanding across the rest of the AC. If we see the best practice documentation, um, it it it shows a really comprehensive information, like your infrastructure spanning the 3 AC with outputs in each zone, providing excellent geographic distribution. There are some recommendations around there is an outpost that is in provisional status, check what's what's going on, and we have multiple instance uh types available in in terms of diversity. We have several offers around instant type in terms of the outpost rack one, we have C5M5R5, and we can also have different types here. So, um, again, this is Kind of uh static information. We just generated the script and now we are providing some information. Any question? Yes. Try to Yeah, you, you can ask Kierra and say, you can use, uh, for example, if you are using AWS organization and you have cross roll, you can say, hey, for example, if you are using control tower and you have a cross roll at corners, many, you can say, hey, use this. Verify what are my accounts, you can you can discover on your account and assume the role to perform this information across all your environments spanning in multiple accounts. Right. He alive right Yeah, I have, I have here my profile. So in the profile itself in the AWS CLI configuration, it has the information around the account ID and the profile of the role that I'm zoom. When I when I I'm curiously light, so. Um, every time I When I use it for the first time, you know, I don't have a profile. and Right, um, if, if there are multiple accounts. uh Every single model Those are decoupled. One like one account is what you would have access would give you access to Quiro, but those other accounts could simply be to pull resources. So the, the, the simplest brute force way is however you've authenticated to have access to, to Quiro that remains unchanged, but you could have a your, uh, AWS con configuration file where you typically. Have your, your, uh, your, your account secrets and you could have multiple profiles and then in that and when you prompt, uh, Kra CLI you could basically say remember we created that edgediscovery.json file we could basically say hey iterate through all of those AWS profiles and then given those credentials you're now talking to different accounts with different resources, pull from there and now you have a more complete set of, of, of resources reflecting that JSON. Now how you segment by account, you would have to decide what's important to you. Yeah, alright, so let's move forward again. Uh, here we are not using the tools. The tool itself is what Quito CLI has, uh, embed, uh, as default. It has like to do thinking. You can enable some of these tools in the experimental part. Uh, so you see here I have like to do list, checkpoint, delegate, knowledge thinking. This is our part of, uh, the curro CLA part. But now I want to introduce a new MCP, all right. So let's, let's create uh a new agent. Uh, let's create, it's called AM 3. So, here it's the configuration of the agent. Um, think around this, it's like you can have multiple. Kero CLI instances at the same time think around like profiling so you can have one agent that has access to different tools at different MCP. In this particular case, I'm gonna be adding this knowledge, uh. Base MCP so. Hopefully it works. So, we have a, this is a public documentation, we will be sharing a little bit of what all the MCAS MCP we have out there so you can consume. This particular MCP is an MCP that provides information around best practice, latest documentation, uh, dynamically, right? So, now we have created, let's change the context and then say, I'm gonna be using this agent. So, if you see right now, and I go my tool now have new MCP here that is AWS Knowledge MCP server that has this kind of information and this kind of tools. So, what if we, instead of saying and create a static file, let's make something different. Let's say that I want to create. Uh, HTML report of edge resilience best practice for my infrastructure, but I want you to find the latest documentation and using the ASS tool that it has, find the resources for me. I don't, I don't file anymore. Yeah, you, you are not relying on anything, on, on any static file. It needs to somehow with the gigantic loop figure out what are the resources that it needs to find based on the best practice documentation, and it creates a report for me. While the report is creating, can we take a question from the audience? Yeah, sure. Um, what's the difference between the knowledge MCP server you just added and the knowledge tool that's already. Why They have. OK different. Yeah, the, the, the, this one, yeah, so I see there and then I saw you guys acknowledge the second time I was just curious. Yeah, I, I'm not sure. Oh, yeah, I, I'll take that offline. Uh, this was like the Quito was rebranded like, uh, one week ago. It used to be, uh, QCLI, so it, it introduced new features like web search and web fetch which wasn't before on QCLI directly. So you can dive deep a little bit. I don't know if, uh, generally speaking, the idea would be that it's the general knowledge that this cure, uh, that the QCLI must have for just coding in general and any best practices for software, whereas. The more specific guidance, uh, you can give the better and so we've created, you know, to keep update documentation to have a specific AWS knowledge, but it's likely to your point that there is some overlap, yeah, so if we see, uh, go ahead. Yeah, I focus on this question, um, so the built-in. Uh, knowledge is it basically using an MC. Right, because those are all, um, those are all tools that it has, and so it might, it need not be MCP, right? So as you'll see later when you create a strands agent, you can just have local tools that you'll see with like a decker. You have just some function like deaf, uh, cool function, and then it just has some logic. It need not be called through MCP. That's just one way for a protocol for different, uh, for an agent to talk to a tool. So in that case it, it may not be called via MCP itself. So if we've, uh, see what is happening behind the scenes. When I, when I ask this kind of request, it's gonna be running the tool to search the documentation, like receiving best practice for availability zones, outputs and fillover best practice, local zone architecture, best practice, and so on. And it think around it's getting all this information and putting it in the context, and now it needs to find all the resources that, uh, because I want this applied for my account, it will find all the resources using the profile. It will run the CLI using this tool to get all this information and create the HTML file for us. Um, it's, it's gonna take a while. Um, let's go back into the presentation to get some of the net thing that we are doing. And we can talk a little bit of the uh MCP servers we have there. Meanwhile, and we go back to the So, talking uh a little bit of uh some of the MCPs, uh, we add this MCP, um, which is the Knowledge MCP server, but there are other MCP servers that you can use. For example, in this particular case, we didn't add because it it was built in in the Qiro uh. CLI, but you can use a cloud uh control API MCP server to have access to all the resources around your AWS infrastructure. It basically have information around how to access with API and integrate with, uh, your, uh, environment. In this particular case, we didn't, uh, we didn't use it because it has already a tool that is AWS tool to get, uh, that information. Any questions? All right, so, um. So, we already integrate this. Let's talk a little bit around strengths. Well, it generates. It it hasn't finished yet, so we can talk a little bit around how we Now we want to move out from uh we create this kind of assessment and report, but it was kind of interactive, right? Like we, we need to dive deep and and type what we have in a prompting style. What if we want to run a kind of agent and we want to build an agent using Python that can run, for example, in my back end system and I can execute and can run every time that I need, instead of having, uh, something that I need to prompt manually. Uh, we can create something using, uh, uh, Python file and a ran an agent that can run in the back end. So that's why we're gonna be using strands agent. Strands agents are open source, uh, SDK for building agents, uh. And you can use a strands with uh multiple online providers. In this case we are gonna be using Bedrock, but you can use uh other provider and it has a set of tools that you can uh integrate, use and uh make it um, wanna mention something? Sure, maybe just uh as we'll see in the code, maybe just a few things, um, when you think about an agent. It's gotta have a model. It's gotta have a prompt, so the brain of how, how it's able to logically reason. The prompt to tell it what it's doing and then a series of tools, the actions it can take to deliver against that goal, and we believe that with with strands it's one of the easiest ways, specifically native to AWS to to marry those three, almost like strands of DNA that are intertwined and so the ability just to have like a really, really simple way you can rate agents in just a very few lines of code, but also native integrations if you wanted to have a rag workflow as part of your tools, so company specific data, you had a specific question and you had that rag workflow and you wanted to expose it as a tool. You could write a bunch of extra code to do that manually or you could just use the native retrieve library that's already integrated with Bedrock knowledge bases so that's an example of where the AWS integration shines with something like strands. So on the other hand now. We are gonna be switching to use Quiro. The IDE is the uh or agentic uh development platform for prototyping. In this particular case, we were using Quiro C line. Now we are gonna be using the IDE for create our uh first agent. And uh let's move on. Let's see. If we should have. Report the report ready. OK, it creates a new report. And, uh, generate this report we can see that it has similar information, 3 availability zone 2 local zones, 4 or 5 output. One of them is in provisional state, but it has also more information around, for example, also scaling group, load balancer placement groups. Uh, it has information also around, um, multiple deployments, uh, of, uh. Information of easy to instance infrastructure that spans across uh other other load balancer, for example, that we didn't have before. So, uh, in this particular case, again, When we create this, we didn't ask any specific resources to be like static generated or we didn't find it statically, it just find it for us. It basically discovered this placement group, local suns and all the stuff. So, it generates this for us based on the documentation that it finds and it creates the report. So let's move into um. Qiro, this is the uh. Development environment. Again, we're gonna be using byte coding. Um, this is a code talk. Let's, let's hopefully it works. So, let's start with Quito, asking something around uh Read the documentation of uh strands and based on the documentation, strands, create our first agent. So, in order to read the documentation of strands, we can use a strands NCP documentation er for for showing that. We have This strands documentation, let's figure out. Let's create our strands. Documentation agent. Mm Alright, so we can use, for example, this agent that is called a strands NCP agent, basically run UVX command, but in, in our particular case, uh, you can also use, for example, you can retrieve the documentation of the software that you are building. Ask the Quito environment to read the documentation if you don't have an MCP server for that, and based on the documentation, start building on top of that. And key and key point that I think, uh, you'll see here is the, the configuration of this, of this agent whether it's the Kire IDE or the Kire CLI was almost identical, and you'll notice when you specify those MCP servers, the first time when we're looking at the AWS Knowledge MCP server, it was more of a URL because it was a remote MCP server. In this case when we're showing strands it was a local MCP server, so the it's the consistency of defining the MCP services or rather to to com uh to define these MCP servers is, is consistent, but it could be local, it could be remote, it could be a combination of both, it doesn't matter so. In order to do that, again, I downloaded the documentation of uh strands, alright? So I'm asking to read the documentation. Also, I create a base template of a simple strands agent, so it has all the information around my profile. I could use even Quiro to to say, hey. You know, create using an agent using this profile in order to to avoid kind of kind of noise or we have a short time, I want to be focused on what we have today and focus on our configuration. So, I create a kind of base agent that it says, I'm gonna be using this profile, uh, I'm gonna be using uh a structured response in strands. That means, for example, if you want the output of your agent to be Structure, let's suppose that you are returning a person, uh, er object you want to have a name, you have to want to have the age, so you can create a kind of, uh, when you create the agent, you say, hey, use a static, uh, output in this particular, I'm gonna say I'm gonna be asking for a markdown report. So, and these are just play horror, right? Uh, this is our main loop and we are gonna be creating using uh R's profile. I'm gonna be creating a runtime client for Bedrock with 300 mill 300 seconds, 300 seconds to create the client. Then I'm gonna be creating the agent itself and asking the agent for something. These are like the most important part, quickly, folks, if this is your first time dealing with strands, what, 44 lines of code, really 3 if you exclude the closing, uh, parentheses, when we say you can, you can define an agent in, in 3 lines of code, that's what we mean. You see the system prompt, you see the model, you see the agent instantiation, and that's it. That's it. So, um, I'm gonna be asking to using the strands documentation from this folder, create an agent with the strands name of B1 using this base and basically the script. Should be running using this model. The 300 milliseconds, uh, 300 seconds is there for real timeout, and also we are gonna be using some tools similar to the static file that we created to create some information around get outputs, get local zone availability zones, and so on. So, behind the scenes is gonna be creating several functions to get information similar to the Python file that we created at the beginning. So, let's go and ask Kira to do that for us. Meanwhile, this create the file, all right? It's reading the documentation. I have a backup here, we can show how it's gonna be the code that it is gonna be generating. I'm only leaving this generating the code, but we can start reading how the code will look like. Shall we do that? Let's go. So, now it's doing, all right, but let's see, in order to get some of the time. And let's go back to the, I think let's start with the agent stantiation, show what's the same, and then let's show what's different, what the agent did. So we should still see somewhere in the middle of the file it instantiated the agent just like it did before. But let's, let's go to the agent part where we define the agent. So we have our main part where we have our conflict, our bedrock client, the model itself, and now we are creating an agent. And we are saying that you have some tools. Which tools are these? Basically we ask to create some kind of functions where it can fetch information around outputs, local zones, availability zones, easy to RDS you mentioned. So, in this particular case, we are creating this agent and the system prompt for the agent, it's basically this. You are an AWS edge infrastructure resilient expert and your role is to analyze the AWS infrastructure deployment and provide comprehensive resilience assessment. And your analysis should cover and there are a lot of points that it needs to cover. This is being generated right now, right away, but I have a back up here just to show you this. So, um, As when we create this, we ask to create a kind of tools, alright? Like easy to instance which are now it's running, let's Meanwhile, it's running. I'll be showing this. So if, if you want to see the tools that we created, uh, we ask to create like get outputs. So it's using both of three, the client to list the res the outputs, uh, and list all the resources pretty similar to the Python file that we created at the beginning. So, er after it run, it's running right now, so, it just created the new one. Let's see the not in the backup, let's see the one that it actually created should be pretty similar. Which is running right now. So, it's it's conducting a comprehensive ALS structure, uh, resilience analysis on the US Central one region, and it's calling the tools. It's calling get output, get local sound, get availability sounds, instance, uh, all the stuff, and now it's basically outputting. It's creating my output. This is the report that is gonna be saved in 1 2nd after it finished, uh, with my MD with all the information that I have. Any questions so far? And I think just a key learning objective is no surprise that the individual tools that are being instantiated like get outpost is gonna be using almost identical code to that first program we created that describes the available the EC2 instances that are available or describes the availability zones, the local zones and the outposts that are available, um, anchored to that region. So it's just we're taking these little primitives, these functions and. Showing it could be standalone Python file. It can be part of a tool that talks to an agent. It can be embedded into some remote MCP server that our, uh, CLI or agent talks to. It's just different ways of using the generative AI toolkit in AWS to get the results you need and often much faster than doing so manually. So, now it's it's it's basically the output of of all the information that it's providing. Once it's finished, it should generate a file with the MD that as we ask, like the big one, And I think it should be finishing in a couple of seconds. Hopefully, it works. After this, we are gonna be doing something similar to what we did with Curioy Line. What if we say, hey, just remove all these tools and let's use a tool that can provide access to my environment? I don't want to define any kind of tool and simplify my agent saying, now you are an expert, you have access to my AWS infrastructure, you have access to the AWS documentation, do it what I need. It's gonna be simplifying our code of uh trans agent in less than 100 lines. So, um. Right. Any questions so far? Uh, I have a question. Uh, but in terms of cost, how many credits costing? Yeah, so, uh, when you are, you are running strands, you can use, uh, the metrics tool, so it generates how many input tokens and output tokens it's, uh, consuming. I don't, uh. Your, your, your particular exercise. Yeah, I, I, I, it should be printing the, the metrics here. I think I have. Right now. We haven't benchmarked this exercise in the specific number of credits that it would consume in Cairo yet. Yeah. Yeah, so I, I don't have right, right away the, the, the point, but we can do the etoator and provide and print the metrics for you, yeah, so. Yeah. Alright. So, er, going back to the report, if you want to see the report um Is the MD. It has pretty extensive documentation. It, it should have also some recommendation around, uh, buckets, uh, information that we didn't have before and also some, uh, links around documentation and even some code of how to modify, for example, the load balancer that we identify that is running a spanning on a multiple AC and so on. Again, if, if we want to simplify this, uh, a little bit of, um. Of the code because now our B1 version is more than 500 lines of code that we may not need all of them. So, what if we say that uh I want to create a new version, alright? It's gonna be B2. Let me show you the prompt here. So, I'm gonna be creating a new version of this resilience agent, but based on the first one, but, uh, er, with an additional tool. I want to configure the MCP, the Knowledge MCP server, and also, uh, remove all the tools to get resources from AWS instead use the predefined use AWS tool that comes from strands. So the idea here is just to simplify the code and similar to what we did in the first uh in the first uh Tro CLI it it's gonna be created in a Python file with a trans agent uh that is simple to read and and basically it's prompting. So again let's see how it should look like while it generates the code for us. So Can you read there? OK. So, it's pretty similar. We have the uh response in a structured format output response, but now we have a system from saying that similar. You are an AWS resilient expert, you have real access to real-time AWS documentation and your role is to analyze the infrastructure deployment and provide a comprehensive resilienter report. Pretty similar. We have our model now. We have this MCP, uh, client that is a streamable HTTP client that it has access to the endpoint of the MCP, and we are creating an agent that with a model and say like now you are gonna be using this tool. Use AWS. This is a tool that is embedded in strands. We will see it here in the strands tool. You can find more tools available, um, uh, on the documentation on GitHub on the strands, uh, tools repo. And also now we're gonna be using the AW Knowledge MCP that it's basically what we defined before. And instead of having all those tools get outposts, get availability zones, get AC2 instances, you could define all those tools manually or you could, you could have your strands agent in turn be an MCP client that connects to these remote MCP servers, yeah, saving you a bunch of time. Now, if, if you see it's, it's running the V2 version. It's uh using the tools to get all the resources documentation and also it's using the AWS search documentation itself. Now it's pretty similar to the first to the second version that we did with Quiro CLI saying like, hey, now you have access to the tool, now you have access to the uh AWS environment and just figure out and and do all the things that you need in order to get that information for me. So, once we run that, uh, we, we should get a report, pretty similar to the first one, maybe with more details because now um it's conducting, OK, it's printing here. It has access to More resources because it can think instead of saying static oh you have access to the ALB you have access to the easy to it can figure out, oh, I need access to the placement group I need access to to understand what are the S3 or how is the NLBs or whatever resources are spanning across our infrastructure. It's generating that, um, I'm gonna be showing a one that is similar we just created this, but basically the output should be something similar of uh what we. are running, right? So, any questions here? Nope. What's the, uh, so I, I see in stantiagent agent. I'm just curious about the code itself. What, what's the event loop or the agent loop? Is it? Are we just like calling agent outrun and it's just. Yeah, so, so let's say the code of the agent loop part here. So, the agent itself, it's basically to, uh, you have to select which is the model, alright? And you can have different model provider here. You can use OpenAI, you can use Bedrock in in this particular case we are using Bedrock, but you can implement even your own provider, right? And then you have a set of tools that it's available. The system prompt and then you want when you want to call it, it's basically agent, it's like, do this, run this execution for me and this is the prompt that we are asking the agent. The agent, what we are asking is, please conduct a comprehensive AWS infrastructure resilience analysis for this region. It's a variable that is passing, and the report must be generated on the current date, alright, today, and include the following things edge infrastructure report, it generates that for you. So, it's kind of you define what is the system from from the agent, the tools available, and then you ask the things. So, in order to have uh a response that is a structure, you need to pass this argument that is a structure output model and it's basically a class that says in description mode, what is the model the agent needs to fulfill as a response, alright? Suppose that you have uh something, an agent that validates information for a person. You may need to have age, name, address, whatever. You may ask something and then you can ask directly information response structure response. age and you have the information there. Uh Clear? We have 8 more minutes. Awesome. So, um, it's gonna be generating this. Let's dive deep a little bit on uh what we have. While this is generate, let's talk a little bit of how we can create Multi-agent patterns on strands. So Now, we we saw information around, OK, now we have the code, we have access to some tools, um, and we can create uh a new version that has multiple agents. Instead of having isolated tools, what if we can create an agent that is uh an expert of assists admin, for example, AWS is admin. It has access to your AWS environment. And it uh can do read operation, write operation, whatever you it is the scope of the agent, so it has access to a specific tool. Then you can do an AOS documentation expert. Where it has access to the MCP of the AOS knowledge and then you can figure out and say, hey, I want a coordinator that I want you to uh I will be sending a request and using those agents, it will figure out it will have as a tool it will figure out and and do um perform the action for me. So, this one is gonna be the 3rd agent that we are gonna be implementing if I think we have time. But then we have other kind of pattern. We can do something like instead of having agents and pack that uh and provide that agent as a tool for other agents, we can do a kind of graph where this agent can talk with other in a kind of er DC direct graph, so we can say which is the flow like. I'm the researcher, I'm gonna be passing the information to the writer, and the writer pass the information to the reviewer, and the reviewer, er, print the output, for example, and it's just a kind of graph information. And then we can create a kind of workflow that in the graph we can go back and we can define the transitions going back and forth, but it's established transitions, all right, stable transitions. And in the kind of workflow, it's basically a structural coordinator of tasks that we can define. First step, do this. Second step, uh, Second step, uh, do whatever like researching, like coding, right? If you want to code an application, maybe we need to figure out first is the architecture itself, send uh the coder, the tester, and the reviewer, and so on. So we can create a kind of flow. Third, we have uh the swarm pattern. Uh, in the swarm pattern it's a kind of multi-agent that there is not a single coordinator. It's basically between them, they figure out the task and they you can specify the Swarm agent and uh in two different ways. You can define a swarm agent saying like, hey, you have this complex task. Create the agents that are experts in this in each particular area, and each particular area they start talking to each other as they believe they need to do, or you can specify a specific agents. You can create agents like AWS documentation expert, the sys admin expert, the report expert. And then, as a whole, say like, hey, I have this problem. Figure out how to solve it. And they will be passing information like, hey, I need to talk with the expert. The expert saying like, hey, I need to talk with the system admin to get this information. Go back to the other documentation expert, and they can er retrieve the information and work as a whole, sharing information across the agent, er, dynamically. Um, you want to see the code of, uh, swarm. How, how it looks like uh. I have a couple of examples here. So, still generating the, well, let me close this in a second. So in the In the swarm part. We are saying here that, again, we have a response, but then we are creating agent one. You are the infrastructure expert. Your name is infrastructure Discovery and you are an agent that I specialize in discovering and cataloging uh AWS resources, so it I have access to the tool use AWS. Second, we have an agent that is the best practice. It has access to the AWS knowledge MCP. Third, we have an agent that it says that what is the gap analysis. It doesn't have any kind of tool, but it says based on the uh specialized agent, alright? Comparing our infrastructure versus the best practice, which are the gaps, all right? And it figures out what are the gaps and what we need to do in order to make the our infrastructure very similar or uh compare against the AWS best practice. And then we have the recommendation. Again, you are a recommendation agent based on the previous information just gather and create an actionable remediation plan for that, and based on that, we create a swarm. And it says, like, OK, let's create a swarm where we have all these agents. The first that will receive this is the infrastructure agent. You can put the best practice, whatever you want, and you have some transition and match iteration that you can er take. As we don't define how the transition will look like, it start talking to each other like, hey, I'm I'm the expert of this information. So it passing information to the other one and it just creates a kind of er mesh loop instead er having Interaction between all the agents. Um And the agent as a tool, all right, uh, which is, let me see if this finish. We can ask our 3rd version to create an agent as tool, I'll be sharing here. But the agent as a tool. Now what we are saying is, alright, now I don't want I just want to have an expert that is uh AWS sales admin expert and we define this as a tool. But if you see the implementation code of this tool is, is again is an agent. So we have the same pattern. We have an agent with a model, we have a tool. And but we are exposing this function, which is an agent itself, as a tool that can be consumed by another agent as a coordinator, uh, agent, all right? So, we can create a kind of tree where we have a coordinator talking with other agents, um, that are being exposed as a tool. Um, in this case, we have a sys admin expert that have access to the use AWS tool. We have a documentation expert that have access to the MCP. And Alright. And we have the finally, the coordinator that provides information to both uh tools and it provides information for you. So that's about all the time we have left here, but just to give you, I, I guess a little bit of parting wisdom, we recognize that we had to make trade-offs in the session of going all in on what those best practices are for resiliency while trying to introduce all these primitives for generative AI. And Agentica specifically, we thought that to strike a healthy balance we would show you how to take or how to go from a static resiliency report with a simple Python file to starting to integrate MCP servers, strands agents, Kiro CLI and Kiro ID uh the Quiro's IDE and to take that one step further if you've had sessions that you've seen around Agent Core, you could run that strands agent that we created in Agent Core so you don't have to run it remotely on your laptop. You could have it running in that secure. Isolated environment so we hope that this painted a picture of what the future of, let's say your, uh, well architected framework review could look like for your applications using AWS hybrid and Edge services or otherwise. If you're interested in getting a little bit more hands on with hybrid and edge services, come with us just across the hall here for this next workshop. We hope you had fun here over the, uh, past hour. Thanks for hanging out with us, and we'll stick around to answer any questions and look into that credits question you asked. So thanks so much, folks. We appreciate it.