---
video_id: 2RWJejXdc-A
video_url: https://www.youtube.com/watch?v=2RWJejXdc-A
is_generated: False
is_translatable: True
summary: "This session, \"Scaling Multi-Tenant SaaS Delivery with Amazon CloudFront (NET316),\" introduces CloudFront SaaS Manager, a new feature designed to solve the complexity of serving thousands of distinct tenant domains from a single platform. Sagar Desara and Bhagirat from AWS, along with Ryan from Netlify, explain how traditional methods (dedicated distributions per tenant or complex shared configs) fail to scale efficiently. CloudFront SaaS Manager enables a **multi-tenant distribution** model where a single distribution handles thousands of tenants with logical isolation. Key capabilities include **Tenant Parameters** for dynamic, per-tenant routing and configuration (e.g., custom origins or paths), **Cache Policies** to isolate tenant data, and **Connection Groups** to assign specific network paths (like static IPs) to tenant cohorts. A major highlight is the automated **HTTP-based domain validation** for SSL certificates, which Netlify uses to onboard customer domains in seconds. The session provides actionable architectural patterns for implementing secure, scalable, and operationally efficient multi-tenant SaaS at the edge."
keywords: CloudFront SaaS Manager, Multi-tenant Architecture, SaaS Scaling, Edge Networking, Tenant Isolation, SSL Automation, Netlify, Custom Domains, Origin Routing, AWS CloudFront
---

Good morning, everyone, and welcome to day 3 at Reinvent. What I'm about to describe right now is something that we keep hearing from AAS customers over and over again. A SAS platform lands a big global enterprise customer, and overnight they need to support thousands of like domains with strict security and uptime demands. And then as SAS architects, like you've all felt that pressure to deliver a fast, reliable, and scalable experience without tripping over operational complexity. So today we'll explore how Amazon Cloudron and especially Cloudron SAS Manager offers an answer to that challenge. My name is Saga Desara. I lead teams at AWS that work with our SAS customers, particularly those building data intensive and AI driven platforms on AWS. I'm joined today by Bhagirat. He's our product manager from our Cloudfront team, and he led the launch for Cloudfront Sas Manager. So if you have any tough questions after, like, he's our guy to ask. And we are excited to have one of our customers join us today. So we have Ryan from NetLify. So, fun fact, Ryan was the first engineer hired at NetLFi. So if anyone can tell us what life looked before and after Cloud and Sass Manager, it's going to be him. No pressure, Ryan. So we are thrilled to have all of you here. Let's dive in. So here's what we'll cover today. I'll start with why running Sas at the edge is hard, what makes multi-tenant delivery so challenging as you scale globally. Then we'll look at tenant isolation strategies and how those choices impact performance and security. From there, I'll dive into Cloudf SaS Manager and walk you through how it simplifies your tenant onboarding experience. Bharat will then take over and run a quick demo in the AWS management console. And the next, we'll have Ryan, he'll show us how he's automated this tenant onboarding experience for NetLFi, which enables him to onboard new tenants in seconds, yes, literally seconds. And we'll wrap up with a quick call to action. And yes, that's the part where I get to give you all a little bit of homework. So quick level set, this is a level 300 session, so we'll assume that you have a solid understanding of Amazon cloud fundamentals like cache behaviors, origins, distribution, request flow. So we're gonna build on that foundation, how that architecture comes together and give you some implementation guidance that you can take back. So Amazon Cloud Fan is AWS's content delivery network that enables SAS providers to deliver multi-tenant applications with low latency edge optimized routing, and it supports tenant isolation at scale. By integrating lambda edge and cloud fin functions, you can execute real-time logic at the edge. You can customize authentication. You could customize your routing. You can surf content based on tenant context. Without sacrificing on performance. But in multi-tenant platforms, we don't just need speed. We need to ensure that every tenant's data routing experience is isolated and secure. So then the question becomes, what does multi-tenancy even mean when we're serving content at the edge. So each of your customers then become a tenant with their own domain, certificate, cash policy, compliance bond and routing needs. You're no longer managing content delivery for one brand, you're now managing delivery as a service. So in a SAS platform then, Cloudfront becomes your edge layer, but not just for you, but for every tenant that you're going to support. You can use one or a few cloud and distributions to serve thousands of tenants. This means that the edge is not just about caching static assets anymore. It's about tenant isolation. It's about domain management. It's about security at scale. And when you're small, like this looks simple, maybe a handful of tenants, one shared config. But then when you're serving thousands of tenants from one distribution, like everything multiplies your certificates, your DNS entries, your cash policies, your custom headers, and suddenly the edge becomes your operational bottleneck. And it takes like one misconfigured header or an SSL certificate to cause chaos. And that's not because that the CDN is not capable, but that's because you're pushing it to act like an application platform, which it is not. So approach one is a single tenant approach. Like every customer receives a dedicated clot distribution. They have their own SSL certificate. But what's the problem here? Cloudfront distributions are not meant to mutate thousands of times in a day. Each update to the distribution takes minutes to propagate globally. So if you treat each tenant as a new distribution, your deployment pipeline really slows down. With approach too, if you use a shared distribution, you now face the opposite problem. How do you isolate tenants logically while sharing one cloud-front distribution? Like, how do you achieve that speed and isolation? So yes, things can go wrong, but then, like, why is this hard at the edge? So let's, let's dig deeper. Sass at the edge is hard because your control and data plane, they both multiply together. When your control plane pushes a single change, that affects your thousands of customers. When your data plan handles a million requests, these requests belong to your thousands of tenants, and they all would have their own unique rules. So you need precision, automation, and isolation. And without that, like one small misconfiguration and you could be invalidating everyone's cash and or you could be breaking SSL for multiple tenants and to top it all, like you're trying to do this in one cloud front distribution that is a blessing for simplicity, but then it's hard to scale with that. In Cloud Friend, as you all would know, each viewer requests, it comes in with a host header, and that's how Cloud Fri knows how to route which tenant it's going to serve. Cloudfriend will then inspect that header. It applies the right routing logic and forwards it to the correct origin. The problem here, you're no longer managing one behavior in a multi-tenant architecture. You're managing thousands of microbehaviors with one cloud rent distribution. It's like when your whole family shares one streaming profile, like everyone has their own taste. Like my daughter, like she loves animated shows, my wife's into K-dramas, and for me I just wanna watch a good action movie in peace. But then because it's one shared profile, the streaming platform thinks that you're all the same person, and now my topics are just like a wild mix of Peppa Pig and serial killers and. Like Korean drama that I've never heard of. And the worst worst part is like one wrong click and someone watches one random like episode of a reality show and boom, the recommendations are ruined for everyone. And that's exactly what we're trying to avoid in SASS. Because in a real multi-tenant system, if one tenant's weird recommendations, they spill into another tenant's experience, we don't just get a messy homepage. We get cross-tenant data exposure. There is shared cash pollution, there is performance contention, and you could be getting a very angry customer escalation at 2 in the morning, like none of us want that. So this is where SAS architecture really begins. Isolation is the foundation of SAS reliability. How we isolate the tenants logically, physically, or by resource boundaries, it determines everything that we do downstream, like how do we route the requests, how do we enforce access, how do we scale, how do we detect noisy neighbors? How do we make sure that one tenant does not accidentally impact experience for your other tenants. So with that as context, I'll talk about tenant isolation strategies that form the backbone of modern multi-tenant platforms or architectures on AWS. So there is no one size fit fit all model for tenant isolation. Like you can choose your pattern based on your scale, your complexity, your tenant expectations. Some teams prefer siloed isolation like dedicated distributions and tenant expectations. They offer stronger security boundaries, but then it comes at a higher cost and management overhead. The upside, you get maximum isolation. You have, you don't have noisy neighbors. There's predictable performance. It comes with the highest security. The downside, it's expensive. It does not scale well. And it, like what happens if you have hundreds and thousands of tenants? You're essentially now running a mini SAS for each customer. So this is typically reserved for your high-value or premium clients who have those strict security or compliance or performance requirements. The other teams, they prefer pool isolation. So here, tenants will share resources in a pool, but then we use logical isolation. Things like namespaces, tenant IDs, per tenant quotas. It's like a co-living space with a rulebook. Like everyone shares the kitchen, but then, but then each tenant has their own locker to keep their stuff. This model is highly efficient and scalable, but the trade-off is a noisy neighbor can sometimes impact performance for other tenants' performance if the isolation mechanisms are not well tuned. Modern orchestration frameworks like Kubertis, they do give us though, fine gain control over how tenants can share this information. We can isolate the workloads based on like name spaces, rate-based access control, network policies, per tenant quotas, and you can combine that with AWS App mesh or IAM boundaries and you get that logical isolation that's both efficient and secure. And this is where the modern cloud native design really earns its stripes, giving a scale without sacrificing on safety. And finally, tier-based isolation, so as the name suggests, it's a combination of both the approaches like high risk or high valued premium customers or tenants, they get siloed resources while the rest are pooled. So it's a way to optimize for your cost, for your performance, without sacrificing on security or SLAs for the tenants that demand it. So in practice, which strategy you use, it's not just about technology. It is a business decision. It's a balance between your risk, your cost, and your customer expectations. So the takeaway here is your isolation strategy is foundational. Like it affects how you scale, it affects how you handle failures, and then ultimately how happy your customers are. And if done right, that is what will let you run multi-tenant SAS at a massive scale without sacrificing on control or security. So once you've decided how to isolate the tenants, the next thing that you're looking at is how do you actually route them efficiently. And at the core of every SAS platform running on AWS or running on Cloud Run. Lies a critical responsibility of tenant resolution, like mapping an inbound request to a tenant config at the edge without hitting your origin unnecessarily. And routing is where the edge really earns its keep. You want every request to find the right tenant origin in microseconds with zero latency overhead. So so far we've seen how to route and isolate the tenants at the edge, but as the tenant count grows, like, so does complexity. So you're now managing your domains and certificates and WAF rules, and this is all happening per tenant at scale. So as our customers were have been building and scaling the SAS platforms on AWS we kept hearing a consistent set of architectural challenges from them. And those patterns became the foundation for how we designed Cloudton SasS Manager. So first, you have 100s or 1,000s of your customers sharing the same platform. But each one needs some degree of uniqueness. So you want to be able to reuse your shared infrastructure and resources that you have efficiently, but then you still want to be able to isolate the behaviors per tenant. The white box here shows your shared infrastructure configuration. And ideally, you would want all of this to live in one cloud print distribution. On top of that, you have different tiers of customers. You have your basic tier and premium tier and enterprise tier, and all of these tiers of customers, they all come with their own unique requirements. Like some have stricter security requirements. There are varying SLAs, there could be different subscription tiers, there could be unique caching needs, and all of this needs to be enforced independently. So you need to have a way to manage each stand's configuration now, separately, without creating this massive sprawl of cloud-front distributions. And trust me, like no one wants to manage hundreds of cloud-front distributions, that'll be a nightmare. So that's the mental model here now. There's a single, we need a single config for your shared infrastructure, and then you want to be able to maintain separate tenant-specific configurations while enforcing isolation. And ideally, you want both your shared infrastructure config and also your tenant-specific configurations. You want all of them consolidated in a single SAS configuration, keeping things simple, centralized, and manageable. And those tenant configs, the boxes that you see at the bottom. They can simply inherit whatever they need from your main SAS config, that's a shared infrastructure config. So you don't have to redefine those rules over and over again. So with Cloud and Sass Manager, we take that mental model and we turn that into concrete constructs. You have your multi-tenant distribution, you have the per-tenant distribution, and then we have tenant scope, parameters and rules. Together, these constructs, they will let you safely isolate tenant behavior, enforce security policies, and they'll help you scale your platform. And this is the foundation that makes per tenant isolation not just possible, but also seamless with Cloud and Sass Manager. So here, what you're seeing is there's a multi-tenant distribution. So that's essentially a single cloud-front distribution, and that's configured to serve multiple tenants of a SAS application simultaneously. So you can now use one distribution, it can handle tenant-specific behavior through routing and caching logic at the edge. This multi-ta distribution acts as your baseline config for your entire SAS config. The tenant config then can apply the targeted overrides, like for the behaviors, for the routing, for security policies that may differ per tenant or for a group of tenants based on how you're set up. So we define distribution tenants as individual customers or logical entities. That a single cloud-front distribution serves. Even though multiple tenants share the same distribution, every tenant's request, cash entries and experience, that is all logically isolated. And then you identify the tenants using domains, sub-domains, headers, cookies, URL paths, and you can use that to route the traffic accordingly. This setup allows you to scale efficiently, supporting hundreds or even thousands of your tenants. While still enabling tenant-specific logic at the edge, and that includes real world examples like you could do tenant aware authentication workflows, you could do customize the branding or personalization for your tenants. You could also have tier-based performance rules. So this lets you deliver like fast isolated experiences for every tenant, all from a single cloud front distribution. And this approach removes a lot of the operational friction. Like developers can stay focused on building tenant logic, like, and then SAS manager can take care of provisioning and keeping configurations consistent, like no more drifts or manual clean up. And because everything runs through APIs, AWS APIs, and even driven workflows, onboarding a new tenant goes from hours of manual setup to just minutes or even seconds. It's fully automated, it's repeatable, and it's auditable. Like Ryan's going to walk us in a bit about how he's automated this for NetLify in that environment today. So now that we've looked at how cloud from SaS Manager enables multi-tenant distributions, I'm gonna step through an actual flow of the request. So when a new SAS customer is onboarded, it will kick off the tenant onboarding service at your end. Your service will start spinning up your tenant specific resources like your compute, your storage, your database instances, or even your per tenant configuration objects supporting the tenant isolation and scalability that's expected of your hardened SAS platforms. The next step is to configure cloud-front rules, and this is where we start making the distribution behave differently per tenant. And one of the ways we express those per tenant differences inside a multi-tenant distribution is through parameters. So parameters are 10 specific key-value pairs. They let you customize routing. You can set your original behaviors dynamically without touching the underlying code for each tenant. So before parameters, if you had routing requirements such as based on user attributes, then I'm sure you all would have, y'all today use either lambda edge or cloud front functions and then you're managing all of that in for all the tenants in a single cloud front function or lambda edge and deal with all the operational overhead, not to mention the extra cost that's associated with running these functions. Now, you can configure these parameters as variables in the multi-tenant distribution. So as you see here, I've actually defined the parameter called tenant one in my multi-tenant distribution. So that's my main config. Then I use that parameter as I configure my distribution tenant. So in the distribution tenant config, I'm assigning a specific value. So in this case, it's golf carts to that same tenant one parameter. So this lets me leave my distribution config untouched, and then by setting the parameter value in the tenant config, Cloud trans Sas Manager can resolve the updated origin path at runtime for that specific tenant. Next, let's look at how you control what gets cashed and how it gets cashed for each tenant, and that's where cash policies come in. A cash policy in Cloudfront defines what part of the requests like headers, query strings, or your cookies are included in the cash key. For SAS platforms, this means that you can cache the content separately for each tenant by including tenant identifiers. Take scenario one as an example here. So with cash policies, I can carve out separate cache entries for each tenant as I include X tenant ID request header as part of my cash key. So tenant 123's analytics data stays completely isolated from tenant 456. So, but both still get the benefit of caching repeated requests for their own users. Next, I'm gonna talk about another important concept that we introduced inside Cloud and Sask Manager. It's called as connection groups. So think of connection groups as logical partitions inside your cloud front distribution, like that control how the end user connections are managed in a multi-tenant setup. So, traditionally, Cloudfront will reuse the TCP connection across all the end user requests to improve on performance. But then in a SAS environment, that means that it could mean that requests from different tenant, they are all in the same connection pool. With connection group, you can actually now set boundaries so that the connection pools can be isolated for a group of tenants or even on a per tenant basis if you have the right use case for it. So in this example, both tenants use the same cloudfront distribution, but each one is routed through a different connection group. So look at tenant A. Tenant A is locked to a small set of static NEA IPs, and that's perfect for customers who have those strict firewall IP allowed lists. Tenant B here, it uses a deal stack connection group so that they can reach Cloud Front or both IPV 4 and IPV 6. So even though the distribution is shared, Cloud Front SaS Manager makes sure that each tenant connects through its own network path that's tuned to its requirement. So no, you don't need to like do any config cloning, no forking, or there is no fear of having any cross-tenant networking side effects. The next step is step 3, and that's where when a new tenant is onboarded, Claufen can request a certificate from AWS certificate Manager or ACM. So as aA provider, you often face the limitation that you might control a few domains, but then the majority of the domains are actually controlled by your end customers. So this makes standard certificate provisioning tricky because you don't, you can't really rely on DNS validation because you don't control their DNS. So Cloudfront solves this problem now by integrating with automatically with ACM. And what we are doing here is to address this DNS challenge, we introduced HT HTTP validation, which lets you fully automate certificate provisioning even for the domains that where you do not control the DNS for your end customers. So you can now provision certificates end to end without involving your end customers at all. And that keeps your entire tenant onboarding flow fully automated. In a bit, Ryan's going to nerd out. He's dive, he's going to dive into the APIs and show us how he's got this set up for NetLify. And the final step involves DNS. So that's step number 4. So at this point you can now set up Route 53 records or other DNS mapping pointing the new tenant's custom origin to Cloudfront. So that covers the key concepts we spoke about multi-tenant distribution, cash policies, connection groups. Now to bring it all to life, Bhagara's going to walk us through a quick demo in the AWS console before we bring Ryan on stage. Thank you, Sara. All right. In this demo, what I'm going to walk you through are 3 things. First, we're gonna create that multi-tenant shared settings configuration that Sagar just spoke about. We'll create our first tenant. And then also look at how we're gonna associate connection groups with specific settings for uh routing to that particular tenant. Let's get started. So I'm going to go into the AWS console into Cloud Run. I'm going to click on create distribution. That gives me options. I'm going to select the multi-tenant architecture. At this point, you'll notice that there's a dropdown for a wildcard certificate. That becomes really interesting because a lot of our SAS customers told us that they usually rent subdomains to their customers, whether they're their free tier customers or they're enterprise customers, they all get a free subdomain. So when you add a certificate at the multi-tenant distribution level that gets inherited to all of your tenants, and typically this is a wildcard certificate so that you can rent subdomains. It could be a san certificate. It could be any kind of certificate you want, but the most common use case is a wildcard certificate. Next up we're gonna talk about the origins and the parameterization that we learned about. Parameters are essentially variables, right? So variables, these values are defined at the tenant level, but the definition of where the variable is, how it's going to be exposed, is defined at the multi-tenant distribution level. Like this is particularly useful when especially you have like patterns. Um, in my example here. Each customer has a separate path or a folder within my history bucket for their content. And so I can define the path by a variable in this multi-tenant distribution, and when I create my tenant, I can tell Cloudfront what's the path for each tenant. So I didn't change any recommended origin settings, but if I had some specific settings I wanted to, I could do that at this stage. Now, you've got that baseline, you've got the origins, you've got that certificate set up. Let's talk about security. At this point you have the option of choosing an existing WAF rule or Web ACL and associate that with this multi-tenant distribution, or you can create a new one. Either option works and in this example I've created an, uh an existing Web ACL. It has the settings I want to be inherited by all my tenant domains, and that's what I'm associating over here. I'm going to review these settings, and we've completed our multi-tenant distribution setup. Next, let's go into a tenant. So when you want to create a tenant, the first thing I'm gonna do is add a name. This is just a tag. It's just for me to remember this easily, search for it. When tomorrow I come in, I have 100 or 1000 tenants. I need to know what this is about without really having to remember the exact domain, um, so you'll notice that the current multi-tenant distribution that we had chosen is already selected in that dropdown. Later I could choose to move this tenant to a different multi-tenant distribution. Do you remember that Sagar talked about this isolation strategy where you have tier-based uh tenants, so you have a free tier, a premium tier, and a silver tier. When your customer graduates from the free tier to a premium tier, you can switch them from one distribution to the next. That way you can enable additional settings at each tier. A good example of this would be something like Origin Shield. You probably don't want to give all your free and hobby tier customers access to that, but when they graduate to being paying customers, you're giving them additional value and performance by enabling Origin Shield on that multi-tenant distribution. The one interesting piece and the good part about how we designed this was we learned when we were talking to, you know, Dana and Ryan here at NetLify that they are doing this often. There are people who graduate, then they go back to the free tier. They do this seasonally. So when all of this is happening, you don't want to impact performance, so the cash is logically isolated at the tenant level. The cash key is defined at the tenant level, so when you move it from one distribution to the next, you have no impact on performance. All the cashed objects remain in the cloud front cash, and they get access regardless of which distribution they're associated with. Next, I'm gonna add a subdomain. This is a vended subdomain that uh a SAS provider provides to all of their tenants, and what you're gonna notice is that the wildcard certificate I had associated at the multi-tenant distribution level covers that subdomain. But most of your tenants, especially the paying customers, are going to want their own custom domain. In this example for simplicity, I'm choosing an existing certificate that covers this custom domain. But you will also be able to automate this, and we've added HTTP based domain control validation, which allows you to make this process of onboarding custom domains for a customer much easier. The existing mechanisms of email-based or DNS-based uh domain validation still exist, but HTTP-based is the uh. More operationally efficient way for SAS providers because they don't control the domain. I use DNS based in this example because I control the domain. It was easy for me to set up ahead of time for the demo, but in the next session, uh, a little later, Ryan's gonna come up on stage and walk you through an automated example of how they use HTTP-based domain control validation in their onboarding workflow. Next, I'm gonna define the values for that parameter we had associated or defined at the multi-tenant distribution level. This allows me to select that path, which path or which S3 bucket and folder that my content needs to come from for this particular tenant and the domains associated with this particular tenant. You can have more than one parameter. The Absolute max you can have is about 5 parameters in total, no more than 2 parameters for each field that you're using the parameters in. When you have tenants that require customizations, especially your enterprise customers, they're going to come in with their own security rules that they want added. They're going to have their CISO, their security team, tell them that their application has to have a certain set of rules, and in these cases. You can choose to override that existing Web ACL that was inherited with a custom Web ACL so they can have their own WAF rules, they can have partner enabled rules, AWS managed rules, all of that just for their tenant. I can also add geo restrictions in my example here today, what I'm going to do is select United States and United States outer islands as the only uh end users that can access these domains for this tenant. This isn't a common example where the customer comes and tells me, hey, I only do business in the United States, or for regulatory reasons I don't want to do business with anyone else. I don't want to handle the legalities, the taxation for other countries, so limit access to my domains just to the United States. That's what we're doing here Now, at this stage, we've created a multi-tenant distribution, we've created our first tenant, all that's remaining is our DNS setup. Before we do the DNS setup, we'll go into connection groups. This is where you can control how your applications or your tenants' applications actually connect into Cloudfront. Um If you notice, there's no connection groups on the left pane here. So first I'm gonna go into settings and enable this custom feature. Once I enable this, there's a shortcut that's added to the SAS uh section for connection groups. I'll create a new connection group and set it up with the settings that I desire. One of the most common use cases in the SAS world is that your customers tell you that, hey, I want to use my Apex domain. I want to use example.com. I don't want to do www.example.com, and that leaves you with a challenge. For example.com in DNS you have to have IP addresses. You cannot have C names. And so they come to you and say, I need a single IP or I need a small list of IPs that I can put an a record in my DNS. And for this we're gonna create a static IP list and for that in the connection group example we don't have one right now so I'm gonna go ahead and create this opens up in a new window so you can go ahead and create your static IP list. There are a couple of different use cases. Sagar spoke about the allow listing example. I'm talking about the Apex domain example, so I select Apex domain. And then I have the option of choosing IPV 4 or dual stack. I'll select dual stack so I get both IPV 4 and IPV6 addresses for these customers that basically gave me 3 IPV4 addresses and 3 IPV6 addresses. These addresses are going to be static. They do not change, and they only serve requests for your workload, no other customers, so it's isolated to just your workload. Next, we'll go back to that connection group that we had created. And associate the NECA static IP list that we created to the connection group. What this does is when you look up the domain that the cloud front domain that we've given or assigned to the connection group, uh, you do a dig, you do an NS lookup, you're always going to get one of those 6 IP addresses that we had seen in the previous screen. Now we're ready to associate this connection group and this NDNS um to the tenants. So let's go search for the tenant that we had just created. You see another drop down there for connection groups now. In the connection groups, I'm going to select the connection group that I just created with the static IP list. And voila, you're ready to go and enter your DNS entries and point those domains to cloudfront. At this stage, you'll wonder, do I have to do this for every single tenant? You can, but you don't have to. You can use the same connection group across multiple tenants. This is because we, we understand that you're gonna have some tenants with specific needs, but these needs are. In cohorts there's customers who say I only have IPV4. There's customers who say I want dual stack. There's customers who say I want static IPs, but beyond that, the rest of the settings remain common, so you can actually reuse those settings. So if you're updating your documents and allowing customers and telling customers how do you onboard an Apex domain to your platform. You can actually have these static IPs provided and the same IPs are provided to every customer and that way all of them use the same IPs and your operations are a lot easier. We can repeat this process in the console as many times as we want. But that's not what you're doing in production, are you? So we're gonna have Ryan come up and show you how to automate this, which is really the playbook of how SASA operations are done. Ryan, take it away. Thank you. So, let's just get right to this. First thing we have to do is figure out what is NetLify. And we're a platform that helps developers take their ideas from their laptop into production. We serve over 9 million devs right now, providing key infrastructure for them to lower that bar to getting online. So much goes into developing a site, from what tools you're using locally to what you're using in production, how you monitor and observe your traffic, or how do you iterate safely and consistently, and so much more goes into a site. We're trusted sites from all around the world, from small personal ones to massive enterprise ones. It's been really fun to build out, but that ease brings a really unique cardinality problem to us. Yesterday alone, we served 6 million websites, different domains all over the place. This means that on top of having high RPS and staggering bandwidth needs, we have to actually deal with a huge breadth of sites. The SAS problem is very much our problem. When we initially set up the network, we found that all of the existing providers didn't really have the features we needed. They're built around singular domains or a small set of them, and the provisioning kind didn't work. They could handle all of the throughput, plenty. They've got plenty of capacity, but the operations was a problem, so we had to build it ourselves. Here's a much, much simplified version of what we built. It's a global network that directly serves the sites. Integrated into that network is a huge host of features, from automatic AI support, multiple levels of edge compute, advanced caching directives, complex routing. The system was designed to handle a small scale set of people up to really massive global presences. And as a business, we needed to really be mindful of scale, performance, and availability. These are really table stakes for any edge offering. When we then started talking with Sas manager after we had built a lot of this, that conversation started to change. We started to see a way forward where we could leverage their strengths to improve our product. Really came down to a few motivations for this integration. There was cost was a huge driver. Cloud cloud front bandwidth is dramatically cheaper than Direct EC2. It's really nuanced around what region you are, what your contract is, where you're serving, but needless to say, it is a, uh, benefit. When you come into network operations as well, what we ended up seeing is that DDoS, in my opinion, is a resource war. The goal is to spend less, less resources on bad traffic, because really we're just trying to compete of who's got more CPUs. At Lla we have many, many different ways that we can carve out traffic from any different type of attack, but at the end of the day, we push down to our EBPF layer and we say we're rejecting this at the kernel level. I can't go beyond that really practically. But once we started saying if we put Cloudfront in front of us, I then have more operational stuff that I can work into. And finally, when I look forward, when I, whenever I've done a large integration, the question comes up is how can I leverage that new integrated thing to improve and use their strengths to improve my product. But let's take a quick look at what was wrong with uh the limits of the standard distribution. In particular, Any CDN needs to have TLS termination and handle the caching for you. Pretty straightforward. That doesn't really work for us in the standard distribution. In particular, it comes down to the fact that the standard distribution was fundamentally designed around a small set of tenants, a small set of domains that it's handling. You can see it in a bunch of different parts of its design, from the single TLS certificate per distribution, meaning you're fundamentally limiting how many places that can serve. And if you want to handle millions of unique domains, you would need millions of distributions, and we can see how that becomes a problem really quickly. And you're only allowed to run a tiny amount of code pre-cache. And I'll touch more on this later, but we run a lot of stuff pre-cache. We wanna be able to do traffic shaping and anything like that. So we started looking at Sask Manager. Once we started this integration, we started talking with Bhagara and team, we really started seeing that it starts checking a lot of the boxes for us that we could integrate with this. Handles all the cardinality issues. It handles improved resilience on our network, we get a much bigger footprint. Overall, all of the gears started working together. But I'm gonna run through how we made that automation. This is gonna get into a lot of the codes and API calls that we actually ended up doing. And this is our starting point. We have just a viewer hits our edge, we serve the content back pretty standard serving. First step was we needed to build a little service that translates our customer context into the AWS languages that we needed to do. And we needed to set up the distribution manually. Single parameter to help us with the routing, and one of the big callouts is we actually disabled all the caching in Cloudfront. Still, fundamentally, the caching is incompatible. Cloudfront is around a path-based caching that you know beforehand. I don't control the URL spaces of people. I don't know what it looks like, so I can't preemptively tell them. Maybe a little bit in the future I'll be able to leverage that. There's some future ideas that we have for it, but fundamentally we just use it as a pass-through network. So, let's jump into it. The customer gives us this little blurb of data. What domain, who they are, and what domains do they want to handle. We hit our service with it, and we make this call. It's a lot of go code up on the screen, but let's run through it kind of slowly. First thing is some table stakes parts. What is the distribution and what are we gonna name it? Really straightforward. We translated that customer information about their parameterization from what they told us into the calls that need to be made. And then finally, we end up with the domain itself of what we're, what domain are we actually gonna use. We send that information through. We make this API call, everything goes smoothly, and we get a distribution, and we get a tenant in the distribution that we made. But I glossed over this part. This part is really gonna be about that TLS certificate that we have to do. Here what we're saying is that. We host this domain, it's not a cloud front domain. Please give me a TLS certificate for it, meaning that I don't have an existing ACM for it. So this is what it looks like in the console. Domains are registered but can't be used. One of the great improvements though is the HTTP validation that we got with uh SasS Manager. This means that we can rely on Sas Manager and ACM to work together to give me back what I need. Here you can see what that means though is that it tells you certain paths that you need to respond with certain content on. That's just proving that you can do this domain. So, the way we do that in the API though, is this uh get managed certificate uh call. What we're gonna get back from it is this block of JSON essentially. And what you can see is that this is the same information that we saw on the console. In particular, all these redirects. We say, hey, we now know what we have to put into the thing. In production. In particular, we get the URL back that we have to redirect to, so they're gonna make a request. We have to 301 that back. One of the things that is really helpful in Netify is that this is bread and butter table stakes for us. You can set up static redirects. There's a bunch of ways that you can handle this, but for this, we just set up a static redirect on the site. This means that when the request comes in for ACM asking us to prove ownership, we'll serve it back a 301 right away from our edge. So that lets us work with ACM to say we do actually own this domain, please trust me. And we get a certificate. With that, We will start pulling a little bit, that same get managed details calls, and it'll take the pending validation into issued. At this point, I have all of the components that I need. To build up and start serving site. I've got the distribution itself. I've got a valid tenant and a valid certificate. The only thing missing is I have to link them all together. That's just one more call into their API that just says, please update that tenant with this certificate. The biggest part here is all about this customization and saying, please, for this tenant, use this certificate. This lets us override like we were seeing in the demos earlier. And now we're up, we're live. We can take traffic here. That's awesome. We're a SAS provider, we have to prove that we can do this. We can't just tell customers for sure it'll work, we have to show it works. So, I reach for my good old friend, Curl, and we make a request. Their stuff is running to our thing, we give back a 200, we give back a let's encrypt certificate. The site is live and working. What I'm gonna do though is actually force a request through to SAS Manager to also verify. Here, the biggest part is this resolve. It overrides DNS for this one request, and what we can see is that with this, I did hit SAS Manager. I get back the uh ACM certificate that we talked about and some headers that are uh cloudfront specific that I biograph this problem to one day I will be able to suppress. So all of that's up and working. Um, we've got the distribution, we can serve traffic. All of this stuff is valid. Now I can work with the customer to figure out how to move their domain. This can be a, you know, hand in glove situation. This can be documentation, this can work with the connection groups. There's a bunch of options, depending on how they need to set up DNS. If I control their DNS, which we can do, I'll just move them. If I'm not, I can just give them recommendations for how to do it. And at the end of this, what we really end up with, this automation lets us bring up new domains in under 10 seconds. Usually much, much faster than that actually, but that was on my slow side. It scales up to thousands of tenants by default. There is a soft limit around 10,000 tenants, but you can go over that if you just work with your AWS team to kind of get your quotas raised up. Turning that page I wanna look at What's coming up next? What what are the features that we could make and what are the capabilities that we are able to do? In particular, let's set the stage. This is where we're at. Viewer hits cloudfront, passes through their network, hits my edge, and then we handle that request. With this new system, we actually unlocked a few capabilities. In particular, we can now start working with Cloudfront functions. And what that lets us do is a lot of power before we hit cache. A little aside on cloud front functions, they run in the cloud front pop, essentially as close as you can get to the viewer. Start times are in sub-milliseconds usually, um, allowing you to scale to tremendously high volume. This has a couple of trade-offs. In particular, it runs a limited JavaScript runtime, meaning library support can get really challenging, but ultimately is surmountable. It also lets you touch all of the parts of the request. Almost all the parts of the request actually. You can mess with your cookies, the headers, the URL pads, anything like that. You just can't touch any of the bodies, you can't rewrite them, or you can't modify the body themselves. And they're configured per distribution, meaning that it can be really challenging if you wanna have multiple ones or if you wanna customize the behavior for tenant. My recommendation is to really write your code there as generically as possible, and use tenant configuration to customize the behavior. So we're gonna give an example. Your viewer comes through, we hit a cloud front uh function, and then pass that back through. An example of what I'd like to do is have authenticated URLs in each site. So for me, it might be slash private and I put a bunch of information under there, for you it might be slash me. That's a configuration that the customer would tell us, we pass through the tenant, and we're able to use. And the secrets have to be per tenant. This is to control bla blast radius and leaks. So if there's an issue, some secret gets out, we don't want it to go to everyone. And we wanna put authorization as close as we can to the user itself. That's because the, in my experience, the moment that you put something behind some authentication layer, it becomes a ripe target for attack. Somebody is going to try to get past that authentication. And so in the resource war, we want to do less of it, so earlier in the network, the better we can do that. And the way we can manage it is actually a pretty straightforward little JavaScript function. What we're doing here is we're gonna take the information out of the request, we'll get its off header, things like that, take information out of the tenant, and then use those to make the decision. If it is, if you're valid, we just let the request continue on, no problem. If not, we can bounce you at the edge. The unique part. is here with Sas Manager. It allows you to query that tenant parameter directly. So during that setup, when we're talking about parameters, you can put things in there and then query that data back. That lets you in your cloudron function, leverage the tenant's configuration for very generic code and then manipulate through it while having a singular or small set of functions. Another use case that we tend to look through. Is leveraging that network in front of us. In my experience, we really end up with two big flavors of attack. There's a ton that happen, but you end up with somebody hitting a single domain, often a single path in that domain, uh, trying to just overwhelm it, just really hammering that endpoint. Then we end up in a lot of fuzzing ones where people are gonna be walking across different domains or different paths and really just trying to explore what's available and maybe overwhelm you because they're trying to see what your uh resource utilization is for the I don't know answer. So, in both those cases, we're actually able to leverage AWS's WWAF capabilities. I can associate with a tenant, specific web apples. I can then bounce people from that particular person. This lets me contain the blast radius of any IP bands. Often IPs are shared for a variety of networking reasons, and we don't want to block them for the entire network because we're gonna impact a lot of sites at that point. Instead, I want to block it for that one site at that one time. But then we have this problem of fuzzing attacks and people trying to walk across the network and figure things out, and when we can identify those, because we have a whole bunch of data about like your path and behaviors, we can then say, we're gonna update a tenant or a distribution level web Ale and just remove them from the network completely. Both of these let us have a lot of capabilities to respond to the different threats in very different ways. And on top of that, because we can associate different web Ales and different firewall capabilities with different tenants, we're able to then offer enterprise grade things. We can integrate more and more of the features of the firewall capabilities per tenant, and then work with the customer for what their requirements and needs are. Great, with that. Thank you very much. So let's recap what we've covered today. So we spoke about why multi-tenancy at the edge is fundamentally harder, because certificate handling, your routing isolation, your security boundaries, your per tenant policy enforcement were traditionally not primitive cloud front enforced. Each tenant effectively required its own distribution, making scaling operationally heavy. And that's where Cloud Fron Sass Manager changes the model. By combining your shared distribution with modular tenant configuration, you can scale even up to thousands of tenants on a single edge control plane while providing isolation for each tenant. So that brings us to our call to action. So if you're managing multiple domains, you have that custom tenant logic or tiered user experiences, this is your signal to evaluate your multi-tenant patterns at the edge. There's really no reason to stick with standard cloud-front distributions anymore. Like everything that you get and you can do with a standard cloudfront distribution, you can scale and do much better with multi-tenant cloud front distributions. And as you design your multi-tenant blueprint, think in terms of the core constructs that Cloud from Sas Manager gives you your shared distribution, your tenant parameters, your origin routing constructs, your configuration overlays, and these building blocks, they let you separate what must be isolated from what can be shared. So that you can support thousands of domains, customer tiers, unique behaviors, all from a unified edge architecture. Next, use SUTP validation in Cloud and SasS Manager to fully automate your SSL onboarding or SSL provisioning for your tenants. And Ryan showed us how NetLify does this today. And last, like, start small, like pick a small subset of tenants with the most complex requirements, and then measure the delta in terms of the operational burden, your onboarding time, your config sprawl rate, your error rate, and then if you do, if you have multiple tenants today doing similar things, you can actually simplify your operational pain right out of the gate. And this is how you build your internal business case for adopting cloud from Sas Manager in your organization. So that brings us to the end of our talk. We have 4 more cloudfront sessions lined up for you. So get a picture if you like. There's a workshop on Cloudfron Sas Manager in a few hours today in the same hotel. Man I'll go back So thank you very much for joining us today. We will be by the stage for any Q&A. And if you found the session to be valuable, please take the survey. Thank you once again.