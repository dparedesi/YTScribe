---
video_id: 5-_l3MYJdLs
video_url: https://www.youtube.com/watch?v=5-_l3MYJdLs
is_generated: False
is_translatable: True
---

Welcome everyone, thank you for joining us, uh, this afternoon. Hope everyone's having a great reinvent so far. Uh, my name is Nikhil Kapoor. I'm a principal product manager with CloudWatch, and this session is COP 361, uh, to talk about some of the new launches that were announced this morning during the keynote, uh, in CloudWatch, specifically where we're, uh, helping unify security and observability data, uh, within AWS in a simplified manner. With me I have Avinav Jami. Uh, director of engineering with, uh, a CloudWatch, as well as Chandra, who's director of cloud Solution Engineering with S&P Global, uh, who'll tell us his story, uh, as an early adopter of the solution. All right, so before we get started. Uh Let's, let's just recap and level set on what are some of the goals we have for ourselves to cover in this session. So first thing we'll do is we'll do a brief recap of what are the common challenges we see, uh, that most large companies and enterprises, uh, uh, uh, and face when it comes to just the sheer volume of data, um, that they have to deal with both when it comes to security analytics and observability. And let's be honest, when we talk about sheer volume of data, it's mostly logs, um, a love and hate relationship with logs, uh. So we'll share some of those challenges that we hear from customers, then we'll talk about what are the enhancements we announced today, uh, to help with those challenges and finally we'll try to leave you with some practical tips you can use, not just about how you can use the solution but also you can how you think about your journey as an organization to streamline some of this data management and and challenges that we see with data fragmentation. All right. So, just a quick summary of our agenda, we'll go through challenges, we'll talk about uh the solution, we'll talk about uh S&P Global's journey with the solution, and finally a recap of takeaways and some helpful resources uh that you can uh use. All right, challenges. So a quick show of hands, um, and I can barely see, but a quick show of hands, how many of you have something similar in your organization where you have a security team, uh, observability team, maybe audit compliance team, a show of hands? OK, good, um, good as in I, I was expecting that maybe not good once you go through the challenges, but let's walk through them. Uh, so typically this is what we see, right? So for valid reasons, so we, you have a dedicated security team, uh, whose job is to make sure your company and your customers and their data is protected, um, and they're using a dedicated set of tools which are purpose built for that solution, which makes sense. Similarly, observability teams have their own set of tools and often what we hear more than one tool, um, to get visibility into the operational health and performance of the of your systems and finally, in some organizations at least there's a dedicated audit and compliance framework where all the data needs to land at one place, nobody touches, etc. um, and they have their own set of tools. Now that's great. Like these are serving particular needs. Uh, these are specialists who know what they're doing, so they need to be able to do that particular job well. So nothing wrong with that. But once we go down the stack, we'll see a trend here. Not only are the tools and the people specialized for it, but the underlying data often is tied to that tool and the end result of that ends up being something like this. So what we see here is when there is a problem. And when it spans multiple domains, somebody in the security team needs to call up somebody paid somebody in the your Dev DevOps team or SRE, hey, what happened? Why are you guys not fixing this? And he or she wakes up at 3 in the morning, he's like, what are you talking about? I don't know anything about it. So we run into this problem of manual communication, slowing down insights, slowing down outcomes. Now we peel a layer further. Why is that happening? Why are those individuals not able to see the same landscape, understand the same problems faster and easier? Often it goes back to the tools, like the tools are giving them insights which are siloed to what specific thing that tool was, you know, designed to do, uh, or set up to do. Uh, so the humans reacting to the tool can only react to what they see. And then go one step further. Why are the tools not able to get the same insights? So let's take an example. You get, you know, all of a sudden security team is seeing a whole bunch of denials through their firewalls, VA, or what have you. At the same time, uh, your application team see, oh, I see a whole bunch of traffic spike. I don't know what to do. What's going on. Um, now the security teams knows there may be a DDoS attack. The application team doesn't even know they're trying to, you know, scale up their infrastructure. Oh, we maybe have a whole bunch of customers. Wouldn't it be nice if they could all see, hey, there's a security DDoS attempt going on instead of me trying to scale up my infrastructure. I need to isolate that particular IP subnet or whatever that is, um. So, so that's kind of going back to the people and the tools they are using are not working across a common set of information. Right, so The last bit of this layer is actually the most interesting. When we peeled in, I'm like, well, how is this data source and because underlying data, if you notice, data sources are largely the same. You have your AWS service logs, you have Cloud trail, you have your application logs, you have some third party CrowdStrike, Octa, what have you that you use. Um, so all these log sources are largely the same. It's the same data, uh, being plumbed into different places. And this, if anybody here is a developer that manage these plumbing pipelines, by the way, um, that sad face is my empathizing with you. Um, so when, when we looked at several of our customers, they were like, oh, we have the same logs. Cloud 1 needs to go here, also needs to go there. Often this copied from one place to another and all these ETL pipeline spider web of, um, uh, information just going from one place to another back and forth. It is a nightmare to manage, and every time you need to onboard a new workload, a new region, and it's the same thing, it just creates complexity. So that that's what we are here to discuss. So these are the high, the high level challenges. How do we get comprehensive insights faster to your teams? How do we increase their ability to detect and react to those problems that DDoS attack example, for example, uh, that I mentioned? How do we reduce operational overhead of these, you know, spider web of plumbing that you do quickly, how quickly you can enable new workloads, and finally. Maybe not the least important, um, is cost. All of this data duplication across 3 different stores, 5 different stores, same data, like not to mention oftentimes the same VPC flow logs, same cloud logs, uh, being replicated in several different places. So that's kind of the goal we set out, set ourselves, um, when we started on this journey. So, how can we help? So to walk you through this, I'll invite Avinav uh on stage. We'll walk you through the solution and tell us how we can help. Thank you, Nick. Thank you, Nick. So, uh, especially on the previous slide when we talked about operational overhead, that's been something that we have experienced firsthand, and that sort of like adds into managing your own ETL pipes, loading data, managing the versions every time the data load gets stuck, figuring out how to do the backfill, and so on and so forth. It adds into all the other aspects as well, like introducing more cost and so on. So we started first by by rethinking how we would How we would um figure out a solution for our customers that simplifies this, and we think we thought about how to blur the lines between observability, security, and the audit space. Well, blurring of the lines here doesn't really mean using a single tool for all of your different personas, nor does it mean uh having the same set of engineers operate. Different business problems, but we thought harder about where the problems really lived. We realized the problems actually lived in that underlying data store layer, which is where a lot of the infrastructure heavy lifting happens when you try to create data stores that are comprehensive, that are consistent, that have all the underlying log data and telemetry data until you can get to the point of using the tool. So we, we thought about blurring the lines in that data store space and help you create a unified store and come up with ways in which we can build flexibility into CloudWatch with new features and new capabilities so that you can build that unified store. Taking one step further, uh, if you want to build that unified store, you need to start thinking about what are all the steps in order to do that. The first step here is to start collecting all the data. The collection phase usually involves bringing your logs from your AWS services, from your applications, from all your third party into a single spot. Well, uh, this is not just about getting the data in the right way, it is also about making sure it consistently arrives in the right way. Next is Curation. So in your curation phase, you're trying to shape the data to the analytics needs, which means you're thinking about what fields are really valuable, what kind of dashboards will I end up putting, what kind of queries will I end up doing, what kind of information is needed to make that effective? How can I enrich my logs? How can I make sure the right fields are present in the right form, the transformations that need to happen for it. It also means creating indexes, which help you build fast dashboards or speed up that query that quickly gets you the right information and so on. All of this has to be backed by a flexible storage layer. Storage is where we usually see a lot of the costs happening for our customers. There are different data sets that customers want to retain for longer periods of time, especially for security purposes, and there are some data sets that are more operational in nature, and you want to be able to transform the two data sets differently while at the same time keeping essentially a single copy. Um, lastly, you want to be, you want to have powerful analytics and insights off of that store. So, you want to be able to quickly look at your real-time logs, uh, where, and also do deep dives that take multiple query runs and so on. So here we introduce Cloudwatch as a unified store with a whole bunch of features that make Cloudwatch. We are single store for security and observability data. So, in the first step of collection, uh, Cloudwatch today is already available across, uh, uh, Cloudwatch today is available for 65+ services. We're introducing, uh, the 30 new services that are going to now be now the logs are going to be accessible inside Cloudwatch. Uh, these 30 services include, uh, services like Network load balancers, Cloudfront, Petrock Agent core logs, and so forth. We are also introducing third party logs via a managed connector approach, which means you don't have to write any code to get your logs from your third party sources. So at launch we have 10. It includes CrowdStrike and Octa, and we're going to keep adding more and more third party connectors as the year progresses. Uh, lastly, customers also ask us for, uh, security and governance controls at the organizational level, meaning being able to turn on logs for your cloud trail or your VPC at the org level. So this year we are launching organizational level supports, enablement support for cloud trail, VPC, NLB, and so on. In the curation phase, uh, customers oftentimes have to shape their data in the, in the way that analytics tools are going to be leveraging them. We are introducing out of the box transformers for OCSF and Oel data. Customers can also use our pipelines to shape the data in which they need. They can use Grog processors for custom parsing, for field-level operations, and for string manipulation. And finally And finally, we're going to add capabilities for identifying source and type information that we are already aware of and pass that down through our logs, which means source and type information will now be available inside your vendor telemetry, and you can also append that to your custom logs so you can use tags and add source and type information so that you can now operate at a level which is more at the application level instead of thinking about resources, log groups, and so on. All right. Finally, at the storage level, the flexibility that our customers most desire for is to help control how long the data will stay, the way the data is structured, and whether it will be present in different places. Many customers, especially in the observability space, want to keep data in their observability accounts, but when it comes to security or for central ops, they want to keep data in a single space. So, we're introducing a new cross-account, cross-reg centralization feature, which brings your log telemetry into a single spot with the help of rules that you can maintain. You can also create separate retention. You can transform the logs as they come in. All the enrichment features that I talked about, they can apply to the centralized store differently from the source stores. So you can create versions of the data that are best suited for your security or your observ needs. Lastly, any sort of unified data store has to offer a bunch of powerful analytics. Cloudwatch today already supports a lot of strong features like insights, life tailing, being able to provide top-end analysis, and so on. Now, we're introducing two more capabilities that are super exciting. The first one is being able to create S3 tables, uh, essentially, Connecting you with any analytics engine of your choice that you can now use. And the second one is facets. Facets is the next generation of our indexing feature, where we, where you, where you get essentially an out of the box experience for different fields when you go into the insights on the query explorer page. This lets you start that first deep dive, you know, when you're like not sure where to start a query. You go to a query you go to your insights page now and you see a field to write query. But after you enable facets, you can then go in and see field values. So, facets that are like at the error level can now help you figure out where exactly my errors are happening. Then maybe you deep dive into it further and find out, oh, I need to look at my service level facets next. And as you click through each of those facets, the page keeps on refreshing without you having to write any query. Uh, within our experience, we found that a lot of our initial deep diving is now easier because there is a place to start. And then by the time you need to write more complex queries, you can use features like query gen to quickly figure out, uh, to quickly give it prompts and come up with a new query. So to quickly put together the the list of features, we thought about our cloudwa store. We reimagined it with the opportunity to make it much more flexible, to make it much more compatible to the needs of the user, instead of thinking about it as like a data silo, make it more compatible with the features that they're using. So we went through the collection phase, added a bunch of features there, including org-wide enablement, including additional support for news sources. We made pipelines a feature available, which is for transforming your logs and for enriching your logs. We operated at the storage level, creating a centralized store and uh giving you open access through S3 tables, and then finally, uh, powerful insights that you can now connect to the SC table. At this point, I'd like to bring in uh Chandra, who will help us, uh, who will talk through how his team has been using uh Cloudwatch. Thank you. Thank you. Hi, everyone. Thank you for joining us, um, for a 4 p.m. session. I'm really surprised it's full house. OK. So, let me start off, start, start with the question, right? Just a quick, quick show of hands. Uh, how many of you here know about S&P Global and what we do? OK, that's surprising. Just one person? OK. OK. And then like you rank all those, like you have a bunch of stocks and all those companies' portfolios, and then you rank them, they come in, they go out. Good, yeah, very close. Yeah, so, uh, how many of you know about S&P 500 index. Almost everybody, right? So, that's interesting, that's expected, right? Because, see, uh, directly or indirectly, most of us here in this room and everybody are getting the benefit of S&P 500 index, right? So, having said that, so I'm, my name is Chandra. I actually lead um cloud solution engineering uh from the corporate division uh at SNP Global. Uh, I'm here to share some of the journeys that we took a few months uh back about our, you know, logs integration, uh, journey. I hope that, you know, it would help some of you want to take the same path, right? Uh, with that said, I'm going to quickly introduce About the company So SNP Global, uh, we are a global company, uh, formerly known as McGrahill Financials. Um, 40 in back in 1800, all right. Uh, we are, um, leading provider of benchmarks, as somebody mentioned it. Um, data and solutions for financial markets across the world, right? We are strong, we have a strong brand, uh, brand over 10 years. At this time, we are around 40,000 employees serving globally, our clients. We do have 50 offices worldwide, 10 data centers, maybe surprising for you guys, 10 data centers basically we use for network gears for mainly on the connectivity part. So moving on, the cloud scale, because we're talking about the logging here, um, when you're talking about the cloud scale, how much of, you know, what is the size of, uh, S&P Global, uh, quickly across the globe, we have strategic, uh, AWS regions to serve our clients within that proximity. Then that actually adds up a large number of accounts, then that adds up extensive BBCs and cloud resources, etc. Then that actually brings substantial volume of logs data. That's the focus of the discussion plus our journey, what we took. Moving on, When you were actually working on the, you know, um, Revising our existing log pipeline. There were, there were also a lot of requirements our stakeholders was asking. So some of the requirements I'm going to share here, some of them are actually common. One is raw logs in a dedicated log archive account. Sometimes we also call it a central account. This is mainly coming from our legal and compliance team because they want the log, they want to see which are unaltered, unaltered, immutable because of the legal purposes. Next one, curated logs in a separate dedicated security tooling account. This is coming from the cloud security team and we call it an information security team. They want to see and also they have to have the visibility on the cloud security, what logs we're collecting and who is accessing what, and so forth. And the next one access logs locally. This is we frequently hear from the business units. We need to, while you're actually taking the logs central location, we also need to have logs locally. We should be able to search. We should be able to access logs for troubleshooting purpose. So that is another, you know, use case what we have. And then we have security operations team. They want to basically have locks, especially curated for the threat intelligence and then incident response. Then, of course, log querying. This is everybody's need, right? They should be able to easily query, get log access when they need, right? And then finally seam, right? Security information, event management, the optimization. This is very crucial, basically, because when we have so much of, you know, the cloud account, cloud resources, the data gets massive, right? At SNP Global, we are actually using the third party SEA. We can't basically ingest everything into the Seam because, you know, SEAM cost will spike up. So we have to basically curate and ingest what basically we need, the NF log that we need, right? All of these requirements basically adds up to the complexity in the design, right? So this is the design, basically, we worked before we knew AWS team actually working on the new requirements. At that time, when we were looking at the design, we thought, you know, let us ask AWS team if they're actually working on any new capabilities on the cloudwatch. Then we realized that, you know, they're working on it, and we are so excited, then we basically partnered with the AWS team. They're, as you know, they're always available. You can, you can pick up the phone and call them, right? So they come in, partnered with us. They actually helped us to revise this architecture, right, and reduce the complexity. What you see here, when I say complexity, right, if you look at it, uh, here, if you see we have multiple cloudwa subscriptions, we had to, you know, flum ourselves to take from, you know, the local, we will call it a divisional account. To the central account. We also need to send logs to the Security account, multiple accounts that you see here, right? This is basically the security tooling account. This is central account, and we have the complexity. And then one more complexity is in the design, in the design is what we had is S3 buckets. We got to manage so many SC buckets. When we actually, you know, partnered with AWs team, they said we can actually improvise this, right? So, this is how it looks now. It's much more simpler, right? Fewer components. And then cloud wash native. The another interesting part is, one is plumbing is taken care and also it is coming free of cost, right? That's the interesting part. So then, what is the result with this journey, right? So we are able to simplify. With the, you know, basically it reduces the cost and it also simplifies the design plus we are able to also achieve the feder federated model of, you know, log, uh, storage, curate, and process that is basically we achieve the scintallation plus also we achieve the, you know, our all stakeholders requirement, right? And then most importantly cost, as I said, right? So cost will also it is we are at least we are expecting 20 to 25% cost reduction from the design what we have today, right? Then what we found Basically, what we learned, right? The, throughout this journey, um, of our planning and development phase, We learned some valuable lessons. The first one is aggregate with a plan. Basically, don't just collect the logs, have a clear strategy for storage, creation and access. Aggregate logs thoroughly, keep foreclose as simple as possible. Collect and aggregate according to your use cases, that's very important. Do this in dedicated accounts to ensure security and, you know, security requirements, right? And the 2nd 1, OK, 2nd 1 is Meet your teams where they are. Basically, you know, this is where you know the partnership will come into the picture because There are many requirements. Everybody wants a data, uh, data, log data, their own way, right? Meet, meet your team where they are, adapt solutions to existing workflows. Avoid forcing one size fits for all approach. Work with the stakeholders, understand what their need is, then build into your approach. The third iterative, and I think most of the projects what actually we run to the agile iterative is very common, right? Basically it's what we learned is we have to start small, learn and scale. This approach reduces the risk and improve adoption. However, ensure that you have enough luck for your teams, right? Um, with that said, thank you for listening. I'll hand back to the Nick. All right, awesome thank you so much Chandra. That was amazing to hear your story and we'll get back to the free of cost part in a minute, um. So, so hopefully this was helpful. So the, the story that Sandra mentioned, there are quite a few takeaways that even we learned as we're partnering with S&P, right? So some of the things you'll see in the design, and I'll talk about the deployment models came from as we saw how S&P was trying to adopt what things we should factor in from and not just what customers and state would be, but what's the incremental journey. So with that, let me talk about some of the deployment models that we have seen so far, um, other customers that we've been speaking to adopting and how it may help you get started in your journey. OK, before we go to the deployment models, let's talk about costs. Uh, I'm sure some of you wondering all these managed pipelines, all this replication centralization, know how much is it gonna cost. So, so, so I broke this down into, you know, the some of the core features of enough mentioned so facets, the out of the box distribution of values available to you without even ever running a query, not just expedites your analysis but saves you all this, uh, data scan you're doing when you don't even know where you should be looking. They're available free of charge, no additional charge in our standard law class. So for context for folks not familiar with CloudWatch, so CloudWatch has, um, you know, 3 high level pricing dimensions, base pricing dimensions as we call them, ingestion, storage, and queries, data scan. No changes to those costs. They would, whatever the price was, we have not increasing anything there. It stays as is. In addition, in ingestion we have standard uh class and infrequent class and what the difference between them really is what capabilities or processing time capabilities are available in them. So standard classes built more for those real-time operational troubleshooting where you can have indexes you can do uh. You know, more fastt type real-time query experience. You can have alarms and all of that. Infrequent access is more for the logs that the compliance use case, for example. You'd need them to query, but you don't need that, hey, I had a 2 a.m., you know, so I have 2 at night and I need the real-time analysis. So we try to give flexibility by giving half the standard infrequent access is half the cost of standard class. It just gives you the flexibility of using which class is best suited for those logs, for instance. So I'll talk about both, but I wanted to set that context. Right, so facets included free of charge for standard for infrequent access because they're not aligned with that use as I mentioned, they're not supported in frequent access. S3 tables integration. So SV tables integration is an interesting one. As we were thinking about this outcome we wanted where end users should not have to give up on their preferred tool just to use this data store, that was one of our goals that it's not like if you take away that, tell your security teams, oh, you, we'll create a unified store, but you have to give up what you're using, it won't work. So instead, the approach we took was we partnered with the S3 Tables team. Uh, to have native Iceberg Apache Iceberg support, so any query tool, whether it's AWS analytics tools, Athena, Redshift, EMR, what have you, or any other Iceberg compatible tool, uh, can query this data, uh, in place. So, SU tables integration, no put, replication, storage, table maintenance, no additional charges. It's included in your cloud watch pricing. Um, again, you only pay for the queries you run on S3 tables, the reads get the typical query cost, uh, no additional cost for that as well. It is supported in infrequent access, same model as standard, no additional cost. Log centralization. So this was, uh, launched, uh, a month or so ago, um, so log centralization, you may be wondering, oh, I have, you know, Chandra talked all these great things about local teams, divisional teams still have access to their data. Now I centralize in one place, and I centralize again for security in another place. That's all this ingestion and duplication. How was, how, who's gonna pay for that. So no from an ingestion replication standpoint, no additional cost for first copy. So first aggregated copy is free. Second copy you only pay 5 cents, which is basically the data transfer cost, no additional ingestion cost. So keep in mind you have full flexibility of re-ingesting, reprocessing those logs in those central accounts as well. But no additional cost for the, uh, you know, first copy and 5 cents for the second copy storage charges still apply in addition because at that point you have replicated, you have independent storage, you're managing retention independently, so this is an important one to note. So when you centralize, you don't wanna have 5 years of retention across 3. Copies like pick what is your central archival account like Chandra mentioned if that's where you want to have long term storage, that's where you wanna keep the real retention in your, you know, local accounts developers testing lambda they don't need 7 years of logs. They just need Live tail to work for a couple of days, right? So lower the retention in local accounts. Pipelines, the transformation enrichment again included at no additional cost. Or 11 enablement, no additional cost. You just pay for the same log ingestion storage that ends up, uh, which you was already included in closure pricing, um, so, uh, also supported in infrequent access. Uh, I, this is hopefully I'm trying to paint a picture to what, um. The comments that I was making is free of cost. This is where like all these features we wanted to build in a way that customers can easily try out, see, iterate without having to think so much about, oh I enabled this one thing, what's gonna, we don't wanna do that, so. So that's the cost. OK, so now that we have that out of the way, how can we get started? So I'll talk about a couple of deployment models that we have seen customers use. One is um what I'm calling distributed centralization. This is the common example. So at the bottom, uh, you see all your log sources that you had already going into CloudWatch, um, whether it's individual accounts, for example, VPC flow logs across, I don't know, 500 accounts going to those accounts because those teams need them too. And same thing with Cloud trail, maybe your local teams also want cloud trail logs and your central team wants them. So you enable all those logs. They are available in local accounts. In addition, they get in a fully managed way behind the scenes replicated to an observability account and whichever logs you want, you can pick and choose also to a security account so you can decide, hey, these logs are only relevant for security. They only go there. But so now at this 0.3 teams. Are using those logs in the way they need without compromise they can transform and enrich independently without impeding each other's workflows and being able to use the tool of their choice, whether it's CloudWatch, whether it's Athena, Redshift, some other tool, um, Apache, uh, Iceberg compatible, they can power their workflows without building all of those complex plumbing that they were having to do. So that's one model. The second model, which is similar to what S&P Global has started with, he shared with you, which centralize and distribute. So this is the model where you bring everything in one central account, um, which serves as your compliance, you know, one single source of truth, but also serves as a single place for you to fork what data needs to go to what other place, right? So instead of having 200 accounts, plumbing this here, plumbing that there, it's one place and across regions, by the way. Not just across accounts. So if you operate across, you know, for disaster recovery or what have you, 23 different regions, you can do that all in one place and centralization offers a backup region as well. So when you centralize across regions, you have the option to create an active, active backup. So in case something happens to one region, you still have access to your data. Um, so this is a centralized and distribute here everything is one place and then you decide how, uh, where to, if you wanna send some logs to the same solution, you can filter what is needed, don't send what is not needed. Alright, so these are the two different, um. Deployment models that we have seen customers use, um, so with that, that's enough talking. I think we should do a brief demo at this point. A lot of slides. Let's switch All right, so in my setup, so I have a demo in my demo set up it's, you know, largely trying to represent what I would typically expect most production grade applications to have. So I have multiple accounts trying to, so I have a PayStream payments company if you will, who's processing payments or for a large variety of their clients. I am operating across multiple accounts and regions to be closer to my end customers, um, so I am seeing I ran a payment and I'm seeing some errors, so some payments are failing. Now, what would I do in this state? So if I get a call, payments are failing as a developer, SRE support person, the first question I would have at that point is, where do I start? So I go to Cloudwatch, I land on this page, right, so logs insights page. Now, I could start by trying to map, oh, this region, this account, where do I go and open 15 different tabs to get to that outcome. Or what I'm doing in this case, These are the facets that we were showing, so I'll zoom in a little so you can see. So when I log into this facet. Without even running a query, I can do two things. First, I can see, are there any errors happening? OK, I see there are some notification failures, some payment processing, some validation failed. I definitely see some errors are happening here. So if I now go to the uh which which service should I troubleshoot? So I'm gonna payment process is the service. So I'm gonna narrow down before I look at all the different events, I'm gonna narrow down to the service. So as I selected it without running a query it updated this other thing that I was interested in live for me. So I see there are definitely errors. So this is an example of what otherwise I would have done 2 or 3 different queries to just confirm what I just showed you here, just by collecting this is a service I'm interested in real-time it showed me in the last hour, we had 690 payments failed. Well, that's good. So I confirmed there were payment failures. I can now validate where are the failures happening. So what I'm gonna do is, now I'm gonna run a, this is the time to run a query, but now I have zoomed in my scope. I don't know how to run a query across everything in my uh logs. I'm just gonna try to understand what could be going. So I have a pre-built query here. I'm gonna select the facets to run this query. Payment process. Not all of them, just one service. Payment process and the event type I was interested in was, actually I want to see where the successes are happening too. Let me just select both. Success as well as Maybe the validation failed. All right, so let's run this query. And see what kind of results we get. I'm gonna make this bigger. All right, so I can see from this. That of all my, so you can see I have uh different accounts across different regions on the left across my entire real estate by having logs centralized in one place, not only was I able to analyze them in one place without opening tabs, but I was quickly able to isolate all the errors, this invalid payment failure reason, what have you, I isolated this account in this region. Now I can jump to my next query to understand why this could happen. Uh, what could be happening is, you know, now I'll, you know, query my cloud trail logs. Um, I'll select cloud trail here. So the other thing we introduced, which Avina mentioned is data source. So now instead of thinking about log groups or resources, I know I'm interested in figuring out, looking at my cloud trail logs. So I'm gonna just say cloud trail, um. Just select cloud trail. I don't need to think about where they are. AWS have already mapped it for me, run this query, and it'll show me what happened to this lambda function, and I'm sure somebody made a change that they didn't intend to. Um, let's see, something happened. Oh, somebody did make a change. They changed some parameter from 128 and look at that, somebody named Colprit. I wonder why that is, um. So, so that's the example of simplified analytics, and it's all like if you notice the query capabilities are not that dramatically different outside of facets, but by bringing the data, curating the data the way we were able to in one place, it opens up all these faster and better insights. All right. So that was the inside part in Cloudwatch. What about this other tool thing we mentioned? So, what I'm gonna do is, I'm gonna switch to SageMaker Unified Studio. Uh, how many of you are familiar with SageMaker Unified Studio? OK, a few people. So I'll walk to SageMaker Unified Studio is AWS's native analytics tool, right? So it gives you ability to run, you know, advanced analytics and data, Jupiter notebooks and what have you, Athena queries, whatnot, um, and S3 tables, when you have, uh, are using S3 tables, um, they are, they are natively integrated in the catalog. So when I go to my SageMaker Unified studio, I can see my custom catalog as well as any SV table catalog. So in this. By having the S3 tables integration set up in CloudWatch, all the CloudWatch logs that I have associated with this, they show up under this managed S3 T bucket, AWS CloudWatch, and all the log sources I've associated are already here. So Now, going back to the scenario of my demo, which I was showing the payment processing, I have figured out somebody made a change in lambda, I broke the problem. At the same time, My product team, people like myself are bugging my engineers to say, hey, we need to know which customers impacted so we can reach out to them, figure out the impact we need it now. Now while I'm trying to do all of this fixing the actual problem, I'm also having to deal with all these people who are trying to get, you know, they're trying to do their job but in the past what I had to do is export data live, give them so they're unblogged and do their thing. Now I don't have to. I just say, hey, the logs are available in SageMaker go find which customers are there equating and they can analyze and connect with their. Business data in other S3 tables or S3 buckets in Athena or Redshift and find that customer usage pattern or who's impacted themselves. So, different personas, same log, getting the insights. All right, um. One last thing I wanted to show in the demo. So one other thing we did from a just a simplify simplification of management at scale. So when we're operating in a single account, you know, log groups resources make sense, but when you think about like Chandra was saying across several 100 accounts over several 100 regions, it can become cumbersome. You need a larger view, a higher level view of your data. So in CloudWatch now we have introduced this new log management experience where in addition to the log groups and whatnot that folks are already used to and familiar with. A, you get a high level summary of all of your data so you can see what was ingested, where, who's taking up, you know, suddenly if you have a spike in something you can quickly identify it here what was the most recent run queries, what configurations are set up, what data protection policies are enabled or not enabled, so it gives you a top level view of, you know, what things are working or not working just from a pure data management standpoint, uh, natively in CloudWatch. Now from analyzing different data sources. How many times in this presentation we mentioned data sources should tell you how important connotation of data sources is. Uh, we all operate at, hey, this is crowd strike data or VPC flow logs or cloud trail logs, so we wanted to make that sticky and easy for you to identify them. So we have added this data source based experience where all most of the AWS sources are automatically mapped for you. You don't have to do anything if you have them enabled, you land on this page, you'll see them already there. Not only that, they are also schematized by default, uh, when, when schema is available. So when you're trying to analyze something, you know, you'll be able to see what fields are available or not available in place. So here in this case, Bedrock run time, it is by default sending me these four fields. I don't have to think about what they are, they're available, and this is important because in, in the iceberg experience, we wanna make that those columns and fields as accessible as possible. So we build this whole scheme schema detection experience on top of that. And you can from here, so if I go back here, you know that S3 tables integration is active. Now if I go back I can enable this S3 table integration for one data source if I wanna be selective on what I exposed to that product manager who I don't know what he wants to do, or if I want to have a use case where I wanna expose all, I can also manage an integration and expose all of those data sources in a multi-select manner. This is a one-time operation. Once you enable this integration, any future data for those data sources will automatically be mirrored and available for you in the tool of your choice. All right. That covers the demo part. Now, let's go back just to do a quick recap of everything we talked about, um. All right, takeaways. So, uh, number one, hopefully we all agree data fragmentation is a problem. Um, data fragmentation is a problem. It's a complex one but an important one for us to solve. So we had this, you know, we discussed what kind of challenges and problems and data fragmentation, it can be within your company, can be within accounts and region and AWS, could be within use cases, um, but different levels of data fragmentation. Our goal is to help you eliminate as much as we can. Second one is an important one again is. Meeting your teams and users where they are is an important part of this journey for our companies and our teams to be successful uprooting them from what they're used to and their workflows is disruptive, so we wanted to minimize that as much as we possibly can, uh, while still trying to limit the duplication and fragmentation of data. And finally this is, this is why we wanted to talk about both, you know, S&P Global's journey as well as some other deployment models. It, it, we've built this feature or set of features. The reason we have built them in these building block manner and not just one enable unified store and it works because we realized as we were talking to customers, different customers want to consume different things in different way. So rather than giving a very opinionated one size fits all solution, we have given you the building blocks and we encourage you to evaluate which features capabilities meet your needs at what stage. Maybe today you only need centralization of laws. You don't need pipelines, curation, third party sources. That's OK. That's a good start. 6 months later you may realize, you know what, Mike, now I can bring in additional data sources from third party that fit my need. You can start there. So that's the idea is, is, is is important to identify the right starting point where you are and where you wanna get to and iteratively get there. All right, so that was a lot of talking. Uh, there's a couple of QR codes here that may help. So one is the news blog which talks about a lot of the features, how you can get started, various links to, you know, uh, other resources available to get started on all these features, and on the right we have a broader our AWS observability resource guide if you will. There's all these resources about AWS observability, uh, a lot of things we didn't even get to cover here available to you. I encourage you to look into those. With that, thank you so much for coming. uh, we appreciate your time and I hope you found this useful. Please give us feedback we appreciate the feedback. It helps us learn and improve. Thank you.