---
video_id: xc9P6wbhLwE
video_url: https://www.youtube.com/watch?v=xc9P6wbhLwE
is_generated: False
is_translatable: True
summary: This session explores fundamental integration patterns for distributed systems, focusing on four critical pillars that architects must understand when designing system integrations. The speakers, Maximilian Schellhorn and Dirk, both AWS solutions architects, begin by establishing that modern systems inevitably require integration with internal, external, and even non-deterministic systems like AI agents, making integration architecture essential regardless of whether you're working with monoliths or microservices. The first pillar examines coupling dimensions, challenging the binary notion of "coupled vs uncoupled" by demonstrating that coupling exists across multiple spectrums including location, format, temporal, and domain coupling. Through practical examples using REST APIs, asynchronous communication, message queues (Amazon SQS), and publish-subscribe systems (Amazon SNS, EventBridge), they illustrate how different integration patterns shift coupling responsibilities between producers and consumers, introducing the concept of topic-queue chaining to combine the benefits of both approaches. The second pillar addresses control flow versus flow control, distinguishing between data flow direction and control flow direction, particularly in push versus pull mechanisms, where queues enable pulling messages which provides traffic shaping capabilities, load leveling, and simplified networking requirements, while push-based systems offer real-time notifications and reduced idle polling loops. The third pillar tackles the heated debates around delivery semantics and message ordering, systematically debunking the myth of "exactly-once delivery" by explaining that network glitches and acknowledgment timeouts make producer retries inevitable, leading to at-least-once delivery as the practical reality, where deduplication can only occur within defined scopes and time windows rather than indefinitely. They emphasize that maintaining strict total order from beginning to end is resource-intensive and often unnecessary, advocating instead for partial order within defined scopes like message groups in SQS FIFO, and introduce the concept of causality through happens-before relationships with metadata like version IDs or monotonic clocks to reestablish order when necessary. The fourth pillar explores multi-tenancy challenges, specifically addressing the noisy neighbor problem where one tenant's high message volume impacts others sharing the same queue, presenting a spectrum of solutions from single multi-tenant queues to per-tenant queues, with the optimal middle ground being shuffle sharding which assigns each tenant to multiple randomly selected queues to minimize blast radius, ultimately introducing Amazon SQS FairQueue as a managed feature that automatically implements dynamic overflow queues without requiring custom engineering. Throughout the presentation, they reinforce the central theme that every architectural decision involves trade-offs, there's no silver bullet solution, and architects must consciously choose which coupling dimensions to embrace, which delivery guarantees to provide, and how to balance tenant isolation against operational efficiency, always being aware that whatever choice you make, "there's always something that sucks," but the goal is selecting the option with the least pain for your specific use case while avoiding the "faith healer principle" of believing any single pattern solves all problems.
keywords: integration patterns, distributed systems, message queues, coupling dimensions, multi-tenancy
---

Welcome everyone to API 315 integration patterns for distributed Systems. My name is Maximilian Schellhorn. I'm a senior solutions architect working with AWS for customers in Germany, and today I'm joined by Dirk. Hello everyone, I'm Dirk. I'm a principal solutions architect with AWS. I also work with software companies on their multi-dimensional transformation, which includes on the technical side, one of my hobby horses, integration architecture, hence the talk today on the transformational part, I'm very much interested in innovation culture and tech communities. Turk and I, we have both been working on distributed systems over the last couple of years, and we think it's safe to say that modern systems do not live in isolation. So modern systems communicate with each other. We might talk to internal or external third party systems, and lately we have seen even new forms of integration evolving where we're talking to non-deterministic systems such as agents, for example, with even new protocols such as A2A or. And this is why we think integration architecture is relevant for everybody, no matter if you work on a monolithic application or maybe one of your components in your monolith has given you some headache and you've started to carve it out. Or you already have a divide and conquer architectural style at home like a microservices landscape, you will always have to integrate with other systems, and it also depends on how much influence you have on those systems that are to be integrated, what kind of mechanism you use. Like if we see the hoodoos in the orange circles that might be your own services and you have more influence, and the ones in the blue circle are far more distant. Might be a self service or so you have less influence and that also determines a little bit about the integration architecture. Now. An integration architecture is often also a journey, and you want to be well prepared for your journey. Like if you want to go on this bumpy road we see here, you want to have probably a 4 wheel drive high clearance vehicle, and in the same way you want to be prepared on your integration architecture journey and are well prepared with good knowledge about integration patterns and the. And that brings us to two insights. The first one is I only do these public speaking activities to show off with my nice vacation pictures. And the second one is that integration architecture cannot be an afterthought. And that is actually not a new insight, Max, right, because it has always been the case that you kind of have to integrate something now. How difficult can it actually be to connect two systems with each other? Drawing a line? Yes, it seems like it's just a talking to B, but if we look closer, we cannot really see if A is actually sending messages to B or is B retrieving something from A. It's not so clear, right? We should also make up our mind what we want to achieve here, and we should maybe put some arrows, but then, The main thing that challenges us is to answer all these questions here. There are so many options when it comes to integrating systems with each other, and all of them have their own trade-offs. They are beneficial for a certain situation, not so much for a different situation, and you typically pay for the benefits that you get from those with additional engineering effort or cost. Which brings us back to my favorite insight that no architecture decision that you take comes without trade-offs. Or in other words, whatever you do, there's always something that sucks. But as a software architect, you should be aware of the options on the table and then pick the one that gives you less pain than all the other options. Now we saw, we've seen already there are so many design decisions that we can do, and then we could probably talk for hours about how to build distributed systems. Now we needed to make a decision. To extract 4 fundamental pillars that we found most interesting for today that we brought and we're going to look at those from the standpoint of integration patterns and the first one is coupling and its dimensions. Then we're going to look into control flow and flow control. We are going to look into delivery semantics and message ordering. And last but not least, we will also look into what multi-tenant architectures can bring us in terms of integration. So let's dive deep into the first chapter. Thank you, Dirk. So Dirk, I don't know about you, but I personally I need stability in life. Absolutely same here, but I also need stability when building software, so I was looking for some guidance and luckily I found Mr. Larry Konstantin saying that the structure is stable if cohesion is high and coupling is low. So we have two things cohesion and coupling. And cohesion is the degree to which a module's internals belong to each other. So this is something that domain driven design cares about a lot, right? Having an order service, we want to make sure that most of the things related to the order stay within the very same module. On the other hand, we have coupling, right, and coupling is essentially the interdependence between systems, so the degree to which a module depends on another module. And the most important key takeaway is that this is not a binary state, so we cannot look at an architecture and say this is coupled or uncoupled. It's really a spectrum across a lot of different dimensions as we will see later. And if we tweak one dimension, we might actually introduce another form of coupling on another side. So to avoid having a philosophical discussion, let's actually look at a concrete example. So you probably are all familiar with synchronous communication like a standard rest API for example. So here we have the booking service talking to the payment service and it takes some time to process and then we get a response, right? And now let's see what kind of implications does that have on coupling. So first of all, in this rest API set up we are location coupled, right, and location coupled essentially means the booking service needs to know where the payment service actually is, like some form of IP address or or something like that, so it needs to be addressable. The second dimension we want to introduce is format. So basically the payment service decides on how the other services are communicating with it. Is it the REST API? Is it Jason RCRPC, XML, whatnot? So we are basically coupled on the format that the downstream services dictates. The third dimension is temporal coupling, and temporal is essentially a timing dependency in two dimensions. One is availability coupling in that case. So booking service relies on payment service to be available. If payment service is down, booking service cannot fulfill the functionality. And also we are coupled basically on the processing time. We need to wait up until the payment service completes its actual processing. And last but not least, we are domain coupled to some extent, and this is usually a form or a dimension of coupling that we see a lot happening because usually we have multiple systems we want to communicate about something, so there needs to be some understanding in the booking service about the payment service domain to actually make that call and trigger some meaningful business logic. And This is totally fine to do. So it's totally fine to do. We've been doing that for quite a while, but there might be challenges if we evolve our architecture. So let's say we have multiple downstream services and we all integrate with them. In a synchronous API fashion, so we basically now introduce a lot of coupling dimensions between those systems, and if the payment service would have another downstream system, for example, we're kind of adding up that effect of having all those dimensions being coupled in between. So Dirk, do you have an idea on how we could reduce that maybe a little bit? Yeah, the fun thing about this talk is that I always need to pay attention even if I'm not speaking because he asks me questions. So what I would do is to ask my friend Kiiro, um, I, I would first throw everything away, ask my friend Kiiro to rebuild it in a GAI fashion. That's how you do it today. Or maybe we start with a bit of a less invasive approach first. So let's see, what if we introduce asynchronous communications? So instead of having a synchronous call, we do an asynchronous call. So instead of waiting for the processing, we get back 202 accepted. So therefore we are not relying on the actual processing time. Now if we bring back our dimensions, that actually didn't really help us with the location coupling is still the same, the format is still the same, domain coupling is still the same. The only thing that helps us is that we do not have to wait until the process is completed, but still from an availability perspective, we still have to have the service available to be able to basically return 202 accepted. So Dirk, do you maybe have another option? I do actually. My most favorite primitive, which is a message queue because message cues decouple, and when we say decouple, of course it doesn't mean to decouple 100% because then systems wouldn't have any interaction anymore. But actually what we mean when we say decouple is to reduce the coupling between systems. The nice thing about queues is it's an additional piece of infrastructure that we put between the upstream and the downstream system and effectively they don't even have to know anything about each other afterwards. It also comes with a very, very handy characteristic characteristic because queues. Buffer messages, that means the downstream system can consume those messages at its own pace, and even more, we still had the problem, the problem with the availability dependency of the downstream system when we used APIs. When we use cues, the downstream system can also have a temporary struggle, and the upstream system can still send messages downstream because we have the queue in between. So that means apparently we are now shifting a lot of operational responsibility to our messaging system. And that means further that we should probably pick one that is very scalable and reliable and available, and this is why I typically recommend to go with Amazon SQS as a cloud native serverless message queue service. But sometimes people are also bound to industry standard APIs like JMS or AMQP. For that you can fall back to Amazon MQ, which is a managed rabbit MQ or Apache active MQ. So Max, why don't we look into our sample architecture, how queues look like there? Yeah, so coming back to what we have seen before with the different coupling dimensions, if we rely on synchronous communication, if we now take that in contrast to how that looks like with message cues, we see that the picture actually shifts a little bit. So the most important thing that you will realize is that first of all we don't have the location, format, and temporal coupling on each individual downstream service anymore, but we are also not completely removing them because we are now location coupled on the messaging system itself. We need to know where the messaging system is. It's also addressable. We are probably on the format we are standardizing via an SDK or like a protocol that this messaging system is introducing. And last but not least, we are now also relying on the availability obviously of the entire messaging system, and that's why it's essential to use something that is very reliable because it becomes a very critical component of our architecture. But you might also realize that the domain coupling didn't really Resolved, so we still basically have a domain coupling point for every message that we send in there. So why is that? Because in our current approach what we are doing is we are essentially sending a certain type of message because we're sending a command, so we basically saying find a ride, do a payment, add loyalty points, so we basically still know about all the downstream systems' functionality because we're instructing them something to do, they expect still a given format so we still have domain knowledge about all those systems and that might be fine, but if we are. Basically increasing our architecture and evolving it, we might get additional downstream services, and if we continue to just keep adding cues in between all of our services, still, the booking service has a lot of knowledge about all the different downstream services. So Duke, do you have another pattern in your toolbox maybe that we could use? I do. My second favorite primitive, which are topics or publish subscribe channels. So the difference between topics and cues is that each downstream system receives every message with topics. That means that the upstream system just needs to send one message one time, and it is duplicated or it's multiplicated for the downstream systems. It also reverses the domain coupling. Previously, the upstream system needed to know about what format and what content the downstream system expected. This is virtually not possible anymore because the upstream system doesn't even know who is listening, and that's quite handy also in this case. We can also look at what service recommendations we have for this one again. The operational responsibility is shifted to the messaging system. That means you also here want to have one very reliable and available solution. And I would personally always recommend Amazon SNS or Amazon EventBridge again as a serverless cloud native service. If you're bound to Apache Kafka, you can also fall back to Amazon MSK as a managed service for Apache Kafka. We should put this into our sample architecture. Yes, so let's see what now happens to our coupling dimensions, right? So this was the scenario before that we have with the message queues, but now we introduced a published subscribed system in between. So first of all, the location and the temporal coupling and the formal coupling do not really change a lot, right? I still have a messaging system, so I still need that to be available. I have a certain format and I need to have some availability dependency essentially there. But you could argue maybe from the location perspective I don't need to know individual message channels anymore, but only a certain topic where I publish my event right booked, and this is my domain now, right? So for me as a booking service, the right booked event is within my domain. And now basically all the consumers decide to couple themselves on my domain because they are just listening to this write booked event and then do their certain actions. So and this is a very fundamental question that you should ask yourself on which side do you want to couple. So in the first example that we have seen with the queues, we actually took the decision to couple on the producer side because we usually do that when we are sending commands because we have some knowledge about the other domain, so we want something to happen and we usually do that when we also need some form of response from that. However, we can also make the decision to couple on the consumer side. So in that case, the payment service would decide, oh, I am interested in that write booked event, for example, so I am coupling myself to that specific event. So in that case we're not talking about commands, but around events, and the producer usually does not care about it. I just say write booked, for example. I don't know who is using that and what they do with it, so I usually also do not need a response from that. And credits here to Bernd Rucker. There is a great book on practical process automation where he covers that basically concept in more detail. So, however, You might actually have the requirement for both in your system, so you might use for one specific scenario you decide to couple more on the producer side while on other scenarios you couple on the consumer side. So we have here booking received and then essentially all those different services subscribe to that and on the lower end we have the. Where we say do a payment and instruct something to do and sometimes we do not even have another choice. If we integrate with a third party system we kind of have to couple up to their contract that they are offering and sometimes even with a synchronous rest. So where is the decision to couple. You look like you have a question, Turk. Yes, yes, I do have a question. So we introduced cues before to reduce the availability dependency, and we got rid of that now, although there was a sneak preview already. I was wondering if we can have the best of both worlds, the availability dependency reduction, also consumer concurrency, for instance, that queues bring you. Do you have an idea? Yes, we can, and I mean it's a very common pattern, as you know, right? So this is. What is called topic queue chaining. So we often see that with S and SQS where we usually use a topic to fan out, but we're not fanning out to the downstream services directly, but we're fanning out to the dedicated queue. So you can basically combine those two approaches effectively. So to conclude, coupling is not an absolute measure. We cannot just look at the different dimensions and say which architecture is better. It's always relative to the producer or the consumers and the dimensions on where they couple and to what degree they do that. So with that, that concludes our very first section. Awesome, we could talk about coupling for another hour, but we need to move on. Next one is control flow and flow control. Seems like you want to be in control most of the time. Yes, that would be actually quite nice, and there's actually also no pun intended. Both concepts are really very important. And let's start this chapter with a look beyond data flow. Why is that? Because in most architecture diagrams that you see. You will only see the data flow, and that has been the case also in our previous diagrams. On the other hand, control flow can actually be in the opposite direction, and you should be aware of your control flow. What we see at the bottom of this diagram is the control flow for pulling messages, and it is in the opposite direction because the downstream system here initiates the interaction between systems, and we can see both things the push flow where the upstream system. Initiates the interaction and the pull flow where the downstream system initiates the interaction when we look at cues once more because here it is exactly like this when a sender produces a message into a queue, it is using the push method and the receiver on the other hand is pulling messages off the queue. That means it initiates the interaction. That brings us to the next insight that control flow can actually influence dynamic system behavior. If we think about cues again, the fact that we have the pull mechanism on the downstream side obviously increases a bit of latency, but that's again the price that you pay for the benefits of the queue, right, the trade-off that we mentioned in the very beginning. The benefits of cues are very many, and we can see it here once again. We saw it already or we talked already about the fact that they can buffer messages, and in that respect they can protect downstream systems and flatten peak loads. So that's a great thing. However, you might also come into a situation where you constantly have fast producers and slow consumers, and that apparently makes your queue size grow and grow and grow, and you want to do something about it. It is not a disadvantage of queues. It is just that queues make this fact explicit and visible for you, and so you can take action against it. So the first very easy thing maybe sounds a little bit brutal, but just throw away messages that have reached a certain age, but then there might be messages that just lose the value after a few minutes. If you have if you have an SLA in the booking system that we saw that something needs to be processed after 5 minutes and your message is already 10 minutes old, then you can probably throw it away because the customer already went away. So while it looks brutal on one hand, it might be reasonable in some situations. And the other thing that you can do is to apply back pressure on your producers, and that can be that can go as much way back as to the end user clients. But also in between if you have something like an API gateway, you can apply throttling there. If you don't have that, you can directly influence your producers, but for that you also need to have access to your producers, and in the end it is more of a business decision than a technical decision how to handle those messages if your queue fills up. So, next insight that we can derive from that cues decouple control flow, but they require flow control. And now we have another very interesting subchapter here, pull versus push. Yes, you already mentioned it in the introduction, right? Data flow and control flow. So let's look at the two examples of pushing versus pulling and if there are any advantages for one or the other. So we will start off with push-based integration, and you might know that from the webhook system, for example. So in that case, the sender owns the integration logic and the sender here is the messaging system itself. So if you register a webhook somewhere, it's the decision of the system itself when to then trigger and send that to your configured URL, for example. Another example is EventBridge. EventBridge is a push-based system, so the producer puts events on EventBridge, and then EventBridge, based on a rule, pushes the messages to the downstream system. And since the producer or the sender is in control, this enables us to have routing, filtering, and error handling capabilities directly at the messaging system. So with Eventbridge rules, for example, I can filter on specific on the message body, for example, I can do transformations and then only then send it to the dedicated downstream system. And because the sender here is in control, it needs to be careful to not overwhelm the downstream systems because it cannot constantly hammer out the messages. So usually you find rate limiting capabilities and backups to not overwhelm those downstream services. One of the most important advantages, however, is that it reduces idle or wasted pull loops because usually if you have a pull-based system you have always someone who says, is something new there is something new there, and now I'm really only sending something when actually something happens and this allows us then to also have real-time notifications. Now in contrast, we have pool-based systems, and in pool-based systems, the consumer owns the integration logic and rate. So we have just seen great examples for traffic shaping from Dirk before, right? So this really allows us to have those queuing and load leveling patterns because the consumer can just decide this is my pace, I want to consume at that pace. I don't go beyond that, so that's very different from a push-based system. And something that is often overlooked are simplified networking requirements. In a push-based system, the sender needs to reach all of the downstream services, so it needs to have the ability to reach them via network or whatnot, right? And in that case, we're kind of inverting that now. Just all the consumers need to reach the messaging system which might simplify your networking requirements. And last but not least, when the consumer is in control, this allows us to have stateful streaming analytics, for example, because the producer can control the offset. It can have a windowing aggregation over a certain amount of time because the consumer is in control about those things. So at a glance, there are several pros and cons on when you would choose the one or the other. It really depends on your current requirements that you have. So, which approach do you prefer actually, push or pull? Well, as a software architect, I need to answer it depends. And to elaborate a little bit more, I want to circle back to the principle to beware of the faith healer principle I mentioned in the very beginning. You need to choose the pattern that fits best with your current use case and that gives you the least disadvantages. So sometimes it's push, sometimes it's pull, and as we've seen in your example architecture before, within one use case architecture you can have both. So it depends, you said, yes, so to summarize it, it depends. Fascinating. So let's move on. OK, so that basically concluded our second section on control for and flow control, and the third one that we want to discuss is something of a very heated debate which is around delivery semantics and message order. So how often delivery, deduplication, and FIFO. So let's start off with how often delivery and deduplication. So Dirk, usually when we ask customers what do they say, what kind of message delivery guarantee they absolutely need exactly one delivery processing. Yes, OK, so let's see if exactly one's delivery is actually possible. But as a quick recap, when we talk about message delivery, what we mean is that a message is delivered 0 or 1 time, then we call that at most once. So we might accept a message loss. We might get a message at least once when we get it one or multiple times, so we might get duplicates, but whenever I talk to customers, they usually say, Max, when we build this messaging system, we can only do it when we get exactly one delivery. So let's explore a bit on a journey together if that is actually possible or we're actually looking for something else. And to do that, we look into what actually happens when we put a message to a messaging system, and usually two things happen. The first one is we need to store the message reliable, and by reliable means on multiple brokers, for example, on multiple disks that if a hardware failure occurs, we still have a copy available to basically accommodate the hardware failures. And after we've done that, we're basically acknowledging back to the producer that we say everything worked fine, right, you're good to go. So let's start with at most one. So when can at most one happen? At most one happens if, for example, we are not storing the message reliably, but we do acknowledge you could say if we only have in Kafka the replication factor is low, if we want to probably let's say prioritize throughput over completeness, so we take into consideration the hardware might fail. I might lose the message, but it might be OK for my use case. The other option would be that we are not waiting for the acknowledgement at all, so we just fire and forget. So we cannot know if the action actually succeeded or not. So in both of those two cases, essentially the producer does not know if the message was received or replicated properly. So this is at most once delivery. Now at least once we assume that everything is sunny, everything is shiny, everything worked out as it should be, right, so messages are acknowledged, messages are stored fault tolerant, so the producer knows that the message was received and replicated properly. However, This can always happen. We store the message reliably, but then the acknowledgement times out. Network glitch, right? Big network, this can happen all the time, right? So. In that case, the producer does not know, OK, went the message through or not, so it would retry, send the message again, and now I would end up having a second time because I'm missing the acknowledgment, right? So messages might be delivered more than once to two producer retries, and this is a given fact in that case, and this is why we cannot have exactly one delivery. Because of those producer retries, because of the time out of the acknowledgement, I need to retry the message, otherwise it might be incomplete and might not have worked out, so I cannot have exactly one delivery. However, if we want to have something maybe that feels like exactly one's delivery, we are usually talking about exactly one's semantics or exactly one's processing, but this is usually achieved by capabilities of the messaging system, for example. So in that case, if we do the retry, the messaging system would actually say, oh, have I seen A already? and then we'll not add this message again, right? And this requires two things the deduplication ID and state, because I need to know what makes a message actually identical, right? So this is the ID and state because I need to keep track of all the messages that I have processed to figure out if it was there or not, right? And that. is an expensive thing. I cannot keep an unlimited amount of data to check all the messages that I ever processed. So this is why those deduplication mechanisms are usually limited by either a time or a dedicated scope. And an example of that is in Apache Kafka. They limited on the scope for a producer ID and a sequence number, so based on the producer, and with Amazon SQS we have a deduplication ID based on a time window of 5 minutes, for example, but not unlimited. Which brings us to the conclusion that the duplication of messages from the producer perspective or the messaging system of messages in distributed systems is relative to a defined scope, so we cannot have this exactly once a month over unlimited amount of time or just with a huge investment on state. But let's say we even only get the message one time into the messaging system. So let's say everything went well from that perspective. We only have it once. Now what can happen on the consumer side is almost identical. So let's say we now have a consumer, we only have a once. We do two things we write to Dynamo D B. We then write to another downstream system. And we then acknowledge back to the messaging system that we can now drop the message essentially from the messaging system like an SQS delete message, for example. But now again, the very same thing can happen. What if that acknowledgement times out? The messaging system does not know was a now processed. Yes or no, I don't know. So A will be processed again and therefore I still have to ensure, even though I only have the message once in my messaging system, I still need to have item potent consumers and targets that processing the very same message would not trigger any unwanted side effects. So I might do that with either potent consumers or targets or have a processing store where I do check against that, but again that can grow again or can grow again quite substantially, and some examples of this implementation are the exactly one semantics within the Pachikafka and their connector frameworks or distributed transactions. But, let's imagine we built this beautiful system. And then it's out of order. Oh, what do we hope not? I hope not. Yeah, let me complete this chapter with yet another characteristic that sometimes leads to heated discussions, which is order. So FIFO first in, first out, and again it's mostly our mental laziness that makes you saying we need exact and strict order, otherwise our application will not work. But then we might forget about the implications and the trade-offs. So let's have a look, and I actually liked it a lot, Max, that you did your previous discussion also with a focus on the producer side. And you might have noticed that most of the time in these discussions it's only on the consumer side and the producer side is a little bit neglected, but it's also very important. So I will do the same. I will also start with the producer side and the very easy way, just one producer sending messages into a messaging system. What can possibly go wrong? Everything is in order. Every message is acknowledged, and the order is kept. So that's actually a very common approach, and the way why it is so simple is that you have only one producer. And so with what you pay for it is that you have less throughput. Apparently there's only one producer running in parallel. And more latency for it, but that might be OK if you don't have to operate on a high scale. That might be perfectly OK. And sometimes you are even bound to that. You cannot do it any otherwise if you work with legacy systems. Now the harder way is that you have concurrency on the producer side. What can happen here? Let's have a look. So the first producer sends message A into the messaging system, at least it thinks it does it. Then we have the second one with message B. And again we have the famous or infamous network glitch that makes message B appear in the messaging system before message A. So Max, do you think this is a serious case? Should we be worried about that? I think it's an absolute catastrophe. It's a catastrophe. Oh my. But maybe it depends. It could be that it depends because we need in this case to understand a few concepts. The first one is total versus partial order. And that brings us back also to the resources that we may have to spend on keeping a total order like from the beginning of time to the end of time. If you want to keep that order, you also need to store all those messages, right, so similar to deduplication, which is hardly possible and maybe, maybe a local or a partial order might be sufficient. We will see it in an example. The other concept we need to understand is the causality or the happens before relation. If we circle back to the booking service that we have looked at before, let's assume now it is about ride booking. The first producer is one customer, the second producer is another customer. So if the first customer requests a ride and runs into that networking glitch and the ride request for the second customer appears before. Message A1 now in the queue or in the messaging system, we don't really care, do we? It's OK. So we can rely in this case on a partial order. We are probably interested in the message order per customer but not across customers. And the other thing that can now happen is that the first customer wants to cancel the ride that it has requested before. And for that it would be really, really helpful if we understand the happens before relation. For that we will obviously need some meta information in the messages to make sure. So we can possibly end up with the 3 cases that we see on the upper right. In the 1st 2 rows. It's all sunny cases. We don't need to worry, but the third one is something that we want to avoid, but apparently we cannot always avoid it. So we need to make up our minds how to operate on this afterwards. But let me also circle back to the partial order, for that's the messaging. needs to apply some sort of sharing, and Amazon SQSOQs, for instance, uses the message group pattern to make sure that the order within one message group, in our case for one customer of the rideshare system, remains when the messages are delivered to the consumers. And again we cannot store the order of all messages from the beginning of time until the end of time because that would require Infinite storage. So the insight here is similar to the previous one that the order of messages in distributed systems is relative to a defined scope, for instance, the message groups within SQSIFO. Now, it can also happen, we saw it before um that the producers already mess up with the message order. And that maybe the producers send duplicate messages. If for instance one producer just misses the time window of the deduplication, then the messaging system cannot know anymore if it is a duplicate or not. So it might be the case that we end up on the consumer side with this constellation that we have a duplicate and a scrambled order of our messages. What can we do now? The SQS FIFO functionality with message groups wouldn't help anymore here because on the producer side it has already been messed with. In this case, the interesting pattern here is that you can use a helper table to first store the messages locally and re-establish the order before you actually consume them. And for that we need the metadata that I have mentioned before in the form of some kind of monotonic clock, and that doesn't necessarily have to be a time stamp or so. It can also be a version ID. And if we look at our example again, the first message that comes out here is the ride canceled event. So we have We have interestingly the version ID here in this message V2 that we can store in a thought as a sort key in our Dynamo DB table. So we put it in and also take a note. The latest version of our messages here is that V2, and we see what we get next. Next message from the queue is a 1, so that represents the right requested message. And since it has that clock ID V1, we can put it before the V2 message in our table. And due to that message meta information, we can even identify now the duplicate message that A1 appears for a second time in our queue. So we can then decide if we want to do an absurd or if we just ignore this message. So again, without that meta information you will hardly be able to reestablish the order, but with that meta information you can do so. And that was the 3rd chapter. We have 1 chapter left, and this is also quite interesting, as I think it is about multi-tenancy and shared resources, and shared resources can actually appear everywhere, but also in multi-tenant systems. Great. So let's take a look about that. So we want to look specifically at noisy neighbors and fairness in queues. So Dirk, are you a nice neighbor? I am a nice neighbor, but I'm at times also noisy. OK, so let's see what I could do about that if I would be your neighbor. So let's see. Great. So now we are basically we have this multi-tenant system. So we basically have different stakeholders that are using the very same system. So think about the SAS that multiple companies have their own kind of access to it, right? So they're all using the same underlying back end, but they have their own kind of. In the application. So usually when we send or have some form of request, I know based on the tenant ID in that specific request or usually a token, for example, I know which tenant it is, and then I can do the processing. So, so we have here a booking submission service, we have a messaging system and a booking processing service. So we have multiple tenants. So what would be your first suggestion on how we go about it if we want to use a queue, for example. I would keep it nice and easy and set up one queue for all tenants, so a multi-tenant queue for everybody that would not require me to spin up new infrastructure when I onboard the next tenant. That's a very good idea. Usually we talked about scalable systems, serverless, messaging systems, so we would expect that we can handle a lot of load and we can do that, obviously, right? So we can handle thousands of messages here or even beyond that, but there are still some. where a tenant might introduce a very high load relative to the other tenants. So what if one tenant now starts creating a large backlog of messages and the system then actually looks like this? So tenant 1 pumped in tens of thousands of messages and is a very noisy neighbor, and I only added my one little message and now I have to wait up until all the tenants, the first tenants' messages are being processed. And I think maybe some of the concepts you introduced in the beginning can help us here. Yes, we can be brutal again. And just throw away messages that have reached a certain age. So that's a generic approach here now independent of cues in the first place, but that's something we can think about. And also we can think about applying back pressure on the producers and then you can dive into the details and need to make up your mind, OK, what do I do with the requests that appear at the producers. If the messaging system tells me I should slow down, do I store them in a database, in another queue? Do I throw them away, and so on? And this is again more of a business decision, so the business owner of that use case should make up their mind what experience the customers would have from that. But aside from that, Max, I would make it very easy. And just create a queue for every customer or for every tenant, so then I don't have to worry about noisy, noisy neighbors at all. But you're also introducing some form of runtime dependencies, operational characteristics, right? I might have empty poll cycles on all of those different queues, for example, so I still have some price tag related to. You're not very excited about this approach, when I think. About it, it actually produces a lot of waste. There might be more idle tenants where the queues are mostly empty, and I still have to have the compute capacity to pull those queues, and I just pay for that capacity for nothing, so there's waste. And if I'm very successful with my multi-tenant application, I on board more and more tenants. I don't only scale. With the tenants, but I also scale the waste that I have in this architecture. So I actually take it all back and state the opposite. I wouldn't want to do it like this. That is one extreme. We had the other extreme in the beginning. Maybe we can look more a bit in between. I would say let's clean up the waste. Yes, that's always good. So what you said essentially is that at one end we had the multi-tenant queue for all of our tenants, and now we have the other extreme solution where we put a dedicated queue for every tenant. Now let's try to do a little bit of something in between, and usually this can be achieved by doing basic charting instead of having just a queue per every tenant, I create buckets of tenants. So I say I have one specific queue and 5000 customers go to the first queue and the other 5000 customers go to the other queue, and so on and so forth. So if tenant 1 now starts sending a lot of messages, the impact is limited to that specific cell if you wish, or to that specific chart, that specific queue, and that example. So the total number of tenants that are affected are not 100 anymore and also not 1, but in that case 25% of my customers would be affected. Of course I can now grow that number. I can have 8 queues here, for example, and only 12.5% would be affected. But then you play that through and then you end up having again a single tenant queue, so a tenant for a queue for every tenant essentially. So you need to find a good balance. And one tool or one approach that can even help you with that, so having a very small blast radius with a limited amount of cues, is shuffle charting. And with shuffle charging what we essentially do is we have, for example, two queues that we assign to every tenant and they get randomly assigned. So let's say I am tenant 1, I get queue 1 and 2, and Dirk is tenant 2, he gets 2 and 3. So the overall to simplify it, we do want to have the most minimum overlap, right? So I always want another tenant to have another backup queue as you wish. So if tenant 1 would affect both of its assigned queues, now tenant 2 would be affected, would be not affected for its other assigned queue, right? And with 8 cus actually if I Count the number of unique combinations for two cues. I come to 28 different shuffle charts essentially because I have 1 and 22 and 33 and 4, and so on, and 1 in 51 in 62 and 52 and 6. So I have 28 unique combinations. And if you do the math with that, that would be only a blast radius of 3.5% now in contrast to what we have seen before. And if we would scale that even beyond that, so let's say we put 100 queues where we create 100 charts if you wish, with assigning a tenant to two of each, we can reduce the blast radius to 0.02%. So with a given set of cues or charts in that specific case, we can drastically reduce the blast radius because we only want to or we want to make sure that we have as little overlap as possible between those different tenants. So in that case also tenant 8 would be affected, but it has another cue assigned which would be Q8 here in that example, and so on and so forth. However, sometimes we do know who the noisy neighbor is, right, so we already came to know that you are the noisy neighbor. So if I know I know Dirk is the noisy neighbor, I could just create a dr queue and just put all the messages for Dirk in a dedicated queue and maybe shout for the others, for example, so. It's really a balance. Sometimes you have enterprise customers. Think about. If you have an enterprise plan, you might give them a dedicated queue. If they have the free or the entry level plan, you might give them a shared queue, for example. And this is how you can play around with those different concepts. But Dirk, do you have even more stuff in your toolbox that we can use? I do so far we looked at static patterns and what we should maybe also mention is with every escalation, if you will, the patterns that give you more benefits in reducing the effect of a noisy neighbor, the more engineering effort you have that's the trade off for that. But as I said, so far we are looking. At static patterns we can also now go with this one that we just see here and extend it to a dynamic pattern. What we could also say is, hey, why don't we let the producers observe if one of those tenants or the users of that talent. Produce extraordinarily many messages and if we detect this, we could spin up a dynamic overflow queue and let the producers send all the subsequent messages of 10 and 2 in this example to that overflow queue. Apparently the consumers need to know about that overflow queue, so there needs to be a signal that the producers send to those consumers, and then we can also decide on the consumer side would we need the existing consumer fleet and then maybe balance somehow. Which in what way we consume from those two queues that we currently have or would we maybe spin up additional overflow consumers coming back to the example, that is something that we could potentially charge additionally to that noisy neighbor. One thing that I dislike with this approach is actually that we have now that direct link between producer and consumer, which we actually don't want to have when we operate with Que. Now there are even other cases where it's not about that a tenant becomes a noisy neighbor because they produce such a large amount of messages, but maybe that message consumption takes longer than expected or runs. Into an increased error rate. So we can also do this from the other way around and use a dynamic consumer controlled overflow queue. So if the consumers detect that, they can initiate that we spin up a new queue, instruct the producers to send messages into that queue subsequently. And take it from there and then we can again decide on the consumer side, how do we balance this? Do we prefer the multi-tenant main queue? Do we do it round robin and what have you. So there's actually a lot of difficult things that we need to do here in terms of engineering. We need to spin up the additional resources. We need to make sure we need to decide on how we balance the message consumption. And we also need to make sure if the noisy neighbor situation is gone to cut down, to throw away the extra resources that we spun up, yes, and also in a nice manner that we don't have a race condition or that we don't delete the queue while it is still full of messages, so. Actually it would be super cool, Max, if we had a feature with some messaging system that does all that work for us, that puts a box around this and makes all those multi-tenant main queues, the overflow queues, and so on appear just as one queue. That would be absolutely fantastic and it reminds me a little bit about the blog post that we recently wrote. You're right, because this summer Amazon SQS Faircus has been launched, and this feature exactly delivers this functionality for you, so you don't have to make up your mind about how to do this. You don't have to have that engineering effort. You can just use Amazon SQS Facus. You can reuse the message group ID attribute for this to express that a certain message belongs to a certain tenant. So we went through a number of patterns and we can still state the insight from the beginning that whatever architecture decision we take, it always comes with trade-offs, unfortunately, and your job as a software architect is to not fall victim to the face healer where people tell you just do this, do that, and all your pain is gone. So let me summarize what we talked about today. We looked at coupling and the dimensions. We looked at control flow and flow control, no pun intended. We looked at the heated discussions about delivery, semantics, and message order. And then we also had a look at how to mitigate the noisy neighbor problem on cues. And for example, for the things we just talked about. Facus we have put together a couple of resources that we want to share with you. So if you scan that code, you will get the blog posts around Faecus and other important integration patterns resources. If you have any questions after the talk, we will be around here or you can also reach out to us on LinkedIn, and we would really appreciate if you take the time and rate our session in the app if you enjoyed it. And thank you very much for listening. Thanks a lot.