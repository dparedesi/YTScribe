---
video_id: mtJnyAHfgSU
video_url: https://www.youtube.com/watch?v=mtJnyAHfgSU
is_generated: False
is_translatable: True
summary: "Kevin Gibbs and Mike Rizzo from the ECS team discuss advanced software deployment strategies on Amazon ECS, focusing on new features released this year. They begin with a primer on ECS services and deployments, explaining that a service is defined by a task definition and service configuration, which together create a service revision. The process of moving between revisions is an ECS deployment. The presentation then delves into the different deployment controllers and strategies available on ECS. While CodeDeploy and external controllers are options, the focus is on the native ECS deployment controller. The default strategy is ""rolling"" deployment, where new and old revisions run alongside each other, with traffic shifting as tasks are created and deleted. In contrast, the new ""advanced deployments"" (blue-green, canary, and linear) maintain both old and new task sets simultaneously, allowing for faster and safer rollbacks. A key difference between rolling and advanced deployments is task capacity. Rolling deployments maintain a relatively constant capacity, while advanced deployments double the task count during the deployment process. Traffic shifting also differs significantly. Blue-green shifts 100% of traffic at once, canary shifts a small percentage first and then the rest, and linear shifts traffic in increments. These options provide flexibility for different use cases, such as performance testing with gradual traffic shifting (linear) or A/B testing with a limited blast radius (canary). The speakers highlight new features like lifecycle hooks, which allow users to inject custom logic (via Lambda functions) into the deployment process for testing or custom traffic shifting. Another feature is ""bake time,"" which allows for a new version to ""bake"" in production while being monitored before the old version is decommissioned. Mike Rizzo then provides a deep dive into the lifecycle of an advanced deployment, using a fictional customer, ""Unicorn Watch,"" as a case study. Unicorn Watch, a platform for unicorn sighting data and videos, wanted to increase their delivery velocity while maintaining high availability. They were using rolling deployments and CodeDeploy for blue-green, but faced limitations. By migrating to ECS advanced deployments, Unicorn Watch was able to: - Use canary deployments for their catalog service, which wasn't possible with CodeDeploy and Service Connect. - Expose their catalog service on a public API using path-based routing on the same load balancer, which is supported by ECS blue-green but not CodeDeploy. - Implement a more scalable, asynchronous notification system using a headless service that pulls messages from a queue, a pattern not supported by CodeDeploy. - Use blue-green deployments with a Network Load Balancer (NLB) for their video streaming service to ensure a clean switch between incompatible protocol versions. The presentation details how advanced deployments work with different service exposure methods, including Application Load Balancers (ALB), Service Connect, headless services, and NLBs. For each method, they explain how traffic is shifted and how custom tests can be implemented using lifecycle hooks. Finally, Kevin Gibbs returns to discuss the practicalities of choosing and migrating between deployment strategies. He emphasizes that it's easy to switch between strategies once the advanced configuration is set up. He also provides guidance on migrating from CodeDeploy to ECS deployments, either through an in-place update or by creating a replacement service. The key takeaway is that advanced deployments on ECS offer a powerful and flexible way to accelerate software delivery while improving safety and reliability. They provide granular control over traffic shifting, support for various testing strategies, and seamless integration with different service exposure methods."
keywords: ECS, deployments, blue-green, canary, linear, software delivery, CI/CD
---

Hello. I love software deployments. I love software deployments because it's how I deliver value to you, my customers, and I see that you're here today. You probably also love deploying software to also bring value to your customers. My name is Kevin Gibbs and I'm here with my colleague Mike Rizzo and we're gonna talk through ECS deployments and some of the advanced deployment strategies that we've delivered this year. Just a rundown of what we're gonna go through today. First we're gonna talk through just a primer of ECS services and deployments. Uh, Mike is gonna go through a deep dive on advanced deployment techniques, configuration, and what exactly we mean when we say advanced deployments. Finally we're gonna go through some choosing and mid uh migrating between different deployment strategies or to advanced deployments from rolling. And we'll also cover some key takeaways and some other ways that you can finish uh and continue your learning process. So first off, when we talk about ECS services, we're really talking about two different configuration parameters or sets of parameters. The first one is your task definition. So a task definition defines a unit of work, a single task within a service, and then service configuration and those parameters say how you want to deploy one or more of those tasks to perform some function. Those two things together, those combination actually come down and result in a service revision is what we call it an ECS, and so you can actually look at how your service changes over time by looking through your, uh, revision history. The process from going from either nothing to your first service revision or from a service revision to your next service revision is an ECS deployment, and that process is what we're gonna go over today, what it does, how it works, and some of the configuration of it. So to get into some of the configuration for your deployments, the first thing that we look at is the deployment controller. So today we're gonna be focusing on the ECS deployment controller, our native controller that, uh, we manage for you. There are two other options that you have. One is you can use code deploy, which also supports, uh, some blue-green, linear, and canary, um, as well as external. And what external means is really it's up to you. You own the deployments. You can do it however you want, um, but you are taking on that responsibility. So that is another option again. We're gonna focus on ECS going forward. So the next thing that you need to decide is what strategy. So now that you have your controller decided on ECS, we have 4 strategies. We have our default strategy, which is rolling, where you're deploying your new service alongside your old, um, revision, and you're just migrating via the creation and deletion of tasks. In contrast to that, you also have 3 new advanced deployment controls or deployment strategies, sorry. These are different in sort of how traffic is migrated, but all of them allow you to have a new set of tasks and an old set of tasks sitting together to allow you to roll back and roll forward much more quickly and seamlessly between versions. Some of the differences that you'll notice between standard rolling and advanced deployments is what your task capacity looks like. So in a standard rolling deployment you're actually gonna create new tasks and destroy and delete or stop old tasks and so you kind of have more of a constant, uh, capacity, whereas in advanced deployments through the lifetime of your deployment you're actually gonna have double the number of tasks. You're gonna have your blue task and your green task if you wanna use the color analogy. And so you have again twice for that duration and then you go back down to normal once the deployment is over. The other aspect, key differences is how traffic is shifted or transitions between the two. So in blue-green you have again blue tasks and green tasks running right next to each other, old and new, and so the total traffic that goes to either one is basically dependent on how many tasks are up and healthy. In the advanced deployments you have additional functionality of how that traffic has shifted and so in blue-green really what that's doing is taking 100% from blue to green one step, um, we just do it once. Canary is kind of a two step process so you get to choose the initial percentage. So in this case I'm showing 10% is the initial transition and so you transition 10% of your traffic. Then you can check metrics, make sure that everything's looking good, and then it'll transition 100%. Linear is is an end stage version and so in this one you have increments and so you can start with as small as 3%, uh, which would result in about 33 stages to your deployment but it does traffic shifting over time and so some of the reasons that you would want to do that is if you're concerned about performance of the new version and so you wanna actually gradually shift traffic and see how it's performing or something like that. Some of the common configuration that you have between all of the config uh ECS strategies are life cycle hooks. So this is something new that we added this year, and it's the ability for you to engage in the deployment process. You can define your lambdas of, hey, after you've scaled up my new tasks, I want to do something. I wanna check something or whatever. Um, you can also implement your own traffic shifting, uh, paradigm via life cycle as well. And then also we have the idea of a bake time. So if you're using alarm-based rollbacks, which we've had for a little while, um, if you want to just bake it with that new version surfing traffic for an hour a day, um, you have that option with the bake time. Finally, there's some specific configuration for the different strategies. The first one is with rolling is kind of how those increments grow and shift over time of what is your total max capacity, um, in rolling and then also what's a minimum capacity and so you can actually have it scale down some tasks before it starts new tasks or start new tasks before scaling down old tasks configured to using those two things. Blue green, there's really nothing specific. You can set up the life cycle hooks and you can set up your bake time, but it's pretty much just a single shift, so there's nothing specific with Canary. You can control that initial shift and you can go as small as 0.1%, uh, all the way up to 99.9% to kind of control how much traffic, uh, is on your new version before you move everyone over to it. And then linear, similarly, each stage can be as small as 3% and as much as 99.9%. And again with those last two you kind of have a time wait, uh, kind of like a mini bake time between each of your shifts to control. Now I'm going to hand it off to Mike as he's going to deep dive into the different advanced deployment strategies. Thanks Kevin for that great intro to advanced deployments. Uh, my name's Mike, and as I say, I'm really excited about uh working with customers to help them derive real benefit from new features like advanced deployments. Over the next half hour or so, I'm going to go a bit deeper into how advanced deployments work and also look at some specific examples of how you can use them in a variety of different scenarios. But first, a little detour, uh just like to share another passion of mine, unicorns. So at AWS we love unicorns, I'll let you know a little secrets. Some of us venture out at night looking to spot one of these amazing creatures, and here's one that uh a colleague of mine snapped last week. And the reason I'm mentioning this is because I'd like to take you through a journey that one of my favorite customers recently went through with advanced deployments. Meet Ada, CTO of Unicorn Watch. So Unicorn Watch is the world's leading repository of unicorn sightings data, the go to place for unicorn research. Uh, it's also home to the world's largest archive of unicorn sighting videos. And When Ada heard about advanced deployments being launched, she came to us and asked whether there was anything they could benefit from using advanced deployments to speed up their delivery velocity. So this is the architecture they had prior to advanced deployments coming along. They had a UI fronted with an ALB, and through this UI they were able, users were able to access the catalog of videos, to access their accounts, and to make purchases. When making a purchase, that would trigger the creation of an order fulfillment flow, and the flow could be monitored through another ALB by polling an interface on the order's service. All the interaction between the front end and the back end was through Service Connect. And then they also had at the bottom, a network load bouncer fronted streaming service that would pick up videos from a media asset bucket. And the way this works as well was that the orders uh would be uh creating videos that were watermarked with a unique identifier uh for security purposes. So this all worked nicely, um, and they were using uh ECS rolling deployments throughout, with one exception in the case of DUI they wanted to use blue-green, so they could um manually check any new deployments in a safe green environment before flipping over uh flipping it over into the production environment. And for that they use codeploy. OK, so, um, now you can imagine that a service like this, it's a premium service with a high price tag, and also those Unicorn photographers want to get their royalties. So it's really important that this platform maintains a high level of availability and reliability. At the same time, there's a growing demand for more new features, and also Ada wanted to expose the catalog service to third party websites so that they could also resell videos from the catalog. So, the challenge they faced was they wanted to maintain high availability and reliability, but at the same time, speed up their delivery velocity. So when advanced deployments came along, they were really keen to understand what that could do for them. So I'll come back to this story in a second, but first I just want to take you through the life cycle of an advanced deployment in a bit more detail. So In the initial states, right, we have a service running in production, we call it the Blue Environment. Uh, all traffic is going there, uh, and, um, when we start a new deployment, we have a new, uh, spec for uh the tasks that need to be run, including a new container image, for example. Um, and we enter the deployment, state machine in the first stage, uh, which we, uh, which is known as pre-scale up, right? So this is before we scale up the green environment. Now you'll see there's a little hook icon on the side there, that denotes that this is a hookable stage, which means you can attach a life cycle hook to it. So with life cycle hooks, you can run your own custom logic in a lambda function, uh and you can use that to perform tests or checks, uh that you could use to determine whether you want that stage to proceed, to succeed and proceed to the next stage, or whether you want to force a rollback. In this example, before you scale up the green environment, you might, for example, want to run some kind of admission control check where you might say, for example, is this container image coming from a trusted repo? So if the hook returns successfully, uh you then proceed to the next stage, and this is where we do the scale up. So here we scale up the green environment, you can see there on the on the far side, uh we have the the green environment scaling up. And once you achieve the full scale up, then we enter the post scale up stage. At this point, all production traffic is still going to the blue environment, there's no traffic going to the green environment. So once we've got the green environment in place, the next step is to start shifting test traffic to it. So we now entered the test traffic shift stage, and here we configure routing, uh so that test traffic, which has to be identified in some way, we'll come back to that later, that test traffic can start to flow to the green environment. Once that routing is set up, we enter the post-test traffic shift stage. Now at this point, the green environment is able to start to receive test traffic. This is a really convenient place to put in some hooks, if you want to do some automated testing on the green environment, right? Or if you want some kind of manual checks, and manual approval uh on something running in the green environment, you can do that through the hook here, and we're going to see some examples of this with Unicorn Watch and see how they use it. OK, so we're now at the position where the blue tra the blue environment's still receiving all production traffic, all test traffic is going to green. Once we uh exit the post-test traffic shift successfully, so any life cycle hooks have succeeded, we then start to shift the production traffic to the green environment. Now depending on the strategy you're using, this can happen in a number of different ways. The simplest is, with blue-green, this happens all at once, right, so we shift all traffic in one step ah from blue to green. But you can also do canary, where you have it in two stages, or you can do it linear, which could be multi-step. We'll come back to that shortly. Once you've shifted the production traffic, uh, and you've got 100% going to green, uh, we, we uh enter the post-production traffic shift stage. Now at this point, there's no more production traffic going to blue. It's all going to green, but the blue environment is still running. We still have the old surface revision running in the blue environment. And this is useful because it means that if something was going wrong in the green environment, we can very quickly roll back and switch back to the blue one. And we keep this blue uh environment running for a defined bake time, right, so as part of your uh configuration for for your blue-green deployment, you can specify a bake time, and for the whole duration of the bake time, your blue environment will continue to exist. Once the bake time is over, the final step is clean up. At this point, we scale the blue down to 0, and we now have a situation where the green is our production environment and becomes the blue environment for the next deployment. Each of these stages can take up to 24 hours, so you can do this over a pretty long period of time if you need to run long tests, for example. You'll also notice I was careful not to say too much about how the routing and the traffic shifting is actually happening, and that was deliberate, because it depends on how the service is exposed. So for example, if the service is behind a load balancer, then it's achieved by manipulating weights on listener rules. But if the service is exposed to Service Connect, then the routing is achieved through changing the routing rules in the Service Connect proxy. So in the case of canary deployments, everything's exactly the same as we saw in the life cycle up until now. The only difference is, you get two production traffic shift stages. So essentially, any hooks attached to production traffic shift, yeah, that will, that hook will be invoked twice. First time will be for the initial canary. Uh, shift, so in the first step, you take a percentage, in this case we've got 10%, but you can configure that, um, and we specify a canary bake time, and for the duration of that canary bake time, we will have, uh, that percentage of production traffic going to the green environment. Once the canary bake time is over, production, uh traffic shift is entered again, so we invoke the hook again, if if if there is one. And then that completes the rest of the traffic shift to production. In the case of linear, it's similar. But we have multiple steps, right? Um, so you can use Canary and Linear to achieve different objectives, right? In the case of Canary, you would, uh, if you want, if you had a situation where you wanted to do some testing of your new uh service revision in the actual, with actual production traffic, you can do that with Canary, limiting your blast radius, so that if things went wrong, it wouldn't hit you too badly. In the case of Linear, what it allows you to do is have a more gradual sort of rollout of the new service revision, while you're monitoring to see, to make sure that any performance and functionality is maintained. Correctly. OK, um, really important that when using advanced deployment, you think about what would constitute a failure or uh underperformance of the new revision. That would require you to to stop the deployment and roll back. And ECS provides three broad mechanisms to support this. First, the simplest is a circuit breaker. So with circuit breaker, you're simply checking that the new tasks actually achieve a healthy, stable state within a specified time period. Um, Next, we've got cloud watch alarms. So here you can choose some metrics that are appropriate for your use case, and set alarms on those metrics, and if those alarms are triggered, then that can force a failure and a rollback of of the deployment. The metrics you can use here, depends, um, for example. If you have an ALB fronted service, you might be looking at metrics for counts of 400 or 500 errors. If you had benchmarked the performance of previous revisions and you want to ensure that the performance of the new revision is consistent with what you had previously, you might want to be looking at things like CPU utilization or memory utilization. Very important that you factor in, if you're using Canary or linear, you factor in that there's a mixture of the green and blue running together, right? So that is something you need to take account of in your alarm thresholds. You can also Think about situations where um you have a task that's pulling messages off a queue, so there's no traffic actually being routed to the task, but instead, the task is pulling messages off a queue. In that case, you might want to be monitoring your queue length as a metric for for alarming. And then finally we've got custom tests and traffic monitoring, and this is where the life cycle hooks I mentioned earlier, come in. This is where you can implement your own tests in lambda and insert them at the right places in the deployment life cycle. And we're going to see some more examples of this with Unicorn Watch. So. So let's go back to the Unicorn watch architecture. So, just a refresher, that's this is the architecture that they had uh prior to advanced deployments. And what I'm gonna do now is just go through some of uh aspects of this architecture in turn and see what changes they make and what they were able to achieve through that. So first, we're going to talk about uh the UI and the catalog service. So they wanted to do a couple of things here. Uh, they wanted to uh switch to using canary deployments for the catalog, so they could try using the new function F catalog in a production environment with a limited blast radius. And they also wanted to expose the catalog. On a public API so that third party websites could also access it. Now, um They couldn't do this Sticking with CodeDeploy because uh first, um CoDeploy doesn't support Service Connect, so they couldn't use it with the catalog for Canaries. But also, what they wanted to do to expose the public, the catalog on a public API is they wanted to expose it on the same load balancer and on the same port. And to do that, they would need to use um the load bouncers advanced request routing capabilities to use path-based routing. With co-deploy, you can't do path-based routing, so by moving to ECS Blue-Green, they were able to achieve their objective, right? So essentially what you can see here is they were able to use p-based routing to to access two different services through the same port on the ALB and also switch to Canaries on on the catalog service. So let's look at this in a bit more detail, starting with the load balancer. What I'm gonna do with this diagram, we'll come back to this diagram uh as we go through the talk. Um, I'm going to go through a number of different ways of exposing services on ECS and show you how advanced deployments work with each of them. So we're starting here with the load balancer. Um So, when you do advanced deployments with ALB. You need to uh provide uh as a minimum, a production listener rule, right? And optionally a test listener rule. And the way it works is essentially the weights on those listener routes are manipulated by the deployment controller to achieve the traffic shifting effects that we talked about previously. Because ALB works at the listener rule level, it is possible to take full advantage of advanced request routing on the application load balancer, which means you can still do pal-based routing, you can do header-based routing, query string-based routing, all the features that you uh you've come to expect with ALB can still be supported. You can also make the same service available on multiple listeners as well, and that will also work with advanced deployments. In order to configure this, in your service configuration, uh, in the load balancer block, you will add this advanced configuration block. And here you specify a second target group, so you now need a 2nd target group in order for the advanced deployments to work. Um, and then you specify a production listener rule, ARN optionally a test listener rule ARN, and importantly, also an IM roll, and the IM roll is needed. To give ECS permission to manipulate the weights on those listener rules. Note that there's no reference to a listener ARM and that means you are free to choose any listener rules you like, whether they are on the same port or not, doesn't make, you know, from an ECS perspective, it doesn't make any difference, right? So ECS is agnostic to where which ports those those listener rules are actually connected to. So let's walk through how this works, uh, taking the example of Unicorn Was UI. So they uh want to do a blue-green deployment, and they want a manual approval on uh on the new version before they allow it to go into production. So in the starting state, they have the existing service running in the blue environment. And all traffic is going to that. First step is, we do the scale-up, right? So we create the green service revision, we go to the pre-scale up, scale-up, post-scale-up stages. We then start to shift traffic. So to shift traffic, we need to, on the test, uh listener, we swap the weights around, right, so we now have 100% of traffic going into the green environment. At this point, Once the traffic shift is completed, we're going into post-test traffic shift, and in this case, uh Unicorn Watch have attached a hook. And what this hook does is it monitors an approval parameter. Until this approval parameter is set to accept or decline, right? So, so something somewhere needs to set that parameter in order to be picked up by the life cycle hook to determine whether or not the deployment should proceed. Note that ECS itself does not provide a UI to do manual approval or anything like that. That's that implementation is up to the um. The implementer of the wider operations and how you orchestrate operations around deployments in your in your organization. In this example, we've just used an approval parameter stored in a parameter store and then there's a separate UI that can be used for an approval to go in and set that parameter. The way the traffic shift, the post-test traffic shift hook works here is it's it checks that approval for the current state of that approval parameter. If it's still in a pending state, the hook returns with an in progress status indicator, and ECS will then reinvoke the hook multiple times until a success or failure is returned. OK, assuming then that the, we got the approval, we got a tick in the box, so that the hook is able to succeed, and we're now able to progress the deployment beyond the post-test traffic shift stage. So now we can now move to production traffic shifts. So here we swap the weights on the production rule. And we start our clock for bake time. Right, so during this time, um we now have ah traffic um flowing to the green environment, but with the blue, still waiting, ah just in case we need to roll back. And then finally, assuming we complete break time without rolling back, the last stage, as I said, is clean up, so that's where we scale down. The old blue and the green becomes the new blue. OK, so that's how uh Unicorn Was used. Uh, the ALB, uh, in front of, uh, their, their UI service now using ECS Blue-green. We haven't talked about the actual request routing. We've talked about the traffic shifting. Now, um remember I said that they also wanted to expose the catalog on the same ALB port, right? So how do they want to achieve this? they they want to do this with path-based routing. So what I set up, as you can see here, is a path-based route for catalog slash catalog star routing to the catalog service, another one frontendtar to the UI service. They also needed a way to indicate test traffic, right? If if you if you have um test traffic for either of those services, how do you indicate that this is test traffic? And for that, they chose to use header-based routing. So if there is an XBG test header, that signals that the traffic is test traffic. And they can do this because they can use advanced request routing with uh with advanced deployments. So here we have, uh, you can see the two rules for production, catalog, production listener rule, and UI production listener rule. The application load balancer is configured with a path pattern matching a catalog and front end to route to each of those rules. And then on, on the far side there, you can see the deployment control has been configured. Uh, Directly to manipulate those, uh, those rules. In the case of, uh, catalog, uh, they using a canary deployment, and in the case of UI using blue-green. In order to route the test traffic, we have another 2 listener rules, and we've added the HTTP header. Rules checks matches onto those onto those rules as well. important to order your rules in the right order in order to get the desired effect. OK. What about Service Connect? So we've looked at the ALB that catalog is also exposed internally via Service Connect. And we want that canary to work with Service Connect. So how does that work? So with Service Connect, there's no listener rules and no weights in listener rules to manipulate. Instead, ah we have the Service Connect proxy ah sitting alongside the application container in the application's task, and that needs to route traffic to the right place, right? So the way, the way it works is um you've got your, you you create your green er vision, and both the blue and green revisions. are registered with Cloudmap, right, with the green one being labeled as your test instance. All clients in the name space in the in the in in the uh Service Connect name space can see both the blue and the green revisions. But only requests matching the test rules we routed to the greener vision. For those of you who may not be familiar with Service Connect, in a nutshell, the way it works is you have your proxy container running alongside your application container in your task, and that is configured to route uh basically all all network traffic passes through that proxy container, and it can be configured to route outbound traffic to the version that you require. The way it's set up by default is that if you set the header X Amazon. ECS blue-green test, then Service Connect proxy will automatically route that to the green uh revision. You can replace that with your own header routing rules. You can use any headers you like, you can use header presence, uh header value matching, header pattern matching. So you can use things like agent strings or API revision numbers, etc. as needed in your use case. So how did uh Unicorn use this? So the catalog team wanted, they were very concerned about performance, and they wanted to make sure they do performance tests on the. Any new revision of the service before putting it into production. So, basically, they implemented this using two things, they put a traffic generator in the service connect name space, right? And this traffic generator generates load, and all the requests are are automatically marked with with this, with with a test header. They then implemented a hook in the post-test traffic shift stage. And what they do in this state is they trigger the load generator. Uh, to start sending test traffic, they measure the, uh, Performance of the service under load. And then at the end of that test, they determine whether it's passed or not. If it's passed, they suc the hook returns successfully, and the deployment continues, otherwise, they failure to roll back. A key thing to note here is that if you are doing testing with Service Connect, your client has to be part of the Service Connect namespace. You can't test from outside, you have to do it from within Service Connect. Right, in this case, they achieve that by deploying this traffic generator in the Service Connect namespace. OK, um, so that gave them a performance test in the green environment before moving into the production traffic shift. But even when going into production traffic shift, they wanted to use the canary to do some additional uh testing. Before allowing the full shift of production traffic into the new environment. So here, if you remember, I mentioned earlier that with canary, the production traffic shift hook would get invoked twice. So what they did here was, on the first invocation, they start a test, right? So at this point, a percentage, a canary percentage of production traffic is going to the green environment, and we start this test to start monitoring the uh the performance of of of the of the canary. That continues for the duration of the canary bake time. And at the end of the canary bake time, the post-production, uh, the production traffic shift hook is called for the 2nd time, and at this point, the results of the monitoring are collected and analyzed to determine whether the canary has performed successfully or not. And once again, if the canary has performed successfully, we allow a return success and allow this production traffic shift to complete 100%. But if not, we can force a rollback by failing the hook. So, so this shows you that you can combine testing of the green environment. As we saw in the previous slide. We're also testing. With, with some of your traffic in actual production, through the, through the canary, um, production traffic shift uh mechanism. OK, so we looked at the uh UI and the uh catalog so far. The next thing they looked at was this mechanism here where they're polling through an application or bouncer for the status of orders that are underway. Now this doesn't scale very well, right? Essentially, uh, you're having to poll for status updates, and obviously, the more orders you have going through, the less uh efficient this this becomes, um. But also from a notification mechanism perspective this was quite limiting as well. What I wanted to do was support different notification mechanisms like you know if people wanted to receive an email or an SMS on their phone or whatever notification mechanism. You know, they wanted to support a wide range and they couldn't do it with this with this architecture. What they wanted to do is switch to a more synchronous approach where orders would post events on a queue and then that queue would be. Essentially would be monitored by a notification service that would pull events off the queue and then send notifications using a variety of mechanisms as configured for each user. And the problem is they couldn't do that easily because they want, if they did that, they they would need a blue-green um. Uh, deployments in order to to be able to switch from one version to another in one step. Um, and they couldn't do that with code deploy, because this service wouldn't have a load balancer in front of it, if if it was pulling messages from a queue. So that's what they wanted to do. There's no load bouncer in front of the notification service. This is what we call a headless service, right? There's no requests actually being sent to the notification service. Rather, it's a notification service pulling messages off the queue. And with ECS Blue-green, again, you can support this pattern, so you can support headless services. Where there's no service connect, there's no load balancer, you just have a task or a service pulling messages off a queue. So this is the next pattern we're going to look at, headless service. Now, with headless service, right, because there's no requests being sent to the service. There's no traffic to shift. So you might ask, how, how does this make sense for uh advanced deployments if there's no traffic to shift. It only makes sense in the sense that you can still, you still benefit from having a blue and a green environment running in parallel for a while, so that if the green doesn't work well and you need to roll back quickly, you can still roll back very quickly to the blue environment. However, because there is no request traffic, you have to think a bit differently about how you manage uh the uh the activation of these services. So essentially, you need to do a bit of extra work to determine which version of the service is picking messages off the queue. And you need some way of turning off uh turning on or off the the actual services, pulling off messages off the queue. So let's see how it works for a unicorn watch. So In the initial state, they have the existing blue revision, pulling messages off the queue and processing those messages. They now deploy a new version. When they deploy the new version, they deploy it in a deactivated state. So the the tasks are created. But there's some parameter, perhaps in parameter store, for example, which is controlling whether or not this thing should pull things off the queue or not. So it starts in a deactivated state saying, don't pull messages off the queue. Once this is scaled up. We can skip the um the usual test traffic shift, we can just go straight through that, go straight to production traffic shift. And at this point, um, when we enter this stage, we can use a hook to disable the blue revision and enable the green revision. So at this point, it's now the green revision that's picking up messages from the queue and processing them. And at this point, we start to monitor the behavior and the performance of the greener vision and make sure that it is functioning correctly and processing messages correctly. So We monitor in this case using cloud watch to monitor the queue. If monitoring is successful, we succeeded, and we can get rid of the blue. Revision after the break time. Otherwise, if unsuccessful, we disable green and restore the blue, reactivate the blue to go back to where they were previously. So that's uh that's improved their um ability to uh notify customers on status updates for orders. The last bit was the video streaming service itself, and here they wanted the ability to deploy. New versions of their proprietary streaming protocol with a lot of new security features, and they needed to do it in such a way that they flipped completely from one version to another, because they couldn't guarantee compatibility between a the new version and the old version. So they wanted blue-green on the video streaming service. Um, and to do this, they used um. Advanced deployments with a network load bouncer. With network load bouncer, it's very similar in many ways to what we saw with application load bouncer, but there are some differences you need to be aware of. First, uh, with natural bouncer at the moment you can only do blue-green, you cannot do canary or uh linear. Second, you don't have the benefits of layer 7 request routing, so, because you're working at layer 4, so you can't do path-based routing, that means you have to use different ports for your production and tests. So you need to have separate ports for production and test. And also, the test listener is not optional in the case of NLB. You must have a test listener. The other thing to be aware of is that for some stages, there's an extra 10 minute delay added on, and this is to do with some of the internal timing issues within NLB to make sure the the routing works correctly. But otherwise, it's it's, it's quite similar to ALB. So, just to recap, we saw here how Unicornwatch were able to use advanced deployments in a number of different areas. And what we've done essentially is cover 4 different service exposure methods. Right, we've looked at ALB, we've looked at Service Connect, we've looked at headless services, and we've looked at NLB. And and the beauty of advanced deployments is you can use advanced deployments with all of these service exposure patterns. And with that, I'll hand back to Kevin. He's going to take you through some of the practicalities around migration and how you choose your deployment strategy. Alright Mike, thank you. um, it was pretty exciting, right? So I went through a little blue-green deployment of my own, uh, and I got a surprise for you Mike with, uh, worked with the Unicorn Watch folks to get a new update on their streaming. So let's go ahead and see how that works. Uh, it, uh, worked on my machine, so I'm not sure what's going on here, but luckily we're using blue-green deployment so we can, uh, quickly roll back if you will. Um, hey, that looks better. All right. So let's go through a couple of the things of choosing and also migrating between strategies. So first changing them is really easy. You can just change the um strategy in the deployment configuration as Mike talked about. The key is adding that advanced configuration if you're using a load balancer. So once you've added that advanced configuration, then you can actually go to blue-green back to rolling to canary to linear. It's very fluid. The key is that once you've gone into advanced deployments, we ask that you just leave that advanced configuration there forever for as long as you're gonna use it because we could be using either one of the target groups and so if you're doing a rolling we need to have both so that we can determine, hey, this one's serving traffic or this one's serving traffic to do the rolling correctly. So again add advanced configuration and then you're free to move uh back and forth. Also, if you're, um, you have kind of two different ways if you're wanting to migrate from code deploy to ECS deployment strategies. You can either do just an update service, so update in place and just move from both the deployment controller as well as the deployment configuration to what you want it to be. Again, you'll need to add the advanced configuration blocks uh and the load balancers all together because if you're using code deploy the load balancer configuration is actually in code deploy land. The other option is basically make a replacement service and do that migration more controlled in your way. So if you want to test out the hooks, you want to test out the process a little bit, you can just create a second service um and then migrate. Some of the considerations, so we talked at the beginning about sort of how your tasks are scaling up and down and then also sort of the speed of operation. So your biggest trade off from advanced configurations and default rolling is kind of that speed of. Rollback so you always have that blue version fully scaled ready to go if for whatever reason your service needs to scale up during the middle of deployment, both of them will scale up to that uh capacity and it's always there so the speed of rollback is, is much higher obviously in that scenario. And then choosing between your advanced deployment types is really just what are you trying to achieve. If you want, hey, I only want one service ever serving or one revision, sorry, ever serving traffic at once, blue-green is perfect. It makes sure that all is served on the blue green the blue or the green. So that could be sometimes you know hey I have a very chatty uh client maybe uh a web app or something that's making calls back and forth and so to make sure they always get a consistent experience I wanna use Bluegreen. Canary again we talked through that a little bit. Some of that is if you can't test uh in production, maybe it's a regulated industry or some other reason that you cannot test in production. Canary is a great way to introduce it to a small section of your customer base before introducing it to everybody. And then finally, linear is a great option again if you're interested in seeing how it scales under uh increased load over time or other sort of concerns along those lines. Again, one of the amazing things that you can do, uh, with advanced deployments is this test traffic, um, so you have a means where you can test the actual running code in your production environment before introducing it to any of your customers and potentially, uh, preventing an outage that way. Again, as we talked about like you can migrate um fluidly between all of the advanced deployments um or between advanced deployments and rolling. Again you can migrate uh code deploy to advanced deployments, um, and one of the reasons that you might wanna do that is to take advantage of some of those advanced features set that you can use with ECS, uh, that you couldn't before. Mike talked through a couple of those, uh, multi-target groups sometime, uh, if you have multiple ALBs you're trying to connect to, uh, as well as using, uh, Service Connect or some other, uh, features like that. Um, finally, um, in order to continue your journey, we have a landing page here with a bunch of relevant, um, links, uh, to, as well as the actual deck, uh, that we went through today. Also there is a uh related session this afternoon at 4 in the MGM covering deployment pipelines with ECS. So if you wanna hop over there, that'd be great. This time I just wanna say thank you very much for coming and um I will be around here with Mike uh if you have any questions afterwards. Thank you.