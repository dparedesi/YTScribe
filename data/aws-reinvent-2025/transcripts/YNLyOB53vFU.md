---
video_id: YNLyOB53vFU
video_url: https://www.youtube.com/watch?v=YNLyOB53vFU
is_generated: False
is_translatable: True
---

All right, hello everyone. I hope you are having an amazing time in Vegas. I know I am. My name is Natalia Leon. I'm part of our strategic partnerships team here at Salesforce, and today I'm joined by my colleague Sid, who's one of our amazing product managers for our data 360 platform at Salesforce, as well as Bill, who is an amazing solutions architect over at AWS. Today we're really excited to share with you how Salesforce and AWS are empowering customers to unlock the full potential of the data that they already have access to to deliver better experiences and set the right foundation for AI. So during today's time we will walk you through an overview of the solution. We'll do a little bit of a deeper dive into the architecture. We'll showcase a demo and then we'll round it out with some customer success stories. As a reminder, please make sure that you make any purchasing decisions based on information that is publicly readily available to you. All right, so before we get technical, I'd look, I'd like to just ground us on, um, a simple truth that I think we can all relate to as customers of all the different companies that we interact with and the and the expectations that come from that. As a customer, I know myself. I expect companies that I interact with to understand who I am, deliver personalized experiences to me, and anticipate my needs when I'm interacting with them. And so. Um, based on those expectations, customers are really struggling to deliver that to their end consumers because all of their data tends to be siloed. I know myself, probably you too, was hopefully taking advantage of some of the Black Friday deals that just happened this past Friday, and when I was doing shopping, I, I caught myself expecting these companies to really get to like know me and deliver the right offers to me. And what is um what was once an option for companies is now really truly essential for them to be able to unlock their full value of their data and deliver those personalized experiences that we all expect from them. But this right now is really challenging for companies, and the reason for that is because companies' data remains fragmented across multiple systems across multiple formats, leading to a lot of data silos, and data silos typically means that companies' data is unreliable for insights. Besides being unreliable for insights, the data is actually also really hard to act upon, um, impeding companies to be able to meet customer needs. And so because of that what we've seen with companies is that their data their data tends to be siloed and they're unable to deliver these personalized connected experiences that we expect from them. Companies that are getting ahead and are really leading the charge are differentiated themselves by providing their customers with the personalized customer experiences that we deserve and need. I found myself on Black Friday. Clicking the checkout button pretty fast when a company was able to deliver a personalized offer to me and even provide shopping assistance for the pair of shoes that I wanted on the Black Friday. So with that, um, because of that challenge that we're seeing customers experience here at Salesforce, we have partnered with AWS to help companies be able to unify their data and activate it across the entire enterprise, which is what we're gonna be discussing today. Additionally, as part of this partnership, we are also enabling companies to empower their agents with the right necessary models and flexibility and control that they need to deliver those AI experiences. Additionally, we're also enabling multi-agent ecosystems depending on the customer's, uh, technology landscape and then of course providing companies with end to end trust and compliance along the way. So with that I'm gonna pass it over to Sid who's gonna show you how data 360 makes all of this possible. All right, oh, let me take the clicker. So the main topic of the presentation is zero copy and that's what Bill is gonna go over, but before that I wanna describe what data 360 is to everyone. So we know that in general an organization's data state is home to many separate but related systems daily houses, warehouses, message brokers, CRM, ERP, and a host of other data products. But data 360 is distinct from those existing systems. It's not a system of record. It is not a warehouse. It is not a lake house, although it is underpinned by one. It has a very singular purpose, and that is to harness, harmonize, and activate every single data point in your data state, whether that data point was generated a second ago, whether it was generated decades ago, whether it is structured, whether it is unstructured. Towards one of three ends. First and most importantly, understand the customer. 2nd, to grow faster sales, and 3rd, to improve productivity, work faster. And this is why you will often hear data 360 described as Salesforce's activation engine. Activate is the most important of the three verbs that I mentioned. It is the bedrock of customer 360, which is the applications that you're all familiar with sales cloud, service cloud, marketing cloud, and company as well as Agent Force 360, which is the agents that dwell and operate within and across those distinct applications. Now what you see on this slide are 3 building blocks of data 360, and each one builds on the previous one in some sense. The first thing is access data wherever it resides and in the manner that is most well suited for the task at hand, optimizing both for performance and for economics. Now the connectivity suite is extremely rich and I will describe it in further detail later, but if I was to summarize it in one word, that would be optionality. Now the keystone or the crown jewel of the connectivity suite is 0 copy, and that's what Bill is gonna talk about more. Now the second building block is a very important set of data data sets in your data state, and those are the ones about the customer. Now in any engagement with a prospective client or a current client, it makes sense that the more informed I am about that client, the more fruitful that engagement is gonna be. And typically we see that data about a customer can be spread across multiple distinct systems and so in data 360 you can create that unified profile of a single customer and that profile becomes the basic or the atomic unit of segment of one personalized automation analytics AI across sales, service and marketing and more. The third building block builds on that, and that is building the world for the agent. So just as every single one of us has a mental model of the world, we need to provide the same for an agent if it is to act effectively. The agent needs to know what it can and can't do, of the things that it can do, what it should and should not do, and of the things that should, it should do, how should it do them. And so defining that world, specifically all the nouns and verbs for the agent, is something that takes place in data 360. So how does data actually get activated? The first thing is to connect data 360 to the different systems in your data state. And as I mentioned, the connectivity suite is incredibly rich, structured, semi-structured, unstructured, real-time streaming, batch, zero copy. There are connectors for databases, for stand-alone query engines, data lake houses, warehouses, application providers. Almost everything will be covered within that very expansive set of connectors. And again, zero copy is the focus and we're gonna talk about, talk about that more later. The second step is to harmonize data, and this is a verb that is very often misunderstood. Harmonization simply means to standardize across the different schemas that you have in your data state to resolve conflicting identities and to construct those unified profiles. So think of this as creating a blueprint for your business, not the actual data sets that are in your different systems, but if you were to identify all the different objects that are important and how do those objects relate to one another, that's what we call the data model object graph and the process of creating that is harmonization so to express it in a different way. Think of harmonization as an abstraction that allows you to decouple the development of agents and applications, functions and analytics from the source schemas in your data state. Is a translation layer that you build in data 360 and it purely comprises metadata and I'll show you an example of what I mean by that. Once you've harnessed and harmonized the data, the 3rd step is to govern, and there is a rich, malleable, auditable framework for creating policies to manage permissions, ensure regulatory compliance, and different levels of granularity at the level of the table, OLS, at the level of a record, FLS, and at the, at the level of a column. Now once the data has been appropriately governed, not just for humans and automation but also particularly for agents. It's time to activate the data, and I'm gonna combine columns 4 and 5 into 1. Now there are many, many different activation services, and I'll give you three examples. The first one harkens back to Data 360's origins as a customer data platform. Segmentation is something that is a very commonly used workflow. By marketers where they segment those different unified profiles and then conduct surgical marketing campaigns. A different and newer one is something called Document AI where a document unstructured content is brought into data 360. It's loaded, it's chunked, it's vectorized and it's analyzed, and then you can create data actions that say if I detect this text in this document, I want you to send it to an agent because I think it's pretty straightforward what needs to happen next in maybe a contractual process, but if I don't detect that, then I want you to hand it off to a human. And then of course there's also predictive AI which a lot of times gets overshadowed by generative generative AI but it's still equally important. For example, in a call center, it would be good to know who is the appropriate team that I should route a support case to based on the knowledge base that I've created of support cases that have been previously closed and were closed successfully optimizing for some metric like CSAT. That's something that you can create a predictive model for. There are many different applications. Those are just 3 examples, but the point is that the 1st 3 steps form that trusted foundation, the bedrock that powers agent 360 and customer 360. It's a model that you define once and that you can use throughout your applications so that the distinct activation surfaces aren't interpreting your data state in different ways when they don't have to. Now Agent Force setting aside pricing and setting aside SKUs, Agent Force is built on data 360 architecturally it is the foundation. But there are 3 important reasons why it's important to go through data 360. The first thing is unstructured data. The unstructured data pipeline resides entirely in data 360, where documents get loaded, they get chunked, they get vectorized. You can imbue them with semantic meaning, and then you can feed them into a rag pipeline for an agent. And we know that an agent's quality, its effectiveness is always going to be limited by the quality of data that it's fed. If the quality of data is low, we can't expect the agent to perform well. Setting aside unstructured data, the second pipeline is structured data, specifically NL2 SQL. How can I ask an agent to execute analytical queries, OLAP queries at scale, whether it's an interactive agent with a human or whether it's an agent operating in the background. Going through data 360 allows you to use its native query engine and execute those queries performantly. Not by constructing a sequel, but by asking questions in English. The 3rd thing is that when an agent interacts with a client, it's important for the agent to realize and learn from previous engagements with that client rather than starting fresh every single time. That's what memory for agent force does. It's a tightly governed construct in data 360. That ensures continuity from conversation to conversation. I'm actually gonna skip this one because I think we covered it. So now we're getting into the details of the connectivity suite. And at the, at a high level there are 4 different categories. The first thing that will get out of the way is the unstructured data pipeline. I'm not gonna focus more on that, but that's what I alluded to with load chunk vectorized index. Now for structured data, there are 3 options. There's batch ingestion, which is your standard ETL process where data gets copied into data 360 periodically. Then there is the real-time data pipeline. Where data can be pushed into data 360. In milliseconds And then there is 0 copy. Which is accessing data at runtime as and when needed and never persisting external data. Now, one thing I want to clarify is that there is often confusion between a real-time and zero copy because zero copy is described as real-time. The real-time pipeline in data 360 is underpinned by a very specific object that's called a real-time data graph. A real-time data graph is effectively a materialized denormalized view. The data gets materialized inside of data 360. Think of this as web engagement data that is brought into data 360 and is used to personalize experience as a client is browsing a website. The data is pushed into data 360. This is different from zero copy, where the data isn't pushed. It is requested precisely when you launch a workflow that requires external data. So what is data 3600 copy exactly? Now typically let's say you have massive business critical data sets in a data warehouse. For 0 copy you would have to duplicate those wholesale into the system that you want to activate that data in. And any time you have two copies outside of some tightly distributed system, one of those copies is gonna be behind the other. That's just physics. And for some workflows that's OK, but for many workflows it's not OK to operate on a stale snapshot. And that's assuming that the organization even allows you to duplicate data in the first place, and many of our clients will outright say no, I am not willing to move data out of my data warehouse. It is managed by a specific line of business and in order to activate it, you must do so at rest. I won't have two copies. So the thesis is very simple, securely access the data at runtime, your workflows always are powered and your agents are powered by the latest data because there is only ever one copy, and that is the singular external copy. So fundamentally this is about maximizing the value of the investments that you've already made in different data products. Now data 360 copy is absolutely the keystone of the connectivity suite, but the biggest testament I can give you to how important it's been is that since 2024, its inception. It now powers over 50% of the traffic that comes into data 360. The overwhelming majority of our clients strongly rely on zero copy to access data from their lake houses and their warehouses, and someone asked a good question, Is that because of an outlier? Is it one tenant that went wild, or was it one query that was misfired? And the answer is no. The number of tenants that use ZeroCopy has grown steadily quarter over quarter. And their footprint and the workloads that they run have also increased in size quarter over quarter. And so what we're gonna focus on next in detail is the zero copy suite with AWS and we'll talk about the different patterns, the different connectors that are available, and we'll show you a short demonstration. So with that I'll hand it over to Bill. Great So we've heard a little bit about partnership. And we've heard a little bit about what it means to do data 360 in terms of Salesforce. Hopefully most of you here are joint customers of AWS and Salesforce already, and this is really where the thing, the magic starts to happen in terms of what we create, because one of the things I've noticed ever since I started working with Salesforce as an AWSSA is one of the primary things we have in common is one of the leadership principles of customer obsession. So when we listen to our customers and they come to us and tell us that they have specific things they want to do with their data, we work with them to try to create those mechanisms, and I think that's reflected on the slide, which isn't just a slide of the current state but also of the roadmap of what we're building out together to make it easier for our customers to have data freedom to be able to put their data where they need it. Which, and as you've already heard, as Sid said, in this new agentic world, your data has to be more fluid. It has to move to where you're running your agentic workloads. So let me talk about a customer example. So we'll walk you through these different things on the slide in just a second, but I was working with a FinTech customer earlier this year. Really great customer, loves Salesforce, has a bunch of different Salesforce products including marketing cloud, data cloud, using Agent Force. Big fans, love what they're doing. And so over time their suite of how they've integrated between their AWS estate and Salesforce has evolved. Of course they came to us with RDMS databases. They had RDS in place and they wanted to pull that data into Salesforce. So already GA now we had built what we call Query Foundation, and as you can see from the slide, it exposes several different AWS services including Athena, RDS, Aurora, and more as traditional data sources now. When I say a traditional data source, picture this if you're a programmer or if you've seen applications working before. It's simply a call out to the database. It does a query and it returns values to you. We'll dig into a little bit of what that means on the next slide, but for the purpose of this customer, it did what they needed. They were able to query their data, bring it into Salesforce, combine the data, and everything was good. Well, now fast forward a couple of years. Now we're in 2025, and most companies have data lakes, and data lakes take us into a different set of AWS services. We're no longer talking about our traditional RDM RDVMSs. We're talking about the flat file systems like S3 and Glue. So now they have these estates of data, and their question to us was, how do we now take this data and make it available to Salesforce. So we want to use customer 360. We're doing marketing campaigns. We're pulling in data from all sorts of different sources. We would like to bring those into Salesforce where we have our customer source of record, use customer 360 to identify all of those unique customers who we're actually going to reach out to, so we're not wasting a bunch of our marketing time. So the process that they went through with us was starting with File Federation, and as you can see, File Federation is actually in beta right now and it's going to be GA in February, so they're already on board, they're starting to build this out now and they're taking their data lake and exposing it over to Salesforce. The plan is to make this available, to let them create customer records, but now the interesting part. It's really not enough just to have that data in Salesforce. So think about the sort of trade-offs between AWS and Salesforce in terms of the roles who are consuming Salesforce and AWS. Most of the people you talk to who are actually logging into an AWS console, they're techs, right? They're geeks like me, people who log into the console, they're willing to go through these experiences. I generally don't expect the business users on my team to come into that AWS console, at least not frequently. On the other hand, while I am a Salesforce user. I don't expect to do a business person's job in terms of running queries and marketing campaigns and everything else. I want to unlock those capabilities for them so they don't make me do it. So, in a nutshell, for this customer, they didn't just want to bring that data back into Salesforce, they wanted to round trip it back to AWS as well. So this is the 3rd column we have here, data sharing. In a lot of ways, when we get into the details about what file federation and data sharing mean, they're actually the kind of the same thing in opposite direction. We're unlocking bi-directional copy for this customer. They wanted to create these cus customer 360 records, copy them back to AWS so they can again enrich their data lake, take all of that data that they've collected, and make their AWS estate of data more effective. So this bi-directional copy really was going to enable everything that they want to do, and you can see a few different services on here. So this is already GA now. You see EMR, you see Redshift, you see Athena on here. All of these provide AUWWS capabilities for the other end of this. Basically, if you think of this on the Salesforce side as Glue feeding data back into these AWS services, you'd be pretty much correct. So Glue kind of in both directions, allowing us to access data catalogs with Iceberg, and I'll get a little bit more into the technical details in a couple of slides. But using iceberg data catalogs to allow us to expose the data and without having the same type of copy and query, which I'll get to in a second, we're going to be able to access to iceberg data and we're going to enable us to have an interesting time sequence where we can get to that data frequently and less and more easily and as we'll see on the next slide here, with a number of different advantages. So query Federation, by the way, I'm not bad mouthing Query Federation. It's great if you need access to your traditional databases and you want to bring that data into Salesforce. That remains absolutely valuable, not going away. We're going to support that as long as you guys keep buying databases from us. On the other side though, there are some real advantages of file federation, and the very first line up here describes one of the main advantages. I'm an old programmer. I've programmed for a long time. Uh, JDBC drivers were something we did in Java back in the day, right? It's been around 2030 years, however long it's been since we actually invented Java. Uh, I guess I should know since James, James Goslin actually works for AWS now, but JDBC drivers simply provide you a pipe to go get data. You say, hey, I'm gonna run the SQL query, SQL engine, you execute this for me, return some data. It's always worked the same way. It works the same way here. We're outsourcing the query to the engine itself and saying, you just give me back the data. Every time I want that data, I hand the same query over, it hands me back a pile of data. The iceberg catalog capabilities that come with file federation are entirely different. Because Iceberg is a catalog, it allows us to over time create what we would call, uh, change data, um, change data flows, where we can actually say, hey, I'm going to pull this data in, but I'm gonna keep a sort of a cached copy of it because I have access to the catalog. Now instead of making real-time queries and every time saying, hey, give me all the data, give me all the data, give me all the data, I can create these change data sets and say, what has actually changed? And now because this is running on the Salesforce side and the queries are running in data 360, I don't have to be going back to that query engine every time and pulling back all of that data. I actually know what has changed since the last time I called it because of Iceberg, and this is a neat part of the capability, right? It allows us to take the, the change data feed, understand exactly what's coming in, what's come in before, and it makes it faster and more efficient. And because it's iceberg on the AWS side, it's also a lot more cost efficient, right? So you can keep your files in S3, you can use glue, and this unlocks a lot of interesting cost advantages. And in addition, on the cost side, if you're doing query federation, as you can see on line number. Uh, 5 there you can actually see you're paying twice for this data. You're paying to query it, you're paying us to query it in Athena if your data is in Athena, and then you're paying for the data 360 side of that. That's different with file federation as well. You're really just paying for the data 360 side of this, and you're not worrying about having to pay the AWS side other than what you're already paying us for your data lake. So it's actually a really nice advantage. There's another nice advantage that I don't want to overlook, and that's parallellyzation. Parallelization. When we are talking about how we get the data back over, this is an operation that allows us from the data 360 side to actually run up multiple, uh, multiple versions of the query. Remember how I said JDBC is like a single pipe, right? I can only have a single query, however long that takes, however long it takes it to stream back up. I can actually do parallelization on data 3 360 with file federation specifically. Because we have data file level push down, we have partition level pruning and data file level pruning, and all of those unlock parallelization in the native query engine that they've built from scratch inside of data 360. So instead of relying on our old school query engines that live on the other side, they've built an entirely new query engine that lives in data 360 and unlock. A lot of great capabilities that you want to take advantage if you're building. So if you're in any way thinking about a data lake, if you're thinking about building a data lake, remember that these capabilities that we're creating today are going to unlock these business use cases that go back going back to my customer, enabled them to do specific business functions, honestly, they wouldn't have been able to do otherwise. So it's pretty cool. It adds a lot of new capabilities, and there's other things on here that are a little bit different. You can do more file in bigger file sizes and more files with file federation. A lot of those details are important for specific cases, but really I want you to focus on the fact that you're using different types of services, right, traditional data sources versus data lakes, and that the iceberg allows us to keep a changed data data set over time so that we're not having to reach back and pull back all of the data every time we query. Pretty cool functionality and I'm pretty excited about it. Let me bring up a slide that kind of hopefully brings a lot of this together. I know I'm throwing a lot of sort of different angles on this, but remember how I said the data is flowing in and it's flowing out. This particular slide is designed to sort of show you how these different pieces fit in. In your AWS customer account again you might have your databases, you've got your, your data lakes, you've got S3, S3 tables, all of those different data estate resources that are available to you. Well, query federation and file federation are sitting there as pipes to bring your data into data 360. So you can see there we've got JDBC on there and then we have table scans, which again implies iceberg. And you can see on the data 360 side, if you look a couple of things down from data 360, iceberg tables. So data 360 is also operating on iceberg tables. In terms of what we're actually working from my customer use case, in their case they were trying to unlock some business, uh, solutions around marketing, but this is important for AI agents. You could take the same scenario with a customer and talk about how they want to run agents on both sides. They want to run agent for us. And execute agents against their customer data and salesforce. Maybe they want to run Agent Core over on the AWS side and run that against their AWS data. Unlocking this data flow bidirectionally allows more agentic AI flows to come to life. And of course analytics and the traditional ad platforms. AIML platforms, all of those are unlocked by this data flow. Finally, at the bottom of this, I want to call out, remember this is bidirectional. You can see that we actually show a Glue data catalog exposing this back into AWS. And again, I said that file federation and the data access going back, it's almost the same thing just in reverse. Both sides are using Iceberg. Both sides are using glue, and in fact both sides are also using something called lake formation. Lake formation allows us to do fine-grained permissioning on data sources in AWS. Some of you may already be familiar with it. If you're doing data lakes, you might already be using this to apply permissions to your database. So, as you can see, it can apply to a number of different sources. We often see it come up, of course, with our data lake function with our data lake functions, and I think that's kind of the key word there. It is a lake formation, right? But you can also use it with a number of different sources as you're pulling them into your data lake. Now that fine-grain access control. is particularly important when we're talking about file federation. If you've ever done any sort of remote work with AWS, you know, one of the ways that you can provide access to something remotely is to take long-lived credentials and copy them over there. It's not really considered a great practice to do that. I know we've there you can find lots of resources out there saying, hey, just get your key and your secret and copy it over here and store it somewhere. It's kind of sad because there was plenty of documentation saying to do that for years, but. If you're doing anything today, if you can find a way to take advantage of short-lived credentials, please do. And this means I am rolls, and in this case it means using lake formation. Lake formation allows us to vend short-term credentials from data 360, so we don't have to keep those long-term credentials on the data 360 side specifically for file federation. And in fact it's using SIGB4 in this case, right? So if you want to understand how this actually works. You can kind of dig into it. We're taking SIGV 4. We're copying that over into data 360, and that gives it API access over to AWS through Glue, right? So this is actually an important piece of the solution. And if you remember nothing else out of that, that short term vending credentials, that's really what unlocks better security for this file federation flow. I'm gonna end right before we go back and show a little bit of a demo of this talking a little bit about glue. Hopefully you've kind of gotten a high level visual. Glue allows us to do uh to to um allows us to do Apache Iceberg, right? Apache Iceberg, if you haven't worked with it before, allows you to take simple objects in S3 and turn them into tables. It sounds a little crazy. How do you just take a bunch of files and turn them into tables? Well, it actually can extract out a catalog that describes the data and also going back to the changed data follow we were talking about, captures what that data state is over time. This is really where Glue is pretty magical, and Glue does a little bit more than this. There's a couple of things I'd really want to call out. You don't have to remember all of these things, but it's serverless. Increasingly with AWS, if you can find a way to take advantage of serverless function and not have to manage compute on your own behalf, please do. This is the way a lot of things are going. If you're looking at Agent Core, it looks very similar, right? Manage services that are serverless and you don't have to manage the overhead. It's got great durability because this is backed by S3. And all of the same types of functionality in terms of durability, availability, scalability that you're relying on in the underlying S3 instances also start to emerge in Glue as well. It integrates with all sorts of different catalogs and of course it provides through Athena additional querying in terms of getting to your data. With that, I think we've effectively covered most of the stuff here around the AWS side, but I do want to hand it back to Sid, and we're gonna walk you through what this looks like. Hopefully the internet holds up and we'll show it to you live. If not, we actually have a bit of a recorded demo, hopefully we won't need to fall back on that. Thanks Sid. Thanks Bill. OK, so there is an actual booth where there is a full-fledged end to end demo, and that's not what we're gonna show here. We'll just touch on a couple of components of what Bill described so that we can put some pictures and actual components to the different things that we talked about. So this is Data 360 set up and it's where you configure the connections to the different systems in AWS and let's take a look at a couple of them. Because there are many different AWS services that we can connect to. So as Bill mentioned, we span query engines such as Athena. They're your traditional relational databases such as Aurora MySQL and the corresponding RDS flavors. There's also Amazon Redshift. This is a query Federation connection. And then there's AWS Glue and so I wanna briefly clarify again one more thing with Query Federation just to emphasize it's a single threaded connection to an external warehouse. And even though data 360 will try to push down as many operators as possible, we're ultimately limited to that single thread, and that's something that is not true with file Federation and is very important when it comes to massive volumes of data, which is typically what clients use data 360 to execute. So let's take a look at the file federation connector for Glue, which is currently offered as a beta feature. Now, as Bill mentioned, there are two services when it comes to an iceberg data lake house. There is the metadata catalog, which is in this case Glue, and the underlying object storage bucket, which is in this case S3. Now here you'll see that you have to provide an access key. And a secret access key which correspond to the IAM user that is being given the appropriate permissions to communicate only with the catalog. Nothing long lived related to storage needs to be given to Data 360, which is the security best practice, and that's because glue will vend temporary credentials, as Bill mentioned at runtime. When the workflow terminates, the external data is forgotten, and so too are the temporary S3 credentials. So I have a couple of different connections already configured to Redshift, Query Federation, S3 batch ingestion, and glue, which is foul Federation. Let's enter data 360, the application proper. We'll exit set up. So here are a couple of different data streams. Now a data stream is nothing more than a link between data 360 and a specific external table. It captures metadata, particularly what are the columns that you want to project into data 360. So let's take a look at what happens when I try to create one for AWS glue. I'll select the connection and what happens here is we query the metadata catalog to understand the objects that are exposed to data 360. The database pick list corresponds to the different name spaces that you've configured in your iceberg glue catalog. Now in this case, the schema's menu is empty, but if each one of these name spaces in turn had a name space, they would be displayed over there because the specification for iceberg allows for arbitrarily deep name spaces. These are the different objects that are available, so let's see one of those in action. Now there are a variety of different applications, but it's not easy to demonstrate zero copy in a way that is visually illustrative so I'm gonna take the simplest of them, which is query editor, and then we'll take a look at a CRM object that brings data in real time from AWS. So in query editor, and this is just firing a simple SQL query. I'm going to select The glue object And we'll just run a query, we'll bump up the. And what happens here is we fetch the data at runtime. Now this looks really simple, right? It wasn't something crazy, but the point is that we query the catalog, we get the metadata, we parallellyze execution, and when I close the tab, the data is forgotten. So let's take a look at something that is a little bit more practical. And we'll enter an application, a CRM application, so one of the customer 360 apps. Click on the contact record. Now let's imagine that this is a resort. And they're bringing in data from Redshift and they're bringing in data from Glue and they want that contact record to display information relevant to the customer that resides in those two systems. And these are displayed as related lists. So I'll click on Sofia Rodriguez and let's go to related and this is where the data doesn't live in data 360 it's gonna be fetched once I go here. So if I refresh the tab, refresh here too, this is the data that is coming from Redshift and from Glue. So it's not stored over here. Now how exactly did this happen? Let's go back to data cloud. And I will show you an example of what I mean by that blueprint that we talked about with all the nouns and verbs. I'm gonna switch to graph view. And so what I have here are the different objects that are important to my business. The first thing is an individual. That has a relationship with reservations which are stored in red shift. And many other different objects, but the important thing to understand is that not every one of these is an actual table in data 360. It's just a pointer to something in AWS. Metadata is something that pops up in almost every Salesforce presentation, but this is what we mean by that we're just capturing information about how you have structured your data state externally. Now the last thing that I want to show. Is identity resolution. Now this is one specific example of. Resolving information. About the same customer that resides in different data sets. Now some of this information came from Redshift, the guest data, and some of it came from CRM, the existing contacts. And you won't actually see zero copy in auction in action here, but notice on the right side the consolidation rate. That means that you had multiple records in AWS and multiple records in CRM, but they weren't all describing distinct clients. They were, they were describing in many cases the same person, and that's reflected in the consolidation rate, saying that it turns out that we only had 50 customers and not 100. And that's how you're able to get that unified profile in CRM. Which is the contact that I showed you. Now, like I said, there's a full fledged demo at a booth, but before I go there just, there's one more thing that I wanna show. To clarify what Bill said about change data feeds because that's something very important. Now let's take a look at something called a data action. I'm gonna create one manually. Let's see, I don't have a data action target. OK, let's see if I created one. Let me. All right, while this loads, I'll explain it verbally. But the point is that a data action target relies on. It's a trigger that says when a record gets created I want you to go do something when it gets deleted I want you to go do something now in query federation if you think about it conceptually, there's no way to detect that something changed in an external table because all I ever see is the most recent version. And to determine if something changed, I need to have a historical record of how the table evolved over time. But with query federation, you can do things like data actions because the iceberg catalog glue in this case gives us different snapshots for the same object. So you say, if something happened to my insurance table, I want you to go take an action. And with file federation, you don't have to copy that table into data 360. The second benefit is performance. For many workflows like segmentation and identity resolution with query federation. You're operating on the entire table every single time, but with file Federation you're only operating on the increment, and that's because we can calculate what that increment is by looking at the metadata catalog. That results in both higher performance and it results in lower costs. So these are the different building blocks. I think that's pretty much what we wanted to show. And Q&A or customer stories, yes. Great. Well, thanks, Sid. I think I would wanna stay at Coral Cloud Resorts given that it sounds like they know how to, um, understand their customers, have a full picture of their customers, and be able to deliver personalized experiences, um, but of course that was a fictional scenario, so we did actually want to share with you guys the results that customers are already seeing from taking advantage of the solution. So, um, 1-801-800 account is a great example of a customer. They are an accounting services. Firm who helps small businesses with all of their accounting needs and I recently had the pleasure of talking to their CTO who was telling me that previously their data was siloed across multiple systems. They had Salesforce, they had data lakes, they had custom tax software systems, and during the tax season that was really tough for them because their client advisers were spending a lot of time managing customer inquiries with data scattered all around their systems and so they decided that they really needed to. Scale their workforce to be able to support their customers and they leveraged Zero copy to unify all of their different systems that they had and allowed their client advisers to actually be able to focus on high value conversations with their customers. They deployed AI agents that were able to answer 70% of the administrative questions that, you know, um, customers were, um, asking on a regular basis and really allowed their client advisers to be more scalable for their workforce. Additionally, another great story was Buyer's Edge. They are a procurement, um, company that helps in the, uh, food industry, and they actually were able to unify 20 plus different systems together to again be able to create that unified view of their customers and be able to deliver them with the right personalized offers. So with that, I hope you're inspired by the demo and all these different customer stories like Sid said we have a great more in depth demo available at our Salesforce booth and experts that would be really happy to talk to you guys about this. Um, additionally, you can scan this QR code to learn more about our partnership and as always feel free to contact your Salesforce account team. Awesome thank you so much for your time we really appreciate it and yeah.