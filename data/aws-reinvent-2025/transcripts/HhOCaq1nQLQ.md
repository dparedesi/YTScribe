---
video_id: HhOCaq1nQLQ
video_url: https://www.youtube.com/watch?v=HhOCaq1nQLQ
is_generated: False
is_translatable: True
---

Uh, welcome to our session. My name is Felix. I'm the product owner for the accelerating Compute in media. Um, I'm gonna get started. So today, this is, we have a full agenda. There's lots of stuff to talk about. We're gonna introduce a little bit about the rapids accelerator first, right? Uh, and then my friend here from FIMR is gonna talk about FINRA and their story around be able to leverage the technology and accelerate their, uh, workload. So, uh, without further ado, just go get quickly into the context here, right? Apache Spark is one of the most popular data processing framework out there. It's basically used by almost every single enterprise and organization out there for a number of different kind of needs, right? Whether it's, you know, just doing analytics, right? Reportings, or even machine learning, you see. But, um, more importantly, data is growing at this crazy exceptional rate in the last couple of years. So, you know, obviously, in the last year, maybe more for AI and whatnot, but it's, it's definitely growing at an exceptional pace. So, in order, in order to, in order for enterprises and organization, To be able to kind of handle this exceptional growth, we're introducing kind of the GPU acceleration factor. So, what you see on the right-hand side here is a stack, right? So, you have the Apache Spark workload. Basically, Spark application, um, running here as is. We're introducing a plug-in that you can kinda Install into the workflow itself with no code change, and then leveraging the highware acceleration layer, the GPU you see running on the cloud or on-premise. Um, and then get you the cost saving, um, and faster result. So, um, even though the GPU might cost some, you know, some amount of money, it's not really a whole lot. Um, but the acceleration is so much that basically, you get, you know, your, your data, you, you resell faster and cheaper. Uh, and so that's kind of what we focus on. And to give you an idea, these are the kind of companies and enterprise organizations that's kind of gone public to talk about their use of this technology. In the last two years or so, there are definitely more of that, but. Um, so, you can kind of see a pattern like online services, retail, uh, finance services, and that's kind of more what's relevant today as well. So, I can give you a little bit more, one other example here, especially around fraud detection. So, with fraud detection, oftentimes, you have to process like billions and billions of records, right? You go back like 5 years, 10 years, 15 years, kind of looking at patterns. That, that pattern is actually using time series windowing operation and analysis. And that's actually something that's really good with GPU. And so, as a result, we're able to get 14 times speed up. In this particular case, at this type of workload, at an 80%, uh, well, 90% saving. Um, and to, if, if you wanna take a look at this, we actually partnered with the AWS FSI team, uh, last year to make this post. You're welcome to use this QR code. Open up the web page and take a look at it yourself, kind of what it looks like. And we'd love to kind of be in touch with you as well. So, um, without further ado, we'd like to introduce our friend Finwa, uh, here to talk about their experience working with this technology. I'm super excited to actually talk here because last year, Is our presentation actually what got them interested in this, and this is how we were working together to, to work on this. So. Perfect. Thank you, uh, thank you, Felix. Thank you for the great introduction on Apache Spark rapids running on, uh, Nvidia GPU uh chipset. Um, I'm Alna Mangi, the senior director of, um, uh, big data and performance engineering at FINRAM. Uh, I've been working for FINRA for more than, uh, 20 plus years, and, uh, I can tell you that I've been involved in many transformative and innovative, uh, projects, and this one, spark, um, uh, Spark rapids on GPU is absolutely one of them, yeah. Yeah So, first, who is FINRA? Um, FINRA has been around for a while. Uh, we were celebrating our eighty-fifth anniversary last year. Uh, our mission, mission is twofold market integrity and investor protection. We are making sure that all the participants can trade with confidence and that the markets operate free from market manipulation and fraud. Basically what we are doing, we are protecting you as investors to ensure that your money is safe and sound. So the nature of our business makes us an AI and a big data company. We are operating over 1000 petabytes of storage in the AWS cloud. In March this year, we've processed a peak volume of 1.5 trillion market events for a single day. Yes, I repeat, 1.5 trillion market events in a single day. Processing this volume require a massive technological infrastructure and sophisticated system. So quickly, our business flow after the market closed at 40 p.m. Eastern time, the member firms have the obligation to report their trading activity before 8 o'clock the next day. Uh, from there we have only a few hours to reconstruct the market activity. Then we are going to run hundreds of pattern models looking for market manipulation and fraud. It generates alerts if it finds some abnormalities, and those alerts are sent to our investigator team, uh, then they can interactively query the system to get the better of the understanding of those alerts. Market volume keep going up. And over the years has been growing dramatically, and the trend that is still going to go up the next couple of years. Modern electronic trading platform and high frequency trading system generate far more transactions than before. And as volume scales up, so does the computing need to handle that volume. Our challenge is not only business logic, it's also scale and cost efficiency, and that led us to evaluate the GPU acceleration option. As you can imagine, Fran. Needs to constantly adopt. We cannot stand still. We are always looking for ways of new capabilities that will reduce the cost, improve the the performance, uh, and the concurrency, as well as strengthening our resiliency and security. And whatever we adopt must make performance and economic sense. So as you know, AWS and the open source community are constantantly releasing new hardware, new services, and new execution model. So for us, evaluating this innovation is not something that we do once a year. It's part of our DNA. This is something that we are constantly doing. Uh, it's not about chasing shiny objects. It's about deciding which EMR version or Java or Spark we will be using. This ongoing benchmarking guide our architecture, roadmap and platform decision. So at FINRAM, we rely on a benchmarking suite that blends industry standard and real FINRA production workload. We use Terrasort to stress large scale IO and TPCDS to evaluate SQL planning, joints, and aggregation. We are also benchmarking 66 of our regulatory workload, uh, with very large data set, heavy shuffle, and complex joint pattern. So for for the sake of time, I will skip that slide, but understand that we are doing a fair comparison between the runs. We have the same input data set, we have the same spark configuration, the same cluster configuration between runs, and of course we validate the output. We are making sure that the result does not change between the runs. Mm So this is really the Moore's Law in action. Look at the performance improvement over the years. Just from the natural evolution of EMR, Spark, Java, and underlying hardware. Those are the results for our TPCDS 9 terabyte benchmark, uh, that we've been running through, uh, all those years. No code change, same data, same logic, just newer Spark version, newer engine, and newer instance families. We went down from 9 hours 5 or 6 years ago when we were using EMR 524 to 1 hour and 45 with the latest EMR 710 that was released just a few weeks ago. That's a 5 times performance improvement, and you can imagine the cost reduction by uh by jumping on those newer technologies. This is really the Moore's laws in action. And that's where Spark GPU entered the story. So this is a slide that I believe I saw a few months ago on the presentation from the Nvidia team and it it it really caught my eyes. We are early adopters in the cloud. We, we, our first workload on the cloud was in 2013. Back in the Adoop era, we were using Apache Hive in order to run our query, our SQL query. Then Apache Spark came along and changed the game in memory processing gave us a major leap in speed and efficiency. So then when I saw that slide, I said, 000, this is really speaking to me. So is GPU accelerated spark the new leap in technology? So that's what we want to try out. So let's use TPCDS 9 terabytes, and wow. Breaking the one hour mark. From 53 minutes exactly in order to run the exact same workload that was running for 1 hour and 45 minutes on EMR 710 and the latest are HGD instance type. Uh, that's roughly 50% performance reduction in, in, in uh in um reduction in time processing time, and it's also roughly 50% in cost saving. Because of course the cluster is running for a shorter period of time. So faster, cheaper, without doing a code change. So that's the moment that say, well, let's do it on some real funeral production workload. So that's one of the cases that we are having here. This is not synthetic. This is a real Fra production workload. The total input of this data set is 11 terabytes, and this pipeline is really a joint heavy and shuffle intensive. Uh, we are joining two very large tables. 50 billion records in each of them. One of them has 125 rows, columns, and the other one has 25. And we are joining the key by processing dates and some raw ID, which is one of the business logics in our application. Um, here the, of course, the, the, the, the data is partitioned by trading date. Um, we didn't change the code. This, uh, application is written in Scala. And then we run, we run this application on CPU and on um GPU. And again, what we see blew us, blew our mind. Again, 50% performance reduction and lower cost of around 45%. Um, No code change, uh, and this is really where we start to sell ourselves, this, there is something that we really need to explore because this is mind blowing from a performance and cost saving opportunity. So One of the key takeaways that I want you to remember, we didn't just switch the flip, flip the switch in order to enable that massive performance gain. On our first try, the GPU run ran for 5 hours. So then we did contact the Nvidia team. We had a very good collaboration between the two engineering teams in order to identify what was the bottleneck. We explored the plan. We saw that sometimes the GPU was falling back on the CPU, identify a bunch of bottlenecks, and the Nvidia team came up with a new plug-in based on those findings on this special workload. Um, so A key takeaway, I say yes, no code change, but it's not plug and play, not always. Then another workload. So this is the key data extraction. I've been telling you that the member firms are submitting their files to us, so they are sending CSV BZ2 files. We need to decompress them. We need to read them, pass into typed columns, and then from there we need to convert them to a parquet format using the snappy protocol, compression format. Firms are sending us hundreds of thousands of files on a daily basis, so this is an application that is going to run quite a few times over a single day. So here again, iteration process. At first CPU GPU, we were even. And then again we collaborate with the Nvidia team in order to identify the bottleneck and uh what we've seen is that the original application is using the data set API and that was not really a good use case for the GPU. So we had to transform to a data frame API and by doing this, by doing multiple iterations, we were able to reduce the time by 2 and then at the end of the day by 10, so which was a major improvement. But again here a very strong collaboration with the Nvidia team in order to arrive to those results. So take away, um, we look at benchmark, industry benchmark, and we look at some funra production workload and. And What worked very well was that yes, we were able to get 2 times faster a join and lower cost on some of the workload that we try. Uh, rapids integrate very well with SUL and data frame pipeline. Uh, no code change in some cases were needed for for some pipeline, and yes, the run time and the cost savings were consistent once we tune up the configuration. What was hard was the GPU memory had some limitation. The spill to this where it needs to be managed very carefully. Some operators fall back to CPU creating mixed execution plan, and the rapid configuration is not straightforward. You know, open source is a little bit finicky and tricky, so one change in one parameter can do some drama dramatic performance improvement or sometimes performance down. And also one of the issues was the GPU instant savability. It's not always easy to get to get them, you know, especially if you, if you need Android of them. So we need to establish some relations with Nvidia in order to be able to get our capacity for those types of workload. So Where are we heading? So I think this is the, the, the, the, the end of the first, uh, proof of concept that we did with the uh uh GPU and Apaches Park rapids and, um, so not every workload get advantage, or you are going to need to spend a lot of time in order to identify what is the bottleneck. Now we have a process where we can validate the CPU versus GPU execution, execution time, so we have a process to test more. We are hopeful that GPU evolved from experimentation to a strategic performance level based on the performance benefits and cost reduction that we've seen. Uh, this is an evolution for us. Uh, we are going to add uh GPU and Spark rapids into our big data, um, work stack. Uh, however, today, CPU remain our default. But GPU certainly will be our future accelerator path. And I'm right on time. Perfect. So, um, uh, thank you, and we are happy to take questions. Um. One more. Oh, there's one more one. Here we go. Welcome to try it out. This is the URL. You can go in and, uh, welcome to contact us in this email as well.