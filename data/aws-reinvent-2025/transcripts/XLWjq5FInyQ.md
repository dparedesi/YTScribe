---
video_id: XLWjq5FInyQ
video_url: https://www.youtube.com/watch?v=XLWjq5FInyQ
is_generated: False
is_translatable: True
summary: "In this \"practitioner's guide,\" Tim (Senior Principal Engineer, Aurora) and Siva (Director of Worldwide Specialist Solutions Architects) detail how to architect data systems for Agentic AI, using a car insurance purchase scenario as a driving example. They trace the rapid evolution of AI from simple chatbots in 2023 to 2025's autonomous agents capable of reasoning, planning, executing tasks, and maintaining memory through a \"Reason-Act\" (React) loop.\n\nThe core of the session focuses on the Model Context Protocol (MCP), which Tim describes as the new standard for connecting AI agents to data and tools, analogous to how SQL standardized database interactions. MCP allows agents to discover and utilize tools (via `tools/list`) without hardcoding specific parameters, enabling dynamic interactions with various data sources.\n\nThe presenters introduce a three-tiered memory model for agents:\n1. **Short-term (Agent State):** Working memory (RAM-like) for the current session.\n2. **Medium-term (Semantic/Episodic):** Retrieved via tools from databases or vector stores.\n3. **Long-term (Prompts):** Static instructions or \"infrastructure as code.\"\n\nThey also discuss caching strategies to optimize performance and cost, suggesting caches at the database level, tool result level, and even a \"semantic cache\" for user responses. Tim maps these concepts to relational database internals to help engineers understand the parallel: MCP is like SQL/JDBC, Agent Memory is like DB indexes/heaps, and the React loop is like a query planner.\n\nSiva then dives into the data architecture, addressing challenges like data silos, lineage, and security. He proposes a \"Data Marketplace\" architecture where data producers (e.g., claims department) create governed data products, and consumers (agents representing users like \"Terry\" or internal staff like \"Nikki\") access them via Data APIs exposed as MCP tools. He emphasizes \"Trusted Identity Propagation,\" explaining how user credentials (via JWT tokens) must be faithfully passed from the agent to downstream systems to enforce fine-grained access control (e.g., Lake Formation filtering rows based on a user's role). The session concludes with a reference architecture that positions MCP as the universal connector (the \"USB-C\") between the AI agent world and the data world."
keywords: Agentic AI, Model Context Protocol (MCP), Data Architecture, AI Memory, Caching, Data Governance, AWS Bedrock, Identity Propagation, Lake Formation
---

Alright. Hi, welcome to Dat 315. Hopefully some of you are practitioners. In data maybe, right, I'm a practitioner in data. Maybe you're a practitioner in um Agentic AI anybody? Well, by the end of this, hopefully you'll probably be practitioners in data and Agentic AI together, so that's what we aim to do. Uh, my name's Tim. I'm a senior principal engineer in Aurora. I'll be joined shortly by my friend Siva who's uh the director of worldwide specialist um solutions architects. And we're gonna talk about practitioner's guide to data for agentic AI. So, if anybody thinks they've completely got their arms around all of this. Congratulations, you're doing better than most of the rest of the world. This is changing really fast, right, so the first thing I wanna do is just look at this evolving landscape a little bit, right? So we started off in 2023, not very long ago. We had these chatbots, we were talking about vector search, we were talking about um LLMs just beginning, right? We had this single shot interactions with the agents, that's what we could do. Then in 2024 we got these more advanced chatbots, we got context, the ability for an agent to um remember a little bit about what it was doing, to do hybrid search, to bring in other data types in here, we had rag, retrieval augmented generation, that was all the rage last year. That was so 2024, right. Then near the end of 2024 came MCP model context Protocol, which gave these agents some tools to talk to each other and data sources that standardized things. And then now 2025, nearly the end, we got to autonomous AI agents. They can reason and plan and they can execute tasks. This all happened in, count them, just a few years, right, and we're still, I don't think anywhere near the end. But now we can have a much more natural conversation with our agents and they can do things much more autonomously for us. So today we're gonna follow an example. We're gonna buy some car insurance, which is I think what everybody loves to do all the time. Buy some car insurance. And we're gonna pretend that we're Terry who wants to buy some car insurance. And back in 2023, your AI experience would have been interesting, but it would have been probably just a thin wrapper around, some pre-processing, and then hand off to a human to actually do the work. Now in 2025, with our auto insurer example, the agent's able to um have a much more fluid conversation, and it's able to actually do probably the entire task for Terry rather than excuse me. Um, rather than handing off to a human. Whether we like it or not, that's what's gonna happen, that's what we're gonna dig through today. So we'll cover some fundamentals, we've got these 3 pillars of uh genetic AI. So first off we have reasoning, this is where we use our large language models. And uh then we have action, we're able to take action from these models, we're gonna call some tools, we're gonna do some things as instructed by the LLMs. And then we have this memory, we have to remember things, otherwise it's like talking to a goldfish who can't remember anything, right? It's very frustrating, not very useful. The agentic AI thing, the special source there is we put this in a loop, we're able to iterate this, and if you've done any computer science stuff, writing a program without loops, pretty difficult, pretty boring. Once we've got this loop, this react loop, reason and act loop, now we're in agentic AI space. That's where we are today. OK, so any company car insurance. Terry needs to buy some car insurance, we'll come through this loop probably a ton of times before we make some answer like your insurance quote is $10 thanks for your business. OK, so we'll dig in just in one iteration of that loop to start with, reasoning and planning. So the user puts in their request. Inside here we have the system prompt. We're managing context, the system prompt comes in, we have some existing state maybe, we have the user request come in saying I need insurance for my Buick or whatever. Next part, we go through the LLM. The LLM does magic stuff. I'm not a scientist who can tell you about how LLMs actually do their work. And then we pass the output from this thing. We make an action agent makes an action plan. We work out what the next step in the action plan is. We might be stopping, we might be uh sending a response back to the user. We talk about this acting and managing context thing, cos that's where we can first start to talk about data, right? After we've called a tool or maybe we call the tool to get the data and send a response back to the user. So that tool does whatever the tool's gonna do. And then it maybe access some data store, this is what we're gonna talk about today. And then we're gonna feed these results into this context manager. And that's gonna maybe do some caching, it's gonna do some uh updating the conversation history so we can remember, and it's also gonna evaluate what it was doing for reinforcement learning, so that we can make a better experience of this iteration for the next time we come through, maybe for the next customer. That's important as well. So, knowing all of that, we'll actually step through our car insurance example just a little bit, and I'll need you to pay attention to the colors here. So the first interaction says from the customer, Terry, I need some car insurance. We get the system prompt, that's the purple thing here. We go to the agent, we give that, we give whatever other context we have, maybe from previous conversation history or something. We format this up, we give it to the LLM, it does whatever it does, we get the output out again. We determine the next step, and maybe the next step is to call a tool. Now this is the first interaction, so we don't know anything about this customer yet, so we need to just call a tool called get driver details. We need to work out what this driver's history is about. So this gives us a purple kind of result. So now we're gonna go around this loop again. You can see in the state we've remembered the purple result, we remember the driver's information and now we're gonna go around and we're gonna invoke a different tool to find out about the vehicle's information. Again, the pink information comes in, we remember that, now we're gonna so calculate some risk about that vehicle and that person. Again, we're gonna get a quote, so see how this the context is stacking up. Finally, we have enough information to generate a response for the user, give it to the user and wait for their input returning again. You'll see how the context all those colors is stacking up together. So this is great. What happens though when we need to keep going, that context is not infinitely large. I'm a really good artist, you can see from my great colors, so we need to do something called compaction on our context here. What we've done is we've merged those colors. So there's a few different algorithms we can be using here, you don't have to worry about this too much. The framework provides it for you, but we can be doing summarization. We can be asking the LLM to summarize it for you, we can be doing simple like least recently used kind of thing in there. Different algorithms to throw away the bits that we maybe don't want and remember the bits that we do. The important thing is that we've gone around this loop 7 times in this case, and we've remembered most of what we've done. That's the react loop. So the memory thing, this is the interesting part. We've got this continuum, short-term memory on the left, long-term memory on the right. We're gonna dig into this some more agentic memory. In the short-term memory we have this agent state. This is kind of like the agent's RAM if you like to think about it that way, it's working memory. This is where um the framework's probably dealing with it for you. You probably don't actually have to code this thing up yourself. Um, you want it to be fast, it might be like Elasticash for Valky here, uh, strands agents as a framework, Landgraf as a framework, they'll typically provide these things for you. Right, and they're simple to use, small, short-term. And in the middle, we have this medium-term semantic memory, episodic memory. So this is typically we're retrieving it using tools. We're going through a tool to get to the source of the memory or to put stuff into the memory there. This is where we'll talk about enterprise databases, vector stores, other things like that. So you probably are more aware of what's going on here. Then on the far right, we have prompts. So this is more you're probably treating them like um, Infrastructure as code kind of things, this is where your builders are putting in the system prompt to say you are an insurance agent, here are your guard rails and things like that. You're probably managing like code in a code artifact repository or a key value store or something like that. You're not accessing them very often. You're not modifying them very often. That's the way to think about these three kinds of memory. OK, so now we're just gonna see a little bit of code. I'm an engineer, this makes it look easier to me. We have two different examples here. I've chosen strands agents and lang chain and Lang graph. Now, if you don't follow code, that's completely fine cos there's not really very much here. All we really need to focus on is this couple of lines at the top there which says I'm gonna use an agent core session memory manager in this case, um, you could use another one. I'm gonna use a conversation manager, summarizing conversation manager in this case, that's one that uses that summarization algorithm to smoosh the colors together like I showed before. And then I'm just gonna pass it down to the framework, that's it. In the lang chain version it's pretty similar, they just use a different name for things and bundle it all together into a thing called a checkpointer. Same idea. So you're not really writing all this code, the agent's probably writing it for you anyway, right? Um but very simple. OK, so we'll dig in a little bit more, we'll just talk about um how we connect the agents to the data. So we get our inbound user query. We'll go through an MCP server probably, we'll dig into what that means in a little bit and we'll talk to some database to maybe get customer profiles. And then we'll go around again, maybe we'll talk to a different database to get vehicle data and telemetry, and we'll go around again, and you can see we're talking to different kinds of databases, maybe they're relational ones, maybe they're key value ones, maybe they're vector ones, maybe some of them belong to our company already, maybe some of them are third party ones like the Department of Motor Vehicles in my example. You're calling these things all through tools, MCP service. Right, so the same basic pattern is applied across everything here, that's the joy of MCP we don't have to write all this special code to deal with this. So how did we get there? Those tools, we need to find them first. So the same example, we just look at the other part there. I'm creating an agent, instantiating an agent. I'm creating a bedrock model to go with it, and I'm giving it a list of tools. There are 4 tools there, search documents and so on. And a system prompt, you are an insurance agent. Now I'm not in love with how I had to specify or hardcode the tool names there, so I'm gonna come back to that one in a minute, but you can still see this is pretty simple. Gonna change gears a little bit and talk about the power of caching here for a minute. So you know all the fundamentals here. You would've probably noticed that every time we go around this loop, we seem like we're doing a bunch of maybe duplicate work. We're fetching some data again, over and over again. Maybe some of that will make things slower, maybe some of that will cost us some more money. We don't like that. Caching is a pretty natural approach here. So here's our similar picture and we'll talk about places where we can put caching into this. Um, why would we do it? We can reduce latency, that's pretty standard. These LLMs, large language models, they're not called SLMs, small language models, right, they have a bit of a cost to them. So if we can avoid calling them or we can avoid using up their token space, we can save ourselves money and time. So caching's a really good way to do that. Even if the backend system is not a uh an LLM, if it's some other kind of database, it still has a finite capacity to deliver some performance, so reducing load on that's probably good as well. And again with this reinforcement learning thing, caching is a really important way to get signals to see if something is popular. So if I'm getting lots of hits in the cache, then it's probably a popular thing and I should cache it more and I should update my plan to do more of this thing. So where can we insert some caches? Over on the right-hand side, this is the closest to the data source, so the the lowest in the stack if you like. This is probably the most familiar to what you do already today. You've got a database, you put a cache in front of it, right, an elastic cache or something like that. This makes your tool responses faster, but it doesn't do anything to avoid going around this loop, just lets you go around this loop a bit quicker. So it probably doesn't save you any money or or context on your LLMs. Next step up, we can put a cache on in the memory context manager just after it's called the tool. Alright, this is on the tool results. So this might reduce your LLM usage, this is the first one that does this. Um, you can reuse an existing action plan maybe and and you know the sequence of tools that can that can be called, so you don't have to ask the LLM to do this every single time. This is pretty optimistic if it helps you, but it's useful to think about. Next we can cache on the other side of the memory context manager, on the output side if you like. So this is within the scope of a user's session probably. Um, so if your session contains a lot of cacheable information, messages that don't change very often, like in our example, we're going around and telling them the vehicle information over and over again, that kind of thing. That's the kind of thing we'll be using here. This won't affect your LLM usage appreciably, but it will um make things go a lot quicker. And finally, you can implement a semantic cache which is as close to the user as you can get before this whole loop iterates at all. So if this works for you, this is really, really powerful because you can avoid calling everything. Now this works for things like where you don't have personal information inside your agent responses, maybe your your agent was being asked a question like, just tell me the legal requirements for insurance in Nevada, right, there's nothing specific to me about that, so I could catch that response. That's the kind of thing we would use there. So this saves latency, saves cost, makes your customers happier cos their interactions are more smooth. Right, so now you're all kind of experts, or maybe you're all just bedazzled. So one thing that I'm a crusty old Unix Greybeard, right? And so I would like to share with you a bit of a an analogy to see if this helps understand what's going on here a little bit. So I'm gonna draw an analogy between Agentic AI and relational databases. If you know relational databases, great, maybe this works, if you don't, don't worry, it's not gonna be on the take home assignment, um. So first we'll talk about the interface. What's the interface into a relational database? Well it's SQL and it's JDBC it's all this very standardized stuff. And Agentic AI is getting that as well with MCP so they're kind of similar. Then there's this idea of memory we've been talking about along today. You've got short-term memory and long-term memory for Agentic AI. And inside a relational database, of course you've got memory, you've got bee trees, you've got heaps, you've got indexes, you've got all that kind of thing. Then there's the execution angle to this, so we just talked about this react loop where we plan and then we execute the steps of the plan. Well, in a SQL engine, you have a SQL query planner that's planning how to use indexes and heap sorts and all those other kinds of things, quite similar. And we got caching we just talked about, that's very one for one. Databases work heavily on caching. And finally we have got this thing we call canned execution. So MCP you can put whatever tool you like behind an MCP server to execute whatever operation you want. And inside a SQL engine you have stored procedures, you have PLPG SQL, got lots of other pre-canned execution. If that makes you draw a complete blank, I'm sorry, if this helps, maybe it's good for you, so. Alright, so MCP model context protocol, we'll dig into this a little bit more. So this standardizes the interaction between your agents and the tools that they're calling in two ways. It lets them find the tools that they're gonna call, work out what they're for, and it lets them call them. They're two very important separate steps. So here's our car insurance agent. Inside that agent, there's an MCP client, standard Python. Alright, it's talking to an MCP server. MCP server advertises in this case 3 tools, get owner vehicles and so on. The tools have parameters. They have some documentation that goes with them to tell the humans and to tell the tool what they do. That tool probably calls off to a database, so it's just doing normal stuff, it's like running a regular database driver, talking to a regular relational maybe database. OK, it's nothing magic once you get underneath MCP server. So we'll zoom in, next level down. That agent embeds that MCP client, what is the MCP server actually doing? So inside the agent we have a tool catalog. It's now got 2 tools inside it. How did they get there? Well, the MCP server advertises these 2 tools. There's an, there's an MCP operation called tools slash list. MCP works on JSON dash RPC, um, so we, the first thing that the MCP client does is say list me the tools. And you'll get vehicle buy Vin, get vehicle owners, um, get owner vehicles. We know how to call them or add them into our catalog. And the agent understands then what it's, how it's useful to call that tool, so you don't have to tell the agent, call the tool, which is named X with parameters Y, right, it's already worked all this out. That's the job of the tool catalog which got discovered by using MCP. So that hard coding that I didn't like in the example gets replaced by this. We don't hard code tools anymore, can if you like, but you don't have to. Now we can have many MCP servers plugged into one of our agents, one of our MCP clients. So we can continue to build our catalog by querying multiple MCP servers along the way, and this is how we build, for example, our internal data stores, and then we talk to the DMV ones and the other ones that we're trying to build up over time. OK, so we're building this catalog, this is all internal to how MCP works. So let's implement an MCP client and server, those purple boxes that we had before, a little bit more Python. On the left, standard MCP SDK, nothing to do with me. All we're doing is we're creating a client session. On the right-hand side, MCP server, this is the thing that's maybe off talking to your database and so on. Important part on the client side, we initialize a session and we call that list tools function. That's it, right? On the right-hand side, there's actually no Python code there at all, that's interesting, it's just a couple of decorators, the little pink things, and all they say is this function is the function I want you to call when you want to list the tools, for example, or call the tools, right? So the takeaway for you here I hope is that it's very, very low barrier to entry here. Making an MCP server not too difficult, just put a couple of uh annotations on the code that you probably already have. Alright. Um, so, as we go on, we're gonna talk about how to choose the right tools for the right job. So I've kind of made this um taxonomy here. We've got the X axis is read only data retrieval versus data mutation. And the Y axis is whether it's a general purpose tool, whether it can do pretty much anything, or a specialized tool, like we've been talking about so far. So on the top left there we have general general purpose retrieval tools, read only tools. So this is maybe your business analyst inside the any insurance company, right? Which needs the full power of a SQL query, a read-only SQL query, and it wants to call this through uh an Egentic AI workflow. So this might be the MCP server, tools that we ship inside AWS for uh a bunch of our databases like Aurora DSQL. OK. And on the right-hand side of that you've got the data mutation version of that. This is very powerful, this is basically general purpose SQL execution or something like that, right, they can create, they can delete, and they can read general purpose data in your data stores. You're probably not giving that to your customers, you're probably giving that to your internal developers. Maybe you're accessing that through Qiro or something like that, clawed code. Now down the bottom, the specialized read only calls, that's like what we've been making in our examples so far, the get vehicle by VIN and that kind of thing, we're getting, so it's read only, but it's specialized. I can't go and learn about tractors, I can only learn about cars or something, right? Um, and then the data mutating versions of that, the specialized ones. So in our example this is maybe when we get to the end and we wanna save the progress on the quote or we want to calculate some premium or something like that. So this is your highest business value is packed into probably those that bottom row there, the specialized ones. They're the tools that you're building for your customers to call. They're for frequently executed tasks, so it's worthwhile you investing the effort in there. And they'll give the higher performance as well. So now that we have this taxonomy, we can see how that maps to the personas of the users that you're maybe trying to um have inside your business or as your customers. So the top left you have the data analyst or the engineer, they're doing those um read only queries against the the general purpose data. If I took the words MCP out of your mind right now, this picture probably looks pretty familiar anyway, right, this is what business intelligence tools and so on doing. MCP just giving you a different lens to think about it. Top right, that's your developers and your DevOps people, and down the bottom it's your end users and consumers depending if they're doing reads and writes. So, so hopefully that sets the framework here so that we can think about this some more. I'm gonna hand over to Siva now and he's gonna tell us about the data behind the tools. Thank you Tim. Uh, so good afternoon everyone. So where does the data behind the tool comes from? And, um, you're probably wondering, I already have a lot of challenges on my data side, um, you know I have data quality issues, um, you know, maybe I need to figure out how to get data lineage. Um, and then how do I have an existing system that's working, uh, that maybe you have some APIs, how do I convert that into, you know, um, an MCP tool, hand this off to the, you know, uh, the agent, um, and, uh, what happens to security, uh, along end to end, uh, this is what customers are asking us. The next section I'm gonna, I'm gonna go through, um, how to. Address these challenges also, you know, is there a, as an architect I always think about is there a reference architecture that I can give customers so I kept the best for the last, uh, so I think we're gonna end the presentation with the reference architecture and a call to action obviously. So let's dive in, um, so where does the on a technical level, where does the data come from? You know, the data comes from your operational databases. This could be, you know, MySQL, or Postress, Oracle, or SQL Server that you're running, or it could be NoSQL databases such as, you know, Dynamo DB, DocumentDB, Mongo DB, and other things. It could also come from your data warehouses, you know, data warehouses such as Redshift or Snowflake or others, right? Um, and, or you probably have data in a, uh, in an open table format like Iceberg, and you probably have this data in a parquet format on top of S3, right, uh, Sagemaker, um, you know, Lake House, right, uh, or other lake houses, or maybe you have a streaming data coming into your organization that, that is in, uh, in your streaming data source such as Kinesis or Kafka, right? Uh, now, let's look at this challenge from a business level, right? Um, like, you know, this is any Company.com insurance, right? Uh, this is an example of company that, uh, Tim, Tim alluded to that Terry is going to buy an insurance from this company. Uh, when you look at this, in any company, there's multiple departments, right? Uh, for example, the claims history, um, comes from the claims department. Uh, they, they are data producers of this, of this data set. And then maybe the legal department has information about rules and regulations, um, and then potentially this company sources data from the DMV in terms of accident records and other things, right, if they have pay as you go insurance, maybe that data has to be sourced from some telemetric data sets, right? Uh, so, um, so essentially you have data producers and data consumers, uh, in the setup, um, and then, well. Any, any time you have multiple departments, producers and consumers, you're dealing with data silos and, uh, you know, data quality issues come in, you know, if for example the sales and marketing department have information about the customer, you know, all the fields have to kind of be merged and then the customer 360 should reflect that, right? And then the data lineage issues, right? How do we track data lineage. Uh, there's this personally identifiable information such as Social Security number. If there is a collision, there's a claim, then there's some medical information involved, you know, you cannot pass this on to every user, right? Um, and then access control, who has access to this, um, you know, what are their entitlements. So, um, the all of these challenges get magnified. In the world of agents, right, because agents are applications, they're going much faster than the humans. They're accessing, they, they know how to navigate various paths, uh, and then, uh, you, you wanna make sure, uh, you're sending them along the right paths. There's proper governance along the way and proper security checks along the way and filters along the way so they don't hand off the incorrect data set, you know, to the end user or an internal user, right? Um, so how do we address that as an architect, uh, probably you're familiar with the data marketplace architecture, right? Uh, people call this data mesh and other things, right? I'm gonna use the term data marketplace architecture. Uh, what this architecture says is let's imagine our company as data producers and data consumers, uh, for example, uh, you know, the claims department, um, you know, the is, is a data producer of, of claims, right? Um, and then the underwriting department, which consumes this data set and evaluates the customer risk is actually a data consumer, right? Uh, then typically data producers are responsible for data quality because they produce the data. It's probably much easier for them to actually put some data quality rules, ensure that this data that they produce actually conforms to, you know, the standards that they expect the data to be at, um, and then. You know, data consumers like this underwriting department is the data consumer typically they discover the data set and then they subscribe to the data set, uh, so we can use this primitives, uh, to help us address some of these issues as well. So let's, let's, uh, see there's probably 3 steps that you want to do. First, you know, build data, data products marketplace, right? Now when I say this to customers, they often ask us like, well, that's gonna take a couple of years, maybe more to do that, right? So here's where I think we need to be a little more pragmatic, right? Um, you can boil the ocean and build all these data products for your for your company. Probably you should be keeping, keep on doing that. But as you identify some important use cases for Agent EKI, you should probably prioritize those use cases and build data products relevant to them, identify the data producers and consumers, and get started there to be fairly pragmatic. And then uh now how do we feed this data products to agents? Um, let's build some data APIs on top of them, right? If you have a claims history, you know, given the customer information, you could, it could, it could actually get all the claims information pertaining to that customer. Why should we do that? You know, the, the fact that you created a data product maybe that's a table um in your lake house, um, the agent is able to actually select everything from the table, but actually you wanna be a little more careful, right? You want to create a specialized path so if you're an external customer coming through your, you know, agent and asking for that, you want to actually restrict that use to just, you know, the data that belongs to them like, you know, building an API on top of that makes a lot of sense. The other thing is like nobody can do this for you. You know you own your data, you know you know what this data set is you know what your business, you know, procedures and practices are, and this belongs to us as, you know, data producers and consumers to actually build this APIs, right? And then again, one of the other challenges with uh with agents and our industry right now is that everything is changing by the day or by the week, right? How do I, as an architect, I always think about how do I. You know, build things in a completely changing world, so I build primitives. Those, those can be extremely helpful, uh, not today, tomorrow, and the day after, right? Um, and then obviously I can leverage, you know, the, the agenttic IDs to build these APIs. They're making my life a lot easier, but you do have to verify them. You do have to build them yourself, right? After we do that, then the next thing is to hand this off, you know, how do we hand this off, um, you know, to the agent. You know, we will expose this. We can expose this APIs as MCP tools, you know, uh, to run through an example, uh, here's where, um, things like Agent Core Gateway, uh, are gonna come in fairly handy, you know, as long as you, you have an API. You can actually define that API using OpenAPI Standard. You can register that with the Agent Core. It'll actually expose that API as an MCP server and the tool on your behalf, so you don't necessarily have to do that. Or if you want to actually create an API, you could, you could simply write a lambda function in Python or your favorite language, and then expose that, you know, to gent core gateway and therefore magically these tools are available for your agents. Um, now when you do that, what happens? Your agent has the brain to pick the right tool for the job, you know, when, when the user says I want, uh, a specific functionality, it is able to reason out that request. It, it's look, it looks in the tool repository that it has and identifies the right tool for the job, or you can guide this by prompt templates as well, so. OK, so we maybe addressed some of the, you know, data silos issues. Let's say, let's take these principles and apply this to how do you move a traditional application to an agent TKI application, right? Uh, before we get there. This, I wanna address we should understand who the audience is, who the user is of this, of this agentic experience that we're creating. Again, I'm gonna go back into the any company insurance. Um, it's easier to explain there. So Tim talked about Terry wanting to buy car insurance or maybe filing a claim. That's one class of users, right? People outside our company they wanna get something done, you know, talking to us, right? That's the external customer. Now when you look within the company, there's may be data producers and data consumers. In this case I'm gonna use Nikki, um, as Nikki is a claims investigator, right? Nikki wants to actually look at all the claims, um, that happened in a specific state. Nikki lives in, uh, is responsible for the state of Nevada, right? Um, and then when, when, when she investigates the claim, she wants to get all the, all the claims, you know, beyond a certain value and take a look at this just to see if there's any fraud, right? That's an internal, internal user, um, and then again you could have data engineers like in this case Jane is a data engineer and wants to build a new data product, you know, I talked about this, uh, telematics, right? Maybe she wants to subscribe a telematics data set from a telematics provider and then host the data set as a data product. That the underwriting department and others can consume, right? So in general you can generalize this to there are potentially 3 access paths, you know, for our agent take applications now with that as the backdrop, you know, Terry, maybe, you know this any company doesn't have an agent tech experience. Um, today what Terry does is maybe goes through an insurance website, you know, fills the appropriate form, and say, say I wanna buy insurance or I don't want to file a claim, right? Um, now, what do we, how do we transform that into an agenting experience, right? You potentially already have the API when you fill out the form and Terry submits it. Chances are your web server is already calling an API, and that's implemented as an in the inner application server, and by simply converting that into an MCP tool, all of a sudden you can hook that tool up to your, you know, agentic application. And great, you know, voila, you have this, you have the, you have the agenttic experience that you can present and start building on this one, right? So in general what we see play out today, uh, you know, it's not a complete revolution, right? The moving an application from, um, non-aggentic experience to an agenttic experience, uh, is an evolution for most, most customers, right? And this thinking of the thinking that of, you know, the existing functionality as an API. Exposing that as an MCP tool and handing off this agent is a powerful mechanism to actually plug that in. Now, um, we talked about RAG, you know, two years ago RAG was the answer no matter what the question was, right? RAG hasn't gone anywhere else, right? Like, you know, the beauty of RAG is that, you know, if I say. You know, I live in the state of Nevada and I want insurance. It's able to say maybe there's some rules and regulations associated with that. If you have a vector database with all this, you know, documents that explains the rules of buying insurance in the state of Nevada, maybe that's already chunked and then stored in a vector data store. And what this does is, you know, given a string, I want to buy car insurance, it retrieves the related information and then hands it off to the hands it off to the to the to the agent, you know, the reasoning loop, right? Um, and then now you can simply expose that as an API too. Input is a string and output is a, you know, set of documents that comes in. So you could potentially expose them as semantic APIs as well, right? Uh, now let's look at the internal user experience. How does this play out? Uh, so in the case of, uh, internal user, uh, again, I'm, I'm gonna take, you know, Nikki, who's a, um, data consumer, right? Uh, and then in this case, Jane is the data producer. Typically today, Nikki goes to sort of a BI tool of some sort and says, hey, give me all the list of the, uh, you know, um. The list of, uh, the list of, um, you know, claims that are greater than a certain value, maybe, maybe sort this and then let's pick the top 5 or 10 and then she starts investigating that, right? And that's the current experience now Jane probably, you know, to move this data set, uh, Jane probably wants to figure out how to subscribe to this, you know, telematics provider since it's IOT data land this data in Kinesis, for example, or Kafka. And build some kind of a streaming workflow to build data pipelines to land this in an open table format on S3, right? That's the current path now. AWS is very busy building a lot of MCP servers for all the services that we have, right? You know, when I looked at it, you know, various MCP servers and various levels of maturity, you know, as Tim pointed out, you can build these things yourself if you, if you want to enhance them as well, um, and then imagine all your query engines and data processing engines as front ended by the MCP servers and tools, right? as soon as you register the tool with your agent. Now all of a sudden, you know, Nikki can rather than specifying writing the SQL query could simply ask the agent, hey, give me a list of high value claims in the state of Nevada, right? as simple as that. The underlying tool knows how to convert that will go through there completely into an experience in a little bit. Similarly, Jane could come up with a, you know, uh, a long term memory, a prompt template, you know, in the data engineering team. Maybe there's a data engineering agent that they have trained. Right, with, with templates that say, hey, here's what you do when you actually have IOT data. Maybe you want to use, you know, maybe the organizational standard is Kafka. Maybe use MSK, which is a managed service for Kafka. I mean they, they're familiar with Spark. Maybe they want to use Spark streaming. If all of this is templated again, uh, Jane could simply say, hey, by the way, can you please create me. Uh, an MSK, you know, a topic, um, you know, a Kafka topic, and get the data from the third party provider and then stage this in S3, right? It knows what the tools are. It's able to actually go through the reasoning loop, understand, get the confirmation from, uh, from Jane, and deploy this whole thing, right? That magic happens by simply exposing your existing functionality as tools to your agent tech applications. And um with that uh now we're getting into OK looks pretty good right now how does really governance work under the covers uh it's kind of pretty important right? um and then let's actually play this thing out like um how does this really work when Nikki as a claims investigator, um you know she has a role and use the rules of a claim investigator right? say say show me all the high value claims in the state of Nevada what happens? Uh, the moment Nikki connects to the agent, the agent kind of inherits that characteristic, assumes the agent that I'm an investigator, um, and then what happens is that it is smart enough to say, by the way, uh, Nikki is asking something about the collision claims, etc. I need to look up in the business catalog, you know, what this means, right, because, you know, the, so it has that assuming you, you have the tool registered. Uh, what it does is it does, uh, let me look, let me find the data product, right, and then it actually, you know, calls, um, kind of a sage maker business catalog and says, hey, you have any data products pertaining to claims, you know, there's metadata in this catalog that says, you know, what those data products are, you know, claims and policies, etc. uh, then as soon as, as soon as the, the first call goes out, you know, it, it, it gets a list of, uh, claims and policies now. Uh, since the tool, uh, the agent here inherits the investigative role, um, it has restrictions. It cannot look at all the data products. It only has access to, uh, what a claims investigator department has access to. It's basically it only, only lists all the data products, you know, uh, from, from their catalog that's relevant to that the user that's relevant to this use case, uh, as well as that Nikki has access to, right? Um, again, the similar thing happens. The next thing it does is it takes the data product and says it's backed by a couple of tables, the claims tables and the policy tables, right? Then it looks up in the technical catalog and says, let me get the structure of these tables, right? And then once it gets that again, the context gets, keeps on building as Tim showed you, you know, we're in that loop, you know, building the context, you know, for this, for this agenttic application, and the next thing it does is like, well, I have the table and I need to retrieve data from this. It says let me call. I have a list of tools, you know, AWS has created the data processing, uh, MCP server, uh, that has Athena as an engine, so it says, hey, let me actually use Athena, which is like query as a query as a service, like, and then I don't have to provision a cluster, etc. I can simply press on the query. It'll execute that and get me the results, right? It fires off a session into Athena, simply packages, you know, the select statement it has built, you know, from the table metadata, you know, remember, Nikki doesn't say anything about the query. The the agent builds it, you know, for her and then passes it on to Athena. Now Athena also, you know, before retrieving this, it, it, it talks to lake formation. And then it applies some filters, you know, column level filters and row level filters. Um, let's say if the data set in the claims has some personally identifiable information, um, and then it has for all the states, it actually looks at the query and says, Well, you're asking for all this data set, every data set. Now, I'm not going to allow you to do that. Lake Formation says, I'm going to put a column filter. I'm gonna rewrite your query and put a ware class because you're only, you're only responsible for Nevada. I'm gonna only show you records from Nevada. And then I'm gonna actually mask and remove the Social Security number column, but you shouldn't have access to that. That happens automatically, isn't that pretty amazing, right? now remember the user didn't ask anything about it. It's simply looking at lake formation, looking at the permission, and this thing magically happens under the covers, you know. So and then, you know, the regular thing happens after that, you know, Athena executes the query again in the context of an investigator and then ensures that only, you know. Uh, she has access to or the agent has access to only the portion of the S3, you know, key space and to get this retrieve this data, and then it, it sends back to the user, and then it, you know, the agent creates a report and then sends it back to, sends it back to Nikki. This is how governance plays out for an internal, internal consumer. Now let's actually look at some of the critical pieces that the agent tech data consumer priorities are. Like if you're if you're catering an experience to a data consumer, you should be worried about data quality right in the in the loop if there is a data quality score associated with that, Nikki, the, the agent on behalf of Nikki, might also put a constraint saying expose this data set only if the data quality rule on a column is greater than 95%, right? Um, you know, I didn't show that in the agency loop, but that could be part of your prompt prompt template as well if you look, if you want, you know, data quality, you can bake that into, into your agency calls, right? And data discountability is gonna be pretty important, right? Your agents have to look up your business catalog, obtain potentially data products, uh, and then associate them with, you know, the, the column metadata in your technical catalog and retrieve the results. Uh, we're gonna actually, I'm gonna talk a little more detail into how this trusted identity propagation works. Let's hold off for a second, but you also have to be, you know, when Nikki logs in, uh, I simply said like, hey, by the way, this, uh, agent is going to actually, you know, take on that role, right? How does that really happen? I'll actually dig into that towards the end. Let's hold on to that and fine-grained access control, you know, users who only have access to the data set that they, that they need to look at. We demonstrated this by, you know, Athena rewriting the query, you know, looking up lake formation, right? And then the other piece is performance is super important, right? Uh, for example, if this report takes forever to come back, you know, Nikki has to go get a coffee or something like that, right? Uh, now, you know, maybe what we can do is actually use a materialized view. We've launched Blue materialized view, uh, last week, I think. And then you can, you can create, Nikki can potentially create a materialized to you that says, hey, create a materialized to you for me that only has records from the state of Nevada, so you don't have to actually, you know, you know, Athena doesn't have to actually scan all of that. That should be baked in, in, in, uh, in a, in a consumer priorities. So far so good. OK, let's keep going. Uh, now let's look at the data producer experience, um, like what is the experience for Jane. Jane, as I mentioned, is a data engineer. In this example, uh, Jane's task is gonna create a, you know, is to be creating a data product, and then Jane is going to actually get this data set from a telematics provider, um, and then host this in that environment, right? Now again here, um, as Jane starts talking to the agent. The agent first thing does is inherits the user's role, which is a data engineer. Then it what's it, what it is, then this tool has all the various, you know, um, tools required for like, like the MSK. MSK has a MCP server and, uh, and, and, and tools to provision a cluster that's already, you know, exposed to this agent, right? And then it simply says like, oh, OK, I'm going to create an MSK cluster. And then creates the cluster and then actually fires up. Then it realizes it needs to also create a topic. This is a placeholder for all the streaming data that's coming in, though, right? Interestingly, the MCP server there for MSK does not have data plane access yet, you know. So it's going to go through. It's going to realize, hey, I don't see that tool there. Maybe I should go through the front end and actually create. You know, create that so the agents are smart enough to actually come up with code templates that actually could go on with this with some prompts are automatically realizing, hey, I don't know how to log on and create a topic, right? Um, and then that happens and then the thing unfolds. The next step is it needs to provision us, uh, uh, a kind of a glue job, um, since, you know, the template says you wanna use Spark, maybe it uses Spark streaming to, to actually retrieve the data set and then actually. The next thing it does is it needs to create a table where it needs to put that data set, right? Remember the data quality that we talked about? This is the place the producer has to think about data quality. Now as part of doing that. Um, Jane also, um, kind of writes data quality definition language is something that Amazon invented and we open source this as well. A lot of our actually tools automatically use this, um, so what, what you can describe the data quality definition language, what do you want to do? Like for example, if you have GPS coordinates, if you have speeds, you can simply say GPS coordinates have to be within this range or speed has to be within this range, right? You know, all that is kind of baked in. So as, as, as a part of creating this table in the Glue catalog, those quality rules can also be specified. When you specify that Glue actually runs a batch job and actually evaluates this rule, and at the column level says, here's the data quality for your specific column. It's at 95% or 90% based on, you know, how many records, you know, are out of, out of, you know, are out of, out of sync with the rules specified, though, right? That's automatically built in. It is Jane's responsibility to go ahead and do that, right? And then obviously you know the table gets created and remember in this case Jane simply said like hey don't create all this stuff. I'm in my beta environment. Give me the infrastructure as code. I want to validate this code and then deploy this right using my classic CICD pipelines. It's probably a good practice, you know, unless you don't, don't, don't have this running around in your production cluster, you know, use a beta cluster if you want, or at least create infrastructure in the code you inspect this and before you deploy this, right, because otherwise you wouldn't be affecting a running system, um, and, um. And again, you know, as the agenttic loop spins, um, it also realizes it needs to publish, you know, Jane is trying to publish the data, you know, this, this entire data set in the business catalog. It also, you know, publishes and enter in the business catalog for consumers to consume this, and, uh, again, it takes a data engineer has the ability to publish to the business catalog that is a role that you already created, um, and finally. Uh, it also associates, you know, it also says, let me, let me actually create lake formation, um, you know, um, fine grained access control rules. Only these consumers can access, you know, this data set that's actually baked in lake formation, right? So all of this magic happens behind the covers, um, so this is, uh, uh, a data producer experience in the world of agents, right? Uh, so let's look at the priorities of the data producer. In the world of agents, right? You know, we talked about. You know, the creating infrastructure of the code and verifying it, the accuracy is still your job, right? The agent is going to come up with some code. You want to ensure that you know you have test scripts and others to ensure that this is actually doing what you intended to do, that is the responsibility of the producer, and it's, it's our responsibility doesn't go away and data quality rules. We, we saw how data quality definition rules are associated with this data set. That's a data data producer's responsibility. And also, you know, putting the information in data catalog, right? You know what do these columns mean, what are the various bounds and how should they be used, all of that should be part of your, you know, part of your business catalog and then audit, auditing, right, um, you know, sometimes you figure out why this happened though, right? So, you know, making sure that the audit. Uh, is enabled, so all of these accesses to lake formation are already logged. It can be, it can show up as a cloud trail that can be, that's a, that's a responsibility of the data, data producer. So far so good. You know, I skipped one detail as I was telling you though, right? I said, by the way, um, the agent inherits the role of. The user coming in though, right? Like how does that really happen, right? Um, you, you're gonna need a couple of primitives, uh, AWS IAM as well as the Open ID Connect and OA 2 rather than delving into the details of this, I'm gonna actually show you how this really works under the covers, right? Um, let's say you have a user. Uh, the user first logs in, you know, to their portal, through the agent portal. Uh, what happens at that point is the user gets authenticated with the federated identity provider. Now for enterprise identity it could be OCTA. Basically the user says, Hey, here's my user ID. Here's my login. Here is your MFA token, you know, that's in my device. You plug that in. What that does is it sends out this thing called a JWT token, right? Uh, I didn't quite understand what this is. I was starting to understand what this is in detail. Here's where, you know, as data practitioners, we have to understand the primitives of what these things mean, right? Uh, the easiest way to imagine this is your license, driver's license, right? Uh, we're talking about insurance, easy to use driver's license. Your driver's license, you have your name, you have your picture, you have your signature and the age details, etc. That's your ID, right? And you give that ID to someone else, they know they look at this thing and they look at your picture and say, yeah, I think, I think that's the person and it also has this entitlements, right, which is what class of car. I live in the state of Washington, um, default is a driver's C class license. It doesn't say that, but that's the default, right? And it also has a restriction for me called restriction B, which means in the US it means I have to wear glasses or contact lenses to drive my car. That's the entitlements, right? So what the JWT token is, it's the access token and the permissions token, right? Um, so that's what this thing is like, you know, you send the token to the agent, um, that token has to be passed along, right? There are a couple of use cases here, um, one where the identity provider doesn't change in your organization, that is the NICKI's use case, right? The NICI, you know, runs a query and then that gets passed on to the tool, etc. and each stage it checks with the identity provider if need be, right? Uh, and then, and then passes faithfully the token along, the identity token and the access token along to the APIs, and you know, beyond the APIs it's the same path, right? Those, uh, credentials may be exchanged with the temporary AWS credentials, right, you know, single sign-on, and then it talks to the database or it could be using a user ID password, you know, and then you get the details from your, you know, security, you know, store, secure store, right? Um, then there's another path sometimes, right? Remember, uh, in the case of, uh. Um, Jane, Jane is actually getting a data set from a telematics provider, right? The telematics provider might use a different identity provider. In that case, what happens is that you need to exchange your credentials, right? You know, like you're showing your driver's license. And if you're gonna go to another country, they don't take US driver's license. You gotta go to AAA and say, can you give me an international an international driver's license? What they take is they look at your ID and then actually they give you new credentials, right? They print this paper and say this is your international driver's license. Then if you go to another country, you show them and they let you rent the car. That's really what happens. The bedrock identity allows you to actually do that. Um, you know, on your behalf, it takes, it's like acts as a token vending machine. It takes that and gives you other tokens that you can use in your, in your, in your other systems, right? Uh, so that thing is what happens as you can just see here. Then you send those tokens faithfully to your tools and then that gets propagated to the API call that you're making and at that point, you know, those credentials could get transformed into IAM, you know, credentials that can be used to access your data store, right? Um, or in this case, you know, whatever credentials that's required to access this third party telematics telematics use case, so this thing is super important as data practitioners and AI practitioners we really have to delve into how do we. You know, tell our agent to act on our behalf, and it's super important, otherwise it can go do kind of unpredictable things, right? Uh, not putting it all together, this is the final set of slides. Are you ready? 5 minutes, right? All right, this is the most exciting part for me, right? So I'm gonna try to put all this in a single reference architecture that you can probably take a look at, right? Here we go, let's go. This is the external user. This is Terry coming in and asking a question. I want to buy car insurance, right? Like the, so you, you're running an, you know, you're building an agent app. Maybe you're using strands, maybe you're using Lang chain, maybe you're using other frameworks. Now we think that Amazon Bedrock agent code is the best place to run this because it gives you a runtime which is pretty secure, containerized, right, uh, so that just owned by, you know, that execution. Uh, Tim talked about memory, you know, short term memory and long term memory. Those constructs are there, right? We talked about the gateway, right? Like if you have APIs, you can quickly expose them as tools if by using gateway. We talked about identity, you know, this thing where you show your, uh, you know, ID to AAA and get that other ID, all of that. Magic can be actually simplified by your agent core identity. That's why we feel this is, I haven't discussed a few other things there. Agent Corres is the best place to run this right now. This doesn't have to be for your external customers or your internal teams are going to as we move forward, are gonna have agents right that they potentially build. Uh, or this may be like IDs, like, you know, Quiro or other IDs, agent IDs that you can prompt and use that for internal deployments as well, as long as you can hook up all your tools to this, right? This is the way this looks, right? Remember Amazon Bedrock LLMs. Amazon Bedrock has a choice of LLM from all the way from NA, uh, to anthropic to, you know, every, every year we add a few more LLMs, right, because you can use the right LLM for your reasoning tasks, right? Um, so we give you a choice of LLMs, right? And then remember bedrock knowledge bases, right? You can basically Bedrock can actually take your string and then actually look up on your behalf the knowledge base and retrieve the other pieces of information like we have a host of functionality there in terms of Postress. If you have a, you know, Postres engine, if you're doing a sequel, if you're familiar with that, if you want to use the engine, you can use the vector capability in Postress through PG vector. OpenSearch is fantastic to combine vector searches along with the, you know, classic searches, right? So we have a host of tools there. If you're doing graph rug, you, you can use Neptune. Neptune as your vector store where you can do both the graph traversals, um, as well as, as well as, you know, vector searches together, right? Um, we also have S3 vectors that we added, right? Mylan yesterday talked about that, right? Um, and then, uh, you can use that, and then you can also expose them as MCP tools, right, like we talked about this API where you give the string and it, it fetches a related piece of information. If you have an MCP server. Uh, you can build an MCP for this server for that and expose that as a tool as well, right now this is sort of the front end now. You know, Tim talked about packaging this as APIs. You know, remember the special purpose path that Tim talked about? You're creating canned queries or APIs. Now those things, if you expose them as MCP tools, both your customer facing agents and your internal agents can actually talk to those back end engines, right? Now we have a lot of capability starting from, you know, Aurora to Dynamo DB to Elastic Cache to Redshift to Athena, and the list keeps on growing, right, based on what tool you want, what engine runs it the best, you can expose your API or general purpose query via this tool. Now what happens to your back end, right there we go, you know, there's the data processing engine, right? You know, this is where Jane goes and provisions, uh, you know, Kafka topic and then actually. Yeah, and, and, and runs a glue job that whole thing can be orchestrated, you know, by tools, right? With that, this is the, you know, picture I've been very intensely, you know, drawing hopefully this makes sense, right? So this is, I feel like, uh, you know, a reference architecture at least for me until I keep changing it, we'll keep on tweaking this. Any feedback on this welcome, right? On the left side is the agent on the right side is the data. The MCP is this big USP, you know. Uh, USBC thing that connects the data and AI together and this is where, this is how the magic happens, folks, and with that, I wanna leave you with a call for action. Uh, to me, I think it's the special purpose paths are something that you have to build. I would build, start building data APIs, as Tim pointed out. These are very easy to build. It gives you a secure path, you know, but you can also build data products and give you a general purpose path that's a little more risky, right? Like, uh, so you can use both, but I would urge you to build APIs, expose them as MCP tools via the MCP protocol, as we talked about data governance. End to end and end to end security are a top priority. You should be thinking about them day one, not after the fact when your agent vends out the data that it should not vend out, right? And finally, um, you know, we are working very hard to simplify this experience for you so you can focus on your business and leave the other muck to us. So with that, I wanna end the session. Thank you very much. I hope this was meaningful. Your feedback is very welcome and.