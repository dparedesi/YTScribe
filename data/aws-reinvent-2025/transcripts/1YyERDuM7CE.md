---
video_id: 1YyERDuM7CE
video_url: https://www.youtube.com/watch?v=1YyERDuM7CE
is_generated: False
is_translatable: True
---

Hello. Hi, good afternoon. Welcome to STG 208. Max maximize the value of cold data with Amazon S3 glacier storage classes. My name is Gayla. I'm very excited to be here and I'm really excited to see you all in the afternoon after lunch. So give yourself a pat on the back for making it to your session. Just by a show of hands, how many folks are already using Glacier? OK, we got a few of y'all out there. Good to know, good to know. We built this session for people who are new to Glacier, for some folks who maybe occasionally use Glacier are familiar, and we even have some stuff for those archiving storage pros out there, so we got a good session for you. You can think of this session in 3 parts. In the first part, for those new to Glacier, I'm gonna briefly cover why your code data is important. I'm gonna go over the S3 glacier storage classes. I'm gonna talk to you about getting your data into Glacier. Getting your data out of Glacier, then I'm gonna hand it over to Nish who's gonna cover some new S3 storage archive related features, and then we'll have a quick demo for the pros. Alright, let's get started. S3 has hundreds of trillions of objects currently. And one estimate out there says that 70 to 80% of cold storage of data everywhere is cold. What we mean by cold means it's data that's rarely accessed. It's sometimes stored for months, years, and even decades. And that's hard to ignore because that amount of data is growing every single day. But what's interesting is that the value of that data is increasing. Across industries we're seeing something very remarkable happen. Cold data isn't just stored and forgotten anymore. It's now become a catalyst for innovation. It's emerging as a huge differentiating factor for a lot of businesses, and our customers are unlocking all different types of ways to use what was once what used to be considered dormant data and turning it into actionable intelligence. So think bigger than storage. Let's say you have a whole bunch of historical handwritten records and you wanna build, use it to train machine learning to recognize handwritten data for those archives. You might wanna derive new insights for archived analytics. Maybe you have a bunch of financial or stock data that you wanna use to build new application. The opportunities are endless and our customers are every day identifying new opportunities for this archived content and it's about turning every bite of cold data. Into an opportunity and that's why Leticia and I are excited to go over S3 Glacier with you all. We're both on the service team and this is hugely important to us. So let's dive a little bit deep into. Why, what the glacier storage classes are. Now if you've been around S3 you've probably seen a slide like this before, but for the new folks, let's talk about these storage classes. Imagine a continuum where you're balancing two key factors. It's your access speed and your cost efficiency. On your left we have our frequently frequently accessed data, so it's ready for you in your applications in milliseconds, but it's at a premium. And as we move right, we're entering into progressively cooler territory. This is where your access becomes less frequent, but your storage costs are going to drop significantly. So let's break this down a little bit more. With S3 standard, you're getting that millisecond data you need for your active, I'm sorry, that millisecond access for your active data and moving through S3 standard and S31 zone and frequent access, you're already gonna start seeing some cost savings. Then as your data continues to cool, you can move it through Glacier and this is where the real magic for coal data happens. You have glacier instant retrieval. It's for that data that you need in milliseconds that's still archived. You have your glacier flexible re retrieval for data that you need within minutes, and then you have glacier deep archive, which is our lowest starch, lowest cost storage in the cloud. And here's the brilliant part. As your data naturally cools and as access patterns decrease, you can progressively move through these tiers and continuously optimize your storage costs without sacrificing the ability to retrieve when needed. And you may ask though, which storage class should you choose? Think of this as finding the right combination of cost and retrieval speed. For glacier instant retrieval, this is your go to for data that's cold but you need very quickly. So some examples of that could be medical records that are rarely accessed, but when you need them, you need them right away. Or if any of you are in the broadcast media space, this could be archival clips that have a tight deadline that you have to pull in for a project. This tier is also popular with those who have compliance requirements that require instant access as well. Next, you have glacier flexible retrieval, and with glacier flexible retrieval, you could get something really cool which is free bulk retrievals. This is for large scale data analytics, um, where your timing is flexible, uh, backup archives where you have some time and you can plan your retrievals, and also historical records that may need occasional bulk processing. And our last tier, Glacier Deep Archive, where I mentioned before, is our lowest cost storage option. This is for your long-term retention at the lowest possible cost. It's storage for data you hope you never need, but you must legally keep. And this typically you're measuring storage archive in years and decades. But how do you get data into these storage classes? For this, we have S3 life cycle policies. Life cycle policies allow you to transition your data. Into infrequent access or archive storage classes over time as your data cools off and it can automatically delete that data at the end of its life. This allows you to automatically control costs based on known access patterns. For example, let's say you have an object that you created on day 0 that's accessed very frequently in the 1st 90 days. And then after that 90 days it's rarely accessed. You can create a life cycle policy that automatically transitions your object to Glacier instant retrieval after that 90 days. And if your application from then needs to access that object, it's available to it in milliseconds and there's no problem. It'll work great. But let's say after 180 days access is extremely rare and we want to further optimize our storage costs. Our life cycle policies can handle that too by transitioning the object to glacier deep archive. And then after that, let's say you have a compliance requirement, maybe you're in the financial industry and you have a requirement of keeping data around for 7 to 10 years, you can build a life cycle policy that will automatically delete that data at the end of that time. And we provide you with a number of filters that you can apply to your life cycle policy which determine which objects that it will affect. You can apply them to an entire bucket if you need to, but you can also apply them to specific prefixes prefixes or objects that match certain object tags. You can filter by object size, which is really great for archiving because you typically want. You typically want to avoid sending a bunch of small objects into archive. And you can also filter by the number of versions an object has. So if you want to avoid creating a bunch of unneeded versions, you can create a policy that maybe creates one or two and will delete everything else. But let's say you don't know your access patterns or they're very unpredictable. We have a storage class for that as well. And that's S3 intelligent hearing. Sometimes access patterns truly are just unpredictable or unknown and S3 intelligent hearing. Excuse me, S3 intelligenteering is a storage class designed for customers who want to optimize their storage costs automatically when data access patterns change, and it does that without any performance impact, operational overhead, or life cycle fees or retrieval fees. Intelligent tiering is a cloud object storage class that delivers those automatic savings by moving your data between the tiers. There's 5 tiers, 3 of them are synchronous, and you get those automatically. They're available as soon as you sign up, and you have to opt into the 2 asynchronous tiers. After you opt in, Intelligent excuse me, Intelligenteering will choose the correct tier for you. So far we've talked about why your code data is important. We've talked to you about how to choose your storage class, how to get your data into the storage tier, and how to get, now we're gonna talk about how to get your data out. You've optimized all your storage costs. So what happens when your data is suddenly hot again? Let's talk about restoration options. The first question you might ask is why are our customers are storing data? Great question. Let's look at some new and exciting emerging patterns. So you're sitting on a gold mine of archived media files. We're seeing our customers breathe new life into their historical content, transforming decades of old footage into fresh, compelling content for today's audiences. And when it comes to backup and compliance, it's not just about checking regulatory boxes anymore. Organizations are finding strategic value in their historical records and using them to track long term patterns and inform their future decisions. But here's where it's getting really exciting. Cold data is rocket fuel for machine learning models. So think about it. You have decades of historical data training models to spark spot patterns we never could have seen before. Your archive data isn't just history, it's your competitive advantage. For example, let's say you are a company that's working on self-driving car technology. Perhaps you need to train your model and you want to restore all your content related to cars taking left turns. You can do that with Glacier. We're seeing a brilliant trend where companies are generating rich metadata from their archive content, making vast data lakes searchable and actionable in ways that they weren't before. So now let's dive deeper into how customers are accessing their stored data in Glacier. First, with glacier instant retrieval, it's pretty easy. It's the same get request that you would use for standard and intelligent tiering and S3 and frequent access. You get the millisecond access and the trade-off is higher retrieval and API charges. Customers often mentioned with Glacier instant retrieval, it's amazing for them because now they're able to save on their storage costs without having to make any changes to their applications. However, if you have retrievals that can withstand some more times from minutes to hours to days. We can pull your data out of glacier flexible retrieval, but that's a a bit different. You can lower your cost of storage while also reducing the cost to retrieve even large amounts of data with glacier flexible retrieval and glacier deep archive. But with these storage classes, they require a request before data can be accessed and once restored, you can get, you can call a get request on the same object key to get the object. So for retrievals from these glacier S3 storage classes, glacier flexible retrieval, and glacier deep archive, you generally have 3 steps involved. First you have to initiate a request. Second, you have to check that the restore has completed. And finally you access your data and I'm gonna take you through each step. When you restore large volumes of archived data consisting of 100s and 1,000s and millions and even billions of objects, one factor to account for is the time it will take to submit all the requests to Glacier. Glacier supports a request rate at 1000 transactions per second. This TPS limit automatically applies to all standard and bulk retrievals requests from S3 Glacier, flexible retrieval, and Glacier Deep archive. So let's take an example. I did do math. At 1000 TPS you can submit 10 million restore requests in under 3 hours and using standard restore from glacier flexible retrieval, for example, you can complete all restores in about 6 hours. So that includes the 3 hours to submit the restore requests. And then 3 to 5 hours to complete the restores. And to ensure you get the highest restore performance, you can also rely on batch operations, which I'll talk to talk about in a bit, and that'll help you maximize your TPS and get automatic retries. Now if you remember our 2nd step, that was monitoring the request status for Glacier restores. For these restores, S3 will create an event for restore initiation and completion. And you can publish these events to Amazon Event Bridge. And you can configure that to fan out to an SNS topic or an SQSQ. You can even have it trigger a lambda function if you need it. You can also configure restores completion events within EventBridge to send events to an SNS topic. Which your application can then follow up and subscribe to. This will allow your application to automatically proceed to the next step, such as a get or a copy as soon as the object is restored. And now we get to our final step. Accessing your restored data. So once an object is restored from Glacier, your application can access it like you would any other object. It's an S3 standard now. You can, you can perform a get or access it any other way that you would access an object in a synchronous tier. However, a tip, so remember this, the restored data is a temporary copy of the object. To move it out of glacier, you can either do a copy and place over the same key or you can copy it to a new bucket. If you're copying over the same key though, just keep an eye out for adding an extra version in case you don't want to keep an additional Glacier version. Now I did briefly mention batch operations. The glacier kind of works like a freight freight train. And if you know the batch, we can do some optimizations on our side to make things run smoother. If you're getting less than 1000 transactions per second, that means the request will take longer to submit. I've seen examples of customers request around 25 TPS and essentially increasing their total restore time by 40X, which is suboptimal, but the good news is that you don't have to optimize your software or multi-threading. You can use back operations and that can dramatically improve your restore experience. Instead of fine tuning your software as I mentioned, you can use batch operations to automatically maximize your restore requests per second. It can also do automatic retries to handle any error failures. And with the completion report that will signal any issues with your job. And then you create a manifest, a list of keys you want to restore, submit to and submit to batch operations. Each manifest will be associated to a single job, and additional jobs from there will then split up your TPS. All right. So We've covered again the importance of your code data and how it can be an innovation for your applications. I talked to you about your storage classes and how to pick the best one. Talked to you about getting your data into Glacier, and I talked to you about getting your data out of Glacier. Now Latisha's gonna come up and talk to you about some new archive related features. Thank you. Dela. Whoa, I didn't fall. Thank you, Gayla. Now you have seen S3 storage offering and the pathways of getting the data in and out of S3 Glacier. Let me build on that foundation and introduce two new capabilities that we have recently added, each specifically designed for archive workloads. We actually, we're going to talk about 3 capabilities. So there is one that will be like coming in Matt Garman's keynote tomorrow, so stay tuned. So the first thing that we are going to talk about is checking the integrity of your archived data at rest. And the second thing that we are going to talk about is using S3 metadata for quick discovery of your archive content. For each of these capabilities, I'll address three questions. First, what problems are we solving here? How does it work and what does it mean for you? Let's start with the first feature, new compute checksum operation in Amazon S3. Why this capability is so vital. Because customers across domains such as media and entertainment, life sciences, crime and justice, and preservation institutions perform periodic data integrity checks to make sure that the data is intact. This could be the master copy of an iconic movie or it can be uh a historical historical artifact like the Constitution of the United States or this is something that is required by the compliance team or uh a proof that the evidence has not been tampered with. In all these cases verifying archived data is an industry standard. Our customers asked us to provide tools in S3 to help them do this, and that's what exactly we have built. Before I dive deeper into this capability, one thing that I want to highlight is that everything that we're talking about checksum validation here is completely optional. S3 performs billions of checksum operations every second to make sure every day, like every bite in transit and at rest is intact. But we also know that our customers are already doing that, and we can provide a better way of performing these checks. So that was the intention behind this feature. But first, let's cover the basics. What is a checksum that we are computing here? A checksum is a digital fingerprint of an object. In this example, we have 3 objects. We are using the checksum algorithm CRC 32, and we have this unique alpha numeric value for each object. Even a single bit flip will result in a different checksum value. Customers store original checksum value on their media asset management systems or in a checksum repository as a source of truth, and later it can be 6 months, 2 days, a few seconds after the upload, or after 10 years, they come back and calculate a fresh checksum of the object that is in S3 and compare that against the checksum that is stored in their systems. It's a way for them to prove that, to prove that the data remains intact. S3 already provides a range of capabilities during upload. So when you upload an object to S3, you can specify the checksum algorithm you want to use, as well as you can provide a pre-calculated checksum value, which is optional. You have support for six checksum algorithms, CRC 32, 32C, CRC 64, MD5, SH1, and SHA 256. For every upload, S3 calculates CRS 64 on the client side and on the server side to provide you end to end data integrity. And only on a on the match, the request succeeds. This work works well for data in transit. Now let's see how customers perform data verification for already stored data in S3. Until now, verifying checksum for data at rest required two steps. First, you download the object. Which takes the, which, which will have a compute, sorry, a bandwidth cost or data transfer fee as well as some time. It will take some time. And then you spin up an EC2 instance or use your own infra to calculate the checksum locally. That means more compute costs, more time, and added complexity. For large archives, this process can be cost prohibitive, or it, it can be time consuming. We needed to eliminate the step of downloading the object and then calculating the checksum. We wanted to come up with an innovative way to do an in-place read of the data and calculate a fresh checksum. And that's what we have done with compute checksum operation. This is a new capability in S3 batch operations, and it provides you a new way to verify the content of your dataset stored in Glacier or any storage class. You can efficiently verify billions of objects and automatically generate a data integrity report. You can use that to prove that your data remains intact over time. This capability works with any object in S3 regardless of the storage class or the object size. Now whether you're verifying your data for compliance reasons, for digital preservation, or accuracy checks before feeding the data into the model, you can reduce the time, cost, and the effort associated in this in in that process. And because it is built in S3 batch operations, you get automatic retries on failures and detailed completion report in the end, which you can use as the data integrity report. Let me show you how it works. Creating a compute checksum job is simple. It has 3 components. First, you provide an object list, also known as manifest in S3 batch operations. You can create a curated list and then submit that as the CSV file, or you can use S3 batch operations automatic manifest generation service. You can also use inventory report and just feed that in as the manifest. Batch operation supports that too. Second, you choose the Checksum algorithm. We already talked about the algorithms that are supported in S3, so the story is consistent. Everything that is supported on, on upload is supported with compute checksum operation. The algorithm that you pick really depends on your business or or compliance use case. So for example, if you want something that is secure and more compliant to regulatory needs, you can go with secure hash algorithm like 1 or 256. And if you need performance and don't care much about the compliance requirements and all, you can go ahead with CRC CRC 64, 32, or 32C. And the third thing that you specify is checksum type. Checksum type, you can provide a full object or a composite checksum. So if you're in media supply chain or you're dealing with third-party providers, where you're, you're, you're providing your content and you need to provide a full object checksum so that everybody is talking the same language, the chain of custody is maintained. In those scenarios you can use the checksum type as full object, whereas you don't have any such needs. It's all internal. Your team knows what we are talking about. You're dealing with large objects you want to perform parallel checksum operation in that case, continue or go ahead with composite checksum type. And of course you'll need to provide the IM permissions to S3 batch operations so that it can read the bytes and write the completion report. That's it. These are the 3 inputs that you need and the permissions, and S3 batchops handles the rest. It will read the object. It will compute the checksum and provide you a nice integrity report in the end. And this is how the completion report would look like. It will have fields like bucket, key, version ID, error code, and result message. Once the job is complete, you can use this to validate with the checksum that are stored on your media asset management system or checksum repository. You can also use, let's say JSON parser and extract the checksum and the values and convert this into a nice table, the result message fields that are available and use that for validation. Additionally, you can use lambda function to automate this job, let's say every 6 months or 1 year if that is the need. The best part is that you do not incur any fee for restore or retrieval. Your data that is in intelligent tiering, it won't warm up, and if it's in Glacier, you don't have to restore those objects from Glacier. You pay a single fee of 0.004 per GB or $4 per TB to process the data, and that is consistent across all storage classes. Now let's move on to the next capability that that would be helpful. And that is S3 metadata. Now, this capability. This is something that we launched during Greenvent last year. And we have made some improvements to S3 metadata. S3 metadata automatically extracts metadata from your objects and makes it available to you to generate valuable insights using simple SQL queries or natural language. We believe that this will fundamentally change how customers manage and extract value out of their cold or archived data. Let, let me explain the problem we are solving here. Here is an example. We hear this from our customers very often that hey we have a we have a bucket with millions of objects or files in that that they have archived. They have tagged those objects, so in this case we have tagged all the objects either as Project Odin, Loki, or Thor, and then a few of them are untagged. Some are in glacier and few are in S3 standard. And the customer's ops team come and ask like, hey, we want to move all the data for Project Odin that is in glacier to standard. How much data are we talking about? Today, there are multiple options to answer that question. You can, you can patternate the API requests like list and get object tags and scan the storage and calculate the, the storage across storage classes. Or you can use the S3 inventory report, which comes like daily refreshes daily or every 48 hours, and then you can use that to answer this question. But both options at scale either delay in time or are not easy to perform. But we saw an opportunity to make it faster. And what if we could answer this question in seconds by writing a single query? And that's exactly what we try to do with S3 metadata. It is a fully managed service that automatically captures all your object metadata, tags, object storage class, object size, everything that you get from the head object and store that into a queriable Apache iceberg table. No APIs to call, no inventory reports to wait for, just instant SQL queries. We launched S3 metadata at Reinvent last year. With starting with journal table, the journal table captures every change to your bucket in near real time. Ps, deletes, metadata updates, tag changes, all of them are captured in general table. It is your complete change of logs of what's happening in your bucket right now. In July this year, we added a new capability, the Live Inventory Table. It shows the current state of every object in your bucket. It backfills all your existing data when you enable it, and it refreshes every hour. Both tables are read-only and fully managed by AWS, and you can think of them as your authoritative system of record for everything in your bucket. Now, here is where it gets really powerful. You can query these metadata tables in two ways. First, through standard SQL queries using Athena, Redshift, SageMaker Unified Studio, or any analytics tool that supports Iceberg tables. And second, through natural language using Amazon Q, Quiro, or any agent that you're using which supports MCP server. Here are a few examples that we tried. First, to learn about storage usage, the question that we, we ask ourselves, how much data is in Glacier for each project. Now, you can simply write a SQL query and it will give you the snapshot of that. Second, you can also identify all the objects that were untagged so that you can classify them properly. And lastly, you can also use it for auditing the journal table to track what data was deleted, who deleted it, when delete, like when was it deleted, and from where. And just recently, we added MCP server support for S3 tables, which means you can connect your AI assistants like cloud, custom agents, Qiro directly to your metadata tables and interact with them in plain English. That is really powerful because it democratizes the access or insight generation from your archive data. Whether like you don't have to be a data engineer, data scientist, like anyone from your team, like finance, ops. Uh, compliance team, they can just write their questions in plain English, and MCP server will convert those questions or the agent can convert those questions to queries and extract the insight out of those out of the archive data and provide them as something that is like a summary or easy to consume. And now, let's move on to the next section, which is a demo. So I'll quickly try to figure out the logistics here. So give me a second. So for a moment, let's assume that we are a space tech startup. And we are building an autonomous space vehicle, and we are using the images, images available from NASA and videos to build a space simulation model, so that we can train how the vehicle how to navigate through obstacles. These images are valuable for training the data, but we need to verify the integrity of these objects before putting them or feeding them into those expensive GPU. Compute thing that we need. So for that, we are going to focus on 3 things today. We are going to use the inventory table to find all the objects that are related to Project Odin, which is our code name for our like autonomous space vehicle project. Then we'll calculate the checksum of these objects, and then we will use Quiro to compare the completion report with the original checksums that are added as tags. So OK. So here what we are doing. Once again, this is still not working. OK. So we'll go to this bucket reinvent storage 208 demo, where we have hundreds of images and videos that we have downloaded from NASA website. And they're stored across different storage classes. Few of them are in glacier instant retrieval, few of them are in standard. We'll open one of them. And just look at the object properties. We'll scroll down. We have tags that is attached to it. So 256 checksum algorithm that was computed at the time when this object was created, the checksum timestamp, project name as Odin, and base baseline checksum or the original checksum hex code. So all these are added to the object and we have added that to most of the objects and kept them, few of them untagged intentionally. Now our plan is to use inventory, inventory table, live inventory table to query all the objects that are in. Uh, that are associated to Project Odin and create a list of that that we can feed into batch operations. So what we will do, we'll go to query, and we'll use live inventory table, use SageMaker Unified Studio. To query our inventory table. And We need to make sure that We, we tweak the query in a way that that can be used with batch operations, so Yeah, so we need bucket and key. And then we need to add a wee clause where We are talking, we, we are adding the work clause where object tags have project name as Odin. And then we'll, we'll run this query. So we have 25 files that are associated to uh Project Odin. Now, let's download the CSV. And let's see the values once before feeding into S3 batch operations. So we need to remove the, the first row, so that uh it matches the format that is supported in batch operation intake process, and then save this file. We'll save this as Odin manifest V2. Now, let's go back to our general purpose bucket, S3 bucket that we had. And we have created one more bucket there for S3 batch operations jobs. So here we have two buckets, one for storing the manifest and another one for uh providing us the destination location to get the results from batch operations. So we'll upload the Odin manifest V2 to this folder. Then we go back And look at, OK, I think we're good here. So this is our, this will be our uh result uh location where we'll be storing the result from uh compute checksum job operation. Now, we'll create a new job. And we will use the existing manifest that we just created, which is stored in our batch manifest folder, V2. Oops, CSV. We will select compute checksum operation, that is the new one, checksum type as full object, and short of 56, I'll go. This is important. We need to acknowledge that this report can be accessed by the bucket holder because the completion report will have checksum value, which is of the plain text data. So that's important to acknowledge. Then we plotted the destination location. And now we will. We will just add the permissions for for IM rule. We, we have created S3 batch checksum rule for for this, and we'll submit the job. We will run this job. And we will wait for it to complete, which can take a few seconds. Meanwhile, just good to know info, we have added automatic manifest generation capability which was available through batch operations API to console as well, so you can use that for generating manifest if you're trying it by yourself. Should be done any moment. OK, so the job is complete. Let's go back to our folder that we created for batch results. F2E9 We'll go to the results. And let's open it to see how does the output ah looks like in the completion report. So this is how you get as the result message. This column is a result message. And it has The checksum Algorithm short of 56, the checksum type full object, the checksum value both in base 64 and hex code. OK. After this, we will use keyro to compare this with the values that are stored on uh with the objects tags as tags. So we are using the keto CLI. And we want to specify that validate Project Odin, check sums using the output of S3 batch operations job, and then provide the The, the link or S3UI for, for that CSV file that we just got from Batch Ops. It has created a Python script. And all the 25 objects that were related to Project Odin, it validated that that they match the, the checksums that are stored as tags. And with that, we conclude the demo, and I want to summarize our session with a few key takeaways that you can keep in mind. So the first is with the advancement in AI. Archived data can be the differentiating factor. And S3 provides you multiple storage options curated for your specific business needs in terms of access pattern or storage time or the cost. And finally, we are adding new capabilities in S3 such as compute checksum operation or S3 metadata that can help you easily manage archive data at the same time extract more value out of them. And with that, we would like to conclude this session. Thank you so much. Yeah, please complete the session. Thank you.