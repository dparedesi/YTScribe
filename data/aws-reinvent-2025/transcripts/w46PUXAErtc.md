---
video_id: w46PUXAErtc
video_url: https://www.youtube.com/watch?v=w46PUXAErtc
is_generated: False
is_translatable: True
summary: "Marc Brooker, AWS Distinguished Engineer, presents a deep dive into database transactions, focusing on the trade-offs between isolation, consistency, and performance in relational databases and the new Amazon Aurora DSQL. He begins by defining isolation levels as a complex trade-off: strong isolation simplifies application logic by preventing concurrency anomalies (like dirty reads or lost updates) but can limit throughput, while weak isolation improves performance at the risk of data \"weirdness.\" He uses a simple \"dog adoption\" database schema to illustrate these anomalies, such as multiple clients adopting the same dog or seeing uncommitted data.\n\nBrooker reviews classic isolation implementation techniques, starting with **Two-Phase Locking (2PL)**, which blocks concurrent access but risks deadlocks, and **Optimistic Concurrency Control (OCC)**, which validates transactions at commit time. He champions **Multi-Version Concurrency Control (MVCC)** as a breakthrough that allows readers and writers to operate concurrently without blocking, by maintaining a history of row versions. He specifically highlights **Snapshot Isolation** (often called \"Repeatable Read\" in PostgreSQL) as a \"sweet spot\" where readers see a consistent point-in-time view, avoiding most anomalies except for specific write skews that **Serializability** would catch. He counters common wisdom by showing that Serializability isn't always slower than weaker isolation levels; performance is highly non-linear and workload-dependent.\n\nThe core of the talk contrasts PostgreSQL's implementation with **Aurora DSQL**. PostgreSQL uses a centralized coordination approach that checks for conflicts early but requires cross-network trips to a primary node for read-write transactions. In contrast, Aurora DSQL utilizes a distributed, optimistic approach combining **MVCC and OCC**. In DSQL, transactions perform reads and writes locally within their Availability Zone (AZ) and only coordinate across AZs (or regions) at commit time. This design prevents \"lunch-time locks\" (where a stalled client blocks others indefinitely) and significantly lowers latency. DSQL leverages high-quality physical clocks to ensure **Strong Snapshot Isolation**, meaning it provides linearizable consistency where a transaction always sees the effects of all previously committed transactions, avoiding the \"time travel\" anomalies of eventual consistency.\n\nBrooker concludes by challenging the traditional CAP theorem interpretation, arguing that \"Eventual Consistency\" is often too difficult for developers to reason about and that strong consistency is achievable with high availability. He posits that the true trade-off is between consistency and **latency**, not availability, and demonstrates how Aurora DSQLâ€™s architecture optimizes this by minimizing cross-datacenter round trips."\nkeywords: Database Transactions, Isolation Levels, Aurora DSQL, MVCC, Optimistic Concurrency Control, Serializability, Snapshot Isolation, Distributed Systems, Strong Consistency, CAP Theorem\n---

OK. Hey, good morning. It's 9 a.m. on a Monday morning. Thank you all for joining me. Hope you're excited about the beginning of Reinvent this year. Good energy to get the week going. Um, cool. Uh, so today I'm gonna be talking about transactions. Uh, transactions are some of my favorite things. So, uh, try and, uh, make this as interesting and exciting as possible. Um, so I'm Mark Brooker. I've been at AWS for, uh, 17 years. Um. And uh I've worked on a bunch of systems, including EC2, EBS, Lambda, Aurora, DSQL, uh, and now I work on the Agentic AI side, uh, spending most of my time with the agent core team. I've worked on services of all sizes, from a couple of transactions a day to, um, let's say 9 digits of transactions per second. And uh, Throughout all of those designs, I found relational databases and transactions to be an extremely useful building block. We're gonna start off today playing a little bit of an isolation game. Um, I'm gonna ask you some questions and see how you feel about your ideal database, the database that you would like to use if you were an application developer, rather than a database enthusiast. Uh, we're gonna look at some of the classic algorithms that are used to implement database isolation, isolation between transactions. We're going to talk about serializability and snapshot isolation and the relationship between them, and maybe touch on some really interesting data and some surprising results. And then finally, I'm gonna end talking about consistency, because I'm only really a pretend database person. I'm actually a distributed systems guy, and so I always want to talk about consistency. So isolation levels are a trade-off. Isolation levels, you might have come across people on Twitter who are super opinionated that there is one best isolation level. Or if you aren't seriallizable, you can't say acid, or real transactions are only seriallizable. Or if you're using anything stronger than read uncommitted, then you're being a fool. Those people are largely wrong. Isolation levels are a fairly complex trade-off between the amount of complexity we put on application programmers to build correct applications and the amount of complexity we put on our application programmers to achieve a certain level of performance and scalability. And so we're going to play a little bit of a game. And this game is based around uh my adoption agency for dogs. And uh this adoption agency for dogs has the simplest possible database schema. Each dog is given a unique integer ID and I have a flag column of whether that dog is adopted or not. Obviously, in the real world, that schema would be more complicated. Maybe I would have the dog's name, or uh or or breed, or whatever in there. But this is enough to highlight the points that I want to make about isolation. The rules of this game are as follows. On each slide, there are 2. Uh, clients, 2 client threads, a blue thread and an orange thread. And they are going to run sequel statements in order where each one completes. We'll relax that a little bit later. And I'm gonna ask you a question about whether you think a certain thing should happen. Whether a certain behavior should be observable to generally the orange client. The database is other than these two clients quiescent, there's nothing else going on in the background, so nobody else can sneak in and change things. It's just these two transactions. Um, and, uh, you will see that some of the, uh, some of the statements are wrapped in explicit transactions with a beginning and end, and some are just lines. The just lines are auto committed, and so that is a single statement that commits at the end of the statement. So here's my, uh, here's my first question. Blue is going to update all dogs to set the adopted flag to one. Then begin a new transaction. And then update adopted to 0 for dog ID 1. Now orange is going to run and say select star from dogs where adopted equals 0. Put your hand up if you think orange should see any results in your ideal database, the database that you want to build applications against. OK. Well, here's the trade-off you've taken. If you said yes, then there are some adopted dogs that look unadopted to orange. Orange sees some dogs that are, uh, that have been adopted, that are in that appear to be unadopted, and that is going to lead to disappointed kids and families. However, if you said no, what you mean is that some transactions should block, abort, or otherwise have to coordinate with transactions. This is the beginning of our performance versus correctness trade-off. Let's make an example that's slightly more interesting. Here I have two interleaved transactions. I begin both of them. They both read all the dogs. Uh, select staff from dogs, obviously not a best practice, but a fairly common one. But they read the entire dog's table, maybe to present it to a customer in a UI. Blue goes to update the line for dog ID equals one, orange goes to update the line for dog ID equals one. Blue commits. Put up your hand if you think orange should be allowed to commit. Well, this one was uncontroversial. Yeah, a couple of, a couple of believers here, a couple of believers in uh in the weaker end of isolation. I'm gonna push these plugs in, I don't know what's going on with. The slides blinking on and off, um. And so if you said yes, you shouldn't think this transaction should commit, then multiple families can adopt the same dog, right? This update could say, update the dogs, set the, you know, me as the adoptee, and that is going to lead to disappointment. Right? A dog is a single thing. We're not going to be able to split it in half and share it. We're just going to have to disappoint one of these customers who we said, your transaction committed. If you said no, then you're against sad children. But again, you are choosing that there are some transactions that have to abort or have to block or can't run to completion. Let's tweak this example just a little bit. Here again, same, same beginnings. I'm reading all of the dogs in blue, I'm reading all of the dogs in orange. And then I am going to commit, I'm gonna update ID1 in blue, and update ID 2 in orange. Blue is going to commit. Put up your hand if you think orange should be allowed to commit. OK, that's, uh, I would say. Small majority of folks. Um, if you said orange is allowed to commit. Then it becomes difficult to enforce cross dog business logic in our database. For example, if we had a piece of business logic that said, no customer is allowed to adopt more than one dog, I wouldn't be able to implement that purely with this schema, obviously because the schema is not wide enough, but even if I added a customer field to this particular table, Because two concurrent transactions could do, could adopt a dog for the same customer at the same time, and go on and commit. If I say no, then I do have this power of implementing arbitrary business logic, because I can look at those select results. I can implement whatever business logic I want in my application. And then Um, I can, uh, have the transaction not commit if that logic is going to be violated. But, if I do think that this transaction should not commit, right, if I think that orange should have been aborted or blocked there, then I'm only allowing one client at a time into this flow, right? They're both going to select all of the rows, and only one will ever be allowed to commit, no matter what they end up writing, or at least only one writer will be allowed to commit. And so I've reduced the effect of concurrency of the system to one. There is no allowed concurrency going on here. OK, last. Version of this game. Here, we're not talking about isolation anymore. We've gone off the rules a little bit, and we're talking about consistency instead. And we're not talking about isolation because isolation is a question of what goes on when transactions are concurrent. There is no concurrency here. Blue is allowed to run to completion as an auto commit transaction, and then orange starts. So I say, update dogs. Set adopted equals 1 where ID equals 1. And then I run another statement after that one has committed. And remember, nothing else is happening in the database. It says select start from dogs where ID equals 1 and adopted equals 0. Put up your hand if you think orange should be allowed to return any results. Not many people Well, if I was building this application against a database replication tree with read replicas, there would be cases where Orange would return results because Orange would read against a read replica that had not heard the news of Blue's transaction. Now there are many, many ways to prevent that. But the naive building out of replication trees doesn't prevent it and does lead to this kind of, what I'm going to boldly call weirdness. So that brings us to our most important question for the day. Is this just pedantry for nerds? And, uh, well, I mean, it's, it's, you know, I don't, I don't mind. um, you know, I'm happy to talk about such things, but I don't think so. I think this is actually the core, a core idea that we need to get into the heads of more application programmers. The shape of the trade-offs that you're taking when you take on different database isolation levels. Stronger isolation means that it is easier for an application programmer to achieve correctness, because they don't have to think as much about uh the weird concurrency anomalies that can happen. However, it is harder for the application programmer to build applications that perform well under large amounts of traffic. And so you get lower concurrency and lower throughput, very dependent and very nonlinearly, as we will get to later in the talk, on very dependent on the traffic patterns and the exact kinds of queries and transactions we're running. Um Weak isolation is known to cause production bugs. We've seen that at AWS. I've seen that in my career, and there is fairly well documented evidence of these kinds of issues. Strong isolation is also well known to cause performance issues in production. And so this isn't some kind of out there claim that I'm making. Stronger isolation also makes it harder to achieve high availability. But maybe not in the way that most people are expecting. This non-linearity is sometimes quite surprising. In the architecture of Aurora DSQL, for example, It would be more expensive for us to implement Read uncommitted than it would be to implement Snapshot. Um, or at least the architecture avoids the read uncommitted anomalies by doing the cheapest possible thing. A lot of people will say, OK, well what I can do here is I'm going to use my database in a low isolation mode. I'm gonna get the best concurrency, I'm gonna get the best throughput, maybe I'm gonna get the best latency, and I'm gonna try and shim correctness at the application layer. Maybe by sprinkling in explicit locks, maybe by sprinkling in for updates and so on. And it turns out, at least in my experience, that for a lot of workloads, trying to do that correctness shimming at the application layer leads to a lower performance application and a worse outcome with more complexity and worse performance. And the reason for that is that if you show the database the workload it is trying to do, it can make the best decisions about how to execute that workload. If you show it only a part of the workload, or some kind of obscure stream that is controlled by locks outside its view, it makes it much harder to make good decisions. And then I said, harder, it's harder to achieve high availability. With strong isolation. And it's harder to achieve availability with strong isolation, but again, very nonlinearly, and so strict serialiability is strictly no harder to achieve than repeatable read isolation. Uh, from an availability perspective, and we'll talk about that a little bit more later. So let's dive in as we go down this path of understanding the shape of the trade-offs and talk about various classic techniques for implementing isolation inside database systems. And here, mostly I'm gonna talk about the implementation of serialiability, but as we get towards the end of this section, I'll also talk a little bit about snapshot isolation or what Postgress calls repeatable read. For those who don't know, Postres is kind of lying to you. It's repeatable read level is actually snapshots. It's not NC-repeatable read. That is the right thing. The Postcres developers made the right choice. NC-repeatable read is a little bit silly. Um, OK, so. Let's talk about making sure that um this orange transaction doesn't see the uncommitted or to be rolled back results of the blue transaction. And here I'm going to use a variant of the classic database isolation algorithm called two-phase locking. This is an algorithm that has been around for, let's say, about 50 years. It was very well documented and had been well analyzed by the early 1980s. And so this is hardly breaking news. So what, what happens here, essentially, the idea is, as you read or write every row in the database, you take locks, you never release locks until you get to your commit or rollback stage, and then you release all of the locks at the end of time. So here, on my update statement, I take a right lock on this row for ID equals one. And that right lock is an exclusive lock. It says, hey, nobody else is allowed to read or write this row while I hold this lock. Then orange comes along. It tries to get a reed lock on that row. Which is a non-exclusive lock, but it's not allowed to, because that row is, uh, one, that row that it is trying to read, or one of the rows that it is trying to read is locked for right, and so it blocks. And now we're going to relax the rules of our game a little bit and say, OK, now orange is blocked, we're gonna get, let blue have its turn again. Blue rolls back. Any changes it made to the database are now removed because of atomicity. And orange can now run. And it doesn't see blue's effects, because blue's effects at this point have been rolled back. And so we have avoided Orange seeing this weird result of reading uncommitted data by blocking its runtime until its run is complete. Let's choose another example. Here I'm gonna do my two where I'm reading all of the rows and updating the two of the same row. So I read ID 1, I read ID 1, read lock, read lock, read locks are non-exclusive, so we're cool. Those can run. Now Blue tries. And this is a little bit sort of outside the 1970s 2PL, but blue tries to upgrade its read lock into a right lock. So, actually, guys, I'm, I'm planning on writing this row, so I'm going to go from locking it for read to locking it for right. It can't do that because it is not the only lock holder on this row, and so it blocks. Again, we've relaxed the rules of our game to allow orange to run. Orange runs, and now orange detects that blue is waiting on orange, and that orange is waiting on blue. And this is a deadlock. No progress can be made here. And so now we have to decide what to do. And there are a whole bunch of classic approaches to deciding what to do. You can stop orange, you can stop, uh, um. Blue, you can panic your entire database process. There are all kinds of fun things you can do here, but you cannot allow either of these to continue. And the reason fundamentally you can't allow either of these to continue is that there is no serialization of these two transactions that works. Because orange has not observed Blue's rights, and Blue has not observed orange's rights, and so there is no ordering of these two transactions that you can put in. Now if these were single-shot transactions, kind of Dynamo DB style transactions, we'd be cool, because I could read all of those and I can order them in whatever order, and I can restart them. But SQL semantics are different, and we have to kind of assume that by the time that select has returned to the client, the client might have had arbitrary side effects, uh, based on those results. Again, that's a little bit controversial about whether you should be allowed to have arbitrary side effects based on uncommitted results. The SQL spec, I think, strictly says no, you shouldn't do that, um, but most people do. It's a super common thing to do. Even when those side effects are things like logging. Here's an alternative way to implement this. With optimistic concurrency control. This is the other classic database isolation algorithm, also dating back to at least the early 80s, where it was first described, maybe first described by Kan and Robinson in 1981. And so here, we do a different thing. We don't take any locks. Maybe we take some locks that commit time, and we can sort of talk about that later. But here Blue is allowed to run, begin. It does its update, but it doesn't actually change the database at update time. Instead, it takes out its notepad and it writes down, I am planning to update the database in this following way, and then it doesn't tell anyone about this. This is its secret notes. Then orange can run. It can read the row. The row has not been changed because Blue has only written down its changes in its secret notepad, and orange is allowed to read the old version of the row. Blue rolls back, which in this case is really cheap. It just throws out that page from its notepad and it's done. And these two transactions are both allowed to commit. But now we can talk about a little bit of the weirdness of OCC. So here, we've got a slightly different set of uh SQL. We're going to set adopted equals 0 for 2 rows. In blue. And now, orange is going to read. Um Read row number one. Now Blue is going to commit. OK. Success. Blue is allowed to commit at this point. Orange is now going to read row number 2. And now something very weird has happened. The first select statement from Orange has read the state of the database before Blue's transaction completed. And the 2nd has read the state of the database after Blue's transaction has succeeded. This is a violation of serializability. It's a violation of the basic behavior of transactions. And so if we are taking those select results and we're having side effects on them, we're telling them to a customer or we're writing them down or we're making a service call based on them, or we're putting them in another database, or many of those other things, now something very weird has happened. But it's OK because Orange is then going to say it's going to try and commit this transaction, and it's gonna be told by the database, no, you can't commit this transaction because of a read-write conflict in the set of rows that you saw. And no, you're not allowed to commit this transaction, and so, you shouldn't have done anything with those lies that I told you earlier. I'm sorry I told you those lies, but you shouldn't have done anything with them. Um, and so we're good. But this is not really how people use databases. And you can say that they shouldn't do this, but this is a very, very weird behavior that often leads to bugs. So how do we fix this behavior? Well, we fix this behavior with what I think is the coolest technique in all of databases, or maybe all of systems, multi-versioning. Instead of writing down a or overwriting data with new versions as new versions come in. I keep all of the data, marked with what version it corresponds to, and I simply append to the end of my versions. Now, obviously, I can't do this, because this is gonna grow forever, but just humor me for a second that this is what I'm going to pretend to do. And then every transaction that starts can come along and say, I started at 3. And so show me the uh the the best version of this row you have, the newest version of this row you have, that is from time 3 or earlier. And so what's cool about this is that we have fixed that anomaly that we had with OCC. We fixed that weird behavior. Um, And uh, but we've achieved two other things. One of them is, we've allowed writers to continue to add versions to the end, without blocking readers, so that's cool. And we've allowed readers to continue to read older versions without making writers block, and so that's cool too. And so by keeping multiple versions of a row, we have made this whole game substantially easier, substantially easier for the database implementer, and substantially easier for the application programmer to get to a level of performance. And so this is what this transaction looks like in the MVCC world. We began Blue does its updates. Those rights to the database, a new version, or maybe not, we can choose. Then orange selects, but orange selects from an older version. Or it picks a time to select from. But it doesn't see blues reads rights yet because they are not yet committed. Orange's second statement reads from the same version of the database as Orange's first statement, seeing a consistent copy of the world, and then we can go on to commit. So in all of these, the pattern here is that some coordination, whether it is locking or checking at commit time whether there are anomalies, some coordination is needed to achieve isolation. These two Clients, orange and blue, need to go somewhere, either statement by statement or once at commit time. Where everybody goes to the same place for the same row and says, is there, am I allowed to commit this transaction that reads or writes these rows. Coordination limits throughput and coordination increases latency. Coordination is probably the only real hard problem in distributed systems. Now, I know there are a lot of really hard practical problems building large scale systems, but coordination is the single hard theoretical problem. And to build scalable and reliable systems, we need to reduce coordination as far as we can. Unfortunately, coordination cannot be reduced to zero if we are going to achieve any level of reasonable transaction isolation or consistency. It can be moved around to statement by statement, to the end of transactions, to different locks and services and so on, or into the application, but it can never be entirely avoided. Let's zoom in one more step. And talk about snapshot isolation versus serializability. What is snapshot isolation? Again, this is the isolation level that post-gress calls repeatable read. Um, and it's my personal favorite isolation level. I'm sure you have a personal favorite isolation level. What does snapshot isolation give us? Under snapshot isolation, we never read uncommitted data. Um, and so we never see the part, you know, partially committed transactions. Reads are repeatable. If we read the same row or same predicate multiple times, we get the same results every time, unless, of course we've done some rights ourselves, in which case our rights need to be merged in. Reads all come from a single point in logical time, and you can think of the logical time as the kind of stream of transactions in the database. All of the reads come from a point in that stream corresponding to a single point in logical time. And conflicting rights are rejected. And so if one transaction writes row one and commits, and another concurrent transaction writes row one and tries to commit, it must be rejected. Snapshot isolation is a fairly strong isolation level, but it is not seriallizable for a reason that we will see in a minute. So what are its opinions? No, we don't see any results in orange, because those results are uncommitted and we never read uncommitted data. No, we do not allow this transaction to commit because there is a right conflict here. Both of these concurrent transactions are trying to write the same rows in the database. And orange is not allowed to commit at this point. What about this case? Again, similar to the previous one. 1 and 2 instead of 1 and 1. Under snapshot isolation, this is allowed to commit. There is no right right conflict here. The right sets of these two transactions is disjoint, and so this transaction is allowed to go ahead and commit as long as the underlying database has preserved the rest of the properties that we talked about, that repeatable read property, that single point in logical time property, and so on. Now, this is not allowed under serializable. These two transactions are not seriallizable. And so, at a serializable isolation level, the database has no choice but to reject one of these two transactions. And so in OCC what that would look like. I Orange would be forced to abort. Why is orange forced to abort? Well, we do a check where we have to check whether any of the reads that orange has done have been written to since orange started. Oh, yeah, but since orange started, so that's a read-write conflict. And we have to abort the transaction because there is a read-write conflict here. Orange red row number one. Blue rotro number one, blue committed first, and so orange has a read-write conflict with blue and must be, must be aborted. And this is what it looks like in open source Postgress. We get to that commit point of orange, and Orange's commit is rejected, saying error could not serialize access. This can actually happen in Postgress earlier in the transaction. If I was doing some more stuff with Orange. It doesn't necessarily have to happen at commit time. But here it happens at commit time. And so what's important about the difference between these two isolation levels and what we pay for with a slight relaxation in serialisability? The difference between these isolation levels, the core difference is that seriallizability coordination scales with reeds. While snapshot coordination only scales with rights. And so it doesn't matter to the decision whether to commit a snapshot transaction, what it has read during its lifetime. It does matter to the decision whether to commit a seriallizable transaction, what it has read in its lifetime. And so here, by choosing seriallizability, we are pushing onto the application programmer, the need to understand this, and the need to minimize how many reads their transactions do, and make those transactions as specific as possible in what they read. And so, what we've done. is we have pushed complexity into the application if we don't want to serialize anything. This transaction. This transaction that reads the entire table each time is absolutely disastrous for throughput under serialisability. But other than the larger amount of data movement, which may or may not be a problem, is not a problem for throughput under snapshot isolation. And so there you have pushed an implementation detail of the internals of the database, implementation detail of isolation into the heads of the application programmer. So let's run a little micro benchmark and see what these results look like. And I'm gonna preface this with saying that all micro benchmarks are bunk and should be taken with a major grain of salt. The only thing that matters is your workload and your workload patterns. Having said that, here is my micro benchmark, and I'm going to make bold claims based on what I see here. So here I have um. The throughput of two transactions, uh, contending. And these transactions, each one of them writes two rows and reads in rows out of a database of 10,000 rows. So, reading 100 rows is about reading 1% of the database. And so I'm running these transactions over and over. In fact, I'm sorry, I'm running 4 concurrently here. At 4 different 3 different isolation levels against open source posttress. And here we see that read committed and repeatable read. Um, Perform about the same. They're doing the same amount of work, they're moving the data around in approximately the same way, but serializable has this huge drop in throughput as conflicts start to happen. I have a theory for why this cliff is so cliffy at 32, uh, but I'm not sure it's right, so I'm not gonna share it. But these results are repeatable across multiple versions of Postgress, MySQL, and various other database engines. Here's a different view. Here, instead of doing reads and writes, every transaction just does N writes and has no reads. And so this is a pure primary key update workload. This is a workload that is only doing updates on primary keys. And what we can see here is Repeatable read and seriallizable. Now, so in this example, repeatable read was way faster than seriallizable. So snapshot was faster than serializable. In the pure updates example, snapshot is no faster than serializable. And again, this is a result that is repeatable across the snapshot levels or snapshot equivalent levels of a lot of database engines. And why is this happening? Well, the reason why this is happening is from the database's perspective, the transactions that need to abort are the same transactions. At the snapshot level, they are all rights, and so they are right right conflicts, and they need to be aborted. And then at the seriallizable level, there are read-write conflicts because the read set and the right set are the same because every update on a primary key is a read modify write. And so this is what I mean. When I say that the performance of different isolation levels is highly nonlinear and very workload dependent. You might say seriallizable is really expensive, but it turns out that seriallizable is no more expensive than repeatable read for this particular example. Or, you might say repeatable read is expensive, but it turns out that seriallizable, that repeatable read is no more expensive than read committed in this example. It is entirely dependent on workload patterns, and requires an understanding of workload patterns to predict performance. And that is a problem for database people trying to explain to application programmers how to write great high performance database code. This is the core problem of understanding the relationship between isolation, correctness, and performance. Now let's zoom in a little bit more and talk about how Postgress and Aurora DSQL implement snapshot isolation. And I'll preface this by saying that I'm not a super deep post-gress expert. I did read a bunch of the code for this talk, and so I think I'm getting this right, but, uh, you know, I'll be happy to be corrected if I'm wrong. So let's jump into the Postcres documentation and see what it says about its repeatable read isolation level, which again is actually a snapshot isolation level. Transaction cannot modify rows changed by other transactions after the transaction began. That is our right, right conflict property for snapshot isolation. A query in a repeatable read transaction sees a snapshot as of the start of the first statement in this transaction. That is our single point in logical time property of snapshot isolation. And then read-only transactions will never have conflicts. This is a very cool property, by the way, of snapshot isolation. Um, and this is true because there are never, uh, never write-write conflicts in a read-write transaction. A read-write transaction cannot conflict with another transaction. There are weird anomalies that can happen if you observe read-write transactions in various orders, um, which I'm not going to go into in this talk, but this really helps performance in snapshot isolated databases. And so, let's talk about the kind of internal rules here. I'm trying to create transaction number 4 at T equals 4. I should see the results of transaction 1. Transaction 1 is committed at the time that transaction 4 runs its first statement. Transaction 2 is running. It might have done a bunch of rights to the database. It might have tried to change a bunch of things, but it has not committed yet at my start point, and so I must not see its results. Similarly for transaction 3, even if it goes on to commit during the run time of my transaction, and then transaction 5 is from the future and I don't see transactions from the future, because seeing transactions from the future is just too weird. So how do we implement this set of rules in Aurora DSQL? So under the covers. Oversimplifying a little bit, but not too much. Each row in the database is backed by a linked list of of versions. So here I say select start from seats where ID 12, and 3. ID equals 1, I see a version for T equals 5. I ignore it because I'm reading at T equals 4. I go back one more step down the list. I see a version for T equals 3, that is the latest version that is older than uh my version, so that one I read and I'm done. For ID equals 2, I see a version of T equals 5, which I ignore, but then the list runs out, and so there is no version, older, and so I, uh, um. Uh, that, that road just doesn't exist. And then for T equals 2, I um, uh sorry for ID 3, the list only starts at T equals 2, and so I don't ignore anything, I just pick that old version and I read that one. DSQL also has a high water mark that we use for consistency. So if that high water mark is at 3, I have to block this transaction until I see all of the rights that come in, um, that are newer than 4, until, until I've seen every right that's come in for 4 or newer. And there is a low water mark that we use for garbage collection, which is the moral equivalent of post-res's vacuum. And we do that based on a very simple low water mark, um, for reasons that I will talk about in a little bit. Postgrace's approach is much, much more coordinated than this. Instead of being able to do this locally down at the storage level with the committed versions in the storage, in the engine, it actually has to pay attention to the whole world of currently running transactions. And so it establishes a kind of lowest transaction ID that it can get all the versions from the highest transaction ID that it sees, ignores all of the versions from, and then it has to sort of build this list of currently running transactions and decide what to ignore and what not to ignore. This is a huge amount of coordination which you wouldn't be able to get away with in the distributed system setting. But in the single machine setting where you have shared memory, it is not a major problem and really simplifies things and allows for a more efficient storage implementation. Uh, Yeah, so, uh, then let's, let's talk about the, the way this shows up in, in sequel semantics. And so here again, um, oh man, this is an old version of the slide. So, I'm selecting star from my dogs. I had a previous version of this, this talk that was about a kind of stadium booking system, which is much more boring. Um, but I'm selecting everything. Um, and then I'm trying to update, uh, the same row twice again. And so what happens in Postgras? Well, the orange one blocks. At that point, it's not allowed to update that row because there is a right lock on that row. Blue goes on to commit, and orange synchronously gets, most of the time, synchronously gets an error saying could not serialize access due to concurrent update. And so orange is then aborted and has to try its work again. What does DSQL do? Well, DSQL takes a much more optimistic approach, where it doesn't do any of the coordination necessary to determine whether a transaction is going to succeed until commit time. And so instead of getting that synchronous, no, this transaction is doomed, don't bother going on, you have to keep running that transaction till you get to the commit. And the commit will be rejected because we check for those right right conflicts, and here we are trying to write the same row twice. This is doing OCC validation at commit time. Um, So both approaches, uh, there's no need for readlocks because we just create new versions, which is pretty cool. We don't need to block writers on readers, and we don't need to block readers on writers, which is pretty cool. DSQL's approach, the advantage, big advantage of it is that clients can't control lock hold times. And so I'll talk about that in a second. And there is no need for long-lived locks or going to find the one copy of the lock in the system at all. It scales much better and has better operational properties. Postgrace's approach is nice in that you get earlier knowledge that your transaction is doomed and it's not going to commit. And so you do less wasted work on the client's side for transactions that will not, are, are known to not be able to commit. Postcredit approach also has no transaction run time limit, and so you can let transactions run for essentially an unbounded amount of time, um, typically until your server falls over. And that is a real advantage. Postgressive approach is also a little bit more on disk efficient and a little bit more. Um, uh, uh, yeah, so it saves a little bit of storage, um. So what is the downside of blocking and why, you know, why did we make this decision in DSQL? Well, because I've read the COE a ton of times. I've read the COE a ton of times where there are two transactions, they both do a bunch of reads, and so they have to, uh, uh, they have to block on each other. Um, actually, no, the reads are irrelevant here. And so, you know, blue updates a row. It takes a right lock. And so orange has to block, and then blue goes out to lunch, right? This could be a GC pause, uh, it could be a period of, of high packet loss in the network. It could be an operator literally going out to lunch, which is a postmortem I have read more than once. And so this is a major operational problem where you are allowing clients, the least trusted part of your system, applications that you don't have control over necessarily. To control the performance of other clients in the system. This is a real practical problem in all kinds of systems, even at relatively modest scale, and it's an even more big practical problem when you allow human operators, because human operators do things like going out to lunch. And so when we were designing DSQL, this was one of the things that we were trying to avoid, and one of the reasons that we chose the combination of multi-versioning and OCC, that means that transactions never block on other transactions, and clients can't block other clients no matter what happens. The other big advantage. Um, Of DSQL's approach is that it allows us to keep work in AZ until commit time. And so we'll get into the data behind this in a minute. Um, But in DSQL, when I do a select, most of the time, I go to a query processor and a storage replica in the same data center as my client application. And that is a major, um, that is a major latency savings. It's even bigger latency saving in the multi-region setting, where regions could be tens to hundreds of milliseconds apart. I go to a replica in my region. And then what's more, when I get down to right time, Um, When I get down to write time, I also go and do my rights to a query processor, it's a sort of secret notebook in AZ. Again, a big latency saving. And only when I get to the commit, do I have to go cross AZ in single-region mode or cross-region in multi-region mode. And this is a major saving of performance, major saving in latency, um. For any non-trivial size of transactions. Post Grace's picture. Isn't as good And so you can stay in AZ if you are doing read-only transactions at the cost in most architectures of eventual consistency and the weird anomalies that come with that. Um, but in read-write transactions, you typically have to schlep across the network. To the AZ where the primary is, to do your reads and writes. And depending on the storage implementation, those rights might have to go out over the network, uh, to a storage layer. Most don't though. But I do have to get from the client to the primary of the database for these transactions, which costs additional latency, especially in the multi-region active active mode. But even in reasonable sync multi AZ architectures. Let's go into one more topic in our last 10 minutes or so here. Strong snapshot isolation. The definition of snapshot isolation, the kind of academic definition of snapshot isolation. Allows this to happen. And it was mostly allows this to happen because the people who were writing those definitions were thinking about single system databases, where this is a silly thing to allow your database to happen. And so when I said post-cres repeatable read is snapshot isolation, I mean it is strong snapshot isolation that does not allow this kind of thing, reading against a primary. DSQL snapshot isolation level is also strong snapshot isolation. And the strong there means strongly consistent. Not in the acid sense, but in the distributed system lineariability sense. Weird things like this cannot happen. Why can't they happen in Postres when you're reading against the primary? Because Blue's transaction ID is marked as committed before orange starts and is therefore included in the set of data that orange reads. And therefore we get strong consistency. And this, I think, is considered so obvious to database people that it is not really even mentioned in the documentation all that much. Now again, when I get to read replicas, now I've got a problem because this read, uh, the orange's read, is just a read, it could be routed by my middleware of whatever kind, or my smart client, to a reed replica that hasn't yet heard the news of blue committing. And so this can happen. And so if I have reed replicas in a normal postgress setup, I do not get strong snapshot isolation. I just get a weaker version of snapshot isolation that doesn't offer strong consistency. It can still be the right choice, but it does cause a set of anomalies that your application programmers now need to worry about. In DSQL, because we use high quality physical clocks, um, and if you're interested in the implementation details for this, uh, you can check out my talk from Reinvent last year or Mark Bose's talk from Reinvent this year, where we go into this in detail. But in DSQL, because we use high quality physical clocks with very well controlled bounds, Orange's snapshot time is greater than Blue's commit time. And so orange will always see blues reads, and so this is a very similar version to the kind of core postres version, um, but is uh in the distributed setting rather than in the single machine setting. And so we get back to our question, is this just pedantry for nerds? Right? We said that isolation is not pedantry for nerds, but what about consistency? Is this something that I need to worry about in reality? I think it is. Because what I have observed working, building applications and working with teams that build applications against databases is that application developers find eventual consistency very hard to reason about and find it very hard to write correct and not surprising application code against eventually consistent databases. I have also noticed that the customers of these systems find eventual consistent behavior very, very weird. I've created a resource. I get a successful creation, I go to describe that resource, and I'm told that resource has never existed. Well, obviously that is a very surprising behavior, and it makes building things like automation against APIs that behave this way very difficult. You have to be super defensive. It also pushes complexity to the API design. Uh, because you have to provide tools for clients to be very, uh, Uh, very defensive. It also pushes performance problems and meta stability problems into our applications because people build retry loops to deal with this eventual consistent behavior. And as soon as you have a retry loop, you have a loop that increases the amount of work that the system is doing during periods of overload, and therefore makes that overload worse and makes systems worse. And so I am a huge fan of strong consistency. It is not the right choice in every setting. There are settings like mobile and IoT and places that are hard to connect where weak consistency is the right choice, but I believe that for most systems built in the cloud, strong consistency is the right choice for applications built on databases. I'm also going to say. Perhaps boldly, perhaps, uh, uh, you know, controversially, shouldn't be controversial, that database systems can be highly available and strongly consistent. You might have heard otherwise. You might have heard people claim that this is not possible, but it very much is possible. If we have a database and an application spread, for example, across 3 availability zones, and one of those availability zones is partitioned off from the network, there is absolutely no reason that the majority partition, the 2 that are still working, cannot continue to be strongly consistent and highly available to their clients. And this is the most common failure case for cloud deployed and data center deployed applications. The most common failure case is that one of the data centers drops off the network in a way that not only makes it unavailable, but also makes the application, the part of the application that's running in that AZ unavailable to clients too. And so you may as well just be unavailable in that partition. There are reasons not to make that choice. But for the vast majority of applications, I believe that it is the right choice. And I've provided some citations here for those who don't believe me about this. I'll also say pretty boldly, you hear the sort of version of the iron triangle version of the cap theorem, you know, consistency, availability, partition tolerance, pick any two. That is bunk. It is just not true. It is not a trade-off that exists in distributed systems. And if people cast it that way, they are very confused about the shape of the trade-off. Partitions always exist in any non-trivial sized system, and so we can't avoid having them. We do need to choose between full availability. Which is this weird definition of A, which isn't the definition that's in your head. If you read the paper of the proof of the cap theorem, A means every client is able to access the database, which I think is honestly just a silly way to define availability. Or consistency, but we can have what I believe is common sense availability to the majority of clients and consistency even in the face of the vast majority of patterns of real world network partitions that happen on networks as they are built. The trade-off that is much more interesting and much more important, and I wish people would pay more attention to, is that strong consistency strongly requires additional latency if only on rights. Your two options here are to go to a primary. Go find the primary, find the one copy, or to make multiple replicas. You can do this asynchronously and make clients, make readers wait, or you can do it synchronously and make writers wait and not make readers wait. But somebody ends up having to wait. So we can look at the performance of our databases. And here it's important to say, what I'm measuring in this table is, is inter-data center round trip time. So these are not milliseconds. Uh, they are just the numbers of times that we have to cross between data centers. And that data center crossing takes 1 to 2 milliseconds. So depending on your workload, this can be a big deal or not. So in DSQL's design, you get strong read consistency and strong write consistency. You don't have to cross data centers for reads or writes. And at commit time, you have to pay on average 1.67 data center crossings. That 1.67 comes from one third of the time, you don't have to go and find uh what we call the adjudicator, it's this kind of lock server in the system. And then the 1.0 comes from having to replicate. Postgrades for primary, if we go to the primary every time, uh, then we get strong reads, we get strong writes. 0.67 RTTs, 0.67 RTTs for reads and writes, because 2/3 of the time in my 3 AZ setting I have to go to a different data center. And then 1.67. Uh, at commit time when I have to load in the reads. Now I can start doing eventual consistency stuff and get local reads at the, at the, um, you know, at the at the cost of consistency and the benefit of my reads going down to zero RTTs. Dynamo DB, if I choose strong consistency, this by the way, one of my favorite things about Dynamo DB is that it has this really selectable consistency mode. Um, but I almost always choose strong consistency with Dynamo DB, uh, read RTTs or 0.67, and rights and transactions are 1.67. Now when we get into the multi-region setting, this gets a little bit more pressing. Because the round trip time goes up from 1 or 2 milliseconds to 20 milliseconds, 100 milliseconds, 200 milliseconds, depending on where in the world your workload is running. And here, if we look at the trade-off between DSQL, for example, where again, reads are local, rights are local, we're not crossing between regions to do those things. So that's great. But to commit time, I have to pay 1.5 or sometimes 2 because we haven't finished an optimization, uh, round trips, um, which can slow things down. And so if I compare that to the last row of the database, which is Dynamo DB Global table. Um, Dynamo DB global table. I don't have to cross for reads. I don't have to cross for rights. That's great. I still have to cross a Z's, but I don't have to cross regions. But the cost I have taken on is eventual consistency for my reads and post-commit merging of rights, which might or might not be surprising depending on the behavior you're looking at from your application. And so this is the shape of the consistency trade-off that I think is much more interesting, and application programmers, database people, and distributed systems people should be paying a lot more attention to this kind of thinking about latency as you think about how to build out systems that span multiple regions. Thank you very much.