---
video_id: tr2ri8I77To
video_url: https://www.youtube.com/watch?v=tr2ri8I77To
is_generated: False
is_translatable: True
summary: "This lightning talk, \"Fast-track to insights: AWS-SAP data strategy (ANT333),\" features Satish, a Principal Solutions Architect at AWS, and Bjorn Friedmann, CTO for SAP Business Data Cloud. They discuss the 17-year partnership between AWS and SAP and how it enables customers to fast-track insights from their data. Satish outlines AWS's comprehensive support for SAP workloads, including various data platforms (SAP HANA, BW, DataSphere) and the new AWS \"Zero ETL\" integration, which allows for fully managed, near real-time, no-code data replication from SAP OData sources directly into AWS Glue and other targets like Amazon S3 and Redshift. A demo showcases how to set up this Zero ETL integration for SAP purchase orders. Bjorn then introduces the SAP Business Data Cloud (BDC), a lakehouse-based solution that manages \"data products\"—enriched datasets with full semantics—in an object store using a medallion architecture (bronze, silver, gold layers). He explains how BDC solves the data integration problem by preserving business semantics and enabling \"derived data products\" that combine SAP and non-SAP data. The session emphasizes enabling AI and agentic use cases by hydrating knowledge graphs with these semantically rich data products."
keywords: AWS, SAP, Zero ETL, Business Data Cloud, Data Products, Lakehouse, AWS Glue, Data Integration, Semantics, Agentic AI
---

Hello everyone. Thanks for joining today's session. Um, does anyone know how long is the partnership between AWS and SAPS? It's, it's 17 years. In today's session, we will be talking about how you can leverage this 17 years of deep partnership to build some of your enterprise need use cases. So myself, uh, Satish, I work as a principal SA with AWS. I have Beyond SAPCTO. So together we will walk you through some of the AWS and SAP features which will help you to fast track insights wherever your data is, whether it's an SAP or non-SAP. So let's get started. So before we dive into the specific features, let's understand why customers are choosing AWS for SAP workloads. So AWS offers the broadest set of SAP data platforms, be it SAP HANA, DataSpear, BW. You have all kinds of options that are available for you in AWS. And not only data products, you also have the option of AWS Native Services for your security, governance, and for building your data strategy, data pipelines, generative AI workloads. You have 200 plus services in AWS which will give you the comprehensive capability to build use cases. So customers are liking the comprehensiveness in this partnership. We have thousands of customers across different industries, be it healthcare, automotive, technology, retail. We have customers across the spectrum. For example, Moderna. Most of you know, they are one of the first COVID vaccine supply delivery firm which delivered millions of vaccines in a very, very quick time across different places. And they run their supply chain optimization on SAP using AWS Cloud. And similarly we have Toyota, Volkswagen, Adidas from retail, so we have these customers running mission critical workloads on AWS. So we talked about comprehensiveness. Let's understand some of the choices you have to begin with. In SAP we have been supporting SAP DataSphere, BW, SAP Hana. All all those products are supported for quite some time. But this year SAP Business Data Cloud was first launched on AWS in February. 2025 with Business Data Cloud, you will be able to build data products to derive insights from your data very, very quickly. Then the next set of options you have are AWS Native Stack for data processing. You have AWS Glue, EMR, and for data warehousing you have Redshift, and for visualization you have Quicksight. And if you would like to build AML or generative A workloads, you can use Sagemaker AI. So that's the comprehensive set you have at hand to build any kind of use cases. And the choice doesn't end here. So you have options and you have AWS Native options, and you also have options with providers such as Click, Calibra, Snowflake, Databrakes. So that makes the total technology stack very, very comprehensive to solve virtually any kind of use case you have. Choice is great, but what you do with that choice really, really matters, and whether you are deriving any value insight out of those choices makes a big difference, and data is your unique asset, and your data might be spread across different sources, some in SAP, some in non-SAP. So what we always tell customers is. Don't leave your data scattered across for generating insights. Onboard that data into a unified lake house and build insights out of it. And customers often come back and tell us that's great. We would like to bring all our data and derive insights, but onboarding that data is super complex because of the different technologies involved, different skill sets involved, and the people involved. That's where AWS Glue offers different native connectors across all these sources, so you can quickly build pipelines and bring the data into a lake house, be it S3, S3 table, or redshift. And in today's session, I will be talking about a new feature called Zero, which is a fully managed service which will help you to replicate data from a variety of sources into Unified Lake House. So let's understand how 0 ETL works. When you set up 0 ETL for the first time, it will create a snapshot of your source data and seed it into your target, be it S3, S3 tables, or redshift. That is the first thing that happens when you set up the 0 integration. And from there on, whenever a change happens from the source, there will be a continuous replication, incremental replication from the source to the target, and this replication often happens in less than 10 seconds. It's a near real-time replication. And you have the choice of picking specific data objects that you would like to replicate. That's what we recommend, particularly in the case of SAP. We recommend the specific data objects which you would like to unify. Do not replicate the whole data, that you have that option with zero ETL. And then you can also choose the refresh interval based on your data refresh requirements. You may have near real-time requirements or you may want a daily refresh. You have that option with zero ETL. Let's see a quick demo. This is Glue management console. On the left hand side you have the option to select zero ETL integration, and you will start with creating a zero ETL integration and for SAP the connection is through SAPO data. And I have the connection pre-created, so I'm selecting that connection and the IAM role with which which has access to the SAP instance. And now I'm choosing only the specific object. In this case, I'm interested in purchase orders, so I'm selecting the purchase orders and I can do a preview of the data before bringing it from SAP. So you see the preview here. And then now you will select the target for this demo. I'm selecting S3 as the target cataloged in Glue database. So that's the Glue database I selected, and what is the IAM role which has access to write into the Glue database. And then the rest of the details are auto populated, but you can choose the specific target table name. Either you can leave the source target table name. In this case, I'm renaming it to purchase orders, so it shows up as a purchase orders in my target table. That's it. Now you will select the refresh interval. Default is 1 hour, but you can choose 15 minutes. So I'm editing it to 15 minutes. And give a name to the integration, name of your choice. That's it. Pretty much you selected the source, target, refresh interval, and you'll click next. You can review all the details of what you selected, and then click on create zero ETL integration. As I told you, it will first create the seed from source to target, and as soon as that seed is complete, the status becomes active so that it is ready for querying the data. No code, it's everything through UI, right? It's pretty straightforward. And once the status is active, you can click on the target and see the list of tables. So you see purchase orders and also a metadata table which contains status of replication and how many records got replicated. So if I click on the table data, it will open Amazon Athena. So there you can see the data source and the different tables that got replicated. If you see here, it is showing the replication is completed from purchase orders, and this is the volume of data that got replicated, and you can also query the actual purchase orders table by right clicking and seeing the preview. You will see the different manufacturers and the plant IDs, the entire data that is available in which got replicated into. The Glue database. So the point we would like to drive through this demo is the replication is pretty straightforward without any core involved here and it's a fully managed, no cost service for you. So before I hand over to, I would like to give a comprehensive overview of different sources and targets that are available for zero tail replication. So you can replicate from AWS native databases or you can replicate from third party applications such as Salesforce, and in today's in this event, in this year's reinvent, we announced support for self-managed databases like Oracle, MySQL, and SQL Server, Postress SQL, whether they are hosted on EC2 or on-prem. So you can use ero to replicate the data seamlessly between these sources to the target of your choice. So with that, I hand over to Bjorn to talk us through SAP Business Data Cloud. Thank you. Thank you, Satish, and thanks for having me. I'm really excited to be here today. So my name is Bjorn Friedmann. I'm the CTO for Business Data Cloud, and today I would like to talk a bit about the integration, um, how you can basically integrate SAP data also with non-SAP data. So as, as Satish was showing, there's quite an easy way how to serial ETL, a subset of the data from SAP into AWS. However, if we look at SAP data and how it's typically being um. Used we very often see that there's a wide variety of different downstream consumption, so you have different user personas, you have application users, you have analysts, you could have Developers or data modelers and typically they used to all the data into various buckets. However, the problem with SAP data is it's so specific in terms of semantics and so complex that in order to really make use of it in a reliable way, you have to recreate the semantics over and over again. And now with the introduction of business data cloud, we actually solved this data integration problem for all data holistically. And now I want to introduce how we did that, right? So in principle, what we want to do with business data cloud is we want to make it super easy for US customers to use SAP data. Either in combination with non-SA data, but also we provide capabilities to seamlessly use that data and other providers like AWS, for instance. So the goal to achieve is also to basically unblock AI and with that agents to make use of this data and to really derive meaningful actions by fully understanding the semantics of the data. So on the bottom layer you see the SAP applications. This could be S4, it could be A Rebar, SuccessFactor, all of these things. And in Business Data cloud we then basically create so-called data products which are a 360 degree overview of all the source data coming from various systems and ultimately we then feed also the data plus the semantic of the data. Into a knowledge graph which then can be hydrated by agents in order to really take action on top of this data by fully understanding the semantics of the data. So what is business data cloud? Business data cloud consists of a lake house layer. So this is where you see on the bottom part of the slide data products. We manage these data products in an object store, and I will come to this in a second when we look at the architecture. On top of that, we have the so-called business data fabric. Business data fabric comes with various options for you as customers to consume this data. This could be, for instance, in our cloud warehouse which is called DataSphere. You can then put analytics on top with sub-analytics cloud, but we also provide a way and a path for BW modernization into the cloud. Ultimately, we then also offer SAB data pricks as a built-in native citizen into our stack AI AI and ML solution in order to achieve all these AI and Agentic use cases. In addition, we're also working on enabling other partners like AWS in order to make use of this data. And ultimately on top what we have is then a comprehensive application layer we call these intelligent apps. Intelligent apps could either be industry solutions or it could be also any kind of application built on top of, which is our business technology platform, but it can also be third party applications consuming or producing data products. Good. So what are data products? Data products you can think of, it's more than just the relational data sets which you typically would extract out of SAP, but we also ship these data products fully managed as a fully managed service, so it's a SARS offering basically, and we also enrich these data products with the semantics of the source system. The metadata which we have for this is open sourced, so this is open resource discovery. You can look this up on the internet. It's a fully open source protocol invented by SAP and we're using basically this as a foundation for integrations with partners like Dataricks or AWS. Now in addition to this, we also offer a various set of API in order to access these data products. So for instance, you can access data products via Deltashare. That allows for zero copy consumption in various downstream services, be that now apps, warehouses, or other applications, but we also will allow SQL-based access on top of data products. Now what we achieve with this is basically we established what we call a data product economy. So if we look on the upper part of the slide on the left side, you see the SAP managed data products. Those are data products which we ship out of the box as a managed service for all SA line of business solutions which are native cloud citizens. So that means as for Hana Cloud, private cloud edition. Is one of the sources, SuccessFactor, Areba, Concur, but also all the other LOB solutions which are running in public cloud. These public cloud sources can be hydrated and can be used as a source for fully managed SAP data products. In addition, we also allow customers to create what we call custom managed data products. Those could be, for instance, coming from sources on-prem. This could be. This could be Sfhana on-prem, but it is also possible to integrate BW-based data and ultimately you can also integrate third party data. And by enabling the combination of these various data products in a zero copy fashion, we also enable partners to create data products and share this back with through the open source protocol or open resource discovery which I was mentioning, and ultimately through Deltashare we enable a bidirectional sharing in both directions. Now let's look at the architecture and how we set this up. So as I was mentioning, business data cloud is built around the lake house solution. That means the heart and center of data products is an object store where we actually manage these data products. It was very important for us to basically decouple the ingestion layer from the consumption layer. Consumption layer can be applications, can be warehouses, can be partner solutions, and for this it was very important that we establish a stable. Which is completely distinct from the source layer. So in order to manage this, what happens is basically the following a consumer, this could now be an app, or it could also be an administrator who is using the so-called PDC cockpit, which is our administration interface, ultimately triggers an installation of a data product. That calls over into the BDC layer where we have an orchestration layer which then basically manages the whole integration, the whole data integration chain. So it starts with um calling an API in the source system, in that case it's S for HANA. And we basically, let's say for the sake of an argument a customer wants to install a sales order data product, so we would then basically tell S4 Hanna to start collecting all the data for the sales orders. Sales orders basically consist of various tables in S4, and then S4 Hanna would start to collect the data and push the data into the object store. Now the data lands in the so-called bronze layer. This is the raw data or ingestion layer, and on top of this we then apply the medallion architecture. So for those of you who are not familiar with the medallion architecture, it's kind of it's an architecture which allows you to to decouple the ingestion layer from the consumption layer. So what happens technically is we take all the raw data. This could either come in as CSV files. It can already be parquet files. It could be completely unstructured data, and then we run a transformation as part of a spark pipeline. This transformation is basically part of a data product definition. Which then basically tells the machinery in our case that spark, how to transform the raw data into a data product and ultimately in the silver layer this is where you then find the so-called primary data product. Primary data product basically means it's more or less a 1 to 1 representation of the source data, but as a stable API for consumption. And then ultimately in the gold layer you could find so-called derived data products. Now what's a derived data product? Let's say you want to now combine sales order coming from S4 with employee data coming from SuccessFactor or with third party data coming from whatever sources you might think of. If you want to combine these data, that's possible. Because you can basically as an input for a derived data product you can point to a primary data product. You can then run transformations and with that you would create a so-called derived data product which you basically find in the gold layer. Ultimately after this transformation has been done, we then basically expose the data product with its semantics in a central catalog service which is called Unified Customer Landscape, and there we actually register the metadata as part of this document and that's now also the service which basically allows consumption in other downstream applications or partner applications. And with this, let me end the session for today with a call to action. So here you'll find a couple of links if you're interested in either how business data cloud works or what the zero integration means and how it looks like. So this is where you can find these links. And with that, let me thank you and let me remind you that you can give feedback for the session in the normal channels. Thank you so much. Thanks.