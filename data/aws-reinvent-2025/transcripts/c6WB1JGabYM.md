---
video_id: c6WB1JGabYM
video_url: https://www.youtube.com/watch?v=c6WB1JGabYM
is_generated: False
is_translatable: True
---

Good afternoon everybody and thank you for coming to the sports forum. We're really excited to kick off your reinvent experience with our session here and with all the exciting activations with our sports partnerships here in the sports forum. My name is Jake. I'm a sports specialist at AWS and I'm joined by my colleague Kasweni Rudra. Together the two of us are on the team at AWS that supports a number of our sports customers. Now what likely brought you here to the sports forum and our session today is your interest in sports or in entertainment. For any of the sports fans in the audience today, I really hope that your fan experience has been much better than mine. Because mine has been just a roller coaster of emotions and heartbreaking losses. But even though I've gone through that rollercoaster of emotions with my sports teams, I still find myself going back to those teams, purchasing the merchandise when I see their promotions, I buy tickets to the games and I'm watching the content, and I'm interacting with their digital content online. And these are just an example of how how organizations in sports, media, entertainment, and games can use data, analytics and AI to drive fan engagement. With the availability of so much content available to consumers today. Organizations in sports, entertainment, and games have to find ways to compete for the consumer's time. So there's all these hours in the day but only a few options that organize and the fans have to uh interact with between. Sports dominating live broadcasts. And when the fans don't have exciting games to watch and they get tired of what's happening on the field. They have their TV channels and their streaming platforms that they can watch their favorite programming on. And when they get sick of that, and they wanna watch something else or do something else, they have games easily accessible to them. They have games accessible and available to them on their mobile devices, other devices like their tablets and computers, and their consoles. So the intent to frequently go back to return to entertainment offerings starts with a positive consumer experience. And organizations need to spend so much time trying to find ways to retain their digital consumers. Since we're in the sports forum today, we'll analyze this through a lens through sports and a fan experience. So within a fan experience, customers and fans are expecting exciting content to be on broadcast and to be on TV when they can watch them. They want their experiences to be valuable and extremely entertaining. They want these games to be thrilling and exciting. And then when they interact with the content from these sports leagues and sports organizations they want these experiences to be streamlined and very clear they want them to be easily accessible they wanna find the information and the and the media and the content that is important to them very quickly and they want to be able to navigate through these experiences very seamlessly. They also want interactive content. Fans and consumers want to find ways where they can interact with others that are watching the same thing as them. But they also find a lot of value when they're able to contribute to what they're actually showing on the screens. And finally, personalization is extremely important because with so much media and content being created every single day from games to live sports, all of that, they want to be able to find what they care about. Fans want to find content that is easily accessible on digital platforms and mediums, so they want to find personalized experiences through uh. Through curated and the recommendations that are designed specifically for them. So experiences are generating data on all of these platforms for each individual consumer, and these data points are being captured across all of the channels that they're interacting with. So a fan who purchases a ticket or signs up for an account creates a profile they're creating that foundation for that fan profile for an organization to capture, and this is their geographic and their demographic data, so the foundation of that profile. Based on what they're purchasing, such as the jerseys or any merchandise, they have the preferences of those fans being captured as well. And on the digital front for applications and online sites, user behavior and interactions are also being captured in real time where numbers of videos and articles that users interact with, how long are they watching all of that information is being captured and designing the the content for these users and enhancing their data profiles. And finally, associations are also being made between different fans, so fans who are supporting one team or one league, who are closely tied and related to other fans, they might share the same interests and support the same organizations. And these are just a few of the examples that data is being captured and all these different domains for building up a fan data profile. And the idea and concept of using data on these fans and profiles is not new. At the very early stages of using these in sports, we use data and analytics to create fan data profiles and create personalization and segmentation. ML models were also being used to create advanced stats for sports, so watching on broadcast to help understand and become more well versed in the games that are being shown on TV help you understand what's actually being uh taking place on the field. These stats in the very earliest days were used to explain the biggest plays in sports and they were very logic based so they would only explain things like how fast is a player running or how far is the ball traveling. And as capabilities in analytics, data and AI became more advanced. Organizations became more privy to creating new experiences, so. Advanced stats became more diagnostic and predictive in nature. And these stats were able to be applied on players, when teams and the different formations that they played against. With the onset of generative AI, media and content became so much faster and accessible. Organizations in sports, media, and entertainment are finding ways to create solutions that help accelerate the process of finding content and putting it online and putting it in front of all their fans and their consumers. And information retrieval became even faster, and the process of solving low level problems became much faster as well. And as organizations become more comfortable with utilizing AI capabilities, they look to enhance the fan experience and create new things and experiences for fans to enjoy. So personalization that we talked about earlier has become hyper personalization or the incorporation of machine learning methods to increase the accuracy and lead to greater and better recommendations for fans. And new experiences and new digital products with AI powered features are becoming even more commonplace. And lastly, organizations are finding ways to create solutions not just for their own teams, but also for partners and others or other organizations within their entertainment ecosystem to find ways to where they can also improve the fan experience. Today we'd like to focus primarily on two areas, the fan data platforms that manage all of the data profiles and also creating new experiences and digital products using AI powered features. And we look to some of our sports partnerships where we've built similar solutions to gain inspiration on how we can do this across sports, entertainment, media, and games uh companies. The National Football League or the NFL created a unified view of all of their fans using an AWS platform built on all AWS services and partner solutions. This fan platform processes over 90 billion rows of data with over 250 dimensions. And all of these records are refined, resolved, enriched, and standardized into over 70 million fan data profiles stored and managed within Amazon Redshift. And every single day, over 1000 data feeds are being processed and standardized using Amazon Glue or AWS Glue and Amazon Kinesis data streams. The result of creating this data platform for the NFL. Enabled them to have a better view and understanding of their fans by at least 4 times, and the result of this leads to a 2 to 3 times increase in the opt-in rates where fans and consumers. Went into their uh opt-in for their emails and promotions. Taking a look at how preferences drive personalization, we look towards the Bundesliga, where they personalize their app engagement. So soccer or football is a very personal sport, and between Bundesliga and Bundesliga 2, there are 36 different teams. Between these teams, the Bundesliga determined that on average, a fan supports about 4 different organizations and teams. That's approximately 1.4 million different combinations of fan preferences. And this becomes even more complex as you incorporate. Factors such as demographics, preferences. And your interactions and behaviors. The Bundesliga uses Amazon Personalized to better analyze and process these different combinations so they can curate better recommendations for articles on their Bundesliga app. And after launching this feature and enhancement onto the Bundesliga app, they saw a 17% increase in the time spent within the app. And users who spent more time in the app, they saw a 67% increase in the number of articles that were opened because of that personalized recommendation system. And out of all the different sy and out of all the different sessions that users went into, they saw a 32% increase in the sessions where users actually went in and opened different articles. Taking a look at how organizations can create an AI product to create a new fan experience. The PGA Tour created Torcast and AI commentary, where traditionally on broadcast. The PGA Tour was only really able to cover about 25% of all players' swings throughout every single tournament, and all 25% of those were limited on that broadcast. The PGA Tour is able to process and manage 53 million different data points every single tournament weekend, and this is all being managed and stored within Dynamo DB. And using all of these data points, Amazon Bedrock is being used to create AI generated commentary. For every single shot that is taken throughout a PGA Tour tournament. And this commentary is not only describing the shot based on the data points, but also providing context on what is happening on the field, on the course, and what that shot means for that player within the context of that specific tournament. And as a result of this, The PGA Tour was able to cover 100% of all players' swings on all of their tournaments, and what's incredibly impressive is that from the time it takes a player to make their stroke on the course in less than 10 seconds, that AI generated commentary and context is created and displayed within Torcast's AI commentary system. So we've taken a look at how we can use data analytics and AI for a different number of our sports organizations and our customers who have created these innovations. Uh, I'm gonna invite Sweni to talk to you about how you can create these using data and analytics services on AWS. Thanks Jake. Uh, hi everyone. My name is Ashmi Rudra, and I'm principal solution architect with AWS. I have almost completed 8 years, 4 months, and 13 days here. Uh, I'm giving data because data is a very strong, uh, point when you're building any architecture, right, and any informed decision you want to take, you need a data and it brings trust, uh, to build a data platform, uh, I want to give you the guidance here that how you can do it, but I want to know that how many of you have, uh, data engineering or ETL or data analysis kind of have a background, those kind of background. OK, so what I'm going to do, I'll. Used to name a lot of AWS services, but I might not get time to dive deep about it. But we'll, we'll learn about the patterns, which is a very common patterns to build any fan 360 or fan data platform or fan genome platform. You can name anything, uh, anything related to that, but it's almost similar nature. So what you'll see that any data platform and F 360 platform, you'll see that they divide into the layers, right? A very common layer you'll see that there is of course data source layer. You decide that OK, what are the data sources or different data sources you are gathering about information about your fans. Then you'll see that ingestion layer where you decide how you are going to ingest that data because you might have 60 sources, 70 sources. Everyone might have 3 different Vs, which is a basic volume, variety, and speed, uh, velocity. And then you have to process the data according to your data pipeline. There could be multiple, multiple pipelines in your whole data platform. One pipeline maybe for your data engineering team, one pipeline for analytics team, one for the maybe directly to the fans or third party vendors or your partners. Uh, then you have to store data according to their needs. Because your machine learning understand the data in a different format or your analytics team uh tool understand the data in a different format, so you have to massage the data and then store into that different format so that you can share information accordingly within the system or to to your fans or to your consumers then. There are 2 more layer you can merge those two layer in one. You can have a separate layer. It's up to you because these are all logical layers. One is new intelligence layer where you prepare data and share with your data engineering or a data scientist team where they can experiment with the data and tell you that what extra you can do. And one is activation layer which is you know about about your metrics, and you share exactly with them that this is the KPI, this is the metrics, this is the dashboard, this is what they want to achieve. So exactly two ways you are doing nowadays. One is for known, you know it, and one is for unknown where you have no idea what could be out of possible. Uh, so this layer of, I'm showing one more diagram here. You can see that, uh, these are the data sources again. It varies, uh, customer to customer, teams to teams, leagues to leagues. Very common is like, uh, your league own sources, uh, you're getting from the teams or clubs. You have a ticketing data if it is for the fans. Uh, they might have a subscription to the different channels or different uh subscription mechanisms. Uh, they might have merchandise information as a fan, you might go and buy a t-shirt and that information that particular company has. You as a league or as a team, you want to ingest all those data and stored somewhere and integrate uh you'll be using different tools for that. We'll dive deep about each kind of normal common patterns here in the next few 10 minutes and then you store into different uh purpose built databases like it could. Be data warehouse which is the hour shift. It could be directly unstructured data, either images or a text-based data you would directly stored in the data lake, or you want to, uh, you know, transform your data, uh, again based on 3 weeks volume or size or variety, uh, either using AMR or either directly using glue. And then you make that data aware to your activate activation layer and then everyone is here. One thing I want to tell you that you can see that data catalog of governance layer is there which generally customers use our, uh, lake formation, our data zone, uh, services, uh, to govern that data to make sure that the data lineage and how data is changing everything is trackable. Uh, the same concept I'm sharing, uh, here with a different, uh, you know, Ados diagram and services, uh, so, uh, all these patterns I'll be talking more, uh, so one example is there that you have a data ingestion patterns and you want to know that, OK, how one example. Uh, how I have to ingest the data. There could be 60, 70 sources. One comment is there that you might have heard, heard about, uh, AWS crawler, uh, Glue crawler. One is that it has more than 60 connections where you can directly connect to those sources and crawl the database and create a data catalog. Others might be your customer exposes your data or your partner exposes your database on API. You have 3 or 4 options out there. You can use containers, Kubertis containers, connect with those API, massage the data, get information, basic information, and put it into the S3 bucket which we call raw zone or bronze bronze zone, uh, multiple noclatures are there. You can call it and store it there. Small payload sizes less than few less than, uh, you can say 1010 MB lambda shines there you can directly you have a surveillance approach use AWS lambda, call the API, get the payload store stored into the AWS services. Multiple suppose your APIs, uh, uh, supports the batch kind of processing. You can use even AWS batch here. Again, EC EKS is uh ECS is also another options which I have not mentioned there, but again it depends that what, how the data expose. Other is real-time streaming, and there I have seen a lot of confusion have that people ask that should I use Kinesis or should I use Kafka? Should I use Flink? What should my approach is there. Again, it depends on your what is your payload size, how quickly you want to store, do you want to transform your data when you, uh, process it or you want to transform it later later. Small data, less payload size, uh, less variety of speed and velocity, go with Kinesis data stream, go with data fire whose, uh, real-time transformation analytics you want to do. Go with Flink, uh, go with MSK, uh, larger payload sizes go with MSK or even Rabbit MQ. It's again depends on that. What is the velocity, variety, and, uh, uh, speed is here with the data again, recommendation is stored into S3, which is your loss, uh, raw zone. After that, if suppose your, uh, customer or partners, uh, stored data using file upload. And just want to replicate, uh, AWS transfer family is there which understand FTP FTPS or SFTP, uh, protocols, and they can directly upload and it connects to the S3, so your data will be, uh, there in your again in your landing zone batch process we talked about it, uh, based on sizes you have option like AWS batch, uh, EMR or AWS Glue. Uh, AWS Blue has, uh, transformation capabilities there. You might have, uh, heard about, uh, Data Blue, which has 250+ transformation capabilities where you can do the data validation, check the null value, uh, replenish data according to your date, uh, requirement, change the date format, even change to the parque format so that you are, uh, selecting the right data format there. Uh, suppose you have a change data capture requirement. You've done the full upload, but you have, uh, incremental changes there every hour, every, uh, you know, day your data is changing. In that case, uh, if it is a normal database, you have a, uh, option called, uh, AWS DMS, which is a data migration services. You can replicate your data. I also use the change data capture that how much data has changed, the data changes there, that only you are replicating in your, uh, data zone. Uh, there is something called AWS app flow. Which connects to the other sources like your data is there in the in the one example SharePoint other file storage or or Salesforce from there easily afflow I remember it has 50+ uh connectors where you can connect to the different SAS application, get, get that information, get that data, and again, uh, make sure, uh, land that data into your landing zone or bronze, uh, zone or, uh, uh, raw zone. Uh, now your data is there in the landing zone and that is available. Generally what people do is that they create a multiple data pipelines. I'm giving an example of one, but again, uh, recommendation is there that work backwards, uh, two ways. One is working backwards that you know exactly that what your partner or customers or team or marketing or sales team exactly what kind of metrics they want. And they, you know that, OK, this is how your dashboard is going to look. This is where you're going to store the data. This is what exactly KPI you need, and then you build the whole pipeline according to that by deciding that what format, what data format, what cleaning is requirement if it is a machine learning based, what feature engineering is needed after that you export that data. Other is your transformation ready enrichment data which you're exposed to your, uh, again a data scientists which is like unknown information like you give it to your experimentation team and say that what we can do, what is the art of possible, and then you expose data that way you can see that here in the slide you have two zones one is business transformation ready and one is analytics ready. And we achieved through that, uh, so most of the common, uh, transformation I have seen, and I spoke about the data brew there which does the transformation, but in pipeline we have seen very common in identity resolution, and I want to talk about it, uh, later. Uh, but this is a very common way. Identity resolution means you get the data from multiple sources. Uh, you don't know the connection between them because one data is getting from a merchandise team, one is getting for the ticketing information, one is from a team or league, how it's connected to each other about your fans, how those fans are same or different fans, how you realize that. Then you do incremental processing. You do the quality check, the QEC check that null data transform according to your, uh, requirement. Uh, ros OC OCR or, uh, or parque format. So all these, uh, transformation you do, uh, uh, fan loyalty scoring is one fan segmentation is the other thing. So these are statistical and mathematical changes, uh, you, uh, use AWS, uh, sage maker. For those mathematical model run and then move it to the curator zone, uh. And after that, uh, your data is available for processing, further processing either through, through, through your uh available through your APIs through your fans or through to your applications or either to your BI team. Uh, then One spoke about the first is identity resolution that what how you realize that these fans are same. So one real world example is saying that suppose I'm a fan and I purchase a ticket and watch a game, right? Same. I went to the other vendor and purchased a t-shirt, which is, uh, one of my favorite t-shirts, and then I went to the other app and then. Look for some video and some article. This information as a team or a league you gather, right? But how do you realize that these three are the same fan or these three are the same people. Other is like maybe same person but a different person but from the same family or same friend friend group, uh, so that they are related to the same fan group. So those realization, those identity resolution you do through normally people used to do through normal checkup like if names are matching and address are matching, they realize that maybe it's the same, uh, fan. But suppose you are getting these much records in the millions of times, right? Millions of records are there. Running information and matching uh this tool through normal processing, it takes time. In that case you have two options. Uh, one is through fuzzy logic you decide and you match. Other based on the machine learning model you do it. So you have to do the fan profiling there to find the relationship between a fan and with the confidence you tell that these are the same people or these are the same fans so that your marketing team or sales. Team can do the better job to inform your fans that OK this is the offer I'm getting this is the ticket you'll get this, these are the loyalty score you have so better information I want to provide it. So ingest and normalize, generally you ingest the data profile, create a fan profile, then you try to match it so you have a fan identity register uh services you can achieve, uh, identity. Uh, using two ways either you use AWS entity resolution, uh, service, uh, which is a managed service. You don't have to manage infrastructure, or either you process and EMR. That is also algorithms are available there, uh, so you do two ways. First is recommendation that always do the deterministic way like based on matching some rules like a fuzzy logic and achieve that. And either after that run the machine learning algorithm. The reason behind that, some, some information might be missing and you don't know. So you get a probability that looks like these are fans from the same group or these are two fans are the same. And you inform that data and if you have a confidence rate is high, you merge those fans that with the confidence that these are the same people which you have information so that you structure as a very with a confidence way that you can take informed decision based on that that what you should do with the data. Uh, after that, your data is ready for Curator June and how you expose it for further analysis, right? Everyone is talking about machine learning. Everyone is talking about, uh, you know, AI, uh, so we have two options there for that. One is Sagemaker. One is Bedrock, uh, for normal machine learning capabilities. Suppose you want to do the segmentations, uh, you want to do the binary classification that you should do which says yes and no, something like that, you, you go to the Amazon Machine Learning Sage maker, which is well integrated with S 3. Your data is already ready according to that algorithm aware data format. And then you connect, run, uh, run your algorithm, get the data, and then stored into S3, and that is like curator zone. Your data is ready. You might have a multiple pipeline. Other is you want to get a summary of the fan that this summary is, uh, this fan is a loyal fan from the past 7 years, and he is a bronze medal. This much points he gathered, those kind of, uh, information you want to get, get the summarization. You can use Amazon. And Bedrock, get the summary result, store the data and necessary and expose it to your, uh, to your application or third party members. You, uh, for generative AI, you may need a vector database. There's multiple options. I have, uh, uh, shared the open search, but you can do, uh, achieve in the postgra as well. Uh, suppose you want to, uh, calculate the relationship between the fan. You may need a graph database. In that case, uh, Amazon, Neptune is another choice. Uh, other option, one I want to, uh, talk later, but, uh, I mentioned here Redshift, uh, which is a data warehouse solution, and most of the time I'm seeing that customers are getting confused that they, they should go for the data lake or lake house or data warehouse and what are the difference between them and which, uh, pattern they should choose. Uh, and how they should expose it to, uh, BI reporting and clean rooms, right? So when your data is ready, which I'm talking, going to talk later, but when your data is ready, uh, BI option, you can use any BI tools which is, uh, which, uh, because it's getting connected to S3, uh, but we have a Quick Suite which is AI, AI enabled nowadays, and, uh, for ad hoc query you can use, uh, Amazon Athena where you can just, uh, run the SQL-based query, uh, and get the result. And people are also sharing with their partners using AWS Cleanroom. Uh, so let's talk about next one that one of the very important architecture decision, uh, that lake house or data warehouse, what should I do, right? Uh, so it's a very basic high level information. I'll, I'll provide that if you have a very strong, uh, data engineering team with the data scientist, then understand the open source model and they have a basic analytics and you also, you want to experiment with your data. Uh, go with the data leak house, right? If you pretty much know that I know what I'm doing for the knowns, this is my metrics. This is my dashboard, and this is this kind of very, uh, query aggregation kind of query I want to run, uh, go with a data warehouse situation. So you had traded, uh, team had traditional SQLBI skill, right? Uh, so a data lake how generally you use Amazon EMR and you, uh, use, uh, either Apache Iceberg and hoodie. In that way, the question is there that how you achieve transaction and asset property on the data lake you use, uh, Apache Iceberg and hoodie and you achieve the asset property there which provides a transactional approach for you and data warehouse, it's already there to achieve, achieve that goal, uh, but suppose nowadays you want to also experiment. Right, you want to, uh, uh, share your raw data to your data engineering team and also achieve the data warehouse, uh, capabilities as well where you want to directly connect to your BI tools. Uh, the best thing about AWS is that you can use the hybrid approach. You don't have to worry much that I should use lakehouse approach or I should do data warehouse approach because Amazon Spectrum runs in the 3 mode, right? Uh, uh, Redshift runs in 3 mode. It has a, uh, a spectrum wherein, uh, based on ad hoc you can create, just create, uh, infrastructure, pull the data, run the query, get the results, or other surveillance mode as well. Uh, but 3 examples are there that where lake house and warehouse approach will be there, where you're creating a data warehouse where you just use the iceberg and hoodie and get the result. Uh, 3 examples are, are just there'll be multiple pipelines in your fan, uh, infrastructure, but 3 examples is suppose you are doing some real-time analytics and engagement fli shines where, uh, run using the on the lake house. Uh, suppose you have a historical data, uh, very structured data. Of course the school approach is better there. Suppose we have a multi-channel journey analysis. Data sources are disjoint, disjointed. Multiple kind of, uh, variety and multiple structure. Again, lake house approach is better. Uh, you have, uh, unstructured data, images, video, of course lake house is better in that case. So everything is works, but these are the better approaches, and, uh, that's why I say hybrid approach is the always the best approach here. So we talked about known and unknown. Known is easy because you know what you want to do. You have a pattern. Unknown is like how this data, which is exposed to your data engineering team or machine learning specialist, what is the possibility, what we can do, and Jake will share some insight here. Jake, over to you. So Sharini just walked through a few examples of how you can create data platforms using AWS and highlighted the flexibility of that. What we'll talk about now are the ways where you can use AWS and the flexibility in our AWS AI suite to create new products and experiences for fans with AI powered features. And AI we find is opening up even more possibilities to deliver new experiences to delight fans more so now than ever. Organizations can create new digital products that bring fans even closer to the sports and the content and the data behind all of that. That they truly enjoy today. And there's a greater demand from the age bracket from the mid-twenties through mid-30s that enjoy stat integrations on broadcast, so the advanced analytics have been a huge, huge success with a number of those fans in that age bracket. And it doesn't only just look like stat integrations on broadcasts and your digital interfaces and applications, but these AI features can be running in direct to consumer apps as well. Indirectly, organizations can create new products, solutions, and tools that elevate the entire entertainment ecosystem with their partners because those partners, for example, in sports, are your broadcasters and the interaction point that that most fans experience sports is through TV. Those broadcasters are the first point of interaction and are there to explain what is going on and by providing them with tools and solutions that help them explain what's going on and arm them to better talk about all the AI powered stats. That is directly catering to that age group that highly demands the stat integrations. And with generative and agentic AI there's the possibility to create new features and improvements on the features and capabilities in your current offerings on your digital applications. So our customers in media, entertainment, and sports are able to incorporate new agentic workflows and generative AI solutions into the solutions and offerings that they have today. So diving into some of these examples of how AI powered solutions can create new capabilities on what they already provide today, we start and look at content platforms. And organizations and customers are able to create ways for their fans and digital consumers to actually explore their media archives and content by creating AI powered search capabilities through semantic search and video understanding capabilities. And this opens up possibilities and new options for organizations that have large media archives to inject new life into their older content rather than having them sit on the digital shelf with no consumption. And as sports expand across the world and becomes more globalized, the localization and translation of content is more imperative, more uh today, more so than ever, so localization and translation of their content. Into the languages that fans understand is mostly important for the expansion of those teams and organizations across the world. And with so much content being generated, whether it's recordings and video on demand, they want to be able to find the content that they are most that they care most about and by having automated highlights and clipping those highlights from these longer forms of video. It creates a short form experience where they can find highlights very, very quickly. Taking a look at digital products such as applications for leagues and sports teams or online sites and subscriptions, we talked about personalization and hyper personalization. So incorporating machine learning methods for hyper personalization within apps to drive greater recommendations is what customers are looking to do as well. And semantic layers such as chatbots and virtual assistants are providing customers and their fans a way to find information even faster today more than ever. So no longer do fans have to spend so much time navigating their interfaces, but they can quickly ask questions for what they're looking for, such as events that they're looking to attend. And looking at how you can create interactive experiences for your fans. Creating or creating ways for fans to contribute content and contribute to the experience that most fans will view on TV is also a great way to drive fan engagement because fans want to be able to contribute either generated designs or contribute contributions whether in the form of contests or other capabilities and uh contests for them to participate in. And we talked a lot about partner solutions and tools. And how they can elevate their entertainment ecosystems with their partners. So looking at sports, we take a look at the broadcasters and how they can prepare for in-game and pre-game analysis with advanced stats and other talking points for them to really highlight throughout the viewing experience for fans. And with the rise of AI agents, they're finding ways to access data, APIs, and other solutions even faster. So how can organizations and customers in media, entertainment, and sports create some of these types of solutions with the AWS AI stack, we see this as a tool kit for customers to use to build their own solutions, and we see this as a way to provide the infrastructure, the compute and manage services that help them create new solutions for their fans to enjoy. So taking a look at the infrastructure AWS provides to compute through different various chips, but we also provide the capability and flexibility to train your own machine learning models through Amazon Sagemaker AI where organizations can build, train and deploy their own AI and ML models. Taking a look at Amazon Bedrock, where customers can have access to foundation models and their own LLMs such as Amazon Nova and leading third party models. And within Amazon Bedrock they have Amazon Bedrock Agent Core which allows customers to have access to the full suite and capabilities to create their own AI agents to put into different solutions and experiences. And for those customers who don't want to spend all the time creating new solutions, there are managed solutions as well, such as Amazon QuickSuite, which helps customers find their information and insights even faster than before and. Automate multiple workflows. So let's take a look at how Some of these AI services on the AWS stack can be used to create an agentic AI solution for fan experiences. We'll walk through just an example where we are creating a agentic solution for creating content and generating content based on games that happen within sports, so users and fans will access these through the apps from their phones or their devices. And using Amazon Bedrock Asian Core. Customers and organizations are able to create these solutions and using multi-agent architectures. Agent core runtime is used to orchestrate and facilitate the different workflows of all these different agents. In this example, we would have one agent performing the data analysis on games, one agent performing an image search within the media library. We, we have another agent generating content and putting it all together. An agent core runtime helps customers deploy and orchestrate these agents with any framework such as trans agents, lang chain, and Lang graph. And provides the instructions and context for these agents to work together. And these agents share the infrastructure and components for memory management, gateway authentication and runtime management as well. And they can be directly invoked or communicate between each other via A to A protocol. And taking a look at how these agents can use other tools and other data sources, we have agent's core gateway. Which is a capability that allows these agents to access the tools such as APIs, whether they be first party or third party data sources. Or your own local data warehouse and storage solutions to create a tool where these agents can actually utilize. So a few key takeaways where we want you and your organizations to start incorporating within your fan experiences. Overall, elevate the fan experiences and organizations can do this by incorporating hyper personalization techniques and with AI powered features to create new experiences for their end consumers. To better understand and interact with your fans and your consumers, you can build robust data profiles with the modernized suite of AWS services to create a new modernized data platform. And you can enhance your traditional data pipelines and platforms using AI and ML methods such as identity resolution and ML methods on top of that. And finally, you can utilize the AWS stack, such as the AWS AI stack and other modernized services to build brand new products, experiences and platforms for viewer fans to enjoy. So overall, we are so excited for what you and your organizations will build that engage and excite your fans and your consumers. And with that, we thank you for being here. We'll open the questions. There's microphones in the back, and we'll also be around in the sports forum today. Thank you. Thanks everyone.