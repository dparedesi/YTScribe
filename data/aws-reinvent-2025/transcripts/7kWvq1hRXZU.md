---
video_id: 7kWvq1hRXZU
video_url: https://www.youtube.com/watch?v=7kWvq1hRXZU
is_generated: False
is_translatable: True
---

Good afternoon everyone. I'm super excited to be here with you today to talk about how you can build a future ready security operations center with AI by transforming your security log into a CSAF format. But before we begin, please welcome my co-presenter, Alan Amanda, Director of Public cloud. Mark Pratima Singh, senior Solution architect for Financial Services and Insurance at AWS, and I am Sushawn, senior technical account manager for healthcare and life sciences at AWS. Alan and Pratima will be joining us later in the stage to talk about Mark's story and a cool agentic AI demo. Thanks, Alan and Pratima. Now before we get into an agenda, let me ask you something interesting. What if I told you that you could reduce your incident response from a full day analysis to just 30 minutes, or maybe even lesser? Please raise your hand if that sounds too good to be true. All right, I can see a couple of hands. Well, let me tell you, Mark did exactly that, and today I'm gonna show you how you can achieve the same outcome at your organization using OCSF. We will first discuss about the common data challenges you face with a different log source, with a different log format, then I'll introduce the Open cybersecurity schema framework, in short, OCSF as a solution. We will hear Mark's real world story about how they have transformed their central logging using OCSF and how they leverage OCSF together with AI to slash their incident response from a full day analysis to just 30 minutes. And finally, you will witness the power of OCSF with a cool agentic AI demo. See firsthand how you can revolutionize your security operations center with OCSF and AI. Now before we begin, let's establish a fundamental truth. Security is fundamentally a data problem. The challenge is not about lack of data, it's making sense of the data you have. Now let's look at modern enterprise security landscape. You're no longer managing just one environment. You have your on-premise data center, you have workload distributed across multiple cloud providers, and then you have a fast ecosystem of a SAS environment. Now let's imagine your security team is trying to investigate a potential breach. But your firewall log sits on on-premise, your endpoint data in a different application, and your cloud data is scattered across multiple cloud providers. To investigate, they need to navigate 3 or 4 different systems which are having a 3 or 4 different lock format. And remember it's all just for one investigation. Your analysts become digital archaeologists, spending hours just trying to piece together what happened. Meanwhile, the real incidents slipped through the cracks because you can't see the full picture. But here is the reality. Your best security engineer, the one who should be spending time building advanced threat detection algorithms, are instead writing endless data parcel. That's because every system speaks a different language, different field name, different structure, different time stamps, hence they're mapping Field A in System X to Field B in System Y over and over again. Every security tool you introduce means another complex data pipeline to be managed, another custom parser to be built. It's like having a Formula One pit crew spending their whole day organizing the tool instead of changing the tires. What if we could change this? Let's have a system that would do all this undifferentiated heavy lifting or doing the data transformation to create one single universal data format so that your security engineer can focus on what matters to them most threat detection and response. The next challenge is the data tsunami, and it's real. Every application, every network devices, every endpoints are generating more logs than ever before. Sounds familiar, isn't it? But here's the kicker, it's not just more data, it's more different data. Hence you're drowning in information, yet starving, starving from inside. Your detection systems struggle to keep up, so as your budget for storage and compute power to process this huge volume of log. So what's the solution of all these data chaos we talked about? Meet OCSF, the Open cybersecurity schema framework. Is open source, take away all the nuances of a proprietary schema, and it's not just a simple schema, it's a framework to render the schema. This isn't generic, it's purpose built for your security team. And irrespective of the source event, will always maintain the same data structure. You can customize it and extend it based on your own needs by keeping the core framework intact. And this is my favorite. OCSF is a source diagnostic as well. So whether your log is coming from a third party vendor or AWS platform level logs, OCSF will always create one single universal data format. And because it's a universal data format, OCSF will always provide same structure, same field name, same query path for your security tool to consume. Now let's see OCSF in action. This is where the magic begins. On your left hand side, you have your event producers, your security tool, your application, your network devices. Each one traditionally speaks their own language, creating the data chaos we talked about. OCSF sits in the middle, acting as a universal data translator, but if you're wondering whether you still need to do the mapping between your event producer on the right to the event consumers, the answer is no. Not for AWS platform level logs. Amazon Security lake does that for you. Remember the system we wished before, the one who should do all this undifferentiated heavy lifting or doing the data transformation to create one single universal data format? Here you go. Amazon Security lake for you. So now, on your right hand side, your even consumers, your SIM, your data lake, your analytical application, your AIML workload, they're all going to receive the data in the same consistent format. Not just that, you pay less for your storage as well, because Amazon Security Lake stores the data in a parquet format, which further compresses your OCSF formatted log data. You pay less for your compute as well. You know why? Because Parque, being a column in a format, inherently provides a better query performance. So your Athena query would run much more faster than ever before. Lesser time to execute a query means a lesser compute power you are using, which translates to a lesser compute cost. Now before I explain you the key benefits, let's try to recap the data challenges we talked about and see if this key benefit can address all those data challenges. First, as I said, OCSF creates one single universal data format that makes you ready to operate in a hybrid cloud and multi-vendor environment, so no more visibility gap that you had with a different log source, with a different log format. It's a one-time mapping at the producer level. That means you set it up once at your producer level, not for every security tool it consumes. No more custom parcel required at your consumer level because you already did it at the producer level. That eliminates the need for you to have that complex data pipeline we talked about. So now you can optimize your security talent in a much more strategic work rather than doing the data transformation. And finally, and most importantly, it's the AIML ready. So a G AI model can work on a clean, consistent data, turning the increasing log volume into a valuable insight. This is how you break down your data silos and supercharge your security operation. Now let's hear from Alan how Mark has transformed their central logging using OCSF and AI. Alan, over to you. Thank you Sushiban for providing the background and introducing the challenges that we can solve with OCSF. Good afternoon everyone. I'm Alan Amanda. I lead the public cloud platform team in Merck. You know, I have been with the team for more than 10 years. It's always been our mission to, you know, provide an environment to our customer by delivering or enabling key technologies, critical technologies to support the business needs. Through this process we were able to support, uh, secure cloud environments to our user which is actually capable of supporting the whole like uh technology life cycle um you know our team believes that uh our team believes that um the path to production should be a happy path it's our goal and mission to ensure that the path should be the easiest. Maybe you're wondering how we are able to, um, you know, uh, do this is through our, um, is through our. It's sort of giving, uh, it was a self service offering providing access to the data in a central logging environment and also guide providing clear guidance to our customer. Now let me tell you about our company Merck and Co. uh Inc. uh, also known as MD MSD out from outside, uh, uh, United States and Canada. We are a global healthcare company that provides innovative health, um, solution through our, uh, medicines, vaccines, biologic therapies. And health care, uh, and, and animal health products, our heritage speaks to our commitment. For more than 130 years we are able to, um, you know, provide, um, humanities, uh, uh, you know, provide hope to humanities by, um, development of important medicines and, and vaccines. This is not just our history, it's our ongoing mission. Um, you know, Sushi Bun gave us a very nice background about OCSF. Jose, I said, but uh, you know, I hope that from this, from my presentation that you will gather, uh, enough information for you to evaluate and use, um, you know, Ose for your own um uh purposes, uh, you know, and, and also. You know, with this diagram, I, I want to explain how we're able to, you know, do our transformment journey to OCSF. By the way, um, is this diagram very familiar to you? Um, for the architects and, uh, engineers in this room, you know, if you have deployed and designed an AWS, um, you know, multi-account strategy, this diagram should be very familiar, uh. Every landing zone, every successful landing zone that is deployed, um, in my, uh, experience would benefit from having a central lagging environment, you know, a central lagging environment is a set that that helps you collect the logs from multiple accounts into in multiple regions into a central, uh, accounts which, uh, in a central account which help you for the purpose of, uh, monitoring, um, analysis and, um, you know, auditing. Uh, no, uh, let's see how it works from our ADBS manage account. Uh, we have a cloud trail that is set up to send the logs into our central logging account in the estuary bucket. From our our application accounts, uh, we have the ABS swap logs and flow logs also configured to send the logs into our central logging account and our customer. would then be able to access this data so that they can generate their own analysis and insight. Well, you know, this setup actually performs very well and it meets our key compliance requirements. However, we also do recognize that there's actually room for improvements in simplifying this process. You know, with the The opportunities opportunities like this to simplify the process is basically uh we wanted to reduce um cost reducing time to spend in operation um overhead so that we can um you know shift our focus more on strategic platform initiative. So how are we able to achieve this? Well, there are 4 key factors from the business perspective. First, we look at opportunities to accelerate our rapid incident detection. Uh, rapid incident detection, um, use, uh, rapid incident detection improving our response, um, to activities that are related to security and, um, and operation. Uh, would help us react much faster and protect our critical infrastructure, um, you know, quickly. We also wanted to improve our, you know, customer experience by enabling AIML. Um, capability, you know, uh, modernizing uh and building our, um, building our infrastructure, uh, to be ready for AI and ML um integration. Uh, help us, um, position us to, uh, leverage advanced analytics to derive business value, uh, quickly. Now from the IT perspective, um, we wanted to optimize the cost of running our central logging environment, so we look at errors in, uh, reducing our storage costs while also streamlining our, you know, our processes. From the operational efficiency, uh, we wanted to ship more, uh, in using managed, uh, services, uh, you know, as you have seen in the previous diagram, um, we for us to to continue using our central logging environment we have to, um, set up and use many AWS services. Uh, this actually creates a lot of, uh, operational overs for my team. We wanted, and we wanted to shift that overhead back to, to, uh, AWS. Also, you might have experienced that as you increase, um, your AWS, uh, usage, uh, the, the amount of data that you are generating also, uh. Increases well, the increase in data create a lot of um inefficiency in using the data for query and um and and and processing and another factor that is affecting this is that um all this data are actually in different lag formats. In different lab formats, OK, um, so how did we, you know, able to achieve our transformation. So back in, uh, Q3 of 2024 we done a comprehensive assessment of our capability, uh, capability looking at, um, looking at the, uh, capability and value and, and, and its value. Looking at capability and its value of the central logging environment, uh, logging environment, and then, um, uh, to do this we work closely with our AWS partners, uh, by asking and giving, uh, giving us, you know, strategic, um, you know, input and, um, key best practices, uh, back in, uh, Q1 of 2025 we started, um. Back in 2025, uh, we started our, um, you know, planning, uh, our team actually lead the deployment planning and um. And together we were able to uh create a a a a a plan that you know a plan that we can execute that have uh optimal central logging capability in uh Kyoto of 2025 implementation actually um you know, started we selected and implemented AWS security lake and immediately gained uh the benefit of it having, um, transforming our data. Um, automatically into, uh, OCSF. We also are able to do some measurement in real world performance against expectation. Now, in Cuper of 2024. Um, we wanted to, you know, shift our focus in finding more opportunities to, um, leverage our modernized central logging environment with the OSF, um, normalized data. Uh, we, we, we, we look for use cases that can unlock AI capability, um, so that we can, um, use it for, uh, other purposes within, uh, within the team. So this is the modernized central logging environment with OCSF that we have built. As you can see, the application account still generates a WS cloud trail, BPC flow logs, and WAP plugs, but I think the key difference now is that we are using security lake and, uh, you know, and. And actually automatically transform our data into OCSF, um, format now, uh, and, and now from our central logging environment, um, our users able to, um, access the data, uh, and use it to still uh to to generate insight and, uh, and, and, and insight and run their analytics. Um, you know, the, the shift to manage service approach, uh, gave us an added benefits of not only converting a login into OSI but also created significant infrastructure, uh, cost saving, and, uh, um, uh, uh, operational efficiency. Moreover, we now have a modernized central logging account with an AI ready, uh, data. So how did we achieve the operational efficiency gain? Um, with all these changes, uh, we have seen a 48% reduction in time that we are spending in managing our central logging environment. Um, nearly half of the operational cost has been reduced. Now imagine what this means to our, to our team, right? Um, now our team can now focus into focus more into strategic, um, you know, in strategic initiative. So how did we achieve the 40% efficiency? Well, there are actually three driving factors. First, uh, rapid enablement with us shifting our focus from managed service, uh, from managed service and, um, doing more evaluation and testing of built-in features and reducing our time in creating new features, we are able to, um. We are able to uh remove the time spent in creating enabling new features, improving our central lagging solution um or in central lagging solution was reduced from uh weeks to uh to just hour. Now uh there's also um reduced operation overhead, you know, um, with the heavy lifting of the, the, the lug transformation is done by, um, security lake. We're actually able to eliminate uh custom code that is used for log um processing also the access to the data for our key stakeholders and now uh we are not able to do that from security like. Uh, lastly, ready to use, um, you know, with the, with the data getting converted automatically to a security like the access to data, the access data is self-service, immediately our users is able to access data to generate analysis and run insight and, and create insights. Now, uh, with, uh, you know, the infrastructure, uh, the, the benefits actually extended beyond operational efficiency to significant cost saving. The infrastructure cost that we have generated about 47%. The, uh, normalization of, of CSF within, uh, security lake, um, actually, um. You know, reduce, you know, reduce the infrastruction cost by improving, uh, data storage, processing, and, uh, analysis with the data coming from the application team automatically being generated to OCSF, uh, installed in parque format. It actually provides several cost saving benefits. Now what are the driving factors in our in in general the 47% um cost savings? First, there's managed service with us uh moving to ADB security like we actually, uh, benefited from economies, uh, aba economies of scale, um, uh, with this we don't need to actually build and maintain custom infrastructure and, uh, we actually was able to, um, decommission our central logging environment and reduce our total cost of, uh, ownership. And uh OCF um normalization, well, the standard schema eliminates standard fields um before storage. The standard uh before storage, um, the OSCF efficient structure, uh, well within Security lake, uh, it was able to store the data whilst eliminating the need to store it multiple times in different lag formats, uh, basically reducing our storage footprint. Now for the compressed storage. Um, you know, with the scale of how we operate and, you know, the data that we are generating with our OCSF normalized data being, uh, stored in parque format, you know, we actually create a lot of storage, um, storage costs because of the exceptional compression that actually provides. And lastly, reduce compute with the, with the, with the, with the data, you know, being partition in columnar data, our, um, Attinaque is run, uh, more efficiently. You know, with these 4, key factors manage service, um, OCSM normalization, compressed storage, um, uh, reduce, um, you know, uh, compute through uh PC of our, our query, uh, we're able to generate that 47% savings which we can actually use to invest in other, uh, innovative, um, uh, solutions. Now, uh, let me shift the focus from us generating infrastructure costs to, uh, and, and, um, generating, uh, operational insights to a more, I would say interesting activity that we have done recently. So after we deployed security lake, we and access our data, we're able to, um, and we are able to work again with our, um, AS expert to, um, do, uh, explore a POC that actually use our logs for security operations. You know, our goal was really to test if AI can help us, um, you know, investigate security incident more, uh, more efficiently. So with Agenix sub AI, uh, prototype, uh, we, what we have to do is only give it the, you know, um, activity, uh, the, the WAP, uh, record ID. Uh, what it does, it was able to generate a comprehensive report or insight, uh, in less than 5 minutes. It actually analyzes the traffic and, um, and requests, uh, pattern, uh, look at the sources including the, you know, providing the IP addresses, look at, uh, behavioral, um, you know, uh, look at the behavioral, um. Conditions and then uh created a very uh nice report that uh will provide action actionable recommendation, um, actionable recommendation. Uh you know, the The AI findings. The findings for um you know that we have generated in here uh was only possible because of the OCSF um uh with the standardized scheme um you know the AI was able to correlate um security context, um, threat intelligence, and infrastructural context, um, you know, quickly generating this report in under another 5 minutes with the security from the security perspective, um, operation perspective. Uh, this represent the, uh, fundamental, um, enhancement, uh, AI driven threat analysis, uh, that can empower teams to investigate quickly, you know, uh, faster and effectively is really better than any manual, um, uh, way of doing things. Now, uh, let's, let's do, let's look, uh, deeper into this process. So in the past, um. You know, uh, investigation, uh, is really very manual, so my team would usually, um, derive concept from historical record. We would need to learn and translate different, uh, log formats, develop complex query, and using that complex query against, um, you know, and partition raw data usually, you know, results into a long execution time. And then when the output, you know, uh, was when the output is given. Uh, we have to translate it into a more meaningful, um, yeah, it's a meaningful, um, you know, information, uh, but with, uh, agentic, um, AI, uh, with agenticA AI prototype, you know, the orchestrator was able to coordinate with, um, you know, child agents basically performing the same manual set that we are doing. But now it can automatically, you know, uh, get, uh, the, the context from, you know, historical records. It's able to quickly analyze slugs because they are in OCSF format, develop, um, optimized query and run it against OCSF normalized data which is also stored in parque format, resulting in a, you know, very nice human readable, uh, insight. So I've shown you how, you know, we can generate um uh you know, security insights, but now we're using AA, but now we can also do that with, you know, um uh operational uh operational insight. Um, you know, right after production, you know, one of our customers actually reached out to us and, you know, reported a spike in their, um, cost usage. Our engineer was able to use AWS MCP servers to, to query our data. But remember with OCSF having uh the standardized schema that applies across several AWSS services, um. We are able to generate that information very, very, very, very, very quickly. I think without OCSF universal format, uh, we would have spent, you know, hours in stitching together all these slugs and finding that, um, you know, specific, uh, uh, relationship and to determine the root cause. By the way, the root cause for this one, in less than 5 minutes RA was able to determine that it was due to, um, you know, a spike in, uh, KMS decrypt data events because the EU data events was actually enabled. So with all these changes that we have done, um, you know, with modernizing our central logging environment and, you know, um, normalizing our data into OCSF, we were able to generate infrastructure savings, operational efficiency, and also enable us to, to explore more AI AI capabilities. Um, this actually generated a lot of interest, uh, for my team as our next step we're actually going to put more, uh, time to, uh, find more opportunities to do to, to, to, you know, increase, um, you know, uh. Our efficiency, uh, our efficiency, you know, uh, reduced cost and explore more AI capability now, um, for us to learn what you know an OSEP can do when it meets, uh, AI, uh, advanced AI, I'll pass it over to Pratima Pratima. Thanks Alan. So you heard from Alan how a single standardized framework has been pivotal in accelerating incident response, essentially reducing the time to detect and respond. Um, when we look at OCSF and when we look at an agentic AI solution, what is most important is the context window of your agent. Now when you're dealing with different types of log events, when you're dealing with different. In diverse schemas when you're dealing with different attributes that need to be correlated, the AI agent's context can grow and shrink depending on the information that it has to process, and that can result in differentiating responses depending on how much information it is going to produce. When we look at an unambiguous and standardized framework like OCSF, it makes it easier to just load up that open source code onto the agent as a knowledge base and be able to guide the agent to learn from it. So OCSF provides that distinct value in the age of agentic AI. Now how many of us have heard about OCSF before coming to this session? You know of OCSF. There's a few hands. I'm gonna give you a very small, like a quick start into OCSF, and if you want to ask more questions, we can, you know, talk more, we can do it after the session, so. At the core of OCSF is an attribute. An attribute is a key value pair. It has a key that identifies the information that the log event is presenting and a value that captures the information that the log event is sending through. Now a collection of related attributes forms an object. Think of an object as what you would do if you were programmers. It's the atomic unit that defines, you know, a collection of attributes that define a thing. Now a collection of objects becomes a class, a class represents a single log event. Now you could have multiple different types of sources, emitting different types of log events. A single source could emit. Various types of log events that match up to different classes. For example, if you look at AWS Cloud Trail, it emits authentication activity, it emits API activity, and it also emits audit activity as a different class. But if you look at, say, SSH activity coming through an EC2 instance when you're SSHing into it, it's a single standard log. It is SSH activity that falls under a category called network activity. Now a collection of multiple classes is what we call a category, and they're all related, so you would never see an authentication activity falling under a network activity category. It'll be SSH, TCP activity, HTTP activity, FTP activity, so everything related to what network resources or network sources could be generating. is designed with the goal to be an open standard that is adaptable to any environment, any solution, or any application, while still complementing the existing security controls at hand, so you're not diminishing any value that you already have in your environment. Now there are 2 sources of logs there. With a show of hand, tell me if you're able to identify where each one is coming from. There's 1. There's 1 half a hand there. Well, if you look at it, these are sources of logs coming from WAF and VPC flow log. Now imagine you are positioned in a scenario where you have to find an indicator of compromise, for example, an IP address that is traversing your network stack. So coming through layer 7 all the way to layer 4. The first thing that you have to do is make sense of that one standard outline that the VPC flow log is, is emitting. So you would make adjacent parsable, you would add attributes to capture the values coming through the VPC flow log, and then you would have the capability to correlate them and generate some sort of business value or what you're trying to do. Now that is the kind of data wrangling that you have to do today with raw log sources, kind of reminds us of what Shown was talking about earlier when it came to different data types and data volumes. But now if I translate this all and map it to an OCSF class, what immediately grabs our attention is how consistent the log schema is. You will find a source IP address in exactly the same location in a VAF activity log and in a VPC flow log. All you need to remember is which table that information is collected in if you have a data lake where all the log sources are going. So you can see the source endpoint IP is exactly where the IP address will be, and what's more important is OCSF is quite opinionated in data types. So an IP address will be of the data type IP address. An AWS account ID will be of type string, not a big in in some log source and not a string in another log source, so you have to do data type transformations. Another value add is OCSF is customizable, so you can use the construct of observables to surface certain common values that you use more often than not to correlate and grab information out of. Now that means that you become completely immune to the log sources under the hood or the or the entity that's emitting the logs. All you care about is what activity are you trying to track or what values are you trying to correlate. Was that was that primer good enough because we are now going to pivot into a typical triage scenario. So how many security operators here? People get paged. There's a security event. Oh my God, I have to wake up at 3 a.m. Right, so what do you do the first time you get an alert? What do you do? The first thing you do is you try to figure out what am I supposed to do with this alert. You have run books, playbooks, or nothing, and you try to figure your way out. The next thing you do is you establish why you should care about that alert. Is it worth waking up at 3 a.m. at night? Maybe it's the production environment and I have to, but if it's a sandbox environment and some devs decided to test after, you know, a big game night. Then you probably don't need to respond just yet because you know it's a quarantined environment it's not going to allow for lateral movement so you can pass it on so that's the business context that drives the rate of uh or the speed of response that it needs. Once you've established the business context, you want to understand who did what, where, when and why, and that's the meat of everything the log analysis of what is happening in this environment, what am I supposed to do once I've established what's happening in this environment and then finally, has this happened before? Because you know it could have, we have historical evidences that we store through our ticketing systems, through our backup systems, and we want to establish whether this thing, if it was to happen again, what are the steps that we need to take. Now when we put all of that into an agentic AI scenario, we can split all of these up into different agents. gives you a customizable, unambiguous, and consistent way of operating operating on these logs and enhancing that log analytics workflow. Now we'll break it up because there's a demo coming up after this of what this particular flow will explain, so we'll break it up. Say we. Say we say the orchestrator is the security operator, right? The first thing that hits the orchestrator is the alert. It needs to figure out what am I supposed to do in this case? Where is my run book? Where is my playbook? Imagine an agent that sprawls out into various sources of, you know, data in your environment and figures out where the run book is maybe something relevant to an application that that's in the picture, maybe something relevant to a resource that's in the picture. Once it has the run book, it wants to establish what is the business context for this particular alert, the resources in these alerts, why are they important or what is important around them. They could have permissions to confidential information, and I need to trim it right away, or they could be a sandbox environment, have dummy data, and maybe you know this can wait for later. So the orchestrator now triggers agents to retrieve business context. Once it has the business context, it's going to look at that contextual information and trigger log analysis. Now log analytics is one of the most human intensive tasks in an incident response scenario. When you think of it, the amount of time you take to analyze logs can either add or reduce time to respond, and such kind of human intensive, very repeatable manual tasks are prime candidates to be handed over to an agent so that US security operators have time to innovate, build better agents, think of better playbooks that are more interpretable by your orchestrator. Once you have the log information, It can interpret it back with or correlate it back with the business context that it received. So imagine you as humans having to go through lines of logs and then trying to figure out this context I saw somewhere or trying to figure out where did this account come from, where does this IP address come from, and doing all of that while you're pushed to respond faster and faster. And of course save the investigation history. Try to figure out how we can make this information readily available the next time something like this happens. Seems simple enough. This is kind of what you would do in a security operation scenario. Now when we put this into perspective with Merck, we built a proof of concept with them to dive deeper into AWS VAF logs. So as a, as a demo, what I did was I deployed a demo application and I loaded it up with a distributed load test. Um, and it had about 10 million requests that were that were blocked by the WAF, but think of 10 million requests if you had to look through WAF logs and you had to look at each of them, whether they're blocked or whether they're allowed, and you had to put filters into place, put Athena queries or whatnot, the amount of time that would take, and when you're operating on raw logs. So here I have one orchestrator. I do not have any investigations right now, so I don't have any investigation history. I have the one orchestrator. It has 3 agents. Those 3 agents are basically available as tools the log analytics agent, the business metadata agent, and the. And the historical evidence agent. Now, the first thing I do is I tell it, hey, AWS WAF has detected some activity that's happening, suspicious activity in a said AWS account. I give it no further context. I don't know which, I don't know what this account means in the bigger scenario of my AWS accounts, and I also don't know what activity the WAF has detected. I just know something's happening. So the first thing the orchestrator does is it, it triggers the playbook agent because it needs to work out what am I supposed to do. It finds out there is an account ID in play. I'm looking at VAF and I'm trying to do something, so it's then subsequently going to trigger two more agents. It's going to say, I need to find the business context of that account ID because that's all the resource I have through this alert. Is it important? Is it not important? And I also need to find out what happened, where, when, and why. Now slowly and surely it does look up the back end. In my case, I've just loaded up values in a Dynamo DB table. In your case, this could be an MCP of your CMDB. You could connect it back to your agent. Now it completes its analysis and it tells me I know what's happening here. This is the account that you have asked me to look up. It gives me all of the information of the AWS organization. It tells me the business criticality. It tells me the data classification. It tells me who I should be talking to if something happened in this account, and this is all coming from information that you today have. And then it looks up everything that happened in our VAF. It tells me I stopped 10 I've, I see 10 million lines of code, uh, sorry, log, and I see that they were all blocked. There is suspicious activity happening. This is a DDoS kind of event. It has been rate limited and it presents recommended actions. Now mind you, I haven't had to train my agents to do any of that. I have just put a prompt in that says look up the VA log activity and tell me what I can do here and look it up for like 3 hours, right, because you don't want to overload the context window as well. And it summarizes it into a small investigation history that then goes back to my data store as investigation history because the next time I trigger this agent and I give it the same account ID it doesn't even need to look at the playbook um agent because it knows through historical evidence that this account ID is is meant to have the same metadata that I looked up in the last event. And we did something interesting to capture the amount, the cost for that particular investigation and how many agents are triggered and how many tokens are exchanged. Now you can see this all happened in a matter of a couple of minutes. Imagine yourselves on the other side of the computer trying to do this. You would take, if not minutes, at least a couple of hours to figure out what's happening and draft out that investigation history. Now all of this is powered by OCSF under the hood. My agents were able to interpret OCSF straight out of the box. They did not hallucinate on the different types of log schemas coming through because they knew exactly which log schema they were working with, or the framework rather, and they understood that it was coming from the log, the WAF log, and they understood what it means when we talk about WAF log activity. So it was as simple, it was pretty simple to build this proof of concept. The OCSF community, having realized these values of versatility, adaptability, has been growing since Black Hat 2022 when it was first introduced. We have about 1,100+ partners who are supporting it, 200+, um, organizations that are using it. Um, we, if you scan the QR code on the top, that's the open source Slack channel for OCSF. If you have any questions about OCSF, ask the contributors right there and then. You can talk to them. They have calls that you can ask to get added if you want to listen in. The second QR code is for Amazon's OCS ready specialization partners, which we recently released, which means these are partners who are who have established their technical ability to consume OCSF logs that are produced by AWS services and build solutions on top of it. So if you're restricted with the number of people in your business who would be able to do this for you, you can leverage these partners. Now, as with everything in technology, uh, and a lot more um accelerated with gentic AI solutions, everything changes all the time. When we built this solution, Bedrock Agents was our back end. Now it's all Agent Core. So think about how you can innovate by using Agent Core that gives you, um, a completely secure environment to be able to build gentic solutions. It's completely managed and it uses constructs like identity and gateway to be able to build isolation between the agents as well. Uh, think of how you can add additional agents. What would benefit your business? What are the bespoke items in the incident response life cycle that you would need agents for? And they, they could be different to what I've shown you. This is pretty much a generalized use case. Think of MCP integration now. There, there is, there is a reason why you may not have everything stored in your AWS accounts. You could be using third party accounts or third party services. You could be using, you know, certain identity services, certain response services. You can use MCP to integrate all of those solutions into this particular, uh, environment so that it, it does feed off of that information. And if you use AWS, Amazon's OCSF ready partners, you're able to then consume OCSF out of the box. And when we built this, Amazon Quick Suite wasn't a thing, um, so Amazon Quick Suite gives you a completely managed, uh, interface to interact with your agenttic AI solution, so you don't have to build a front end anymore. You can just hook it in through your MCP integrations with, with, uh, Amazon Quick Suite. So the key takeaways from this session is keep learning more about OCSFs, see how OCSF can give you value with your log analytics use case and your security operations use case. Learn more about agentic solutions. There's an agentic AI learning resource that I've linked there. And we've recently, or in fact yesterday, so we released the Amazon uh AWS MCP server, which is a managed interface into the AWS API MCP and the AWS knowledge bases which can then help you build agents that can directly remediate resources within your environment so you can use that to remediate lower order environments automatically and then put like a human supervision or an approval workflow to remediate production grade environments as well. I hope you learned something new today. Thank you so much for attending our session, and please leave us feedback on the session survey in the app. Thank you.