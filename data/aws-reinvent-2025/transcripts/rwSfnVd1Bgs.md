---
video_id: rwSfnVd1Bgs
video_url: https://www.youtube.com/watch?v=rwSfnVd1Bgs
is_generated: False
is_translatable: True
---

So reinvent day 4 last session before replay. Appreciate you all coming out to the northernmost, uh, you know, uh, area or place for reinvent. So, uh, making the trek up here. I'm actually down in the MGM. I was at Mandalay Bay, so it's, it's fun traversing all these different locations. So definitely appreciate y'all being here. Uh, so how many folks here. Data architects, uh, data scientists, DTL programmers, all right, good, a big majority of you, that's great, uh, we're gonna cover some things in the session that's gonna help you out, hopefully make things a little bit easier. So in those types of roles you're going to worry about data pipelines. Uh, how do I get my analytics on my operational data. Uh, how do I bring together maybe multiple transactional systems into one analytical system and then normalize all that data together? So is it working OK? Yep, good. All right, uh, and then the last thing is, aren't you tired of those late night, you know, pages escalations and things like that. So if that's the case, good news is we're gonna take a deep dive into AWS's Zero ETL integrations to try and help make your life easier, simpler, better, so you can go focus on all those cool Gen AI things that we've been talking about all week long and those agents. My name's Dave Gardner. I'm a database special solution architect, been with AWS for about 9 years out of Charlotte, North Carolina, and we're walking you through the session today. So first we're gonna cover the business or use case for bringing that operational data into our analytical systems, kind of why we're going to do that. We're gonna cover the key features of ero ETL. Then we're gonna go walk through the steps of creating AWS Zero ETL for Dynamo DB and Aurora, which, uh, a quick show of hands, folks using Aurora as their source system, that's pretty much everybody, that's good. And then Dynamo DB. So good morning. OK, it's great. I'll also cover there's lots of permutations on the zero ETL. We continue to expand it, so we'll also talk about Oracle RDS, uh, and then also last week we announced some new things we'll cover that as well. So it's pretty broad, but we'll go deep on a couple of them which seem to be the most popular, uh, in the audience. That's, that's good. Last we'll cover management monitoring. On that, so why do we bring our transactional data over to analytic systems? It's to enable key insights and drive business value. So a couple of examples customer relationship management, fraud detection if you're in financial services, gamer leading boards if you're a gaming company, inventory optimization, sentiment analysis, as well as product insights and sales, and then there's many more use cases, right? So we have a lot of analytical needs we wanna do on that transactional data and mine all that gold out of there, right? So let's, since this is a 400 level session, let's take an example and let's dive deep on it. So our transactional data is in our Aurora MySQL database. We've got reviews data from customers that have purchased products from us. We're gonna use some Gen AI functions to determine sentiment of those reviews of those products, and then we're gonna use Gen AI again to generate a prompt, uh, based on the negative, neutral, or positive feedback of those particular. Uh, sentiment analysis and rewards that we've got. So let's walk through what that looks like. Sorry for the small text. Uh, I tried to squeeze it all in one at the very top, uh, up here you've got the integration, uh, between Aurora Zero ETL and our red shift cluster. Then so that brings us over our transactional data. Then we're going to, uh, bring in our products. Our reviews were already on there. So we're gonna, then we're gonna join these two together and build a table that has the reviews and the transactions together based on customer ID. Now we're gonna build an external model to call some cloud and bring in our LM to determine what the uh sentiment is of those particular reviews. And then we're gonna basically build that out to a table. Then the fun part is we're gonna proactively take that sentiment and we're gonna use our LLM or our agent to go build a response based on the customer's feedback. So if the customers had a great positive review, we may tell them, hey, this is the next best product that you should go, go with what you purchased. If it was a neutral, well, maybe we'll, we'll give them like, hey, you know, um, a different kind of response. But if it's negative, maybe we give them like a free shipping or a discount coupon off their next purchase to try and earn trust back for that customer or make things right that were there. So this really just gives you an example of why getting that transactional data over to our analog systems is important, and zero ETL is one of those ways that's gonna make it easier for us to be able to do that. Now we're gonna dive deep into the architecture of both the transactional side and on the analytical side and so the architecture that does this is called CR, which is command query response segregation and so the other analogy I like to use for this right horse for the right course, right? So for our transactional systems. Think about the last week, right? So last week we had a major US holiday. There was lots of travel, a lot of people going to see loved ones, and then there was Black Friday and then Cyber Monday. So from a retailer perspective there were lots of transactions going on, uh, in the background for those. These transactional systems need that high availability, the performance, speed, and availability to support those peak workloads when they happen. They can't be bothered with analytical functions, right? So, uh, we have Dynamo DB, DocumentDB, and our RDS relational databases in Aurora as those, you know, data persistence layer in that architecture, and it may be, you know, this may be your classic three-tier web app. This is a serviceless example, but you kind of get the operational side. We've gotta run the business. Don't mess with it, right? It's kind of the thing now. We need to extract that transactional data out of those systems so that we can go mine that for gold and run those analytics that we want to on top of that data. So for dynamo and document we'll turn on streams. We'll have a lambda function that ingests that, uh, extracts that data out, ingest it for your relational databases. We'll use replication or maybe it's DMS or maybe it's your favorite ETL tool to pull that data out of those relational systems. Once we have it, we're gonna load it into our analytical systems, and I got one more build out. So before you take pictures, you might wanna hold on to that. So, so we got S3 Data lake, Redshift, OpenSearch, and then maybe we just have another, uh, database we use for reporting purposes. Now here's the cool stuff, right? So if you wanna take your pictures now it would be good. That's drawing out the full architecture, right? Now we have the data over analytic systems we can really start to mine that data. We can do data enrichment processes, data value added processing, sequel analytics. We can plug in those LLMs, all those agents, do data discovery, and do all those cool things we just talked about that really impact the business and earn trust with our customers and and help drive us forward. And again on the operational side, you know, it needs reliability. It needs to have independent scaling and drive drive the business. So this is the overall architecture of bringing those transactions and those analytical systems together now. This part in the middle is the fun part, right? And this is the part that, you know, it gets complex, and we'll go into some of the reasons why it's complex. It's also fragile, right, or fragile, right? Uh, and now with AWS 0 ETL in a lot of cases that's undifferentiated heavy lifting that you can offload to a managed service, right? So let's get into how that managed service is gonna help you. So AWS Zero ETL integrations we wanna make it simple to set up those pipelines of data for you. We also wanna make it simple to manage those. Enable those powerful analytics that we talked about for the sentiment analysis and every other cool thing that you wanna do with Quiro and your agentic AI agents out there, right? And so the zero ETL integrations are gonna enable that, make it easier for us to do so you can go focus on those other value added processes and things. So again, simple, secure, easy way to enable analytics on your petabytes of transactional data. So let's get into a little more detail how the Zero ETL works and operates and things that you'll need to do for it. Now we talked about the complexity, right? So let's take a look at our Dynamo DB table, which is the first source system that we're gonna look at. It's a NoSQL database key value pair. It has a partition key and a sort key, and you have multiple attributes or varying attributes associated with that NoSQL database. Here's an example. So here's my product description. So the product description has a product ID. It's got some different attributes on it. And then I've got this nested JSON description field that I have baked in there. Mm, getting that over to our, you know, redshift our columner database, and I take some mapping, right? So in your own ETL pipelines you have to do that yourself. We're gonna show you how zero ETL will help you with some of that. The other challenge with Dynamo DB is single table design, right? So single table design basically means I have one table, but I'm storing it's an item collection, right? So I'm collecting data that has similar characteristics or key but it may not be the same row. So now when I now in this case I have invoice and billing data in the same table. Now I'm gonna have to ETL and extract that out and and do all that logic, uh, to get it out. So let's talk about our targets. So Redshift, it's a columner MPP database. It's got, uh, it should be really columns there, uh, but it has distribution keys. It's got sort keys, and it got a super data type. And so now I've got map this no SQL database over to this columner database. That's one of the things we're gonna help with. Now the Lake House S3 target from Dynamo similar except it just doesn't have the the the super data type. For even more fun, right? Open search is also a, a target system that we can do from Dynamo DB. Now we're talking documents, index IDs, and search criteria, right? So it's a completely different data layout. This is part of that complexity in managing your data pipelines and figuring all this mapping out. This is where Zero ETF is gonna help you with that. So Dynamo DB as a source for Azure ETL. There are some upfront things that we have to do before we can start the process of walking through building that integration pipeline. First, it needs point in time recovery, so you turn that on. The other thing that you need to do is it needs to have a table policy up front that'll enable red shifting glue to go scan our table and our export table. All right, so what does that policy look like? And there's examples of this in the documentation, uh, as well, but just kind of walk through. Hey, we tell, uh, the table resource policy glue and red shift can get to this table. What can it do? Well, I can do the export table point in time, describe table, describe export, which table can it access? We then describe that, right? And so this basically is the policy that you set up, and then the rest of it, zero ETL will kind of map it out for you. So just wanted to cover that up front. So this is the glue console again it starts a little uh small but uh on the the screen shot. But you see here we've got multiple source types for this particular use case we're gonna focus on Dynamo DB. Now you also have Salesforce and other things so you can see that 0 ETL is a big thing for us. We continue to invest based on customer feedback like y'all we'll add things in here that we need to. Alright, so for this particular walkthrough we're gonna pick Dynamo DB as our source and in this case it's Redshift and Lake House or S3 is the target for this particular, uh, walkthrough. So we define our dynamo DB table. We one of the cool things that's here is that cross account is out of the box, right? So maybe I have my transactional operations in one AWS account and my analytics are on another account. This will do that for you, right? So, uh, the other thing I'll define whether it's gonna go to Redshift or it's going to a data lake, and then what's the actual catalog here and then what's the target database. And then what's the policy, and you'll see this little fix it for me here. This was done before AI agents, you know, early kind of came about, but it's a similar concept, right? It says, hey, you don't have the right resource policy out there. Would you mind if I go out and build it for you? And so that's a fix it for me feature that's there. So what does that look like, right? So it says, hey, your data catalog resource policy and S3 location need to have an IM policy that allows zero ETL to work. And here's what the policy it is. You say, Hey, that looks great, go do it for me, right? Now on the output settings this is where we get into the mapping that we just talked through the different data types and and data layouts. So you'll see here it's got a couple of options. I can either unnest just the top level fields. I can unnest all the fields, or don't unnest anything. And then from a partitioning perspective you can either use the partition keys that are native to Dynamo DB or I can use custom keys and then you also configure the table name that you can name it here. All right, so the data mapping partitioning options. Now this does vary based on target, right? So if I've got red shift, we'll see what it looks like and if I have the data lake as a target, it's an iceberg table, right? So they're gonna operate slightly differently. And here's what that looks like. So here's my products table again. Here's what the definition looks like over in Redshift when I do the zero ETL to Redshift. And so you'll see I've got my key is my distribution key in redshift. So it's gonna distribute on the same keys. So that'll give us good, you know, spread normal distribution, assuming our Dynamo DB partition key is good. And then we're gonna take the rest of the values and we're gonna put them in the, the super data type and value, so. And Redshift does have, you know, functions in it where you can, you know, pull data out of that super data type and be able to use it, uh, method. Now if you wanted to split those out into separate columns, you could certainly do a value added process and and do that as well. Alright, Lake House or S3 as my target. So, uh, I copied an excerpt of the iceberg table. The iceberg table definition is like this big, it wouldn't fit on the screen, but I did pull the description field out here just so you can get an example of that. And then here's the Athena query of what that iceberg table looks like. So you can see I've got the product ID, product name, the different attributes, and then the description field in the JSON format here. So similar capabilities, right? The good news is that 0 ETL did all this for me, set up the tables. I can create it in just a couple of minutes. All right, so last couple of steps for setting up the 0 ETL, we're gonna configure the security integration. You can have a different KMS key for your pipeline, uh, so you can have your customer encryption key or use the service key. You can also, uh, there's a one time move so maybe I just wanna take a one time snapshot of my Dynamo DB table or I can do ongoing replication. So as my products change and things happen that'll percolate over from, you know, Dynamo DB over to Redshift or the lake house. And then also just give it a name so that I can describe it. And so it's pretty much the Dynamo DB to red shift or the lake house so. Yep, so once you activate this, create it, it takes, you know, probably 5-10 minutes under the covers we're doing all the setting up the glue and all that other pipeline steps for it. It'll go from creating to active and then once that's done, it'll start, uh, every 15 minutes, you know, basically do a pull of that data and then send it over to Redshift if you do the ongoing replication. Just kind of under the covers what it uses here so it does leverage the Dynamo DBS3 export as well as the point in time recovery, right? So your data freshness, uh, on your target system or analytic system is about 15 minutes, right? So if something happens in Dynamo, about 15 minutes or so it'll show up in the data lake and that typically works well for, you know, red shift analytical type query. You don't need that real-time response. Now the next target we'll talk about you do want to have it, uh, much faster, and we'll, we'll talk about why. So Dynamo DB is the source. Now we're gonna use OpenSearch as the target. Now, this is gonna use Dynamo DB streams. And so you wanna have new and old images turned on your Dynamo DB streams and of course another IM policy to allow the process to access your table. The reason that you want the the use case here for using streams and getting that data updated in seconds, I cover travel and hospitality industry. So let's say that my either property management or my car fleet management is in Dynamo DB, so I know what is under construction, what's available, etc. but I wanna enable my customers to be able to search for I want a red Tesla in Vegas the first week of December. That's not a very easy thing to do in Dynamo DB when those queries and access patterns change all the time, but open search can really rock that, right? And so that's really where, but I also wanna make sure that that gets updated in seconds so that once the car comes off maintenance and becomes available, now it's available for that customer to, uh, to be able to rent, right? So that's why the data of freshness in seconds kind of matters on the open search side versus on the other two, the 15 minutes should cover most use cases. All right, so now we have our blueprints in the open search and we're basically gonna pick a blank, uh, pipeline here and then we're gonna have a lot of JSON mapping on the next slide. All right. So that 4:15 was a workshop that was on Monday. This is basically a screenshot of that exact workshop. So if when you get back to your, you know, back home and you wanna have potentially this as one of your, uh, use cases. You can set up an immersion day with your AWS account team. We can do, uh, walk through this, and you get some hands-on experience with setting this up, uh, and so definitely contact your AWS account team to, to do that. Alright. So let's kind of walk through the JSON here without it being too, and again I apologize for the small print, at least the screen's big. So first we're gonna, uh, the mapping. So we have our Dynamo DB table. We're gonna define that. We're gonna say what's the, you know, new image, uh, we're going to have an S3 bucket that's intermediate kind of data landing zone from coming from the streams into S3. Then we're gonna give that a name of the feature and then we're gonna do the sync side or the open search target side. And if you wanna do pictures, I'm gonna go, I'll go ahead and build it out. So on the sync side we're gonna find what's the open search cluster that we want this data to go to. What's the index that we want this data to go to now you can have multiple, you know, pipelines going into the same cluster to different indexes or to the same index and then the key thing is, is this mapping function. And so yes this is a little bit of work on your part. You're gonna have to write the mapping and the JSON here, uh, to, to get this kind of done and, and make it mapped the way that you want to, right? But it is, uh, so it's, you know, it's a er ETL with a little bit of a caveat you're gonna have to write the mapping piece, you know, for it so but it'll do all the pipelining, moving it, and monitoring it for you. Alright, so that sets it up. So I just talked about index type fields mapping the actions configuration, you know, start streaming changes on Dynamo DB. All right, so once that's created all up and running now on your open search cluster you can start to do your product searches on that same data set, and this is updated in in seconds on your open search cluster, right? So now you get that enabled searching on pretty much anything. You can also, you know, open search is really good for natural language processing, semantic searches, things like that. It really opens up a lot of, uh, possibilities, uh, there. Alright, one caveat with, uh, open search is, uh, on that pipeline there is open search compute units, or, and that's OCUs, and that can each OCU can process 1 megabyte of data per second, um, and this is a typo on my slide. I apologize for that. I didn't notice it, but that's actually equivalent to 10 WCUs on your Dynamo DB table. So what you wanna do here is you wanna match your WCUs to your OCUs so that your pipeline can process your data and keep up with the changes, right? So it does OCUs do support auto scaling, and you do wanna keep that where it can scale up and down. It is serverless, but it'll scale up and down as your throughput, you know, to your Dynamo DB table comes in. So just something to call out. The integration does support that letter cues so if there's some type of error or uh something doesn't get processed, it'll handle all that with CloudWatch. All right, so that covers Dynamo DB as our source. So we took a deep dive in that. Now let's switch gears to Aurora, which a lot of you use, so that this should be, uh, very applicable to you. So on the Aurora 0 ETL, same thing, we wanna make it simple, perform it, make it easier for you to get that transactional data over to your analytical systems to be able to mine that for the gold that's in there. One of the cool things that the engineering team did with this particular integration is it's at the storage level, right? So your compute Aurora cluster primary instance, right, no impact there. It's all done at the storage level. So we're gonna automatically, you know, data seating and the continuous replication is at the storage level between Aurora and then also with Redshift. So there's also monitoring with Cloud Watch in terms of lag, number of rows, performance, and then you also have some tables on the redshift if that's, if that's your target. That you can monitor the data coming over to it. So let's take a little bit deeper dive on how that works. So as long as you're running MySQL 352 or above, which I think 352 is now also out of support, so hopefully nobody's running that. And then also Postress 1614 or above, now you, you'll get the data. So we're gonna use a parallel direct export out of the storage from the Aurora side over into our Redshift storage. So we'll make that happen so the seed data cuts over. It's like a full move, right, with if you use the DMS terminology. Then we're gonna do CDC. So under the covers it's gonna use the MySQL enhanced spin log to move the data over and then for Postgress it's gonna use the PGological replication. So we set that up and CDC comes from storage. CDC streams runs, and then we replicate it over to Redshift, and this is typically done in seconds. So from a data freshness perspective it's usually up to date in a matter of seconds. Transaction happens over here. It's over in Redshift in just a matter of seconds so I can start to do an analytics on top of that. That's the basic setup of that. And that's really distracting. I'm sorry. All right. So now let's walk through our data set up again. So on Aurora, it's a relational database whether it's MySQL or Postress. It has primary keys, indexes, and it has your columns in there just like your relational database would. Now going to Redshift, Redshift is, uh, also kind of relational, but it is a column or format of the data. And then you also have that super data type like we talked about earlier. So we have to map that. And then Lake House similar just doesn't have the, the super data type. OK For the Aurora 0 ETL, some of the key differences in it is that it, it does enable filtering, right? So now this filtering is kind of an include exclude type of filtering capabilities. It's not full blown ETL, right? So we talked about multiple transaction systems. One customer that I supported for for several years, they had 8 different, you know, transactional systems that they would bring together into their data warehouse and then normalize. All those customer codes, product codes, etc. that's not this, right? This would help you bring all 8 of those over into like a staging area or a raw area inside your data warehouse, and then you do your value added processing to take all those, you know, customer code product codes, normalize them into something you have in your data warehouse. But this is fun. You can do includes excludes, and this is straight out of the, the manual just for my sequel examples. So I have a book database you can bring over just the mysteries if you're a mystery freak, or you can exclude those, uh, if you like Stephen King, you can bring all his books over so you get, you kind of get the idea of what the filtering capabilities. Now one thing that I'll call out is when we initially released. This 2 years ago filtering wasn't there, so feedback from customers like yourselves said, hey, we wanna be able to filter this data and then the service team added this in. So if you do some experimentation with zero ETL and you find like, hey, I really wish it did this, tell your AWS account team and we'll, and we'll get that feedback to the service team and we'll try to bake those features in. So, uh, we're definitely not done in this area continue to continue to invest and expand. So again filtering logic include exclude but it's not full blown ETL and that's really the key thing or or difference with when you're setting up the zero ETL with Aurora as the source. Uh, there are some fix it for me capabilities here as well. Word of caution with this one, right? You can see that this says here requires reboot. If I'm doing this on my production system, I don't necessarily want this to go rebooted in the middle of the day, right? Uh, this would probably be something that I would wanna schedule it. So just a little something to consider, right? And the reason that this needs to reboot is, let's say that my Aurora MySQL database doesn't have replication service turned on, right, or Postgrass. So that does require a reboot when you when you set up the logical replication and so that is something to consider there uh when you're setting up Aurora as a source. All right, on the red shift side, the, uh, it also, there's a fix it for me for case sensitivity. So maybe I, when I set up my red shift, uh, cluster, I didn't set it up with case sensitivity. This fix it for me will do that. And basically here's kind of, hey, do you want me to do that? You can press continue, right? So again, they're not AI agents, cool little bots, but at least it is a fix it for me, click it and it'll take care of all that for you, so. Uh, don't be surprised that if you know in the future Kyra will basically do all this for you at some point, right? Uh, OK, so now my zero ETL from Aurora to red shift is done, and here's just the example. It's the same screen that we showed earlier, uh, with the sentiment analysis, right? All right, so that covers Aurora. So now we're gonna move into other RDS engines, and one of the things you'll find is that those are similar to, uh, the Aurora, right? So we're basically going to set up the zero ETL. We're going to have, in this case it's Oracle. So it's gonna be redo archive log instead of the Vin log or PG logical replication. So it's a mechanism that the database has for. Replicating changes to us, um, and there's no storage level integration here, right? It's gonna be EBS or the log miner out from RDS Oracle into our red shift or our lake house S3. Now Oracle and AWS, so, uh, last year at Oracle World, uh, AWS and Oracle announced that we would support. Uh, OCI exit data inside AWS data centers and so a, a reason that I mentioned this, some people may not know what Oracle and AWS is, so I just wanna make sure everybody is familiar with that. So this is for customers that have critical business workloads that need the exit data performance and availability. But they want to run it inside their AWS infrastructure with all their other key pieces. So we do have zero ETL from the Oracle at AWS. So you, you're basically connect your ODB over to your AWS analytical services like Redshift and the data lake. So some of the things that you'll need to do for that, uh, you'll need to enable Zero ETL on your ODB network. The secrets manager, KMS IM policies will all need to be done. And then on your Oracle AWS, well you'll need to enable the redo log to be able to capture those changes and then you'll need the SSL ASM user name and password grants to allow the access to that. All right. That covers Oracle at AWS, which that's still I've got one customer that's moving into production that shortly, but that is a relatively new thing. DocumentDB to open search. So this, uh, was announced, I think last quarter. Uh, so zero ETL document DB to open search very similar to Dynamo DB, uh, IM permissions. You'll need an intermediate intermediate S3 bucket. You'll need the pipeline permissions and then you'll also need to do that JSON pipeline mapping your document or collection to your open search, uh, index. Couple of uh things to call out for document DB as a source. Uh, there are some limitations. You only get so if I have multiple collections on my document DB cluster, it's only one document DB collection per pipeline. Now you can have multiple pipelines going against that cluster, but it is a 1 to 1 relationship between the collection and the pipeline. Now one thing to call out the cool cross region cross account features that were in the other ones, uh, is not quite there yet for DocumentDB. So, um, if that's something that you need, uh, definitely reach out to your AWS team and, and let them know, and we'll put that feature request in. Also, the elastic cluster flavor of DocumentDB doesn't support this yet. Um, and then the open source, uh, pipeline setup is very similar to Dynamo DB. Last week we announced uh AWS Glue Zero ETL for self-managed databases. So now if you've got a MySQL, SQL Server, Oracle, Postress database running on Prim or running on ECT. 2, now you can use that same zero ETL magic to bring that data into your AWS analytic services so that, uh, you know, I did this presentation, uh, most of it a while back. This got announced last week, but I did wanna include and mention it here, uh, just so that you're aware of it as well. So to kind of summarize the options, right, you've got 8 new 0 ETL connections. This includes AWS managed, uh, services like Aurora and Dynamo DB databases. This includes your databases that run on Amazon EC2, as well as your on-prem databases, as well as third party clouds, right? So as long as you have the networking set up between AWS and if you have a third party cloud or, uh, something running on prem. You can use the zero ETL services to keep that data in sync and add it to your analytic, uh, analytical systems. So lots of options again. We really wanna make this simple, performant, and easy for customers to be able to, you know, get the value out of that transactional data. So that kind of covers all the permutations in terms of sources and targets. Now let's talk a little bit about monitoring and also some of the benefits of zero ETL. So I've got my pipeline all running. I'm happy go lucky and then the development team adds a column tells no one, right? What happens to my ETL? Well. Typically I'm either not gonna get that column because I didn't know that it was there when I set up my ETL or my ETL is probably gonna break in the middle of the night the first time they deploy it because I have an extra column and my insert doesn't match my data that I've got in the raw layer. Not fun either way. Now with zero ETL, what happens in that case? Well, since we're using the Ben log or PGological replication. That DDL or DML change operation of adding altering the table, adding the column percolates over across the zero ETL, and now my red shift table gets updated with that additional column. I get to sleep in at night, right? Uh, the data is also kept, so if they do kind of a big update that flows over as well and updates all my records. So there's definitely benefit. You get to go focus on other things and sleep well at night, right? So. And again this just kind of shows on the red shift side what that looks like. So again, glad to do an immersion day and and walk y'all through this and get some hands on and and test this out so. Right. OK, so I did wanna just cover that it should help address the the fragile, you know, complexity as the retail will take care of that. Is it, is it gonna take care of every possible scenario? Maybe not, but it definitely will cover the common use cases of altering a table, adding a column, or adding a new table. All that stuff will, will be covered in the zero ETL. Now let's talk about the monitoring and management features that are baked into the zero ETL. First is the status. So you know we're creating our, you know, is one of the statuses that's there so you can keep track of that, which that may not be that interesting. Uh, what might be interesting though is modifying or syncing or needs attention. That probably would be something I'd wanna know about, right? So I would set up a cloud watch alert that says, hey, if my status of my 0 ETL says zero attention or it failed, I probably wanna take a look at that, so. Um, just, you know, just things, and that does plug in to the normal cloud watch alerting mechanism and, and that's there. Now tracking how the data is flowing across 0 ETL that comes from cloud watch and our normal metrics. So you've got things like what's the insert update delete counts, and you'll see here on this since you've got CDC and you also have the seed data. So it'll tell you how many rows did I move over in my full move and how many rows am I CDCing over the course of the day or or the hour, um, and you can also set up those cloud watch alerts on that. So if you know that. I typically see, you know, 1000 rows per minute, uh, on CDC. I can set up an alert that says, hey, if this drops below 500 for more than 10 minutes, I probably need to take a look and see what's happening on that, right? So all that, you know, monitoring and, and alerting mechanisms that you're used to with cloud watch that can be applied here with zero ETL as well. OK. Open search. So open search, uh, pipeline does have some additional, uh, monitoring metrics. You can set up a dashboard through CloudWatch. It's also on the metrics tab in the pipeline, and it's going to track for you what's the CP utilization, what's your OCU uselet utilization, uh, memory. It's gonna also tell you how many records it dropped into S3, which is the intermediate piece, uh, and then the record counts and if there's any errors. And then on your sync it's gonna give you details around how many bytes have transferred, how many documents are written, what's the latency, uh, etc. so you can kinda keep track of all your zero ETL. And that'll also help with your troubleshooting, right? So, and all this goes into the cloud watch logs and so if you did need to troubleshoot it, you can go, you know, mine the cloud watch logs to, to get that. So in summary, You know, AW 0 ETL integrations again we're trying to make it simple, easy, uh, for you to manage to get that transactional data over to your analytical systems to enable all those cool gen AI and analytical capabilities that that we've talked about. So that's pretty much our presentation. So other resources to consider, uh, just some, uh, URLs here, uh, the documentation, uh, and if you've got any other questions I'll be glad to sit up here in front and answer those. So thanks again for, uh, coming up to the win for the last session before replay. Hope you have a great replay experience tonight and, uh, thank you for being here and don't forget to fill out your survey. So thank you.