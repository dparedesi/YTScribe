---
video_id: R2QxpJDEmY4
video_url: https://www.youtube.com/watch?v=R2QxpJDEmY4
is_generated: False
is_translatable: True
---

Good morning and welcome to Reinvent. Every one of you here has followed the advice I received when I told my friends I was going to Vegas. You made good choices. Choices which have led you here to this session today. At an event like Reinvent, you have a variety of interesting options to choose from, but you decided to be here and for that we are very, very grateful. My name is Arvind, and I will be jointly presenting this session with William Yap, Principal Product Manager at AWS. Let me first set the stage and tell you what we're gonna talk about. We're gonna kick this off by discussing why we are here. We'll share what confidential computing means, what are some of the workloads that require confidential computing, and what are the requirements of these workloads. We'll then proceed to share AWS's perspective on confidential computing. And what we have done to make sure confidential computing is always on for our customers. We will then proceed to talk about additional requirements. Like enhanced isolation. And the workloads that can take advantage of capabilities like AWS and Nitro Enclaves. And with that, in the true spirit of Vegas, you will learn that what happens in an enclave stays in an enclave. We will then switch gears and talk a little bit about AI workloads. And the requirements that have come with these new workloads and what we have done to respond to that. William will then take you through. And talk about our new capability, EC2 attestation. We launched it earlier this year. We are very excited about it. This brand new stuff we're going to be talking to you about. And then he'll tell you how we've brought enclave-like capabilities to EC2 instances. And then he'll dive deeper into attestation, tell you how to take advantage of attestation, how to use it, demystify the approach for you. And then we'll walk you through some use cases, like confidential influencing, federated learning, and some others. And then we'll close the session out with some resources for you to take advantage of. So you can start building when you leave this room. Sound good to you? All right, let's get, let's get to it. So why are we here? We are here today because you and I share a common goal. We care about building solutions that enable data protection. But let's first talk about data. Because just like us, not all data is created equal. I'm just more handsome than some people that's just it, right? There's innocuous data, and there is sensitive data. When we're talking about data protection, we are talking about data that needs to be protected. Sensitive data. Now if you look at data and you look at what we do with it, everything we do with data can be bucketed under 3 operations. storing data, moving data, and processing data. Everything we do can fall under one of these 3. And data is constantly in that flux. It just, it just keeps moving from data at rest. To data in transit and data in use. The mechanisms to protect data while the data is at rest and in transit are fairly mature. They existed for a long time, very well documented, people know how to use it. And many of you probably use it already. I hope So what we're going to focus on today is protecting data in use. Confidential computing is the ability to protect data while the data is in use, while the data is being processed. And that's what we're gonna be talking about today, and that's why we are here. We are here to learn how to build solutions that will enable you to protect data while the data is being processed in memory. On the screen you see a smattering of some workloads that require confidential computing. It's just a few examples. There's a lot more. Decryption is an operation, right? It could be decrypting data, revealing plain text. You may want to protect it if it's sensitive data. Signing transactions could be financial transactions, could be something else. Signing is a very, very popular workload and use case. Tokenization tokenization could mean many things, many different things, depending on on the industry, on the vertical you're you're focused on. It could be tokenizing personally identifiable information. It could be tokenizing financial assets, digital assets. The tokenization is a workload that is begging for protection of the data that is tokenizing. Masking masking is another very popular use case masking credit card information masking your your name, your Social Security number, whatever it may be, but you can see there's a common theme with all of this it's the data types. It's either personally identifiable information or protected health care information, financial assets, digital assets, sensitive IP models. It could be a whole bunch of things, and all of these different workloads come with a common set of requirements. Customers want to make sure that there is no unauthorized access to any of this data. And they also want to make sure that only authorized code, code that's already been approved and authenticated, is the only one that's, that has the ability to process this data. While keeping their tax surface really low, because we're processing sensitive data, we want to contain it as much as we can. And some workloads here are more CPU heavy compute heavy, and certain other workloads are more maybe memory heavy depending on the workload so customers want flexibility with resources as well and all of these set of requirements need to get fulfilled while making sure we are keeping things easy to use. So these are some common set of requirements that we have had to deal with when we work with confidential computing workloads. So when I talk to customers, it often comes down to two considerations. What are you looking to protect and who are you looking to protect it from? The answer to what you're looking to protect has already been established, right? I think we all agreed. We're looking to protect sensitive data, so when you're building specialized capabilities to protect data, it better be the case it is for sensitive data, otherwise it may not be worth the effort. So we know we're talking about protecting sensitive data. It's a straightforward answer. When you get into the question of who you're looking to protect it from, the answer is oftentimes twofold. Depending on the workload and the customer. They are At first, looking to protect their content from operators of the cloud provider, AWS operators. And depending on the workload and data types, sometimes in addition to protection from AWS operators, customers want to make sure they're protecting that data even from themselves, from customers' own operators. To summarize everything I just said, if you weren't fully following it, Confidential computing. Is the ability to protect data while the data is in use. And customers want to protect the data from us. And from themselves that's it that's the summary of everything I just said. Protecting data and use. So now let's get into looking at how AWS addresses these considerations. How are we making sure we are protecting the data from AWS operators? Let's start with that. And then I'll segue into how to protect it from yourselves as well. To understand how we have operationalized protecting customer content from AWS operators. We'll have to step back and understand how we reimagined virtualization. On the screen here, you see a representation of what I would like to call classic virtualization. This is still how virtualization looks like. In most hyperscales, in most implementations, and it works fairly well. If you look at the picture, you have these small white boxes at the top they're all customer instances they're are your VMs then you have these colored boxes at the bottom. Let's just say they're all virtualization functions that need to happen to enable the whole virtualization stack. And between these, you have a full blown hypervisor, the Zen hypervisor, which orchestrates all of these functions, ring fences the VMs, allocates the resources and everything. So now the host, in addition to providing the VMs, has to contend with having to take care of networking and IO functions, storage and device models. And other operational functions. Like management, security, monitoring, all of that. So you can quickly see, in addition to providing the VMs, there's a lot of resources that's getting spent on taking care of operationalizing these virtualization functions, which is all then being orchestrated by a hypervisor. So when we set out on our journey to reimagine this, we challenged the status quo. It's working well, but can we do it better? Right? Because we wanted to utilize the resources on the host better than we were, so we can provide more VMs to you. So what we did in that process was start looking at how can we abstract these virtualization functions away from the host. So we free up more resources from the host. To basically offer more VMs. So we took each of the functions that you're seeing at the bottom. Starting with networking and and IO functions, and ran it on custom silicon. We hardened it and we ran it on custom silicon and we abstracted it away from the host and it went into a separate box of its own. We did that first with networking and IO and we were pretty successful with it and we said, OK, let's go and start removing piece by piece and then we removed storage and device models and finally we came to the operational functions monitoring, management, security and all of that and removed it. Each of these was hardened in its own separate PCIE card we call them nitro cards. And we have a series of nitro cards, which we, you know, for, for, from a mental model perspective, which we have in a separate box, if you will. Which implement all of these functions in tandem with the host. So we ended up with a system. That looks like this. By abstracting all of the virtualization functions away. We were able to remove ourselves also from the host. Now you can quickly see if the host is just left with VMs, it doesn't need a full blown hypervisor like the Zen hypervisor. So we wrote our own, a very thin, lightweight hypervisor called the Nitro Hypervisor. Which has very little to do actually. It just has to ring fence the VMs, take care of some IO functions, allocate the right CPU and memory resources. It doesn't have to do all of the orchestration that the Zen hypervisor had to do, so it's very lightweight, very little overhead. So when we developed and deployed the system, in addition to utilizing the resources of the host better, we also ended up with a system which was more performant. And it was more performant because there was very little overhead from this hypervisor. So we are able to provide near bare metal-like performance with our nitro-based EC2 instances. But most importantly, what's pertinent to what we're talking about today. is we created this natural isolation between us and the host, because we removed all of the virtualization functions away from the host. So together, the Nitrocs, the nitro Security chip, and the Nitro Hypervisor, which makes up the nitro system. Provides that isolation. By default to our customers. So if you are looking to deploy a workload on AWS and if your consideration is to make sure your content remains protected from operators of AWS, then all you have to do is just spin up an EC2 instance and run your workload. You're done. Because in AWS confidential computing is always on. It's like that switch on the screen. Doesn't matter which way you flip it. It's an on on switch. It's just gonna stay on for you. And you can take advantage of always on confidential computing. With no code changes because it's no different for you. All you have to do is spin up a nitro-based CC2 instance and you're good to go. And it comes at no additional cost to you. So remember the considerations as you start looking into capabilities to implement confidential computing. If your consideration is just I need to protect my data from the cloud provider, you're all set with us. It's on by default for you. So now that's just the gist of what this is, right, but if you want. To gather more details about how we have implemented the security design of the nitro system, what have we done to create that degree of isolation I talked about and how the nitro system was built, we have a white paper we published about a couple of years ago. It's good bedtime reading. It's 27 pages long, so if you're really interested in it, you know, I recommend taking a look at that. It gives you a lot of details about how this was implemented. In addition to the white paper, there is a third party report that you could refer to by NCC Group which examined the security design of the nitro system and has reported that there are no gaps in the nitro system that would compromise the security claims that we made in that white paper. We have also updated our service terms, I think, about 2 years ago, to reflect. That there is no operator access to customer content if you're using a nitro-based nitro-based EC2 instance. So that's how we've addressed. The requirement of making sure your data remains protected from AWS operators. Now let's move on and talk about your additional considerations. What about protecting data from customers operators, from yourselves? Only when you have such additional considerations do you start thinking about capabilities like nitro enclaves. So let's first see why you need the additional isolation, right? On the screen, what you see is a 30,000 ft level of what an EC2 instance would look like. You have your instance and within the instance you have a bunch of different users who have different levels of access to the instance. You have your application and third party libraries you perhaps leverage to build your application. And then the OS Now, if you were to process sensitive data in this instance, assuming you had your data encrypted and stored and everything and you're moving it still encrypted in transit and bringing it to the instance, when your encrypted data hits the instance, if you want to process it, you have to decrypt it. That's just how things work at scale today. You're gonna have to decrypt the data inside the instance, but once you decrypt the data inside your instance, you're revealing plain text to all of the entities you're seeing in there. And that's really what you're looking to protect from when you're talking about additional isolation. And that's when you start thinking about capabilities like nitro enclaves. With nitro enclaves, you have the ability to allocate CPU and memory resources from your own EC2 instance to spin up a hardened and isolated compute environment called nitro enclaves. Nigel enclave. Has no external network connectivity. There is no persistent storage. There is no administrator or root user access. There's no way to SSH into the into the enclave by default. The only communication channel that exists between from from the enclave is a secure local channel between the enclave and the parent instance. That's it. That's the only channel through which data comes in and goes. So now if we go back to that scenario we talked about where you're bringing encrypted data into the instance and you don't want to unpack the data in the instance, take that encrypted data and continue to pipe it and forward it to the enclave, still keeping it encrypted. Once the data hits the enclave, then proceed to decrypt it within the enclave, at which point nobody has visibility into the data except the trusted application that's running inside the enclave, which can then proceed to process that data. And that's how we achieve additional isolation. And fulfill the requirement of protecting content even from yourselves. Now here's the cool thing, and in my personal opinion, this is the coolest feature that the enclave offers. Nitro enclaves can provide proof of identity. By doing this, you can be sure that when you're revealing secrets to the enclave, it is the specific enclave running the expected and trusted application to which you are revealing secrets too. When you decide to spin up an enclave, you're going to build an enclave image, and when you build the enclave image, you're going to get a set of measurements. Let's call it a reference hash. And then when you proceed to deploy the enclave, after the enclave has been deployed, the enclave can fetch an attestation document signed by the nitro hypervisor. The attestation document contains computed values about the enclave that was launched. It includes information about the enclave image, the application in the enclave, the parent instance ID, IAM roles, and a few other things. These are the computed cryptographic hashes. So when the enclave wants to go and access secrets, What it's going to do is present that attestation document to the entity it's requesting secrets from. And that entity is going to compare this attestation document against the reference hash we talked about when you built the enclave. And if the values match, that is the proof the entity needs to make sure. That the enclave has provided proof of its integrity and it can now establish trust with the enclave to share secrets with it. This is a high level of how attestation works. I'm not gonna get deeper into this because William's gonna come, come back later and talk to you about attestation in great detail. He's gonna show you how to use it, but the mental model for the new capabilities we have launched and capabilities that have existed like Nitro Enclaves, the mental model is the same. At build time you get a reference hash at run time you get a set of hashes. Are these two matching? If they match, then you know you can share secrets. That's how we have established trust. Nitro Enclaves come with first class integration with KMS. KMS is our key management service. A lot of customers who use enclaves also use KMS. Um, I was talking to folks in the audience earlier who were, who were, uh, inquiring about how to manage keys, how to bring your own keys, so we're happy to talk about that later, um, after the session. But I just want to leave you with the thought that KMS is how you're going to use. Keys to bring it inside the enclave so you can then decrypt the data we talked about, but to do this, KMS now is that other entity we, we, we discussed earlier with attestation. KMS is gonna have to verify that the right enclave with the right application is asking it to share secrets. And with attestation it's able to do that and what we've done for you is integrated this whole attestation flow, but you're by no means tied to KMS. If you wanna use your own key store, feel free to use it. You just have to build the attestation yourself. What we've done with KMS has made it easy for you. That's it. So here's a quick summary of everything we talked about in terms of protecting, protecting content even from yourselves. With nitro enclaves you achieve additional isolation security. There's no external network connectivity, no storage, no administrator root root user access, and the only communication channel is a secure local channel. Nitro enclaves are highly flexible. They are processor agnostic, so it doesn't matter what instance type, what CPU instance type you're trying to leverage enclaves from. It could be Intel CPUs, it could be AMD. It could be Graviton. Nitro enclaves are functional in all of those instances. And more importantly, bringing you back to that flexibility we talked about earlier, you can choose varying combinations of CPU and memory to allocate to your enclave depending on the demands of your workload. So it could be more memory, less CPU, whatever you need, you can, you can choose the combination yourself it's pretty flexible and lastly, enclaves come with cryptographic attestation which helps it prove its identity so it can establish trust and you get all of this isolation again at no additional cost. If you're using your EC2 instance, all you're doing is dedicating CPU and memory from the same instance to create an enclave. So that's that's the high level of of enclaves. I'm more than happy to take questions about it later. We have more planned for you today, so I'm gonna keep moving. Here are some customers that have used Enclave successfully, and this is just a subset of customers who are using it. The reason we chose to show you this is to give you an idea of the different verticals and segments that take advantage of capabilities like natural Enclaves. So you can see that our customers in the ad tech space, customers in financial traditional financial, um, industry. There are customers within the web 3 blockchain space, crypto exchanges. Privileged access management customers. There's a wide variety of customers who can take advantage of such foundational capabilities to implement confidential computing. As you go through this week, I want to direct you to some sessions as well. Uh, two of the large payment processor processors are presenting this week on their use of confidential compute and nitro enclaves at AWS. So both Visa and Mastercard will be talking about real-time payments and how they have leveraged enclaves to build secure and low latency solutions to do payment processing. Coinbase will also be speaking this week, um, actually with me on Friday. To talk about their implementation of secure wallets with enclaves, it is actually a session focused on agentic payments. So if that's something that tickles your interest, I invite you to join us for that session, and Coinbase will talk about their use of X402 protocol to integrate with agentic payments and how they integrate a nitro enclaves-based wallet for the solution. There's also additional reading on on how customers like Fireblocks have implemented custody solutions with enclaves and how Stripe has done key management solutions and and brave rewards computation solutions using blockchain technology. So these are all what we've already done and accomplished with customers, and we'll continue to work with you on these. But now things are starting to shift a little bit. AI is here, and with AI there are new workloads, and the requirements that we talked about earlier have evolved a little bit. With the new AI workloads, customers want to now make sure they're running their workloads on GPUs and AI accelerators for the large part. So whatever capabilities we talked about earlier need to be made available on all of this. Customers want to protect the models and the weights that they're bringing inside the the compute compute entity. Not only that, they want to proceed to seal these models and ways to that so they can be really sure it is the right instance, it's the right isolated environment that's that's fetching, that's fetching data, that's fetching the model. In addition to that, no surprise with AI you need greater bandwidth. So this consideration with no external network connectivity was starting to create some friction points. And customers also wanted storage, access to storage because they need a lot more data now. It's not just about dropping a small payload or a few keys. And all of this again with no code changes. So we started to look into this and launched a new capability which William is now going to come and talk to you about. I invite William to talk to you about EC2 attestation and take you through the rest of the session. I'll come back and close it out with some resources for you later. Thank you. I got it. Thank you, Irene. Good morning. My name is William Yap. I'm a principal product manager for compute services. I've been handling confidential computing for the past 6 years. So all the stuff that Irene talked about that I call Nitro. Nitro enclaves, nitro TPM, Nitro System are capabilities that that we've worked on building. And so we're, in this talk, I'm gonna try to give you some of the insights of what we were thinking about when we're building these capabilities. Now I've mentioned AI, you know, AI is something definitely that's caught a lot of interest the last few years. But we're gonna use AI as a template, a reference case study to talk about the new confidential computing features that we've we've built. And 2025 has been a a busy year for the Nitro team and we're excited to tell you all about this. Generative AI is interesting for confidential computing because of the sheer number of sensitive data that you're looking to bring there. As well as the multiple parties that are involved. I've simplified what it means here for user data, but what I'm talking about, it could be prompts, it could be your business data, financial information, personal data, things you'll bring through via rack, knowledge bases and all. These are things that you want to protect. The second thing that's sensitive is the model itself. For closed models that the model weight represents significant investment to train those models, you would like to protect that too. And depending on the type of outputs that you're you're producing, it could be sensitive. So as you can see, multiple sources of sensitive data and different parties that's owning each of it. And we want to protect that. So which, when you have multiple sources of data that are going into the models for inferencing. We're seeing customers, enterprises who are looking for ways to give stronger data privacy assurances to their end users. Uh, if they want to hand over their data. What are the assurances that we can give them. So let's break it down. Let's break it down into 3 goals here. The first goal that we want to solve is we want to make sure at the base foundational infrastructure level, the infrastructure provider has no mechanism to access the data. The cloud provider has no mechanism to access the data. The second goal they want to make is to make sure that when you go up one level, uh, the service provider, the enterprises also do not have access to the data that's gonna be used for inferencing. And lastly, you have encrypted data that's going into this environment. You need a way to prove that this environment is what it says it is. It's isolated before data gets decrypted, before data gets processed. So we're gonna tackle this together, we're gonna work through this as a group to try to solve this problem. And using AI as a, as a template, we're gonna look at how from AI we can look at other case studies as well. Item number 1, we solved that pretty quickly on EC2. As Arvin mentioned, the Nigro system. Has no operator access. It's by design, it's something that we have, we've spent a multi-year investment to build the chips, the software with the number one design principle to making sure that no AWS operator has a mechanism to access the data. Our, our EC2 nitro instance is very different from the classical traditional virtual machines where you get this type of protection and that's gonna give us a very strong foundation to build upon. So number one checks out very easily for you as a customer because on AWS we spend significant investment to make sure that's easy. Let's focus on #2, how do we make sure there's no mechanism for you as the the enterprise, as the service provider to access the data yourself? I've already mentioned that we needed to evolve. Your requirements are evolving. You want confidential computing in GPUs, you want confidential computing with networking, with storage. And when we, you know, as me as a product manager, when I look at building this on the nitro enclaves. As we add more and more capabilities to the nitro enclaves, it's gonna start to look more like an instance. So instead of evolving nitro enclaves that way. What we've done is that we've taken The capabilities that you like in nitro enclaves and bring it back to the regular nitro EC2 instance. And this is what we have launched We call it EC2 instance attestation. It's a capability that makes it easier for you to validate that only trusted software, trusted code is running on your EC2 instance. No, before this, you can, you can definitely build trusted EC2 instances, you can definitely build an EC2 instance that has no SSH. But what you have over here is a capability to evidence that, to prove that, and to allow decryption of data only when that's true. If you notice one thing, I, we didn't give it an AI name. It's called easy to instance asitation because we see this as a way that customers are gonna use across all different types of use case. We see this as something very powerful that you're gonna use generally. So What is EC2 attestation? It comprises of several capabilities and I'm gonna talk a bit about right now. At the base layer, we have the Nitro system. This is a Nitro EC2 instance. It has zero operator access. We have this device that you can attach to an EC2 instance called the Nitro TPM, the Nitro Trusted Platform Module. This is something that we have already. This follows the TPM 2.0 standards, and it does measurements, it does adaptation for the EC2 instance. You can do this on an EC2 instance that has an AI accelerator. So many of our GPU instance, ourranium 2 has this capability. This is the starting point. Let's look at the new stuff. The first new stuff that we have launched. is attestable Amazon machine images. This is a new way to build armies. When you build it, you'll get a cryptographic hash that represents the content. The brute process of that army. We're gonna dig deeper into that shortly. Next thing that's new, nitro TPM attestation document. Now you can generate the same style of attestation document that you get from Nitro Enclaves with Nitro TPM on a regular nitro EC2 instance. This makes the attestation process very easy to do. You no longer have to do the multi-step dance, you just have a document that's signed with all the necessary information. And the last thing that we have launched. Is that we know that every customer is gonna need to figure out key management and attestation and how you're gonna have the data encrypted and only allow decryption when the attestation passes. And so we've done the heavy lift for our customers by integrating that with AWSKMS integrating Nitro TP with AWSKMS. With these 3 capabilities combined, this gives you the tools to configure your EC2 instance into an isolated compute environment. In an environment you yourself cannot get access to. Let's dig into each and every one of them in detail. First one, attestable armies. It's a new way of building armies where at the time you complete building and creating that army, you get a corresponding hash. This is a, it contains a measurement of everything you have put inside the army. And the hash that you get from this army is your reference measurement. This is the true hash cause you know all the ingredients that you have put inside the army. You know that this is the true one and you, you're gonna keep it securely to to validate against later. Let's keep that in mind. So if you, if I go back to the problem that we're trying to solve here. You now have an inferenencing environment where you want to lock yourself out. What kind of configurations can I do here? How should I set it up in a way that I don't have access? Well, we do have some best practices to share here and uh this is documented as well. And, and I say best practices because it does vary according to the type of use cases depending on how you're you're planning to use it. But the first one, the most important one, you want to remove all forms of interactive access. What I mean by interactive access, no SSH, no serial console, no uh SSM. And the lights And we're not talking just about disabling a user. Access to the SSH function. What we're talking about is the SSHD demon is not even installed. There's no mechanism at all. First one is removing that access. Second thing is that you want to validate and ensure that only trusted code are running inside that instance, inside your army. Because you're doing highly sensitive processing, you don't want to install all types of packages that may may or may not be necessary. You're going to look at only the necessary packages, the only the necessary tools to be be able to run that workload. The 3rd thing, you want to have a network, so since you have networking right now, you want to have a network firewall within the instance boundary. To make sure you get protected access there. Number 4, You want to make sure that all storage and file systems is immutable and read only. So now you have an isolated environment. You can't get in, nobody can get in. How do you make good use of about it? So borrowing the stuff we did with the nitro system. We can look at building APIs. Not just any APIs. Build trusted APIs that needs to be authenticated, that needs to be verified, it's gonna be logged, authorized before it can be used. And ensure, because you are the designer of these APIs, ensure that this APIs doesn't do anything that's not supposed to do, that's not documented, that doesn't interact in with customer data with user data in a in an approved unapproved way. So these are the 5 best practices, and you can put these ingredients into an isolated compute configuration file. That's the ingredients. Let's put it into a recipe. At the base layer, you have an Amazon Linux, just a base image, very minimal. You have your trusted code. And then you've got your configuration. Now, we work with an open-source tool called Kiwi. This is an open source tool that allows you to build images where you can provide the recipe to the Kiwi tool that we have. Integrated with and generate the trusted army with the corresponding hash. Now, remember, this is your reference hash, this is the hash that you trust. You, you provided the ingredients, you verified the ingredients, this is the hash's trust. Let's keep that in mind because we're gonna use that. With nitro TPM, when you launch an EC2 instance and you enable nitro TPM. The Nitro DPM device gets attached there to your EC2 instance and at launch, it will measure everything about that EC2 instance. And notice something, the GPU and AI chip are within the protected boundary of the EC2 instance. It, when you measure it, you get a calculated hash. And you can compare this calculated hash of this instance, check it against your reference hash to see, is this the instance running the configurations that have stated that have zero operator access? If it's true, I've done done it the right way. So, before this, you can absolutely create an ECitu instance that has no SSH. Difference here is that you can get the measurements and compare against them. That's how we solve goal #2. No mechanism for you to access user data. The isolated compute environment. Now we're gonna talk a bit more about how we're gonna solve #3. And in order to do 3, let's go back from the beginning. How does cryptographic attestation works? Well, the reason why you want to know why attestation is, is how do you know your instance is truly yours? How do you know instance I 123 or instance A here has the right code, right measurements that you have stated? I think if you spend time you can definitely figure that part out. But imagine if you have 100s, 1000s, 100s of thousands of instances, that problem gets multiplied. We need a more scalable way to determine whether this is the right instance, your trusted instance, before you're sending highly sensitive data. to it. For the humans, we kind of solved this problem already. I know most of us who travel into Vegas uh by plane. We have to go through, everybody has the passport or identity card, real ID uh to to get over here in Vegas. If you think about that And attestation document is kind of like a passport. To an EC2 instance or to an enclave. Instead of a name, instead of the date of birth and location, you get measurements, and it contains measurements of the code that's present and boot, the bootup process, all the configurations, and the passport has to be issued by an authorized authority. You know, anyone can can just issue it. And so, the attestation document that you're getting here is gonna be signed by the nitro hypervisor, the nitro system. That is the first part of attestation, being able to generate the attestation document to produce it. That's the other side, another side attestation. How do I verify it? Once you present the, the, the passport. Which database do I check against to make sure that the values that I'm looking at in the passport is correct? That's the validation piece. And you can figure out the validation piece. By having your service refer to the reference hash. Remember at the beginning when we build an army, you get that reference hash? You can take note of that, get your service. To look at it or to check against that. But the challenge there is that remember we want to encrypt the data and send it to the isolated compute environment for processing. So it's not just sufficient to do that, you kind of need to Figure it out and add key management somewhere in between as well. So borrowring what Arvind said with what we have done with Nitro Enclaves, we know every customer is gonna try to figure that part out, so we have done the heavy lifting for you as well. So we've integrated attestation with the AWSKMS. When I say integration, what I mean here is that AWSKMS has the ability to ingest attestation document. It knows how to compare the measurements against your trusted measurements. You would place your trusted measurement as just a KMS key policy, it will compare against that. The other thing it does as well, it knows how to compare that and get that and check the the the validation of the nitro system that's correctly signed. This is the capability that makes it easy for you to do multi-party computation with an easy to instance. Now, We've looked through all the pieces that we have launched. Let's put it all together and look at how we can make a flow there and, and, and the flow is gonna be simple because it's designed to be simple. So if user data that you want to protect. You encrypted it. And you're sending it to an EC2 instance where you have configured no SSH, no interactive access, you have, we have to build that isolation. It goes to the ECU instance. EC2 instance will then send the attestation document. The AWSKMS that says, hey, I'm, I'm the real instance, check out my configuration, here's the measurement. AWSKMS takes that, compares that against your key policy, compares that against the real, your, your, your reference measurement there. If it checks out, it will release the encrypted data key back to the EC2 instance where only the EC2 instance can decrypt it. So now you have user data that have been decrypted. You can process it in that LLM, do that inferencing, inferencing. And depending on whether the outputs are sensitive or not. You can encrypt it and send it back out. This is the flow that we think is very powerful for customers. By the end of this week, we have, as I mentioned, we have workshops, we have cold talks. Everyone can leave Vegas with an application that does this. And we're going to have the workshop online as well if, if we don't get to it. This is how we solve the 3 privacy goals. And we've we've made that process very easy for our customers by providing the combination of these tools to do it. Now, I promised at the beginning of my talk that we're gonna talk more about AI and we're just using this as a reference case study. So let's talk a bit more about. How are we're gonna look at this and how does this vary? In the example that I provided earlier. You have this flow here. You have the end user that you want to protect, end user data you want to protect, you have the model data you want to protect. And the processing in the middle is inferencing. But let's say we take away inferencing for a second here. And instead of the model provider and user data, we say let's have two hospitals. They have patient data. And Hospital A and Hospital B are saying that if we can find a way to collaborate, share some patient data, we can get some metrics, we can get some information that's gonna help with our research. This is a privacy preserving way of doing it where you can encrypt the data, send it to an environment that neither party can get access to, do the the processing that both parties have agreed to. And only allow decryption of data when that's true. So if we have changed the processor in the middle, we changed the party, the diagram looks sim looks similar. It's a multi-party. Computation process. Now, let's change the hospitals to something else, uh, consumer goods. Let's say you have two different companies that are processing user preferences. In the ad tech space, they want to tokenize user identity, so instead, instead of looking at William's preferences for shoes, they're gonna look at token ABC's preferences for shoes. The sensitive processing part of this is a tokenization process, changing the name William into a token. Cause if I get access to that tokenization engine, I can reverse it, I can see it. You wanna run that in an isolated compute environment, in a way that neither parties can can do it. So we're seeing a lot of interest in attack space. Graduated learning. Let's say you can't send the patient data out, you know, cancer centers, hospitals sometimes have a requirement that they can't do that. So with federated learning, is what they're doing instead is that they're trading the data locally. Building the models locally and just sending the model weights out to a central location. The central location will be an aggregator that combines all the model weights together to get a global model where all the parties can use and gain benefits too. What I've changed here in this diagram is just the number of users and the processing. The concept that we have here is still the same. Multi-party computation, building the isolated environment that neither party, that that no party can get access to, encrypting the data whether it's model weights to to sensitive data, sending it to the environment and decrypting it when it's proven to be true. So I hope we we we learn the new process today. As I mentioned, everyone here can leave Vegas with the the new application that does this with a confidential computing application. I'm gonna hand it over to my colleague Arvin, who will give you more resources on how to how how you can leverage those capabilities. Thank you, William. So moving forward, let's talk about, you know, let's summarize everything we talked about over here, about confidential compute, right? You heard us talk about different capabilities for different considerations. We just have the nitro system upon which our EC2 instances are based on. So if you're just looking to protect from us, to spin up an instance, you're good to go. Now if you're looking for a very opinionated, isolated computer environment that's hardened already for you, you have nitro cliffs. Now if you're looking for AI use cases, you're looking to protect your models, you protect your weights on GPUs and accelerators, and William talked to you at great length about EC2 attestation, how to use that on GPU and and AI accelerator enabled instances. All of this, however, comes with a common theme. And the common theme is nitro, and what makes it really special is that with any of these that we talked about, You, it comes with the assurance of zero operator access, no mechanism whatsoever. And now, if you, if you notice the language a little bit different from no operator access to zero, because if you're just using an EC2 instance, you're protected from us. But now when you tag on the additional capabilities, you've also protected from yourself. That's really how we're trending towards that zero operator axis, right? And then as I mentioned earlier, with all of this, you get near bare metal-like performance because of the way we architected it. So between the virtualized instances and the bare metal, it's like a single digit percentage difference. Highly performant. So very well suited for your AI workloads. And with the new launch, we have now supported it across all of our instance types. So you're not restricted to CPUs. You could use GPUs, AI accelerators, any CPU if you choose to use enclaves and all of that. So there's a lot of flexibility with the instance types we're offering. And lastly, as I mentioned with attestation, we already ensured we are integrating all of this with KMS. So for most customers who leverage these capabilities with our key management service, it's already built out for you. But again you're not, you're not, uh, you know, bound to it. You can always use your own key stores, use your own external services. You just have to learn how to consume that attestation document and then, and then build it out yourself. And lastly with the new capabilities that William talked about today, we have launched it in all commercial regions and Gov Cloud as well. So if you are working on mission critical applications and workloads and you're leveraging GovCloud, all of this is now available on GovCloud in addition to commercial regions. Now if any of this piqued your curiosity and you were looking to build more, we do have a couple more sessions specifically on this topic. One is a code talk where we will deep dive into demystifying attestation. We'll actually open the attestation code and walk you through what's been implemented and how to use it. And then there's another workshop that you're welcome to join tomorrow on creating trusted execution environments specifically for generative AI applications. So we will show you how to stand up the whole thing with a GPU or inferential or a training based instance. I would like to thank all of you for taking the time to listen to us today and come join the discussion for confidential compute at Reinvent 2025. Thank you so much.