---
video_id: pbk4qWnqjFc
video_url: https://www.youtube.com/watch?v=pbk4qWnqjFc
is_generated: False
is_translatable: True
summary: "This lightning talk by Shubham Mehta introduces two new AI agents for AWS Analytics designed to eliminate undifferentiated work for data teams. The first is the **Apache Spark Upgrade Agent** (for Amazon EMR on EC2 and EMR Serverless), described as the industry's first automated Spark upgrade agent. It can upgrade applications from Spark 2.4 to Spark 3.5 (Spark 4.0 coming soon) by orchestrating the entire process: planning and orchestration, dependency updates (Maven, SBT), code modification using an error-driven loop and a first-of-its-kind breaking changes knowledge base, and data quality validation. It is MCP-based, allowing integration with any IDE. The second is the **SageMaker Data Agent**, a domain-specific agent for data engineers, scientists, and analysts. It is aware of your business data and catalog via MCP, can write runnable SQL/Python/Spark code without modification, and handles multi-step tasks like building ML models. A demo shows building a customer LTV prediction model end-to-end (data exploration via Athena SQL, visualization, feature engineering with one-hot encoding, linear regression). The agent includes a spark troubleshooting sub-agent and security guardrails."
keywords: AI Agents, Spark Upgrade, Amazon EMR, SageMaker Data Agent, MCP, Data Engineering, Machine Learning, LTV Prediction, AWS Analytics, Apache Spark
---

Good afternoon, everyone. Today, we will be discussing about AI agents and AWS analytics. Let me start with a number. Data teams spend 60-70% of their time on undifferentiated tasks. Data engineers are spending months doing upgrades. Data scientists are spending a bunch of their time on preparing data analysis that they can use for. And then platform teams are firefighting issues. Now today we do have AI assistants. We do have AI assistants which can take you the first step, which can give you the code, but a lot of times these AI coding assistants, they're not aware of your data, of your resources, of your things that you have done already in your, in your environment, and that's why a lot of the times they will give you the code that will not work, and then you end up repeating and changing the code, trying to customize it for your environment to make it work. But today we are going to change that. I'm Shubham Mehta, product manager for AWS Analytics, and I've been focused on bringing AI agents to analytics directly within your workflows. Today we are going to talk about AI agents that can actually solve the problem that you're looking to solve. They can help you upgrade your Spark code. They can help you build code faster. Before we get into details, let's quickly look at the agenda. We are going to talk about overall overarching strategy of how we are approaching AI agents in the analytics space. We're going to look into one of the agents which is Sagemaker Data agent, and then we are going to look into a Spark upgrade agent for Amazon EMR which can help you upgrade Amazon, which can help you upgrade your Spark code. Now, before we get into details, let me first describe the problem a little bit further. When we talk with customers, we hear problems in three specific areas. First, we are seeing workflow complexity. Every task that you want to do in your organization requires multiple things, multiple tools to be used, which means that if you're trying to build an ML model, you will end up having to use 4 or 5 tools for that simple task. If you're trying to build a Spark, write a Spark application or upgrade a Spark application, you need to understand your build system. You need to write the Spark code, you need to upgrade and change the Spark code, etc. and then the knowledge gap. You don't have a single engineer who is expert in everything. You have multiple engineers who are expert in different, different aspects of the entire workflow, where some of them might be good in Spark, some of them might be good in SQL. And lastly, we do see the capacity crunch where the data is growing. 10 times every 3 years, but the data teams are not growing at the same scale. Now we think that AI agents can actually solve this problem because AI agents, the agents that we are trying to build, they can orchestrate workflows end to end. They can actually embed deep expertise for Spark, SQL, and other complex engines within them, and they can scale infinitely. Now, before we get into the agents, let's look at what are the guiding principles that we have for AI agents in the AWS analytics. Now 4 things guide our approach. First of them, the domain specific agents. We think that over the years we have learned a lot about how, what are the problems customers face with Spark, what are the problems customers face with SQL, and that's why we are trying to embed all that expertise in the domain, in the domain specific agents. Second thing, we want the agents to be adapted to your role, to your tool. If you are a data engineer or a data scientist, you get a different experience than what you get, what you get as a software engineer. Thirdly, the multi-agent ecosystem. We don't think that one agent can solve all the problems. Not one engineer was enough to solve everything, so we are building an ecosystem of agents that can work with each other to solve certain aspects of the problem and then collaborate to solve the entire end an analytical workflow. And lastly, we believe in MCP-based interoperability. Because we know that you are used to using your IDEs, you are used to using your tools, and you want to utilize these agents where you're working rather than going to AWS for each and AWS console for each and everything. Now, Let's look into we have covered the high level principles. Let's see what are the launches that we have done around this so that you, it's not just principles. We have actually followed these principles and we are actually making it reality. So the first agent that we have launched just yesterday, we are proud to announce Apache Spark upgrade agent. This is industry's first Spark upgrade, automated upgrade agent. Where it can take you to a complex process of Spark upgrades from planning to code edits to building your Spark application and doing the data quality test across the data, across your application. And This agent is available as of yesterday for Amazon EMR on EC2 and EMR serverless, and it can take you from Spark 2.4 to Spark 3.5. Spark 4.0 support is coming soon, and this entire agent is based on MCP tooling. Where you can, we, we are launching a remote MCP server where you can configure this remote MCP server in IDs of your choice, and you can use it there directly. Then after you've set up the MCP server, you just say that I want to upgrade my Spark application from this version to that version, and here is the Spark application in the project, and it will read the code and it will go through all the steps. We'll look at the steps in detail in a second. Now it takes you through 4 steps. First of them is planning and orchestration, where as you give your project, we analyze the structure of the project. We see what are the How you're doing the spark submit to EMR because based on that, the approach to the agent differs. We see what kind of language you're using, what kind of do you have integration test in your project or not, and based on that, we define what are the steps we will do during the entire upgrade process. And here you can actually give the feedback that no, I don't want to take this step. I want, I want to ignore the integration test. Can you actually update the upgrade plan, and it will upgrade the upgrade plan. Then it will look at your dependencies whether you have palm.XML or requirements.tx. It will go through them. It will identify that OK, these are the dependencies that need change for Spark 3.5. Let me make those changes, and it will then go through your build process and actually. Build your application right now it supports Maba and SPT-based build, but because it is MCP based, you can bring in your MCPs to hook into your own custom build process and actually ask the agent to use your build process to build the application. And then once it has gone through all the build, all the dependencies, it will give you a list of updated dependencies that you can then verify and see that, OK, this makes sense. Let's go to the next step. In the next step, it actually looks at your code. And looks at all the changes, breaking changes that Spark has introduced and makes sure that your code is not having any of those breaking changes, and this entire process of code modification is based on error-driven loop where we try to run the code in the EMR cluster that you have given for the for the target version and we see, OK, what is the error your application is facing, and then we have built a First of its kind knowledge base of all the spark breaking changes where we make sure that we make the minimal changes in your code in order to make sure your code runs successfully in the target platform and then the next, the last step is we make sure that the data that the application is producing after the upgrade. Actually adheres is exactly the same as the data that you started with in your prior Spark version because it's not just about running the application successfully, it's also about making sure that the data that it is producing actually is the same as what it was producing before. The next release that we have done in this segment is data agent. Now, the previous one was a domain-specific agent. In this case, we have built an agent that is specific to data engineering and data scientists and data analyst persona. What makes this agents different is that it is aware of your business data and catalog. It uses MCP-based tooling to make to get all the information from your catalog, what tables you have, what's the schema of your tables, and then when you ask it to write the query, it actually writes a query that can run without you modifying anything. Now, this agent can also do multi-step planning. This can take you from com this divides the complex task of let's say a machine learning, building a machine learning pipeline. It will divide it into 56 steps. It will take you with each of those steps, and we'll see those in action in a second. It will take you through each of those steps, and it will help you actually write the code for each of those steps separately, giving you verification that things are working fine or not. And if you run into an issue, you can actually troubleshoot using a spark troubleshooting agent that is running behind it. So let's say you're writing a Spark code and you run into an issue. In this case, we actually rely on a spark troubleshooting agent, which is a third agent that we have. To fix the spark specific issue, and if it's not a spark specific issue, we resolve it without relying on the troubleshooting agent. And it also has security guardrails built in where it prevents any destructive action in your account, where if you ask it to write a code that deletes your table and all, it will give you appropriate warnings and make sure that you are aware of the actions that you are taking. Now I'll go over the demo, and this demo will take up the role of a data scientist who is trying to build a machine learning model to predict lifetime value of customers, and the reason we want to predict this lifetime value is because we want to provide the customers the right incentive early in their journey so that we can make sure they are growing, like our business grows with them. Let's go over the demo quickly. Now, in this case, we, we are in the notebook interface. On the left hand side, on the right hand side, you have the Spark StageMaker data agent. We are simply asking, going through the discovery phase where we are asking it, can you list all the tables that I have in my database? And it finds out, OK, you have 3 tables in your database that you wanted to look at. And in this case, we found out that there is a digital wallet LTV table. Now, the first thing is I want to see what is the sample data in the table. Here, I actually ask it to use AthenaSQL to write a query for this digital wallet LTV table, and the agent gives me a very simple query. We select from SageMaker sample DB and this is the new notebook that we have launched which actually have interactive like renders the data frames as interactive tables, and you can see that what are the columns I have, I have this customer satisfaction score. And sport ticket and preferred payment method. Now I asked the agent, I want to explore more. Can you actually help me analyze the impact of customer satisfaction score on the LTV trends? Now the agent will actually understand that you are trying to use the data that you have already loaded in your in your notebook, and it will use that data to actually build further. Now if you see, the agent actually says that let me create the visualization to show and I will create the multiple charts, and in this case, the agent used the same data frame that you had loaded using the SQL and this data frame is now used in Python code. So these notebooks are actually polyglotted in nature where you can use the work that you've done in SQL and you can use them in Python. Now in this case, agent comes up with graphs, OK. You can actually see that with customer satisfaction score, the LTV is actually increasing, and the agent has created this entire code. I didn't have to edit a single line. This entire code was written by the agent and running successfully, and here the agent also created this high level overview of what is the minimum and max LTV, median LTV for different satisfaction score. Now we get to the final task. OK, I want to build a linear regression model. To predict LTV based and I want to do an 80 to 20 split, and I want to do, I want to use one hot encoding. In this case, the agent will come up with multiple steps in order to do that task. So in this case, The agent comes up with the code for each of the steps. The first step is actually So in this case, the agent actually comes up with multiple steps, and I'll go over the code. In a second. So you can see in the first code, we are trying to understand, OK, what are the categorical features so that I know how to deal with these categorical features in my actual run. So I identified that there are 4 or 5 categorical features and I will be using 1 hot encoding. The agent uses 1 hot encoding to actually divide these categorical features into multiple steps. Now, and the agent has produced the entire code with 23 features and 18 features for total and then the rest of the features were categorical features created using one hot encoding, and it has created the model and the model is 81% R2 score, which means that it is able to explain the 81% variability. And finally it created the graph for the predicted LTV where it is able to show that. This was the LTV trend and these were the LTV trends. Like this was the predicted LTV and these were the LTV trends. So in this case, we created a simple model, but we could have gone and created a more complex model as well. And here we can actually see the feature importance where we can see that the low income level actually decreases the LTV, but if you have a high middle income level, it also decreases LTV. But if you have a high customer satisfaction score, it increases LTV. So you're able to see that entire code was created by the agent end to end. It went through the splitting the training and test split. It went through feature engineering. It prepared the data for one hot encoding, and then It actually did the analysis and gave you a feature importance. Now the same agent, you can use it for building data pipelines. If you're trying to build, let's say data pipelines or you want to run some query on S3 tables or on glue catalog, the same agent can actually write a Spark code or DDB code or Polars code to run those data transformations on using any engine that you prefer. And in this notebook we have actually preinstalled around 320 packages, like all the essential packages that you would need for. Your complex analytical task with including SageMaker SDK and we allow you, of course, the freedom of installing more things in case you would prefer. Now, if you want to learn more about these two agents, I would highly recommend scanning these QR codes. These QR codes will lead you to documentation. And you can, you can see more about like what are the other capabilities data agent has and how you can set up the Spark upgrade agent. In the Spark upgrade agent documentation, you can actually see the remote MCP server configuration that we have. So I would highly recommend you go in, you use VS code or whatever ID you have, you set up the MCP server and actually use this agent. To do the upgrade for your sample application and then use it to upgrade your production applications. Now, this is a wrap of our talk to lightning talk today on how you accelerate data engineering. And I really appreciate all of your time for coming in here and listening to me.