---
video_id: xIF9vncLY8M
video_url: https://www.youtube.com/watch?v=xIF9vncLY8M
is_generated: False
is_translatable: True
---

Well, good morning. Welcome to Reinvent. Uh, we're excited to be with you and, and hope you have a wonderful week here. There's a, if you've, this is your first time at Reinvent, it's a marathon, so pace yourself. Uh, you'll be exhausted by Thursday. But, uh, hopefully it's a great week for all of you. My name is Jeff Bartley. I'm a product manager on the Data sync team. I'm gonna be joined by Smithha, uh, who, uh, owns product management for a couple of our services, including Transfer Family. She'll talk about that as well. So again, excited to have you with us. So in this session we really want to show you how the services that AWS provides can help you modernize your manage file transfers, and when we talk about managed file transfers, we're talking about the ability to. Uh, move files at scale whether it's migrations or whether it's, you know, synchronization or whether it's part of, part of your business processes, but really enabling you to move files at scale quickly, securely, and reliably, and we put a lot of work into making it effortless to integrate our services into your business workflows and we hope you'll lead this session with ideas on how you can better leverage. AWS, uh, for your file transfer needs. So, uh, we'll start this session talking about some of the challenges that, uh, our customers as we go out and talk to them that they experienced today when it comes to managed file transfers, uh, some top use cases they're trying to address, and then, um, our approach to how we make it easy for our customers, uh, in these each of those use cases up there and as we go we'll weave in some real customer examples. Uh, we'll talk through some architectures, and I'll show you a demo of using two of our services, DataSync and, uh, Transfer Family web apps together so you can see how those can integrate, uh, with each other, and then we'll, uh, wrap up, leaving you some resources to try it out in your own AWS environment. So managed file transfers are critical to a wide variety of use cases across uh businesses and organizations and industries such as financial services, healthcare, life sciences, manufacturing, and many more, um, you know, unstruct when we talk about unstructured data. We're meaning things such as files like spreadsheets or machine generated data, AI models, reports, images, videos, all of that. It's everywhere. If you manage storage of any kind, you're probably well aware of the proliferation of this data. There's a ton of it out there. So enabling customers to move this data quickly and reliably is key to manage file transfers. And so with data increasing in size and volume and a lot of organizations being spread more globally, exchanging this data quickly and reliably has become more important than ever. And so as we go out and we talk to our customers about what they're trying to do, especially when it comes to file transfers, we consistently hear about some of the challenges that they face with their legacy file transfer solutions and maybe you face similar challenges around managing things like hardware and software renewals or responding to security issues that can take you know so much time and effort to deal with. And we're also hearing more and more from customers about how difficult it is to find talent that's familiar with these legacy solutions, um, you know, they need to manage them effectively, but it's becoming difficult to find people with the right kind of experience to manage this. So, uh, a variety of challenges that our customers are facing when it comes to managing these legacy MFT solutions. So over the last 7 years we've been helping customers address some of these challenges that I just talked about helping them migrate their legacy MFT solutions to AWS and taking away a lot of that undifferentiated heavy burden, the heavy lifting. So they can focus on the core competencies of their business. Data sync and Transfer family were both launched at Reinvent 2018, so just about 7 years ago, with the goal of making data movement effortless for our customers regardless of where their data is located. And over the years as we've talked to customers like you, uh, we've expanded our capabilities to support more data types, more protocols, and more ways of moving data quickly and securely. And a significant expansion of our capabilities came with the launch of B2B Data Interchange, which we'll often call B2BI, uh, enabling customers to automate the exchange and processing of EBI documents with their trading partners. And EDI is core to many workflows that depend upon managed file transfers, and BDBI addresses key challenges in this area around automation and document translation, and Smith is gonna talk about that a little bit later in this session. Now our approach to tackling the challenges of MFT really focuses on some of these key principles here. um, so our services encrypt data in flight and at rest, and we validate that all data is moved over the wire. Uh, bite for bite, uh, using checksums all along the way, um, automation is key to MFT workflows. If you've built some of your own, you know this, and our solutions integrate seamlessly with other AWS services like step functions, lambda, and EventBridge so you can build those automated workflows. Um, we also know that generating reports, uh, of file transfers is critical to meeting things like regulatory and your and compliance needs, and our services provide extensive logging to help you meet those needs. Now this focus that we have on addressing customer challenges is why tens of thousands of customers around the world have really benefited from our services in the MFT space, and you can see just some of the customers across a wide variety of industries. Now MFT workflows can cover a broad array of business specific challenges as I've already mentioned so often, uh, this requires a high degree of customization to meet the needs of your specific business needs. So this is where partners can really help and we've had uh a variety of customers who have told us that working with. Partners has really helped them streamline and accelerate the migration process of bringing their legacy MFT solutions into AWS leveraging services like Transfer Family and Data Sync and uh partners can really help them, you know, tailor these solutions, uh, providing things like customization, uh, automation, integrating automation with some of those other services that I mentioned to really again meet the the needs of their specific business. So as an example, these are uh a number of partners uh who are validated to follow best practices with AWS transfer family um so as an example, uh scale capacity down there in the right hand corner, um, they helped the city of Los Angeles and uh who was working on a file transfer system where they needed to enable their employees to be able to download financial records or maybe uh partners to access these records, um, and they were initially looking at a solution to run on premises. And they found that it was gonna cost them hundreds of thousands of dollars annually in costs and by working with uh scales capacity they were able to implement a cloud native solution leveraging the AWS transfer family, um, and it saved them a significant amount of money, um, you know, many thousands of dollars annually, and as a citizen of Los Angeles it's always nice to see my, uh, city using my tax dollars efficiently. So, uh, this was a, this was a great success story for the city of LA. Uh, leveraging a partner specifically uh to get this work done. OK, so as I mentioned, we're gonna cover a few use cases. The first of them is migrations, so we'll dive into that and I'll start by talking about AWS Data Sync, which was really built to help our customers uh streamline their migrations into AWS. So, uh, DataSync was built to move data quickly. It uses a custom in-house built protocol that moves data over a network, uh, in a parallel fashion, enabling it to move data very, very quickly at, at high scales and high speed. It's built to be secure and reliable. All data is encrypted in flight using TLS, and we check every byte that gets transferred with checksums all along the way, from reading from the source to writing to the destination. We're validating checksums all along the way to ensure the data is transferred securely and reliably. Uh, Data Sync can scale to massive sizes. We've got customers using Data Sync to move petabytes of data, billions of files, very large data sets. DataSync can scale to that, um, and then it's designed to be easy to use, so it's got built-in filtering, scheduling, um, and also audit logging, which is again, as I mentioned, critical for a number of MFT use cases. So Data Sync really focuses on moving data in three areas, and the first of these is enabling on-premises transfers. So Data Sync supports systems that can talk a variety of protocols. This could be like a NAS file server that can communicate over NFS or SMB protocols. Uh, it supports, uh, on-premises object storage systems that use the S3 protocol, and then it also supports Hadoop clusters. So if you're transferring migrating data from Hadoop into the cloud, you can use DataSync for that. And then at the other end. Data Sync works natively with a variety of our services, our storage services in AWS, including Amazon S3, FSX. We support all file system types in FSX as well as Amazon EFS. So you can use DataSync to copy data both to and from these various locations. A data sync also supports moving data between other clouds in AWS. Uh, we support a number of clouds including Google Cloud, uh, Azure Blob, Oracle. Basically, if a cloud has an S3, has a, uh, cloud storage that supports the S3 protocol, we can typically talk to it. So, and again, uh, you can use DataSync to move between, uh, other clouds and AWS. And then finally we've got a lot of customers who are using DataSync to move data between storage services within AWS entirely. This may be for backup purposes. This may be to move data around between regions. Um, using DataSync, you can move between any combination here. You could go S3 to FSX, EFS to S3, any combination of those. Uh, we support inre, cross region, and also cross, uh cross account. And all data as is transferred with Data sync again it's still encrypted but it's all writing over the AWS network. The data never leaves uh the AWS network at all in this particular situation. And so to help our customers really streamline their migrations, we launch enhanced mode, which helps customers move data sets with virtually unlimited number of files at speeds up to 8x faster than basic mode. Enhanced mode provides things like detailed metrics that help you track files throughout the data transfer process. It also streamlines cross cloud transfers. It allows you to move data between clouds without deploying any infrastructure. So we've had a number of customers take advantage of that, and they really like the ability to move data between clouds completely API driven. There's no infrastructure to deploy, so making it super easy for customers. So let's dive a little bit deeper into how Data sync helps with accelerating migrations. So if you've performed a data migration in the past, you're probably familiar with this kind of pattern. So most migrations go this way, where they'll start with an initial transfer of data. It's a lot of data that needs to be copied to the destination. And then you run a series of incremental transfers over time on you typically like a fixed period maybe daily every couple of days until you're ready to do your cutover, which is the point at which your application goes from moving your initial data set to moving the data set that you've migrated to. And Data Sync is really built to help with each of these stages, so it has a number of key capabilities that help make migrations simple and efficient. It transfers both data and metadata, which is critical when you want to maintain things like file system permissions. It also provides filter. That helps you ensure that only the data that you need to move is actually migrated. I can scale up to maximize your network bandwidth, and it's got those metrics I mentioned earlier with enhanced mode that can really help you understand the scope of your data and what was moved. And here's just a quick video of Data sync running uh in an initial transfer so you can see it's using enhanced mode right now and these metrics can help you really understand exactly what's being moved. So this is a data set of about 200,000 files. Um, you can see as Data sync goes, it's listing, it's preparing, it's transferring and verifying. This is one of the advantage of advantages of enhanced mode is that it's doing all of those in parallel. And you can see as it's going it's transferring and verifying you can see it's hitting a throughput speed of close to about 600 megabytes per second, but this is this is really nice for that initial transfer. It helps you see, yes, all the data that was actually found at the source was actually transferred all the way through and ultimately verified and getting good throughput and so you know, in under 30 minutes transferred close to 1 terabyte of data. So that was the initial transfer and then as you go into incremental transfers, Data sync also has additional capabilities there as well. So um after that initial transfer you can configure Data sync to run on a schedule, um, so that you can perform those incremental transfers until it's time to cut over. And running the incremental schedules uh uh sorry, running the, uh, transfers on a schedule, uh, is really important because it can help you estimate how long your cutover will take, which is, uh, frequent, uh, ask of customers when they're working with us, which is like, hey, when that I have to cut over because cut over involves downtime, like how long is that cutover gonna take? So when you run that incremental transfer with data sync that can help you estimate roughly how long you need for a cutover period. And so here's an example of an incremental transfer using that same data set. In this case, what you'll see is that Data sync is still listing all the files, is still finding them. But during the prepare phase what it's doing is it's comparing those files against the destination and in this case there was only 581 files that actually changed, which is pretty typical. And when you're doing an incremental transfer you'll find that most of the data is static, it's not changing as you go, so those are all marked as skipped, meaning they didn't need to be transferred or verified. In this case you can see it took a lot less time under 3 minutes um and so you know if this was typical of the change rate of my files I would probably estimate like a 5 minute cutover if you had more data or your change rate was higher, obviously it could take a little bit longer, but this is a good way to estimate how long you need for that cutover period when you're performing migrations with Data sync. And then this is a pattern that a number of our customers use when they really want to maximize their network bandwidth. They may have 10s of gigabytes or 10s of gigabits per second of network bandwidth, and maybe a single data sync task isn't enough to move data to totally fill that pipeline. Um, and so they can leverage this pattern which is basically partitioning your data set into multiple, uh, into multiple subsets of data and then running individual tasks to move that data, uh, in parallel and so we've had customers, uh, use this to move petabytes of data a day um and if you want to learn more about that pattern, definitely check out that QR code there that was a storage blog that was written by uh one of our solutions architects who's sitting in this crowd right now. Just letting you know, OK, um, so, uh, in fact we had a customer London Stock Exchange Group who needed to, uh, do, uh, a large migration, about 30 petabytes of historical market data from another cloud into AWS, and they leveraged a partner, uh, Data Art to help them scale out, uh, this migration. They moved, uh, 30 petabytes in about 3 months and. I remember when they were doing this they were moving close to a petabyte of data a day. That's a lot of data to move, um, but you know Data sync can scale and they achieved their timelines in a, in a, they were well underneath their timelines, so a great story for them. Another win too is that moving off of the other cloud and into AWS into Amazon S3, specifically S3 Glacier, they were able to see a significant reduction in their storage costs. They saw about 80% reduction in this case, so a big win. They were able to make their migration timelines as well as save a significant amount of money on their storage costs. OK, so we talked about migrations. Let's go ahead now and move into a 2nd use case around recurring transfers. And so whereas migrations are usually project based or it's a one-time data movement type project, there are other use cases where data is being moved on a regular basis, and we're going to cover some of these with customer examples throughout this section. So a lot of our customers continue to have data on in on-premises storage either because they haven't migrated the application yet or maybe they can't move the application and it will need to continue to run on premises for an indefinite period of time, um, but they still want to make a second copy of that data, uh, just, just to protect it and so they'll often leverage data sync using this pattern where they'll set up a data sync agent in their on-premises environment. And configure Data sync uh to run on a schedule and then just run it on a regular basis, uh, enabling them to uh meet their their data protection goals and using this uh data sync will automatically run on the schedule uh safely and securely moving that data anywhere it's needed. And a lot of our customers will often use this to uh make a second copy of data in S3, taking advantage of its uh of its of its low cost storage uh so it's it's a great pattern. In fact, we had a customer, Santos, uh, who needed to do something similar to this, and they, um. They produce their their their workflow produces terabytes of backup files on a daily basis, and they needed to make a second copy of that data necessary in order to meet some of their internal compliance needs and the customers started off as I've seen a lot of customers do, trying to write their own solution and they quickly ran into a number of headaches that Data sync was designed to address. Things around the overhead of managing and deploying scripts and dealing with errors and validating data along the way and so after learning about DataSync uh they were able to get it up and running quickly and vastly simplifying their processes and it really enabled them to focus on more of their critical business challenges and just get away from do it-yourself solutions. So a great story for them. Now I think you know at this point we're all well aware of the explosion of the use of Gen AI and we've uh we're seeing more customers where data sync is becoming increasingly important to their data pipeline, particularly when it comes to building out data lakes or on AWS for like AI training and inference purposes. So, um, these data sets can often consist of billions of files and petabytes of data. And uh Data Sync can help customers get that data into AWS storage services like FSX for Luster or Amazon S3 which were built to scale out and really achieve the levels of data movement that's necessary for a lot of these Gen AI workflows. Now another area where we see Data sync helping customers with recurring transfers is with machine generated data. And if you think about all the machines out there that are running in on-premises environments, they're generating tons and tons of data, these are things like genome sequencers, manufacturing systems. Lab instruments and hospitals, um, you know, our customers want to get all of that data into the cloud so that they can process and analyze it. They don't want to build out compute systems on premises, um, so Data Sync helps helps customers streamline these data pipelines and reduce the time it takes to get the results from their data. And this is an example of a common solution. That we see with customers using machine generated data. So this data is typically generated or it's generated and then it's stored temporarily in some kind of on premises storage system. It might be a file server like you see in the in the picture here, um, and they'll set up Data sync to copy the data triggering a job when the lab instrument job starts and they'll transfer the data into something like S3 and then when the job is complete, it'll trigger an event bridge notification. And then using that what they'll do is they'll then go and clean up the on premises environment, remove the data it's no longer needed, it's all up in the cloud, um, and then it makes room for new jobs that need to be run on premises and this way they can keep their on premises storage uncluttered and make it available for all of their processing needs. So Merck is an example of a customer in the life sciences space uh and they had the challenge of trying to reduce they were seeing high levels of rejects in their manufacturing processes. And um they in in working with AWS they realized that some of the AI services and machine learning services that we provide could help them address this challenge of this high level of rejects and so they, they utilize DataSync to bring in a lot of the uh reject data, the uh defect images that they were gathering across various sites. They brought that into S3 so it could be utilized by Amazon Sage Maker. Um, and then leveraging these services, Merck was able to, uh, root cause their issues, uh, optimize their processes, and they ultimately reduced their rejects across their product lines, uh, by 50%. So a great story showing how DataSync was part of the overall workflow of bringing the data in so they could be better analyzed in AWS. OK, all right, so now I'm gonna go into a demo of uh Data Sync and Transfer Family web apps working together, so. Uh, kind of building upon that pattern in life sciences that I talked about in healthcare where, you know, machines or lab instruments are generating data on premises and customers need to process that data in the cloud. Many times these processes produce reports that need to be reviewed by users in on premises environments. These could be scientists across the organization for maybe it's for quality control or other purposes. And using Transfer Family web apps, which we launched to reinvent last year, end users can now securely and easily access their data stored in S3. Um, and this would be something like generated reports. There's no need with with trans friendly web apps. There's no need to give your users access to the S3 console. There's no need for complicated third party applications. Um, so using these two services, DataSync and web apps together, our customers are able to streamline their machine generated workflows and pipelines and, uh, make that data really available to their global workforce. Um, so let me dive just a little bit deeper into web apps and what it is. So, um, so web apps again, as I mentioned, it's a, it's a, uh, it's a capability of transfer family. That, uh, you, you enable that gives you access to data stored in S3 but it uses a web application that runs entirely out of the AWS console so it uses AWS Identity Center for authentication and S3 access grants to authorize access to data in your S3 bucket, um, and it so it enables you to leverage your existing mechanisms. Um, it also, you can also customize web apps, uh, with your own, uh, branding, and, uh, it's got a simple interface that makes it easy for you to get your end users up and running. So what I'm gonna do is go into a demo now, um, and, uh, what I'll show specifically is Data sync, uh, using DataSync to copy data from an NFS server, uh, in, in the demo, and, uh, and then it'll it'll be copied into an S3 bucket and then with the data in S3 I'll then show how, how easy it is to leverage web apps in order to access that data in S3 and make it available to your end users. All right, so let's kick off the demo. So here um I'm logged into my NFS server. And you can see that uh my current directory is Mount NFS demo, so that's the folder that contains the the data that I wanna copy, and I've got an NFS export on my server, Mount NFS that'll be important when we come to configuring data sync. I've got 5 folders in there. I wanna copy the 2 archive folders up into my S3 bucket. Uh, but I'm gonna keep the, the prod ones, uh, on, on there for now. You can see, uh, what's in my archive folders. I've got a handful of files. Um, I've got a text file that gives a, uh, a listing of MD5 sums in the archive one folder, and then I've got this ignore.temp file which I don't want to copy. And so I'll use that to show how you can use exclude filters with data sync to avoid copying data that, uh, you don't want to copy. So from here we'll hop over to the AWS console. And this is the S3 bucket that I'm gonna use to uh store my data. So this is where I'm gonna copy data from my NFS server into this bucket. You can see it's currently empty uh right now there's no data in it, but it's called JB Web Apps. So that's, that's where we'll store our data here in a little bit. I'm gonna jump over to the Data sync console and I'm gonna create a task, and a task is what tells Data Sync how to move data from one place to another. So I start off by configuring a source location. And my source is my NFS server. I deployed an agent earlier, and an agent is a virtual machine that I use to connect to my, uh, on-premises NFS server. So I select that agent and then, uh, I specify the, uh, IP address or domain name of my NFS server and then the mount path, which is how DataSync is gonna access the server. So if you remember earlier I showed you that mount, a point which was mount NFS, so I use that. And then I'm gonna specify my destination location, which is an S3 bucket, uh, that's that JB Web apps bucket that I showed you earlier. So that's gonna be my destination location and then the next phase is to configure my data sync task. I'll give it a simple name just so I can recognize it when I'm working in the data sync console and then I'm gonna specify which data I want to copy. And so in this case I'm gonna use filtering. Uh, to add and include filter, if you remember, I only wanna copy my archive folders, so I'll specify and include filter that looks like that and then if you remember, I also wanted to exclude the temporary file so that, so I'm gonna exclude anything that ends in dot temp, um, so just copy those files and exclude the temp file, everything else I'll keep it as the same. I'll review it, everything looks good and I'll go ahead and create my task. So at this point I've created my data sync task and I'm gonna go ahead and start it and that will kick off the process of actually running the task to copy the data from my NFS server into my S3 bucket. And so while that's running, uh, we're gonna go ahead and hop over to the AWS Transfer family console and take a look at a web app that I configured already. So I already uh configured this web app server. Uh, I configured it with my identity provider. You can see up there that's the IAM identity center provider that I created earlier, and I have a user Jeff down there, and I've given Jeff access via S3 access grants to my bucket, my S3 bucket. And then I've got my web apps endpoint which is the actual endpoint that I will access in my browser to access the uh the actual web apps interface itself and that gets automatically generated for you and so this is what it looks like a very simple interface you can see my bucket is in there. I click on my bucket it's empty right now there's no data in there. Um, and so, uh, once the data is copied, we'll come back and we'll take a look at that. So we'll jump back over to the data sync, uh, to the data sync console. You can see we've copied our data. It's about 14 files, small amount of data, um, again, not a lot, but uh it was copied in a quick amount of time, um, but we can see that the data was copied successfully and then if we jump over to our S3 bucket. And we, we do a refresh. Uh, we can see that the data was copied in there successfully. I've got my two archive folders. The prod folders were not copied. There's the MD5 text file that I'll, uh, look at later, and then you can see that the temp file was not copied because it was excluded using that exclude filter that we, uh, specified in data sync. So he successfully copied the data over. I go and refresh my web apps environment and you can see it just mirrors the data that's in S3 again. This is because of how I configured the S3 access grants to provide access to my web apps, uh, to my web app. You can see I can download uh the file and if I open up that MD5 text it matches what I had on my NFS server so I can use that if I wanna look at any of the MD5 sums for any of the files there but you can see very simple utilized data sync to copy the data from on premises up into AWS and then from there leveraging web apps to access that data in S3 and again, the great thing about web apps, you notice. You don't have to go through the S3 console. You can easily enable your end users to access data stored in S3, leveraging single sign-on capabilities or whatever authentication mechanisms that you have available today. So again, a great example of showing Data sync and Transfer family web apps working together. And with that I'm gonna hand it off to Smitha who's gonna talk more about transfer family and some other use cases. Thanks, Jeff. Um, yeah, so let's switch gears a little bit here, right? Um, Jeff talked about DataSync, um, and a little bit about Transfer family. Now what if you don't have control over both ends of the file transfer? So let's say there's an application that's generating data that you need or you need to deliver data to an application that's outside of your purview in a business partner's environment, and that's where AWS Transfer Family comes in with its fully managed file transfers over industry standard protocols like SFTP and AS 2. There are a lot of use cases that customers use AWS Transfer family for today, uh, notably one is exchanging documents that are subject to regulatory needs like PII, PHI, HIPAA, you know, if you're operating in, let's say, financial services or healthcare, pharmaceuticals, oil and gas, a number of industries need to exchange data securely, and that's where transfer families' MFT capabilities are used. Second is ERP integration. Let's say you have an SAP implementation and so does your business partner, and you want both of these to talk to each other because maybe you operate a supplier and you need to get purchase orders from your manufacturer or your end customer. So that's where MFT transfer families MFT is used to connect these two applications that are cross business boundaries. If you are a content distributor, let's say you produce value added data sets, right, whether that data is, you know, for analytics or their media or software, you want an MFT that can scale and expand your subscriber growth as well as protect your revenue source. So Transfer families fine grain access controls helps you give access to just the data that your users are subscribed to. And fourth, if you are operating a manufacturing facility and you want to capture that data into S3 so that you can detect errors soon or run proactive maintenance, transfer families SFTP is commonly used as a data ingestion pipeline. So there are num way more use cases where transfer family is used, and this is, these are just a few examples. So looking at this architecture a little bit, Transfer Family helps you capture data from a number of different source types, whether it's manually written scripts like SFTP, FTP scripts, or you have non-technical users kind of like what Jeff showed in the demo with Transfer Family web apps who want to get data in through. Um, commonly used client called the web browser, right, um, third is a remote SFTP source, so you might be, um, processing healthcare claims or you're part of a, you're writing a payments reconciliation workflow and you need to talk to a clearing house's SFTP server. And finally, and also let's say you are using this protocol called Applicability Statement 2 and you need to interact with a partner's AS2 implementation. So that's where Transfer Family gives you a number of resources from servers to web UI to connectors to be able to get the data in your Amazon S3 bucket or EFS file system. So then after that, as the slide says, the endless, the possibilities within AWS are endless. One specific service that I'll talk about in a little later is AWS B2B data interchange for managed EDI integrations. So talking a little bit, we've invested a lot of time into making MFT event driven so that you can automate your file processing end to end. So let's take a quick look at this architecture. Let's say your business partner sends you a file over one of these industry standard protocols and you're using AWS Transfer Family. As soon as the file lands in your S3 bucket. AWS transfer family emits a very specific file transfer event that has all the context about the file transfer whether the transfer failed or was successful or was it a partial upload what was the user source IP where did that file come from, the user name, um, and then the file name do you get a lot of information um that is very that the transfer family knows about whether the file transfer was a success or a fail. And along with that you can easily kick off a step function workflow and as part of that step function you can tag the file, archive it for, uh, you know, audit for auditors later on. Um, a lot of our customers wanna do file format checks. So let's say you expected a CSV from your training partner, but they sent you a JSON and you wanna reject that file right away so you can make it right as part of that workflow. Pretty good privacy encryption is commonly used, so your trading partner encrypts the file because it was a PI data. As soon as the file lands, you want to be able to automatically decrypt the file, and soon after that, if the file traversed over the internet, many of our Customers wanna scan for malware. So if it, if you detect any, any malware, you wanna be able to toss the file right out, uh, right there, and then only let clean files be used by your internal business systems so you can run all of this within step function and the beauty of this is you combine Amazon EventBridge so that you can run a very, you can tailor your processing on a per user or a per source file basis. Now there are 2 components here that help you do that customization. One is our integration with EventBridge, and second is the use of Dynamo DB to store a lot of this information where the service can read from. So if you look at this, the service emits these detailed events, right, as you can see, this is just a snippet of events that is emitted where you can parse these events and run rules-based processing on, hey, if the file came from this source IP this is what I wanna do, or this file was a partial upload. I don't want to take it in my data pipelines because it's gonna mess up my calculation so you can run a lot of these rules right here uh as part of your step function workflow. And because the service service recognizes your Dynamo DB tables, you can use it to store the username, the credentials, what data they have access to, uh, and what source IP you expect them to come from when they access your data, and most importantly, what rules you want to run, right if this trading partner is from Company X and they sent a JSON. What do you, how do you wanna process it? So super customizable at the end of the day on, you know, what kind of processing you want to run in AWS, um, so there's, if there's a QR code there with a self-paced workshop where you can build the exact same architecture that I showed you on the previous slide and automate your MFT for greater scale and flexibility. So I want to talk about FICO because they built this exact architecture. So FICO, uh, you know, helps businesses, uh, you know, globally in around 80 countries to do anything pretty much from protecting credit cards from fraud to, you know, improving financial inclusion or even increasing supply chain resiliency. Now as a global leader in credit card analytics, a FICO processes a massive volumes of sensitive data using an MFT. So that means secure file and efficient file transfers are super critical to their operations. Earlier they were using a legacy MFT implementation. And with that, we had to uh manage a whole lot of infrastructure, uh, you know, like SFTP servers, storage, whether it was in use or not, that resulted in a lot of management overhead and costs. Now once FICO has switched to AWS stands for family, that's eliminated a lot of infrastructure that they needed to manage before. They're able to deploy their infrastructure MFT globally using infrastructure as code within minutes, which was not possible before, and they've also lowered their TCO and not just lowered TCO. Now they can calculate costs of running an MFT more granularly and more accurately across different business units at FICO. There's a QR code there that details F FICO's MFT uh transformation journey that I'd love for you to check out. So let's talk about another important use case for many of you, uh, who need to submit, uh, you know, regulatory filings, uh, as part of either it's you're in pharmaceuticals or you're in the food industry, um, what you wanna be able to, what our customers, uh, have been able to do using Transfer family is super streamline that submission. So let's say a file is generated by an application that you're running in AWS so let's say it's a PDF of, uh, you know. Of medical records and you drop that file into S3. Once you drop, transfer family can listen to that events and send those files over to a documents portal or a documents server hosted by the regulatory body like the FDA or the SEC. And once they receive the file, if you're using AS2, uh, you can receive a confirmation of receipt from the regulatory body so that you can audit, so you can archive that receipt for future purposes so you can, you can automate this. Whole thing using Transfer Family and Amazon S3 that way the whole submissions process, you know, scales up you know as your business grows and also is reliable so you can make sure that you know you're submitting these submissions on time. So let me talk a little bit about Trustworld's Food Logic platform. Now they help their customers manage supply chain and traceability, uh, challenges, which is super important, you know, for customers who operate in the food and drug industry. Trustwell's Food Logic platform uses AS2 SFTP because that's the way they get messages from their supply chain partners. Earlier they were also using a legacy MFT implementation. And what that, what would happen is when the license expired, the whole MFT, their MFT would go down, uh, or if there was a certificate expiration, they wouldn't even know when that happened and that bringing down the MFT meant their business was disrupted for days. So what uh Food Logic did was they consolidated all of that file transfers using AWS Transfer Family. So now they run their AS-2 and SFTP using our service. As a result, they have improved availability and up time. Um, they're able to onboard new relationships, you know, in a much shorter time than before, uh, and because it's a managed service, they're, you know, automatically up to date with the latest security and compliance standards. So now let's talk about when processing is part of the file transfer, and electronic data interchange is a major driver of that type of processing used in different industries. So if you're in healthcare and you're doing claims processing. Uh, or you're in transportation and logistics and you want to track shipments, um, or you're in manufacturing and you have an SAP implementation and you want to send a purchase order, let's say to your, uh, your supplier's ERP. So this is where EDI is prevalent because it's an industry standard. Some of the standards are X12, EDA, and in healthcare, HL 7B2 is very common. Now while EDI is used as part of moving data, the problem is it's not compatible internally with your business applications or your data lakes as you want to build these data pipelines out for, let's say even AIML just like Jeff showed you earlier. And that's exactly the reason we launched AWS B2B Data Interchange at Reinvent 23 um to automate the validation and processing of X12 documents to and from JSON and XML. JSON and XML are common representations. formats that you could use with your business systems and data leaks. The service gives you extensive logging and monitoring. Errors are common in this, uh, technology, so you wanna be able to troubleshoot faster before they become a business problem for you. Um, being an AWS managed service, uh, the service automatically scales to your needs and is up to and it's up time and highly available and with a high degree of uptime. My favorite feature about this service is it empowers you to use generative AI to generate mapping code between X12 and your custom format for XML and JSON so you can use so that really reduces the time and effort needed to generate that mapping code. It's a one-time setup so that automatically documents are translated between these different formats. So let's take a look at an architecture diagram again, right? So let's say you how you interact with us in our trading partner and they send you a purchase order like X12850. Many of you might have heard of that. If you receive that document over AWS transfer family, let's say SFTP, FTPS, or ES2, an event is omitted, as I mentioned earlier. Now B2B data interchange listens on that event and your bucket to automatically pick up that file. Transform it, tell you whether it's good or bad, give you acknowledgment files, and then place the processed file into another S3 bucket or same, but it's your S3 bucket so that you can use that data to either build your data lake or feed it to an application like SAP or your transportation management system or your claims processing engine and the and there are two things that I really wanna call out about this architecture, right? One. You're combining the time-tested industry formats like SFTP, ES2, X12 with modern paradigms like event-driven and generative AI so that you can achieve scale, flexibility, and lower your total cost of ownership of managing an end to end MFT. Um, the second thing I wanna talk about, may, uh, it's, which may not be very obvious, is you've leveraged now your trading partner relationship into a data pipeline. So your trading partners, you know, your, your trading relationship is now doing double duty in giving you a data line into giving you a data pipeline so that you can get real-time insights into your transactional data. So, um, there is a video we just published last week, fresh. Of the press, um, that where you can, there's a demo of this architecture where the business application is an SAP. So, um, I'd love for you to watch the video. It's just about 8 minutes long and it talks about how you can build this end to end architecture, uh, using, um, SAP as the, as the ERP. Obviously it'll work with many other business applications, but the demo talks more about focusing on SAP. Uh. So Biscloud Exports is an IT consulting, um, you know, um, company, you know, who who automates, accelerates, and empowers different customers of ours to scale and innovate with confidence. So, uh, a customer in the food and beverage industry approached Biscloud Exports, um, because they were using a legacy EDI solution that was, you know, very difficult to manage and also extremely expensive. BisCloud experts stepped in, um, you know, developed their expertise around our services and helped this customer, uh, move from, um, you know, from the move their 100+ trading partner relationships and 250,000 message volume per month from their legacy implementation to AWS B2B data interchange in a very short period of time. Um, they also avoided, you know, renewing their license, and now they save over a million dollars annually in licensing costs as a result of this migration. Um, also cherry on the top is they, they, they're able to. Onboard trading partner relationships much faster as you can see from the slide in days because now the customer controls the entire process by themselves. So I just wanted to give a quick shout out to BisCloud experts who really help this customer in time of need. OK, cool, um, I know we talked about a lot of concepts today, um, just to quickly wrap it up, here's my request, this is the time, pull out your mobile phone, scan that QR code right there. And send an email to your AWS account manager saying, hey, I attended STG 361 with Jeff and Smither, and I learned about how making file transfers super effortless can help me unlock innovation in my company. I learned about 3 use cases and arch and architectures for each of these use cases and I wanna easily meet my compliance and regulatory needs so I wanna get started today and I really hope to hear from your account manager so I can so we can help you with that. Few more sessions later today, tomorrow and Wednesday where each of these sessions will dive into the services that we talked about so um you know if uh I'd love for you to attend and uh learn more. Yeah, and finally thank you for attending. Um, I'd appreciate if you can fill out the session survey and leave us feedback so we can keep refining on our content. Thank you.