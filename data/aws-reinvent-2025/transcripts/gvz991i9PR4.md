---
video_id: gvz991i9PR4
video_url: https://www.youtube.com/watch?v=gvz991i9PR4
is_generated: False
is_translatable: True
---

Thank you, Tamara, for a very powerful and warm introduction to this panel. Before I introduce my panelists, I want to share a small story with you. Picture this. A 52 year old woman walks into an emergency room. She's complaining of nausea, radiating pain in her jaw, and unusual fatigue. The attending physician runs her symptoms through an AI power tool that has been designed to diagnose early detection of cardiovascular risk. The system tells that there is no risk or if she's a low-risk patient. She's sent back home and advised to take rest, manage her stress levels. 3 days later, she's back in the ER, this time in an ambulance, and experiencing a heart attack. The AI based system completely misdiagnosed her symptoms because it was built on a data set that was predominantly male subjects. It missed her symptoms completely. The technology was great, the intentions were good, however, the outcome was devastating. Because empathy was missing. But here's where the story shifts. Women technologists and researchers saw this as a pattern. And they refuse to accept the inevitable. They ask questions, whose symptoms are we building for? Who is represented in that data? Who are we building for when that person is not in the room? When technology is guided by leaders who combine technical excellence, along with deep empathy, the result is we just don't innovate, we transform for what's possible for humanity. Let's talk about how we will rewrite the code and build a future with AI that's inclusive, intentional, and truly empowering for all. With that story, I wanna welcome our panelists here. Please join me in welcoming Kavita Prabhakar, US AI and engineering leader at Deloitte. Kavita co-leads Executive Women and Technology Initiative for Deloitte CIO Program. She serves on the board of Girls Who Code. Kavita has been recognized as one of the fortune most powerful women, NextGen, and as Crane's notable minorities in accounting, consulting, and law. Thank you, Kavita. My next panelist here is Mackenzie Hessock, vice president NFL, responsible for player health, safety and strategic innovation. McKenzie is an experienced sports injury and orthopedic epidemiologist and a PhD in injury epidemiology. Thank you, McKenzie for joining us. And our very own Sherley Stanley, vice president AWS Professional Services for North America, responsible for helping customers realize their desired outcomes within the AWS cloud. Sally's a professional journey centers around building large global teams for professional services. She has been recognized as a top female leader in consulting. So one of my favorite Amazon leadership principles. I think big. So I want to start with thinking big. I want to ask all of you, starting with you, Charlie. If you had a magic wand. What is one systemic barrier that you think AI truly has an opportunity to eliminate? You know, one of the things that I was raised with Sonu is education is one of the most powerful enablers for anyone's future, and it has generational, multi-generational impact. And so I would focus on education. One of the things that really excites me about AI is access. It's access to knowledge, access to instruction. It's available through the Internet, of course, but across language. Barriers and I think that's incredibly important. We can take very sophisticated content and make it understandable for different age groups, different geographies, different cultures, different places and so for me we have this ability to bring a tremendous educational system. To the world, great for that, Kavita. You have a point of view. Absolutely. Uh, first of all, thank you all for being here. It's a pleasure to be here at AWS Reinvent every year. Um, definitely the most current topics are discussed here and for the lady who talked about wanting to come here to see women in tech and women in cloud, trust me, it is super important that you stay the course because. I'm on the board of Girls Who Code and there's so many young women looking to people like you who are absolutely willing to trail base the plot. So thank you for that. Thank you for being here and recognize you're doing it for others like you're looking to this panel to do it for you. So, uh, definitely wanted to open with that. The second thing I'll say is, you know, when we talk about think big. I think it's so interesting, um, my son wanted to solve teleportation, uh, because I was a traveling mom and he wanted me home very quickly. I don't think AI is gonna quite do that, uh, but if there's one thing from a systemic barrier perspective that I'll talk about, it's probably in financial literacy, um, and what I'll tell you is, um, my grandfather used to tell me. Let your money work for you when you sleep and when I think about the concept of financial literacy and the ability to reach people of different economic stratas uh with financial planning, financial education. Uh, it is, it is a great thing I believe AI can do for our future. Um, it can reach, like you said, language barriers, different, um, user styles, different risk appetites, very similar to the education point you made, but financial literacy, I think, is. Super important for everyone to reach a lifestyle that's better than today potentially and equalizing that I think can be pretty huge for us um so that's something that I think about when I think about uh the future and you know the equity in the system. Yeah, I think, you know, echoing what Kavitha said, all of you being here today is so important because each of you will have your own perspective, and that will help us grow just in the same way that all three of us are going to have our own perspectives. For me, I think when I think big, I think about healthcare and I think about access to healthcare in particular. Um, I grew up in a rural town in Wisconsin and access to healthcare was very challenging. Um, we just didn't have access to good hospital systems and doctors close by, and I grew up in a family where we have rare diseases, myself included, and so getting access to health care when you had something that was rare was infinitely more challenging, um, and that happens everywhere in the United States. And so for me, where I think AI can really break barriers is by helping. Um, folks who need help access the right healthcare, um, resources that they need, and also I think there's a huge opportunity for pattern recognition for rare disease detection that can help the 300 million people worldwide who are living with rare diseases that may, may not even know that that's what they're struggling with. And so I think there's this huge opportunity for AI. To really impact how we access health care and I think that comes from a personal place for me that I grew up in that situation and that that impacted me, but that's why it's so important that each of you are here because you each will have your own story that will then um be infused into the work that you're doing that helps us have an impact more broad and more broadly in all these different areas. Thank you and that's very bold and very promising. But I know that technology doesn't solve by itself. It needs intentionality. In your own personal journey, what has been that owner moment for you where you saw an AI bias, where you felt that this was not really built for keeping me in mind? Charli, do you want to share your point of view? I think there's going to be so many um that are what I would call medium to small examples for for me. I I think um I'm left handed and the whole world was not built for left handers either so I I have this recognition of it's just an unconscious bias. And, and you live with it. I think we run across that with AI in so many places you called out some in healthcare we see it in education, we see it anytime you call out images that AI creates, right? It shows up in lots of places. We know why because of the, the data sets, the history, we we, we know why it's there for me it's just a really good reminder to be intentional and to remind AI. To check for all of those biases with intentionality and improve the outcomes that we get. So when I think about. The reality, Sonu, it's being mindful of the unintentional biases and thinking like every time you see one. Make note because we can improve the outcomes for the future if we make notes along the way and provide AI with that right instruction. We have an opportunity to accelerate the correction of bias, but everyone brings a different perspective to it. At Amazon we always have a leave an empty chair for our customer mentality. What would your customer say if they were in the room? If we extend that a little bit and think about that intentionality around bias and everybody contributes, we have an opportunity, like I said to really correct for some of that bias in a way that we probably couldn't have without AI. Of that you wanna build on that? I think, uh, you hit on all the key points there. It's interesting, you know, we are also in the same moment when we think of oh no moments encouraging experimentation, right? We want experimentation. We want fast failure, um. If you do, how do you then say, how, how do we handle oh no moments because we will have them, uh, we will absolutely have them, but we have to keep on the path of experimentation. And keep it in controlled environments as much as possible to test to fail, to learn, to address intentional and unintentional biases and to repeat and I think the culture of experimentation and fast failure is gonna. Be extremely important in this day and age, so I have quite a few oh no moments. We did a marketing, um, solution that focused on, uh, customer personas and full digital campaigns and we were so eager to release it for one of our clients. Because speed was, you know, the name of the game. Speed became the name of the game, so it, we did release it only to recognize that the data set was very focused on one ethnic group and didn't speak to almost 65% of the client base or the customer base of that particular organization now. What happened, yes, it was a oh no moment, but with testing, we quickly realized that that didn't work and we applied the techniques of, you know, really changing. Uh, the base set of data. So I think the question that comes up is speed has become a huge part of the pressure in the system. How do you then say, OK, do I really understand my cost of error. Do I assess my cost of error? Do I convince my board of my cost of error and recognize that making errors is going to be part of the culture and how do we catch it fast enough uh to address it in the system. Yeah, I think for me my own no, oh no moment was actually more of an aha moment and it was understanding how much responsibility I had as a programmer back in the day when I was in, um, pursuing my PhD. I was working with a lab on um building a predictive model to try to help identify patients who might be at risk of getting progressive osteoarthritis in their knee, so they might be progressing in their arthritis journey with their knee and there was a new therapy on the market that we could offer to patients if we knew who to identify. If we could identify those those patients. A time could we find a way to um get them that treatment earlier? And for me what I realized was I was the analyst on on this project and we built a large predictive model and we were really excited. We actually found a predictor that identified who is likely to get progressive osteoarthritis and it was fairly easy. It was that we saw that people who. reduce their walking speed year over year, that sometimes was an indicator that they had progressive arthritis before they got an X-ray or before they had symptoms. And so we found, we found this, um, finding with the data and thought, great, this is something that we can implement in a clinic. You can imagine going in for your annual physical and doing a walking speed test. And if you start to reduce your walking speed, maybe they send you for further treatment. But what I really realized and why it was such an aha moment for me was that I built the model and I knew the methods that I use and I knew how to build a predictive model, but I also knew it wasn't a causal model and that was so important that my audience might not understand the methods I use. And now my example of that you reduce walking speed is probably pretty logical. You understand that it's a. Predictor, a signal of something else going on in your body. It's not something that caused the osteoarthritis, but you can imagine the logical fallacy that people go through all the time where they say, oh, how about just speed up your walking speed and then you won't get arthritis. That doesn't make any sense, right? But that is on me as the analyst, as the, as the person writing the code and designing the study to understand. How the, the, um, results can be interpreted and how they can be applied and I think that for me was not so much an oh no no moment although a little bit more of an aha moment that we have such a responsibility as the ones that are implementing these methods to make sure that we understand exactly how they can be interpreted. And so to, to the points that have been made, there are always going to be gaps in data sets, and they're not going, not necessarily going to be completely representative, but we have to be able to understand those limitations and interpret things that make sense of that for the, uh, the user of whatever we're, we're building. And so for me, I think that was the aha moment was that. I realized that it is the responsibility of those who are building the code who are creating the methods to make sure that these products are used in the way that they're intended, um, and that there is a risk there of misuse if we, if we don't take if we take that responsibility lightly and so I think that is both a huge responsibility and really exciting because now all of us who are in this. Room that have these different perspectives can use that responsibility to make sure that we're creating things that are um applied in the way that is intended to help better not just health but lots of areas that that's a great great story McKenzie and thank you for sharing that you clearly had empathy when you're building that solution, right? And you touched upon something about the data, right? And this Kavita, this is my question to you because you, you work with a lot with customers and with data. We know that generative AI system learns from the data and the data from the world that it has been versus not necessarily the world that it should be, right? When you're working with solutions with your customers, how are you thinking about this data gap? How are you thinking about representation? So, I think it's a really tough question and you answered part of it too. You touched on it, which is, you know, we get into the mode of saying. What is the data telling us, right? Um, it's based a lot on what it is today versus what it should be, perhaps, and that's the nuance I think that gets hard to understand and measure because there is no perfect state. Um, that you could say, OK, this is the destination state that has the right level of representation, that has the right level of thinking for left-handed people and right-handed people, right? Shelly? So it is, it is hard to get to that perfect data set, but. I think the key here is to continue to understand awareness is what I will tell you is what we focus a lot on when we're working with our customers and clients. What is your awareness of the patterns that exist in the data that you're using? What is it that um is overrepresented or perhaps underrepresented, right? Both are things you'll have to um keep, keep a close eye on and close. Understanding of um if there are overrepresented voices, how is it impacting your design? How is it impacting your thinking if there's underrepresented voices, how are you going to bring those voices to the table, um, such that they are not we're just not rewriting algorithms but we're rewriting the tables at which these algorithms are written, right? So those are the big things I would say that we are. Consistently pushing on, but that also doesn't take away from the basics, right? You do need good data you do need high quality um information you do need um the ability to constantly keep it up to date you need customer insights constantly refreshing what it is that uh you are perceiving to be the problem sets you're going to address all of. Those I think are super important and those basics cannot be lost um I think there was probably a phase where data readiness for AI was getting dismissed as not important. I do believe it's back in the conversation because we're learning that without uh focusing on data readiness for AI you're not able to build solutions that are truly scalable for the variety of customers you do serve. Great Sherley, I wanna talk about intentional design. AWS is a leader in AI. Can you tell us a little bit about our responsible AI practices and as a leader in AI, how are we building bias detection capabilities in our solutions? The, the AI suite and I and I say suite because it's, it's really a number of different products and it's a number of different mindsets that you bring to bear so there's parts that are technology, the bedrock suite of services is very, very powerful as technology goes when we think about responsible AI we think about. Observability, right? We want to know how decisions are made. We want explainability so we can follow the reasoning and determine whether it's accurate or suspect, right? We have hallucination, guards, guardrails that we put in place. We have bias detection, guard rails that we put in place. We have information security that we build into the solution. There are so many things that are there. But we have to use them. And using them properly is a journey because as, as we were saying, we discover something and we have to update it. We'll keep watching and then we'll learn something new and we keep updating it. Now, the technology gets smarter and it can start to update itself with permissions and allow you to do that, right? And so this rapidly evolving journey that we're all on to provide safe AI is great. And there are bad actors in the world we all know that there are bad actors in the world and so, so this is the ongoing back and forth and and tension that we all have to be super mindful of. I think someone um and in the beginning said I'm worried about our critical thinking capabilities. It really reminds me when I think about the importance of this journey and the constant updating and learning, I think it requires humans to be on their toes actually because all of the things that we see as suspect are probably. The ones that humans will recognize as suspect or not right, and that is a tremendous amount of critical thinking. I think the things that are less tricky. The technology will take care of it, it, you know. It reminds me of maps. Map software did not exist when I was learning to drive. I had this atlas stuffed in my door in the car, and I was not a good map reader, and I certainly was not a good map reader while I was lost, right? And I'm very thankful for map software. It is a savior for me to get from place A to place B. But maybe that critical skill of map reading. Isn't the thing. That is focused on critical thinking. I think different skills come and go, but that need for us to. Be responsible with AI and check it along the way. I don't think that goes away. I think it evolves and the things that are tricky stays at the top. Great, yeah, absolutely, Kinsey. I think everybody wants to know what are you driving as innovation specifically at NFL? What opportunities exist between data and empathy to support well-being of athlete more holistically? Can you share something with us? Yeah, I think it's a really exciting time to be in, in player health and safety at the NFL. We have, um, really we've been on a decade long, more than decade long journey on player health and safety to really to really collect and, and gather data that can help us understand the athletes experience in order to reduce injuries and improve their player. Health and now we're at this point where we have all of these different data sources. We're very fortunate in sports they collect a ton of data on things like how the players act on the field and and what their outcomes are in games and we collect a lot of data on player health through an electronic medical record and so we're at this point now, um, starting in. In 2019 we've been partnering with AWS on the digital athlete, which is an AI backed platform where we synchronize all of those data sources together. So the goal is to really understand the 360 degree view of what an athlete experiences on field through data. It's essentially like a virtual laboratory that we can test different, um, different, um, opportunities to reduce injuries and to improve improve player health using data. Um, and that is an incredibly exciting point to be at. We now have a, have, um, the opportunity to have the data synchronized, and so now what do we do with that data? How do we actually action it? And so I think that is that is the key is understanding the player holistically. How can we stitch all of these data sources together so that we see not just a silo of here's what they did in in a game or here's what they did in practice, but how can we put that all together and not only see their experience on field but also their past medical history and what they're experiencing in, in, in life and put that together to figure out how can we then. Um, harness the data to be able to be able to identify injury reduction opportunities and so for me I'm so excited to be at the NFL and player health and safety right now because I think we've already had an, um, so many wins in terms of having the fewest concussions on record last season. We've changed the kickoff, um, and it's the first player health and safety backed rule change. We have, um, incredible innovations in equipment with. Helmets, um, and we've had so many successes already, but now we're at this point where we have the data synchronized together and we can do even more. And so I'm excited to get to the place where we are now looking at an individual level on how can we, um, use the data to guide, um, player specific injury reduction initiatives and how can we give the data back to our practitioners who are on the field who know these athletes better than I do as sitting at a computer, um, at the end of. but they know these players. How can we give the data back to them in a way that they can, um, make smart decisions about how to improve their players' health and so I'm really, it's, it's really a great time to be at the NFL and it's been a really exciting opportunity to be able to use data and to use some of this technology to improve, um, the players' experience. Can I ask McKenzie a question that's fascinating to me. So do players ever reject the data? No, it, it's interesting. So we, um, have a very data driven league and I think it's come from a number of years of gaining trust and sort of talking about empathy is really understanding the users and how they wanna receive the data and so it's true that sometimes you get pushed back. I remember when we implemented the guardian cap. I don't know how many football fans we have in the room, but we have a guardian cap that goes over a helmet in in practice and we implemented the guardian cap. And asked players to wear it in practice because the guardian cap reduced head impact, um, forces by about 10% if you were wearing the the guardian cap and 20% of you and the person that you collided with were wearing the guardian cap. We asked players to wear this and the first, the first season we had it out there, I remember a player saying it can't fit through the door. It's way too big on my head. And so you have to understand their perspective and really show them the data and and advocate for why does that matter. And so, um, I think it comes back to this empathy piece of our players are very data driven now. They will actually sit and look at the evidence and they'll say, OK, I understand it's a 20% reduction in magnitude. I'm more likely to, to, to, um. Use that data and to to adopt it, but it's also hearing them out, hearing coaches out and understanding what are they looking for from the data and what are their perspectives and sharing, um, the results in a way that really makes sense for them. So from a player perspective, from a coach perspective, from competition committee, from our, um, league executives, we've had a ton of buy-in. But it's because we've, we've gone through more than a decade now of really hearing them and, and showing them the evidence that exists and it being a two-way street that we're able to share that data and ultimately have these successes with injury reduction. What you said there, the whole concept of trust, I think is huge because you've built the trust in small bits over time. And I think that's huge in anything we do right? and the solutions we develop. I think the trustworthy AI focus is super important by the way, it doesn't have to be AI like whatever tech we build, whatever solutions we build. I love the way you said it's an adoption journey and you have to stay patient with it, hear why it works, why it doesn't, and keep building on it. That's a great story, MacKenzie. We're very focused on the implementation at the end of the day. Our data and our our research is only good enough if we can implement it and if we can gain the buy-in from, from the end users to actually action what we're finding and so we've focused as much if not more on that translation piece. How do we explain what we're seeing? How do we really make it land with the right audience? So you said something very, very impactful there, right? Technology can be great. But if it's not intentional it's it's really of no use, right? So we touched upon biases and data we touched upon empathy a little bit, but let's let's dig in a little bit more on this, right? Um, there's a general myth that if you have to be a great builder you need more logical skills. We don't really talk about empathy we don't talk about emotional intelligence, right? In the space of technology, how do you really define emotional intelligence? What does it really mean? And why is it becoming important or relevant today? And someone spoke about critical thinking. Why is this important in the AI revolution? Do you want to share your thoughts, Kavitha? I'll, I'll start. So this is interesting. Discussion, right? I think the part that we haven't discussed much in the AI revolution is this word accountability, um, and I think about accountability a lot because. We are continuing to automate. We're continuing to really use technology to increase speed to market. But There's a lot of democratization of um technology in in an age like that. I believe the value of human potential just went up and what I mean by the word accountability in this setup is I think we haven't thought through where does accountability lie. As we continue to detach more and more from things being built, products being built, where does the accountability lie? And the reason accountability matters is because at the end of the day. It's gonna be extremely critical to have empathy, judgment, accountability as core skills that support all this automation that's coming, all this technology that's coming in my opinion, I believe it's super, super critical and the critical thinking that the gentleman talked about is. Extremely important as you think about new systems of governance, new systems of really understanding, uh, what is happening, who's accountable for the outcomes that potentially a probabilistic technology versus a deterministic technology put out there. So in my mind we have to keep pushing. On what are those governing structures, what are the human skills, the human edge is the competitive edge, right? What are those human skills that are going to stand out tomorrow. That will drive and sustain a very well governed new technology, um, system that is getting built today. Charlie, what are your thoughts on that? You know, I, I think we're at a very interesting time. If we go back a few 100 years and we think about the amount of change humans went through in their lifetime. It actually wasn't that much. When, when we think about the amount of change that we as humans have experienced over the last even just 50 years, I mean, if you take the whole last century, it's tremendous. 100 years ago, 1925, imagine what life was like versus in 2025, the pace of change that humans. Lived through is unprecedented. And I think our ability to absorb that change, right? is harder and harder. And so we. We go through in what I call, we skim. We skim, we get the, the essence of it and we move on. You said it already. Speed is the new currency, right? With AI speed matters. Speed matters more sometimes than money. And so our ability to keep up with that change is harder too, and we need assists or assistance to be able to do that. So I think it's a little bit of a um necessary that AI is there to help us to keep pace with that change or new technology is there because it's, it's a cycle, it feeds each other, right? One of the things that I think about around emotional intelligence is what do we need help with and what is innately human. As humans, we are emotional beings. Not purely logical beings. We lead with emotions, right? And anybody that's, you know, truly in tune with strong leadership knows to appeal to the emotional side versus the logical side. I have yet to see emotional technology. And so when I think of emotional intelligence, I think it really is thinking about the people and the role that technology plays to assist us along the way. And how we navigate that and where we need the most help from humans to stay human and, and I think that that's not gonna go away. At least I can't imagine it yet. I hope not. So the most innovative teams share one common theme, which is psychological safety, right? The belief that it's safe to take risk, admit mistakes, fail, and start over, especially when you talk about speed, right? What approaches do you take as a leader to foster emotional intelligence within your team? Kenzi, would you like to answer that? Yeah, I think, I, I think this concept is so important, and I, in my previous role, I led a team of about 30 scientists, and they were about 90% women, and this concept of, um, taking risks and being afraid to fail is really challenging. Um, it's really challenging to develop that psychological safety on the team, but it's so important, and, um, I think for, for me a couple of things that we did to really, um. Try to develop a psychologically safe environment where we can innovate without being afraid of taking those risks and failing or of making a mistake along the way was one, we talked about it a lot. We talked about it on the team about how scary it can be to make a mistake and how scary it can be to fail. Um, and we normalize it. Once you realize that everyone else in the room feels the same way, that they're afraid to admit that they had a bug in their code that they didn't notice or that they're afraid that they tried this new software and it, it turned out to not work the, the way that we thought it would. Once you realize that everyone else kind of feels the same way, it, it gets a little bit easier to take those risks. We talked a lot about. What did we learn along the way and how did it make us better? But I think my favorite, um, tactic with my team was reframing the whole experience and talking to my team about how all of these experiences are ultimately developing them into leaders. So even the worst experiences that they had where they made a big, big mistake and maybe the person they, they told it to didn't react the way that they wanted. What can you learn about how you would, you would have reacted as a leader? How can you reframe the experience as something that was not a bump along the way, but in fact necessary in your growth of however you want to grow as, as an employee and so that to me was my favorite way of, of kind of normalizing these experiences we all go through them. We're not going to get to the places that we are without going through bumps along the way. And that does mean that you have to be a little bit brave and and not afraid to fail and not, not take those failures so seriously, I guess, and sort of be able to reframe them and learn from them. Um, and so that for me was sort of the way with a team, particularly of younger women scientists to kind of get them to feel brave enough to innovate and to make those changes because at the end of the day that's really what made us great. That was why our team excelled and was able to break barriers in terms of injury reduction, was able to break barriers in terms of being women working in a sports environment. Um, it was because we took those steps and we kind of got over that fear of making the mistake. Charlie would like your thoughts on that. Yeah, I, I think I love that it's embrace, um, embrace that for, for me, for psychological safety, I think there's a couple of parts. One is as a leader being willing to listen is so important. Listen and ask good questions. I have found that. The people I work with, truly amazing people. But boy are we hard on ourselves, like really hard on ourselves. And so when, when you have a situation, when you're your own worst critic. And someone tells you. But I would never do that. I would never judge you the way you're judging you. Like it's eye opening. Oh, I wouldn't, you know, do that to anybody else either, right? And people are allowed to make mistakes and learn from it and sometimes just having that conversation to say it's OK and. And as a leader role modeling that as well to share vulnerabilities and mistakes and being brave to say I have no idea if this is gonna work let's try it and if it doesn't we'll figure it out like I I think there's a mindset that goes with it, but to me listening, talking about it and and creating the opportunity and just reminding ourselves that it's OK to not be perfect. It's OK. Yeah, I think that's so important too. We I. I come from an environment where, um, my leaders said it was OK to try and fail and, and try something new again. Absolutely. I was gonna add. I think the only thing that both of you didn't say was vulnerability. I find vulnerability is a key, um, key aspect, which you touched on here at the end to psychological safety and trust building. When you're able to as a leader really say what you know or don't know or what you're struggling with, it creates such a coverage of distance. If somebody's 1 mile apart, they just came 0.5 mile closer to you or 500 ft closer to you because of your vulnerability. And the second thing that I would tell you for psychological safety, I think is, uh, communication and transparency. I think transparency matters a lot. The ability to continuously, uh, talk about what we're trying to achieve, what we faced, um, as a team in terms of small challenges or roadblocks, how we overcame that, uh, change in direction sometimes is an important aspect of transparency. And it is, I think, so important to be transparent with your teams. Uh, to me, these aspects of psychological safety really help bond teams and the last thing is fun with purpose. I call it fun with purpose. Have some fun, right? Life is just too serious. We are just too serious, and our ability to have some fun with purpose, I think, is a massively great equalizer in bringing people together and moving towards the mission at hand versus trying to just show your individual skill at hand. Thank you. Um, we're almost at the end of our session, and I have a couple of questions that I'd like to go over. I wanna ask you a personal question. Why does this really matter to you personally? What has made you believe that artificial intelligence can only be successful when it's combined with emotional intelligence? Sally would love your inputs. I shared a little bit about how I, I think that we've got assists with AI and it reminds us about the parts of us that are human and the support that we need to keep going so for me they go hand in hand. I don't separate them as two separate things. I think they're deeply intertwined and as humans we need assists. And AI cannot function properly without humans whether that's all the guardrails and building in the biases and all of the things that we need to do to keep it responsible. I really do see it as as a joint thing. Why it's important to me is I feel like we have a role to play, right? as someone who. Really does believe deeply in people and how I work with others. Um, I feel like as a technologist, I have a role to play to bring those two perspectives together. I think it's very easy to say I'm gonna ignore AI or technology. You can do that. I don't think it's gonna work very well, but you can try, right? Or you can be all about the technology and forget the human, but I don't think that's gonna work very well either. And for those of us that can navigate the course and help others navigate it as well, I feel it's accountability, responsibility. It's, it's something that I take seriously. Kavitha, would you want to build on that? It's a hard question actually, Sono. You know, it is so intrinsically what. I believe in first of all I think one part of the conversation that has become hard in society today is AI is being seen as this thing it has a shape it has a form it is outside of us fundamentally any tech is designed to make our lives better and we are designing it to make our lives better so I do think. There's something in the conversation where the power equation is off, um, and I really think about it that way, like what, what is giving. What is giving the narrative of AI that much power? Clearly it's highly, highly new tech, significant advances you said, amazing what it can do for health care for education. It is absolutely the vision is brilliant, but we as humans will always be at the center of this and we will lead. We will drive, we will absolutely define how it brings. Better lives to all of us. That's really how I see this, and I do think that'll matter a lot, that perspective and that opinion will matter a lot as we continue to move down this path. Yeah, I think that idea of bringing better lives to all of us is really why it, it matters to me. I shared at the beginning that um where I think there's an opportunity is with rare disease because I, I have a rare disease. I have a connective tissue disorder. That's why I got into sports injury prevention because as a teenager I had a number of injuries and I realized that these injuries aren't, they don't, um, they're not. Life threatening, but they impact your quality of life so much um and so that really gave me this drive to get into this field to figure out how can I help reduce injuries for other people, um, how can I keep people physically active and so for me it's a very personal journey, but I think. Um, where AI and tech comes into this is it allows us to make an impact, an even larger impact, even faster by having that, that core drive, that real personal mission of what we want to accomplish. And what I think is so interesting is for me, I, you know, I'm, I'm motivated to improve life by improving healthcare and by reducing musculoskeletal injuries, but every single person in this room, I bet you have your own why. And that is infused into the work that you're doing and collectively that means that we can transform the lives of those around us in so many positive ways and AI and technology just helps accelerate that even faster. Thank you for sharing that. Um, my one last question for this audience, uh, that you could share, we spoke about so many things today. If there is one thing that you'd want everybody in this room to take away, we have a mix of individual contributors, technologists, managers, what can they take away to build a more equitable AI for tomorrow? I can see. I think it is that why. I think it is so easy to think about the how and the what and the day to day of what do I need to get done today, but really thinking about the why of what am I trying to accomplish in having intention in what we're doing and also processing information. We talked about critical thinking. I think that is so important and is going to continue to be so important in everything we do. Um, the critical thinking piece of what we do, I think we can take that to everything that we do this week and everything that we do in our work and everything that we do in our lives is really take a moment to think about the why and to infuse your own personal experience into it because that's ultimately what's going to make our work more inclusive is by infusing our own perspective into it. I think you said it really well. I can say really, really well. I will add a couple of things, you know, Andrew Maslaw has a quote which I'm a lover for quotes. Um, at every given moment you have two choices to step back into safety or to step forward into growth, and I. Invite all of you to step forward and growth, um, because it is an incredible moment for us as leaders, as technologists to really think about the future. Um, and think about how to make lives better, lives more equitable, lives more inclusive, right? And systems more inclusive, and you have the ability to define what that growth looks like for tomorrow. I'm just gonna build on two amazing responses and bring it back full circle, Sona, to how you open the conversation, and that was think big. I think if we think big, we can accomplish amazing things and if we think small. We'll accomplish small things and so it's our imagination that limits what's possible so think big. Thank you. So we come to our wrap up. We started with a story of a woman in ER who was invisible to AI because it was not built by keeping her in mind. Today we end our session with a vision for a future for AI where AI just does not see her, it serves her. Technology we learn today truly has the power to dismantle the biases of the past. And women today, you can see from our panelists here, they are just not the participants in this AI revolution. We are architecting the future of AI, building it with intentionality, inclusivity for a more equitable future. The question is not whether AI is gonna change our future, it will. The question we need to ask, who gets to shape AI? With that, I'm gonna leave you all with this deep thought. With our 3 panelists, and thank you so much. I really, really appreciate you coming here, sharing your experiences and learnings with all of us. Thank you.