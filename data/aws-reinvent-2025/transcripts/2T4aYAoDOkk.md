---
video_id: 2T4aYAoDOkk
video_url: https://www.youtube.com/watch?v=2T4aYAoDOkk
is_generated: False
is_translatable: True
---

Heard of Jenna I lately. Most likely you hear about it all the time because we do live in the AI technology revolution. For those of us old enough, we've actually lived through many technology revolutions in the past. When I was a kid there was no internet, phones were not smart, and clouds were certainly not made out of Linux servers. And yet here we are, where all these technology revolutions have changed the world forever as we know it to the point where every single organization today. Has a website, a smartphone app, and some cloud presence, and here we are in the AI technology revolution. Like all the other ones, this one will change our world forever as we know it. Yet this one feels so much different than anything we've experienced in the past, doesn't it? Why is that? Well, for one, it's because everything is happening so fast, and we've been really feeling that fast pace of innovation since 2023 and if you think about it, it's still 2025. We're still here. And not only everything is happening so fast, there's not just one thing happening. If we were to zoom in, there is a new and better and faster foundational model coming up roughly every 2 days. There is model inference and Rock and Agenic AI and MCP, and I don't know what next week is gonna hold. The human brain is not designed to ingest that much change in that short period of time and if what you're feeling right now is anxiety, well, congratulations, you're a human and the rest of us are right there with you. My name is Christine Andanov. I am a senior specialist solutions architect at AWS, and in today's session I'll give you enough directional clarity to navigate this overwhelming technology revolution. And if you were to step out of this room with your AI anxiety down by one notch, I'll call that a success. Let's get started. So when you come to AWS and you have a business use case in mind to bring to life, you encounter our portfolio of agentic AI and inference services, and the first question that you have is which one do I pick? And the answer to that is always the same. It depends. It depends on your preferences and it depends on your business requirements, but here is how you choose. The more on the left you go on this chart, the higher up the stack we manage for you. The more to the right you go on this chart, the more control you have over the underlying infrastructure, and we see customers choosing Kubernetes for both their gentic and inference workloads for three primary reasons. The first reason is exactly that control over the underlying infrastructure. Because with that control comes the ability to tune and optimize your workload so that you get the best cost performance out of them. The second reason is because it's Kubernetes and it's portable. It runs on multiple clouds. It runs on on-prem, and we've heard loud and clear that portability is top of mind, more so with Agentic and AI workloads than it was in the past with business applications. And the third reason is that Kubernetes is your one-stop shop platform for all your workloads, your business applications, your agentic applications, your inference, and fine tuning applications, uh, workloads. So when customers approach this, they usually come from one of two directions. Option one is they start with the model, they want to inference and fine tune it. And then they progress to agentic AI or they start building their agents and later on decide whether to run and fine tune the model on Kubernetes. Um, we're going to take the latter in this talk, but you can choose whichever one you'd like. So we're gonna go in the first section we're gonna go of how do you run your AI agents on EKS. Gardner predicts that by 2028, 33% of all enterprise software will have agentic behavior and that 15% of all day to day work decisions will be made by agentic uh applications. In other words, agents are the new business application with one key difference, whereas traditional software can solve problems where deterministic behavior is required, AI agents can solve problems where reasoning is required. So for this example, we are going to um get a travel agent uh up and running and on Kubernetes. For example, we're going to start with the weather agent. We want to get to a point where this agent gives us itinerary of activities we can do based on the weather. So Alice here would ask what's the weather in San Francisco because she wants to go to San Francisco and this weather agent, the first thing that any agent has to do is talk to an LLM to figure out whether it should answer the question straight up or it needs more information to answer that question. And um how agents talk to LLMs or access additional information is by agentic uh frameworks. Agentic frameworks are simply Python libraries. There are many Python libraries out there open source. Uh, we're not gonna cover them in this talk. There are plenty of talks that cover them, um. Feel free to check one of them for this talk we're gonna use the strands Agentic library that AWS open source back in May of this year and we've used internally for a few years before that. So with strands agents again these frameworks are just libraries. It is 5 lines of code you import the strands library, you instantiate your agent, and you ask the question and as you can see. Again, this is just Python code. As any code you can containerize it and you can push it to your container registry and you can deploy it on EKS. So now that um your agent is deployed, Alice can ask what is the weather in San Francisco? Well, maybe you need a chat interface for her to ask that question uh and the agent will say in December the usual weather in San Francisco is highs of 60s, lows of 50s, but it doesn't know what the weather in San Francisco is next week. Because it needs access to the weather forecast, so in order for this agent to give you the weather next week, it has to talk to a weather API and how agents talk to external APIs and databases is via tools. What tools are, they're just capabilities of these agentic libraries. For example, in the strands library, you will import the tool capability, and on the left hand side here I have a traditional application that has a function that calls a weather API to turn that. A traditional program into genic you're just going to simply put one line of code at tool in front of that function and what the agent will do it will decide whether to use the tool or not to use the tool. So now if Alice asks what's the weather in San Francisco, the agent can determine we need the weather now or next week, so it's gonna go and call the weather API. It is a good idea to go one step further here. And wrap that tool in an MCP server. So MCP server, um. The reason that you would use an MCP server is because Uh, maybe if you start building more than one agent, uh, all these agents have to talk to the same API, and it's a good idea to consolidate, uh, all API calls in their own, uh, MCP server database calls in their own, um, MCP servers. So how you do that is you import the MCP2 library, uh, which integrates with pretty much any agentic framework out there, and Uh, you split out the tool, you wrap it in an MCP server, and then on the other side, you put in the code to, uh, for the client to call that server. To build a fully fledged agent, you just need a couple more things authentication that you can use auxiliary AWS services to do that. You can use Amazon Cognito to build your authentication layer. You can use Amazon S3. It integrates with Strand Session Manager and you can use Dynamo GB to store your long term and short term. Uh, memory agent. Agent memory The last thing before you put this agent in production and actually while you're developing is to put some observability. And the three pillars of observability that you're very familiar with from your business applications apply here as well, uh, logs, metrics, and traces with one difference. Traces here become more important than usual because of that non-deterministic behavior. You like traces are your go to place to see what path that agent took. Again, there are two open source, uh, libraries that have become very popular, uh, for you to instrument your agents with. Those are ragas and Lang fuse, uh, with ragas, uh, you can check how. the quality of the response of that agent is where with lang fuse it can give you, it can feed those logs and metrics into traces and it will give you the round trip and a few other um nice metrics. So we covered the libraries, we covered the protocols, we covered the observability, and I hope by now you notice that there is a trend and you don't have to learn a whole new universe this. It is very familiar to what we already know. In fact, if we were to take that agent and put it in EKS, we can have a very familiar architecture diagram for the agent. This here looks like pretty much. Any other service that you would run on Kubernetis, meaning you're going to use the same pipelines, same tools, same everything that you've already built. And as I mentioned earlier here, You start asking the question, well, when should I start running the model on Kubernetes as well? And the answer, of course, as all the previous answers, it depends, but we see customers influencing and fine tuning their models on Kubernetes for three primary reasons. The first reason is that model for your use case that was trained on the generic knowledge of humankind, very common knowledge, and you in your organization have some very business specific deep knowledge. So if you were to take an open source model and augment it with that. Custom knowledge that you have, you're gonna, chances are you're gonna get much better responses out of that model. The second reason for running the model on EKS is the law of physics. When you put the model next to the service, you get lower latency between the service and the model. And the last, the third reason is that. At scale, as I mentioned, you can tune and optimize your underlying infrastructure so you can get the best cost performance out of it. We'll cover both inference. Actually, we're going to cover mostly inference workloads and I'm going to pinpoint the differences of fine tuning. Uh, the first difference is that the workload pattern of these two is different, where inferences are variable, uh, have a variable pattern with your traffic, uh, fine tuning workloads, you have a job and you need a steady capacity. One thing that these two workloads have in common, unlike agents, is that they run on GPUs. And when I think about of GPUs, I think of tomato plants and what tomato plants need to thrive and what do you need to build a lushing tomato garden. You need to pick the right tomato seed. You need to plant it in the right soil. You need to give it proper care, and you need to figure how to do all that at scale. So, starting with picking the right seat, uh, in other words, how do you choose the right GPU for the job? You start with the model of your choice that you want to either inference on or fine tune. This is a model on a hugging face. You go and you check the size of the model. In this particular case, this model is 40 gigabytes, and then we're going to do some back of the napkin math. We're going to add a few gigabytes for KV cache and. Uh, token generation memory and we're going to pad it with another 1 or 2 gigabytes. So this is a rule of thumb for inferencing and that will account to 45 gigabytes roughly of GPU memory that you need to run this model. Our G6 family instances start at 48 gigs of GPU memory, so you can fit easily this model on a G6. If you'd like to fit it on a smaller size G instance or another generation, you can quantize it, which will cut the 45 gigs. So let's say you quantize it and you can Reduce it by half. That would be about 2223, which will fit on a G5 instance that has 24 gigs of memory for fine tuning, you're going to do similar math and using quantization techniques such as Q Laura. You can also reduce the GPU memory you need and you can fit it on a G instance. So in other words, from our portfolio of services we can choose the G5 and G6s to inference or fine tune this model. So once we have selected the instance or the instance families that we want to Uh, run, the next question is like how do we purchase them? And how do we purchase them is somewhat similar to how we purchase any other instance. On-demand savings plan and spot all apply here. We have heard that Capacity has been tricky in the past. Um, so because of that you might wanna if you have like production workloads that um to make sure you have that capacity, uh, you can do on demand capacity reservation, on demand capacity reservations integrate with your savings plan and on demand. So you just have reserve capacity and you can cancel those reservations whenever you're finished with um the workload and then you can do capacity blocks. Those are prepaid capacity blocks from 24 currently it's from 24 hours to 28 days you can uh reserve that capacity. So after picking the right seed, we said we're going to plant that seed in the right soil, meaning putting it in the EKS cluster and the networking. So there's multiple ways that you can hook up a GPU to EKS cluster, and we recommend two of them, and I'm gonna cover both. The first one is Amazon EKS Auto mode. Auto mode comes with our open source carpenter auto scaler already managed out of the box for you. We run it on AWS site where we run the control plane. For you, so you can just spin up a cluster. It comes with the Carpenter API available. The second option is for you to manage open source Carpenter yourself, uh, and install it in the cluster. Once you have a carpenter installed, the next step is to for auto mode. Once you spin up a cluster with auto mode, the next step is to create a carpenter node pool. With the carpenter node pool, we can say we wanted to inference that model, so we can choose spot or on demand. What Carpenter is going to do. At this step it's going to go and it's going to check all the availability zones that are at its disposal and it knows the spot pricing in each and every availability zone and also if I give it, give me a GPU that is. G series and greater than generation 4, it would check the spot prices for all the G5s and the G6 instances and it's going to bring the most cost efficient instance up and ready for you. To do this with open source Carpenter, uh, you need a couple of things to be different. Auto mode, the default node class with auto mode supports GPUs out of the box. There is no node class you need to install on auto mode. On open source Carpenter, you should bring up a uh EC2 node class that is a GPU node class and uh we highly recommend you use one of our EKS accelerator. AMI Auto mode by default or not by default, Auto mode uses the bottle rocket version of these AIs. They come with everything that you need for the GPU to come and be hooked up to the cluster. For open source Carpenter, you have a choice. You can use the bottle rocket version or the AL 2023 version. For the AL 2023, you need to install the device plug-in at this time. So, Once you have the cluster with the note pull ready to go, and you know which model you want to inference or fine tune, you wanna get this model as close as possible to that cluster. So you can put it in one of our storage solutions in this example S3. A good recommendation here is to use S3 VPC endpoint, so the model gets downloaded through the internal network. Uh, same thing with the VLLM container image. You wanna, uh, make a copy of that image as closer as possible to your cluster that is Amazon ECR container registry, and you wanna use ECR VPC endpoint, same thing, um. And then to run that model, you need to create a deployment pointing to the image and saying where the model is located. So when you cube CGL apply this. Uh, carpenter will spin up a GPU. The image will download via the internal network. We have some optimizations here that we're able to get that image uh downloaded rather quickly. And then the model will load into the GPU. This process here is what our customers always want to optimize. Uh, one more optimization here you can do is use an open source project called Run AI in order to, uh, cache the weights and streamline them into the GPU. For inference, you want the cycle from when you have an instance up. Uh, downloading the image and loading the model in general to fit in under 2 minutes. Why 2 minutes? Because of this, uh, vari variability of traffic. You wanna be able to. Utilize our spot capacity. And our spot interruption notice is 2 minutes. That means you want to have the next instance up and running within 2 minutes for fine tuning, because you have the capacity already there and instances are up, you still want to optimize how fast the container loads and how fast the GPU loads. And if you optimize that, chances are you are also optimizing the checkpointing because you're optimizing the storage layer. So with that, we got the GPUs planted in the cluster. The next one comes is how do we care for these GPUs and how we care for the GPUs is first make sure we observe what they're doing and we observe um how fast. We can get a token out of the model and we observe our throughput of how much. Um, tokens per second can that GPU handle? We should also monitor the health of those GPUs and monitor the temperature and the power. And uh GPUs are hardware and like other hardware, maybe even more so, considering the temperature requirements, sometimes they fail. And when they fail, you need to take remediation. Earlier this year we announced node health monitoring and auto repair for um GPUs. Auto mode comes with node health monitoring and auto repair ready for you, configured so if there is a hardware failure on the GPU site, it will automatically detect it and it will take remediation step 10 minutes after that. It will either restart your GPU or it's gonna replace it. These are open source projects. If you're not using auto mode, I highly recommend you uh install them, tune them, and manage them in your clusters so that your GPUs can be well taken care of. And once you have all that down, the question is how do I scale my inference workloads. So For inference workloads, how you scale them. Uh, Let me take a step back here. Usually Kubernetes and how Kubernetes scales CPU workloads is out of the box, the horizontal pot out of scaler integrates with the CPU and memory metrics, meaning I can tell HPA, hey, at this CPU scale up when you reach this CPU. There is no out of the box metric yet to scale on GPUs. Instead, you can have a custom metric and use an open source project called CA and pass that metric into the HPA to scale up and down. Or if you're scaling, you probably anyways are using. Inference frameworks to distribute the load over multiple GPUs. The most common inference frameworks we see are AI bricks, Ray, and Dynamo, and all of these frameworks already come with a metric. So you can take that metric, you can pass it to the HPA, and you can scale your GPUs up and down. Uh, you can expose the model endpoint, uh, with some of our, uh, services and a few other things you should monitor here are exactly this, um. Up and down scalability and utilization and how well um That behaves. And now that you found out how to choose the right GPU, add it to the cluster, care for it, and scale it up, I think what you got out of this is that EKS and the AWS ecosystems give your GPUs the best growing conditions, and with that I will hand it off to Chris. To cover um. Recent lunches. Thanks, Christina. Everybody hear me? Thumbs up. All right, cool. Um, hey everyone, I'm Chris Flinter. I'm on the EKS product team and my focus on the EKS product team is helping customers run AI workloads on EKS. Um, I have to start by saying that. The thing I love most about my job is getting to work with folks like you and customers on all of the really cool things that you're building on EKS in these coming slides, I'm going to cover some of the trends that we see across various customers and I'll also touch on the recent launches that we hope can make your work your work lives a little bit easier. So to start with a level set, um, while EKS itself is not classified as an AI service, um, we see an incredible amount of AI workloads running on EKS. And the stat that you see on this slide here, every week we have millions of GPU powered EC2 instances running in EKS clusters, and that metric has more than doubled since 2024. And so this really to me shows the affinity between Kubernetes and this whole AI adoption trend that that we're seeing in the industry at large. And the fact that Kubernetes is at the center of this AI trend, like to me really isn't that surprising. Like if we look back over the last 10 to 20 years, a lot of the innovation has happened in open source. And many folks at AWS, myself included, believe that Kubernetes is very well positioned to be that foundation for AI workloads going forward because of its open source roots, because of its vibrant community, and of course also because it's a great technology with its extensibility and just the fact that it's massively scalable. Gardner seems to agree, um, and they predicted that by 2028, uh, 95% of new AI workloads will run on Kubernetes, a metric that's up from less than 30% today. That's a lot of AI workloads running on Kubernetes, and I know I'm excited to see how customers use this going forward. So I wanted to share just a few of the customers that are running AI workloads, so inference, training, fine tuning, as well as agents on EKS um you'll probably see a few familiar names up here. I think if there's one thing that I've certainly noticed, it's that this AI adoption trend is really affecting all industry verticals, all customer sizes, all use cases, and I think that's only going to increase as we go into the future. And from the EKS side, it's been both a privilege and also really challenging to run some of the world's largest training and inference workloads. So with that said, running AI on Kubernetes is not all roses, and it certainly does come with challenges. And the top challenge that we hear frequently from customers is just the ability to get their hands on the right GPU in the right region with the right size at the right time. We have folks working every day at AWS to solve this problem, and it is something that very much we look forward to improving as we go forward in the future. I think there's also unique cost optimization and GPU utilization challenges that come with running AI workloads on Kubernetes, and I think these two challenges kind of go hand in hand as you drive utilization up. You're optimizing your cost because you're getting the most out of the GPU instances that you're running in Kubernetes. This really also underscores the importance of making sure that the entire Kubernetes stack, including the tools that you use to manage your clusters, are GPU aware and GPU native so that you can do things like efficient auto scaling both up and down. You can monitor your GPU utilization when it's spiking and which teams are driving that GPU utilization. And so as I'll touch on in a few slides here, we're very much focused on making sure that you have the right primitives to be able to manage your costs and also optimize your utilization. And so then when you layer in this complex landscape of both hardware variants as well as software, I mean we've seen even teams that are really, really good at running Kubernetes struggle to bring AI on top of Kubernetes because this space is just moving so fast and there are unique things as Christina was touching on earlier for running models and serving models on EKS and also Kubernetes. And so all of these things, this is kind of like a very summarized list, but these are the things that we're laser focused on solving myself, my team, as well as across the board at AWS, um, and I'll go through a few of the ways that we've been chipping away at some of these challenges. So one of the ways, um, like that first challenge that I mentioned, it was GPU capacity. We've launched several new GPU instances this year and also increased the volume of GPU instances that are available, um, going through the ones that are listed on this slide here, these serve both the largest, most demanding AI workloads and also the smaller scale, maybe single business unit use case type of workloads. Um, at the top of the list here, uh, earlier this year we launched support for GB 200, um, and at Reinvent this morning, we announced support for GB 300, um, and these are really meant for your, uh, largest, most demanding AI training and inference workloads. These are powered by Nvidia, uh, Grace Blackwell GPUs, um, and they also enable multi-node GPU to GPU, uh, communication via EFA and NVLink. Earlier this year we also launched support for the P6B200 and P6B300. Um, these are also Nvidia Blackwell GPUs also suited for a medium to large scale training and inference, um, and up to 2 times performance compared to the previous generation of P instances. And then on the lower scale side we launched support for the single GPU P54X larges, which is an Nvidia H100 GPU for small to medium inference workloads, as well as fractional GPU instances, the G6F. Instance types and all of these come with EKS support at launch. It's kind of an ongoing treadmill for us on the EKS side, making sure that when EC-2 does launch these new instance types, you can rest assured that those are going to be supported by EKS from day one. And so what does it mean to have EKS support for these GPU instances? It means that we are pre-qualifying and validating the full stack of GPU drivers, kernel modules, software packages, and bundling that up in AIs that. You can use off the shelf. These can give you confidence that when you use these AI they're going to work in your EKS clusters instead of having to piece together all of the various versions of the different components that you need to run these instances in EKS clusters. One of the benefits of these AIs too that I don't know if it like gets talked about enough, but by building all of this stuff into the host into the OS image itself, you don't have to install those components at runtime and coming back to the GPU utilization and cost optimization challenge, this is going to cut down your scaling time. This is going to cut down your time to workload because all those components are already on the node when the instance boots up because of they're all baked into the EKS AMI. And this year we've continued to update these AIs with the latest version of the Kernel Kernel 6.12. We also bumped the version of container-D to 2.1, and as well as the Nvidia driver version, it's now on the latest 580 Nvidia LTS driver version in those AMI as well. All right, uh, so now I talk about one of the upstream Kubernetes features that I'm really excited about. Uh, it's called dynamic resource allocation or DRA. Um, if you've been running AI workloads on Kubernetes or on EKS, you're probably familiar with device plug-ins. Device plug-ins have traditionally been the way to allocate extended resources or devices to workloads that are running on your cluster, and DRA is a new. Take on that now what DRA brings is a much more flexible and expressive language so that platform teams can define the types of devices that are available in the cluster and application teams can request those devices via workload definitions. And one example that this new API and resource model enables is oftentimes when you're running GPU workloads, you want the GPU to be on the same PCI route as your network device to maximize to maximize the network traffic, and you can do that with DRA with the CEL that it exposes. And so I'm very excited to see what the device vendors do with this. We enabled this in EKS as of Kuberne's version 1.33. At G8 upstream and 1.34 and it is still fairly early days for DRA, the various vendors Nvidia, AMD, us on the EFA side, we're very much working right now to build DRA support at the driver level so that you can transition to this as the new way to expose GPUs to your applications when you run them on Kubernetes. All right, this next one here is fast container poles and specifically seekable OCI or SOI for short. Um, if you've run inference or if you're running models on EKS, uh, you've probably experienced the pain point that. You have to wait a few minutes for that container to pull down from the registry and start up on the node. The reason for this is many inference frameworks are very large Pytorch, VLLM, SGLAN, whatever you're using. If you're storing your model in a container that's even larger, Christina showed a 40 gigabyte example earlier that could take a really long time to pull from a container registry onto the node. What we did here with Sochi is introduced a new snapshot or mode called parallel pull and unpack, and what this is doing is it's making the container pull from the registry a concurrent process, as well as the unpack on disk a concurrent process. And so to summarize, it's really just utilizing the underlying network and disk infrastructure that's available to it to get that container down from the registry onto the node and start it up faster. One of the nice things about Sochi is that you don't have to change anything. It works with OCI format container images. It should work with your existing build processes, and in fact we enabled it by default in EKS auto mode when you use GP, or ranium instance families by default. So if you're using these instance families with auto mode, hopefully you're just seeing faster container image pulls now. All right, sticking on the compute side of the house here, Christina talked a fair amount about Carpenter as well as EKS auto mode earlier and how those YAML definitions look for provisioning nodes with AI workloads, we very much had to kind of take another look at carpenter and auto mode and make sure that those, the nature of these AI workloads works well with the provisioning and the auto scaling of carpenter and auto mode. And so the first thing that we had to add, Christina had the slide with all the various purchase options for GPUs and AWS. So the first thing that we had to add was support for EC2 capacity reservations in Carpenter and auto mode. So you can use those by specifying your capacity reservation ID that can be an ODCR or a ML capacity block. Use those with Carpenter and Auto mode. We also had to add static capacity provisioning. One of Christina's slides earlier showed that both for inference and fine tuning, there's often a baseline of traffic and compute that you know you need to serve the workload, and then inference in particular can be bursty beyond that. The way that Carpenter works with looking at the pending pods and then going and spinning up instances based on those pending pods didn't really fit that well with the inference in fine tuning patterns as well as training. And so what we have to do is we have to add static capacity provisioning which allows you to define a node pool with a set of nodes, and you can define that as your baseline. Carpenter is just going to go spin that up without even looking at pending pods. So customers used to deploy dummy pods or balloon pods that Carpenter would go provision. Instances to host those pods you no longer have to do that. You can use static capacity provisioning now and that's enabled in both Carpenter as well as auto mode. The last one on the slide here that I want to talk about is node overlay. The quick way to summarize this is as Carpenter goes and selects instance types, it's using the EC2 instances API to learn about what those instances have available from a resource perspective, how much they cost by using the fleet API, and with node overlay you can pass additional information to Carpenter that Carpenter is going to use in its instance selection for a given pod. Where this is really helpful for AI workloads is oftentimes customers have custom pricing agreements or they need to select nodes that have certain huge pages or other resources available on them, and so node overlay is an additional thing that you can pass to Carpenter that it will use in its instant selection. This is currently only available in Carpenter, although we do plan to bring it to auto mode in the future. All right, so we talked about the compute side of the house. Um, now I want to talk about uh the control plane and with uh one of the key launches that we had here at Reinvent was uh EKS provision control plane and what this allows you to do is select from a new set of higher tier EKS control plane sizes, and you can do this all in a self-service way via API. Where this is important for AI workloads is oftentimes if you're running large scale, so we're talking hundreds of nodes, high traffic use cases, you can pre-provision your EKS control plane so that as you scale up your nodes and as you scale up your traffic, you can have confidence that the EKS cluster is going to be able to handle that load. Another really nice use case for this is if you have a launch coming up or if you have a peak event like a Black Friday for example and you know all of a sudden you're going to have this surge in traffic, you can use provision control plane to scale up the EKS control plane before your event and have confidence that the EKS cluster will be able to handle the load when it comes in during your launch or during your peak event. One of the things that we've been focused on for a while in EKS is making sure that customers can use EKS and run Kubernetes in an easy way no matter where they need to. Last year at Reinvent we launched EKS hybrid nodes. This allows you to use your on-premises or edge capacity as nodes in EKS clusters. We've seen a really strong pickup and affinity for customers who said, hey. I bought these GPUs last year. I just want to run them in my same EKS cluster that I run all my GPUs and AWS. They use hybrid nodes to do that now. Another interesting use case that we've seen along the lines of GPU utilization is bursting to AWS from on-prem and also vice versa, bursting to on-prem from AWS when your primary capacity pool gets completely consumed. And we've had customers like Flawless AI that's highlighted on this slide really increase training times while also reducing their operational overhead by using EKS hybrid nodes. So now to touch on a feature that was implemented outside of EKS uh but I think is very relevant for customers who are running uh particularly inference on EKS. This feature is called ALB Target Optimizer. At its core, ALB target optimizer kind of changes the way that ALB has balanced load in the past. So traditionally with ALB it's very much a push model where the load balancer is pushing traffic, pushing requests down to the targets either with a round robin or a least outstanding request model. With target optimizer, it flips that where it's now a pull model. There's an agent running on each node that's letting the node balancer know when it's available for work based on a max concurrent request. Why is this important for inference? Inference workloads typically have a much lower concurrent request rate than your normal web service type of workflows, and so you can configure, let's say a max concurrent request of 1 or 2 or 10. And ALB is going to use that information to route the requests. This is another way that we've seen customers drive up that GPU utilization while also reducing the error rates based on the load that's incoming for the inference workloads. Another use case where this is really interesting is we've seen customers that are running CPU-based workloads alongside their GPU-based workloads. And so with Target Optimizer for the CPU-based workloads, you can say, OK, these have a much higher concurrent request for the GPU workloads they have a lower concurrent request. Those can be running on the same EC2 instance, and ALB is going to use that information that it's getting from the agent to route those requests efficiently. All right, so now I want to touch on um specifically something that we launched for folks that are building AI agents. Um, I've talked to several customers who have their platforms that they built on EKS have run on EKS for a number of years. They're looking to use AI as a way to make their troubleshooting observability SRE processes more efficient. We launched the hosted EKS MCP server at Reinvent this year. It's currently in preview, um, but this is really something that you can use with those AI agents to get up to-date contextual information about your EKS cluster fleets. You can do things like look up pod logs, Kubernete's events, uh, cloud watch metrics. Um, we also did enable uh right operations in the MCP server. Um, I generally recommend customers approach with caution. Um, definitely start with the read side, start with the observability side, um, and then transition into those mutating operations as you go. Um, this is available in all of your AI assisted. IDEs, it's also available in Q Console, Qiro. And the one thing that I want to call out here about the, the security model with this, there is a local MCP SIGV4 proxy. And so what that is doing is it's taking the IM credentials that are configured on the local client. It's passing those through to the hosted EKS MCP server. Those are getting passed through down to Kubernete's RBA. And so it's very much integrated in how you use any other AWS service. The hosted MCP server looks and feels like any other AWS API. We host it, we scale it, we keep it updated. You don't have to worry about that if you're going to use this with your AI agents. All right, so all of those things that I just talked about um really adds up to EKS being a trusted and reliable way to run your AI workloads, um, and one thing that was announced recently at CubeCon North America in November was the Kubernetes AI conformance program. On the EKS team we're proud to be one of the providers that was included in that first set of validated Kubernete's providers from things like DRA that I talked about earlier, gang scheduling, auto scaling, observability, all of that that you need to run your AI workloads is there in EKS. All right, um, talked about a lot of the stuff that we launched recently. I now wanna connect that to kind of how we're thinking about AI workloads on EKS broadly, um, and also looking a little bit into the future. Um, a lot of the things that I touched on here and to Christina's analogy earlier about planting the right seeds we're at the foundational level of making sure that you have the right things that you need to reliably run AI workloads on EKS. This includes the work that we're doing to support new EC2 instance types. This includes the work that we're doing in the EKS AMI. This includes making sure that all of the things that are happening upstream get into EKS very quickly for you so that you can have that reliable foundation and that reliable infrastructure to build upon. We're also very much focused on making sure that all of the tools and automation that you're using today extends to and is adapted for GPUs and AI workloads. There's a lot of different pieces of this stack from observability with like the node health monitoring agent and EKS cloud watch observability metrics, making sure that Carpenter is adaptable to the types of workload patterns that we see with these AI workloads. We are and have been. Making a lot of progress on making sure that all of those things that you're using today for your normal workloads, you can also use them for these AI workloads, and we're going to be continuing to make sure that across the board with all of these tools and all of these features in EKS that they also work for your AI workloads that you run on EKS. So with those things in place, we also do see opportunity to start moving up the stack and providing things out of the box so that you don't have to sift through a jungle of open source projects just to run gang scheduling with topology awareness on EKS so that you don't have to write your own homegrown load balancer just to run disaggregated or distributed inference. So that you don't have to be concerned about like giving your agents that are running on EKS code execution privileges. These are all the things that we just wanna be part of EKS so that as you all transition to running more and more AI workloads, you have these things out of the box. They're fully supported by us, um, and they're fairly easy to use that you don't have to piece all the puzzle pieces together yourself. And then lastly, the holy grail of what I consider to be intelligently automated, um, this is, this is a balance of where EKS is going to increasingly handle the things that cause you pain today. Upgrades, really painful. We need to make those smarter in EKS proactive cluster alerts so that the cluster is telling you what's going wrong and ideally how to fix it. This of course needs to come with the right levers so that you can tune how much you want EKS to intervene and do things on your behalf, but this is very much where I see us going on the EKS side, using AI within the EKS service to make your lives easier overall. And so that's a little bit different than running AI workloads on EKS, but it's something that across the board on the EKS service team we're very excited about. And with that, I will pass it off to Christina to bring us home. Chris. So in summary, we covered what is the difference, what is the delta of running your regular workloads and your AI workloads on Kubernetes. What's the delta of running your business applications and your AI applications, uh, sorry, gentech uh AI. On Kubernetes, how to use agentic AI to manage your Kubernetes clusters, how to run your inference workloads and best practices to also fine tune those workloads. Chris covered uh some of the recent investments we're making into Kuber. so you know, uh, it will sustainably take your business to the next level. It is flexible enough where you can run CPU workloads side by side GPU workloads. Uh, these workloads, Cooper80s is portable, so, uh, you can port them. It will scale with your business needs. And if you don't need it to scale and you just need static capacity, it does that too now uh and by tuning and optimizing it you can get the best cost performance out of your GPUs so that your organization. Alongside Your website, your smartphone app, your cloud workloads can solve problems for your customers that were unsolvable before using AI. Where to go next and learn more, we have our workshop series for inference and agentic AI. We run those virtually every single month. We update them every single month. You can come back as many times as you'd like. Uh, and if you'd like to get started on your own, you can use our AIML EKS user guide or our Terraform blueprints that are set and ready for you to create your cluster and optimizations. If you're sticking around for a couple more days here, I highly recommend these three related talks. Check them out tomorrow and. Thursday and with that, if you do walk out with your uh AI anxiety lowered by one notch, I call the session a success. Thank you very much.