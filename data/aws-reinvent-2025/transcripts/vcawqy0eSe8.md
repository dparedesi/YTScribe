---
video_id: vcawqy0eSe8
video_url: https://www.youtube.com/watch?v=vcawqy0eSe8
is_generated: False
is_translatable: True
---

Hello everyone. Welcome. Thank you for joining us today. I'm assuming this is your first reinvent session for 2025. Whew. All right. Thanks for giving us that privilege. Um, so today, um, we are going to talk about bringing or bridging structured and unstructured data using AI agents. Uh, let me begin by asking you a question. How many of you here are building AI AI agents or maybe they're running AI agents in production or you're experimenting with it? How many of you? Well, quite a few of them. Right. Anybody want to tell me what kind of data you're using for these uh AI agents? Is it going against unstructured data, like files in S3? OK. All right. Is there any data that is In relational data sources like structured data. OK. Yeah, a few of you. Yeah. So usually in enterprises, a lot of Knowledge that we have of all these enterprise systems from the last 1520 years is the data is logged in relational databases, right? So that that's the predominant data storage mechanism in enterprises, large enterprises. And we also have a lot of unstructured data over the past 1015 years because we have so much of data that's being generated in all forms and shapes. So, this talk is about how do you use AI agents to bridge that data that is in different silos, in structured data and unstructured data, you use AI agents to bridge all of that together and um Have an intelligent system that can go across these data silos. This is a core talk, um, So we will be doing, uh, demonstrating a building an application, uh, application. Uh, you don't need laptops. If you try to follow us, follow along when we are doing, it's probably not, um, you know, not enough time for us to, um, type all the codes, so we'll be using some, uh, um, you know, snippets, VS code snippets to bring in blocks of code. But you'll generally get the idea how to build this, uh, type of uh application. Uh, my name is Anil Murari. I'm a senior solutions architect with AWS. I work with our nonprofit financial industry customers. I've been with AWS for 5 years. I'm joined today by my amazing colleague, Rafia. Thank you, Anil. Uh, hello everyone. My name is Rafia Tapia, and I'm a senior solutions architect also working with nonprofit customers. Uh, I've been with AWS for a little more than 6 years now, and, um, really excited to share some of the, the, the code we've, we've, you know, created for this code talk. So with, you know, we can, we can start. All right, yeah. All right. So the first thing we'll do is we'll kind of briefly talk about the architecture of the application, the code which we will be building today. On the, um, on the far Let's see, your left-hand side, you'll see, um, the, you know, there's a, a UI application, right? We've written that in, uh, streamlet, nothing, nothing complicated. But that UI will be able to take a user question and then be able to answer that user question via two types of data. One is the unstructured data which we have shown on the top uh yellow pane on the on the architecture diagram. In the um the unstructured data would be coming from Bedrock knowledge bases we have one slide where I will be talking a little bit more about Bedrock knowledge bases and then the structured data which will be coming from Aurora, uh, database now um. If anyone who don't know what Aurora is, Aurora is our managed relational database service. We support, um, MySQL, PostScris SQL, um, and some other, um, databases, but for this demo, we will be targeting Postress SQL for all the structured data. Uh, let's move on to the next one. So, uh, before we go in, I just want to kinda, uh, you know, make sure everyone, um, knows what are the, the SDKs and the agent, um, tools we will be using. So we will be building our agent using STRN, um, SDK. Is there anyone who has already done some work with STRN in this room? Oh nice. We have a couple of hands. Nice, very nice. All right, so Stre allows you to build a single agent as well as multi-agent where the agents can talk with each other. STR, um, you know, uh, supports MCP as well as A2A protocol. Here, um, you know, we are showing you a simple agent where a prompt. Sent to the agent. The agent has certain tools built into it, and based on the tool, it will, uh, it will be able to provide the service which this prompt is asking for, right? So that is the model, a simple, um, agent, single agent which we will be creating today's court talk. The next is about uh how do we access uh unstructured data. Now, uh, how many of you have written RAG applications? OK, I, I see quite a few hands, right, quite a few of them, yeah, so you, you guys know, right, the way when we bring, when you write a rag application, we take our data, most likely unstructured data, maybe in a PDF format or word format or plain log files, and then. Convert that into into vectors because that's what our LLM understands and then be able to use that knowledge which is stored in those unstructured documents and be able to provide AI solutions on top of it. Bedrock knowledge base allow us to build all that reg infrastructure very effectively and easily. And this is what we will be uh showing you today that we will be taking some of the um files which has some content in let's say Word document and then we will be, uh, putting it into a Bedrock knowledge base and be able to query that via our agent. Now because of time constraint we have already uh created a knowledge base for our uh code talk but if any of you are interested in, um, you know, knowing how to build a knowledge base from from scratch, please come and talk to us after the session and we'll be more than happy to, um, point you to some resources. OK. All right. So, this is the application that we're going to build today. So, um, Let me play this. Uh, let's see. All right. So, no surprise here, we both are from the nonprofit group, so we picked a charity use case. So we're building a charity chat assistant, a chatbot, you know. That will go against structured and unstructured data. We have all the campaign information, donation information in a relational database in Aurora, and we have some unstructured campaign related data in a S3 bucket which is we are accessing through our um Bedrock knowledge base. So as you can see, if you ask some questions that needs it to go against the. Relational data source, the the AI um application that we are building, the agenttic application that we are building, will figure out what is the right data source to go to and get that data automatically from that corresponding data source. So we're going to see all of this and how we're going to build this. All right. Um, let me switch to my ID mode. All right. And, and for the most part we will, uh, you know, some of the UI aspect we'll, we'll try to, um, uh, you know, uh, take it out very fast, we, you know, because there's really nothing, um, nothing, uh, you know, which is extraordinary or something that you guys might be interested in learning today, but, so, um, the reason why we, we have this UI so we can show you a complete agent, right, but. Um, as, as, uh, Anil is, uh, bringing the code, um, you'll see that we'll start with, uh, with a very basic structure of the UI, and then, um. Um, so, yeah, you know, we'll start with showing you a very, um, basic UI skeleton, you know, how, uh, uh, yeah. So, um, apologies for that. So as you can see, I'm using my favorite ID Quiro here. So if you're not familiar with Quiro, it is a, um, Um, you know, an IDE that, um, Amazon supports and released, um, recently, it's an it's a Um, ID where you can do white coding and also spec-driven development. Uh, we're not going to do any of that today, we're just going to demonstrate, uh, building this application, but, uh, um, you can use any, uh, you know, IDE, um, for that matter. So I'm going to start with, um, an empty folder here. I Already have some um files here. Uh, let me quickly go through what those files are doing. I created a virtual Python virtual environment of 3.12 and I have a requirements file that with all the libraries that we're going to use today, uh, Rafael talked about, uh, strands, strand's agent and agent Tulia. So, so if you are working with strand. There are two packages, uh, and by the way, right now, strand only is supporting Python. So there are these 2 packages which you need to make sure, uh, you bring it in your, uh, environment to be able to use some of the, some of the APIs and, and tools we will be showing you. One is strand agent and the other is strand agent tools. Yeah, and another file I have is uh Run.sh. So we're going to build a streamlet app for, uh, you know, the UI app. So this Run.sh it just sets up my environment. If there's no virtual environment, it creates one, and it starts my Streamlet app at the end. So that's all it's doing. So just by, you know, to save some time, we have these two files already here. So let me start by creating a new file. I'm going to call that uh app.pi. And we're going to start putting some code into it. So this F.pi will have most of our UI code in it. Yeah. So, as you, um, as you can imagine, you don't want me to be typing in front of you all this code and probably, um, you know, going through that pain of Looking at me typing, so we, um, what we did is we are using, uh, VS code, um, snippets, so to help us just bring in, um, code that we want. So we're going to first just build that shell of the UI that we showed earlier. So nothing fancy here, we're just going to, um, do bring in some snippets. This first one is. Doing imports and then setting up some constants like if you, for those of you who probably um noticed uh on the left hand side we have some sample questions and things like that. So this is just bringing all those things um again this is this file is not the most exciting part um other files are but so I'm going to just quickly go over this um, this is going to set up my uh theme for the app which is like a dark theme. And then I'm going to um just bring in the snippet for the chat interface. So again, like I said, it's, there's nothing much here we just brought in a shell of an application streamlet app. There's one thing that I do want you to note though, I commented out this piece of code which we will uncomment later, which will bring in, add in the capabilities of the agent. For now it's just doing nothing. So let's try to run this and see what happens. So you'll see that right now, the, the, the UI is really not, nothing intelligent, right? Even if you ask a question, it might not be able to answer it correctly. So, I mean, why don't we ask a question? Yeah, so, um, this is the empty application. So, let's see, let's see if we can ask a question. I'm expecting an error to come because we don't have any agenttic capabilities yet. As expected, there is an error here. So let's move on to a little bit more exciting part of the. session today Alright, if I can kill this one. Alright, so I'm going to add another file. Um, I'm going to call that agent.pi. And this file will have our core logic of the strand agent, which you, which you'll see we'll be building it later on. Yeah, so, I'll start with, um, just getting some import statements. Right. And yeah, go ahead. So as we are building this agent, we'll walk you through line by line, right? The first thing, um, of course we need these, uh, uh, we need to import some packages, particularly we need to import the strand packages, and we have some other, uh, uh, packages like Porto 3 because we will be going against, uh, Aurora database and, and Bedrock KB and so forth. The next thing you'll see that because we are going to be going against LLM model here we are using a model ID which is cloud model. So our agent will be using cloud to when a user gives a question, it will take that question. It will feed that question into the agent. The agent, of course, has to use an LLM to be able to answer that question. And the model it'll use is going to be clouded now. SRN SDK do allow you to talk to any LLM, even those LLM outside of Bedrock. We've chosen it to be Bedrock, um, you know, a supported LLM, but you, if you are doing it, you, you can, you're free to use any other LLM. Um, and then the next thing we are doing is declaring some, uh, inference parameter like maximum, uh, token allowed and, and temperature and so forth. Um, so now the first thing, uh, Anil the code Anil has put in is that, uh, the code from line, um, 18 to 22 is the, the base model we are creating, right? We, we are using a model ID which is a cloud model ID. We've given certain parameters. Then the, the code of line which Anil just put from line 28 to 29 uh or actually uh to 32 is. Using the strand agent to just create a base agent. Now this agent has no no intelligence into it. We just wanted to show you two lines of code, how an agent is created. So you'll see it's as easy as instantiating an agent object, right? That agent object takes two things. It takes. A model ID which we've already built in from line 18 to line 22 takes a model object and then the second thing it takes is a system prompt. Now at this point, the agent is really just a blank canvas. No intelligence has been built in. Yeah, notice in the prompt we said don't make up answers. If you don't know, say don't know, right. Uh, so let's run this application again and see, but before you run this, I think you have to uncommit the. Yeah, very good point. So, so Anil had remember he showed you in the app.pi he had commented some, some code because that code was what was calling the agent. So let's uncommon and go through that again, right? Yeah, thank you. Let me just explain that code, Anil, what's your uncommented so the code which Anil uncommented, right, that code, what it does, it first. Make sure that the agent object is not in the in the state of the streamlet. So streamlet maintains a there's a state management aspect, right? So first it's going to check if there is already um uh uh the session state has an agent object. If it does not, then it will create that agent, the same agent which Anil just created in the agent.pi file. So that's what this code is doing and we've just uncommitted, but keep in mind at this point all it's doing, the streamlet is going to be calling our agent. Our agent has no logic, so you don't, you would not expect much functionality, but when he run this code, you might not see some intelligent answer, but at least you won't see the error which you saw earlier. Yeah. So let's ask the application the same question we asked, how many members do we have? Um. And let's see what it says. Um, so it says, I don't have access to your organization's current membership data. So it's basically telling us that it doesn't know the answer. So, so at this point we've completed the entire, uh, structure of our, of our code, right? We've built the UI. We've built an agent which does not have much intelligence but at least it's functional and, uh, and then as we move forward we will start building more, uh, intelligence into it. Yeah, so let's begin by adding um code to retrieve data from. The knowledge base, yeah, sore, uh, remember the, the, the architect diagram which I showed in the slide after that we showed that an agent can have tools in it, right? Strend allow you to create your own custom tool and it also come up with a lot of ready to use tools. Uh, today you'll see that we will be using one custom tool and one, which is already available to us from the strand SDK. All right, so I added the retrieve tool here, uh, for this, um, retrieve tool to work, we also need to set some environment variables. So let me, as, uh, Anil is typing this in, let me tell you, explain what retrieve tool is. Retrieve is a, um, out of the box trend SDK tool which allow you to query Bedrock knowledge base. You don't have to write any, um, any custom code, OK, to access, uh, knowledge base. It does it everything. The only thing you would need, you need to create a bedrock knowledge base which, as I mentioned earlier, we had already created and the code which Anil just put in, um, online, um. Uh, uh, 26, you'll see that the knowledge base when you create a bedrock knowledge base, the knowledge base gives you a unique ID, and this is the unique ID which we have, uh, which we have kind of hardcoded here to, to let, um, retrieve to know which knowledge base it has to go through. I'm gonna stop here in case if anyone have any question on the knowledge base. So one quick one, like, as I'm kind of following, listening, writing, would you mind if time permits, just give it one more rundown after, after all this wrapped up? Sure, absolutely, yeah, sure, but, but just to recap, OK, when you are creating knowledge base. You identify your your data source where your unstructured data is coming from. Most likely it'll be Word document, PDF documents, whatever, and then the knowledge base you also when creating the knowledge base, you also say where is the, the, what is that vector database where you'll be storing that. All you have to identify is a data source, a vector store, and then the knowledge base does all the heavy lifting for you. OK, once you do go through that, it will give you a knowledge base ID which we have, uh, in, uh, put it in here so we can let our strand agent know, oh, this is a knowledge base you have to use. Yeah, I have to put in a disclaimer here. I know we're hard coding these IDs all over the code. This is probably not how you would do it if you want to take this to production, you know, you want to store this in some parameter store or some other external configuration store, but this is just easy for us to demonstrate this way. Yeah. And, and as part of this, uh, retrieve strand agent, it has to have again the knowledge-based ID as well as the region. Now there are two. Ways you can do it right. You can actually give that in two pieces of information to retrieve tool via environmental variables which we are doing it here. So if you look at the core line 29 and 30, this is where we we establish environmental variable where the knowledge base ID and the region where that knowledge base is created is is injected right into the into the environment. So, so the retrieve tool will be able to pick it from the environmental variable and will know where to go, OK. So we are assuming the knowledge base and the vector database is ready to be exactly, exactly, because we, we had a time constraint, we did not go through that exercise of creating the knowledge base. standard patterns for embedding or to create those vectors that you want us to. So, you know, with, with, um, Bedrock knowledge base, it's a few clicks, right? So you just, um, we can, you know, we can stay after the session ended for the specific format of the data, for example, if I have the data in the Word document, right, any best practices you want to suggest that if you have a Word document. Or the decent format right what embedding models we can use the size. So Bedrock supports, yeah, so, so Bedrock supports two embedding models, Titan and a coherent model. Uh, the chunking strategy, it's a, it's a topic which is. Definitely a very important topic, but I think if you want, we can definitely, uh, talk later on. It would be way out of scope for us to focus on chunking, but it's an important thing. Let's talk after the session. Absolutely, absolutely, yeah. All right, so let's, I'm sorry, there's a question. Yes, it is. Can it be attached with any other vector store like OpenSearch, or it has to be Bedrock knowledge base? Retrieve is for Bedrock knowledge base. OK, but, but there are, and by the way, the, uh, strand or tools, it's a, it's a community development tool, so tools are coming up every day. You might be able to find, um, uh, tools, you know, which, which can go to other data. All right, so I, I ran this application again. Now let's go back and ask the same question we have been asking. How many members do we have? And let's see what it does. It's doing something now because. It's not saying I don't know. All right, so it looked in the knowledge base and came back with some answers. Um, and it clearly states here, like, Um, based on the information I found in the knowledge base, this is some like number of donors we had, but it's clearly not coming from the structured, uh, you know, relational data, which may be more accurate. So the next step is to extend this to go to that. Yeah. So, so now because what we did, we did, we, you know, had some documents which were related to our campaign. And it had some data in it, but, but the answer was not 100% accurate. The 100% accurate answer is is sitting in our, in our databases, right? Our database is where most of our, as you know, most of our enterprise data is, uh, or especially when you are already going against, uh, uh, you know, your enterprise application data, right? So now what we will be doing, we'll be creating a custom tool. That custom tool, what it will do, it will take a prompt from the user, try to answer if this prompt can be answered through our relational database data if it can be, it's going to convert that into appropriate query language and be able to retrieve that data. So for that, the first thing we will do is create a uh. Yeah, I'm adding that, uh, custom tool that so, so all, all Anil did was uh in the agent definition we already had put in a, a retrieve tool. Now he also created a query SQL, um, DB tool, right? We have not created this tool yet, but that's the purpose of the tool. So now we'll create the tool, uh, that the entire logic of that custom tool will be in a, in a separate file, and that's what Anil is doing. Uh, let's call that file query SQL query_SQL_DB.file. And this is where we will build our custom to. So of course the first thing we need to do is import all the uh appropriate packages which uh which we'll do first thing um again strength um you know uh agent and tool is one of the fundamental um packages you need to bring in and that's what we have. We also have Boto 3. Boto 3 is again anybody who have used AWS SGK, that's a Python SGK to access most of our, our AWS services. Um, strands we will build the logic and then we will feed it into strand. Yeah, so this, this custom tool is just a Python script. So strands will use this, but there are some, um, hooks of strand, right, which we need to put into this code so that we can, we can feed it into strand, and you'll see that the next step, that's what we will be doing, right. So, um, The, the first two, thing Anil is doing right now is actually writing that code and you'll see on line 6 to 9, right? How do you take any code, any custom code you've written, and make sure that that code can be converted into a tool for strand? OK, so, uh, what all you have to do to, to take that code and convert it as a tool for strand is annotate that, you know, create a function. Put all whatever your logic you need, you know, in that function and then make that function annotated by this, uh, tool annotation so you know all you need is a tool annotation. Once you have that and once you feed that into the strand, strand understand that whenever it needs to answer a user using the tool, it will run this function, whatever it is written in the function, it's going to run it. OK, so I'm gonna just make sure everyone is clear about how the strand plumbing is happening. Right, it's very simple as just one annotation. OK. Now what we have done because we don't want to put all the logic in into this one function right we have we're gonna be creating a supporting uh class where most of our our our logic is going to be and and in this particular, uh, tool function all we will be doing is calling the different, uh, methods of that supporting class, OK, so, um. Any, uh, so let's first, uh, no, yeah, I brought in some constants, and then, OK, fine, so, OK, let's go back to line, the two line, right? So 2, yeah, go ahead. OK, so again, uh, you, the first thing you probably need to do is create the, the, yeah, so I, I added an empty convenience class here, so I'm going to, um, initialize that. So this is the, the, the, um, the DB structure, um, data agent class, OK? It's nothing but a supporting class which will have all most of our data, uh, most of our logic. Now the constructor, uh, constructor of this class takes 4 pieces of, of, uh, argument. The first is the ARN of where our Aurora database is. Uh, those for those folks who might not have worked with Aurora, when you create a database in Aurora, it gives you a unique ARN through which you can identify the database. So that's all we're doing. And you know, as Anil has already mentioned, this is not, um, a true production ready code. So we are kind of hard coding things here. But, uh, what Anil did, Anil, can you scroll up? He, he defined few, uh, constants, and you'll see, um. That you know in this 4 constants we've defined where our database is coming from. So we've defined the ARN of Aurora Line 8 is, is an ARN for the secrets manager. So as a best practice, right, whenever you are accessing a database, of course you need the credentials for the database. As a best practice you should never hard code those credentials in the code or even in config files, especially if you are. Uh, publishing those that code into some sort of a repository, right, so Secrets Manager is one of the AWS services which makes the storage of credentials very easy and convenient, and that's what we've done. We've stored our database credentials in Secret Manager, but here we're just bringing the ARN of that secrets manager. Um, then the other, um, constant which Anil declared was the name of our database which is called NPO_membership. Um, here you'll notice that we also have identified LLM which is not a clouded model which we had used earlier but we are using a different NA model now first of all, why, why we are doing this, I want to take, uh, take a minute to explain that, OK. Our agent itself, so, so agent is communicating with LLM, and the tool within the agent is also communi communicating with LLM for the agent communication we are using cloud for the tool which will be part of the agent we are using Nova. The reason we did that. Because we wanted to make sure you guys understand that you can, you can use multiple models for different things, right? The model evaluation is a big, big, um, component when you are when you're creating AI applications. So just so that we can give that demo we've purposely are using a different model here. Uh, by the way, is this, um, legible to everybody like in the back, or do you want me to increase the font? Yes, thank you. I should have done that. Thank you. You know, I could also not, but is the tool not just being used for a rack because you already have that cloud agent. The tool is just reaching out from Aurora and feeding back to that agent, agent. So an agent has a rag component and a another component which is going against structured data. Rag is going against unstructured data. Yeah, yeah, the retrieve tool was going against unstructured data. This custom, uh, so now you have two branches of data. And, and you'll see why we need another model, or we could have used the same model, but why we need an LLM model at all for this tool, you'll, you'll see, you know. No, so we will make sure the LLM is smart enough to decide, should, should I be going against the rack tool or should I be going against the structured database tool. That would be the cloud model. OK? So now, uh, everyone is clear, right? So far what we've done, yeah, please. What we're using. Just basically to figure out what the what your data is absolutely, exactly. You, you, you nailed it. OK. There are two things, right? One is we need to make sure that LLM understand our database. We want to make sure that if it's good answer any question, then, you know, and, and you'll see as we, as we move forward, right? But, but everyone is so far clear on the way we've defined the, the plumbing for strength tool and the, the creation of the helper class, right. Now we, as we will, we will start building function in this helper class to do different things. The first function we will build, OK, it is, uh, it is so that our LLM can understand the database schema. Without understanding the database schema, it cannot, it cannot answer, right? So the first, um, what are we calling this function? I think I pulled the wrong function. Sorry, yeah, go ahead, OK. Um, generate schema. OK, yep, so now you know every relational database, OK, has a capability, uh, via some system tables or, you know, that you can actually use that capability to query what, how your database looks like, what are the, what are the tables, what are the columns of those tables, what are the data types of the, the, the columns, and so forth, right. So the first function which we are showing, it's called get table schema, and you'll see that this is nothing, nothing complicated, OK. All it is going to be doing, and can you show that's query please? Yeah, so I need to pull in the query too. Yeah, so this is a query, and again we are defining it as a variable here in, in order for us to go to PostScri SQL and be able to pull what's in the PostScri SQL, this is a query, OK, if you are using, let's say Oracle or if you're using, um, Microsoft SQL Server, there every database has a mechanism. And that you probably just have to change this this query syntax for whatever database you would be using, but the rest would be the same, right? So the first, uh, uh, you know, function we, uh, Anil put together was get table schema. Uh, Anil, did we already add that in the two? No, let me add that. So I'm going to add that, um, thing, yeah, so we created our helper class. Then the second thing we will do, we will actually query the sequel, uh, query our database. In our case is Aurora Poscri SQL. We are querying it and saying give me the entire schema in a Jason, uh, format, and that is now being stored in the DB schema variable which is on line 82. I'm gonna pause here, make sure everyone is clear on it. Right? No question. Oh, you have a question? So or some. So right now you'll see schema is enough, but yeah, sample data might, I don't know, depending on what you're trying to do, you might think that, you know, maybe providing a sample data in a prompt might make it more effective. Yeah, you'll see how we wrote the prompt so that without the sample it'll still get it. OK, so once we know the um the database scheme of, I'm sorry, I have a question. Let's say you import the schema with this, but you want to add some more information to schema because the bottom of name is probably not very intuitive for the end is that. Yeah, so, very good question, right? Maybe you can then inject some sort of a transformation logic before you send that schemer to LLM. Right, that's one way of doing it. Other, another way of doing it might be that that transformation logic you can embed in the, in the, the system prompt which you'll be sending to the LLM whenever you send, uh, uh, anything to LLM. You do provide some context, some prompt, right? Yeah, just adding to that, that question, uh, if for example, my schema has foreign keys and stuff, and sometimes the LLM needs to do like 2 or 3 queries to help it, uh, there you have only the table schema with the columns. Is it possible to, uh, put in the schema, the, the relationship as well? You absolutely so. Right now we chose our schema which is very simple, OK, but if there was relationship which we need to make sure because let's say right now when the, the queries which will be answered from will be most likely from one table. If that is the case, then we will make sure that you know the, the, the prompt which we are providing has that relationship so the sequel generation has the joins in it right at the end of the day when you're creating a database you are only going against, uh, you know, sending a SQL statement. Your SQL statement has to have that relationship built into, right? All right. Um, so then now once we, uh, the second thing we will show you is that system prompt generation, right? See, remember that system prompt gives that intelligence, OK, uh, in your case you could have added like the system prompt that, hey, this transformation should happen, you could have added the, that relationship information. Let's see what we are producing in the system from. So, so I'll go a little bit line by line. Can you move up a little bit? Uh, all right, so this is the system prompt we are, we are creating. We are saying the first thing we are saying you are an AI system that creates system prompt for database query. OK, at this point we are asking LLM to create a prompt which itself has the database schema embedded into it. So we're going to take that prompt, we're going to embed with the user question and send the whole thing together. I, I, I'm, I'm gonna make sure everyone got it right. So, let's say you have a user question, right? User question now needs to be sent to cloud model to to answer in order for us to send that user question to the cloud, we need to build a context which we will be building through the prompt. Everyone clear so far, right? That context which we will be building needs to have that database information into it, right? So for instead of us hard coding that context, what we are telling Nova model use that. User schema which you've got, give me a prompt which then I'm going to subsequently send to cloud model so that it can tie the database schema with the user question. Let me see OK, yeah, so, so we'll be here after. So if you want to just go over the code again, you know, but, but this, so here we are saying to in the prompt, hey, look, you, your job is to create a smart prompt with the database knowledge embedded in it. We are giving it the, the, the scheme underscored Jason, uh, on. Line 96 which holds our schema information in a in a variable and we've we've built this whole thing and now we're going to be sending it to the LLM and at this point all the LLM did was give us a smart prompt which we will then be able to send it to cloud model. Yes, yeah, we will see that we will see that, OK, so now we got our, uh, the first line was our, uh, getting our schema. The second line was make, you know, creation of that system prompt. And in this system prompt we provided the DV underscore schema to this function. So at this point, the line 137, now we have a prompt which has our SQL schema embedded into it. Right, yeah, any specific reason why we are passing this information to other agents like this agent should be intelligent enough to answer and build the query, right? It has the context of the question and as well as the scheme. So, yeah. So, so we are, we are building just a tool for the agent. Right, uh, the, the, what the agent will do, the agent takes the, the user input, sends it to this tool. Now this tool, along with the schema it has, is going to come up with a SQL query if it, if it can be answered. Right? All right. So the third is now we got our database schema, we got a prompt. Now the thing is we need to tie these two things together. We have to now ask an LLM to generate a SQL query if that SQL query can be, if the user question can be answered through a SQL query. So let me, let me rephrase this, right. Let's say my prompt was, and by the way, our database is related to a membership model, right? It has a table like membership, uh, uh, events, you know, uh, campaigns, donors, those kind of things, right? But what if if I ask a question. What is the capital of France now? That question cannot be answered from a SQL query from my database. So you know, when I send my schema and my user, the LLM will say, well, this, these two things don't match, so it cannot come up with a with a valid SQL query. So the third function is. Uh, it's actually is now taking the user input, knowing the database schema is going to go to the NA model and say, can you provide a valid SQL statement which can answer this particular user question. In your previous statement you said that you go back to the cloud model and then it comes back again. Now what I understand that. Once you have see the system prompt, there's no need to go back to the cloud model. You can directly go back to the Nova. You can then escape to execute, but the cloud model is, uh, there's a first step, right? Yeah, yeah, right. The cloud model will go not only through the database. It also could go against the rag after the ski. There's no need to go back to the cloud model, right? Yeah, the, the, the Nova model is doing exactly, exactly, yeah, yeah. You bury the schema with the cloud you go to the, get the executed. Yeah, so, so the, the cloud model will only be utilized to say if I need to go to the database, once it gets the context of the schema, yeah, yeah, yeah, yeah, yeah, because, because here you'll see if my, if I can generate a. Valid SQL for the user prompt is going to not only uh you know, give me a SQL, it's going to actually give me the data of that, uh you know, simplest way to think of it is this tool could have been static, right? Like you just, if this is the query, this is a SQL query. If this is the user query, this is a SQL query. We are putting intelligence into this tool that we're building, using the LLM model to generate that query dynamically for us. Uh, quick time check. We have 14 minutes, so we, yeah, move a little bit faster. OK. So now, uh, we got our system from. The third thing we, you know what Anil is typing in is actually, um, getting those two plumbing things together, right. The database schema and the the um the user prompt. So uh the third thing we will have is a is a helper function called generate SQL. Let, let's uh type this in and let's go to the definition of generate SQL please. So again, in our supporting class, we have a generated SQL. Let's let's go a little bit and and make sure we understand this, right? Oh, where did I put it? Uh, where is, yeah. OK, so if you go, uh, if you look at this, uh, yeah, generate SQL, OK, what it did was it took the user prompt, which is in the question variable, and at this point it has the system prompt which, which has the database schema into it. It defines the model which is our NA model and then it builds, uh, you know, it as LLM. Hey, look, LLM, this is my schema, this is my user. If you think a valid SQL can be generated out of it, give me that. SQL. So once this function is executed, you will have the valid SQL. But now what we're also doing it, we want to make sure the valid SQL is in some sort of a delimiters so we can extract them very well, uh, conveniently, right? So you need to, uh, the delimeters, yeah. OK, so these, so in our, uh, in our, uh, going to know our models, right, when we are saying can you, can you create a valid SQL, we are also going to say if you can create a valid SQL, make sure the valid SQL is in this, um, SQL start dillimeter and, and SQL end dillimeter, OK, once our, our valid SQL is in there, so when. The next function which is going to pull that SQL out, knows exactly from the output which I got from the model where I have to pull the SQL out from. Oh, everybody OK till for this? OK. All right, so you know, the, the line 1 uh 226, right, is there, uh, by this time if everything is working fine, I would have a valid SQL. Then line 227 is another function which is extracting that SQL. All it's going to do is going to take the output from LLM, make sure between those two start and end dillimeters, I can pull the valid SQL out. OK, nothing complicated here. Yes, please. See you. I'm sorry, um, can you schema I see that. What if the colonies Let's say city is spelled C T Y, something like that. How do you that call. Yeah, so it's a very good question, and I'll, you know what, I'll give you, uh, maybe we can run an example of that. Our model in our database, a table is called campaign, right? What we will do instead of using the word campaign, we will use the word event, for instance, in, in a prompt, OK? And that's a beauty of, of some of this, you know, doing it through LLM. The LLM is smart enough to understand that there's a, there's a semantic meaning between event and campaign. So even though the database itself has a word campaign in the in the table, it associated with the word event and be able to get the gene uh SQL correctly. That, yeah, that's a no, but that's a no, but that is part that could be part of any LLMs, right? OK, so this question is completely different. For example, I give you a schema. That has column M less column 1, column 2, column 3, right? And So by seeing this column 1, column 2, column 3, nobody knows what it is. Exactly. There is also information about this column. Yeah, yeah. So if I pass the information schema, column, column 2, column 3 is China, yeah, yeah, yeah. But, yeah, so, so if, if the column names are so cryptic like that, exactly, you can pass the data dictionary also into your prompt, and then it will know like like how to do it. Not another tool. You, you don't want to have so many tools. You might want to make this tool smart enough that along with, uh, yeah, maybe along with your database, uh, you know, uh, um, information where you're pulling the, uh, SQL, you, you can have it. Oh, by the way, we did not talk about memory. Yeah, so, uh, in reality, if you're doing this in production, for example, um, you don't want to keep going to the database and generating this, um, system prompt over and over again because the schema is fixed, right? Maybe it changes once a month or only when you want to do that update when the schema changes. So, you can use memory tools, like, for example, if you're deploying this using Amazon Agent Core, where you run your AI agents, there is a uh feature. Functionality that comes with agent code that's called memory so it can keep track of um this kind of um you know caching that you want to do with the and and you know your data dictionaries could also be part of that memory you know exactly you don't keep on changing, you know, you don't change your schemas in your data dictionary so often, right? So make it, you know, again for, for this particular it's um we we've already said it's not 100% production ready. Because we wanted to make it so that it's easy for everyone to understand what what's happening. All right, so I added, um, also a function to execute that same sequel that we just got, and I'm going to run this again. Yeah, so after we extracted our SQL, we, we wrote another function called execute SQL. Again, no brainer there, right? It's just going to take our SQL. It knows how to, how to query a PostScripts SQL is going to feed that SQL into PostScripts SQL. Uh, if you were going against any other database, Oracle or any other, you probably will use whatever mechanism of querying the database you're using, OK, as long as you have a valid SQL, that's all you need to talk to a relational database. Right. So I'm asking the same question, uh, theoretically now, it should be going to the database to get the exact number of members that we have. Let's see if we Can do that. Or if I missed some step somewhere, it happens. Uh, looks like, uh, right, so, so you see now the 5000 members is not coming from reg. OK, and then, maybe we should tell them where it's coming from. Yeah, so this is coming from the database. I mean, you, you, you see there's a toggle here that will actually show you how the query is going. Yeah, so you, if, if once, uh, maybe let's run it to, to, to show the audience, you know, because the, the. You'll be able to see whether it's send it to the SQL tool or the or the rack tool. So when now when the question was asked, I asked another question, what membership levels exist. Um, so again, this is in the database we have 3 membership levels, um. So you can see that um when you expand this tool usage, it will show you what it does. It went to the database, it got these results. But if I'm asking a question like, um, did we Did we run an event to support elderly? This probably will go against the knowledge base instead of going against the database because this information it's not in a database so the Claude is making exactly once Claude make the decision which tool it's going to use if it database tool, uh, database needs to be used for the answer, then Noah will come into the picture. Um, one thing I do want to highlight is in our app.pi, our system prompt is, um, sorry, on our, in our agent.pi, our system prompt is still basic, but our tool, our application It is simple enough that the LLM is able to figure out that it needs to go to the database or structured data. But it's always a best practice to give good system prompt to make sure your LLM behaves in more or less the way that you want to behave rather than just leaving it up to it. So, you know, we could have replaced this system prompt with something more robust like uh These are. He is not able to convert aque is throwing like giving an exquisite. Yeah, yeah, so, so. So right now you'll see when, when Anil put, you know, is going to put a very smart system prompt, uh, because our our, our code example was so simple that Claude was able to find this out. But imagine if you had multiple knowledge bases, multiple databases you had, you were even doing transformation. You were defining data dictionary. When you are making that complicated type of, uh, you know, agent where it's accessing multiple uh structure and non-structured data sources your system prompts becomes very, very important, right? The much the more impor uh you know, information you provide in that system prompt, the more intelligent cloud would be to. Give to decide which tool it's going to use. Yeah, you're kind of defining your agent's personality in with this system prompt. So um it is very important to have that good system prompt. All right. So, I want to wrap this up. We have only 2 minutes. Um, as you can see, we just built a demo application. Um, it's not production ready, but if you want, if you have built this application, if you want to deploy this in AWS. You have multiple choices. You have um choices like you can run this in AWS Lambda, container services like ECS or EKS or use our latest and greatest, um, you know, runtime for running these gentech applications, agent code. It's because it's just Python, right? So it's, it's, you can't run it in any, um, compute. We are not deploying this into anywhere. We're just running it off of an EC-2 machine that's sitting in AWS, but theoretically you can pick any of these choices, um, to go there. And finally, uh, if you want to learn more about Asiantic AI, there are some resources here and. Please don't forget to submit survey for um for yeah yeah and we will be here um after the session so please feel free to um you know stay uh I don't know if you can stay in this room, but we'll we can definitely go outside and then uh you know, we'll be happy to answer any question. uh, I know, sir, you wanted to give us like a a 11 more rundown, OK. OK, yeah, so, yeah, we, we, we'll stay here, we can, we can chat, yeah, and, and we can also, uh, you know, talk about, uh, I'm sorry, uh, we can talk about your question. OK, all right, so, yeah, sorry, yeah, I'm sorry, I didn't get. Yeah, so you want to, you want to know how to create a knowledge base, of course, yeah, we can, we can share, yeah, yeah, we can, we can do that too, um, yeah. Share the code. Um, all right, we, we, we, we will get it up on GitHub and maybe, um, share it if you can connect us with connect with one of us on LinkedIn, we'll, we'll make that actually we were going to do share the code we submit the survey session, but uh don't go to. Review yet. If you go to the app, um, you pick the session and you submit the so thank you all for coming and uh yeah, thank you very much. We will be.