---
video_id: 5GxPoCeENmw
video_url: https://www.youtube.com/watch?v=5GxPoCeENmw
is_generated: False
is_translatable: True
summary: "This session, titled \"Automate insights and drive innovation with cloud and AI solutions\" (IND384), explores how Consumer Packaged Goods (CPG) companies essentially leverage cloud automation and generative AI to shift IT from a reactive utility to a strategic business partner. Aparna Galasso, leading Retail and CPG Business Development at AWS, sets the stage by describing the \"Red Queen's race\" that many CPGs face—running faster just to stay in the same place as consumer expectations rise and technology accelerates—arguing that future transformation requires IT to co-author business strategy rather than just support it. Chris, a Cloud Engineering lead from Mondelez, then shares their three-year journey of building a \"Greenfield\" self-service platform to manage hundreds of accounts and thousands of pipelines. He details how they employed a strict \"Infrastructure as Code\" (IaC) philosophy using Terraform and Spacelift to automate absolutely everything—from vending AWS accounts with standard roles and network configurations to enforcing policy-as-code guardrails that ensure security without blocking developer velocity. This \"golden path\" approach allows teams to deploy compliant infrastructure in minutes via pull requests, while a centralized \"belt and suspenders\" strategy uses tools like Wiz and OPA to maintain security posture and granular cost transparency. Following Chris, JC from Granger, a massive MRO distributor, outlines their \"Project Nightingale,\" a FinOps initiative designed to solve the \"impossible math problem\" of scaling expert-level cloud cost analysis to 50+ executive stakeholders. They moved beyond manual reporting by building a multi-agent system (using AWS Bedrock and Agent Core) where a \"FinOps Analyst\" agent queries billing and usage data via custom Model Context_Protocol (MCP) tools, and a \"Summary Writer\" agent synthesizes that technical data into consistent, executive-friendly HTML narratives delivered directly to inboxes. This \"headless\" agentic workflow, which runs deterministically without human interaction, reduced the time to generate comprehensive financial reports from a full engineering day to just minutes, effectively democratizing FinOps data and empowering leaders to make cost-aware decisions at enterprise scale."
keywords: Cloud Automation, FinOps, Generative AI, Infrastructure as Code, Agentic Workflow

Good afternoon everyone and thank you for being here today. My name is Aparna Galasso. I lead the retail and CPG business development teams for North America at Amazon Web Services, and today I'm really excited to be here with two customer stories of how our customers in retail and CPG are innovating using cloud and AI to drive competitive advantage within their companies. I'll set off the stage with a couple of minutes here on why this matters, and then I'll pass on to my speakers Chris and JC. So let's get going. Let's start with a conversation. Quickly about just the, the, the most common technology that we're all talking about today, Gen AI. So as we've seen over the past few years, Gen AI has moved from art it possible to proof of concept to we have lots of use cases in production to the point where we're really starting to think about business value with Gen AI. The problem is just as all of this has been happening, companies are still feeling stuck because now we're talking about agents and multi-agent orchestration. There never seems to be a moment in which you're able to get ahead of the curve with Gen AI or with any other technology for that matter. And that's where I, for any of you in the audience who were there last year, I use this example and it still stands true a year later. How many of you remember reading Alice in Wonderland? Anybody? Alice in Wonderland? There we go. So for those of you who don't remember or those of you who need a reminder, Alice is running and running and running in Wonderland, the Red Queen's race, and she's running as fast as she can, but she realizes she cannot get ahead. She's stuck in exactly the same place. And this is where our CPG companies, this is how they're feeling. They feel that no matter what they do to get ahead of the Gen AI curve, that they're stuck in exactly the same place. Now we're in a pivotal moment with CPGs in general. We're, we're at this point where there's unprecedented unprecedented change in the marketplace. Consumer expectations are higher than ever, and consumers are expecting you to move at the pace at which their minds are changing. However, margins have gotten tighter because the cost of serving these consumers has increased, and at the same time, you also don't see that the pace of change of technology is decelerating. It's actually accelerating. Everything is moving faster, and companies are forced to have an ability to keep an agile approach to their future development. Now the problem is this the C-suite in these companies is used to thinking about consumers, and they may know where to head strategically, but they are not very well aware of how to use algorithms or how to keep ahead with the pace of change of technology to serve their consumers. And so really where this puts us is a very simple place, right? We're at this pivot point. You could think of it as a pendulum swinging, and the pendulum has swung from a place where IT was more of a reactive service provider following what business would set as a strategic direction to being there in the front end helping set the business direction with IT. Now, arguably this pendulum method analogy fails because I don't think the pendulum is ever going to swing back. The pendulum is probably going to move into a future new state where you think about CPGs not only being in the business of selling products, but being in the place of building new digital first business models. Now what this means is if you think and you look forward, you know, companies all talk about transformation and what does transformation really mean? Transformation in the future is going to require IT and business leaders to show up together and to tackle business decisions together. You're going to have. IT that is making sense of the AI capabilities that are out there and translating that into business opportunities. You will also have IT part of the situation on the back end, um, taking those, those business opportunities and converting them into technical road maps. And so essentially think about it this way. The point here really is that IT is no longer sort of invited to have a seat at the table. Really, if you look at the future and how business decisions decisions are going to be made, you need IT to be an essential voice at that table. And so today we're gonna have two stories, different paths, same outcome, elevating IT into that position of strategic partnership at CPG companies. We're first gonna have Chris. He's gonna talk about how he's been building an automation platform at Mondelez that has allowed him and his team to be positioned as agile innovation partners to the business. Then we will have JC and he will talk about how he's gone more on a fin ops route to use agentic AI to elevate decision making around um AI and other, other cost plays to the executive leaders in the company. And with that I'll pass on to Chris. Thank you. Thanks Aparna for that, uh, great introduction. And so as uh you said, we've been focused a lot on trying to make sure that our foundation is sound, right? We need to be agile. Uh, we have a very aggressive, uh, strategy of many acquisitions and, and, and, uh, divestitures in our company. And so we really need to be agile and therefore our foundations need to be sound and we need to be able to enable the business to move quickly. Um, so just a little bit of, uh, introduction or, or my agenda I wanna share with you today is around, you know, a little bit about Mondelez, um, our cloud journey so far, and then the platform that we've built, uh, and how we've scaled that, uh, for hundreds of, uh, accounts and, and thousands of, of pipelines and applications, uh, how we manage our costs and then a little bit about what's next for us. So, uh, I guess, you know, you may not have heard about the Modalez brand, but hopefully you've had many of our, uh, products. Uh, Oreo is a big favorite of mine, obviously Cadbury, uh, mini eggs. I'm really thankful we only get them at certain times during the year because they would be bad if we didn't, uh, for me, but, uh, yeah, so just a little bit about Mandalis, we do about $36 billion in revenue, uh, in last year, so, and growing, as I said before. There we go. So, um, I'll start with a, a little bit of a history of Mandalez, right? So we're, uh, technically a young company. We were born from the craft food split, uh, in 2012, but we have a very long history, right? You recognize a lot of those brands are not just from 2012, right? So, a lot of our company is, uh, taking on that heritage of all of those companies that and brands that have joined together to make uh Mandalis what it is today. Uh, our cloud journey, like a lot of CPG companies, started around 2018. Uh, we were doing very much a lift and shift, uh, from the data centers at the time, and more in a, uh, managed service provider model, right, where we were, uh, didn't have maybe all of the expertise at the time, like most of us didn't at that time, uh, and we were relying on getting everything to the cloud. Um. The, the journey lasted a couple of years and, and actually late 2020, uh, our largest, one of our largest regional SAP systems, we actually uh failed to get that cut over and it's been in the data center ever since. So, um, I, I like to think about it as, you know, a lot of the, the hardest things that we had to migrate, unfortunately we still had leftover uh in 2021. When we started our cloud engineering team. So actually that's when I joined Mondelez and I think I had a really great opportunity because uh we didn't have a cloud engineering capability within Mondelez at the time. But I was uh empowered or able to build a team from scratch, which was really an awesome opportunity. So, between 2021 and 2024, we gradually grew the team, um, from just me in 2021, uh, the first year we hired our 1st 67 engineers, and by 2024, we have a, a global team of uh over 30 engineers that are working on our platform. Um, and this culminated in 2024 with our strategic partnership that we signed with AWS. Uh, and then later in 2025 when we announced that we were gonna do all of our SAP RIS environment on AWS as well. So, um, when we started this cloud engineering, uh, capability, we got to do a green field environment, and these were some of our guiding principles. So, uh, a lot of the engineers and I that, uh, started, we had already kind of gone through this journey once, learning our way through it, and we really tried to be very thoughtful about what were those principles that we wanted to have that would be different. And, and uh what we're trying to achieve. So, you see a lot of them there, right? Uh, we really had this concept of making sure that we were gonna have a self-service platform for developers, um, that was enabling and empowered developers to do the right thing and to do it quickly and easily. So, you know, secure by default, uh, automation of absolutely everything. Everything is infrastructure is code, making sure we had microsegmentation. I remember talking to our SA who's here in the crowd on our first day saying, I know we have no accounts today, but I want to know what happens when we hit 1000, right? And uh it was a little bit funny then, but really it's satisfying to see it come to life today. Um, so, uh, a lot of these principles were really what we tried to take forward into the design that we had. There we go. So what we have today is, I would say, you know, this controlled, uh, easy, um, self-service platform, um, and we started to adopt as we were building it, some of the buzzwords, uh, in the industry around, you know, platform engineering, golden path, enabling constraints, um, and really what our kind of mantra was, you know, we wanna put the power of the cloud in the hands of all capable developers. In a secure and centralized way so that we can make changes rapidly and not have things age and get out of date and need to be constantly replaced. So kind of an evergreen approach to making sure that we have the guardrails and the controls in place to enable people to do the right thing. And really the, the biggest principle here, and I think the thing I'm, I'm most proud of that we've done at the, as the team is making sure that all of this is uh infrastructure is code. So when a request comes in for a new product or a new application, what we do is we deploy all of these things that you see here, the GitHub uh code repository, all of the branches, uh, um. Static branches for the code that correspond with all of the AWS environments that the application needs. So if you need a production and a non-production, you have two branches in the code repository that are mapped there automatically connected with the CICD pipelines to watch those repos for pull requests and get those deployed to the AWS applications. And this is just a a kind of a simple example to show you another exam, uh another version of what we had on that previous slide. So, you know, you have some product or some application, uh, with one request, all that you see here is, is automatically provisioned. Um, and this includes, as I said, the GitHub repository with all of our controls, all of our, uh, branch protection rules, uh, where we can enforce our GitOps standards that we wanna follow. Uh, we use Spacelift, which is our terraform, uh, um, terraform deployment, uh, uh, CICD tool, and we automate. create individual stacks that are tracking the static branches so that when a developer pushes change and terraform into one of these branches, it gets picked up and it does a terraform plan against the AWS environment and the developer can see what those changes will be and then either reject them or confirm them and push them to production. And then on the top we also generate automatically in that single provisioning standard roles um and you see them across the top. There's actually more than this because there's an individual role per environment, but for the sake of simplicity, um, you know, we really have DevOps owner and DevOps contributor and read only, um, and so these roles have different capabilities across the different platforms automatically. And this kind of gives you, uh, you know, kind of how that works, right? So you see the rolls across the top, all of them uh are automatically created and available within 2030 minutes, maybe from the time that the request goes through in our identity, uh, governance system. And all of the requests for membership automatically get routed to the business owner of that product, right? So there's no central team that has to decide whether or not John gets access as a developer that goes to the business owner of the application to manage themselves. And again, I think really the, the most. The, the best thing about the environment that we've built is if you see on the bottom again no matter the most powerful role that we're provisioning is all just read only access in AWS so you don't have the ability to go in and manually edit, click, create, change things, and this is how we're making sure that we are owning and controlling the, the path to production. So a little bit more code or detail around those requests. So again, on the left you'll see just some, some terraform. It's obviously too small to read, but another view on the right of all of the things that get provisioned automatically when you make this request, the GitHub repository, the branches, all of the standard rules, the CICD pipelines, um, and then all of the standard components in the AWS account that gets vended. So we're using Account factory for terraform, right? And then we're able to seed that account when it gets created with all of the, the things that each account will need to be able to manage its own environment, including delegated DNS for the account and uh our, our standard backup strategy and tags for uh AWS backup and all of our security tooling and all of the automation to get all of those logs securely into our sock. that And then another view. So again in the middle you'll see that same code snippet, but just to kind of give you a flavor, this is how we provision network, right? So there's nothing special or a separate ticket that needs to be created. So if your application in this example is a public facing application, the system knows that and we know that because in the request, you can see it says that it's requesting regional public subnets. So we've got standard regions that we deploy to and all you need to tell it is what's the the IP the cider block range, the size that you need per region, and then this will provision automatically through the provisioning process all of the VPC and network that you need to connect. Both to our AWS core network and then back in our transit gateway into the the Mondelez network itself and so this is something I, I think is um pretty easy once you see it but it was a very long and hard process that a lot of really smart engineers were able to put together so this just kind of gives you a flavor of some of that automation that we tried to strive for. Um, so I, I've said this a couple of times, right, but really we're trying to make sure that we're enforcing GitOps, right? So this is the, the slide that I think kind of tells that story the best. So again, it's all been provisioned. You have two environments in this example called non-prod and production, and those static branches are the ones that have the branch protections on them. Uh, a DevOps owner or a DevOps contributor who has access to this GitHub repo can go and create a pull request, make their changes, initiate the pull request, right? And once that pull request is seen by our spacelift stack, it sees that, it triggers a plan, and then it gives immediate feedback to the developer of, here's the resources that you are trying to create in this environment. Do you want, do you want to approve or apply it or reject it? And then once the merge is complete into the static branch again it officially runs and the developer gets one last chance to look at that plan, make sure that it's not gonna do anything uh awful, and then uh confirm that and that's how we get all of our resources in each AWS account. I think the um the important things on this slide too there's that little circle on the bottom that says OPA policy enforcement this is key for us um this allows us in every every pipeline that we've automatically provisioned we can attach our policy to all of those stacks so this is very powerful because we're able to keep um. Updating our security standards and our guard rails and controls as we go to make sure that as plans and applies come through they meet our latest and greatest standards. So an example is, you know, we have a lot of uh private terraform modules that we develop that enforce our own standards at this. This layer we can say, you know what, you can deploy as many S3 buckets as you want, go right ahead, but you have to do it with our space lift module with our terraform module that enforces all of the Modali standards and again this is really I think where the the concept of enabling constraints come in because. We're constraining you, right? We're constraining the developers telling them you can only do it this way, but it's gotta be an enabler, right? It's gotta be something that they want. It's gotta be a pull, not a push, right? It's gotta be something where, oh wow, I don't have to worry about the 50, 150 settings that I could set on an S3 bucket. I can actually say S3 bucket name. And that's it because I know it has sane defaults that are going to meet all of our security policies and all of our best practices. And again, because we've deployed all of this with infrastructure as code, and I have an example where we realized that we weren't enforcing TLS 13 in our load balancers a year and a half in. It was just an oversight. We just didn't do it. It's like, oh, that's bad, we need to fix that. So we went and pushed a policy that just checked to make sure that that's there in uh in every stack that we have. So it gives us one single place to make a single change in line of code and impact thou thousands of CICD stacks and hundreds of AWS accounts all at once. So, um, so I think that's really the, the heart of what it is that we're building. Another example is, uh you see Open tofu, right? So, when, uh, the terraform licensing changed, uh, we were able to relatively quickly, I don't wanna, uh, do disservice to the people who had to make sure that this change was tested and rolled through, but, you know, we We can say now all of the stacks in our environment are on open tofu, which is something that would be extremely hard to do if you had hundreds and thousands of different CICD stacks. You would have to rely on each team to go and make that change, whereas we said, we want to be open tofu now, so that's what we're gonna do. So really uh power in that. So a little bit more again about some of this and, and some of the key concepts behind it. Again, the, you know, when Amazon Que we're a big Amazon Que developer shop, right? So when Amazon Que developer started doing PR reviews where you could have Amazon Que do your PR review, again, one line of code and all of the different repositories, we could go hook that up and enable that in across the environment very, very quickly. And um on the space lift side, right? Again, our OPA agents, so we can write our policies, we can enforce the, the various policies to make sure that not only are you following best practice, but you're doing it in a way that forces collaboration. Uh, so again, if our standard module is so restricted that a development team finds that it's not usable for them. There's one of two things that's going at play there. Either one, they don't quite understand what best practice is, and it gives you a chance to say, oh no, you can't deploy that S3 bucket without encryption at rest, right? So no request denied, or it's a new use case, right, that we've not thought of before. Again, one place, one source code, even do a pool. Request, right? We can review it. We can update that uh module so that the next developer, the next team, and in our environment, it might be another third party developer coming in, but they all benefit. So it's this forced collaboration which also is made possible by the enforcements and the guardrails and the controls that we have. And then again on the AWS side, on the accounts, uh, really proud to say that we have read-only access primarily, right? So instead of wondering whether or not an account is in a certain state, uh, it's been kind of neat that I've been able to answer questions for security by just going to our GitHub org and searching. Right? Cause it, I don't need to check the account. I know that what's in the Git repo is gonna represent, at least if it's in the main branch, right? It's gonna represent what we have in our environment. So I think very uh powerful there. And again, this line between what we can do in our CICD pipelines from a control perspective versus what we do in the AWS account itself, we try to do belt and suspenders, right? So we still have Wiz that's trying to scan everything. If it finds something that's bad, that's not right, we can go and fix it, and it allows us to then figure out, well, how can we write more policies so that it never even gets there as opposed to finding it and trying to remediate it and uh get it later. So, um, I guess the, the, the last bit here, of course, it's always about cost. So this is just a random snapshot from uh our AWS kudos dashboard. So if you're not familiar, the kudos dashboards is something AWS will give you for free and quick uh Quick site. Um, what I really like about it again, this whole concept of microsegmentation. Uh, not only can you not screw up someone else's application, right, but you also know all of the costs that are in a particular AWS account. You don't have to worry about, well, are all the tags up to date? Are they all right? Are they all allocated the right way, uh, you know, my former, uh, experience when we had more. Um, I guess monolithic mega accounts is tagging was always an issue, right? And so I never trusted what was the actual cost for a particular application because I was always worried that someone, you know, fat fingered something and it had a typo and it didn't show up in the report or maybe a developer had forgotten to do it. So again, there's a lot of power in making sure that I can say to any application or product owner, you wanna know what your cost is? You know, do you wanna know it by the second? You wanna know it by the day? You wanna know by the month? Do you wanna know your trend, you know, your budget? What do you wanna know, right? And so that microsegmentation, uh, has been something that's really helped us with that other principle that I put up there around cost transparency and being really efficient and being able to tell, uh, what our costs are per environment. There it goes. So, um, a little bit I guess for what next, right? So, um, we're actually at Mandalits we're on a, a, I guess more of an organizational journey to products and platforms which is really helping us, uh, we talked, my team and I, about how we kind of created a platform where there weren't a whole lot of customers, right? So we kinda built something that no one was begging for, but this has been really awesome for us, uh, because now we're really creating the in our product teams we're creating the. Um, the customers that really don't know it yet, but they really want this, right? So when they're asked what's your TCO and they ask us how much of my cloud costs, I go, here's your dashboard, go look at it yourself, right? So I think, uh, really trying to enable or leverage what we've built so far to, uh, take that next step to really drive even more transparency in our, um, in our company. Um, we're also in the middle, or I guess not even in the middle, we're trying to wrap up a lot of our legacy, uh, cloud migration, right? So, uh, I said we were left with a lot of on-prem data center stuff that, uh, was kind of the hardest. Things to tackle, uh, we're on the cusp of shutting one of them down, uh, well, two out of the four down, uh, early, uh, in 2026, with the last one being our big SAP environment that never got moved, which has a separate timeline, but really excited to close down those data centers. and then, you know, of course we're not done then, right? Like we've done some lift and shift and some clean up and some opportunistic uh changes, but what we need to do then is go back and refactor and re-engineer and make serverless where we can make serverless and uh and really continue to evolve from that perspective. And then of course, you know, plug the AWS, right? To leverage our AWS strategic partnership, really looking as, as a CPG company, right? Like, well, we also sell a lot of products to Amazon.com. How do we make that better? How do we leverage the digital media and advertising and everything else. So, really excited for that next step as well. Um, so that was it for, for me. So thank you for allowing me to share our story. So, And now I'm gonna turn it over to JC to share some of his experience uh with Granger. So thanks, JC. Thanks Chris. Thank you, Chris. Thank you Aparna. Uh, welcome. Excited to be here. And talk to you a little bit about our Fin Ops journey. But first, our roadmap today is going to follow a problem to solution journey. That I think a lot of teams of specialists can relate to. First up though, I'll provide a brief introduction to Granger. And then dive into our strategic challenge. How did teams of specialists. Close the knowledge gap at enterprise scale. Then we'll dive into an initial proof of concept. Where we inevitably hit an impossible math problem. And we'll talk about why we chose generative AI as a solution. And it's important to note. It wasn't without its challenges, so we'll step through the journey on the path to production. And then I'll introduce Project Nightingale. And lastly, we'll talk about the results. I'll give a brief demo. And we'll discuss where we think this is going next. So, first, Granger. Granger is a leading broadline distributor and the largest MRO company in North America. To give you a sense of our scale, in 2024, we generated over $17.2 billion in revenue. And serving over 4.5 million customers with 30 million plus products available globally. We employ over 26,000 team members. And we've been fulfilling our core motto of keeping the world working since 1927. What differentiates Granger. Is the fact that we combine assortment. Technology And high touch solutions. Via complex technology and digital infrastructure. That digital infrastructure. That's where our Finop story begins. So first, The challenge, right, the strategic challenge. I think anybody in security platform SRE or DevOps. Will relate to this challenge. Critical insights remain trapped in complex systems. You have valid and thorough reporting, but your stakeholders, they're busy. They don't have expert knowledge to interpret the data. No one's coming to your dashboards, right? Attempting manual communication with these stakeholders through email, Slack, one on one meetings, workshops, it simply cannot keep up with the pace of enterprise growth. So for us in the context of FinOs. The reality was that as our federated cloud adoption continued to accelerate. We faced this pivotal moment where we had to Pivot from centralized cost optimization and cloud cost management. To democratize spin-offs. With enterprise-wide adoption of FinO's practices and principles. So what did we do first? We identified a target demographic that had the highest return of value. That for us that was a product in technical domain leaders. These leaders hold large swaths of cloud utilization and control the levers of prioritization and road maps. Penopso also serves multiple other personas. We have product of indirect procurement, engineering teams, senior executives, and finance. But this demographic, this particular demographic. Was critical to influencing our overall cloud efficiency and effectiveness. So how do we meet this demographic? We landed on executive summaries, a format that they were familiar with. Delivered to their inbox, the place where they were already working. We couldn't expect them to come to us. We couldn't expect them to change their ways of working and visit our dashboards. So no new tools, no behavior change required on their part. These executive summaries were generated manually by our team of FinO specialists. A few of them are in the room today. And essentially they consist of everything an executive leader needs to know. And they can walk away with 5 within 5 minutes with a full understanding of their cloud usage, its growth, and optimization opportunities. But we quickly ran into a problem. These summaries took on average about one full engineering day to write. We had, we started off with an initial group of just 6 domain leaders. But we had an eventual audience of initially 30, growing to 50 recipients. So we were talking about up to 4800 engineering engineering hours a year. To serve just one persona. That simply didn't scale. The good news though Leaders engaged with the summaries. And they took action. We actually saw people starting to take action. Now that they were informed about their cloud usage. So Meeting this impossible math problem, we knew we had to look elsewhere for the solution. We had validated the theory that executive summaries delivered to the inboxes of leaders. Would bring about engagement and action, so we had to find a way to scale it enterprise-wide. Initially, as I said, manual summaries were quite time intensive. But they had quality. That only spin-off specialists could provide. We evaluated essentially 3 core approaches at the end of the day, manual. And programmatic and then ultimately generative AI, so why not programmatic, right? At the end of the day, programmatic solutions solutions could meet the scale that we were facing, but the problem is they lack the nuance, the tailored customized metrics, trends, and analysis. That our FinO specialists were able to create on their own. So we had to shelve the idea of a programmatic solution and seek out alternatives. That's what ultimately led us to generative AI. And it changed what was possible. We found that with the right constraints. Templated prompts. Organizational data. Served through the right tools. Generative AI can meet that specialist quality of our fin ops analysts, but at an enterprise scale. So It's not working. Let's talk about some of the initial challenges though. Because it wasn't all gumdrops and jelly beans. That's one thing to, you know. Evaluate and assess a generative AI proof of concept on your laptop. Most of us could get a little POC working in a weekend. That's an entirely different prospect to take that to production. For us, when we started out as a cloud team we were familiar with traditional IAC approaches using Terraform and BO 3 to interact with AWS resources. But they simply weren't keeping up with the pace of generative AI innovation. We found Terraform to be really clunky and often the AWS provider. Didn't have the configurations we needed for our agents, so we pivoted and we found strand's agents. With strands agents we were able to define agents and line in our source code. And quickly accelerate on our goals. The second core problem though was unlike most gene generative AI solutions, we weren't building a chatbot. We didn't have variable questions and variable responses. We needed consistent deterministic outcomes from what are essentially probabilistic models. Our solution, we developed a headless MCP client. No human interaction required whatsoever, and the output is the product itself. Thirdly, enterprise constraints. We couldn't just go pick up any. Vendor tool or solution off the shelf right within the enterprise we had to adhere to security. Procurement requirements, legal requirements. And so we had to find a best in class solution that was able to Meet our requirements will also provide an answer. Enter with that, we stumbled upon Agent Corp, which was still in preview. But quickly learned that it would provide the enterprise requirements. As well as The configuration and ease of authentication. Observability and monitoring that our team needed within a very short runway. And then lastly, as I mentioned, productionizing, it's not exactly that simple, right? We were developing something that was semi novel, not being a chatbot. And so we didn't really have a lot to reference. Agent Core was also in preview. They lacked documentation. And often we, we had to dive into the source code of agent court itself to find answers. So That path to production eventually led to us creating. Project Nightingale Leveraging Agent Core, Amazon bedrock with cloud models, we're able to construct. Analyst quality finops cloud cost intelligence at enterprise scale. This essentially consists of two key agents, a Finnops analyst. And an executive summary writer that specializes in synthesizing financial analysis into executive friendly narratives. This multi-agent architecture. Unlock the capability to meet that enterprise scale. But the agents alone weren't just the answer. We also built our own MCP server with custom MCP tools. Underneath this is a series of different data sets including organizational data. Recommendations from Cost optimization hub. And billing data from our cloud cost management tool. So, let's talk a little bit more about this solution. As I mentioned, one of our requirements where we need a consistent. Output with our executive summaries, we couldn't have hallucinations. We couldn't have errors or we would quickly lose trust of our stakeholders. The data had to be reliable. The format had to be consistent. And we had to be able to Have confidence that what we were sending to our executive leaders was reflected in our tooling. And so, what's somewhat unique about our solution is the sequential nature of the workflow. Instead of the variability you'll find often with chatbots, we have a structured sequential workflow, essentially an agentech pipeline. With the input being a templated prompt. It's also somewhat unique, as I mentioned, it's not a chatbot, and so we're able to control the input, which is a prompt template, as well as the interaction between the agents. That ultimately gave us the consistency and accuracy we needed. To have confidence in sending these to executive leaders. So we'll follow this work flow from left to right. Initially, the domain analysis prompt us into the Finops agent. This prompt has a set of criteria for the analysis, as well as context about the domain the agent is performing the analysis on. The agent has at its disposal. A couple of important MCP tools. The first is our billing data. It's important to point out initially we tried to wire up the billing API for Cloud Zero directly, just wrapping the open API spec in an MTP server. But the volume of billing data quickly overflowed the context window and tool failure or tool use failed. So we had to pivot And write our own custom MCP tool for the billing data. And what this provided for us was even more consistency and even more confidence around no hallucinations. The cloud zero MCP tool returns structured data to the agent and simplifies the task that it has to perform with the analysis. As I mentioned, optimization hub data from cost optimization hub is also fetched from an MCP tool, and these are combined with organizational data to generate that cloud cost analysis and the cost intelligence by the FinOs agent. Once that's completed, it's handed off to the summarine agent. That agent has a couple of important tools at its disposal, but at first, as I mentioned, synthesizes that analysis into a distilled executive friendly narrative, right? Our customers and stakeholders don't have the expertise of a fin ops analyst, so we have to bring the data to them in a format that they'll understand and they'll engage with, and that's the key role of the summary agent. The two tools this summary agent uses, first and foremost is an HTML template tool. We evaluated a couple of options and we knew ultimately these executive summaries had to be consistent. They had to follow. The same format across the board. You couldn't have one director getting format A and another director getting a completely different format or every time they're seeing these summaries, there's something new or different. We wanted the consistency so that as they began to adopt and learn from these summaries. They had the reliability and they knew what to look for every month they received them. So the the tool for HTML rendering essentially is a pog template underneath the hood. It takes in a JSON object from the summer agent and renders that as HTML. The agent then passes it to the 4th tool, which is an SES service, and it's important to note our cloud native products team built a fantastic SES wrapper that's really simplified this tool. It was virtually plug and play to send emails to every director at Granger. So, we talked a little bit about the workflow. Let's talk about the architecture. Nightingale consists of really two core runtime. We've separated the agentic layer from the MCP layer, and these two runtime interact with the MCP protocol. On the left are 2 agents. On the right hand side are the 4 MCP tools, and what's nice about this partitioning is we can continue to refine and add new tools to our MCP server layer without a single code change to the agentic layer. Authentication is handled out of the box by Agent Corps. In this case, we're leveraging IM authentication for its simplicity. But we evaluated OAth as well, and ultimately for us in this specific use case IM authentication was ideal. The tools themselves. are backed by data layers using Redshift. Exports from cost optimization hub and external APIs and services. So, this is the output, right? As I mentioned, the output is the actual product, not the input. Let's step through this executive summary a little bit. And again, the idea here is that a leader can walk away in 5 minutes with everything they need to know about their cloud cost and usage. They weren't coming to our tools, so we brought the data to them. At the top is the headline metrics, right? What is the trend for the last 90 days and what is essentially the total spend for that time period. Followed by key points and so with this right, we really wanted to keep the most important information up at the top. So that if they only had 2 minutes, they would have sufficient information. To walk away with at least the bare minimum they needed to know about their cloud footprint. Followed by a deep dive, which goes in a bit further into service and account driven cost increases or decreases, we also highlight potential anomalies. And like what's important to note here is this is a gentic inference, right? Most people reading this would not know that this was generated by an agent unless we put the little disclaimer at the bottom. And we found with this that In many ways it met the same specialist quality as our fin ops team. And as I said though, as opposed to a matter of a full day, it can now create this summary in minutes. Lastly, we have recommendations and outlook, and this is kind of key. Currently we're in the informed phase of the FinOs life cycle as we begin to democratize and broaden adoption across the organization, but we really wanna get to this place where we're operating, right? We're optimizing and operating within FinOs capabilities, and that's where the recommendations come in. These provide real actionable insights for our people leaders aggregated at their level and tied to a dollar amount. They can pass this along to their tech leads, and these tech leads can take action by identifying. The root account where these resources live. Similar to Mondelez, we're able to attribute account level ownership because each account belongs to a specific team. And lastly, the outlook. This contains essentially a forward looking perspective, assuming. The recommendations are acted upon and current trends continue. So Where do we see this going? There's a lot of interesting possibilities here. I, I, I think there's nothing unique to FinOps, as I mentioned, when it comes to the scaling problem. We've spoken with DevOps, SRE teams, other platform engineering teams who are in a similar boat, right? They have critical insights trapped behind dashboards. And the customers and stakeholders simply aren't coming to them. Um, But also within Finops itself we have multiple personas that we serve. It's not just domain and product leaders we serve. Uh, product owners. Finance procurement executives. Each one of those personas has a different way of working and ultimately a different signal channel. Where we need to bring the data to them. So as we look to extend this tool. I believe it involves a number of things. First and foremost. We didn't use Agent Core Gateway when we started at the time they didn't support native MCP tool use. Today now that it's GA, they do. And so adding Agent Core to Nightingale really unlocks the scalability of adding more MCP tools and servers at the enterprise level. We can start pulling right sizing recommendations for Kubernetes and pushing pull requests to GitHub repos for each specific Kubernetes deployment, or we might be creating Jira stories. And for Jira projects for our product teams. It could be pushing data to power BI dashboards for our finance team, all controlled through the MCP layer and routed via Agent Core gateway. At the agent layer. As we continue to expand. We're looking at adding more agents with varying expertise, right? We found that it's best to narrow the focus of an agent to one specific use case or one specific field. So, efficiency scores, right? As we start meeting our engineering team demographic where they work in developer portals, we could have an agent. Specialize in building out efficiency scores for each team. This would essentially be followed by an analysis from the executive summary agent. And linked back to our cloud cost and usage tooling. And finally, to kind of tie it back together. With Project Nightingale, we were able to solve the scalability challenge and began to democratize FinOs. By using Finop's quality analysis. And meeting an audience of 50. Through generative AI solutions, Project Nightingale has unlocked a path for us. Where we now have a force multiplier. That we didn't have before, right? We're a team of 7. There's no possible way we can meet every persona with this level of quality. At this scale And In a feasible amount of time. Um, as I mentioned, this is only one single channel among among many, but now we have the foundation. To extend beyond just products and domain leaders. Lastly, thousands of specialist hours are now saved, right? I'm not saying generative AI is replacing specialists. This unlocks our fin ops team to now go focus on the hard problems that humans are good at. Whether that's deep dives into architecture, helping platform teams re-envision a more efficient and effective platform, consulting product owners about PCO. Our team of specialists now has more time to be served to better purposes. So, if you're an SRE. DevOps leader, platform owner. Don't just wait for your customers to come to you. Seek out solutions, leveraging generative AI. And bring the data to them. Thank you Chris. Thank you JC for sharing stories from both of your companies. We heard a little bit about how you can use automation to create a way for the IT department to move at the rate of change of technology and be best at supporting the business. We heard a little bit about how you can use, uh, agents in FinOs to create better visibility of costs and revenue implications of IT decisions up all the way to the C-suite. So. These are two examples. What I'll leave you with is just sort of a framework to think about um how you can be on your own transformation journeys and just as you know consumers don't move sequentially through their path to purchase, they jump around through it, where you might be in this in this um in this framework might differ based off of your idea or your solution or where you are simply in your maturity with AI, um, but you know. I'll leave you with this for you to think about what you might be able to innovate in next and then we'll leave the extra time that we have for you to come and talk to the speakers afterwards on the side thank you and before my head of marketing kills me, please, uh, fill out the survey on the app. Thank you.