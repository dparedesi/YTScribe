---
video_id: iTcVCqpLsbU
video_url: https://www.youtube.com/watch?v=iTcVCqpLsbU
is_generated: False
is_translatable: True
summary: "In this technical deep dive, Philip from BaseTen outlines the architecture, principles, and specific engineering techniques required to build high-performance inference systems for frontier AI models. He begins by highlighting the explosive growth and increasing quality of open-source models, citing examples like DeepSeek R1 and Flux, which are now closing the performance gap with closed models across modalities including Large Language Models (LLMs), image generation, and voice. This shift necessitates a new discipline he calls \"Inference Engineering\"â€”the art and science of running AI models in production reliably and efficiently. He establishes three core principles for this discipline: optimization requires constraints, meaning you cannot maximize every metric simultaneously (e.g., throughput vs. latency); scale unlocks new performance techniques, such as large-scale parallelism that is only cost-effective at high volumes; and systems must remain dynamic to adapt to changing traffic patterns in real-time.

The presentation details the \"Inference Stack\" across two critical layers: Runtime and Infrastructure. At the Runtime layer, the goal is maximizing the throughput and minimizing the latency of individual execution units. Philip explores advanced applied research techniques now used in production, such as **Quantization** (selectively reducing precision to 8-bit or 4-bit, specifically mentioning NVFP4, to leverage tensor cores without sacrificing quality) and **Speculative Decoding** (using smaller models or algorithms like Eagle 3 to generate draft tokens for faster inference). He also discusses **KV Cache-aware routing**, which reuses computed states to dramatically speed up code completion tasks, citing a customer, Zed IDE, achieving 2x faster completion. Furthermore, he highlights **Disaggregation**, a technique that separates the prefill (processing input) and decode (generating output) phases onto different workers, allowing each phase to be optimized independently.

However, a highly optimized runtime is insufficient without a robust Infrastructure layer to handle scale. Philip emphasizes the importance of **Auto-scaling** to match GPU provisioning with fluctuating traffic, preventing wasted spend during quiet periods and service degradation during spikes. He introduces the concept of **Multi-cluster Capacity Management**, which abstracts compute resources across different physical regions into a single global control plane. This allows for \"active-active\" reliability and ensures that if one region is saturated, traffic can be intelligently routed to another, maintaining low latency and high availability. He references \"Latent,\" a pharmaceutical search company, as a beneficiary of this reliable infrastructure.

Finally, the talk expands the scope beyond text generation, explaining how these principles apply to other modalities like embeddings, video, and audio. Philip uses the example of **Superhuman** (formerly Grammarly's acquisition), which cut P95 latency by 80% for their varying embedding models by treating embedding inference with the same architectural rigor as LLM inference (e.g., batch management, multi-worker tokenization). He concludes by summarizing the four pillars of a production-grade inference service: world-class performance metrics, four-nines reliability, global scalability across regions, and the flexibility to serve any model from the open-source ecosystem, enabling enterprises to build differentiated, mission-critical AI applications."
keywords:
  - Inference Engineering
  - BaseTen
  - Open Source Models
  - Quantization
  - Speculative Decoding
  - KV Caching
  - Disaggregation
  - Auto-scaling
  - Multi-cluster Management
  - GPU Optimization
  - Frontier Models
  - Embedding Inference
---

Hey everyone, uh, thank you so much for joining us this afternoon. I'm Philip from Base 10 and I'm going to be talking about high-performance influence for Frontier AI models. Really quickly, we're gonna cover a few things. We're gonna cover the idea of influence engineering, the rise of open source foundation models, the components of an inference stack, including run-time performance optimization and infrastructure, and then look at what that does in production. So I'm from Base 10, we like to say Base 10 is influence. We are an influence provider, we're in the AWS marketplace and we serve open source, fine-tuned, and custom models on infrastructure that's purpose-built for production. We think about influence in a few different parts. Number one is performance. At the run-time level, you want to make sure that your actual uh GPU is utilized to its fullest extent to create the highest possible throughput at the lowest possible latency. On the infrastructure level, once you have that incredibly efficient model service, eventually you're going to have enough traffic that you're going to be saturating it, so you need to be able to scale horizontally to 2 replicas, 10 replicas, 100, 1000 replicas across clusters and across regions. And then all of that needs to be wrapped in a delightful developer experience and delivered with hands-on support from expert forward deployed engineers. So why did we build this? Why does all of this exist? It really starts with open source frontier AI models. And today there are well over 2 million open source models on hugging Face. This is up from just a few tens of thousands 4 or 5 years ago. And with these models, it's not just about the number, it's about the quality. Open source models are now routinely hitting frontier quality across different parameter sizes. When you look at stuff like the Kimmy K2 thinking model or think back to January, Deep Seek R1, we see open source models finally crossing the gap versus closed models and delivering frontier intelligence. But it's not just for LLMs, though doing this in voice, we have outstanding models for automatic speech recognition, for text to speech generation, for dualization. We have great image generation models from Flux and stable diffusion. We have models for generating and processing videos, as well as embedding models that can be used for all kinds of tasks on various data including both text data and multimodal data. So with all of these different models, all at Frontier Quality, you need something that we call influence engineering. So inf influence engineering is the process of running AI models in production, and there are 3 principles that kind of define this this engineering practice. The first is the idea that optimization requires constraints. So if we're going to optimize our system's performance, we need a goal that we're optimizing toward. We can't just try and make it better at everything. The second is that scale unlocks more performance techniques. If we're going to use something like large scale parallelism, disaggregation, or other performance techniques, we're going to need enough traffic that we can cost effectively leverage the hardware required to actually pull this off. And finally, we're going to need to stay dynamic, we're going to not configure our system one time statically and deploy it. We want our system to be able to update itself in real time, um, to adjust to the different traffic configuration that's coming in from our live user base. And one thing I like to talk about to sort of illustrate these points is the idea of, you know, I'm, I'm a big fan of the NFL and one of the reasons that I like watching the NFL is you see those guys out there on the field, and they're, they're big, they're fast, they're strong, and they're playing this sport. But they're not as big as sumo wrestlers. They're not as fast as Olympic sprinters. They're not as strong as a, say, champion powerlifter. And I think this is a useful lesson when it comes to inference optimization. Sometimes it's easy to get caught up in just trying to be number one on whatever benchmark. What really matters is making sure that you have the right mix of capabilities to actually serve the unique needs of your application. And a great example of, of someone who's really found that balance is open evidence. So the easiest way for me to explain open evidence is it's a chat application for doctors to be able to get up to-date information. And so with uh there there are. They're a massively successful AI healthcare startup, and the CTO said about Base 10 that we support billions of custom and fine-tuned LLM calls per week for serving high stakes medical information to healthcare providers at just about every major healthcare facility in the country. So how do you achieve this scale? How do you do that with this excellent performance and reliably enough to be used in a healthcare setting? So that really comes down to the idea of an input stack. Again, influence is multiple parts. You need both the one-time component, you need to make sure that your individual GPU is working as well as possible, and you need the infrastructure component. You need to make sure that you can scale that optimized instance across many, many replicas. So let's start with the one-time layover. What does that look like? So inference run time is really about applied research. There's all of these really interesting papers that are constantly getting published, you know, NAS is in San Diego right now. There's a lot of really great research happening there. And the question is, how do you take these ideas from papers and actually apply them in production. So one of the major pieces of optimization technology that we use often is quantization. Quantization is the idea of moving from say like a 16-bit floating point number format down to 8 bits or 4 bits, so that you can access higher power tensor cores as well as access uh or take better advantage of your memory bandwidth by sending less data with each pass through the model. So we think about quantization both in terms of being really selective with the formats we're using. I just gave a talk at Nvidia's booth yesterday over there about everything we're doing with NVFP4, which is a new micro-scaling data format with Hopper or with Blackwell. And then we also think carefully about what we quantize. Quantization isn't as simple as taking your entire model, chucking it in your model optimizer, and bringing out a, a quantized model. You want to think carefully about quantizing maybe just weights, just weights and activations. Maybe you can carefully quantize the occasion. cache to FP, you probably want to leave attention alone and even within these, you know, within these neural networks, within each of these components, maybe you quantize only the middle layers and leave the input and output layers intact. So there's a lot of granularity that you can do in quantization to make sure that you preserve quality. Another main driver for us of performance is speculation, speculative decoding. You can use these different algorithms to generate draft tokens, which increases tokens per second on memory-bound decode. With every pass through the model, we can create more than one token, which is really promising. With speculation, we do a lot of different algorithms. Some of the ones that are. Really great results right now are Eagle 3, which uses a sort of specialized trained model to do the speculation. The model takes hidden states from the target model and generates draft tokens. We also do a lot of look-ahead decoding gram speculation, especially in the code completion field where you have a very constrained vocabulary. Another really important optimization for us is caching. Um, we do a lot of KV cash-aware routing to ensure high hit rates on KV cash reuse. And with KV cash-aware routing, um, this is especially important for stuff like code completion. So a customer of ours, Z, uh, which is an IDE, was able to With the base 10 Inference stack, including a good deal of KV cash reuse, achieve 2x faster end to end code completion. So as you're typing, those suggestions pop up twice as fast with a 45% lower P90 latency and 3.5 times faster Ohio system throughput. So that's just one example of what these techniques can achieve. A couple other things we think about a lot are parallelism, especially with the rise of mixture of experts as the predominant architecture within large scale language models. You have techniques like tensor and expert parallelism that need to be carefully balanced to ensure that you're making the right trade-offs between latency and throughput. And also with other modalities like video generation, which challenge even 8XB 200 systems, you need uh approaches like context parallelism to ensure that you're able to split attention across these 8 GPUs and use all of your compute efficiently. And finally, one more runtime optimization I'm really excited about is disaggregation, where you separate prefill and decode onto separate workers that scale independently. This allows you to do a lot of these things I just talked about, as well as adopting different, you know, kernel strategies, different runtime. And specialize each of your workers for the specific challenge of either compute-bound prefill or memory bandwidth-bound decode. So disaggregation, another thing supported by Dynamo, by the way, is another one of these model performance techniques that we've found a lot of interest in. But again, all of that together, I just went through 5 of the primary model performance techniques. Even if you do a world-class job of implementing those, that's not going to be enough for your production inference service. You also need the infrastructure to match that. And a big part of the infrastructure component is the idea of auto scaling. So a lot of companies, especially companies that start out with large training clusters, end up with sort of fixed amounts of GPU capacity. The problem with fixed GPU capacity for inference is that traffic fluctuates. Maybe for business applications, it's highest during business hours, lower overnight, lower on the weekends. And when you have fluctuating traffic, you know, maybe you. consumer application and sometimes you go viral on TikTok and get a million users overnight. Some days it's a lot quieter. In these cases, your static capacity is not going to be a great match for your traffic. When traffic is low, you're going to have wasted spend, and when traffic is high, even with all of these optimizations, your system won't have enough throughput. You'll start missing USLAs. So that's where auto scaling comes in to closely match the GPU capacity that you're provisioning at any given time to your traffic. At Base 10, we do a lot of traffic-based auto scaling decisions, and we're able to get this capacity via something that we call multi-cluster capacity management. While many influence services rely on only capacity within a single region, so let's say you're only in US East One, you build your entire influence system there. With multi-cluster capacity management, you're able to pool compute across multiple regions, across multiple independent. Clusters and with a single global control plane treat it as a unified resource where you're able to, if you have say 10 replicas schedule I don't know, 8 of them on one instance, 2 on the other. That gets you know great things like active active reliability across regions and makes makes sure that you have access to both more capacity, more resilience, as well as more geographic proximity to your end users for globally distributed applications. And that matters because no matter how fast you make your model server, uh, no matter how fast, you know, this bit is where the actual influence workload is running, if your network latency is slow because you're sending a request from, I don't know, Singapore to California and back, or if your queue depth is high because you're waiting on, you know. You only have 10 GPUs, but you have 20 GPUs worth of traffic. No matter how fast this model services from that influence runtime layer we talked about earlier, your end to end influence time is going to suffer. So that's why it matters to do both the run-time and the influence run-time and the infrastructure optimization together to ensure great end to end latency. And another sort of customer who's who's experienced this is you know a company called Latent. Latent is a pharmaceutical search company, and they they like to talk about how we save them a lot of stress and developer time in implementing highly reliable influence with this multi-cluster strategy and with these auto scaling capabilities within the influence optimized infrastructure. But everything I've talked about so far has been mostly focused on the idea of large language models. And I feel like large language models and AI are deservedly somewhat synonymous, but there's a lot of other modalities of open source models, like we were talking about at the beginning. There's image generation models, video generation, embedding models, text to speech, speech to text, and all sorts of novel modalities being developed today. So a great example of adapting this specific setup to a given modality is the idea of embedding inputs. So embedding models take text as input and output vectors that encode the semantic meaning of that text. And I should say also there are multimodal embedding models that take images of videos and encode those similarly. So if you have this great capability around running language models, well, how do you then turn around and run embedding models? It turns out that embedding models, like many other modalities, are architecturally very, very similar to large language models. Most of the frontier quality. Embedding models today are stuff like embedding Gemma, Queen Embed, these models that are built out of open source large language models. As such, you can use the same runtime at the end here if you're able to build the rest of the system around it. Sort of a common theme in what I've been talking about with Inference today. So for example, for based on embedding inference, you would have a model server sitting in front of it to process the requests, a multi-worker tokenizer that's able to take the generally hundreds of thousands of individual sentences or individual inputs that might be batched together in a single inference request to an embedding model in particular, stick it into a batch manager, queue it up and have. Take advantage of the same sort of token by token in-flight batching, continuous batching mechanism that your runtime provides for language models, and then all of a sudden you have the same high quality influence service in a completely novel modality. So we've done this for embeddings, we've done this for speech to text, for text to speech, for image generation, for video generation. So all six major modalities of model that you could think of. And a uh a customer of ours who's using this is Superhuman, um, you might know them as they, they were recently acquired by Grammarly, who then themselves rebranded as Superhuman. So Low, um, he was the CTO of the original Superhuman email app. Um, with this base 10 embedding influence, they were able to cut the P95 latency by 80% across tons of different fine-tune embedding models that power key features in their app. And you'll notice one thing I, I like to highlight, I'm, I'm always proudest to highlight these P90, these P95, P99 latency gains, because they show the impact of this two-part problem that I've been talking about all day up here, which is, you know, it's not enough to be able to run fast, you need to be able to run fast reliably, and that comes not only from the runtime, but also from the infrastructure. So two of you, um, if you're going to build influence in production, you need to build 4 things. You need to deliver world-class performance, state of the art performance in terms of time to first token, tokens per second, or whatever other metrics matter to you and your end users. You need to pair that with infrastructure that delivers 4 9s or better of reliability. So that you can trust your applications for mission critical deployments in fields like healthcare. You need to be able to scale GPU capacity across regions and you know maybe even across different VPCs so that you can handle, you know, AI AI powered applications are growing fast. We see customers. Growing by multiples and multiples compounding within a single year. And so you need to be able to very rapidly both scale on a month to month sort of global view. You need to be able to scale compute as well as individually within any given day. You need to be able to scale automatically up and down with traffic. And then finally you need to be able to do that not just once, not just for one model, not just for one modality, but for any model, any of the 2 million open source models on hugging Face, any fine-tuned model, any customized model that enterprises are increasingly turning to to deliver differentiated value and production. All 4 of those things need to be done together. Thank you for the time today. Um, I really enjoyed speaking with you. So we're going to be at booth 1632, which is like right over that way behind the Reddest booth. Um, we've got these artificially intelligent t-shirts. We've got a bunch of my teammates over there doing demos. If you have any questions whatsoever about the content that we talked about today, please join me over there. Thank you so much for your time.