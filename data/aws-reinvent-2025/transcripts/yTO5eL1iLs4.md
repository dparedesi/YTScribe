---
video_id: yTO5eL1iLs4
video_url: https://www.youtube.com/watch?v=yTO5eL1iLs4
is_generated: False
is_translatable: True
---

My, my name is Shika Verma. I'm the head of product for Amazon Sage Maker. Um, some, some familiar faces in the room here. Thank you for being here. I'm also joined by two of my colleagues here, Leo, who's our chief, uh, demo officer. He's gonna show you a very cool demo, and Karen, who's our chief data and analytics officer from NatWest and a customer, and we're delighted to have her on stage with us. Thank you guys. They'll join us shortly. All right, so let's get started. We do have a power packed agenda for you. So I'm going to, of course, start with asking a question. How many of you have been working on new initiatives in the last 12 months? You can raise your hands. Many of you. Awesome. That's what I expected. And how many of you consider that you're fully successful and created net new value for your company? Some, yeah, yeah, yeah, yeah, well, that's why we're here, um, so that's why we wanted to bring this group together and with this agenda because, as you know, um, Gartner says that by 2027, 60% of organizations will fail to realize that anticipated value if you don't have your data and your data governance together. Do we agree with that statement? Yeah, that's why you're here. OK, awesome. All right. So we know that data is the foundation for AI, right? Tip of the iceberg is the AI stuff, the cool stuff that we all want to do, but there's a ton of work underneath that we must get in place first in order to get the true value of it, right? There's obviously managing your data correctly at the storage layer, then processing it all, then cataloging it all, adding the right metadata to it so that you can discover and use it, and so on and so forth, right? So let's come back to what you guys have been telling us, AWS, as to what you would like to do. Please chime in with your hands. I know you, I won't be able to hear you because this is a silent session, but please chime in with your hands. So the voice of the customer, I've been in the industry for 25+ years, and what I have heard over and over again from customers is they want really 3 things to enable their NEA initiatives. One is they want a single place for all of their structured and unstructured data. Right? All kinds of data as well, your data, your models, your dashboards. Even the agents you're creating should go into a place where anybody can discover and use them, right? Then you want metadata to be sort of the, the, the main context that is added to all of these things that you're cataloging so that you can discover them easily, not just as humans, but AIs are not gonna call anybody, right? Our agents are not going to call another human to get context, so we better bake that context into our data. Then of course all of that comes together with consistent governance across all of these data sets, right? How do you manage it? How do you give permissions to all of these data sets? Who can give permissions to these data sets? How is it being used? Are you able to share it with others or not? It all very consistent, um, consistently needs to be applied across this. Does this make sense? Again, I'm gonna do another show of hands. OK, cool. Many people. OK, perfect. So let's take it, take it back a little bit, right? I think a lot of us have been sort of in the data space for quite some time, and how does it really start again with structured and unstructured data sources coming in. Right, we process it all. We used to do data warehouses, then we did data lakes, then we bought all of it together in lake houses, and now it's like, OK, just leave the data where it is, and we just manage wherever the data is, but with the right context. So what do you need to do this in today's day and age, right? The central thing is metadata, right? The data about the data which gives us context, it gives it meaning so that you can use it in the correct manner. This is vital for this new day and age, right, because again, as we talked about earlier, AI agents might call other AI agents, but you know, like Tom called Rick, like, hey dude, like what's this data? they're not gonna do that. You don't have time to do that because you want agents to be uh have as much automation as possible as well. So just like we all need maps and navigational systems and as that evolution has happened, really AI needs metadata. Metadata is cool. Metadata may be the word of the decade, um, you know, we have to spend a lot more on metadata so that it becomes the data becomes usable for humans as well as AI. So let me talk to you about some of the solutions that AWS has around this. How many of you have heard of Amazon Sage Maker? OK, well, that's why you're in the session, of course, right? So we have, um, created a new solution, um, we launched it last year, Amazon Sage Maker Unified studio and Amazon Sage Maker catalog. Which really gives you that underpinning to have all kinds of assets cataloged, right? So in the center of Sagemaker catalog you'll see there's data, models, your Gen AI, um, agents you're creating, your dashboards, all of that can be cataloged, right? And then you have the centralized metadata repository for all of these things. You have end to end lineage of where is all of this data and assets coming from. You can add context to it via the built-in tools that we have. Leo, our chief demo officer, is actually going to show you a demo of just that. How can you bake in metadata automatically without having to create it yourself, because it's a ton of work to add all this context to all of these assets. And then of course data quality, it may be old fashioned, but boy it's important, right? I hope everybody is paying attention to that because with bad data you're going to get bad results and agents with the hallucination that they do, oh boy, they can create a lot of interesting insights and a great deal of confidence with very bad data. You don't want that to happen, so data quality is paramount right now. And of course a centralized way of discovery and sharing is something that we, we always encourage, right? So there's 3 things that I would like you to sort of remember from this. What does Sagemaker Catalog offer you? Number 1, it gives you that metadata-driven approach to all discovery, right? You can leave the data where it is, but you have metadata come together so that that becomes how you search for your data. Then we have AI, the AI ready data. There are two ways that AI plays into this ecosystem, right? One, you can use AI to get your data ready. And second, you have to get your data ready for more AI, right? So it's a loop that feeds into the other. So we have both of those things that we're going to show you in a demo. We're keeping it very show and tell today. So I'm gonna kind of run through some of the material and hand it over to Leo, who's going to show us an end to end demo, and then Karen will help wrap us up with a real life customer use case which I'm sure a lot of you might be dealing with some of the problems that Karen has dealt with and solved. So, um, please do, um, engage with her as well, uh, after the session if you need to. And then last but not the least open by design. So one thing I think we have, we have to all accept is, you know, there are more systems than just AWS systems. There are more databases than just what AWS offers, right? How many of you use things other than AWS here? Yeah, me too, right? So what you really need is to adopt a solution that will offer you options to when you need to bring in data through, say, you know, an iceberg compatible API from data bricks or Snowflake or wherever into the same ecosystem, as well as you have things like Open lineage to capture lineage across the breadth of your data assets that you might have going through your system. So we'll show you all three of these things today, so, you know, it should be fun. All right, so let me quickly show you uh what is new in these categories with Sagemaker this year. So we've been hard at work since we launched last year. Uh, 200+ releases have gone in across the breadth of services, right? So there's Amazon Sage Maker, but then S3 at the storage layer level level we have added metadata capabilities. At the processing layer level we have added capabilities with glue, lake formation, and the whole stack sort of comes together in Sagemaker. So there's tons of stuff here which I won't call out necessarily, but just wanted to just show you that we are investing a ton in this space. So metadata being the, the golden child of the decade, we are recognizing it and accepting, you know, really investing a lot in this space. And since last year, a lot of customers, some of you I see in the room, uh, here, um, thank you very much, um, Karen is here from NatWest as well, uh, on our top left corner here I see Tomaso from Hema in the crowd. Uh, there's other folks from NatWest in the crowd as well. Thank you guys. Um, does anybody else see that logo here? If you're in the room and you see a logo here, please raise your hand. All right, we'll get you in the next year's slide. All right, so something that is new here that we launched a couple of weeks ago is column level metadata forms. So, What this gives you really is that your metadata doesn't stay at the table or database or asset level, at a column level you can add additional context to your data, so you can say this particular column can be used for this kind of analysis or this column. Is PII, so don't use it for, you know, any non-PII related purposes, whatever it is, right? So you can manage it at the column level. I'm super excited about this, and some of our largest customers were waiting for this feature to be alive so that they can manage their data at a much more granular level. So super excited about this one. Leo's gonna show us this one in, in, um, live in demo as well. So now, um, you know, just going with the same metadata theme, right? Last year we launched automatic metadata generation, which Leo is gonna show you in our demo, which is that you pick a table and you say generate me a description, and it generates you a description based on the content of the columns that it sees. It gives you recommendations on how you can use it. So to that this year we have added automatic glossary term um suggestions. How many of you use. Glossary terms in year. Yeah, some of you, OK, cool. So now you can, you can imagine, right, that that's very, that's a lot of hard work trying to create all these glossary terms, then make sure that you can associate them with everything that is coming in, right? So with this feature what it does is if you have glossary terms generated in terms of you have a standard set of glossary terms. With any new data asset coming in, even at a column level, it can correlate information and it can show you, like in the screenshot here you can see that it can generate gloss returns for you and it can correlate and show you, do you want to apply these glossary returns to this asset, and you just click a button and it does it for you. So simplifies and really speeds up the whole process. And then also at, you can enforce metadata rules and glossaries as well. So, you know, it's not just like. We come up with these things, but nobody uses them. We know the story, right? Oh my God, business teams are not using the data, using our rules to actually upload the data. Well, you can enforce it through this, right, because that's the advantage of having a centralized system is you can have producers and consumers playing freely, but then you can build the technology in the middle to help you get the controls that you need for your company. And you can relax them as much as you want. So if you're a no controls kind of company, well, congratulations, you can relax the rules. But more and more enterprises are becoming very, very conscious of the data that they use. Um, this is net new today, so with all of this metadata that is getting generated and all of the assets are being used, we are putting this back into the S3 layer, right? How many of you use S3, by the way? Most of you, right? So one thing that we are doing is that anything that we create on the top of the stack in terms of additional metadata, we want to push it down all the way down to the storage layer, so that no matter where your agents are interacting. I see somebody really agreeing with me there. All right, um, so wherever your agents are interacting, either they're interacting at the top of the stack or they're interacting at the sources there, you get the right metadata to be pulled up and used, right? So that really, really simplifies a lot of um. Uh, a lot of stuff for you guys and for us actually. We are actually also using this feature internally for ourselves because we push our data assets back into the back into Sagemaker as well, you know, we like to drink our own wine or whatever you may call it, right, but we use the feature ourselves as well. And then another advantage that this gives you, I know Tomaso from Hema had this feature request, is where I want to see who is using these assets and how many assets are being used by whom, right? You get all of that from this particular feature. So down back to S3, it can be shared as a reusable asset. You can bring it all the way up to the stack, or you can use it where it belongs. All right, so now we talked a ton about the AWS ecosystem. I often hear from our customers that our lives are just always diverse. We have so many third party players that we already have in the system, and we want you guys to just work with them. So raise your hand if any of these names are familiar to you. All right, cool, so we don't want you to think that all of that investment went to waste, right? If you're using Colibra Atalona relation, we have sync solutions that we have built with them. So I'm very excited to share with you guys that you can sync metadata across the non-AWS catalogs with AWS catalogs such as Sage Maker catalog. Which means that the hard work that your data stewards and your governance teams may have done in any of these catalogs can be synced into SageMaker and can be used from SageMaker for your discovery and analysis for your developers. So we'll show you this in um in um action as well. Is this exciting? Yeah, OK, cool. Well, let me give you something more exciting. So we're also introducing catalog federation to external iceberg catalogs. How many of you use Snowflake or data bricks here, right? Almost all of you. So what this gives you is if you already have Snowflake or data bricks. Um, in your companies, right, as long as the commonality is iceberg, you can pull all of that data and use it with AWS analytics engines. So you can bring that data, the metadata over to Blue Data Catalog. And then you can use Athena, Sage Maker, any of our engines to basically process and use that data. What that also shows you is that this data will become visible in Sage Maker catalog just the same way all of your AWS data, and you can use all of our generative AI capabilities to generate auto business descriptions, etc. on all of this data. So imagine the power of that. Like this is one of the most exciting things I feel that we have done for customers because this really opens up the entire ecosystem for you, and you can truly trust hard to bring it together. So I'm super excited about this one. Now you put all that data to use. Now how do you bring it together and actually use it, right? So, so far we have talked about you brought your AWS and non-AWS data together. It's all available in one central catalog. So now this new feature in Sage Maker, which is one click on boarding of existing data sets, how many of you are familiar with IAM and you know just how policies are set up, right? And every AWS customer is familiar with that. So what this allows you to do is if you already have permission set up in IM for your data sets through single click you can bring it over into Sagemaker, and what that gives you the power to do is you can now use things like this new notebook, this fancy manchy new notebook that we have just launched, which you'll see Leo showing. It's a polyglot notebook, server less, no provisioning required. And it's like you can have a cell in which you can have Python, the other cell can use SQL, you can correlate the cells, use data sets between them. You can use visualization. It is super cool. I can't wait for you to see this one. And then There is also an agent bill in the notebooks, of course, like we had to do that, right? We couldn't have not had the agent and what this agent does is with a simple prompt it can generate your entire code base. You will see it in Leo's demo. It can create new pipelines. It can schedule it for you. And of course with, with, uh, a great explanation of how, you know, the step wise thinking that it's going through. How many of you are using any kind of, you know, IDE like cursor or Windsorf or something in your environments? Some of you, right? So you know the, the, um, are lovable, no, not many. Some of them, OK, so you should definitely give this a try because this is with a prompt. I was able to do deep analysis and I just fed it a few spreadsheets, honestly, just raw spreadsheets. I didn't even apply any data quality rules on it, and it gave me a, a beautiful analysis of just this thing correlates to that, right? I was using dress sales, sorry, like just, you know, types of things that I like and the sales data that I just uploaded from somewhere. And it can give you a really cool analysis with very less work, so I encourage you to give that a try for sure. So now, who's ready for a demo, hm? All right, Leo. Our chief demo officer for all of you, thank you, Chika. Hey folks, so thank you Chica for all the announcements. Now let's see every single one of them in action, OK? I have a very comprehensive demo showcase showcasing all these features. OK, let me give me 2 minutes. I'm going to switch to the demo mode and we can start. Perfect. Can you see the screen? Good, perfect. OK. So, uh, we are going to start with the use case, OK, for the use case we have 4 different personas. We have Leo. He works for sales. He's a data engineer. He's going to create a data set for us, OK. Then we have Samantha that also works for sales, but she's a data steward. She's going to help us to document that data asset with business context. Then we have Sarah. Sarah works in a different department, in this case, marketing, and she is going to play with the data. She's going to run some SQL. She's going to create dashboards, and so on. And then we have Oliver that works with Sarah. He's a data science. He is going to play with the new notebook that Chica just mentioned. And he's going to create a forecasting model using Sagemaker notebooks, something that just to give you a more context, for example, Sarah and Samantha, they're going to interact on top of the same data, but they don't know each other, OK, personally they don't know each other, but they are going to innovate and collaborate on top of the same data. So let's see it. OK. The, the first use case, as I mentioned before, we are going to work with Leo. OK. I'm going to show you very quickly how, uh, he's going to create a, a, a, a data pipeline using SEM Unify Studio. OK. We are here in the home page, OK. We go to our, uh, to our build session. We select the visual ETL flows. OK. Let's create one very quickly. OK, but you know what? Well, here you can see everything that we have available sources, transformations, and destination targets to create our ETL jobs. But today we are lazy. OK? It's 4 p.m., so we are going to use a GAI in order to generate the pipeline, OK? I'm going to paste here the prompt, OK, that I'm going to use, OK, and you can see here all the description, join this table with this one, do this aggregation and so on. OK, and I'm going to submit it and just with that folks, I create a data pipeline, OK, so. Next step I'm going to save it uh just to let you know, just because I use GII to generate it, that doesn't mean that I cannot change whatever I want. OK, here I'm changing parameters, OK, um, in order to customize my job. I'm going to save it and after I save it, of course I need to, to run it, OK. So let's click run. And we have the confirmation message that the ETL job ran successfully. OK, now let's see the result. OK. It's an aggregation of different tables as you saw before. So here we are in our Data Explorer and you can see the new table that was created, OK, sales performance by buyer. So actually here from the Data Explorer you can see the columns, the schema, and also you can see a sample of the data, OK, in order for you to double check that everything is correct. So now we have our new data set and we are going to transition to our next persona, in this case Samantha, as I mentioned before. She is going to be our data steward and I'm going to show you how to document how to add business metadata to this asset, OK, so. Let's go back again to the home page of Seese Unify Studio in this time as Samantha, OK, we are going to go to the data part, the Data Explorer that I showed you before, but in this case I'm going to select data sources. Why? Because we are going to harvest the technical metadata that was from the asset that was created by Leo. OK? I already created this data source for a job for you for the demo. I'm going to just run it, OK. So it's going to take just 3 seconds and just with that when it finished, that means that we harvest the metadata from that source, OK, as you can see there. So let's go and add metadata to it as I mentioned before. Let's go to the asset section, the inventory part, and let's look for the asset that we just create, OK, sales, sales performance by buyer, and that's going to take us to the home page of our asset. OK. So here you're going to be able to add metadata at the column level, at the asset level, and so on. So something that you should notice here is that we have this starts icon in different sections of the asset. That means that you can generate descriptions using DNAI. OK, I'm going to show you, but I just wanted to let you know that that that's the reason why we have that icon in different sections, OK. So the first thing that I'm going to do is I'm going to generate the descriptions. So based on your technical metadata and using GNAI, of course, we are going to generate a description for your asset, OK? You can reject it, edit it, accept it, up to you, OK. In this case we are going to accept everything. OK. And also we can add a rhythm. Uh, we support markdowns, so you're going to see the format in, in a minute. So in that way you can make it more visual to your consumers. OK? You can see it here. Now let's go to the glossary terms part. So you can see the traditional add terms uh uh option, but now we have the generate terms, OK, using GAI. So this is pretty cool because based on the glossary terms that you have already defined in in Sage makerify studio, we are going to automatically associate those glossary terms to your asset, OK? And something that is very important is, for example, we also have The ability now to detect if your data has PII information or not and based on that we are going to add a glossary term related to PII to that asset. OK, actually you can, you can see it here. Here we have the suggestions. That's why, that's why they are grayed out. If I click the spark option or the start option, you can accept or reject the recommendations. We are going to accept it and you can see that the third recommendation is PII. So we identify that your asset contains PII data and based on that automatically add the glossary term. Also, you can go and add your terms manually the, the, the way that we always have support, OK, but just to let you know that you have now the hybrid between the GAI recommendation and, and the manually assignment, OK. Let's move on. To the metadata forms part. So here you can see all the technical metadata that is coming from the source. In this case, because it's glue, we are showing Glue information, but if if the data is coming from Redshift, we are going to bring the metadata from there, OK. Also, you have the option to add your custom metadata. This is very important because here is when you add your own flavor to to this asset. In this case for this demo, I add a very simple example, OK, like the date in which this asset was certified, the business owner, the classification, and the SLA, OK. Again, you can customize this form wherever you want, OK? And also you can add multiple metadata forms to the asset if you want to. OK, with that we complete the asset level metadata, but now we have more. OK, let me show you how to document everything, um, uh, at the schema level, OK? We already generate, uh, suggestions for the description and the column names and also to the, uh, for the glossary terms. I'm going to accept, uh, all of them because I want to show you everything, uh, um, no, uh, no grayed out like the normal thing. So you can see here all the recommendations. Another thing that you have to pay attention here is that also we add, we associate automatically glossary terms to each column. In this case you can see for example the second column, uh, first name, we identify that it contains PII data and also is related to to name, OK. Again, you can edit, you can change this. Let, let me show you how. So if you click view and edit now you have your own uh metadata section at the column level. Actually, this is a nice feature because it was inspired by you and actually the person that inspired the creation of this feature is here in the audience. OK, so it's pretty cool that you are seeing this right now. So, OK, we add the, the, the metadata, the rhythm section. OK? You can also add the. We also super markdowns here. Uh, you can see the, the glossary terms suggested. You can add more glossary terms at the column level if you want to, OK? And also you can go down and you can add metadata forms at the column level. Remember, uh, before we were, uh, adding metadata at the asset level, metadata forms. Now we are doing it at the column level. So, in this case now, uh, the columns are becoming a, a first-class citizen in, in, in, as part of the catalog, OK? So we are going to add some information, ownership, and purpose of, of this specific column, OK? And I'm going to click save. Perfect. Now we have our asset well documented. We have also our columns well documented. Of course, doing this column by column could be a lot of overhead. Uh, we also support APIs for you to do this, uh, programmatic, OK. So now let's move on to asset filters. OK. This is the place in which we can implement security inside of SageM Unify Studio. So here we create, we are going to create a filter that we can apply when we approve a subscription request, and in that way we can filter the data that a specific requester is going to have access to. In this case, as you can see, we support column and row, and I'm going to filter by state. I'm going to put equal Florida. OK, so if I approve a subscription request and I apply this filter, the consumer is going to be able just to see data related to Florida. OK, again, you can have a combination of column and row filters, OK. Let's move on to data quality. Here you can see that we have 3 sections. The first one is the overall score of the asset. The middle part is going to show you a rule by rule which passed and which failed. And the last one is a histogram that shows you how the quality score changed in a period of time, OK. Now let's go to my favorite part. We have now a data lineage, OK, so this is pretty cool because if you remember the pipeline that we run at the beginning of the demo, just by running the pipeline we automatically generate this lineage diagram for you, OK? If you can see you can extend the diagram and you can see. The source of the asset that you are consuming and not only that, you are going to be able to see also who is consuming that data asset. So you are going to see not only how it was created, also who is consuming this asset. Remember that our lineage functionality is based on open lineage, so it's based on open standards. OK. Perfect, folks. So here we have everything that we need as a, as data producers in order to publish our asset. So I'm going to click publish asset, OK? And when I click this button now this asset is going to be available, it's going to be visible for the rest of the company, OK? Perfect. It's published. It's well documented. Now let's switch to the consumer persona, in this case, uh, Sarah, the marketing data analyst, and see how she explores the data catalog. OK. I'm going to show you two different ways to do it. The first one is the, let's call it the traditional way, using a search engine. OK, I'm going to browse by assets. You can see here all the assets that we have in the catalog. OK. You can see here that I can filter by grossary terms, as you remember, we add the PII tag to the to the asset that we create. So if I filter by PII, I'm going to see all the assets that contain PII. Now let's go to the more innovative way to search for assets. You can use Amazon Que. OK, just using natural language, you can ask questions on top of your catalog. In this case, tell, give me a list of the assets that contains PII data, and that's it, OK? It's going to give you a list of all the assets. Also, next to the asset, as you can see here, we show you the criteria that we use in order to tell you that that asset is the right one for your search. OK? You get more details about the context that we use in order to identify this asset. So here remember we're in the seeing everything from the consumer point of view. Look at this, it's beautiful. It's well documented asset level, schema level, data quality scores, data lineage diagram, everything, OK? So I can make an educated decision as a consumer based on all the metadata that I have here, OK? I'm going to click subscribe. OK, and here you can, as a requester, as a subscriber, you need to fill out information. Just to let you know, you can customize this form and you can ask here whatever you want, OK, based on your use case. I'm going to click request and that's it. That's all that I need in order to request access to an asset. OK, let's switch back to Samantha, the data steward. She has the ownership to review and approve the subscription request. OK. Go back to the home page. I'm, I'm going to my uh data tab. er I can go to the subscription request section. I can see my subscription request there. OK, I can see the details. You can see here all the information is coming from marketing. It was Sara, the use case that Sarah put here, and here you have the option to select full access or access with filter. OK, of course that we are going to implement access with filter. You can see here the filter that we create together, OK. And then I'm going to put here just the the the reason why I'm approving. OK, I'm going to click approve and just with that starting now, Sarah, she has the option now to access the actual data. OK. Let me show you that very quickly. OK? We are going back to Sarah and now let's play with the data. OK, we here, let me show you now we have a notification of Maconify studio. Sarah got a notification directly from the UI showing that Samantha approved the request. She can see here all the information that Samantha added, OK. And now let's consume the data. I go directly to my data explorer, OK, and here I can click preview data and that's going to take us to our SQL experience, OK? I'm going to run a quick query, a discovery query. I can see all the data there, but folks, there is a problem. I, I, I haven't told you that yet is that uh Sarah doesn't know how to run a SQL query. OK, she got the data analyst job, but she doesn't know how to run a query. No problem. Sarah can use GAI to generate the, the queries that she needs to run. OK, so for that she has to go to the Q agent that we have as part of the, the SQL experience and ask questions on top of the data. OK. So give me the top 5 cities by revenue. OK, just by that she was able to execute that query and not only that, she can keep a conversation. OK, based on these results, give me the type of event that is more popular in those cities. OK, so it keeps the context of the previous cell, and if you, as you can see here, just by executing the query, OK, she's going to get the result that she's looking for. So as you can see, she's interacting with the data without knowing how to run a SQL query. So Sarah identified that this is the right data that she needs to consume. And now she is going to create a dashboard, OK, a BI dashboard to show some some visuals, OK. We go back to our asset in this case and we select the action button and we can click open using QuickSuite. OK. The good thing about this is that it opens a QuickSuite instance, but it has everything already associated, OK, in this case, the asset. Something that I also forgot to tell you is that Sarah doesn't know how to create a dashboard either, so she's going to use DNAI in order to create it. So here we have the build option. So she's going to ask the assistant for the visual that shows the top users by by by revenue or by spend. She likes the, the, the visual and she's going to add it to, to the dashboard, OK? Perfect. We can see it here and now she's going to add a, a 2nd, a 2nd dashboard or a 2nd visual, sorry. OK. Different question is related but kind of different, and she got the, the visual that she, that she was looking for. OK, Amazing. She's going to add it to the uh to the vis to the dashboard as well. Amazing. I'm going to clean it a little, OK. I'm going to close here. I'm going to remove this default option, and I'm ready. I'm going to publish it. I click the publish option and I need to put a name for the asset. OK. In this case, what we are doing here, when I, when we click the publish button, that's going to create a new asset inside of the catalog. So you are going to be able also to catalog a Quicksight dashboard. OK. So let me show you how that looks like very quickly. OK, and let's go back to SageMaker Unified Studio. Give me a second. We are there. OK. Let's go to data. Let's go to assets again, and in this case, I'm going to look for any, any assets related to Quicksight. So as you, as you can see here, I have a new asset called revenue, and if you see the type, it says Quicksight dashboard. Also you can augment that asset using metadata, business metadata, OK? I'm adding the read me section here to my dashboard and also adding glossary terms to the dashboard. You can add also metadata forms. I'm not going to do that as part of this demo, but just to let you know that you can document a dashboard in the same way that we document a glue table, OK. Amazing. So we are going to now publish the asset, in this case, the, the dashboard. So in, in this case, as you can see, Sarah was not only a consumer, she became a producer because with the data that she consumed from Samantha, she was able to create an asset, in this case a Quicksight dashboard that she published as part of the capital, OK? She became a data producer. Now let's go to the last persona, my friend Oliver. He's a data science, OK, and I'm going to show you how Oliver is going to create a a forecast model using our new notebook experience, OK. So remember Oliver worked in the marketing department, the same with the same as Sarah. So here you can notice that we have access to the to the Adrias console, OK, because that that's the different experience that we are offering with these notebooks, OK. You can have a hybrid approach in which you have users using Sagemaker in 5 Studio using IDC, and you can have more technical users using IM roles and users, OK, to access directly to the experience. So let me show you, OK, I click Sagemaker. OK, it identifies my, my, the roles that I have assigned in this case Oliver, and based on that, it's going to offer a customized experience. OK. Let me show you very quickly. This is a new experience, as I mentioned before. You have the homepage and you have all the options available data pipelines, machine learning, everything, OK. Let's focus on the data part for now and you can see that. Oliver has the same level of access as Sarah. Why? Because they work in the same, they are part of the same project, the same line of business, so they have the same level access, OK. Even though they're using different experience, so here you can see the same data asset that Sarah got access to, but in this case he's going to use a notebook in order to play or work with the data. OK. Another thing that I forgot to tell you is that Oliver doesn't know how to create a, a, a forecasting model. OK. So for that he's going to use the AI, a system that we have as part of the notebook experience. So here I'm asking, OK, Oliver is asking to create a forecasting model about sales, OK. Look at this. This is very interesting because the agent starts analyzing the the the the request and divide the answer by different stages. OK, I can provide the answer, but I'm going to do it step by step, OK? And the thing that I love about this is that Oliver is going to interact with the agent to execute every single step, as you can see here. OK. Oliver says, yes, let's go, let's implement the step number one, OK, and you are going to start seeing how the agent is going to start populating every single cell. OK, it's generating the code. It's executing the code with me, with my permission, of course, OK, and you can see here visuals, you can see here that after we explore the or the agent explore the data, it identified that we have to do some aggregations in order to get the the the information that we need for the forecasting model. OK, we are running the aggregation right now. Again, I need to approve the, the, the. I need to approve that that step. OK, let's do it right now and start populating the, the, the cells, OK, in order to perform the aggregation. Of course, that you can see the results and you decide if you want to continue with the steps or you need to change something. In this case, it's a demo. Everything is, is straightforward. I'm going the step 2 worked beautifully. Now let's execute the step 3. OK, but again, something that I like about this is that you need to validate every single step, step before you execute the next one. OK? So now we are actually executing the, the, uh, the predic predictive analysis, OK. I haven't, I haven't. Oliver didn't have to go and, and, and run any line of code, OK? And then the last one, the last step is executing the, the recommendations and visualizations. In this case, we are more focused on the recommendations more than the visualization. OK, once I approve the the step and if I scroll down, uh, we are going to see all the recommendations, the forecasting model, based on, on my data, OK, as you see, the only thing that I had to do was adding the the right prompt, of course, and then interacting with the agent in order to get all these recommendations. OK. I know, long demo, but if you see we cover different, uh, different personas. OK? Give me a second. You know. So as, as you can see, as I mentioned before, you have 4 different personas, all of them interacting on top of the same platform, OK? They don't need to know each other, but they were able to collaborate and innovate on top of the same data. OK. Now Karen, please come up. Karen is going to show you that all this is possible in reality, OK. So Karen, all yours. Thank you. Thank you everybody, I am delighted to be with you this afternoon and um I hope you share my concern for both Sarah and Oliver's apparent lack of skills in their chosen profession. But um for those of you who don't know NatWest, if I can start by bringing to life um our story or our journey, we are er one of the main high street banks within the United Kingdom and um. We really are on a mission and a mission that I'm proud to be, uh, leading a part of. We, um, have been in existence now for 300 years and I share that with you. We will celebrate our 300th birthday in 2027, and it's only because when I join AWS, my strategic hosting partner, uh, I'm always entertained by the difference in our ages relatively. But as a 300 year old bank, there is no way we would exist today without having a strong culture of innovation and for constantly striving to find new ways to meet and serve our customers' financial needs. And as we stand today, our ambition is very much to succeed with our customers as a sustainable partner, and we want to be the bank that turns possibilities into progress. And I am a banker to trade. I've actually spent 28 years with NatWest Group. Um, you can all tell me I don't look old enough later. Um, but what I would say is that today more than ever before, banking is dynamic, it's exciting, and it is full of possibilities. That's why our purpose of turning those possibilities into progress really resonates with where we are today and where our customers are. So we want to use the data that we hold about our customers to understand their hopes, to know what they need. And it's a very dynamic and changing world that we operate in. So our data and our responsible AI is critical to being able to help our customers when they need us most. I've captured some of our core statistics. We do have over 20 million customers in the UK. We process almost 750 million financial transactions every month. And we would not be able to serve our customers if we didn't use our data as an asset, and we know that in the future AI is how we will evolve to better meet our customers where they are. So that's a little bit about us. So as the chief data and analytics Officer, I am er leading NatWest's data transformation. And actually as we've been running through our session this afternoon, there's been a number of things that have really struck home to me. One of which is now I feel is the perfect time to be doing this work. So the capability that Leo's just demoed isn't make believe, it's real. We spent a day with our Chief Information Officer Scott Marker as a team, as a data team. And we were able to demo almost exactly what Leo has just demoed for you today but using our data, creating our own assets in our own marketplace. So genuinely as a customer, I'm here to say that it's real. But if I can talk to you about our wider data transformation, so I am here to deliver high quality, well curated data in the cloud. And one of the big shifts that we're leading is moving from being a very small focused team of data professionals into making our data accessible for all of the roles and all of the teams who need to use it. Now I'm proud to say we have quite a strong track record in the deployment of AI. We recently achieved 16th global ranking in the Evident AI league tables, which is no mean feat for a bank of our size, and we've been working with AI. Diligently to ensure that we are able to leverage that capability at scale and indeed to get ready for generative and agentic AI at a much higher rate than we use them today. So we're on a transformation journey. We need to move to a modern data architecture that is simpler, so removing some of our heritage or legacy data platforms, and we need to do it really quickly so that we're able to deliver generative and agentic AI solutions to our customers. So we will support our organisation's strategy of growing our business, simplifying our estate to give us greater agility. And having strong governance is at the absolute heart of how we deliver control. I was laughing with Leo to say in the UK and in Europe we have stringent regulation that helps us to achieve the right level of quality and control over our customer data, um, but I understand that in the States it's mostly FS and healthcare who care about that, but in Europe and in the UK we have really strong regulation. And some of the demo that you've seen today helps us achieve that regulatory standard. And listen, how we're going about it is a little bit unique as well. We have created and launched in July of this year, um, a unique three party partnership across ourselves, NatWest Group, across AWS, our strategic hosting partner, and across Accenture. And this is all to make our journey go faster, so we are bringing the best of each of these 3 companies to move quicker to that modern data architecture, and our outcome, we'll know we are successful when we are able. To deliver generative and agentic AI at scale, but how that will feel is that we are personalized, we are relevant, we are able to be tailored when we're dealing with our commercial and institutional customers and our customers across the whole organization. We had a provocation to say if we did data transformation differently, could we go faster? Could we ensure that we would deliver the results, um, and could we make it safer? And I genuinely believe we're on a path to do exactly that, reducing our timeline to completion from 5 to 6 years to 3 to 4 years. And this therefore becomes our actual customer mission. So we are looking to move the whole organization to something we call our digital spine and that's how you take a 300 year old bank and make it modern, agile and able to compete with data cloud born companies. We will be using radical technology and organizational simplification to get there. If you think about making a change to some of our core systems, we carry a huge overhead and actually through simplification we'll be able to reduce the cost of change and make it faster to keep up with growing and changing customer needs. We want to be known as an AI powered bank and we're committed to do that. And the thing that NatWest brings to our partnership is this relentless focus on the customer. What do they need, what do they expect, and how can we deliver? All of this should translate into sustainable returns for our customers, for our shareholders, and for the societies in which we operate in the UK. So a really compelling mission. So what is my challenge? So as a 300 year old organization. With a small number of really important data teams, you can see that I have a lot of data, but it is difficult to find, it isn't organized, and often it's difficult to get the insights or to help support making really good data driven decisions because of the mess that you can see in front of you. Um, one of the best examples of this is today sometimes it is still fastest to find the data you need by phoning someone. Than searching for it, so that is why we need um the glossary that's part of Sagemker Unified studio. And actually I think that Shia did this incredibly well to say we are getting ready for a world where metadata is the way in which our agentic AI solutions will find the data that they need and access it using the control that we're building through the Sage Maker Unified Studio. So I think we said that humans need maps, uh, but AI needs metadata. So we've touched on many of the core technical capabilities that we're seeking to leverage as part of the NatWest Group's journey to that modern data architecture. So let me skip past that to say, so realistically, how am I using catalog and how am I using it today? And I think the key thing that I would like to hammer home is. I'm very proud to lead a team of almost 3000 trusted data professionals, whether they're engineers, whether they're analysts, whether they're data scientists, and I know that I can help them find what they need. But to be faster and to be more customer focused, I need to federate or. Increase the access that all 72,000 employees of my company have to the data that we hold about our customers, and the only way that I will achieve that outcome is having my data and my APIs discoverable through the catalog as part of Sage Maker Unified Studio. It is also the only way in which I will drive reuse, which makes me faster and it makes me more consistent in terms of the products and the services that I'm able to offer. I can share my data without moving my data, without storing duplicate copies of my data in every different team. And most importantly for me, I can keep my most critical asset, my customer data safe because I get to control who is able to access down to a really fine level through the catalog within Sage Maker Unified Studio. So I'd like to invite Shia and Leo back to the stage, and we're gonna share with you, I think our top 3 takeaways. Thank you Karen. Was that fun, folks? Did you learn something new? Yeah? All right, so we'll bring this to a wrap. We'll all be around. Um, if you guys have questions or anything, we'll be hanging out here, so please feel free to grab us. But here's our top 3 takeaways. Um, number 1, you know, I think we talked about metadata a lot today. Extend your architecture so that you have metadata added to your mainstream. You have governance added to your mainstream so that you can. Amplify your journey with AI. All right. Second takeaway, Leo? Yeah, uh, as, as you saw, uh, SageMaker Unified Studio is a central place that you can use. It doesn't matter the type of personnel that you have, uh, in order to have them innovated on top of the same data. So using SageMaker Unified Studio as a central place, as NatWest I just mentioned, was the, the magic formula to make everything work. And mine is start small and build incrementally. We've been proud to be part of the journey of innovation with AWS starting with Datazone and moving into Sagemker Unified Studio. And as part of that work we've been a strong voice of the customer to help shape that design. But as a company, it is incredibly important to make progress quickly, supported by our partners in AWS and Accenture and I would say start small, build incrementally but deliver. Initial and ongoing value in order to continue to get your organisation's support to make this change. Cool, thank you very much. Thank you. Folks, uh, don't forget to leave your feedback. Please leave your feedback. Also, I wanna highlight that we tried something new with this session, so we're looking for your input. We did very less tell and a lot more show, and we included a customer example, uh, for you to learn from. So if this was useful, please do put it in the survey so that we can do this more going forward. And of course there is a ton of sage maker sessions throughout the remaining days. I think we have 30 of them, so you can drill into whatever areas you want to get into and uh come grab us. We are here. Thank you all. Thank you folks.