---
video_id: xDXsN9hYmlo
video_url: https://www.youtube.com/watch?v=xDXsN9hYmlo
summary: "In the session \"Tapping into the Power of Agentic AI: Driving Mission Success with NVIDIA & AWS,\" an NVIDIA expert presents a comprehensive strategy for transitioning Generative AI from experimental chatbots to production-grade \"Agentic\" systems. The speaker posits that the industry has evolved rapidly since 2023—moving from simple LLM interactions, through Retrieval Augmented Generation (RAG), and now arriving at Agentic Design. In this new paradigm, success is not achieved by a single \"monolithic\" model, but by orchestrating a collaborative ecosystem of hyper-specialized agents (e.g., a coding agent, a reasoning agent, a tool-use agent). To facilitate this, NVIDIA introduces its \"operating system\" for AI: the NVIDIA NeMo (Neural Modules) framework, which includes the NeMo Agent Toolkit for building pipelines, and NIMs (NVIDIA Inference Microservices)—highly optimized, pre-packaged containers that include the model, drivers, and inference engines needed to run anywhere, specifically on Amazon EKS. The central theme of the talk is bridging the \"Production Chasm.\" The speaker warns that while POCs are easy to build, scaling agents introduces exponential complexity. Agents generate massive token volumes due to their internal reasoning loops (\"thought chains\"), leading to latency spikes and potential cost blowouts. Furthermore, security and governance become critical stumbling blocks when autonomous agents start accessing proprietary data. To counter this, NVIDIA advocates for \"Defensive Data Science\"—a rigorous approach that anticipates failure. This involves using NeMo Curator to clean and deduplicate training data on GPUs (achieving 16x speedups over CPU methods) and employing LoRA (Low-Rank Adaptation) to fine-tune generic frontier models into domain-specific experts with just a single line of code. To prove the efficacy of this \"NVIDIA on AWS\" stack, the speaker shares compelling metrics. By using NVIDIA Blueprints—canonical reference architectures deployed via Helm charts—developers can reduce their codebase by 57%, saving weeks of boilerplate engineering. In a real-world healthcare virtual assistant example, these optimizations led to a 2x improvement in response time, a critical factor for patient user experience. The speaker repeatedly emphasizes that this ecosystem is \"framework agnostic,\" meaning developers using LangChain, separate vector databases, or Semantic Kernel can still plug in NVIDIA NIMs to get the underlying acceleration without rewriting their application logic. The session concludes with a practical call to action for deployment. The speaker champions Amazon EKS (Elastic Kubernetes Service) as the ideal substrate for running these agentic workloads at scale, offering the necessary elasticity for dynamic token generation. He encourages developers to visit `build.nvidia.com`, where they can instantly test hundreds of pre-optimized NIMs (from Llama 3 to specialized vision models) using standard API keys, effectively treating supercomputer-grade infrastructure as a simple software dependency."
keywords: Agentic AI, NVIDIA NeMo, NIM (Inference Microservices), Amazon EKS, NeMo Curator, LoRA, AI Blueprints, Defensive Data Science, Generative AI
is_generated: False
is_translatable: True
---

Hello, hello. Thanks everybody for attending. I hope you can hear me just fine and you're ready to learn about some agent design for mission success with Nvidia and AWS. It's a strong partnership. Yeah, it's the correct slide deck. All right. And that we've been working on for a long time and it keeps increasing. So if you think about Nvidia and AWS we go hand in hand. Today, specifically, we're talking about generative AI agentic design. It's, it's very important and it seems like it's been a very long time, but as we see the dates are started 2023, really the, the adoption started January 2023 of gene of AI. People became comfortable with LLMs, comfortable with using human language to interact with very highly intelligent scientific models. As we go forward, then we, people started scientists and human users started. Interacting with RAG and being able to increase the effectiveness of these general purpose models, very important. You want to create something that is for your mission success. So that's where RAG came in. Parameter efficient fine tuning came in. Next came agents. It's just kind of the natural progression of what we're doing. Agents are really important because it's putting together many hyper focused intelligent designed subsets of models to be orchestrated by another model. So as you go forward it's very important to think about agents as a huge ecosystem rather than just one single monolithic being. Before I jump right into what, what that is, I wanna throw some terms out there, so you're not scratching your head as we go on. This is Nvidia's terminology for uh how we put together our generative AI processes, AIML pipelines, and ultimately, AI agentic design. NEMO stands for neural modules. Uh, over a decade ago, we identified the modular modularization of the machine learning pipeline process. It's very important because, again, not a single scientific process. Each thing along the way needs to be optimized or else you reduce in performance. And um I'll identify what those are coming up. Nemo agent toolkit, specifically using Nemo modules for agentic design. The NEMM. Nvidia inference microservices are containers. Most importantly, they're optimized containers of frontier models or hugging face models with Nvidia driver packages, with rapids for uh spark processing, large data jobs, etc. It depends on what it is. Each one is specific, but it allows you to run inference very quickly by changing just one line of code. Blueprints. Take a bunch of containers, you need Kubernetes to implement them. So blueprints are our term for helm charts and implementation of multiple containers, such as NIMs. So you have the effectiveness design of that machine learning pipeline, so you can just go ahead and lift and shift and put that into your uh. Process and the ultimate goal is to use the word scale a lot, accelerated computing a lot, and we wanna get you into production. It's very easy to start with something that's really good and ends in a fizzle because a lot of, like, as you start to scale out, a lot of things get amplified and can be run incorrectly. So what is an agent? What does it look like? How does it work? We hear a lot about it. Here we have an avatar in the middle, that's kind of our orchestrator agent. Just, just telling what to do. Multiple elements are involved in the agentic design. You have tool use, computer use. The memory is very important. When you use an agent, that memory gets maxed out rapidly. Other agents, you start to talk with them and interact with them. It's an ecosystem again. And that human is the input prompt. Then we create tokens, then we do more processing. A lot of people may not be aware, maybe you are a developer conference, but agents actually have an exponential increase of tokens. That reasoning is just creating more and more tokens. As a human, you don't see it unless you're, what you ask the model to show you its thinking process. So, that leads to the complexity of the whole situation. The, the, uh, agents, you're going into production, you have a lot of legacy code, a lot of existing pipelines, heterogeneous data coming from all types of sources, and they, to get that to perform reliably is quite difficult, and you want to think ahead. It, it's kind of like defensive programming, defensive data science. You're expecting the pitfalls before you get there because what happens, you enter a chasm. The more you scale, the more that chasm grows. It deepens and expands, and so many pitfalls happen. The worst thing is you have an amazing POC. You get great stakeholder buy-in, you get investments, you got people supporting you. And then as you go forward, you start to see performance suffer because that governance of the data gets difficult, the profiling gets difficult. The security is very, very difficult for agents because humans need to be authenticated, so do computers need to be authenticated rapidly at scale. So there's all of these need to be thought about and addressed as you're going into that uh production level. So avoid the chasm. Let's show you how. OK. This is a simplified chatbot. And it, it, it, it's a lot of arrows, takes quite a bit of learning to get to this process. But the, to understand the pipeline machine learning process is quite important and knowing where everything goes. So if you want to scale this out from your POC, every little white box is probably 10 more things added to it, and more and more and more processes. So, you start to hit a lot of issues with that. Because you're multiplying it by 10. All your problems are amplified and everybody can see them. Because the last thing you want is users to use your product and it immediately failed. We want a best developer experience. Nvidia creates things for people who code, for companies who code. We wanna get our, our services in the hands of software frameworks and SDKs into the hands of those developers so that you can implement it into your. Into your uh production pipelines. The best thing about Nemo, blueprints, things like this, is it's mostly free. You can go on GitHub and use this. You can go with a developer license, which is just a login to our NGC container registry, and you can pull any of these, and you can use them. So, in the white, we have the Nemo. This is each different process. We'll talk about that in a minute. But coming from the, the bottom up. People start with GPUs, accelerated computing. It's very important. But in order to get the best performance out of what you just purchased or what you're using on AWS, how about you use some accelerated libraries for that? We, we put these out for free so that you don't have to learn a whole new set of skills just to make sure everything's running correctly. The understanding and reasoning, AI safety, all of these issues, and we see that line going across with the Nemo agent toolkit. This is just itemized modules to accelerate the pipelines for agents. And at the top are different types of blueprints that Kubernetti's helm chart design. And you just kind of bottom up and you can put all these together. And we have all the uh ML pipelines for you figured out. It's modularized, so you can plug and play. You can use the Nvidia Nemo packages, and then with whatever other frameworks you happen to be using. It's OK. We're agnostic. I like to say science a lot when I'm talking about anything, machine learning and AI because I think um it's very important to remember that even though we're interacting with human language, we're still interacting with a computer. And that computer needs to be taught to do things. And you, it's on the human to be creative, to fix it, and to identify where to accelerate. Cause the computer doesn't know where it's messing up, it'll just tell you a random error most of the time, if you even get an error. So we start with that. The data kind of stacked right there and just start to rotate around this flywheel. Everything here is very important. A part of that scientific process for AI genic design. You wanna curate your data, you wanna customize it, customize your data on your proprietary holdings. You want to evaluate it, make sure it's good. It's probably a good idea. Guard rails, don't want it to go off the rails. You don't want it to hallucinate, you don't want it to data drift, etc. etc. These are big problems. Lastly, you figure that out. Create the NIM, create the container. Now you have a snapshot in time of reliable processing. You can use that, plug and play it. You can update it. It's very easy just to work with containers. A data flywheel is a self. Didn't know that was gonna happen. OK. It wasn't me. So, uh, next, what was included in kind of the agentic design. I just wanna call out a few things. Sorry for the eye test if it is difficult to read back there. But the framework agnostic, if you're using lane chain, that's great. Using something else. That's great too. We're here to support you, semantic kernel, that's, that's cool too. Yamels, I, they're difficult if you've never interacted with a yaml, but once you get comfortable, they're quite easy to change. Uh, you can also interact with a yamlvia Python. There's lots of ways to work with this. Uh, the safety and security is really important. The evaluation and again the agentic ecosystem connectors or like MC MCP custom plug-ins, things like that. We wanna alleviate that stress of you having to write those, so we give them to you, so you can start to connect all of these tools together. Best accelerated uh production from your pipelines. Happier success, stakeholder buy-in. It's, it's kind of a, that's the flywheel as well as a go to market strategy. So Here's some metrics for you. As a coder, I like to think about the 57% fewer lines of code. That's weeks of development time and expertise that now you've reduced that so you can actually get to implementing your services and your your production. The higher throughputs very important. That's your tokenization. So when you're accelerating the amount of tokens, that means it's faster, it's less latency, possibly can use less compute as well, depending on how you work with it. And then faster response times. This is specifically for the healthcare virtual assistant, which is a blueprint for healthcare. So if, if I'm a patient, I really enjoy uh almost 2x speed up of the response time. A couple of other metrics to, to call out. But Nemo curator. Previously, uh, A lot most data munging, data processing is CPU based, and that's great, but it can only go so fast. If you start to process that data on GPUs, on accelerated computing, you're gonna see a, a 16X speed up in this one example, 16X. That's quite fast and it allows you to get to building your models quicker. Nothing's worse than cleaning up data and sitting there and watching it processed. It's the most boring thing ever. I, I like to get to where we're implementing the data. Data pipelines, not just watching it and getting it ready, but the deduplication, um, getting a large data sets to, to be processed all at once is quite important instead of waiting and waiting and chunking. It's boring. We wanna get to the cool stuff and then develop anywhere. Python API is, it's a very good language and Python's almost everywhere. API calls are ubiquitous. Keep it as simple as possible. Nemo curator. Oh my gosh. Sorry about that. It should have all been muted. So Nemo customization is quite important because you wanna customize your models on your own data. That's, there's no more pitfall than trying to use a generic model on something hyper-specific. You're not gonna get performance on that. And, and lastly, you're not gonna see the results you want as a human being. It's gonna give you generic ideas, and agents are designed to reduce that generic and make everything hyper-specific. Let's see if it's sound, yep. Oh well. So the Nemo customization architecture is another way to look at that. I showed you some metrics, but if we look at the The customization architecture, there's parameter efficient fine tuning or low rank adapt adaptation. If you've heard of um those before, that's awesome cause that's what ties into agents. You're creating, creating subsets of a generic model that's hyper-specified on your proprietary proprietary data. It's the best way to do it. And if you see down here, the, where it says training type, if you can read that, you just type Laura, now you can fine tune. The rest is done for you. OK, no sound. Another way to look at this is it's kind of a linear, but then you get to the end, there's a bunch of flywheel again, rotating, thinking through it. This is all iterative process. There's gone are the days of something that just, you hit run or you send it off and it's good to go, and you sit back, you know, kind of like this, and watch it, watch it happen like a DBA. But, um, now even DBAs have to be interactive with everything. It's, it's keeping us on our toes, but it's really fun because the amount of really awesome software and products being produced is astounding. It's very, very useful. Just look around the expo, and you can just find a lot of this implementation. But with Nemo, it's pretty interesting. People may not know, but Nvidia service, Nvidia software such as frameworks and SDKs already exist in a lot of services you use, such as much, much of the AWS stack. If you open search, for instance, we just got implemented with that. So the list goes on of what you can use us for. And a flywheel. Just keep that in mind. So ultimately, we've worked through the Nemo processing to create optimized agents with the agent toolkit. So now everything's involved in this agent toolkit. So it's modularized, you can use it off the shelf, it's ready to go. And you won't have to worry about where do I go to fix this problem? How do I write this new process? Well, it's done for you just to implement the process. That's way more fun. Creating a good developer experience is key to creating good AI agents. So where can we find all this and how can we use it? Any framework you wanna work, work with, I just have many listed here. If you work with any of these, uh, frameworks, that's great. If you, there's some you don't, reach out to us. One thing about Nvidia, we want to find out your hardest problems. Whatever you're struggling with, if you can find an Nvidia, we always answer the phone. We'll, we'll work with you to fix that problem. Where can you deploy a lot of what we've talked about? Well, pretty much everywhere in the AWS compute stack. I recommend EKS, especially as you go to scale. EKS really does, it's, it's the easy button for deploying, such as your blueprint design, and agents really are blueprints at Kubernetes. It's a lot working behind the scenes to implement an agent. You wanna get the best performance out of that. You also wanna get telemetry. You wanna see how things are running and the profiling. Well, AWS makes that easy for you, and our partnership with AWS helps, uh, alleviate a lot of the issues. Uh, I just wanna also call out Sagemaker. It's a great place to develop and design what you're working on. You probably already know these things. And also we are on the marketplace. I, I heard earlier the, uh, people talking, we're talking about I, uh, top secret, and we actually are implemented in. ICMP we have an offering there in Marketplace, as well as commercial. So you can find us wherever you may be via a private offer or if you just wanna use the services, you can do that and use runtime. I do want to say that we have a big booth over there. If you haven't seen it, it's 10:22, and we've run through quite some, uh some of the topics I just uh went over. But really, I think the reconstruct large scale, large scale environments is something really cool. I think uh the best way to To to interact with the generative AI and other models such as that is, is to see it. And lots of times when it's text-based, it's hard to really wrap your head around it. If you can see what it's like with some type of, uh, I'll say computer vision, but now we do VLMs and things like that, uh, but it, it's a good way to kind of create the show for what you're getting at. But also we have the edge to cloud vision, AI agents is quite important. Um, anything else. And, and what's really neat too is the people staffing our booth are the actual engineers who worked on it. So feel free to, to deep dive and ask some really hard questions, or just if you're interested, we'll talk to you forever. Uh, I wanna leave this up for a minute if anyone's interested about where you can interact with our services, uh, software, SDKs, but the, there's the, on, on the far left, we have the marketplace. You can see what we have listed there. The AWS Nvidia, it's just a lot of, uh, tech blogs we've written of how to implement. Nemo fine tuning, for instance, on StageMakker. How to then take that out, make your AI agent, and run that in EKS. We have the whole walkthrough design for you, so all you need to bring is your data, or if you just wanna run through the, uh, the tutorials, that's fine too. That's always a good starting point. And then lastly, I highly recommend everyone check this out, build.video.com. You can, you can lose hours playing with this. It's an AI generative AI playground of all the NEMs and blueprints we keep creating. We have hundreds on there now, and we keep creating more and more. And what that is, it allows you to not only see what's listed, read about what's listed, but then you actually get the technical documents for how to, to work with what's there, and you get to play with it, and you can just one click, create an a. API key, copy that code out and put it into production. It's quite simple. We took all the thinking out of it, so you just work with it. And I, since I have a couple of minutes, I'll expound a bit on what API keys and when we're interacting with the NIEM, the Nvidia inference microservice containers. When you're calling a generative AI model via that way, we use the OpenAI protocol. So it's quite simple that you just change your direction, like what model you want to use. That's one line of code, you just changed. And then the second line is your API key, that's a copy and paste. After that, you're good to go. And if you wanted to do fine tuning, training type, Laura. So that's 3 lines of code. You don't have to spend weeks figuring out how to do this and doing it from scratch. Um. I think we still love to read scientific papers and then write the code ourselves, but why, why start from scratch when you can, uh, use an existing framework? And with that, I'll say thanks for having me. I appreciate your time. I hope you have a good uh expo.