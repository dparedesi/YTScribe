---
video_id: fOP_Or2gv8I
video_url: https://www.youtube.com/watch?v=fOP_Or2gv8I
is_generated: False
is_translatable: True
---

Hi, my name is Mas Tanaka. I'm the senior, uh, strategic engagement at the AWS. This project is a joint project between Bandai Namco and AWS, and we'll be talking about how we created the metaverse space throughout the session. Uh, in this session, uh, you'll talk, we'll talk about, uh, some of the, you know, the technologies we used in this metaverse creation, uh, namely three different solutions that we provided. Uh, first, professional services. Have any of you ever used professional services in your projects before? Anybody? 0. So, uh, professional services is uh offering that uh we offer to most of the customers when there's a complicated issues going on in the project, and we offer a strategic con uh consulting to tech consulting to offering a best practices from worldwide uh practices to jumpstart your project. And we also offer uh the actual integration using professional services engineers and also we have the partner network to utilize. So we can take on a large uh project to, you know, give you the best results. Uh, Amazon Game lift streams. So this is a technology we use to provide on-demand long latency, uh, streaming of the video, uh, to all the customers regardless of which devices they were using. So in this project, we were able to stream the Gundam metaverse space to any users, um, I mean, any users using a smartphone, instance, for instance. We also used Amazon API to connect Amazon to the space so that one-click shopping was possible. Let's dive into the team structure. Um, on the left side, we have the Van Dynamco Entertainment team. Uh, we're fortunate to have two of the gentlemen, uh, from the team on the stage today. On the, on the right we have the uh AWS Proserve team uh at the peak, uh, the number of engineers uh went up to 80 engineers in this project. Panda Namco uh took care of the strategy planning, creative direction, and quality control of this project, and AWS Professional Service took care of the infrastructure and game uh servers and also the 3D space development as well, and the project management. I'm not sure if you're aware of uh the IP Gundam, but we'll we'll start off the, uh, you know, uh session with uh the video so that you have the idea of what Gundam Metaverse is about. Let's welcome Mo Morissan to the stage. Yeah. Thank you for joining us today. I'm Daiske Omori from Bandai Namco Entertainment. The Ban dynamical Group is built on character brands. We cover toys, games, animation, music, and amusement. Within the group, we are a digital entertainment company focused on console and mobile games. We hand over 500 IPs each year. From long-run titles to new ones, we bring them to people worldwide. This scale shows why we must connect funds across generations and regions. In this project, we are taking on a new challenge with Mobile Suit Kingdom. Mobile Suit Gundam started in 1979. It's known for realistic world building and human drama. Since then, many TV and film titles have followed. For decades, people around the world have enjoyed this IP. How do we give fans a place to gather worldwide and help the community grow? That is why we are building the metaverse. This is the concept for the Gdan metaverse we're building. First, fan's experience can dance iconic worlds together. Second, Accessibility, so anyone can join anytime, anywhere on any device. Third, one smooth journey. Watch, join live, and shop in one place. With these 3 core ideas, we aim to create a place where fans around the world gather, have real conversations, and help the community grow. That's the background. Now, our business challenges. First, global connectivity. With different regions and networks, we required scalable ways to bring everyone into one shared space. Second, accessibility. A smooth 3D experience anytime, anywhere on any device. Third, one combined experience, watch, join life and shop in one smooth journey. Working with AWS professional services, we solved this with Cloud Tech. On the next page, Akinori from AWS will present the technical details. Hello everyone. I'm Akinori from AWS Professional Services. In this section, I will introduce the system architecture of Gundam Metaverse, which has been provided to Gundam funds in more than 20 countries worldwide. Let me first highlight professional service's role in this project. We provided comprehensive support across multiple areas beyond cloud infrastructure. From strategic planning support to the build, learn, and growth phases of the Metabus application development. Our deep understanding of both strategy and applications enabled us to build a global scalable cloud infrastructure that maximizes addresses value. Today's session will specifically focus on the cloud infrastructure area. Here is the architecture overview of the Gundam metaverse. Let me walk you through each component following the user flow. In step one, players authenticate using band dynamic ID to access the web UI. Next, Step 2 is a key point. When players access the web UI, the game application is launched on the GPU instances in Adverse cloud using cloud gaming technology. Then the game session is established. And in step 3, Players access the game session through their web browser with sending mouse and keyboard input. Moving to step 4, the game server backend provides online multiplayer functionality to clients with sharing information such as other players' positions and movements. In step 5, we have implemented integration with Amazon to provide an immersive shopping experience within the metaverse. In step 6, all player activity logs are collected in the analytics platform, providing dashboard insights in near real time. Finally, in step 7, we have also implemented a CICD pipeline optimized for cloud gaming. This allows developers to deliver updates to players smoothly. This slide introduces the key technologies used in our architecture. We utilized Amazon Game lift streams for browser-based metabus experiences and Amazon API for seamless shopping experiences. The game platform is built with Unreal Rang 5 and Nakama game server. Also, in our development process, we leveraged Team City and Powerforce to enable automated build and uh deployment system. Today we will explain in detail about Amazon Game with the streams and Amazon. Which were key components of this project. Those two services played crucial roles in addressing our business challenges. Amazon Game lift streams enabled multi-region and multi-platform delivery, achieving global rich and accessible experience. And Amazon API enables seamless purchasing of Amazon products within the metaverse. Facilitating massive shopping. In the following slides, we'll take a deeper look at Amazon Game lift streams. Firstly, let me explain what what Amazon Game Lift streams is. It's our recently released AWS service that enables browser-based game delivery. With Game lift streams, games can be run on GPU instances in AWS cloud, allowing on-demand low residency streaming to players worldwide. This streaming functionality supports up to full HD resolution at 60 FPS. Game Lift streams operates through a global network of endpoints. As shown on this map, we have access to 10 regions worldwide, including better access locations. This extensive coverage enables us to provide low latency streaming by connecting players to their nearest endpoint. For the Ganda metaverse, this global infrastructure has been essential in delivering consistent performance across different regions. Another powerful feature of Gamelift streams is its device accessibility. As shown here, streaming is possible on any device equipped with a web browser from TVs and tablets to computers and various streaming devices. This allows us to deliver gaming experiences even to devices with limited processing power. As a result, our potential player base is significantly expanded. And this slide shows actual gameplay footage from the Gundam Metabus. By utilizing Game lift streams, we have successfully provided a browser-based metabus experience. Enabling Gdan fans from over 20 countries worldwide to interact with each other. Furthermore, while we initially only supported PC, we were able to quickly expand support to mobile devices using Game lift streams. In the following sections we will explain detailed Game lift stream's architecture from two key aspects. First, multi-regional scalability. The Gundam metaverse required global game delivery. And game lift streams enabled us to scale by region based on demand. We also optimize capacity by analyzing real-time and cross-regional access patterns. Second, multi-platform accessibility. Browser-based access through Game lift streams allowed us to deliver, uh, deliver to multiple devices using a single build. We also ensured quality through automated end to end testing, integrating with standard browser-based testing tools. For more detailed insights on these aspects, I'll hand over to Nat Hiro from Van Dynamic Entertainment. All right, thank you everyone. I'm Natsuhiro from B and Amco Entertainment and um I'd like to walk you through some of the thought process that went into the technical implementation of the Gundam metaverse. So there were two key requirements for making the Gundam metaverse globally accessible, um, and we, so we needed to provide a comfortable experience from anywhere in the world and enable access from any device. So we went with a multi-region, multi-platform strategy supporting over 20 countries and 3 platforms, but this came with 4 technical challenges network latency optimization, dynamic capacity management. Platform experience equity and multi-platform testing framework, each involving some complex trade-offs. Network latency optimization came with 3 additional challenges to address. We needed to 1, optimize latency in each region, 2, secure thousands of GPU servers to match user demand, and 3, manage the operational complexity of a globally distributed infrastructure. So to solve this, we utilized Amazon Game lift streams feature called multi-location stream groups. The advantage of this feature is that once you configure a build in the primary location, Game lift streams automatically deploys the same binary to all the other locations. This centralized the infrastructure management across multiple regions, significantly reducing engineering resources and operational burden while enabling global deployment. In terms of performance, we achieved a low latency of under 20 milliseconds within the Tokyo region and under 150 milliseconds between Tokyo and Oregon regions. Operationally, we can deploy to multiple regions with a single CICD pipeline, which has shortened the release cycle and maintained development velocity. By leveraging this feature, we were able to clearly separate client side and server side responsibilities, greatly simplifying both development and operations. So let's now take a look at the specific implementation details. So here um I'll be explaining how we select regions using game lift streams. First, the client measures latency to each region. In this example, the latency to the Tokyo region was 20 milliseconds and to the Oregon region was 118 milliseconds. Based on the latency measured on the client side, we pass regions ordered by latency as the locations parameter in the start stream session API. In this example, Tokyo is 1st and Oregon is 2nd. Game lift streams evaluates regions in this order and automatically allocates a session in the optimal region. You can confirm the allocated region with the get stream session API. So as you can see, the region selection logic was quite simple to implement. Now, let's see how it behaves when a region has no available capacity. Just like before, we call the start stream session API with Tokyo first. Game lift streams checks Tokyo's capacity first, but let's say there is no availability in Tokyo. It then automatically checks the next region, which is Oregon in this case, and allocate a session if there's availability there. What is important is that there is no need to implement retrial logic on the client side. This clarified client and server responsibilities and that really simplified our development. By sequentially trying multiple regions, we improved our success rate in allocating session and made overall operations more stable. Also, it's easy to add regions with multi-location stream groups. As the number of users increases, we can flexibly add new regions. In this example, we've used the Game lift streams console to add 4 regions Ohio, Frankfurt, Ireland, and North Virginia. These additional regions work immediately by simply passing them as a list in the locations parameter. In this example, we've specified 6 regions. If there is no available session in Tokyo, the system will allocate a session in Ireland, which is the 2nd region on the list. As you can see, multi-location stream groups enabled us to optimize latency in a global environment, but we faced a new challenge, how to predict capacity in each region and adjust it according to demand. So let's take a look at this dynamic capacity management challenge. There's a trade-off we have to consider. Having more available sessions improves user experience, but it also increases infrastructure costs. Game lift streams offer two options. Always on provides instant access, but it's costly. On-demand takes over 60 seconds to start, but scales dynamically. Each had its own pros and cons, and neither alone could really resolve this trade-off. For the Gundam metaverse, we chose the always on option to prioritize player experience. To balance this with cost optimization, we implemented two approaches. First is custom auto scaling. This dynamically adjusts always on capacity according to demand. I'll come back to this later in the next slide. And the second component is adaptive region auto scaler. Under normal conditions, we consolidate users from Asia in the Tokyo region and users from other areas in the Oregon region, which offers the lowest infrastructure cost. But when available sessions run low or network latency increases, additional regions automatically activate, expanding to as many as 6 regions. This allows us to operate with the minimum necessary regions while flexibly responding to demand surges. These two approaches optimize the balance between cost and user experience. So going back to the custom auto scaling, and I'm gonna explain in a little bit more detail. So first of all, let me go over the foundational concepts in game lift streams there are 3 key metrics desired capacity, allocated capacity, and idle capacity. Desired capacity is the total capacity you will request, and it consists of two parts allocated capacity, which is actively in use, and idle capacity, which is running but not yet assigned to any session. For example, if desired capacity is 100, allocated capacity might be 70 and idle capacity would be 30. To maintain a good user experience, we need to secure a certain number of idle capacity while appropriately performing scale in and scale out operation to reduce wasted costs. Therefore, we implemented a custom logic that takes all three metrics into account. The next slide will explain this implementation in detail. The custom auto scaling logic dedicated to game lift streams is implemented in a lambda function triggered every minute by event bridge. This diagram shows the lambda function implementation. First, lambda retrieves the current desired capacity, allocated capacity, and idle capacity metrics with the GetStream Group API. Then it calculates the surplus rate, which is idle capacity divided by desired capacity. If this surplus rate is 30% or less, it determines that the idle capacity is low and cannot accept new users and calls the update stream group API to increase the desired capacity. Conversely, if surplus rate is 50% or more and capacity is excessive, it scales in. By achieving dynamic capacity management like this, we were able to balance cost and user experience. Next, I'll explain the technical challenges related to the multi-platform strategy. Here we face two technical challenges. First is platform experience equity. How do we provide a fair experience across all devices? And second is multi-platform testing framework. How do we ensure quality across diverse platforms? I'll explain how we address these two challenges. So the Gundam metaverse is designed to be playable on any device, PC, iOS or Android. We identify the device using the user agent header sent by the web browser and switch the web UI layout accordingly. To properly send player control inputs to the game on PC and mobile devices, we implemented Gamelift Stream's web SDK in the browser. The the web SDK comes with default functionality for keyboard and mouse inputs as well as automatic controller recognition. Additionally, it's possible to add virtual game pads and specify custom key mappings. For the Gundam metaverse, we implemented virtual game pads for mobile device operation. This slide shows a sample implementation of the virtual game pad for mobile users. On the left side you can see the UI component definitions. We define joysticks and buttons along with event listeners for player operations, and on the right side you can see the SDK integration code. First, we create a virtual game pad object and register it with the Game lift streams SDK. Then, when browser events occur, we receive them and convert them to virtual game pad axes and button inputs. Finally, by calling process game pads, these inputs are sent to the unreal engine client on Game lift streams. So, and to wrap up this technical portion of the presentation, let me talk about the development environment unique to cloud gaming. In traditional multi-platform development you typically need to create separate binaries for various devices and conduct testing in QA for each platform. However, with Game lift streams, a single binary supports multiple platforms because the browser handles all input and the server only streams video. This drastically simplifies the development process and enables faster iterations. Additionally, because it's browser-based, web automation tools work seamlessly, allowing us to execute large scale multiplayer tasks with ease. To test multiplayer scenarios at scale, we spin up multiple game lift streams, instances and simulate browser interactions. Using step functions in Amazon ECS, we launch puppeteer-based headless browsers that perform automated operations and record sessions to Amazon S3. This video shows how Puppeteer enables scalable E2E testing. It automatically spins up multiple clients, controls the browsers, handles logins, and performs in-game actions. By running hundreds of these sessions in parallel, we can simulate thousands of simultaneous players. All tasks are recorded, so we can verify issues anytime, whether after hours or after fixes. This helped us identify issues that occurred only when multiple users connected to the same server simultaneously. Interestingly, these problems weren't visible in server monitoring. This brought unexpected benefits to our cloud gaming project and significantly improved the overall quality of our game development. So, that concludes the technical walkthrough about cloud gaming, and I would like to bring up Mass from AWS again, and he will talk about the integration with Amazon. Thanks, Natsu. I'll now introduce the immersive shopping experience in the metaverse space realizing through Amazon. In the Gundam metaverse, we utilize two technical elements, namely logging with Amazon and Amazon API to create a seamless purchasing experience within the metaverse space. In the seamless purchasing experience integrated with Amazon, players can discover and purchase products without leaving the metaverse space. More specifically, users can obtain Amazon product details within the space along with pricing and customer shipping information, and they can choose to when they decide to uh purchase the uh whatever the, you know, uh, item, they can just press a button and that information gets sent to Amazon and it gets delivered to you. In the Gundam metaverse, we developed an Amazon integrated metaverse shop that enables such seamless shopping experience using two technical elements which we'll talk about in details. One is logging with Amazon and the other is Amazon API. This diagram shows the architecture of Amazon integrated metaverse shop. When the player visits the shop for the first time, they're prompted to log in using login with Amazon. Once they click the button, uh, they input their information and their information is connected to their account within the uh metaverse space. Then the player can browse and purchase the products. All they need to do is just click a button and all that information gets sent to Amazon by via API. After the order is placed, the order information uh gets processed within Amazon and you'll get your uh purchase through Amazon. You can also use Amazon website to access your customer support. Let's begin with uh uh dive deep into how we integrated logging with Amazon. Logging with Amazon actually provides uh ways for users to link their accounts uh to any of the uh uh spaces such as the metaverse space. In Gundam Metaverse, uh, when user uh presses the login with Amazon button, the metaverse, uh, shop pops up the web screen displayed as an overlay. We'll describe to you why we, we use the overlay to do this. The Gundam metaverse is running on a cloud gaming, as we explained, and because it's not running on cloud, we needed to link the Amazon account constantly. Which actually present a couple of uh issues. Normally, linking Amazon account requires displaying authentication screen, uh, showing terms of services, privacy policies, um, and then the users needs to input their username, password, and then you will have to display the permission, uh, confirmation screen. That's a lot, right? So, during that process, if they fall off, um, they will redo, they will need to redo the task again. So, uh, we've uh decided to use, um, Overlay to overcome this issue. While web screen can utilize cache on the user's PC, uh, if you're doing this on the cloud, that's not possible. We address these issues by implementing over a web screen external to metaverse game on the player's PC using the PC cache to implement authentication screens. This allowed us to obtain authentication token for subsequent Amazon API users. Um, also created as a way to obtain user actions during the, uh, the purchases. This page shows the implementation example. As you can see, we just implemented a small amount of code which allowed us to actually display Amazon standard authentication uh screen without having to create everything from the scratch. Next, I'll describe uh examples of Amazon API usage within the metaverse. Uh, using the API we are able to pull any information that's available at, in Amazon. For instance, users are able to see, uh, the product details, uh, they're able to see when the delivery is going to happen, uh, preview order, um, once they see all that information and decides to purchase, all they gotta do is click a button and uh all that information gets sent to Amazon via API. And through the API we were tracking where the users actually dropped off doing the purchase uh steps, enabling us to collect data uh useful for uh shop design. Finally, uh, the post-purchase process. So, it's, this, this is definitely uh easy to explain. It's, it's the same as uh when you make a purchase on the Amazon.com website. Um, after you purchase your order in the metaverse, you'll receive the goods from Amazon and you can use the website to do, uh, any type of interaction with Amazon, including customer service. So, this mechanism basically offered uh Bandai Namco to create a shopping experience without having any back end, you know, all they, they had to do was uh create a connection to Amazon using Amazon API and Just place their goods on Amazon. That's all they had to do. So that concludes our presentation on Gundam Metaverse. Uh, some of the, uh, I guess, takeaways from the sessions are listed here. Uh, first is the development of the region's scalable metaverse. Second, we optimize cost and experience by uh custom auto scaling. 3, we enhance service quality with automated end to end testing. And last but not least, we integrated Amazon into the space to provide. Um, seamless shopping experience. I hope this insight benefits you in the future, um, and, uh, I'd like to pass on the mic to Omorian for the last time, uh, for what's next. In closing, here is what comes next. Metaverse technology can be used across businesses. We are going to start with a character metaverse and with the same base, we can expand to digital trends, simulation, and training. This setup allows us to launch small 3D spaces quickly and take them worldwide with steady quality. We are making the system standard backend, front-end, and client so partners can use it as well. Several partners have already begun building new spaces with us. We are working with partners on real use cases. If you're interested, please reach out here at Reinvent. Thank you for your time and attention. It's been an honor to share our work with you.