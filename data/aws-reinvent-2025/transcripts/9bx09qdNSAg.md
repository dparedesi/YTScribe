---
video_id: 9bx09qdNSAg
video_url: https://www.youtube.com/watch?v=9bx09qdNSAg
is_generated: False
is_translatable: True
---

Today we will be talking about how you can streamline AI model development life cycle as well as monitor uh and do life cycle management for your AI workflows using Amazon Sage Maker AI. My name is Khushbu Srivastav. I'm a senior product manager at Amazon. I manage the Amazon Sage Maker Studio product. I've been with AWS for almost 6+ years. Along with me are joining Bruno, uh, a senior specialist essay, and Moni Ganan, one of our esteemed customers from Coho, uh, who will also talk briefly about themselves. Hello everyone. Thank you for joining this session today. My name is Bruno Pistone, coming from Italy, Milan, probably from the accent you can hear. I'm a senior worldwide specialist solutions architect in AWS almost from 5.5 years focusing on model training and now model customization for large language models. Hi everyone. Uh, my name is Manikanan Parramaivan. I'm a senior staff architect for Data ML and AI at Coho. Um, I manage and lead the architecture, infra and operations for our data and ML platforms. Excited to be here to share our story about how we use SageMaker at Kho. Awesome. Um, innovations in generative AI are quickly transforming the business landscape. IDC predicts global spending on generative AI is expected to reach $202 billion by 2028, representing around 32% of the overall AI spending. With a compound annual growth rate of 29%, citing significant breakthroughs in generative AI, Goldman Sachs predicts that generative AI can increase global GDP by as much as 7% or almost $7 trillion and lift productivity growth by 1.5% basis points over the next 10 years. We're examining the growth trajectory of generative AI as you can see from our slide, we are witnessing unprecedented growth, uh, and adoption rates across multiple fronts. 89% of enterprises are advancing in generative AI initiatives as of right now, with 92% planning to increase investments by 2027. 78% of organizations now use AI in at least one of their business functions. And 77% of organizations choose AI models that are of size 13 billion parameters or smaller. Which suggests a preference for customization and for cost um customizable and cost-effective models. Achieving these benefits is not without challenges. Let's talk a little bit about challenges that our enterprise customers face. Our enterprise customers are constantly sharing with us a couple of major challenges in ML development. First, disparate and disconnected ML tools are significantly increasing time to market. Teams are spending more time managing tools than developing solutions. Second, isolation between team members is a real killer in productivity and collaboration. Data scientists, AI developers, and business teams often work in silos, leading to duplicated efforts and missed opportunities. Third, governing AI and ML projects efficiently becomes exponentially more complex as you scale. Without the right framework, security and compliance can become major bottlenecks. And finally, availability and management of infrastructure is key. To training and fine tuning, machine learning and large language models. This slide talks a little bit about the similarities between a classic traditional machine learning workflow as well as a generative AI workflow. We always start from preparation of data in Gen AI project, that means format the data into the prompt template or structure of the identified LLM. Second, we select the right foundational model. That we want to fine tune for our use case and with our own data. Third, we run the workload on the compute cluster required. And finally we evaluate and deploy the model. Amazon Sage Maker Studio to address these challenges and help our customers, build end to end ML workflows. We launched Amazon Sage Maker Studio a couple of years ago. It provides a purpose-built end to end ML development platform where data scientists can. Not only build, deploy their models or fine tune their models, but also manage and monitor their AI workflows. Data scientists can select an ID of their preference. They can select from a choice of our studio, Jupiter lab, or code editor built on open source VS code. They can use Sage Maker Studio notebooks for data preparation. They can write uh data scripts for data generation and uh preparation, or they can use the built-in EMR connections to run their large scale Spark workflows for um running their data preparation jobs at scale. Finally, they can, uh, select from a hub of foundation models. They can choose one of the pre-built, uh, foundation model already available. Select a fine-tuning technique of their choice, any fine tuning techniques or reinforcement learning techniques. Or you can also bring in your own model and train it from scratch. Finally, deploy it in production. Sagemaker Studio provides you a visual interface for deploying your endpoints in production as well. And finally, you can manage and monitor all your endpoints, all your models, everything under a single pane of glass. The key thing to note is that Sage Maker Studio provides multiple Sagemakker AI service offerings within a single platform that you can use for each step of your AI development workflow. You can also run experiments using ML flow, and you can monitor your experiments, you can also build your pipelines using Sagemaker pipelines within studio. It could be as simple as drag and drop to build a pipeline, or you could write your own code in the Sagemaker Studio notebooks. The flexibility is really your choice. Today, there are tens of thousands of customers using Amazon Sagemaker AI. To name a few, we have 3M, Coinbase, Intuit. Dominoes and many others that are not listed on this slide. Before we jump into the details of each offering involved in an end to end generative AI project development for building, fine-tuning and deploying LLMs, we would like to see in practice how Sagemaker AI can help developers. With these fundamental activities, for this, I will hand over the mic to my colleague Bruno. Thank you, Khushbo. So In this demo, we would like to actually give you a tangible example of what are the main activities that are related to machine learning as well as a generative AI project, and in particular, we are going to start from the data preparation to fine tune a large language model and then deploy this model by using different types of Sage maker AI or services. In particular, I actually prepared this architecture by impersonating two main personas as a platform administrator, I actually prepared the environment for my development and training and deployment, and in particular I created a private networking environment. As well as deploying the cluster where I'm going to submit these jobs by using Hyperpod uh with AKS orchestration, as well as using the same cluster for the deployment. For the uh development part, I deployed Sage Maker Studio, where basically I can give the possibility to data scientists, engineers to connect, prepare data, and link Studio with Hyperpod by using a shared file system with Amazon FSX for Lustra. So as an engineer, data scientist, or developers, I can connect to studio. I can select the idea of choice that can be, for example, Jupiter Lab or Code Editor. I can start prototyping my code, save the data in the shared FSX in order to have this accessible from the cluster itself. And then once, once I'm ready. I can submit jobs for training and deployment by using the hyperpod CLI as well as the Kubernator CLI cube cutter. Once the job is running, I can monitor everything in terms of what is happening, as well as training metrics or system metrics directly in Sage Maker Studio by using task governance capabilities as well as to manage the flow. Now we would like to actually jump directly into the demo. This is the Sage Maker Studio user interface. I prepared the, I can select the Jupiter lab space that I already prepared. There are all the necessary components installed. I can connect to the Jupiter lab space. What is happening is that in a few minutes it is loading. So here I have my user interface. I can click on custom file system, select the FSX. I already prepared the entire content. The first step is the data preparation, but initially we want to actually install all the necessary Python models that I need for preparing the data. So I can actually install a requirements.tt that contains all my Python models, and I can do interactively. Once I'm ready, I can restart the kernel and I can start the data preparation piece. I selected a model 34 billion, and I want to improve as a task the possibility to reason and invoke tools. That is pretty important for Agentic AI. Here, I can define all the functions for preparing this data directly in the notebook, such as extracting the tool content, the thinking content, and validate the messages. And after that, I can actually prepare the data set with a proper function. What is going to happen is that this function is formatting the entire data set into the proper prompt style accepted by the model we will see here with all the specific tags and highlighting what are the main components such as, for example, system prompt, how to invoke tools, the reasoning, and how to generate the answer. Once I'm ready, I can directly upload on Amazon S3, as well as save in the shared file system. So here, for example, I can even access this data directly from Sage Maker Studio. Now it's time to start the training activity. So the first thing is to define the parameters for my training workload. So I specify where the data are saved, are stored on the FSX for luster volume, and parameters such as learning rate epochs. Since I'm operating with Hypercod with AKS, I'm actually defining a manifest file which contains all the information surrounding the Pytorch job, such as the number of instances, the GPU type, and Docker image that I want to use. Once I'm ready, directly from Jupiter lab, I can open the terminal and start interacting with uh Hyperpod. In this case, I use the Cube Catle uh SDK. I applied, so I deployed the workload, and here I can see that there are 2 new pods in my cluster. I can investigate the logs of the first one, and we will see that the first step for me is actually to align the same environment that is available on Sage Maker Studio with the pod environment by installing the same libraries to give continuity to my workload. I can clear the terminal, and now I can actually investigate what is the main node, since it is a distributed workload by analyzing what is the main address that is acting in the cluster. So it is the same pod. And now I can actually connect in order to see what is happening in terms of workloads. So here the model is actually downloading. Then the data set is prepared and after a while it is actually connecting also to ML flow because we are going to track all these metrics directly accessible from Sage Maker Studio. Once the job starts, we will see a new log that is actually showing the evolution of the epochs. So now we want to actually move directly in the Studio UX and see directly in ML Flow, what is going to happen. So I already created an MLFlow tracking server. I can open it. We will see that there is a new experiment. I can analyze directly in the graph the metrics that I want to collect for training metrics as well as system metrics. In this case, for example, for the first node, I'm also analyzing the GPU utilization for this specific node, how it is evolving during the time. But if you want to actually have more details around what is happening on the cluster directly in Sage Maker Studio, I can actually access information on the cluster on the Hyperpod cluster, and in particular under compute Hyperpod, I can select the cluster that is available. I can see the tasks that are ongoing, such as, for example, here there is a new task that is running. And under metrics I can actually see. The evolution of the utilization, for example, for the GPU CPU, the total number of GPUs available, as well as the evolution and utilization over the time. Now, I'm just waiting to mm for the for the the completion of the job. Once it's finished, I can see that the pod are completed. And the same thing is actually available also on Stage Maker Studio in the console, so the job, the task is actually succeeded. The model is actually available directly in studio, so I can even use for offline evaluation, which is actually pretty important since there is this shared FSX. Now I want to deploy the model. So the first thing is to copy this model into Amazon S3 in order to make sure that this deployment is accessible, this model is accessible for the deployment component on Hyperport. This is the manifest file which is describing the deployment, so it is saying where is the model located, as well as what is the instance type that I want to use, and the container image also in this case that I want to use a pre-built one that is the LMI container offered by Sagemaker. In a similar manner, what I can actually do is to deploy this component from Sagemaker Studio. So I clean the space and I apply this new deployment. JAL file. This deployment is actually creating different components such as for example the inference endpoint configuration which is actually describing what we saw in this manifest file, but it is really important because in case you have multiple endpoints and you want to understand what is happening, this is actually the description of what is the configuration for the specific endpoint. As we can see, there are the same kind of informations. Now, I can understand the pods that are available, so there is a new pod that is under deployment. I can describe it, and what is happening is that there are multiple images that are created in order to have, for example, the web web server up and running in order to expose this endpoint. I can go in studio under deployment endpoints. I will see that there is this new endpoint that is under creation. I can refresh the page, and once it is in service. I can start interrogating my model. So here I prepared a notebook that is directly using the Sagemaker Python SDK. I define the system prompt in order to ask the model to use the reasoning part within this tags thing and just to repeat, this is, this was not actually a reasoning model, this was an older version and once I'm in booking. And the prompt is say hello to Reinvent 2025, we will see that the model is now improved because it is actually now following what I want to do. There are the think tags, as well as we will see the generated answer that is actually reflecting what is my specific task. So As probably you understood, there are different phases that are really, really important for machine learning as well as for a general project. The first step is the data preparation piece. For the data preparation piece, there are multiple ways in order to prepare our data for machine learning. We saw in the previous demo that we used an interactive approach directly in the Jupiter Lab notebook. So as a user, I can connect to the idea of choice in say Jamaica Studio, Jupiter Lab, or Code Editor, install whatever libraries I want, and start the preparation piece with also the possibility to select multiple instance types in case, for example, I need more computational power. But in case I have a lot of data and I need to probably distribute this workload for data preparation, I can actually connect Jupiter Lab notebooks to EMR servers, EMR servers that can be self-managed. So basically you manage the EMR server as well as Amazon EMR servers. So the possibility to actually use the serverless utilization of Amazon EMR and prepare this data in a distributed manner directly in the notebook by using Spark framework. But if you want to actually operate more in a programmatic approach in a synchronous manner, so have this step into a proper pipeline for machine learning, while also in this case we can use a service of Sage Maker AI, that is Amazon Sage Maker Processing, in order to create a job that can use whatever framework you want, so custom images as well as pre-built container, for example, for Spark, and make this step as part of your pipeline. The second, after the data preparation piece, there is the training of a machine learning of a generative or a generative AI model. And in particular, when specifically for generative AI when we are talking with leaders about generative AI projects, there are new challenges and new questions that may come. For example, which model should I use? You probably asked your question, this question by yourself. Every day the open source world is releasing a new model. So which is the right one that I should use? And now I can access this type of model. The second question is, how do I customize my model? Well, there are different techniques that you may want to apply that might be related to your business goals that you want to achieve. And of course, how can I optimize my training performances in the next slides we are trying to answer all these questions. From a technique approach, there are different types of techniques that we want to apply based on your business goals. If I want to adapt a model to a specific industry, what does it mean a specific industry, for example, have the model that knows specific terminology of the industry you are operating, such as, for example, healthcare and life science, financial services. We are using continued pre-training activity. What does it mean in terms of data? We are taking a text that can be, for example, a PowerPoint or it can be a Word document. We translate into a machine readable format. So for example, TXT file, and we feed the entire document into the model itself. So the task for the model is to predict the next token, that is basically the word or the combination of the words, based on the entire context that is passed to the model itself. In case we want to teach to the model a new task or we want to improve the capability of an existing task, like, for example, as I did in the previous demo, well, in this case, we are actually using supervised fine tuning techniques. With supervised fine tuning techniques, we are actually using a data set that is named labeled data dataset. Labeled dataset means that like in this example, we have an array of messages where there is an alternation between user and assistant. So the user is basically the user prompt that you want to feed, so the user question, and the assistant is the expected answer that the model should generate. So basically we are instructing the model that given a user prompt, the expected answer should be that one. Then there are techniques that are named post-training techniques that are extending into the preference alignment part. Preference alignment part can be also related to reinforcement learning. It means that we want to align a model in order to be more humanlike in the sense that it is actually we want the model that is generating answers that are more similar to what a human can generate. So basically what we are doing, we are applying different types of reinforcement learning techniques. That is just an example of a dataset where basically in that case we are providing input to the user prompt and we are giving to the model an example of what is a good answer, but we are also giving to the model an example of what is a bad answer. So in this way the model is actually will learn how to distinguish between the two. How to do it on Sagemaker AI. In the previous demo, we actually used a self-managed cluster with Amazon Sage Maker HyperPod. With Amazon Sage Maker Hyperpod, we are talking about a purpose-built, resilient and self-orchestration infrastructure for maximized resource control. In this case we have full control of the cluster that can be orchestrated with Amazon as well as for example with Zlerm, and we have also advanced capabilities for task governance and organizations in order to organize, for example, the execution of the tasks based on the policies that you want to define. But in case you want to focus more on the machine learning part, so you don't want to actually manage a proper cluster, well, in that case, we can use sage maker training jobs, that is a fully managed, resilient infrastructure for large scale and cost effective training. In this case we training jobs, we actually prototype our code. Once we want to submit the job, we are actually invoking an API, an API by specifying what is the instance type, the number of instances, and SageMaker AI is taking care of everything. It's turning up turning up the infrastructure, executes the job. Once the job is finished, the infrastructure is turning down, so you are paying in this on-demand approach only for the amount of time that the job is under execution. But of course if you have workloads that are more predictable, so you know when to use and how many instances to use, there are options such as, for example, flexible training plans or spot instances where you can reserve some capacity upfront. The observability part is actually super important that we saw in the previous demo and in this case in Sagemaker and in particular in Sage Maker Studio, we have actually a lot of options in order to do that. For example, we saw that with Sage Maker Hyperpod we can use task governance capabilities within studio. In order to analyze what are the tasks that are under execution, we can also define customs policies in order to prioritize specific customers, for example, specific tasks, I don't know, based on the team, as well as based on the priority that we want to give to the workload. In a similar manner, we can actually do the same thing with training jobs. We can actually analyze the metrics that are related to the cluster that is used for the training job itself and orchestrate the execution of these jobs by using, for example, the connection between training jobs and AWS batch. Regarding which model should I use, well, last year during the reinvent 2024, we released this capability that is named Amazon Sage Maker Hyperpod recipes, which is a curated, ready to use set of parameters so that recipes for open source models such as for example Dipsy, MetaLama, Mistral, as well as first party model. Since starting from the New York summit of this year we have also the possibility to customize Amazon Nova models with CGMaker AI. But what is a recipe is a collection of parameters that are preconfigured and related to the model itself, so we basically have to just specify the recipe that we want to use without actually writing any kind of code, and Sagemaker AI takes care of the execution of the workload itself. The recipe is available for both SageMaker training jobs as well as for Sage Maker Hyperpod. In the training job part, we can use the SageMaker Python SDK just to specify what is the recipe that we want to use, and in a similar manner with Hyperpod, we can use the Hyperpod in order to submit those jobs. Regarding the deployment part, also in this case we have actually two options in order to deploy those models. The first one is to use sage maker managed inference. So if we want to stay in the area of fully managed approach, what is sage maker managed inference? It is offering the possibility to use fully managed real-time endpoints with automatic scaling. So what does it mean? We can actually define auto scaling policies in order to scale up and scale down based on the spikes that you can receive during the day based on the request of the model itself. But if we want to maximize the utilization of the cluster that we used also for the training workload, well also in that case, we can reuse the same hyperpod cluster as we did in the demo and deploy those models in the same hyperpod cluster. And in particular, in order to accelerate the deployment of open models, since the New York summit of this year, there is also the possibility to actually deploy directly from Sage Maker Studio in literally a few clicks open source models directly from the studio interface by selecting the iPod cluster that might be up and running within your account. Now, I'm calling back KUB uh in order to discuss about the recent launches that we did. Thank you, Bruno. All right, um, so let's talk about some of the recent launches in Amazon Sage Maker Studio. Before we start, I just wanted to reiterate that Amazon Sage Maker Studio provides you a suite of IDEs. AI developers can pick an IDE of their choice. These are all fully managed IDEs, and the three key IDs that we provide are Jupiter Lab. It's a web-based IDE for notebooks, code and data. It's flexible and extensible interface allows you to easily configure ML workflows. I think you have a lot of time. Code editor based on open source, uh, code OSS boost productivity with its familiar shortcuts, terminal, debugger, and refactoring tools and our studio. Our studio is a fully managed IDE for R with a console, a syntax highlighting editor that supports direct code execution and tools for plotting history, debugging, and workspace management. Remote access from local IDE. For AI developers who prefer to operate and develop code in their local IDE such as their local Visual Studio code IDE. But at the same time, want to benefit from the compute infrastructure defined for cloud IDE on Sage Maker Studio. We recently released the capability where you can connect your local IDE from your Sagemaker studio spaces. With remote connection to IDE we leverage Sage maker AI's powerful compute resources. And your data to analyze, process data, develop AI models while maintaining all the existing Sagemaker Studios security controls and permissions without any additional configurations required. With AWS toolkit integration, your visual studio code IDE can show all your Sagemaker studio spaces in the left nav. As you see in the GIF here, all you need to do is enable remote access, open it in Visual Studio, and there you go, you have your spaces right there. Trusted identity propagation. Trusted identity propagation is a feature of AWS Identity Center where you can send. The end user's identity, which is the human user identity across all the workflows within Amazon AWS services. So let's say a user logs in using AWS Identity Center into their Amazon Sage Maker Studio. And open Sage Maker Studio notebooks to connect to downstream AWS services such as Redshift, EMR Lake Formation, or ML services such as SageMaker training, Sage Maker processing, or SageMaker Inference. Their their human user identity or the corporate identity defined in IDC will get propagated through all these workflows. And they will get logged into cloud trail logs. So as an admin you can now go ahead and audit who accessed which resources. Not only that, if you want to apply fine grain access controls using S3 access grants, lake formation access grants, or red shift data APIs, you can do so. You can say that Kushboo as a user should have access to these S3 buckets only. And Sage Maker Studio will honor that, thanks to the TIP integration. Amazon Nova Customization on Sage Maker Studio. Early this year, we released the capability where users can custom customize Amazon Nova models on Sage Maker AI. Everything from sage makers studio. Users can explore multiple Amazon Nova models, such as Nova Micro, Light and Pro. They can select their preferred customization techniques for their use case, such as supervised fine tuning or reinforcement learning techniques. They can open a pre-built notebook to start customizing their workloads. On sage maker training jobs or sage maker hyperpod. As you see, This is an example where you can open a sample notebook. And it drops you into a notebook where you can do the customization. IDEs and notebooks on Sage Maker Hyperpod, you can now maximize your CPU and GPU investments across each stage of ML life cycle by running IDs and notebooks also on Amazon Sage Maker Hyperpod clusters. Last week we launched a new capability which lets you install a new add-on on your Hyperpod EKS clusters. The name of the add-on is Amazon Sage Maker Spaces. What is this add-on? What is a space? A space can be thought of as a self-contained entity. Where you can define all the configurations such as image, computer sources, local storage, any persistent volumes. And even more configurations that are required by AI developers to run their IDEs. For um. AI developers, this means accelerating gen AI development. Now AI developers can launch their Jupiter lab as well as code editor IDEs on web browsers or connect their local IDEs such as local VS code IDE to run notebooks on the Hyperport compute cluster. Which means they can now run their IDs on the same persistent clusters where they have their training and inference workloads using their familiar tools such as Hyperpo CLI or sharing data across interactive AI workloads and training jobs using the already mounted file systems such as FSX or EFS. It provides faster ID startup latencies with image caching and supports idle shutdown. Not only this, AI developers can now maximize cluster utilization. We support mid profiles for GPU sharing. So now AI developers can bin pack multiple spaces onto a single instance, but not only that, they can also bin pack their multiple spaces on a single GPU. Thus they can run IDs on fractional GPU as well. For the admin persona, this means unified governance and observability. Administrators can now leverage Sagemaker Hyperpods task governance to efficiently utilize GPU investments across diverse workloads, and they can view. Memory, CPU and GPU usage across diverse workloads and reprioritize using SageMaker Hyperport task awareness as well as hyperpo observability. Now we're going to see a brief demo um for uh the capability uh for Amazon Sage Makerspaces add-on, but before we get into the demo, I wanted to quickly call out a few things that we would be seeing today. First, we will see how can you go ahead and install Amazon Sage Maker Spaces add-on on an existing Hyperpod cluster. You can also install it on a new Hyperpod cluster. There are two ways to install it. Quick install comes with. Um, automatic defaults and custom install is where you can provide your own custom install also comes with automatic defaults, but you can override those defaults with your own, uh, configurations. Uh, we discussed what a space already is. It's a self-contained entity where, uh, you can define different specs and configurations that will be used, um, to run your IDE on hyperpod. I also want to briefly talk about um templates. Templates are a mechanism that you as an admin can use to create default templates for defining some default configurations for your Amazon Sage maker spaces. So let's say if you want uh teams to use a specific set of custom images or if you want your teams to bring in their own images. If you want to define already mounted file systems, local storage, you want to define the range of the local storage, minimum to maximum, you want to define compute resources default resources required to run a space. Everything could be done using templates. We provide you, uh, two default templates, one for Jupiter Lab and one for Code Editor, but you can go ahead, create your own, uh, template, and then mark it as a default. And the last thing I wanted to call out is um that your AI developers can access their spaces in a visual interface as well. How can they do it? There are 3 ways they can do it. One is using web browser. In this web browser, the admins provide a custom DNS. And All that the AI developers have to do is run a hyperpo CLI command to generate a web UI URL. If they click on it, it opens their, uh, Jupiter lab plain vanilla IDE on their, uh, web browsers on the custom DNS provided by the admins. Second way to access their spaces is through their local ID EVS code. Um, Again, as an admin you can choose to define that you want to enable remote connection. If you do, then all that your AI developers have to do is run a hyperpo CLI command to get the um URL or a string that you can then click and it directly prompts you to open your VS code uh IDE and opens your space directly in the VS code IDE. Let's say if you, if your local port forwarding feature which will open up your um plain Jupiter lab IDE on your web browser. So let's get into the um demo. So we click on the custom install. Here you see options to create remote access configuration, which is connecting to local ID as well as. Web browser access configuration, where you provide a custom DNS. And a valid certificate. You provide your EMS key. And hit install. Awesome. So your add-on is now installed on the ML training cluster. Now, you can see 2 default templates that we provide by default. We will now create a default another template for your end users. You can choose the application type. If you have task governance enabled for this cluster, you can select a pre-existing task governance priority label to assign to all these spaces created using this template. Here is where you provide your images. Please remember that you can use SageMaker distribution images, SMD images, as well as provide ECR repos. Your data scientists can bring in their own images. This is where we define define the um storage. The storage is we've built storage using local EBS. You define your compute, GPU CPU, as well as memory per space. And finally, you define a life cycle configuration script. That you want to use to run your spaces, so you can install any custom package that you need to. And you have your template ready. Now you can go ahead and make this new template as your default template. Now let's see how you can set up name spaces as well. Here what we will see is how to create a new name space or use an existing name space for existing name spaces we will also show you task governance name spaces. You can then go ahead and create EKS pod identity associations, which are the service accounts with which your space will run. These, once you create the service accounts, you can assign a runtime role to these as well. Now that you have defined some service accounts and their runtime rules, you can go ahead and create users. You can create users and groups. When you do so, you can assign them one or more service accounts, which means when a user is running. A space in a given name space, they will be able to use from a set of service accounts to run their pods or spaces in this scenario. Awesome. You can also see a list of spaces that the users are running on the hyperpod cluster as an admin, you have the controls to manage these spaces. Let's say you want to stop a space, restart a space, or if the user leaves the company and you want to delete a space from consuming any further resources, you can take those actions directly from the console. Now we will go ahead and see the how the data scientist persona will create spaces. What we see here is the data scientist persona, um. Running the help command. And finally, creating a space with the name Data Science Space. And using default configurations, if you list, you see the space is running and the status available is true. If you describe the space, you can see all the configurations or specs of the space. Key thing to note is that um data scientists can use the default configurations, but they can also override or customize the spaces spec. If you stop the pace. Endless the space, you will finally see that the status has changed from available to false. All right, let's start this space again. And we see these status changing to true. As we talked about, data scientists can update the space too. In this scenario, we're updating the memory computer sources as well as the display name. So once the update is done and you run a described command, you'll see that these specs of the space have been updated. You can get logs for your space as well, if you want to deep dive into a specific scenario or troubleshoot. Finally, you can access this space, as we talked about, using a web UI URL if you click on this URL. It directly opens up your space as an open, um, as a plain vanilla IDE on your local web browser. You can run delete command as well. And then if you describe, sorry, if you list it, you will see that the space doesn't exist. All right, with this we conclude the demo. Um, one thing that we haven't covered here in the demo is, um, how you can generate the web UI URL or the VS code, um, connection URL that can directly open, uh, so if you click on those URLs it will prompt you to open your VS code IDE or open up your, um. IDE space in the local web browser. Awesome. With that, we will talk about how Sage Maker AI has helped Kho accelerate their ML journey, and I'll give the mic to Manikanan. Thank you. Hi everyone, can you hear me OK? Alright cool. Thank you, Chris, and thank you, Bruno. Uh, quick intro, uh, about me, Manikan Parma. I'm a senior staff architect at Kho. I lead the architecture, infra engineering, and operations for our data and ML platform. All that, plus I'm also a busy dad to a six year old daughter. I'm excited to share how we use Sage Maker Studio in Khor. First, a quick context about Koh. Coho is a Canadian fintech founded in 2014, with a mission to provide better financial solutions for all Canadians. Today, we have over 2 million customers using our products across banking, credit, and lifestyle. We have products ranging from checking and savings account, line of credit, buy now, pay later, insurance. We even have eSIM for international travelers. We are about 230 employees in total. That's a startup sized team, but we are facing and and and growing at enterprise scale and facing and um building enterprise scale pro uh solutions. So that's exactly where Sagemaker Studio became critical for us, that friction of startup resources with enterprise demands. Let me start with the challenges we faced. First vendor cast. Like many startups, we started with different vendors for different machine learning use cases. We used to spend about $1.5 million just for our real-time fraud reduction use case alone. Because typically the vendors-based solutions charge based on the API based. As as we grow, as our customer base increase. As our transactions volume increased, our costs exploded. Next performance requirements. In real time use cases like fraud detection, we cannot compromise on speed. We need to make the fraud decision and return the response within 50 milliseconds as our customers are waiting at the checkout using their card. Next, security and data access. We are a regulated financial institution. We wanted to make sure our customers' data is safe. We wanted to make sure the data stays within our VPC. At the same time, we wanted to give secured access to all this data in our warehouse and lake, to all of our data scientists for experimenting and building models. And finally, we needed an end to end ML platform. We are a lean data scientist who cannot afford our time stitching together different tools to do our job, so we needed a single ID to access data, to engineer features, to train models, and deploy and operate in production. So those are the problems and those are the situations we were in. I'll be showing you how we use SMaker Studio in our development environment. So with that vendor cost as a primary problem, we started our in-house model development journey back in 2023. With Sagemaker Studio as a core foundation. Today we have over 20 data scientists and ML engineers using our domains across 4 different teams. Fraud risk, credit risk, marketing, and our own platform team. Jupiter Lab is the primary ID most of our engineers use, but some prefer code editor as well. Studio gives both So when our data scientists log in to our internal SSO portal, They get redirected to the theme-specific studio domain. In which their own individual Jupiter lab environment is provisioned and everything is ready for them. Everything is already connected. They get to access the data in the lake and warehouse securely. And they get to access Glue and EMR and Sagemaker processing jobs for data processing and model development. And every team has a dedicated S3 buckets for storing intermediate data, and access for all this is managed through IAM. From a platform perspective, we manage provision and manage all this through infrastructure as code, terraform. So when we want to onboard a new data scientist, all we have to do is just a single PR to create a user profile, and they're ready to go. So that's how we set up our development environment. Now let me show you how we deploy models in production. This is a complete end to end ML platform architecture. I know there is a lot here, but I just wanted to show how this all connects end to end. So it all starts with the SageMaker Studio as a model development environment. This is what our data scientist. Explore the data engineer features. Experiment with different models, and finally, when they're ready to move this to production. We use AW's managed airflow as our MLOs pipeline's orchestration engine. So these pipelines trigger the SageMaker processing job for feature engineering at scale. And they trigger training jobs for model training. And everything gets worsened and registered in the model registry. It's the same pipelines that we built for model evaluation, model monitoring. Our scoring pipelines or rollout pipelines for different models. When we want to deploy these models for real-time serving, we use Sagemaker. End points That's a key component in meeting our latency requirement of 50 milliseconds. And the entire ML platform is built on top of our core data platform, which is S3 for raw data storage, Redshift for warehousing, and we use HMaker Feature Store for storing and serving ML features both in online and offline mode. So SageMaker Studio is not only giving us the IDE for model development, but it's also giving us a visibility of everything you see here in this diagram. So from the same IDE our engineers can explore all the existing features in the Feature store. They can look at all the training jobs and the history, their experiments, results. All the models that are registered and all the endpoints that are deployed, all that from one single IDE. So they don't have to jump different consoles to gain visibility into their ML life cycle. And everything is deployed within our VPC. And the data never leaves our boundary. So that's our complete architecture from how we build model, how we deploy in prod, how we monitor and we operate. Now let's talks results Cost. That's our primary problem. We went from spending $1.5 million per year with our vendor to just $26,000 per year with Sagemaker. That's a 98% cost reduction. That's right, 98%. With over $1.47 million in annual savings. That's huge for a startup, and any company, for that matter. Um And we are processing over 1 million transactions per day. With just 15 milliseconds average latency, way below our budget of 50 milliseconds. Because we build the models on top of our own data from our data platform, from our own custom ML platform. We get to Improve the model's accuracy, reduce false positives in our fraud detection use case. That means less dispute claims, more happy customers. Would you stop in the real time fraud detection use case? We scale the same solution across many different use cases. We're building models for underwriting our loan products. For predicting the user churn. And marketing use cases like LTV and more. And now we are moving to Gen AI and LLM based applications and solutions as well. The way Sage Maker Studio helps us is. The same studio environment our data scientists and ML engineers get to access to the foundation models through Jumpstart. So that's where we access these foundation models for prototyping, test our prompts, compare our prompt results, compare different models, model distillation, fine tune as needed for that particular prototype. Once those prototyping is complete, And we prove the value, then we take those solutions and deploy them in purpose-built tools and infra like EKS for example, or Bedrock directly APA calls to Bedrock in some cases. So with that, Sagemaker Studio first approach. We have many use cases in production today. Our compliance team is generating anti-money laundering reports faster. We have an enhanced merchant categorization. That drives our accuracy of our customers' cashback percentage. We are analyzing our customers' feedback faster. We are prioritizing our product request and road map based on those insights. So the key takeaway is Sagemaker Studio. It's a single ID that can help with both building, deploying and operating traditional machine learning models, and also experimenting, prototyping and building Gen AI and LLM applications. To conclude, Sage Maker Studio gave Koho the power to build enterprise scale and cost-effective solutions with our startup agility. Thank you, everyone. I hope you find this useful in your Mal journey. Thank you, Mani. Before we conclude the session, we would like to share some material related to the official Amazon Sagemaker AI website, as well as customer quotes and references on how Sage Maker AI helped across multiple types of use cases, as well as some links related to workshops that you can start with for understanding how Sage Maer AI works and test all the different capabilities. Before we complete the session, I kindly ask you to rate the session on the app. This is really, really important for us to improve the content of the future session, and I would like to thank you again for attending this one and hope you can enjoy the rest of the day and the rest of Rainvent. Thank you.