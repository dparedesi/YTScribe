---
video_id: u2O3vNuSCG0
video_url: https://www.youtube.com/watch?v=u2O3vNuSCG0
is_generated: False
is_translatable: True
---

Hello everyone. Welcome to our session, AI Pioneers Shipping Transformative GEI Architectures in Production. I'm Pranav Sharma. I'm Emia lead essay for AWS SMB Business, and I'm joined by Jay, who is principal Solution architect again supporting AWS SMB Business, and we have our customer Safwan, CEO of Misra JI. Let's get started. So the agenda for our session is we'll begin with providing a quick overview about AI pioneers, who they are, how do we identify them, and the kinds of architectures that they build. We will then dive deep into 3 AI pioneer architectures and use cases that we have seen our customers build this year. The first one we will be talking about building LLM serving platforms. Now, over the last 2 or 3 years, LLM inference has become one of the most widely adopted workload across the industry. So here we will present to you a few architecture patterns that you can use to scale LLM inference within your own organization. Next, Jay will come up on stage and talk about creative content generation. Now AI's ability to generate images, videos, short form videos is at the heart of transformation that we are seeing in industries like marketing. Media and entertainment and others. Here we will present to you an architecture using which you can fine tune an image generation model using images from your own organization to tailor it to your specific use case. And then we'll have Safan come up on stage and present the story of Misraj AI who have built a pioneer Arabic visual language model for document processing use case. And we'll wrap up the session by talking about an emerging architecture pattern. That we see our customers gradually build as we move on to 2026. So let me begin by talking about AI pioneers, who they are. So AI pioneers are the organizations that are using AI to build transformative, customer facing architectures and use cases. These are often small teams, very agile, but with very big ambitions, and they are often trying to disrupt the industry where they're at. Now the common architectures that we have seen them build, right, they are not your usual consumers of AI over APIs, right? These are the organizations which sit and develop much closer to the AI infrastructure itself. So typical profile would be they would be either model builders, customizers of the models, or they might be running LLM inference or any other inference of foundation models at scale. Now let's talk about our first AI pioneer use case, that is building LLM serving platforms. So what are LLM serving platforms? As I mentioned, over the last 2 or 3 years, LLM Inference has got widely adopted across the industry. Now what has emerged is that we have seen LLM influences now become one of the foundational building blocks of the modern architecture. Much like compute, storage, networking, we now have LLM serving as one of the other foundation blocks of the modern application. But scaling LLM inference is actually quite complex due to a number of reasons. The first one is the sheer model size. Now some of these models are actually, you can't even fit them not just in one GPU, but you actually need multiple GPUs, multiple nodes to serve them, right? So that makes it complex. The other reason why these are complex is also they have these thinking budgets, right, so you may have seen that reasoning models you assign these thinking budgets to reasoning model and then they consume that during the runtime and then produce a response for that, which is unlike any other workload we have seen. And there are other reasons as well. Like, for example, the request response to an LLM is quite varied. It could just be like one word, one line, 10 sentences, maybe tens of pages, and the outcome is also quite varied, right? It could be just again same one word or tens of pages. So again, very different from the classic workloads that we are used to scaling like like microservices and others. Now this makes LLM inference famously called as an extreme engineering problem. Now organizations rather than trying to every at application level solve LLM inference, what they're now gradually doing is they're building these centralized LLM serving platforms through which what they're doing is they are bringing in all the engineering capability within this centralized team and making it easy for the application teams. Right, so that they don't have this engineering overload, and by that they are able to improve governance and also increase the rate of delivery very, very fast. Now let's talk about what are the common requirements of an LLM serving platform. Now let's begin with thinking about a mid-sized organization. What they want to do is they want to serve their customers very quickly with a number of generative AI features. The first thing they need is good model choice. They want fast access to foundation model very quickly without worrying about, you know, how do I scale this, etc. So they want to do it fast, quick, but then they want access to foundation models and also innovation in the open source community. So the first requirement is model choice. As they start building generative AI applications, the second requirement of an LLM serving platform is supporting services, right? You would need vector databases, different guardrails, security, observability, and other requirements as well to build LLM serving LLM generative AI applications. So the second requirement for an LLM serving platform is supporting services. Now let's think about a startup, right? They want to offer LLM capabilities as a service to their end customers, right? So for them, SAS-like capabilities are very important. So they care about rate limiting, cost attribution, usage reporting, and other features. Now let's think about an AI pioneer organization, right? They want to build their models, deploy their models, and run them at scale on an infrastructure that they manage, right, so that they can fine tune it, optimize it to serve the needs of their user. So they care about access to accelerated computing instances, fast networking, fast storage, very different requirements. So the fourth requirement of an LLM serving platform is. Ability to host self-managed models. And finally, the fifth requirement, let's think about these large enterprises or banks. So while they use and deploy LLMs on cloud, but they also have requirements to deploy them on-prem for a variety of reasons. It could be regulation or if they want to have their applications and LLM inference sit side by side due to latency constructs as well. So the 5th and the final requirement of an LLM serving platform is ability to host models anywhere. Now let's see how we are going to build that gradually what we are going to do in the next few slides is build a singular architecture which will solve all these 5 requirements that you have on screen. We'll begin with a very simple architecture that is a managed architecture, which will meet the first two requirements, that is model choice and supporting services. Then we will gradually add the SAS capabilities on top of it and come to the SAS architecture pattern. And then we will further elaborate it to include self-managed models capability of your own infrastructure through the hybrid architecture pattern. And finally, we will add the capability to bring your own infrastructure that is deployment on on-prem and other places as well. So we will iteratively build them over the next few slides. Let's get started. So the first is the manage architecture, and we will be talking about the top two requirements that we discussed, that is model choice and supporting services. Now this is the simplest one, right? You have generative AI application, you can talk straight to Amazon Bedrock, which provides model choice and also supporting services as well. Now it's super simple to get started, but it can scale also to the extent that you need to serve your customer, right? The simplicity of this architecture is critical. Now let's take a quick example, right? Imagine you're building a retail shopping assistant, right? So the first thing you need is access to a foundation model because you are building something customer facing. So using Amazon Bedrock, you can very quickly get access to frontier AI models like Anthropic clotson at 4.5, etc. Now, you may have a number of product specification, documents, etc. that as your customers talk to your retail shopping assistant, you would want to use information from those documents to provide response to your customers. Now you can do this using Bedrock knowledge bases, right? You can ingest all these documents. Knowledge bases will vectorize it, and your generative AI applications can start querying over these knowledge bases. The next capability often you would need as your users start talking to this retail assistant is some kind of safeguarding capability, right, because you would want to identify and redact PIIs. You want to ensure that there is no toxic content, so you want to quickly identify that. And you also want to ensure that the model is not answering through its own knowledge but using the documents that you have uploaded in the knowledge base, right? That's called contextual grounding. All of these features are available through Bedrock guardrails with very simple configurations or APIs. You can very easily build this capability into your generic applications. Now as your assistant gets popular, you will want to analyze over a period of time what are the types of queries my customers are asking this assistant to further improve it, right? So you want access to chat logs, the number of sessions that might be had, because your business might want to get some reporting on top of that, and all of that. You can do through the session management capability of Bedrocknowledge base. And then finally you have awesome prompt management capabilities as well baked into Bedrock wherein you can version control your prompts, reuse your prompts so that you don't start from scratch when you're building the next set of applications. With this, I hope you get a sense that this might be one of the simplest architectures somebody can show you, but you can see the power of it, that how quickly you can start and scale to support your customers. Now let's move on to the next one right here. Now let's add SAS capabilities to the architecture that we have already built. Now if you pay attention here, we've introduced one block that is an LLM gateway block between your generative AI applications and Amazon Bedrock from the previous architecture that I presented. Now this particular LLM gateway block will introduce the capabilities, SAS-like capabilities, right? Again, let's take a look at this with an example. I'm sure all of you and in your organization, you are using agentic coding Assistant, right? They are the story of 2026. Now imagine you are a team of say 30, 40, 50 developers and you want to use Agentic coding Assistant. But you can't agree on one, right? Many of them have their own preferences and want to use their own tool of choice. Now how do you support that in a in a right way within your organization? So you can use this architecture that I'm showing you here. So imagine the generative AI applications shown there are different agentic coding assistants. Say for example, Cloud. Client root code, or for example, even Codex from OpenAI, right? You have diverse of these applications. They all connect to LLM Gateway, and LLM Gateway then forwards the request based on the need of the application either to Bedrock or to any other provider like OpenAI in this case. Now this LLM gateway, one of the first requirements that you will have is I don't want one or a few developers to consume all the tokens that I might have provisioned for my entire organization, right? So you want some kind of rate limiting capability that you can define using this LLM gateway so that at per developer level or at team level you can create boundaries of rates. You can add further governance and control capabilities like cost attribution, usage reporting, chat logs, etc. using LLM gateway. Very, very easily and LLM Gateway also brings in resilience capability. How that happens. So for example, if you have been using Bedrock, you know that you provision quotas for these large foundation models. Now using LLM Gateway, you can also load balance it across multiple AWS accounts. So Bedrock sitting in multiple AWS accounts, you can load balance across that. So if you get token consumed from one account, it can start pointing out to other accounts. So this way, even the resilience capability gets added. Now let's dive deep into that particular LLM gateway block to quickly look at what is under the hood in that block. As you can see, this is a containerized application hosted on Amazon EKS, but you can also host it on ECS if that's what you like. Now this application is packaged up around the Light LLM open source solution, which brings it all these number of the SAS-like capabilities like rate limiting, usage reporting, etc. that we discussed. Your generative AI applications will be given a simple cloud front URL to which it will connect, and then Cloudfront, as you start connecting with that, it will route the request to application load balancer, which in turn will forward the request to the container or the pod running on EKS which then as as required by the application, will route it to underlying provider either you know Amazon Bedrock or if it is OpenAI to OpenAI or any other. Provider that you might wish. So as you can see, this is doing all the transformation under the hood, and this particular architecture actually is also very helpful if you are doing migrations. So imagine if you build some application on top of OpenAI or any other provider and you want to bring it to Bedrock without making any change. You can just put LLM gateway in front and without much changes into your application, you can start using Amazon Bedrock. With this, I hope you understand we have now built satellite capability on top of our architecture. Now let's move on here, sorry, here we provide a ready to use solution. You can just download this LLM gateway and set it up on your AWS account and get started today. Now let's talk about the self-managed infrastructure, right? That is the 4th capability that we want to add to support the AI pioneer organization. Now here this particular capability will be supported by Amazon Sagemaker Hyperpod. This is a service that we will be using to support AI pioneer organizations and their requirements. Now this AI pioneer organization has imagined built a new foundation model, right? It is an awesome foundation model, quite big in size. So one of the first requirements this organization will have is access to accelerated computing instances, some of the latest, so they might want access to the latest Nvidia. Chips or custom silicon innovation from AWS or they might want to use AMD as well, right? All these different accelerated computing instances are available through Sagemaker Hyperpod. Using flexible training plans, you can get very quick access to these accelerated computing instances. Now since this model size is huge, right, you can imagine that maybe it doesn't fit into one particular instance or a node. So you need multiple nodes to deploy like one instance of this particular model, right? So one of the requirements the organization will have is how can I have these nodes in close proximity to each other, right? So that the time is not wasted like when the nodes communicate with each other, and that is a requirement that Sagemaker Hyperpod fills out of the box. So when Sagemaker Hyperpod provisions these accelerated computing nodes, it provisions them over a single spine, so that there is very low latency between them as they communicate. Now once the nodes are over a single spine, the next requirement they will have is how can I have superfast networking between the two, and that comes through preconfigured elastic fabric adapter. Elastic fabric adapter provides superfast network connectivity between nodes, even including OS bypass itself, which is required for LLM inference and training workloads. Now, as they scale this particular LLM model in production, one of the requirements they will have is how do I measure metrics like time to first token, time between token, and other LLM inference specific metrics. Now they don't have to build anything, right? It is available through one click observability solution which comes with Sagemaker hyperpod, right? You just come, click a button, and through that you start getting inference specific metrics in there, and I'll show you a bit more detail in the next slide. And then as you scale in production, things will go wrong. One of the nodes will fail or some other issues will come. Sagemaker Hyperport comes with a number of resilience features, and one of the most important ones is this whole automatic repair capability. What happens with this is basically through preconfigured health checks, the nodes are always analyzed, and as soon as a node faces some challenges, it is replaced automatically by the service, thereby reducing the impact on the LLM model that you might be serving. Now, let's zoom in into this particular block and take a look what is under the hood. Now as you can see, Sagemaker Hyperpod is being orchestrated by Amazon EKS here, right? So all the nodes that I described that will be provisioned by Sagemker Hyperpod, those will appear to you like any other node in your EKS cluster, right? The deployment experience, working with it experience is going to be the same. So how do you deploy models now on Sage Maker hyperpower provision nodes, right? You can use models from S3, you can deploy models from Amazon FSX for Luster, or you can also deploy from Sagemaker Jumpstart, which is our one-click deployment solution or Model hub available. Now within your organization we also appreciate you may have a number of different personas, right? So there'll be developers, there will be MLOs people, there'll be data scientists, and all of them will have very different tooling expectations, right? So for administrators who are comfortable by using Kubernetis, they can use CubeCTL to do the model deployment and governance. The data scientists which are comfortable using the notebooks, they can use Python SDK and then interact with the Sagemaker hyperpod cluster without knowing anything about EKS, etc. And then we also provide Hyperpod CLI, which makes it super easy to manage and deploy models on this particular cluster. As you can see, it very easily has integrated into the rest of the stack that we presented, so iteratively we are able to include the managed model capability here as well. Below you can see the one-click observability solution that I talked about, right, that is provided by Amazon Cloudwatch, Prometheus, and Grafana, right? Together they bake in this capability of one-click observability solution. Now, let's move on. Here again, we have this QR code which you can scan. It provides a lot of good guidance on AI inference running on Amazon EKS, and it will help you get started very quickly. Now let's move on to the final architecture that we want to add to our overall architecture, that is capability to host models anywhere, that is the ability to host models on-prem, yet it seamlessly integrates into this architecture that we are building, meeting the requirements for large enterprises and banks, etc. Now this particular capability is provided through EKS hybrid nodes, right? EKS hybrid nodes is a capability where we give you a deployable package which you can set up on any commodity hardware or any virtual machine on your on-prem estate, right? And we do provide a CLI that is node ADM CLI through which you can achieve the configuration of these nodes. And once you set it up properly and then there is a connectivity between your on-prem and cloud, these nodes that are provisioned on your on-prem estate appear in your EKS cluster like any other node that you might have on cloud, right? And your deployment, your deployment experience to these on-prem nodes actually are again. Very similar as if you're deploying on cloud. Now let's dive deep into it in the next slide here towards your right, you can see the on-prem environment and on the left we have the cloud environment on AWS, right? As you can see, there is this underlying underlying connectivity between the two over site to site VPN, but you can also use direct connect if that's what you would like to do. The EKS control plane shown here sits in the EKS service team account, and then the customer VPC obviously will be sitting in your own account, and the connectivity will be established over side to side VPN as shown here. Here you can also see we are showing two EKS hybrid nodes, right? These EKS hybrid nodes have been set up using the deployable package that we share with you and the node ADMCLI that we presented to you. Again, through load balancer, as you can see, your generative AI applications talked to LLM gateway, routed through load balancer all the way up to your on-prem estate. So as you can see, this now comes together as an entire architecture supporting many different requirements that you might have of an LLM serving platform. To summarize, we discussed different requirements of an LLM serving platforms and talked about 4 architecture patterns that we gradually iteratively built to support all these 5 requirements. With that, I would hand it over to Jay to talk about the next AI pioneer use case, that is creative content generation. Thank you, Prana. Um, all right, so we've heard about, uh, the LLM serving platform. Uh, let's talk about the next use case around, uh, training and, uh, fine tuning. And this one is a very common challenge that everyone working in the creative industry or role might immediately recognize. How do you generate consistent visual at scale? How do you incorporate your content or characters, proprietary characters and objects into the content that you're generating? One of the major requirements for this use case is how do you maintain that consistency, the essence of the character, the details, the traits, and also scaling to the production level. So let's let's dive deeper and You can use generic AI model, the image generation model, but they would not capture your proprietary content and also they would not produce the consistency. So look at the example here. We have this same prompt that I'm using. It's generating an amazing image. It's also following the prompt, but the character here is differing between the images. And this might be fine for one-off projects, but imagine if you're creating a children's book or educational content or if you're generating a marketing campaign where you want to generate hundreds of images and you want characters or objects to maintain that consistency across multiple scenes. Generic models simply cannot get you. So imagine and and uh and what you want in that case is having this consistent experience across a number of images similar to the images shown here. So let's talk about a real scenario. Uh, a few years ago, one of the studios within AWS created a short film called Pichu. It follows a real life story of a young Peruvian girl, um, uh, and I think, uh, and they use the cloud-based infrastructure with hard and and creators. Let's use that scenario and let's see if we can extend the same content that they generated using this technique of customization. So we know that the generic models cannot get you the consistency. Before we dive deeper into the the solutions, let's just take a second to understand the customization, uh, because that's the key here. You're all familiar with prompt engineering. Um, it's a low effort, simplest thing that you can do to address adjust models output. But then obviously you're all familiar with it for the last few years. Next is the context engineering. This is where you are providing the additional context to the model by fetching the right context from the data source that you might have indexed into, and these are really powerful techniques, and you should absolutely use them when appropriate. But for our challenge or for our use case of maintaining that consistency across images, prompting and context alone would not get you there. You cannot simply explain the characters in prompts and expect a picture perfect consistency. So this brings us to the middle area where you are actually customizing or fine tuning the model, and Bedrock provides a number of those techniques, starting with fine tuning with parameter efficient fine tuning or theft, where you are adjusting a subset of parameters of the model, so providing an efficient way to fine tune. It also provides distillation where you are training the smaller model using a larger smart model while maintaining accuracy, and then you also have this continued pre-training or CPT where you are exposing the model to additional domain knowledge to expand its knowledge base, and these techniques require more effort than prompting or context engineering, but they provide they generate tailored output. And then lastly you have Amazon Sagemaker AI-based customization options. Here you can get the maximum flexibility and control, starting with the full rank fine tuning where you can adjust all of the parameters of the model, and these are really powerful techniques, but these do require you to have ML expertise. For our use case, we wanted to we wanted to generate consistent images, so we want to have enough customization power that allows us to have this visual consistent across multiple images, and that's what Bedrock fine tuning provides that's what we'll select for our use case. So let's see how Bedrock fine tuning works. So in Bedrock fine tuning, essentially you're providing three inputs. You'll have a base model. In our case, we'll be using Nova Canvas model, which is the image generation model available in Bedrock. Then you provide the input data, right? That's a key thing here, and we'll we'll dive into it. And then the third thing that you provide as an input is hyperparameter that basically controls how the model training happens. So for data, as I talked about, we are starting with videos because that's the videos that are generated by the studio that we're going to be using. For your use case, if you have the data available in image format, obviously you can ignore that optional block you see here, but in our pipeline, since we are starting with the video, we can sample the video, extract the frames from the video so that we have the images. And from that point on, you can use services such as Amazon recognition, which is a computer vision AI service, to identify whether first of all, if there is any human or a face in that image so that you can use that image, otherwise you can discard the image because that's of no use to you in the fine tuning. Another thing that Amazon recognition allows you is the face collection features, where you can identify which character is in that image, because that's also key, especially if you are training the model to generate multiple characters or have multiple characters, you know, consistency. This also is a key step here. Outcomes of this stage is the raw images right where where you have this organized images extracted from this video. The next stage is essentially the preprocessing. This is where you can do a lot of things like if your images are in the right format that your model supports, right? If they are not in the right image format, you want to convert them at this stage. Also, another thing is this is a stage where you can remove the low quality image or you know noncompliant images. One of the key things that you can do at this stage also is identify duplicated images or the images which are similar to each other because you want to provide the diverse data set to the model while you're fine tuning, because diversity in the data will matter for the model quality. So how you can do that is you can use the embedding models similar to how you might have been using embedding models for the rag use cases. You can use the same capability of the embedding models such as NA multimodal embedding. To identify semantically similar images and above a certain threshold, and you can discard and remove the duplicated images in your dataset. Again, like outcomes that you're organizing those data in the curated directory, another S3 bucket or maybe different location in the same bucket. The next stage is where you are generating the image caption, the description, right? So you now have images, you need the description, and you're providing those data to the training model training for the generating the description of the caption. You can also use large language model. Obviously you can use it with humans, but if you're doing it at scale, it will not scale well, so. You can use a large language model here like some of the vision capable model like Cloud Sonet or Nova Pro. You provide the images along with the the prompt to generate a descriptive caption, uh, and this is where your prompt engineering skills will matter. And if depending on the number of images that you are generating the captions for, you can also consider using batch inference for cost optimization, right? You don't necessarily need the real-time inference in this case for for the data pipeline. But you also just don't want to trust the AI generated captions, and this is also where you can introduce a human review of those captions. You can essentially use services such as Amazon Augmented AI or A2I to introduce or build this human review pipeline where your human reviewers will validate the captions which are generated by the models are accurate. One of the requirements of A2I is you have to provide or create a template. The template will dictate how the interface, the user interface for that human review would look like, and A2Y requires you to provide that template in a liquid HTML format. I personally am not familiar with that format as much, so I fired up my favorite AI assistant, Quiro. It allowed me to generate a custom template for my use case, so I described the Quiro, how I want that human or reviewer interface to look like. I provided some of the sample templates that are available in GitHub repo for A to I. And then Quiro was able to generate this custom template for my needs to create this side by side visuals so that I can have an image on one side and the description that are generated by AI on the other side, and this is actually the human interface review interface would look like, as you can see, there are images on one side, captions are on the other side. The human reviewer can mark whether this caption is accurate or not accurate. If it is not accurate, they can go ahead and edit the caption. And this is key for your training data because you are providing and creating this quality data that will go to your model training process. And out comes this is actually the image, this is actually the data format that the Bedrock fine tuning requires for your input data to be in. So this is what your output, final output should look like that will go in the training process. So we talked about the input data. We talked about the model. Let's talk about another key parameter of training is hyperparametters. So there are 3 hyperparametters that you have to use with bedrock fine tuning. The first one is batch size, which essentially means that how many samples that model will seize before it adjusts its parameters, right, adjust its understanding. The second is steps, also known as EPOC in some of the other model training processes. This basically, you know, the fewer steps means faster training, but potentially you have less learning. More steps means deeper learning, but you're also risking overfitting, and overfitting in ML is essentially the model is really good with the training data that you provide, but it would not perform well with the new or unseen data, and you don't want that. And the last key hyperparameter here is the learning rate, and perhaps it's the most critical parameter. So think of it as like driving. You're driving too slow, you'll not reach your destination, also known as convergence in the ML world. Or if you're driving too fast or your learning rate is too high, you might crash, end up crashing, and that might cause the model instability, right? So you want to maintain the balance in the learning rate. The table here is showing the default value as well as the range that is supported in bedrock. Also, along with it is some rule of thumb guidance on where you should start in terms of the value for each of those hyperparametters. And the key thing here is you don't necessarily need to remember this. The the information here is like Bedrock makes this fine tuning accessible to you. You're not managing large clusters. You're not writing distributed training code. What you're doing is you're adjusting the parameters based on the amount of training you need for your use case. So let's take a peek into what's happening in the background when you start the fine tuning job. Your customer training data sits in your account in S3 bucket, and then there's Bedrock owned and operated account essentially temporarily uses that data, accesses through the VPC endpoint, copies it over, and then starts the fine tuning job along with the hyperparameter that you chose in the earlier step. And then out comes the customized model, and then at that stage you you will either deploy the model or you can generate another iteration of it based on the output value. And then you can access this model using the same way that you are accessing any other bedrock model, same API, just a different endpoint. So let's say you fine tune the model. What's next? How do you know that the model is actually better compared to let's say a generic model or any other previous iteration of the model? And this is where evaluation comes in. Evaluation typically means that you're providing an output for a set of input to an evaluator. Evaluator could be a programmatic evaluation. It could be human. It could be another large language model and for our image generation or consistent use cases, there are multiple valid ways to generate image, so it's not easy to compare it to a reference images just like it might be for any other like a text-based generation. So for image generation use cases, comparing the output to a reference is generally difficult, and that's where you want to. Consider rubric-based scoring. It's very similar to how you might have seen your teacher grading your essays or you might have participated as a judge yourself in certain contests and you're given a rubric. Very similarly, the same concept applies here is you're giving the rubric to the to either human or the model, and then they will assess the quality of the output or the images against those rubric elements. And the critical thing here is because you can evaluate a number of things. You can evaluate subjective elements using rubrics, whether it's the aesthetic appeal or whether it's the creativity. You can also scale efficiently without requiring the number of reference images for each test cases if you are using the rubric-based scoring. And lastly, you can also gain granular insights into where your model signs and where it fails so that you can adjust it in the next iteration with the rubric-based scoring. And as I mentioned, you can do this rubric-based scoring using human. Humans can be that evaluator, but there are a couple of challenges. One, it wouldn't scale well, especially if you're evaluating hundreds of thousands of images. And the second is humans inherently are prone to bias. We like certain images or certain output more than the other, even when you are given the rubric, right? So that's where LLM-based evaluation would be key here and also often known as LLM as a judge evaluation. So let's see how we can build a sample architecture of a HLM as a judge evaluation for our character consistency use case. On the left hand side you see here is the image generation pipeline. Here you have this customized model that we fine tune, the Nova Canvas model, and then you're generating the set of output images for the input. We are storing those images in S3. We're storing the metadata information in Dynamo DB. The metadata could be the key attributes, including the input prompts, because you want to have that input prompt information for the corresponding image because you will be using that for the evaluation. The next is you have this evaluation orchestration. This is where you can use the event bridge to trigger the event. You can trigger it based on certain schedule or you can trigger it based on specific events like let's say end of a training job, and then step function will basically manage the whole workflow orchestration. At the bottom here you see is the evaluation service with lambda, which is containing the main logic here. It basically fetches the images from the S3 along with the metadata from Dynamo DB. It fetches the rubric template and then creates this evaluation prompts and then sends those images, the metadata, and the prompt to Bedrock for the judgment and then generates the formatted output. And the rubric itself shown in the box in the pink box here again you can customize that rubric based on your use case and your needs. You can even make it weighted so certain elements might have higher weight than the other, but the key thing here is like you have this rubric that you're providing to the model, for example, this prompt adherence. Are the models generating the output that you're expecting? visual quality, are the images technically sound? Are they, are they realistic, right? And the character consistency, which is perhaps the most important our use case, are the characters, are the visual traits consistent across multiple images. And like I said, you can have different elements based on your use case. And then on the right hand side you see is the LLM just system. You can use any vision language model like Cloud Sonnet, Nova Pro as a judge from Bedrock, and then you're providing this set of images, and the key thing here is the set of images. See, we are not evaluating one image at a time. Because we want to have this consistency check across a number of images, and that you can only do that when you're evaluating a set of images together, similar to how humans would do it, right? We would reference the images compared against each other. That's what you do here as well. And then what comes out is the final data or the results will go into the data store and that it can use to visualize using Quicksight or some other visualization service. What makes this powerful is we're not getting subjective opinions here. We are getting the actionable feedback, right? We are getting what's inconsistent and where, and then this is also scalable whether you are evaluating hundreds of images or thousands of images. You can use the same pipeline to do that. With human. It may not be scalable, but with LLM as a judge evaluation, you can evaluate thousands of images with the same consistency as the first image that you would evaluate, right. And this is the results of the output would look like. There are 3 different scenes, 3 different prompts, all generated using the same customized model that we fine tuned, and look at the consistency. We have the same facial features, same style, same character essence across different images. There are different emotions, different settings, but unmistakably the same person in all of those images, and this is what a production ready system would look like. And this is what you cannot get from generic image generation model. So in summary, in our comprehensive workflow, we have combined the automated video processing with intelligent character extraction using Amazon recognition. We did fine tuning with Amazon Bedrock to create a customized model that maintains the visual fidelity and then dramatically accelerates the storyboarding process. So now I want to invite Safwan to share another pioneer story about how MisrajiI created Arabic vision language model. I want. Hm. So let me first of all introduce Misraj. Misraj, actually it's an AI lab based in Saudi Arabia. We are one of the top pioneers labs existing in our region. We're helping our, we're helping our ecosystem to adopt the AI specializing for Arabic. We have, we are specialized in the small large language model, which is small mediums can help for the agentic AI. Our team is working on the couple months ago for first vigin language model which is actually helping to fill the right to lift the script and also to fill the gap between a different existence. The challenge in Arabic is not helping in the existing model, the large language model, so we begin to gather different data to get an understanding of how we solve this reasoning and fill the gaps. Our solution Basir actually it's working on first of use cases is the OCR. So we came here to have different solutions from gathering the data and labeling the data and how to make it a hybrid and then supervised fine tuning and finally we got the right fitting Bassir. Let's talk about the four main things here the dataset and what's the experiment we have done, as well as the training, finally, and we will see the result. In the data we worked in two pipelines or in the main pipeline that hybrid between the real data and the synthe data. We started with the we started with the data itself, the real world data, with more than 300,000 data.data, and with the synthic data we worked in around. 200,000, which is augmented data can be fitting for the real world, and these data, it's an image that has the captions have different approaches with it filling the issues that can be in the realistic world. We work in a different barriarity here. We are taking crawl data from different resources and we take it from the books and magazines and images. We take it and shifting from the H to the BDF to processing a different approach with a caption and with a different color, with different dark downs and with the breakdowns. The quality filtering as well, it was one of our challenges that we worked on the pipeline. All of these, it's happened. It's giving us to reach to our pipeline we calls the ability to have the right data fitting and preparing for our next challenge with it, the training. So we came before we go to the training, the experiment we tried, we tried with working on experimenting with more than 7 small models, and with these models we worked on in sequence to shorten the time and getting what is the more benefit for us that can be saving the knowledge and can be very fine tuned well with our requirements and as well its vision models. We worked on three top things. The first one is what is more fit with us as a selection model, and the second one, what is the right strategy to keeping the knowledge there with the coder, with the decoder as well? And then we go with what is the highest length we can take with them. With this approach we worked on several things, choosing three main models. Then we see what is the more winning with us for the requirement in the fine tuning and what is the best for our decoder and encoder. We can work with it and which one can be held with us with the long length for the context. In our process for the training, we came to the point that Have 3 main things is in working in all by blind data sets to be in training at the same time and with the surgical fine tuning processing that helps us to keep the knowledge itself within the model and how we adopt and inject the new knowledge with our new data there. So we are choosing a different strategy here from perspective of the training and perspective to being in the parallel as well in how to configuration and monitoring the result at the same time monitoring, and this is giving us an advanced way to understand what is the best decoder only fine tuned and what is the output can be saving for our existing there. The cost optimization also helps us a lot to keep to keeping the barcha happening and training in the same models. Here we come, we come to the result that we can call itsota, a state of the art based as small, small models that can be vision models competing with the old top in the market in our Arabic language, so we are enabling the OCR for the Arabic language. Competing with all the key achievements here. So today we are available in the Bedrock marketplace as our model, vigil language model and also is existing, working together with a lot of customers. Now we are working with more than 140 million documents to be digitized and achievement in our models. What we also make it here, we make our own benchmarks that help all the other models to be contribute and also sharing on this. Thank you so much, and I will ask Bernard to continue the next chapter here. Thank you, Safan. I hope you enjoyed listening to the three AI pioneer architecture patterns for 2025. Now we're in December, right? So we start looking forward to the next year, that is 2026. So what exactly we see as an emerging architecture pattern, right, for AI pioneers? And of course it has to be agentic, right? You, you can't get away from that. So let's discuss about what we are seeing now. What we are presenting to you is what we call an intelligent control and operations plane, ICOP, right, in short. What does this actually mean, right? Now think about that, that what is, imagine you have workloads like for example, LLM survey and model customization that we presented to you, right? What is our current experience to deploy these workloads, right? You can start with UIs, right? You can go to AWS Console, click a few buttons, and it will provision the infrastructure for you, and you can start using that. So UI is the first approach to do the deployment, but it is obviously limited, right? It is not repeatable. You need to learn the UI. So if a new workload comes in and it will have some option hidden somewhere, you need to go and find that option and all that stuff, right? So it's complex. The next option that we have is APIs, right? APIs give us programmatic access to the infrastructure and applications, so that way it can be automated, but APIs have got its own limitations, right? You cannot, for example, progress much with API until you learn the request and response formats and many other things that come along with the APIs. The story for 2026 is agentic coding assistant, right? You don't need to use UI, you don't need to use API, use some agentic coding assistant of your choice, maybe Quiro. Use MCPs with them and start working on it, right? With very limited upfront knowledge, you can start deploying these workloads much easily. But if you have worked with these agentic assistants, I'm sure you have faced a lot of frustration as well, right? The problem being they take multiple iterations, multiple engagements to come to a point, because they are general purpose. They are not focused on a particular workload. They are just a broad intelligence capability which is very generic, which is very beneficial, yet it is challenging when you want something very specific like LLM serving. You don't want to be wasting time and tokens talking with it over and over again just to come to a point which you hope can be done very quickly. So that is where there is an opportunity to do better, and that's what we are seeing as where ICOP plays a role. Now what exactly it is. So imagine an experience, right? where uh let's take the LLM serving platform as an example. The user comes and says, I want to deploy an LLM for a customer support agent. Very simple requirement. So user is not telling which model to deploy. User is not saying which infrastructure to deploy in. User is just saying, I want to deploy an LLM which will support my customer support agent. Think of ICOP as a simple API endpoint, right? It will be an intelligent API endpoint exposed by the provider itself. So for example, imagine if we as AWS start providing you such an endpoint over our Sagemaker service or for our Bedrock service, right? So you can come and just talk straight natural language to our APIs directly. There is no agentic assistant in between, right? Straight API endpoint to which you talk in natural language. Now. You come and say I want this, and then we ask a few clarifying questions to you like, OK, you want this LLM. Like how many users do you expect to be running concurrently? What are the latency characteristics you need? What is the cost that you are ready to accept? A few clarifying questions, yet you will not want to overload the user with asking 5000 questions, right? That is where the knowledge of ICOP comes in. That because it will be specialized on the particular workload here LLM serving, it's not general purpose, very focused on LLM serving, so it will know exactly the questions to ask, but it will also know what are the smart defaults, right, for a particular use case because it is focused on that particular use case. So it asks those clarifying questions, you understand better and then pass on a specification. To the next next module that is the planning module. Planning module takes it, looks at the specs, and go back and proposes a few deployment options to the user, saying, OK, based on your requirements, maybe here is a performance optimized option and here is the cost for it. And then it will say, OK, maybe there is this other more cost effective option, which is maybe you'll have slightly higher latency, but it will be more cost effective. So it's option number 2 also. And then the user makes a choice like, OK, please deploy the highly performing option for me, and then the deployment module goes ahead and does the deployment for the user, right? And finally, after deployment, it shares that infrastructure inventory to the monitoring module, which then starts monitoring it, right? So with these construct of understand, plan, deploy, monitor, as you can see, these are fairly generic constructs, right? You can apply to we are talking about LLM serving, but you can apply for customization. You can apply it to DevOps agents. You can to anything or any workload you might have, you can apply this construct. Now how do we build So here we present a high level schematic of what an architecture might be, as you would have guessed by now, it will be like a multi-agent architecture, and there is one specific agent catering to each of these stages that we have described here. Now agents will talk to each other over A2A protocol, and then they will have a number of tools to which they can connect and The job done. So for example, the Search MCP and AWSMCP to actually do the deployment, etc. Now one key point to note here, right here, if you see the block, we are talking about a customized small language model. We are not saying go to the biggest and baddest model out there, right, because all we want is to do that one task and do that one task really well. So there will be these customized small models which will be achieving this task. Now what would be the anatomy of such an agent that we are showing you, right? This is where Amazon Bedrock agent core comes into the picture, and it provides you those key constructs like runtime, agent's memory, agent's observability, identity, many other constructs that you might have heard in reinvent over the last couple of days as well. Using that, you can build all of these agents today, right? So you might have a workload. Imagine you're a platform engineering team, right, and you offer services to applications within your organization. It might be worthwhile to think about such an endpoint that you might provide to your developers within the organization. With that, uh, let me summarize what I present here, right? I think the key difference, the first one is this particular endpoint is fully supported by the service provider themselves, right? So it is not like a generic agentic coding model, and that is why it is well tested for that specific use case. So you don't waste a lot of iterations going back and forth because you know nobody tested it and they have just given it to you. It is purpose-built SLMs, so it will be cheap, fast. You wouldn't worry about token consumption because mostly such services might be offered free of cost by the providers because overall you achieve the workload on top of them. And then you can also think about ICOP being offered as a tool which can be consumed by your agentic coding assistant, right? With that, we complete our session. Thank you so much for your time today. I hope you enjoyed listening to us as much as we enjoyed presenting. Please do not forget to provide the survey, as that is super critical for us to improve in the subsequent years. Thanks a lot.