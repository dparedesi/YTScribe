---
video_id: d9C-jvrDYYk
video_url: https://www.youtube.com/watch?v=d9C-jvrDYYk
is_generated: False
is_translatable: True
---

All right, uh, thanks, first of all, thanks for waking up early with us, uh, today guys, didn't stay out too late, I guess. So Let me first uh tell you about a problem that probably exists at your organization today. And someone probably you know a staff member, a student, a faculty member, maybe you or or someone on your own team, um, you're working on a processor or a workflow and you know you've been using it for years and it's either broken or inefficient and you think to yourself or they think to themselves, you know, I wonder if I could use AI to improve this process or fix this process. And there's just so much noise and buzz going on around AI services, um, you know, what services are available, what models are available to use, um, what they're allowed to use, um, they just don't really know where to start, right? Um, and they, they just get kind of stuck before they can even start experimenting with things. They don't know who to ask for approval internally, um, and in order for them to actually start testing these services, start using these services, um. They start spinning up things like shadow IT or they start taking their sensitive data and sending it up to public AI services, um, just using like their own personal, um, individual logins, um, or even worse, they basically just give up because they're stuck and you know, no innovation happens, they go back to their old legacy, um, inefficient process that they've been using forever. Now, Cornell had that same problem until they built a platform that everyone could centrally work towards and give everyone in the campus a starting point to start innovating and experimenting using AI solutions. And today you'll meet two people who helped drive this change, uh, my two wonderful, awesome customers that I, that I personally get to support, um, and they took their campus, Cornell University, from, you know, I wonder if AI could help solve these problems to AI actually is helping solve these problems. They built an AI sandbox platform using Amazon Bedrock and some of our serverless architecture that you'll see in a little bit. And this allowed professors, students, faculty members, central IT members all to have a central starting point to start experimenting with their own workflows and processes and seeing how they can innovate using AI services. My name is Mike Bergnoni. I'm a senior solutions architect here at AWS, and joining me today are Marty Sullivan and Furman Romero of Cornell University, and you're gonna be hearing from them in a moment. Uh, they're the real, uh, brains of the operations up here today, um, how they implemented the strategy at Cornell, as well as the key AWS services that they used along the way. Now Cornell really had the same challenges every other campus, um, or, or a lot of organizations did, you know, Chat GPT came out around 2022 and everyone suddenly wanted AI, right? But they're really at that time there wasn't a safe place to experiment with it. There was no real way to integrate it into existing workflows and processes, and there was no controls at the time around cost or or data protection and privacy. Um, you know, and they could have done what many organizations did or, are still doing today, uh, create this centralized AI team that focuses just on AI projects, um, take in requirements, and like they're the only ones that are allowed to decide how AI is used and where it's used, um, and you know that runs into your typical project backlogs and by the time they release a an AI solution to their end users, the use case probably already changed because AI is changing so rapidly as we've seen. But instead, Cornell asked themselves a different question. They asked, what if we stopped trying to build the solutions for our end users and what if we let them, you know, gave them the tools to build these solutions themselves. What if the professors in the classrooms could actually see the problems that they're having and actually be the ones to solve it using AI and you'll hear one of the use cases actually is just that. And their belief was that AI shouldn't be this, you know, specialized project that no one else has access to. It should be something like a daily tool like like we use email or Slack, and everyone should be allowed to use it. That meant that the barrier had to be near zero so that everyone could easily adopt it, right? And that philosophy that the individual contributor is really what matters most when finding these problems to work on in your organizations is really what drove Cornell's AI sandbox platform to fruition. Now Marty and Fuhrman are going to walk you through 3 specific use cases today. Uh, a chatbot that aligns student values to their educational goals. A Socratic chat application that can show whether a student actually understands the course material that a professor is teaching them, um, versus just them, you know, maybe memorizing answers to test questions. And then an AI powered expense application that actually helped clear out a a constant backlog of expense reports. Now, before I turn it over to them, I just wanna show, these are some of the key AWS services that all work together to help streamline Cornell's application deployment process and allowed for this rapid innovation and experimentation at Cornell. Cloud formation manage their infrastructure deployments and they use code build and code pipeline to orchestrate their CICD workflows. UCS Fargate was what they used to deploy their containerized applications and it allowed them to handle scaling automatically without having to have anyone manage individual servers or worry about OS maintenance, things like that. And throughout this whole process, uh, Bedrock was the service that they used to provide access to foundation models which helped power their AI applications. And as I mentioned before, you know, technology is rapidly changing, and when Anthropic was releasing their new models of cloud, the new versions of cloud this year. Uh, Cornell had immediate access to those through Bedrock, which, which was huge. They were huge. There was no additional vendor negotiations. There were no API rewrites or anything like that. All they had to do was update the config in their code. One line, change the model name, and redeploy the application. Now they're on the new version of Cloud. Now the combination of all these services are what created this automated efficient system for Cornell to experiment and deploy these AI powered applications into production. All right, that's enough from me, so let me turn it over to uh Marty and Firman to tell you how they executed the strategy, but also listen in on how you can take away some of this and apply this to your own organization. Thanks, Mikey. Good morning everyone. My name is Ferman Romero. I'm an application development manager at Cornell University, and uh I just kinda wanna recap a little bit what we're gonna talk about. So we're gonna talk about how we built this platform, right? One of the things that I've found out as I've gone around and talked to people is by learning how we sort of rolled this out to bring it to our entire university, it made it so we can have so much less friction to be able to deploy new products, you know, Mike mentioned new models are able to come out as soon as we can load them up as soon as they show up on Bedrock. We're gonna talk about the 3 use cases. I'll walk through one of them. Marty's got a couple of the others, and we're gonna take a technical dive into how those use cases work and how we're actually solving real time and solving real problems with AI right now at our university level. And uh we'll talk about the technical stuff on how we got there, and, uh, our hope is that you're gonna understand that if you sort of follow this sort of model, you'll also be able to deploy on a much broader scale for your organization, doesn't just necessarily have to be higher ed. So, as Mike had mentioned, our, our journey started out with a question, right? When Chat GPT came out, the entire university, all of the colleges across the university and their individual departments, they started reaching out to OpenAI and reaching out to anthropic and trying to figure out how they could get accounts for each one of their individual schools. And, uh, right away we knew that this was going to be a long term headache. There's no management, there's no observability, there's no anything, um, and also we, there's no way to control costs in that scenario. So we started to think like what if we could build some sort of a central AI platform, right? Like what does that look like? What, how do we even bring a central AI platform or what can we build? Like how does that even work? and uh how can we make it accessible to all of our developers on campus? One of the things. that was really important to us is that we can put this technology in the hands of as many people as possible, because when you confine it to just a small group of people, that group of people may be making decisions or not understand other people's use case and so we want everyone to be experimenting with this and uh we learned about keeping humans in the loop and we learned about how uh our data works with all of these things. So, uh, I'm gonna talk a little bit about the high level and then I'm gonna turn it over to Marty here, but basically our platform's made up of three components, and the first component is light LLM, right? And that is what we use as our AI gateway that lives on top of Bedrock. Then we have another automation platform called N8N, that's what we call our agent studio, and, uh, that, uh, that's where we, we actually are able to build all of our automations in a low code environment. Um, this allows us to have what we're calling citizen developers. And then last we have Liber chat, right? And that goes back to that chat GPT thing where everyone wanted to have that, but if we can implement this on top of our own gateway, we can provide them a lot more model access. So I'm gonna hand it over to Marty, he's gonna give you a little bit more uh technical information. Sure. Thanks Firman. Uh, so I'm Marty Sullivan, so I'm kind of the one who architected all of this stuff and, and implemented it, uh, kind of, you know, as fast as possible, and, uh, you know, put it in the hands of as many people as I could. So, um, the first thing I wanna point attention to is the, is the diagram, um, so what we're calling our AI comments is sort of like the. The first thing that like the end users on campus are seeing. So this is like your day to day people who are using AI and they might be fine with just using like a chat application like chat GPT or cloud desktop, but we're also trying to build things in a way that reduces our technical debt so what we're trying to focus on is building more conversational interfaces, so building things into, you know, applications like Slack and Microsoft Teams or. Just email back and forth we want to treat AI agents as a coworker rather than just another application that we're building and managing so we're trying to veer away from building things like web interfaces, web applications where each person is building something different and we have to figure out how to deploy it and then maintain it for years, um, you know, we might still have to maintain the workflow, but. We really want to, you know, not have to maintain like applications. So one of the ways that I'm enabling that is with our agent studio. So we're, uh. Powering our agent studio right now with uh an application called N8N. So NAN is a visual and low code, you know, workflow building service. So really we're what we're trying to do is when we're building agents we want to consolidate all of the technical debt we're building into a no code low code platform like that so that we can have many experts across campus on that platform. And then they're able to sort of quickly understand like what other people have built in there rather than trying to have to go out and like learn an entire tech stack of like what some developer built in a, you know, or grad student sometimes even in their uh tech stack um and then all of this stuff is brought together where with our AI gateway um as Furman mentioned we're using the late LLM software which is open source um. And the great thing about that is like we can gate access to not just AI models but things like MCP servers, things like that that hook up to um chat clients and and uh you know, give people access to the agents we're building and it makes it so the IT organizations themselves can govern access to not just the data sources because we have many data stewards who need to like build special views of data. Um, for our agents, but then it gives the IT organizations the power to control like, OK, like who, if you know if we're building a chatbot around, um, you know, something like the registrar like. That has student information in it, we need to make sure that only the right people can access that particular agent. Um, so, This all matters because I mean you know we wanted to build something that gave everyone access to whatever they needed so you know we're not locked into a single uh AI model provider um anybody can switch to different models, you know what you know based on what task they're working on they can have fallback models to make sure that their application is resilient and highly available. Um, or like maybe you know they just wanna use the cheapest model and uh their, their problem is solved by really anything, um. You know, obviously, you know, since we're building a platform that critical applications might depend on, we needed something, you know, we knew we wanted to build this in AWS because we needed to be able to scale out and, you know, accept, you know, thousands to maybe someday even millions of requests, um, and so we're testing that out today, you know, uh, already we have, um, you know, 24 of our internal projects are using the gateway every day. Um, uh, 70, you know, and just think about the scale of this, these types of projects we have, you know, brought on 70 master's level students to, you know, help us with, you know, tackling all of these problems that people have submitted to us, um. And people like Furman have joined us as, uh, you know, tech leads who come from all of the different departments and campus who are really interested in being involved in AI and want to lead these types of projects and work with different customers that they're not used to working with, um, and then obviously for the for the IT organizations themselves we're really interested in like also just monitoring like just knowing like what are people doing with AI, um, and how are they using it and how much are they spending on it. Um, and it just gives, you know, us insight into, you know, if we do need to produce an audit report someday for like what is Cornell University using AI for, um, we could pretty easily put that together. Um, so we provide centrally, um, you know, we're trying to take sort of a similar approach to like AWS and this like we, we're building sort of a shared responsibility model. Um, you know, because if we are putting these tools in the hands of just basically any, you know, we're not just trying to target, um, like IT people, we want, you know, some tech savvy power users on campus who might, you know, just be able to automate some of their own daily tasks to be able to use this, so you know. We have to have some sort of like agreement with our customers that like hey like these are the ways you know that you should use AI this this is the type of data that you're allowed to use um if you have like really sensitive data we might need to have a conversation about you know how we can work with you and make it so you can use our gateway but also you know protect the the sensitive parts of your data, um, whether that's due to compliances or just the university policy. Um, but it also, and you know, some ways that we can do that is we can, you know, create guard rails and monitoring and our own automations that look at everybody's, uh, agents that they're building and kind of give us, you know, alerts and stuff like that if people are maybe doing something they shouldn't with certain credentials or things like that. Um And then you know my background at the end of the day is a DevOps engineer so um I also wanted to make sure that we had a rapid release cycle for these things because we all know how fast these AI models and availability are changing so I built, uh, you know, our stack around AWS's code pipeline. Um, you know, we have everything in source control and GitHub and store our container images and ECR and we build those containers using code build then we deploy containers out to ECS using cloud formation. We also, you know, have everything built into our, you know, single sign-on uh that everybody uses. So, um, and then, you know, we use the application load balancer to. Uh, be able to, you know, create high availability and serve, uh, our services across availability zones, and we use the Fargate service to simplify, you know, infrastructure, um, so we don't have to provision, you know, EC2 instances or anything, we're just sending out, uh, Fargate containers, um, so really like the, you know, the greatest thing about this, uh. Release cycle is, you know, generally the software that we're using like Light LLM and N8N, they have zero, you know, day zero support of new models, and then generally, you know, they'll have a stable release within 1 to 2 weeks of new API features, uh, for the different model providers. So really this all leads to, you know, we're not. Forcing people to use some platform where we're 2 months behind on what the latest and greatest is, we're trying to keep up as fast as we can with new releases and, you know, pretty much for the last, you know, 6 months, every time there's been a new model, we've had it in there within, you know, ideally the day it's released, but usually within a few days. So people have been really happy with that. Um, so now I'm gonna start talking about some actual use cases. Um, we think we picked some really interesting ones to talk about today, um. So the first one I'm gonna talk about is our uh cylinder Center's uh values exploration and reflection assistant or VERRA um so just to describe the problem a bit, um, every year we have about 900 new incoming engineering freshmen, um, and an activity that the associate dean of the Cylinder Center, um. Likes to have happen is she wants her students to come in uh knowing what you know what are my values and what are the goals that I wanna reach like you know basically why am I here at Cornell like over somebody else who came here and how can I embody and reach those goals, um. Some students might come in thinking like, well, I know what my values and goals are, but Maybe they have an abstract sense of what that is, but they can't really articulate to another person like what those are, and that means they can't really articulate it to themselves. So what they do is they bring in professional coaches. I think they hire around like 70 coaches and every student that comes in gets a one on one session with a coach, um, so what we needed to do was find a way for the students before they met with the coach to. Generate basically what their values and goals were and give a summary to the coach so that they could go in knowing sort of where to start talking to the student. Um, So how we implemented this was we uh built an N8N workflow um and we worked together with the assistant dean to create this so she spent time using cloud for desktop to actually write out a very, very complex system prompt. Uh, you know, she worked really hard on this for, I would say probably several weeks, um, and it was very detailed and it had like PDFs with like, uh, research information and everything about how she wanted the model to behave, and she was able to do that all on her own. And then she came to us with that and we were able to basically plug in her uh data set and her um. Uh, system prompt into an agent, an NAN, and, uh, it was like ready to go, you know, I mean we, uh, we're able to use the same models that she was using in cloud for desktop to make sure that, you know, it was behaving exactly the same as it would when she was testing with it, um, and we were able to configure some automatic failover some things that she was worried about was she didn't want students to be able to like, you know. Do some of their own like prompt injection and change the way the model was behaving or have inappropriate conversations with the uh chatbot so we were able to use bedrock guard rails also through our gateway to um sort of keep students on track and make them uh you know not be able to uh you know alter what the uh purpose of the chatbot was um and all of this came together. Um, pretty quickly, so here's sort of what the workflow looks like, and you can see it's not even really that complicated, you know, it's like, uh, NAN has a chatbot built into it. Um, you can see there's a shared conversation history between in the top right is where the actual agent is configured and you can see that the conversation history is shared between what the user sees in the chatbot and what the agent sees when it's interacting with what the student is sending back and forth, um, and we have a a specific, uh, step where we're applying those guard rails I was talking about. And then we can, you know, handle errors and gracefully and things like that. So, um, it's really a great platform, um, and so just to go over some success metrics, um, we pretty much started talking about this with the Celliner Center in I would say June of 2025 and by July we had launched it and exposed it to over 900 students. Once we configured this, we got zero support requests and you know that made me think, well, are people actually using this? But I found like by the end we had had over 40,000 back and forth interactions with students, um. And the greatest thing about this was, like I said, the most work that went into this was the person who cared about this project the most. She was able to make the chatbot essentially and we just sort of on the back end threw together an agent that was able to, uh, you know, do exactly what she wanted it to do, um, without any, you know, IT knowledge for her and really for us it probably took a few days to put together at the end, you know, at the end of it all, um. And so here's just a few quotes uh to show and and you know I will say like you know these are really positive quotes and we didn't uh actually cherry pick these at all. I mean really we had a lot of um feedback that um Erica Dawson, who is the assistant dean um collected from the students, coaches and her staff and really it was overwhelmingly positive. So that was Vera. So that was kind of like a, you know, here's a once a year type of thing that we need a chatbot for. Um, the next thing, the next thing I'm gonna talk about is our Socratic chatbot. Um, so this is more of a everyday application that, um, instructors can use in their classrooms, um, so. Again, to start with the problem itself, um, the challenge we're trying to solve here is for specific to large classes. So I've worked with Professor Toby Alt for a long time. Um, he's actually my advisor because I'm also a PhD student on the side. Um, he, uh, teaches a course, uh, called Climate and Energy, and it's a very large class with over 300 students in it. Um, and typically it's not, so it is a science course, but it's not a science, you know, based, you know, it's usually people from outside science majors who are coming in to take it to kind of, you know, reach their science requirement for their degree, um. And so with these types of students, one of the challenges is is like how do you really assess that these students are understanding like what's being said in lectures, um, you know if you read uh Toby's quote here you know it's like the only way he always says this it's like the only way that I can know is based on what their faces look like when I say something and a lot of times you know students might be just tired or something and it's like, you know, there's not really a great way. Uh, to really know if the material's hitting. So I worked together with Toby. I sat down with him and, and, you know, took sort of the product manager approach and came up with some user stories and acceptance criteria with him for what we wanted. I also worked with our uh Center for Teaching Innovation which sort of manages all of our, uh, teaching technology at the university that's used in the classroom, um, and so the things that I came up with were, you know, instructors need to be able to. Use the tools they're already using we didn't want to make a separate application and make professors go somewhere else, so we knew we wanted to build it right into Canvas, which is our learning management system that we use at Cornell, um. And it really needed to be easy for the instructor. I had one specific professor who I talked to, and they said, look, if this is gonna take me longer than 10 or 15 minutes, like I'm just not gonna use it, so I had to meet that threshold in engineering an application like this, um. And then for the students, um, the feedback we got from the, um, Center for Teaching Innovation was they said you really need to be using a, a structured like learning framework that's pedagogically like proven so we chose Bloom's taxonomy because that's actually a common taxonomy at Cornell. I'm not gonna go too deep into it but basically it's just like there's different levels you can picture it like a pyramid. And if you're able to, you know, it starts at like, hey, if you can recall basic facts about a topic that shows like the most basic level of, you know, your ability to understand this topic. All the way up to like a PhD level should be able to create new solutions around that topic, um, and then also for the students they need to the in in a technical sense a student if they're given an activity in a course they need to know like when am I gonna be done with this so you can't just give them a chatbot and have them have to go back and forth with it. So one of the things that we built in was a uh. A progress bar for them that's actually transparent and guides them through Bloom's taxonomy. And that's, you know, basically at this level of course we're really only interested in them getting to level 3. We're not expecting them to be PhD students, so we just wanna be able to have them be sure if they get a a question around, you know, climate change, they should just be able to understand like how to apply that knowledge in the real world, um. And so you know the value here is like you know students are able to you know on their own time come back and forth to a topic and you know spend time thinking critically about it rather than just going in and answering uh multiple choice questions. And it really creates the same what we were going for was trying to create the same type of dialogue that students would have with a TA or a professor during office hours. Um, and they get, you know, the immediate feedback, and then for instructors though, what we also do is we're able to analyze the conversations afterwards and gain insights into things like, you know, OK, well if, if 20 students got topic A and everybody got to the applied level of Bloom's taxonomy, then, you know, we can assume that, you know, the class has a general understanding of that. But if 5 students out of 20 struggled on topic B. That's a clue that hey, maybe I need to like revisit this topic in the next lecture. And then I'm gonna hand it over to Furman to talk about our next before I go, I'll please visit the uh public sector booth in the expo because you can actually view a demo of the Socratic chat application there so. So for our third use case, uh, we're gonna talk a little bit about automations, um, because, you know, as we know that these AI tools, there's more than you can do than creating chatbots or things like that. And so the N8N platform allows us to do some, uh, some real automated work and save some real time. So I wanna talk a little bit about this project that we've got. Um, I've got some graduate students that I'm working with this semester who are also working on this. Uh, shout out to my grad team on this. Um, so when I was in college, uh, I would raise money for my club by going across the street and buying a sheet pizza. Then I would sit in the hallway and I would sell the pizza for 2 bucks a slice. And after all the pizza was sold, I'd go up to this lady named Judy, and I would take her the receipt and I would hand her the receipt, and she would write me a check to refund me out of our club funds for that pizza. And I did this all the way through college. Fast forward to 2025 and that same person who is doing the job that Judy used to do is suddenly looking at contract negotiations and gigantic catering bills and AV stuff. So for example, the, the chess club on campus recently put on a tournament, and that tournament had an AV company and a catering company and all of these things that that suddenly. This person is spending all this time on to refund these students out of their campus funds, and we have 1500 campus groups on campus, and they get about 10,000 requests per semester. And when she first told me 10,000, I actually thought she was exaggerating, right? Because that's the number we all use. How many, how many things do you have? I got 10,000 of them, but no, she legitimately gets 10,000. So we had to figure out what are we gonna do, how are, how are we gonna kind of, kind of solve this. So with 1600 groups on campus, uh, and 10,000 reimbursements, um, it sometimes can take up to 30 minutes per request. There's all these rules that they have to follow, you know, you can't buy alcohol, you can't do this, you can't do that, and, uh, and these are just things that, you know, you, it's not easy to automate. It's not a, it's not a thing that we can make this job easier for her. She's looking inside our campus financial system, she's looking inside the campus groups, uh, system, and all of this stuff with just multiple screens that have to come up that integrations would be complicated to build. So, this is just a time consuming uh process that somebody said, hey, I think that AI can solve this problem. So if the student goes to, I don't know about you folks, but like when I go out to dinner, I don't know where that itemized receipt is like the next morning, let alone like an 18 year old student being asked a month later where that itemized receipt is, right? Because again, she's gotta look for things like, is what did they buy alcohol or did they buy gift cards or anything like that. So the way that we put this together through automation on the NAN process is we do what we're called contextual prompting, right? And so instead of like prompt engineering, what we can do is we can go and gather a bunch of this data. So through this, we can program uh through API access to our various systems where it can actually go and programmatically begin gathering this data. So this is just code part of it and uh goes and looks for that budget. It goes and looks for, for various items, right? And uh and then it takes that receipt and it sends the receipt up to uh up to the LLM and begins asking questions like is this an itemized receipt? Does this have any of this or any of that and uh and that goes through multiple processes and the whole, the whole workflow sort of takes it through getting to a final prompt that gets sent to the LLM. So the question is, right away, one of the things that we figured out was, what is your biggest pain point? And honestly that was the first thing that we were told was the itemized receipt thing. Like they, they just don't realize like the difference between those two receipts they hand you at the restaurant. So we send that up to an LLM and we just ask a simple question, is this an itemized receipt? That way we can get some hard information and the LLM is super good at that, as you probably know. Uh, next we can ask, is there any alcohol on this, right? And uh, there's a couple of gotchas on there, and I'll talk about that in a minute. But um we can we can ask all of these basic questions and say is this this and we ask it to put out a JSON format in a very definitive yes or no because I know on questions like that LLM isn't required to hallucinate, it's just required to make an analysis on a particular image and receipts of of have text on them, so it's pretty easy, um. So this is kind of what our workflow looks like. So you can see there's that data gathering process that I was talking about where we go out and I actually go and download the guidelines and I actually go and download the audit instructions and uh we save these in an S3 bucket so that way we can modify them. It's not hard coded into the workflow at all. It's just an MD file that sits out there. And uh then we start putting that all together, right? So I make a call out to the campus group's information to see what kind of transactions they've got because when the student submits for this reimbursement, they go through a survey process and answer various questions. So I can pull all that information in and I start building this prompt that effectively is you are this kind of auditor, you are looking for this, you are looking for this, here's the information that you have. Amongst this information, answer questions about these particular items. And it starts to develop this overall what we call like an answer card, and that answer card just ultimately comes to a very simple yes or no, like can can this be approved, right? So if it's a pizza receipt, like I had talked about back when I was in college, the thing that you're gonna find is very easily the AI can say, yeah, that's good, right? But if something a little more complicated like that tournament that I had talked about, you might get a thumbs down, but that doesn't mean that it's bad, it just means that there's something to review. And as part of that JSON export that we require from the LLM. That JSON export has all of the answers in that card, and I force it as part of the process to talk through why it made certain decisions. So when she, when the, the human looks at it and it's got a thumbs down, it will say thumbs down for no itemized receipt, or thumbs down for. It looks like they bought a gift card, or in the case of a couple of students who went to uh Olive Garden and ordered vodka pasta, no matter what I try, I can't get the LLM to not tell them they're buying alcohol. I've put it in there to use your common sense about alcohol, but the the LLM just continues to flag that one. But these are some of the weird things that we have to overcome. And uh and it's been, it's been an, uh an interesting project to work because like like we had said uh we've got students who are helping us with this and uh and all of this is able to be rapidly uh iterated because this platform is is a low code, uh, platform with nodes. So The one thing that we know about is these models, they can be, they, they can, uh, hallucinate pretty easily, but, um, the, the main thing that we're finding out is, um, uh, when we go in and we, we say to the AI, all right, give your initial draft of this audit, like tell it, tell us what's going on. It's so often that we get a green thumbs up that we can simply go in there and the LLM will have talked through like here's why it's green, these two things match, they've got enough money in their funds, it's gonna come out of this account. And so in time, even though we've got a human in the loop right now, in time, I think the auditors who are that human are gonna be a little bit more trusting and they're gonna start to understand, but right now, every single one of them stops and every single one of them gets looked at. So I'm working with a guy named Jonathan Hart over there at the finance, and, uh, he was the first person who came to me about this, and this is another kind of lesson that we've learned along the way here is it's the individual contributors who are really the folks who are going to be able to tell you exactly how AI is gonna solve their problem. So where you have a manager who might say, I think AI can solve that, right? Oftentimes they don't fully understand what that means or what that process is, and what we're finding is that the individual contributor is your partner in this, they're the people who you can look right at them and say, what's the hardest part of this thing? What do you get stuck on? In this case, we found out it was those receipts. So we're saving over 30 minutes per request on 10,000 requests per semester. I know that math doesn't work for how much a single person has in a semester, but, um, the reality is some of these things do take a very long time, and, uh, uh, generally, so coming into the fall semester, uh, I went in and looked at the queue, and I think there was still a couple 100 in there, and we were getting ready to come into the fall semester, still finishing out from the previous semester. And uh you know it's it's at a certain point you just pick the ones that are under a certain threshold and you make sure that you've got, you know, basically everything there, but this is going to save all of that time because going into it, uh, the person, the auditor can look at that, they'll have that green thumbs up on it and they'll just be able to quickly go in and look, yeah, it looks like Furrman bought a pizza and he wants 25 bucks back for the pizza. So, uh, the other thing is, is because all of the same rules get applied and because it's looking at those guidelines and we've got those guidelines saved in an S3 bucket, they can go in and change them. So one of the things that we also learned about was every semester the rules are slightly tweaked because they learned some student exploited something in a certain way or one of the clubs didn't spend their money exactly the way they should. And so this allows us to quickly change that that system prompt and uh and be able to just continue, uh, iterating and moving forward. So, uh, and then lastly we built a dashboard for this and, uh, that allows us to get a lot of insight into, uh, what's being submitted and things that we didn't have before. So Moving past that, this is, uh, all made possible like everything about this is sort of made possible by the fact that we've built all of this on top of bedrock and built this with, with light LLM because this allows people to be able to see how our AI works on campus, right? It's not a nebulous thing where they come to a central department and say, hey, build me this AI solution, because they understand what those workflows look like. We were able to talk to them up front and say, hey, here's how this workflow works, and that's what we've been doing is going around campus and talking to them. What does workflow automation look like? What does it mean? How does it work? And that requires us to really start at ground zero, and I'm going in there and I'm uh spending time with another colleague telling people about the definition of GPT, telling them about how neural networks work, and giving them that baseline information that a lot of us kind of picked up over the past couple of years, but these are folks who've never been exposed to that, you know, when they're playing with cha. GPT they think that's the chat GPT is that chat interface. And so once we were able to roll out the um our uh sandbox which was built on LiberChat, people start to understand sort of how these platforms sit on top of a gateway that that gateway being bedrock. It also makes it incredibly easy to deploy stuff too, right? So as Marty mentioned, a new model will come out. I, I make a fun thing is every time a new model comes out I'll email Marty directly and I'll make some like image of people like holding up protest signs, you know, like we want Claude 4.5, and I'll email to him on the morning and uh and say hey uh we gotta get the new model up and uh usually I see it that day. So this means immediately my dev teams on launch day for a new model are able to use uh these new models and uh because we know that our gateway is already uh negotiated for medium risk data. There's no friction there either, right? So if a new plug-in comes out, and that plug-in can run through our gateway, that's it. It, I can, I can immediately put that into production because I already know it's the gateway that is approved for our medium-risk university data. And everyone on campus knows what medium risk data is too, so when we label it with that, people get it. I'm gonna turn it back over to Marty to talk about the rest of the technical stuff. Um, thanks, Furman, um, so. A lot of what we started with when we started thinking about this was, you know, we really decided that we wanted to go with self-hosted, uh, software, so that's both the N8N software and Light LLM have self-hosted options, um, even with their enterprise licensing. So, um, we, that was one of the things that we wanted not just because of like hey we want to make sure data stays within our environment. But also we have to access a lot of like sensitive data sources at the university and having to configure some software as a service, uh, you know, cloud service out there to be able to get into our private networks is really difficult whereas when we're in AWS we have a direct connect set up back to our campus so you know we can even access systems on our private network on campus, um, so that was really important to us, um. And we really like open source, uh, or source available software in the case of N8N because then we can make contributions to the source code to meet our needs. So all of those things came together for us, um, you know, it's, it's things that we wanted and why we chose the particular pieces of software that we did, um, in terms of integration patterns, um. You know, we really want an API first design because it really allows anybody to do stuff right so we can have developers who are building applications and they're they're just like, look, I don't want, I don't care about your, you know, no code platform. I just wanna build a really cool application on my own, you know, we can enable that for them. Um, or there may be a developer who wants to use uh AI coding assistant in their IDE like VS code or something like that or Quiro in the case of, uh, you know, if you've seen that around on, uh, AWS, um, you know, I wanna configure, you know, I wanna configure this IDE to use my, um. Gateway, you know, and be sure that my private code is going to our, you know, approved, uh, models, um, that was another really cool thing that we can enable through our gateway. Um Some of the challenges though are, you know, we still need to work with our data stewards and, uh, global, you know, sort of Microsoft admins and things like that when we want access to things like Microsoft Teams to build a conversational user interface or, uh, accessing certain email, you know, like Furman was talking about a project where we need to access an email inbox and create drafts for people, so we needed to work with multiple teams on campus to give us that permission. Um, and sometimes that can create friction, so there's still are all of these, um, sort of organizational and political challenges that we have to work through, um, but one thing we're surprised by is like people really are into AI at Cornell. I mean, so it's like usually you know it's a. You, I mean you saw the titles that we're working with. It's an assistant dean coming to us. So when you have somebody with that kind of title asking for these things, it's kind of easy when somebody says, oh well, I don't wanna give you that access. It's like, we'll go talk to the dean about it because she, she's the one asking for it. So, um, so some of these things have actually been pretty straightforward just because we have the leadership level people who really want this to work. Um, so that's an important thing to have at your organization is make sure you have your leadership's buy in, um. And uh in terms of monitoring, I mean, you know, obviously like I said at the beginning, we wanna be able to see what everybody's doing, monitor what everybody's doing and make sure people are like sort of following the guardrails and rules that we've set um. And the bigger picture that I really want anybody who's attending here watching this later to understand is, you know, we're trying to build. We're trying to create builders at the university. I mean this is what Cornell is all about. We want to have not just the students going off and you know, I think Cornell is like one of the top 5 or at least top 10 universities and built and, you know, new startups right now. So it's like we don't want just our students to have that entrepreneurship mindset. We want our staff to be able to have the same mindset when they're building things. We don't want them to just drone on doing the things that. You know their boss tells them to do every day we want them to innovate in their jobs and make it, you know, so they're not bogged down with 10,000 requests, um, and, uh, they can just have things, you know, go look at least a little bit smoother every day, um, and we also, you know, we want to set the standard in higher education, um. You know, we, uh, meet with and talk to other Ivy League universities and R1 institutions across the country and share what we're doing and they share with us and you know some of the things that we've talked about have come from advice from other universities so we think that, uh, you know, having that mindset of collaborating across the country and across the world really, uh, you know, that's what we're all about. So I'm going to pass it back to Thurman again. So I had talked a little bit about our student projects. So when we first started this journey, uh, we reached out just to wide open the campus and said, Hey, what do you guys wanna do with AI? And, uh, we got like 200 responses because it's one of those things where, as I had mentioned, you know, where a manager says, I know AI can fix that thing, right? And uh as we started looking through them, one of the things that we found out was, you know, some of them are just like, well, that's not really an AI job that's, that's kind of just a code job and I'm sure you know you folks are are familiar with that kind of uh of thing but um, you know, being a university, it's nice that we've got, you know, the ability to bring on grad students and so the first semester we reached out, we said, hey, you know, do you, who wants to participate in this and, uh, I think we set it up like an independent study and uh we had 50 students who came forward and, uh, you know, we started putting together. These projects and trying to understand how we could use student work to automate processes and so this semester, uh, the, the program's a little bit more mature and now we've got, um, technical leads who are uh participating in this and we reached out again and we said, hey, here's your chance to work with a real Cornell University professional who understands about enterprise grade deployment because the first time through, right, students are gonna build what they know how to build, right? It's gonna be pretty hacky and it's not gonna be deployable. So on our 2nd time through we had 75 students come forward and uh that that allowed us to start put together some teams of students and working with technical people so there's a couple of people on my team who are a tech lead. I'm a tech lead on 4 separate projects and uh we meet weekly with our students and they, you know, we do sprint meetings with them and they show us, show us what they've worked on that week, but um. It allows us to really get in there and and sort of show them this is this is at professional grade how we're gonna do an automation at an enterprise grade and it's been really cool because you know every time they kind of go outside the lines, you know, I messaged Marty a few weeks ago and I was like, hey, I, I think, I think they're using some AWS services here and he was like, well, we gotta rein them back in because, you know, we, we want them to build in our platform using our, our standards so that way we can actually deploy these things and put them into production. So, um, we've got all these tech leads around and, uh, we meet with the students and, uh, administration, uh, has, has really supported us a lot because they understand what we're doing or they understand what the goals here are, which as Marty had said and sort of our whole thing which is we want people to understand how this technology works so that way we can all learn from this thing, right? I often say, you know, when you're in higher ed, you, you work in a house of teaching and learning, right? That's the only, that's your mission is teaching and learning. And so we have to bring this technology at scale to the whole university so that way if one individual contributor says I think AI can fix this, we don't want them to be buried under a pile of, you know, bureaucracy that there's no possible way that they could ever try to solve that by us bringing this API gate. Way access by us bringing the platform forward by us bringing these tools forward, allowing them to chat inside a safe area that we know is good for medium risk data we know they can go in there and experiment um we've allowed code execution inside our Liber chat so that way they can even do some, uh, you know, Python experimentation. And overall this has just been a really, really rewarding, uh, sort of process because as we get more and more mature with this we're gonna start having more and more projects come to fruition and we're gonna have more people who see another project and are inspired by that, you know, that's something that I often will tell people is when I'm going around talking on campus I'll say I'm not telling you how to solve your problems. What I'm doing is I'm teaching you how our platform works and how we're going to bring AI to you and my hope. Is that you'll be inspired by something you see here to be able to say I wonder if this works and so as another part of this we actually do every other Thursday, another colleague and I, we host a one hour long what we call our open office hours and it's just wide open to the entire campus to come in and see cool new stuff and chat with us and tell us, hey, you know, I, I've got a 500 page PDF document that I can't figure out how to make it load into the LLM, you know, and usually there's somebody in the room who understands. Like, 00, I know exactly how to help you with that. It's because it's a 500 page PDF and, uh, and so this allows us to engage with the community because someone tends to come to those meetings and uh we'll ask a question that completely stuns us like, oh I didn't know somebody wanted to try that, you know, and so in, in one of the meetings a few weeks ago, uh, when somebody came forward and started asking some pretty complicated questions, Marty sort of jumped in and said, why don't we schedule a separate meeting with you so that way we can talk through what your needs are and kinda understand. And uh that's been really excellent because it's it's gotten people excited and it keeps us able to at scale provide services so that way our individual contributors can go out there and experiment and try new things. So I kind of talked a little bit through this, but if you are getting ready and you are thinking about bringing this to your organization, uh, we really would suggest trying this out, trying to build a platform that will scale, right? One of the things I talked about, uh, yesterday in a hallway conversation here was Cloud Code had recently, uh, has recently come out, and, uh, we already had our gateway completely set up, and Cloud Code allows you to use a custom gateway, so. A tool like Claude code, normally you'd have to get all kinds of permission to be able to use it and you know, where is the data going, how is the data being used, but because Clawed code plugs right into our gateway, which had already been approved through security on campus, I was able to put that tool into the hands of my developers on the first day, and that's really cool because it makes it so as a new AI tool comes out, as somebody on campus says, hey, I wanna try this AI tool, we can show them how to hook it up to the gateway or. They can take our documentation, load that into the sandbox, and ask the AI how they can hook it up to our gateway, and it creates this thing of citizen developers. It creates, as Marty said, you know, some, some savvy person on campus who doesn't necessarily work as a developer says, you know, hey, I understand how this AI works. Let me try to hack together a thing and experiment, right? And then that could become a project. And, uh, that's been really cool to watch the projects come forward. Uh, right now, Marty. And I are having an active conversation with a person on campus who I think works in desktop support about doing an audio to text transcription service and so we just enabled the new dioretization models which will uh identify the individual speakers and uh so we slammed together a quick little demo, a standalone HTML thing, and threw it up on the team's channel and uh, you know, it requires an API key, but you can load in a file and, and, uh, get that uh. Get that transcription to work and uh this just inspires people to see what's possible, right? Like, hey, give this demo a try, you know, it's not, I don't want to tell you this is going to meet your needs, but this is going to be something that you can try out and test and uh it, it's worked pretty cool. We're having a lot of really excellent conversations with folks who are asking questions of, is this AI, is this AI, is this AI? And as they ask those questions and we answer, yes, that is, no that isn't. We are learning in public and I think Marty had that on one of his uh one of his slides. So because of that, this allows what, you know, like I hate to say grassroots, we've been using that forever, but you know, from a bottom up um ability to say, you know, if a manager is saying to individual contributors like can't AI solve some of this stuff that you do, those individual contributors might not otherwise be able to answer that question. But because we have this platform, because we've been going around and doing, uh, you know, talks on how our platform works, um, I've, I've spent the past, uh, 6 to 8 weeks going around the university to the various colleges and various, uh, staff and faculty groups and talking to them about what we're building, how we've built it, what it's for, and the response and the feedback has been really, really cool. We've been, um. We've been impressed by the number of projects that have come forward. Like I said, you know, in that first time we sent out an email saying, hey, do you guys wanna do any AI stuff? Um, you know, the, the initial response that we got was like, uh, there's no way that we're gonna be able to handle this. And so that's why we put that governance model into effect. So I wanna go back here. oh, no, can I turn it over to you, Mike? All right, so I'm gonna turn it back over to Mike now to close us out and uh thank you very much. All right, um, so where do you guys go from here? Um, hopefully you learned a lot, but, uh, we actually just published a blog, um, I think it was about a week ago now or so. Um, so grab the link right there from the QR code. This actually, uh, we interviewed, uh, Doctor Professor Ault, um, that worked on the Socratic chat use case, um, in his pilot class and then expanded out from there, and he kind of talks, uh, a little bit about like, you know, what, what incentivized them, how they got it into, um, you know, that pilot class. Um, so check that out if you're interested. Uh, we also have an interactive demo if you have not been to our EDU booth in the industry pavilion. Um, it's booth 111, so you can actually click through and see a, a live demo of, uh, that Socratic chat application along with some, uh, quiz generation capability too that's built right into their, uh, learning management system. Um, So yeah, so um we will be uh around for maybe a few minutes after this session uh we can't take questions here but we'll be, uh, in the hallway there if you wanna ask any, uh, follow ups from, uh, Marty or Furman, um, and remember to also complete the survey in the mobile app once you have a chance. Thank you.