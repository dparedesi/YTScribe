---
video_id: l-mjftGWueA
video_url: https://www.youtube.com/watch?v=l-mjftGWueA
is_generated: False
is_translatable: True
---

Good afternoon. Hope, uh, welcome to Reinvent. My name's Joe Kirby. I'm a global go to market lead in our next gen dev experience team here at AWS. We're the folks that talk to customers and, uh, partners and so on about their software development life cycles, how to take advantage of AI in that space. Today I'm joined, very happy actually to be joined with Krishna Kanan, head of product for one of our partners, Jellyfish, and also Craig Dollinger, one of our, uh, customers, our joint customers with Genesis. So let's dive in. So today we've kind of got a story in 3 parts. I'm gonna tell part one, which is how do we go from this conversation to develop a productivity that we've had over the past couple 3 years to get us to this place now of talking about AI impact? How, what does that journey look like? How has that changed the way we think about it? Then I'm gonna hand over to Krishna who's gonna talk about. The way that jellyfish thinks about this situation, right, I'm gonna give you the Amazon perspective. He's gonna give you the jellyfish perspective, and then I'm very excited to have Craig come up and then share his perspective as a customer. So you're gonna get all three perspectives in this next sort of 45, 50 minutes or so. So, I've had the pleasure now of talking to customers about this space for about three years. So before we launched Code Whisper back in 23, all the way through 24, we've been really diving deep into this space. And so we've seen this evolution in tooling. Right, in 23 we saw this idea of like fancy autocomplete and then we moved into 24 with this idea of chat and more complicated transactions and interactions to get through to 25 today and now what we're seeing is all of these sort of more agent agentic type experiences with reasoned models and other things, right? They become much more interesting types of experiences, but it really has challenged this idea of what does productivity mean in this age of AI. You know, when we, when we first started having these conversations back in 23, it was very much like the developer productivity, right, the productivity individual human at Reinvented 23, we started talking about development productivity, the productivity of the team, the system, and last year at Reinvent, we were very clear in sort of talking about the distinction between productivity and toil, right, kind of two sides of the same coin. And then this year at New York Summit, um, Amazon released our framework for how we think about impact in software development, right, this idea of cost to serve, and I'll be diving into that a little bit later on. So as the tools have evolved, the conversations have evolved, and with all of the conversation this week about agents and other types of experiences, this is only gonna get more interesting, more complicated, uh, depending on how you think about it. So, before we get started on what kind of works, there's some things here that I come across pretty consistently with customers and the sort of idea of impact myths. The first one, AI can fix complexity. Nope. If your processes are complicated, lots of steps, lots of people, lots of interactions, AI can't fix that. It's all about creating 100x engineers. This also is not true. We see a lot of stories, maybe a bunch of things on LinkedIn, right, about people creating applications by themselves, but what we know is that generally speaking, you gravitate to the performance of the team that you're on. So yeah, you may have one or two individuals who are able to do amazing things, but in the whole of your development org they gravitate to the poor performance of the team. Focusing on a single measure is enough. Now I wish that this were true. I wish that there was some metric that we could say like that's the one. It is simply not the case. It is not possible. And as I go through the next sort of 15 to 20 minutes or so, I hope you'll get a sense why. And then also understand how these other platforms and our customers sort of worked with a range of metrics. And then finally this idea that we can um just turn it on and people will figure it out. I've had a number of conversations with customers when we're talking about this idea of enablement and training and how you're gonna let your folks know how all of these things work. We're like nah, I'm just gonna turn it on, doesn't work. If you look at things like the Dora report for this year, there's a lot of conversation there about structured enablement, we see it very much as well. You cannot simply turn these things on and and hope it'll work out. So that's kind of what doesn't work. So Joe, what does? So. If there's only one thing that I would hope you would take away from from this conversation, it's it's this, and it's how we sort of think about applying AI thoughtfully, yes, to software development, but in many ways any process. First off, this idea of elimination, what can you eliminate? I said AI can't fix complexity, so how do we start to remove some before we start doing other things manual steps, tools, processes, checklists, all of those things. What are the opportunities to eliminate those things? Also manual interventions. One of the things that that we found internally was the very high number of manual interventions in pipelines. How can you get rid of those things? What do you need to fix to make those things go away? Automate. What is it possible to automate with AI? Are these are typically lower value, repetitive, right? Maybe relatively low complexity type of things. I would say standard things like documentation, test generation. You've also probably heard us talk about our uh Java uh JDK updates that we did. So automating the sort of runtime upgrades, you'll see that in some of our AWS Transform agents, those are the things where we think about automating through the use of AI. And then finally assisting. This really has been true, I think, maybe in the last 6 or 8 months as the models have become better. But how do we now take advantage of AI running on your laptop as a dev, as a thought partner, as an assistant, as someone or something that is capable of complexity and thought and really being able to achieve high complexity in frequent tasks. So, you'll hear me refer to this, eliminate, automate and assist, but it's a really good way to think about where you're going to prioritize and use AI. So what are some accelerators for impact? AI is a new way of working. I've had a couple of customer conversations just this morning about this, right? We are in a place where everybody's journey to your aha moment where you say, wow, this AI thing is really cool. Every individual's journey is different, right? It's a very personal. So how do we get to that point where you're able to, where your devs are able to say, oh wow, this is really cool. By giving them permission to play. This is something that we do here at Amazon a lot, and we have a leadership principle, learn and be curious, which encourages us to take advantage, experiment with these new new kinds of tools. What we know is that if you have a development team that is on a delivery schedule and they have story points and features to deliver, and then you're asking them to rethink the way they do the job at the same time, it's not gonna work. You have to be willing in the organization to give your dev team some slack in order to give them the brain space to actually be able to play. Software development is a team sport. We know this internally, and, um, what we find is that developers gravitate to the performance of the team as it's as a as itself, right? It'll coalesce to the performance of the team. So as you're thinking about these things, how do you accelerate the performance of the entire team? How do we focus on those kinds of enablements, that kind of activity, right, to really drive, um, acceleration. Embracing sense making. What do I mean by sense making. So one of the things that talking to customers even today around legacy code or bringing new developers into existing projects or even on boarding new developers into your organization, AI can speed up your new hires or ability to onboard to projects, to onboard into your organization. When you're looking at legacy code and you want to be able to describe it, understand it, there's an enormous amount of value in using AI in that role. Right, you'll see this, uh, showing up in things like um. Uh, key rules, steering documents, all of these various ways to make your AI better, you can have your AI help you do those things. And encourage AI fluency. What do I mean by that? So I'm gonna say that we have the mechanics of your developers understanding how to use Q Quiro, those kinds of tools, but there is real value to be had from understanding more about the underpinnings of that. My team and I have done some really deep dives with customers this year, and what we found is that this idea of AI fluency, which I'll dive into more in a moment, is really important to get the most impact possible out of these kinds of tools. So let's talk about fluency for a little bit. Two broad buckets here that I think of. The first one is prompting. Who heard of prompt engineering a year ago, right? It was a big thing. Everyone was gonna be like prompt engineers. Well, we know that that's helpful and understanding how to write a good prompt is really valuable. And the tools can't read your mind. It's, it's sort of trite to say it, but it's very true, and we have to work on this idea of making the implicit explicit. I've been an engineer pretty much my entire career. I have things in my head, I don't say them out loud, and I expect people to know. AI can't read your mind in that same way. We have to be precise. Precision. If you wanted to write test cases, tell it write test cases and the format that you want and the framework that you use will make it much, much more effective. Structured problem solving, um, anyone heard of spec driven development? Show of hands, a couple of folks, right, so what we're starting to see is moving, and this happened with recent models is the models being able to build actually a plan for how they're gonna break things down and do things in small pieces when we're interacting with models and I certainly have seen this in the early days where you would say something like build me a shopping cart. Sure, but that's not precise, it doesn't break it into tasks, it doesn't really tell you what you want, and what you're trying to do is have the model give you what you want and not off-road, and the way you do that is with more structured problem solving. And then finally, the thing that I think a lot of folks forget is you can ask the tools for help. Hey, can you summarize this conversation into a prompt? What else do you need to know? How else might we write this? What is missing from this prompt, right, using the tools to help yourself be better with them, really powerful. So let's talk about context, the other side of this context, what else do you need to know? So context engineering has really been a conversation mainly that I've seen sort of in this year really. As the models have become better, the context windows have grown, the reasoning's improved, um. Same problem occurs, right, we're trying to use additional context to make implicit explicit. A couple of examples Q could create its own rules files. Quiro can recreate steering documents. You may have seen agents. MD, right? Ways to have context in your, um, repos that can help guide these tools every time. More is better, but only to a point. In the same way, if you were onboarding a new developer into your projects, you wouldn't necessarily have them read every repo and every application you've ever built. You wouldn't ask them to read all of your confluence. What you would do would be point them to more specific examples that would be valuable. That's the idea of getting to the right context. And you'll see a number of innovations that we've had with Q and Kira this year around constraining agents in the CLI, being able to pin context, being able to be more specific with tool usage, all of those things are trying to drive to the right context, not just more. MCP is incredibly powerful. Where we were 6 or 8 months ago was, well, just more MCP tools, right? I'm gonna give it connections to everything I see, but what we know is that that can very quickly overwhelm both the tools and the models and everything else, so it's very, very useful, but be thoughtful and again ask for help. Which of these tools would be helpful? What else do you need to know? I have this other file. Does that make this better, right? Engage in this idea of of it being assistive. Rather than you having to prescribe everything, ask it how it can help you be more effective. So Gone through what doesn't work, some things that we know do work in order to get this idea of how we then measure the impact, right, we talked about how to make it better. So measuring impact is a journey. Um, if any of you have followed the content that we've had for the past 23 years we've talked about this a lot. There are industry frameworks like Dora and Space. A couple of my customer meetings this morning we're going, we're talking about the same things. Um, I'm going to talk about the Amazon framework cost to serve software, which is something that we've explained how, how we think about it. But, and, and Christian will show you this, right, it is a basket of measures. There are a number of metrics and data points that depending on where you and in fact your teams are in their journey will vary. Qualitative and quantitative indicators are incredibly valuable. Qualitative, the why, quantitative, the what, right, so quantitative coming out of tools, qualitative coming from your humans. Um, and, and there's a lot of value in, in pulling those things together, and I'll give you an example in just a sec. And then. If I was going to say to you today, which is a lot of customers ask me. Uh, hey, we were looking for, for gains in, in productivity. I would say, well, how do you baseline it today? What tools do you have in place? How do you think about a measure productivity today? My suspicion is, I know many of my customers say we don't, it's ad hoc. We've guessed, we've got some bits and pieces. I suspect that a number of you today are sitting here in that same boat. You've got some idea, some tooling, some data points, but really the idea is that you baseline where you are today. And then look at those trends shift over time is the way to think about this. And finally, they're both leading and lagging indicators. What we often find is that customers and, and a lot of times it's the finance people want to jump to the ROI piece, the qua qualitative the quantitative impact, but we haven't gone through the leading indicators around things like adoption, engagement, how are people using it, what's the feedback, do they feel good? And so they are both lead and so I'll explain more about that, but there's basically this idea of leading and lagging indicators. So let me give you an example of how those things play out. So let's assume you wanted to roll out Quiro in your organization, how might these, how might you use the metrics and how might they change over time? So in the first instance, what you care about is adoption. Who's got a license, who's signed in, who has used the tool, who's enabled it, has anyone got authentication problems? You're working through all of those basic things, right? You're basically managing a roll-out. And what you want to do is observe that early usage and see if you can get any interesting insights from it. But at this stage, it's a lot more about does everybody have it, do they know how to use it, rather than trying to drive to an ROA conversation. Then we moved to engagement. Now I'm looking at different metrics. Now what I'm looking at are who am I highly engaged humans, which are the teams that have strong adoption, which are the teams that don't, right? You've heard me describe multiple times that team is where this is you're looking to enable teams, right? Team impact is important. So which teams are on board, which are not? How are your individual teams internally sharing wins? Are you sharing wins? Do you know of people that have had some really interesting stories? And how can you then mitigate low engagement, right? Craig and I were talking about this internally, and he has a great story about how Genesis was able to use these metrics to identify and mitigate low engagement, right? You might go and find, talk to people and find out that it's something simple you can remediate. The qualitative, the why, surveys, talking to people, walking the halls, all things that that we use internally and we've seen customers use to get to understand why, look over people's shoulders, right, really just observe and understand what's happening. And then finally we get into the quantitative. So now we're talking about measuring impact to your baselines. In order to have conversations with executives, conversations with finance folks, you need to be able to have a baseline and then show some improvement. Hey, we improved X to Y. That's why we encourage you to look at the quantitative as a set of lagging indicators after you know that people are using the tools, they're engaged and they enjoy it. So What about the tooling? So if I think about the tooling that comes from Quiro and Q Developer, and frankly what is likely to come from all of these various agents that we're all talking about, it's going to be activity that maps to the A in the space framework. Right, you'll notice there are 4 other letters there that we don't touch. The data that we provide can be used to drive to sort of engage, figure out adoption, figure out engagement to a point, and I'll give you an idea of the differences in a minute. But what we cannot tell you, because we don't see enough of what's happening inside your organizations, is the AI impact. Right? You can't tell AI impact simply by looking at the use of a tool. You have to look at other things, other baskets of measures, other data sources within your organization. The other thing that's really interesting and why this whole conversation of developer productivity has moved into AI impact is this idea of idea of second order impacts. When I talk to a lot of executives, the idea of these tools is, oh, can I write more code? Sure. But where else is there value? What else is happening? We we measure this internally, I've talked to a number of customers about it, and it's this idea of reduction of onboarding time. How long does it take for a new developer to reach the speed of the team that they're on? Time to 1st commit, time to 10th commit, typical measures, but that can be a huge 2nd order impact of the use of AI. Um, both of these apply here at Amazon. We tend to move developers around between projects quite frequently, and so the ability to reduce the time it takes to take a new developer or a developer onto a new team and get them up to speed is really important and it's impactful for us. The increase in skill flexibility is something I was talking to earlier when a customer was asking me about skill makeup, right? What do you think team makeup looks like? We talk internally about a couple of different stories, one of which is when we're writing the QCLI, we wanted to be able to write that in rust. We had a shortage of rust folks, so we were able to take Q basically and build itself and have experienced rust programmers do the code reviews. So we're able to increase the flexibility of the skills. You might also see this, um, for example, maybe you've got a shortage of front end folks, but you've got some back end folks with time. Is there opportunity to take advantage of the tooling as an assistant to expand the skill set, increase flexibility. That's a great 2nd order impact. Shifting bright Shifting into cloud operations, we've seen a lot of customers take advantage of tools like Q and Quiro moving into their cloud ops teams, site reliability engineering, cloud operations, troubleshooting, all of those things can take advantage of AI context engineering, promptte and prompt engineering. You can take advantage of those. That's not directly related to the developer necessarily. And this would show up in things like Dora metrics, right, other places that would show you the impact of AI in your cloud ops team. We also see it recently moving left into product management organizations. I've talked to a number of folks there who are trying to understand what's the value of vibe coding. Right, we've got technically minded product managers and product owners who have been able to build prototypes, and it takes a lot of time out of that spec design process. People can react to those. How are you able to represent that in the metrics and data that you capture? It's really interesting, right? One of the things that when, when the three of us were talking about this, one thing that came up consistently, what are the projects that would never have been done if this tooling didn't exist? We talk about internally, there was a team um at Amazon that was responsible for heating and air conditioning, who needed some simple app built, couldn't seem to get time, so they were able to take AI and build it themselves. Right, what are the opportunities there to undertake projects that would have never happened? How do you find that, represent it, calculate it, right? There's value there. And then the other thing that happens a lot. Um, when I talk to customers, especially ones who are undergoing a lot of change in their devorg, is how do I isolate the impact of AI relative to these other changes that we're making. Right, we have a complicated system. I might be doing a dev portal and changing my tooling and merging with a company, all of these things add complexity. So it's really intriguing, right? We've gone from this idea of an individual human getting code completion to we're able to take advantage of essentially the same tools in multiple places in the organization to drive a much larger AI impact. So How do we do it, right? Um, luckily here at Amazon we can, we're, we're a large organization, right? AWS, Amazon, we have lots of different businesses, and so we tend to look around and see where there are ideas of things that have worked before, where can we learn? And so when we looked around, we looked at the supply chain work that we've done with um Amazon.com and this idea of cost to serve. It's something we have talked about in shareholder letters. And so the cost to serve at Amazon is all about this idea of we have units, packages, and customer delivery, and how can we reduce the friction, delay, waste, and defects in that process. Anytime we can do that, we are reducing the cost to serve, the cost for us to be able to get value into our customers' hands. And so we looked at this and we said huh that's sort of intriguing because software especially here is a complicated supply chain it's all kinds of different interactions we have everything from um. You know, hardware build to hardware software, uh, software running on our hardware devices to AWS to .com to all of these various businesses using a variety of different things and it's complicated. And so if we look at this opportunity for friction, delay, defects, and waste anywhere. That can improve our cost to serve. Intriguing There's actually a, there's a science paper and we've published more information about this if you really wanna dive into the science, but the fundamental equation is the cost to serve software is the infrastructure costs, the sort of the human and tool costs divided by the number of units. The number of units will vary if you're shipping a mobile app to an API to a monolith to something else, so the number of units is really whatever the unit is that your team typically thinks of as a unit, but ultimately this is something that you can track over time and it does allow for the complexity and it does allow the acknowledgement of some of those second order impacts that I described. Luckily we have teams of folks that can do this work, but we were also able to isolate some of the ways that we improved or were able to reduce our cost to serve. CICD, it works. We have data. One of my colleagues was very happy about being able to make that statement. Manage templates and abstractions is something we found to be really useful and of course AI. Any individual one of these was helpful, but by bringing them all together, we really were able to drive a lot of value and a lot of improvement across our processes, ultimately with a reduction of almost 16% in our cost to serve across the organization. For a development organization the size of ours, that is a very material impact. But you'll notice I didn't say a coding companion did that. This is the impact of change across your software development processes. Now we're lucky that we have the size and scale and folks to be able to do this for ourselves. But what I'd love to do now is call Krishna up to the stage to talk you through how our partner Jellyfish thinks about this. Krishna? Alright, um, thanks, Joe. Uh, my name's Krishna Cannon. I work for Jellyfish. I lead our product organization and we're really excited to be here talking with our friends at Amazon and our partners at Genesis about measuring AI impact. So I'm gonna take a half step back here for a second and talk about measuring engineering productivity because that forms the backbone of what we'll talk about in terms of measuring AI. So Jellyfish has been measuring developer productivity for several years now and thinking about how to do that, and our approach has been first to take a look at data from your issue tracking system, whether that's a Jira or a Linear. app or an Azure DevOps and combine that with your source control data, and that gives us a couple of things. One, that gives us the business context and project context for what you're working on and why. And then combine that with the source control data like your GitHub or your Bitbucket helps us understand how long it actually took to do that, what language it was in, all the other metadata, so we can create a signal of what folks worked on through that and then we combine that with data from your HR system, your CICD pipelines, your error reports, incidents to really get a complete picture of how engineering is happening at your organization and so we've been doing this for a number of years now. And through that we produce developer productivity metrics among others, and I expect many of you are probably measuring things like this already either with jellyfish or a similar tool or maybe with something that you built yourself to understand things like throughput, cycle time, team velocity, and so having a good understanding of engineering productivity is a prerequisite of course to measuring the impact that comes from using AI. So shifting gears here into our approach for measuring AI impact, I'll start with a few general observations and then I'll present a framework that we recommend you use that aligns with what Joe's talked about and with what Craig's gonna share in a few minutes. The first observation is that across our database of, you know, 6 to 700 customers. You know, I'm hand waving here a little bit, but virtually every software company we've talked to has adopted AI at an organizational level. There are a few stragglers, of course, but organizational adoption is not the, the barrier today. Uh, the barrier today is that despite that 100% organization adoption. You really only see about 30% of those companies benefiting at scale from improved productivity through AI tools. You know, this is, this is our conclusion. This is jellyfish's conclusion. But if you look around at other studies throughout software or the knowledge economy as a whole, you get very similar findings. I read a BCG report just recently where they interviewed a public company executive and found a very similar, uh, report. So a couple of things that we're seeing here. First, Jellyfish did a state of engineering management report, uh, over the summer where we talked to a ton of CTOs, VPs, directors, and interviewed them about their AI journey thus far, and this is a qualitative statement, um, but the question was what percent improvement in productivity are you getting today or do you expect to get over the next year through AI? And the surprising thing here was that it was a very open ended question that sort of begged for an optimistic response, and yet from that. Only 30% said they were getting 50% improvement or greater, which is of course great for that 30%, but you see that the bulk of the curve is actually at 10 to 25 to 50% gains, which is good, but that's not the expectation that we're getting from the hype over the last two years about what AI could do. We expect more from this. We also see that of the companies that have adopt adopted AI fully across their organization, you actually do see fairly dramatic improvements in productivity. This is a a quantitative take here. This is from another study we did across 13,000 company engineer week uh observations and found that over a 2x gain in PRs per engineer when you get to that top end of the curve. So the improvement and productivity is there, yet many are still struggling and so we've been calling that the AI paradox at Jellyfish and try to help companies through that. And so in thinking about that we interviewed a ton of companies, talked to a lot of company leaders, talked to a lot of developers, and what we found is that the gap between organizational adoption and individual adoption is generally not one of effort or interest in using the new tools, right? Engineers by. Personality training many of us are are engineers here of course are tinkerers and experimenters and we want to use new tools we want to use AI we want it to work. But we're also many of us skeptics, right? We're not gonna use a thing because someone told us to, we're not gonna use a thing that makes our jobs harder, slower or worse, so the key to unlocking greater adoption and productivity doesn't come through. Organizational mandate or fiat, right? It comes from understanding what are the blockers and then how do we get past those blockers to get true success. So Through our work with companies we identified a 3 step journey to go from adoption to productivity and then to business outcomes in that order. And then we generalize that into an AI measurement framework and so I'm gonna talk about what that framework is and I encourage folks to hear the framework and figure out how it can work in their organization because it's not a specific set of instructions around metrics to track or numbers to look at but it's a way of thinking about those metrics to try and drive success for you. The first part of the framework is adoption. And when we say adoption, we are really thinking about Are engineers using the AI tools that you've procured? How many tools are they using? How often are they using them? And then what percent of their actual work comes through the use of those tools? So you can come up with a whole litany of metrics that sort of satisfy this criteria. So I have an example I wanna walk through with you. This example here is from a sample company that I worked with closely and then anonymized for this purpose. And what they did was they said out of all the potential adoption metrics they want to focus on 4, which is a good number, 2 to 5 metrics is a good number. They want to say first, are we actually trying enough different tools? And so in their case they're they're trying 5, which, which is awesome. Then they want to say, Are enough engineers using something at least weekly to begin to learn how it's working from that they want to go to power usage, which we equate to daily usage, and then actual output from those folks. So what percent of the work they're doing was assisted by AI in some way. So they selected 4 that align with their sort of methodical step by step approach. And they compared them each to an industry trend. Now the good news is they actually compare very favorably to those industry trends, as you can see here, but there's still room for improvement, right? Even though they have 44% of their organization using AI daily. What about the other 56%? Why is that the case? And so this is where segmenting the data becomes your friend. Right, a naive organization might go in and create a, a mandate or some sort of demand about it, but we recommend that you identify the 56% that are not using AI daily by segmenting your data, cohorting your data, and understanding is that specific teams, specific locations, tenure of employee, or if you can even understand if it's a certain type of work. Where AI is simply not effective yet for your engineers and that's why that connection between your uh GitHub and your Git repositories and your issue tracking is super important so you can understand what types of projects maybe AI is not being used where maybe it's less effective and maybe that's driving these numbers. The second part of the framework is adoption. And so when we talk about adoption, we're excuse me. It's productivity obviously. The second part is productivity. When we talk about productivity, we're thinking about the actual outputs from your engineering team. So are you getting more things through your process per unit time? So cycle time, throughput, counts of commits, PRs, etc. These are examples of productivity of your engineering team. Now in our example here, this sample company centered on the pull request as their atomic unit of measurement. And it's important that they aligned on what was what was important for them to measure. Because there's a lot of different ways you could approach this we've seen companies look at cycle time they might look at issue throughput, epic throughput, any number of things, and it's important for your culture to all be aligned on the same thing that you wanna look at. They chose pull requests because that is something where they are generally smaller, they're voluminous enough that, you know, outliers and other noise sort of drops out and you can get a good signal on what productivity looks like. And so they were looking at PR throughput as their main as their main metric here. Now in this case, despite the fact that they're going faster, they are trailing the industry trend. And in this case our recommendation was less about segmenting on cohorts of individuals we assumed it was less of a this group either junior or senior engineers this location that location. And much more likely about the type of work. Because what we found is that The age and complexity of the repo you're working in, whether it's an old code base or a new code base, what type of project it is often matters more for productivity gains once you've got that base level of adoption and engagement for your engineers and so in this case we recommended they segment uh by the type of work they were looking at. The final one is business outcomes. And business outcomes are often the most misunderstood because this is really where your organization's culture and what you're trying to get out of AI matters the most because once you have people using it through adoption they're shipping more work more quickly. You're probably expecting some sort of change to occur in your company now when Joe talked about this a few minutes ago, he talked about Amazon's cost to serve model, right? So that's just one way to look at a business outcome is, are you able to produce software more cost effectively? That would count as an outcome here. Other outcomes might be things like R&D savings. They might be type of work you're delivering. Are you delivering more innovation work per unit time, etc. And so that's what this company was looking at as well. This company was very concerned with the amount of time they were spending on maintenance work, keeping the lights on work, bug fixing work, and so the entire goal of driving AI and driving adoption was less about cost for this company and more about could they produce more innovation, roadmap, and growth features per engineer than they could before. And so in this case they had an improvement there of 37%, which was awesome ahead of the trend. Interestingly, they wanted to do that through using AI to fix bugs. Now they were doing that, but they actually trailed the industry trend here uh by a little bit, so they were succeeding at their top line goal, uh, despite that intermediate goal not not being met. So here there's an opportunity to find out why were they succeeding despite the initial strategy not actually playing out the way they expected it to. So this was good news. They could then investigate that and find out if they could actually go faster, uh, with the benefit here. So before I pass it off to Craig, um, one thing I wanna note here at the end is, you know, Joe talked about how we went from, you know, autocomplete to code assist now to an agenttic world, and I think, you know, the early data suggests that there's a lot of talk about agents, but the actual adoption of those agents is not quite there where the, the code assist tools might be yet, but it's probably coming. But the point is that we actually don't know what mode the AI tools will take over the next 12 months, let alone the next, you know, 24 or 36 months. So the framework that we've presented. We really intend to be sort of mode agnostic, right? It doesn't matter if you're using code complete or chat or agents or augmentation. If you're not adopting those tools, seeing increased productivity and then seeing some type of business result on the other end, so regardless of what tool you adopt, what mode it works in, we recommend some type of framework like this to help measure progress and drive impact. So with that I'm gonna pass off to Craig to take you through the journey at Genesis. Thanks, man. the Hi, my name is Craig Dollinger. Um, I'm the senior director of platform engineering at Genesis, so I'm gonna walk you through, um. Basically take you through what our journey was when we first adopted Q, um, then I'm gonna show you how we scaled all of it out with as far as jellyfish with our reporting metrics and then along the way I'm gonna share the lessons we learned, um, and how we achieved where we're headed and kind of makeshift reporting we did in the beginning and why we're using jellyfish now. So when we first brought Q on, um, our goal was simple. We wanted to give developers a boost in their productivity and their velocity. Um, we also wanted a way to chip away at some of the technical debt we had. We wanted to take care of some of the SDK migrations going from AWS SDK V1 to V2, even some deprecation of, uh, lambda run time, stuff like that. Um, so we started getting some unexpected results where we started getting better documentation and repos. We started getting much stronger opportunities for our unit testings, so Q was being adopted. Developers were happy with it. Um, but the challenge was we didn't understand how it was being used or who was using it and who is kind of the power users or the idle users, um, so the gap in visibility was basically the problem we need to solve. How, how can I tell my, my devs or the teams, the teams are actually engaged in cu and getting the most out of it. So We realized there was no clear way um to understand how they're actually using it. We started looking at the Q metrics. Um, we use AWS SSO for sign in, so the users were good with IDs. Um, we couldn't really tell how the engineers were engaged, so we kinda hacked together this kind of makeshift metrics, uh, database so we can tell our higher ups how Q is being used. Um, we basically ended up taking the data from Q, piping into Postgress, um, and then our internal teams, they started visualizing everything in Quicksight. But we started realizing we can't scale that because every team we wanted a different view, so we had to make all different dashboards so it was becoming more and more of of a real pain to scale that out to everybody's different needs. So in the end we really knew this was a stopgap and we needed something basically automated enterprise level for us to actually do the reporting on cue and understand how our devs were using it. So one of the cue check-ins with Amazon, um, actually, actually Joe saying that we're partnering with Jellyfish. We already use Jellyfish for our Jira intake and seemed like a perfect opportunity for us to start using their AI dashboard. So we joined the program we're able to basically retire that postgrad stuff and move the metrics right over to the jellyfish AI. Um, so the impact right now we have, it consumes all the queue data and basically give us our leadership in near real-time stats on the queue. And since they're already involved in our Jira taxonomy information, we're able to roll everything up to our team service and actually get the Jira taxonomy. So as far as our leadership groups, they're actually able to understand how Q is being used by their teams, by their product taxonomy. So it was. Help solve a lot of the questions that were being asked. Um, once we actually connected, we basically were able to see who our power users were, who our idols, um, it sparked a lot of conversation with different team members how they wanted to use jellyfish, I mean how they wanted to use Q. And People who were really not adopting it that fast were able to engage in conversation, understand why they weren't so. Basically also answered a lot, probably 90% of the questions that were coming to my team we were able to answer with the dashboard. And something happened that was really important was it sparked a lot of conversation with teams basically trying to see see how AI was being amplifying their work so it's not a replacement, it's basically amplifying the developer. And engagement and then basically the engagement with the team's leads and the developers started growing from there and when they were able to push Amazon Que to adopt it faster. So looking ahead, um, we basically embedded AI across our entire software development life cycle, um. AWS and Jellyfish, they ensure our consistent unified metrics across all our tools, so it's one stop shop for dashboards. It reduces the time that the developers spend on the mundane tasks, and I usually tell my team members too, like we're not replacing developers, we're amplifying them so it's just another tool in their tool belt to make them better at creating the features that they wanna create, um, and help solve the hard problems moving forward. So what you're seeing there is basically how AI AI metrics helps us qualify the real productivity gains, things like faster co-reviews, faster PRs. The improvements are theoretical. They're visible. We can actually see the teams making progress, and it gave us something we've been missing for a long time is clear proof that AI was helping individuals. And if it wasn't helping individuals, we knew we had to engage in the conversations to understand why it wasn't helping them. Maybe it was prompt design. Maybe it was, maybe they just didn't wanna use it, so. The point is the metrics aren't black and white. It engages you in conversation with the devs. Later on, uh, Amazon Kiiro came out. We adopted that right away. Um, that took off. Uh, everyone in the team really liked it. They integrated right into VS code, introduced spec-based program and vibe coding, so two modes. Um, we started seeing teams like whip out really cool utilities and tools, um, but it really changed the developers and how they thought about building stuff with AI. It was actually with spec-based programs more structured and intentional, um, developers came more efficient and productive. And we make a point of score here the same way we describe all all of our other AI tools. Um, it's an amplifier. It's not a replacement. It's not just an assistant, but we tell the devs just another tool in your tool belt to make you better. We're still waiting on some of the kiro metrics. Um, once we consume those, we'll work with jellyfish, and jellyfish will consume those kiro metrics. So we'll have actually cue usage and cure usage in one place. So makes life a lot easier and everything also be broken down the same way as that for our team taxonomy and everything, um, so one unified dashboard, one single space for usage adoption, your return on investment across the whole platform. Um, for us it meant teams can choose the AI tools they wanna work with and the leads and directs can actually get go to one dashboard for reporting to see how they're actually developers using the tools. So internally, Using Q and developercuro for us it's a journey. Um, AI is helping us build a culture internally with creativity at the forefront and speed and basically we wanna, we wanna move forward from experimentation to basically real measurable impact within the groups. And I'm gonna hand it back to Joe now. Thank you, Craig. So what are some, some key takeaways, uh, from this conversation? Some things I hope you'll, you'll take away from it. The first one is measuring AI as a journey, right? You've, you've heard me describe it. You've heard Craig and, and Krishna will describe this. So baseline what you have. Pick a framework and start right. There's no, it's not gonna get any better. Just, just go work with what you have. And then think about this idea of leading and lagging indicators, right? A lot of the finance folks are gonna want the lagging indicators, right, the ROI improvements, all of these things, but what are the leading indicators that would make sense in your organization to help you move you forward? Map your usage to your internal taxonomy. I think one of the most interesting things that I found working with folks like uh with working with jellyfish is the ability to overlay the usage across both organizational and product and feature taxonomies. It's been really interesting to be able to see how adoption and engagement works, right? Impact is not gonna be universal and it's not gonna be consistent, so being able to explain to. The execs, like, here's why this team appears to be lower, here's why this team appears to be faster, and what are the lessons are incredibly important in those kind of conversations. And then as Craig talked about, think about your organization. Having the good data enables you to help leadership understand what does and does not matter, what is and is not relevant. And finally have a very clear vision and a clear message. So with that, thank you very much for your time. If you want to learn more, uh, we have a chalk talk session tomorrow, um, uh, which is myself and a colleague where we're, you know, we're gonna be able to talk about all these various things more. And if you want to learn more about cost to serve, there is a session tomorrow, DVT 207, which is a session like this where my colleagues from ASBX will go super deep into how we think about measure and manage, um, our cost to serve framework. So with that. Thank you very much. Please enjoy reinvent.