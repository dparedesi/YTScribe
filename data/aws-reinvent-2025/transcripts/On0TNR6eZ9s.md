---
video_id: On0TNR6eZ9s
video_url: https://www.youtube.com/watch?v=On0TNR6eZ9s
is_generated: False
is_translatable: True
---

In the last year, I've been approached by people asking about their AI costs that I wasn't prepared to answer. For some of you in the audience, this might be a CFO asking about the ROI on model training. Maybe an engineer asking for budget approval for GPUs. Or a board member asking about the cost of your of your AI strategy. Whoever, whatever the question is, hopefully you'll be happy to hear you're not alone if you're feeling unprepared. My name is Katherine Graham. I lead the optics team. It's a FinOps team with an AWS. Everybody, uh, my name's Chris Hennessy, and I'm on the, um, uh, executive in residence team. Uh, I'm, uh, really excited to be here today and prior to, uh, being on that team for the last 5 years, I was a technology CFO at Capital One for 20 years, so excited to share insight from that experience and from the content we created here today. And today we're gonna talk to you about AI for FinOps and FinOps for AI in the hopes to educate and enable you. And in preparation for today's presentation, Chris and I spoke with over a dozen leaders from different companies about their AI and FinOps journeys. And in doing so, I shockingly found parallels to my personal life. So I wanna start with a quick personal story, and then we'll get started. So 4 years ago, my husband and I were planning for our 2nd kid, and some of you in the audience may recognize this misplaced confidence, but we were going in thinking, OK, hey, we've done this before. How hard can it be? Can't be that different. Fast forward, about 2 weeks after my son was born, we found out he was deaf. All of a sudden, information is flying at us at a rapid speed. Lots of unknowns, lots of uncertainty. To say we were overwhelmed. is an understatement But we had people in our corner, subject matter experts, a community, helping us with the transition and helping us make decisions. Why am I sharing this with you? And what on earth does my son being born deaf have to do with AI or Fin ops? It was in the conversations that we had with these leaders that we saw not just a parallel into expecting the unexpected or those unknowns that I mentioned, but in the advice that leaders were giving to people just starting this journey. So I want to share 4 of those with you, and then we'll dive in. So first, You're not alone. You may feel like you are, you're not. Second, focus on the real problem. I was focusing on the fact that my son was deaf. That wasn't the problem. The problem was communication. If you can focus on real problems that your customers are having, or real pain points your teams are experiencing, you will deliver value. Don't overcomplicate it. Fanops 101. Cloud financial management basics, those principles still apply. The variables are different. And then last, never stop learning. I thought we had the proper tools in place for kid #2, and we had a decent foundation. But we still had to learn new and different things. We had to learn a completely different language. And the same thing is true here. And this isn't just parenting advice, but our hope today after you leave, if that CFO board member, engineer comes up and asks you about AI costs, you'll have a better answer. So let's get started. Great. And as we look at the elements of what we're gonna cover in terms of the perfect storm, there's a convergence of activity happening. I think all of us see how much cloud growth is occurring. If you look at any cloud provider, it's in that 20 to 40% per year, but AI growth is at a much larger clip, uh, that we're seeing, and that's across all of the ecosystems and all of the players. The other thing as we see as we go through this, uh, uh, specifics are, um, there's a lot that's changing, uh, and new nomenclature and, and dynamics you need to learn as you go through this, specifically on the AI side. So understanding some of the terms, the dynamics, and how that fits into the FinOps practice is something we're gonna cover here today. And then lastly you need to focus on both sides of the equation. This isn't all about cost for costs sake. It's about the value articulation, and that's an area that we see as a blind spot and came up with a lot of our customer conversations that we had. So today we're gonna look at both sides. It's almost two sides of the coin. One is how can you leverage AI to enable FinOs, and then specifically how do you bring FinOops into AI and the capability that exists there. So as we start out there's this convergence that happens uh uh in terms of this, and there's a good overlap that exists and there's 3 key areas we saw as we had conversations with customers. The first is on intelligence. I think a lot of us have leveraged AI in both your personal and professional life as a way to help with interaction and prompting and engaging. It's a way to provide answers and insights, uh, to the information that exists. Automation is a big deal, uh, within FinOps. I know most of the companies that I meet with. The FinOps team is usually 1 to 3 people, even in large enterprises. It's really trying to scale through the engineering community and help them self, uh, self-sustain. So automation is a key element of that. And then lastly it's all about, uh, getting scale and democratizing this to the organization. A lot of the use cases we're gonna share that we learned from customers, it's finding ways to scale and get more reach and benefit and insight to those, uh, who need it inside of the organization. And one thing that's important to remember is you don't need all three on day one or even in year one to add value. The importance is remembering that the value lives in the intersection. And that's when AI stops being just an experiment and starts being a business enabler. For FinOps, it starts stops being about just cost control and starts being about proactive optimization. So just to set up the next 45-ish minutes, we're gonna start with AI for FinOps. How are we using AI to transform cloud financial management? What are the challenges? How are leaders approaching it? What's actually working? Then we're gonna jump to FinOps for AI. How do you budget for something that's constantly changing? How do you determine a return on investment, or build a strategy? Then we'll wrap with tips. Advice from leaders and some educational recommendations. But before we do that, can everybody get out their phone? I can't believe I'm actually saying that we're gonna, we're gonna do a roll the dice analogy on this. So you know, when you bring a survey out, it could either go well or not. So we're hoping it goes well. So we'd love to get insight for you all to get perspective from those in the room. So the first question you'll hear and you'll see up there is where is, uh, your organization in the AI journey? There's 4 options. You can only vote once, um, um, but would love to get your insight, and we're gonna share the results here in a minute so everyone can see. I'm gonna keep it up there just for a sec, and then there's another one. Where do you think most organizations are in their AI journey? And as you're doing this, I just want to mention, several months ago when Chris and I started planning for this presentation, we decided from the beginning that we wanted this to not just be cloud agnostic, but to reflect the reality of what our customers are going through and to include action items that y'all can take as soon as possible. And so that's why we have these conversations, to check our bias and hopefully what we learned and share with you today is helpful. So let's see if this works. We're gonna see if it works. Before I do, I'd love maybe your estimate on the first question if you could go back. I know people are taking pictures. What do you think? What do you think the predominant answer is? I mean, originally, before, not to give the answer away, not that there's a right answer. I thought it would be piloting specific use cases. Did you think something different? I thought it was the same as well, but we're gonna try the toggle. Here are the answers. Let's see if it works. All right. Almost 300 of you responded, and it is a little bit leaning more on the lower end of the piloting specific use cases takes the, the majority there. I'm like smart group. That's what I thought too. Alright, you want to go back to the second question? If anybody hasn't answered, feel free to. Answer that. The fact that it's been smooth is nice. The polls so far, knock on wood. Did the second survey work for everyone? Yes, it did. I see it now. All right, it wasn't on there. Um. Little less participation maybe the two QR codes we only have one back to back. So, uh, on this one, let's look at the results here. Where do you think most are and it's in the same wheelhouse, similar, uh, vein in that same little bit more, a little bit skewed to the left, yeah. All right. Perfect. And is any good, um, uh, CFO, uh, you'll see throughout the presentation, I love data, data to help inform, uh, information. So as you look at this chart, um, and again we're specifically focusing on AI for FinOps here on the front end, what do you all think? Any guesses on what this chart may represent? Or like Anyone? So How many orgs are doing, yeah, AI fin-ups, it's a very good estimate, good forecast there, uh, so as you look at it, um, of all the companies we talked to, only about 10% were at scale of using AI for finups. All the examples that we're gonna talk through in the use cases, it's very early in the journey, um, so, uh, when, uh, that you're not alone. Analogy you gave earlier, I think that's where a lot of customers are. I know I meet with hundreds of customers in a year, and a lot of them are looking for use cases, proof points. They wanna see and understand where do they actually see value for AI's sake versus maybe just having to feel like you have to do AI and everything, uh, which is I know what a lot of customers feel. So the first topic I'm gonna talk about is when not to use AI and specifically Gen AI. Um, I think I know there's a, a big, a lot of pressure in the ecosystem of AI, AI everywhere, but I think when we met with customers, it was very clear, as you think about FinOs, there's a lot better solutions and especially more cost effective solutions on certain ways of leveraging capability, especially in forecasting. Uh, I know many of you probably done forecasting to your, uh, to your, uh, in your experience, um, and there's. Better machine learning capabilities, other analytical capabilities that you can balance. So we thought it was important to frame out, uh, one thing that popped in the conversations is what are some of the use cases of when not to use, uh, AI, um, and as you look at this, especially as you think about the financial side, um, the difference between probabilistic with generative AI and deterministic drives a big difference in terms of what you can use and when you can use it. And, and this decision tree just gives a little bit of an insight of what you should use when you need it. Um, and we're gonna dive a little bit more specifics into what some of the customers shared, uh, on this slide as well. The next one, as you look at some of the capabilities that are here, uh, that exist in some of the use cases, forecasting is obviously a great one. I know I've spent some time recently trying to leverage AI on the, on the generative AI side to help predict patterns. I think Agenic will do a better job of this than generative, but I know, uh, a lot of great services exist both within Amazon and within partners that leverage more traditional analytics. And do a really good job in that. You also need to balance as we'll go through, there's a big balance we have in putting this together around what's the value expected versus the cost that's informed. So that framing is a really important framing as you go through this. So we just wanted to make sure and start out the conversation what the answer for everything is an AI. Uh, there's different solutions that can be applied to this, and it's something you need to keep in mind as you go through it. So the reality is, is that there are challenges, and these aren't all encompassing, but what I wanna do is touch on a few that made a repeat appearance. So we've got the data quality and preparation. This came up in every single one of our conversations. This is the hard part. Not AI, not the tools. And most customers are doing this, but they're doing it retroactively, which can be painful and expensive. Then we've got multi-cloud. Most of the organizations are running multi-cloud. And if you're doing that, you've got different billing constructs, different terminology. And in order for AI to be useful, you have to normalize that data into a common framework. Then we have what I call the trust gap, hallucinations, AI reliability. We had one leader mention to us that their system recommended a $12 million savings by buying document DBRIs. It sounds good But those don't exist currently. AI has the ability to increase productivity 10 + X, but if we're not careful, it can make more work or worse cause distress for our customers or our teams. Skill shortage. Most people aren't great at writing prompts. And this doesn't even begin to touch on the technical gap that compounds this challenge. And then integration, the people, the processes, the systems, the operational friction points that can add up and demotivate people from using the systems that may actually help them. So I'm sharing these to let you know that they're universal, not to discourage, but let's see how leaders are approaching it. We had a lot of conversations, but these are the four main themes that came out. Build the foundation, all about the data. By proven solutions focused on the tooling. Enable with guardrails, the culture, and then the process, experimentation and iterating. Let's dive in a little bit deeper on what these look like. So build the foundation, data first. This is fundamental. This is what I call table stakes. No amount of AI can compensate for a poor data foundation, and you'll hear me say that multiple times today. Buy proven solutions. There was a time when buy versus build was a real debate. But it seems that in AI for Finops, it's mostly resolved and it's by. The landscape is moving so fast. One interesting thing that we did find out is that customers' commitment timelines, those are changing from annual to quarterly or even monthly, so the customers can keep up with that fast-paced change. The next one we saw as a trend was around the culture um around enablement, um, uh, this notion of experimentation and freedom to test and and have fun and and see how you can guide within this so and ensure there's guardrails that exist. I know for recently I was in Mexico City and uh I, I meet with a lot of banks just given my banking background and they're obviously very worried as most are around security and data, uh, especially their customer information. They don't want that being trained on LLMs. They wanted to make sure they retain that in-house. And while they were building the foundation of the elements of building out LLMs to be available for the workforce, they were pride. They had a lot of pride of not having access for their, uh, uh, internal associates to LLMs, um, and as they went through that, I was like, oh, OK. Have you set that expectation for your workforce, um, and they're like, oh yeah, it's very clear, and I literally walk out of the conference room and someone's on chat GPT putting company information into a public chat GPT and I'm like cringing, so you have to have the guardrails in place and the expectations. That doesn't mean every associate can do whatever they want. You have to be clear around those guardrails and the culture that you wanna create within that. And the last one is around learning first, um, this notion of experimentation, uh, and, uh, starting small, we get a lot of questions I know in, in the job, uh, that, that we're in around how do you get started, um, how can you even begin to experiment, uh, using AI for FinOs, and I think the whole mantra we all have heard, which is start small, think big, is a good mantra here. I think I always try to focus on a pain point. In an area where there's an opportunity, varying ways, and I'm sure you all have heard many of these, uh, how do you have promptathons specifically locally for people to learn about that? How do you encourage peer to peer connects with maybe someone who's more advanced and less advanced? How do you create the environment and the time? It's usually about time trade-offs that come up with customers of being able to learn and experiment as a part of using this. So the first case study, uh, we're gonna do 3 or 4 of these, uh, was actually in a large bank and how they were driving through their AI, uh, evolution. And the challenge they were having just like many of us is you needed to have a little bit of deeper knowledge to be able to answer the questions from the product owners and the engineering community, um, and some of the harder questions took a little bit more time and there was a very small level of capacity of analysts who can actually support this so they were looking at a way can AI really help, uh, uh, build this and. Uh, and solve this, and they really focused first on one, could they have a Gen AI kind of chatbot solution on top of the data to enable people to self-serve. Um, they knew they wouldn't answer every question, but they were hoping if they could pair that data along with historical analysis they've done, um, using, uh, RAG as a way to bring that insight to bear. And what they saw when they started creating it, it caught on like wildfire. They had a 20 times increase in reach. Specifically around the adoption of the tool they were able to enable people to self-serve as a part of it and it was a very small level of investment to build this out and be able to uh see this and the customer quote was, this isn't about just giving people more data, it's trying to provide answers and insights to them to enable this. So this is one of the use cases that we saw from a customer interview that we were really excited to see it was being put to use, uh, especially for the group that was in scale. So let's look at the value chain and what it looks like in real life. So it starts with the raw data and metadata. This is not just your cost and usage reports, this is context. Business events, seasonal patterns, org structure. Then you've got your data normalization and quality. This is when you're translating across clouds. This is invisible to the end user, but critical. Then you have AI processing and analysis. This is when the machine learning is actually doing the work. Insights and human review. Do you want to be confident in the decision or the action? Because if you do, you want to get explainability in there or human review. If you've got a system that says, hey, Catherine, let's right size 500 instances. OK, why? What's the risk if I do, what's the risk if I don't? Is confidence high, is it low? If it's low, let's get a human in there. If it's high, let's kick it over to the developer's queue for approval. Then you have automated and semi-automated actions. I say semi-automated because even the most mature organizations are keeping humans in the loop. And then last, outcome measurement and learning. Did optimization work? Did it save costs? Was performance stable? You want to feed what you learned back into the system, so AI gets smarter with time. And you'll notice it's a loop and not a straight line, because the actions should train the system and the outcomes should continuously refine the model. So there were 3 themes that popped out. I'll go ahead and put them all up, um, that in the conversations, and it's from most popular to early stages of development, uh, on the other side. The, the no-brainers are the conversational interfaces. These are the, the, the, the use cases that probably you've seen grow in the last year as Gen AI has been fully adopted. How do I use our internal information along with leveraging a large language model to enable capabilities and reach that exists, um, and there's a good, a lot of good examples, uh, of customer questions that exist there. The middle is where things are evolving and it's growing much faster, which is how can I have either either potentially an agent work on my behalf or escalate and elevate thing exceptions. Um, there's a lot of great anomaly detection solutions. AWS has them, um, but this is a way in which you can start internally leveraging this on other data sets just beyond your cloud and usage data. And then last is everyone's nirvana, which is how can I automate some of this? How can I enable potentially agentic agents to do this on my behalf? I know I've been a little close to how this is evolving both in the startup and the large uh company ecosystem, and it's coming quickly. I think the pace of change is happening. So this was an area we didn't see a lot of in the customers we met with, but there's a lot of talks of the development that they're doing as a part of this, uh, and where they're beginning to focus. So, uh, one SAS company we met with, uh, to do the next case study, um, specifically was beginning to narrow in on this, and again slightly similar to the first one, not a lot of people, but a lot of demands on their team, and I know many of you feel that same way, and, uh. You'll see the last one. Engineers avoided FinOps. So I, I led a FinOps team and started a FinOps team for a long time, and it's a hard team to be on because not that engineers don't wanna do the right thing, but there's a lot on engineers' plates. And when you're trying to take their capacity away from developing features and asking them to optimize, there has to be a really good incentive for them to do that, to, to focus on that. So in this case that was true for this SA company. The engineers were trying to avoid the FinnHub's team, um, so, uh, as you think about the solution that they built, um. It's not going for me, but maybe for you. Now It was going too smoothly. Yeah, yeah, the survey I thought would take us down, so I didn't think the, uh, let me see there I'll just did page down that worked all right. So they, uh, the other big thing I know I talk to customers a lot is you don't want to create a bunch of dashboards and expect, expect people to look at them. You need to bring information to where engineers work, um, and I know. So, uh, uh, for me in the past, big areas were both GitHub and Slack, those were areas where a lot of engineers are spending time. So how do I bring financial data there? And this SA company did the same. They integrated with Slack in terms of notification capabilities and natural language queries directly within Slack that enabled them to deliver this information. Um, and the results were incredible. Uh, they had a 100 times increase in the user community of relative to what was being reviewed before to what was then being accessed and leveraged then. They could monitor the conversations and the questions, look for key themes and patterns of those questions, and they'd elevate FAQs and other things as a part of it, and it was really driving a cultural change inside of the organization and the quote from the customer, as you read through it, um, um, it's take data where engineers are. That was the big takeaway for them, which is that was the breakthrough when they started to see the change specifically there. Um, one question we get a lot, and this is just to give a timeline for the same company, they started out in almost like a pilot kind of agile sprint kind of construct within this. This is the timeline they leveraged in terms of building out the capability. They focused on 10 development teams as a start to really work out the kinks. And then scaled it to over 500 users inside of the organization. So, um, as you all I know, are experiencing the pace of change with AI and LLMs is moving quickly. It's also true with how people are leveraging it inside the company and trying to bring those mindsets to to kind of fail fast and move quickly as a part of it. So next up is another survey question. So, uh, if you, uh, if you pull out your phone again, um, uh, I would love to hear from you on this one, which is what's preventing faster AI adoption in your fin ops practice. Uh, there's 4 options. Uh, choose one, but if you don't mind sharing your insight, um, we don't wanna taint the audience, but I know we, we talked a little bit before about what we thought this would be, and I don't wanna give what I thought it was gonna be one thing. I was pretty confident. And through these conversations I learned I was wrong, very wrong, um. But it was humbling to learn. Great. Got some great results here. Uh, pull it up. There's only one more, one more, I believe, yeah. All right. And the results are, it's pretty evenly distributed. Expertise is probably the one that's what I thought it was, so I'm glad I'm aligned with the audience, you know. OK, let's see if this works. Hold on, is mine working again? It is. OK, so while we wrap the first section, if you're going to remember anything, let it be these three things. So data quality over everything. No amount of AI can compensate for that poor data foundation. Trust but verify. Use AI to do 80% of the heavy lifting, but keep humans in the loop. And then problems not wish lists. Focus on something that changes behavior or saves time, money. If it doesn't, delay it. All right. So now that we've talked about how we use AI to manage costs, let's talk about managing the costs of AI itself. So I wanna start with a quote from one of the leaders we spoke to because I feel like it frames up this topic nicely, so I'm just gonna give. A minute for everyone to read. So I like this for multiple reasons, but I want to call out two specific ones. First, it's something that I mentioned at the beginning, reminding us to not overcomplicate things. And then the second thing, which was our biggest takeaway in our conversations, is that AI is not that different. And as you balance kind of the notion of traditional cloud, uh, and the, uh, AI workloads, there's some differences, and I know this comes up pretty actively as we talk to customers, um, um, some may laugh about predictable patterns with cloud. I know I get a lot of questions of how do I forecast cloud, how would I expect it, but relative to AI it's much spikier in terms of consumption within AI than it is within the cloud. Um, as you read through the rest, there's a little bit more of, I think it's just a little bit more maturity and knowledge on the traditional cloud side, and it's new on the AI side. Um, I do think you've called out the one which was how do I make common nomenclature across providers that I'm leveraging that definitely has come up in our customer conversations, um, but I think the key is the principles are the same and that was true in every conversation we had, which is although there's different terms, although there's a different dynamic, it was clear that we had the mechanisms in place between how do we hold leaders accountable, um, how do we plan for these elements while it's a little bit different, um, it was very similar in terms of the optimization, uh, structure itself. And uh and what better way than the the iceberg analogy yeah and you know you don't wanna miss make the mistake and only budget for what you can see. You also wanna like plan and consider everything below the water line and it's the data preparation that I've been mentioning integration, fine tuning, training. Yeah, and the hard thing about the things on the on the bottom is it's taking existing capacity to do that. So a lot of times customers are saying, oh, I can plan for the top because I can point to it, but if you look at the bottom, it's around redirecting your capacity of your teams to do this work. So that takes prioritization and trade-off. Which is not an easy thing to do and I know a lot of customers are focusing on that. So just make sure as you go through this you don't ignore that just because it may be viewed as a sunk cost of capacity internally, it is capacity that could be directed elsewhere. So make sure you're considering that as you go through, uh, the focus. And this is a high-level simplified checklist, but we want to run through it. So budget for about 30 to 40% for data preparation. Your FinOps team capacity, if you're scaling AI, you're probably gonna want to scale your FinOps team as well. Budget and plan for governance, integration, training, about 20 to 30% for experimentation and failure, and then org change management. But once you've planned for all that, what next? GPU capacity. This is something we've all seen or heard and or experienced it. And it is a big ticket item, big light item. And it's a big challenge to manage. Yep, so another case study we did was with a global bank. Uh, they had a pretty good analysis and a framework to leverage on making trade-offs that were necessary around that GPU. We thought we'd share, we'd share with you. So the challenge is, as you know, it's scarce capacity, uh, at least, uh, for in some instances, and a lot of times even when you allocate that capacity, it's unclear of how it's actually being leveraged, especially inside of the organization. Um, in this one it was a highly federated organization, so every business had kind of freedom and rights to do what they want. They just allocated capacity in the aggregate, and once you do that you kind of lose insight and visibility into how it's being leveraged. So they decided to build a framework and an approach to help solve that. So they, this got very detailed. I know when we were reviewing with them they had 19. Unique GPU metrics, uh, that they were looking at around utilization and the specifics of that they were trying to aggregate those to understand where the patterns and utilization was, um, but the big, the big element was how do we give visibility around the usage patterns and elevate where that was being done to make sure it was delivering the most value to the organization. And they had a really good 2x2 which I loved, uh, uh, so I want to share with you as well, um, and there's nothing better than a good 2 by 2 to frame a conversation, but if you look at the bottom, it's usage on the, on the y axis, it's criticality, um, and there's some no-brainers, especially on the bottom left. If it's low utilization of a GPU and it's providing low criticality to the business, get rid of it, redirect that. Capacity elsewhere, uh, the top right, it's all about high value, high criticality there could be ways to optimize that, but I think thinking of how are, what are the use cases in your company, how are you leveraging this kind of scarce capacity that exists and is there some framing or approach you can put in place to ensure that you and your leaders are making the best decision possible for your customers? So now that we've planned for the budget, let's talk about recognizing ROI. Get out your phones one last time. What's preventing you from measuring your AI ROI today? This is another one that I felt like we had differing points of view on, but I'll be curious what the results are. The response rate has dwindled every single question. So, uh, uh, that's a one observation. So, uh, we won't read too much into it. Yeah, yeah, we won't it. Done with the surveys. All right, now we're back even over the last question. So, um. Great. And the results are, Scattered costs and no metrics is what, you know, no metrics number 1 and 2, but again fairly evenly distributed as well, uh, there. Perfect. So I'm gonna dive deep into this, um, because I know for me, um, I've spent, uh, I feel like I've spent my career creating business cases, and I know if any of you have done that, it's pretty easy to create a business case. It is very hard to track it after you are going live. Um, it's a mixed master of information, but I think from a framework standpoint, the nice thing, uh, about a kind of a traditional ROI analysis on the front end is there's costs investments. That are necessary. There's expected benefits as you go through them. You can tie things back pretty easily to a cause and effect for most things, not everything, but most things. And I think once you introduce AI into the mix, things change a little bit because there's a lot of 2nd and, uh, 3rd order impacts as you go through this, um, and I think, uh, there's a good example here, uh, which is, you know, an externally facing chatbot. When you, uh, and I'm not sure if many of you probably have, uh, your own experiences with certain providers, uh, and cust and consumer, uh, experiences when they may be add a capability and you can start getting answers quickly, you feel like it's better service, things are improving, that's awesome, that's really good. Brand perception is improving, customer satisfaction is improving. But as you think back internally, I meet with a lot of CFOs. Uh, one CFO said to me, I need numbers, not stories. Um, this isn't about feel good around this. I wanna know how that translates to either the P&L or to performance and metrics that are there. So I, I get a lot of questions specifically around how to measure this, and I think there's a couple of things that come as challenges. Um, one, we mentioned the iceberg, uh. Which is the denominator effect. There's tends to be a lot more cost that's involved than you're actually quantifying as you go through some of this on the numerator side, I know most organizations struggle with value articulation, um, and that can range from being a private enterprise where you're for profit to a government entity where it's mission driven. It's a totally different conversation of value creation. And then the last one is it's kind of a moving target in terms of timeline and expectations. So, um, uh, these are the three patterns that came up, uh, and have come up in customer conversation. The first one is, uh, the productivity paradox. Um, all of us have experienced this. Um, when you've leveraged AI to do something in an everyday task, it can provide a great experience and it can do things better and faster, which is awesome. That's great. But in the example here is on the Qveer, just because you can code faster, what does that mean? Does that mean better quality? Does that mean more features? What does that actually translate to in terms of customer value? That's one of the challenges that customers have. The second is a baseline absence. Um, I know I always, uh, I always love to ask the question to IT leaders, do you track time? And I, I know that's a thorn in everyone's side in terms of time tracking. Like, I don't need somebody looking after me and what's going on, but baseline creation is really important as you go into this. I spent also a lot of time on total cost of ownership of cloud or of IT. If you don't know where you're starting, it's very hard to know what's changed and how it's evolved. So, and as you think about this, there's a lot of activities we do that you just don't measure, um, and it almost doesn't make sense to measure in this regard, but that's where some of the challenges are coming was what was it before? What is it after, and what do I do with it. And then lastly is the diffusion of value that was one of the items that was called out there, which is there's almost this downstream impact of things that can be unearthed as you go through this, but how do you. Quantify some of these items we've always had struggle I know on the qua the qualitative side of if you're more secure, how do you translate that to an articulation of value like you could say we're more secure, but what does that actually translate to in terms of performance? Um, obviously a lot of this is reputation risk and a little things that are softer because they are mitigating events, but, uh, those are the things that come up as a challenge, uh, that, that's there. So there's a couple of things I've seen some patterns of what's working and what's not working. A lot of times people say if you save time you save money and I know for many one of us we've probably gotten more productive in the last year and a half years in certain ways, some ways maybe not as much, but in some ways we have. That doesn't mean you don't need us. Uh, it's still resources that are necessary. So just because you save time and the stories of I saved 1000 hours, most I know financially minded people are gonna be like that's 1000 hours. That's a lot of money. Where's that going? Uh, where's that being redirected to? So a lot of this is resource reallocation that exists in companies. The second is around revenue attribution. When you roll out a capability or a service, I know I, I work with a lot of ISV and SAS companies that are looking to build AI into a lot of their solutions now, so they're thinking about how do I price it? How do I assess this? What's the value that's expected through this. So just because you add that, it's very hard to attribute revenue only to that in isolation. Um, the third is around cost avoidance, the age-old, uh, which is if I didn't have this, I would have had to hire these people. I would have had to do this. Well, that's a good storyline in aggregate is sometimes it can feel like phantom money, uh, when you talk about it, um, but it's a real thing. So I think it's important as you go through this, I use this wisely as you specifically go through it. And lastly is around efficiency metrics. I'm, I'm a big believer. And metrics, um, and ratios, um, but as you go through this, be careful of the cumulative effect of all the ratios because if you add up all these ratios together it seems like you don't need a team or a resource or something else that's there. So if that's true, that's great, but if it's not, be really mindful of the words you use and the metrics you use as you go through it. So there's 3 patterns I've seen to be successful for companies, um, as they think about value articulation. One is something called a kind of a value driver, um, think of like a value tree, uh, which is there's primary measurable items. Lower cost, less losses, more revenue, that's very clear that's a traditional measure, but the secondary areas, uh, you need to make sure you're articulating as well. You may not have numbers behind all of them, but you need to have the, the qualitative assessment of that and not ignore that as you're talking about returns. Um, the second is around outcome based metrics. Uh, this is something that came up in a lot of the conversations we had, which is how do I think about what the outcomes of these activities are so that we are measuring that sometimes that in isolation won't tell you, but this in concert with the one on the value tree. Yeah, it will definitely help, but the one that's probably resonating the most is this portfolio approach, which is thinking like a VC firm. And as you know with a VC firm they make a lot of investments. I know there's always the sound bite out there, um, 9 out of 10 businesses fail, so that means you need to start 10 businesses because 1 will be successful. Um, it's the same construct here, not that 9 will fail with AI, but it is an element of you do need multiple avenues and things, stokes in the fire. Because some will work out and some won't work out, um, I just, I was at an event, uh, two weeks ago in New York around AI and finance, and someone speaking gave an example of where is the value coming from a lot of the AI projects and as you think about it, it's usually the more obscure, far reaching elements, not the basics that are delivering the most value. So you need to have a wide spectrum of areas you're investing in in the space as you're going through it, uh, in that regard. To summarize this section, if you're going to remember anything, let it be these things. So traditional cloud cost planning for that will leave you 3 to 5x short. So plan for that from the beginning. You can't control when or if AI features get adopted, so instead focus on cost per user, cost per transaction, or focus on optimizing model selection or token efficiency. And then don't let the fact that you can't measure ROI values stop you from investing in AI. Think outside the box. Think of those proxy metrics, time reduced, errors reduced, things of that nature. So now let's talk about building the strategy, putting it into action. I'm gonna start with AI for FinOps. So this follows the crawl walk run for anyone who's familiar with the FinnOps Foundation. So we've got phase one, it's the base camp, it's your foundation. This is when you're building the basics, you're tagging your data quality. What does success look like here? This is when your team can ask simple questions in natural language and get an accurate answer. Then you have the climb, the enablement. This is when you're scaling intelligence. This is when you have more conversational interfaces for more teams, and you've got those auto generated recommendations that are actually useful. Success here is your FinOps teams are no longer pulling reports, but they're doing that strategic, proactive optimization. And then the summit, automation. This is a mature state. This is self-healing optimization. This is when you've got predictive controls in place that are preventing waste proactively. Success here is you've got those auto-generated recommendations with autonomous execution. And those cost surprises are starting to become a thing of the past. I'm not going to spend a ton of time on this slide. Because we don't have a ton of time. We've got a pretty good amount of time, but, but before I hand it over to Chris, I just want to mention. You don't have to be at the summit. To get value. We spoke with a lot of companies who are recognizing ROI at base camp. And if base camp is where you are, you're where you need to be, you're where most of the market is. Yep, and as we think of very similar construct on the path, uh, for FinOs for AI as you think about the base camp, um, a lot of this is around, uh, visibility. Um, I know a lot of organizations are yearning for insight to how are we leveraging AI, how much are we investing in it, what are, where are the ways we're investing in it. It sounds very similar to what we just talked about before. You need to have the right level of perspective, uh, and insight of who's consuming it inside of the organization. Um, on the optimization, it's obviously all about model selection, uh, and, and a lot of choice. I know there's a lot of evolution moving from large language to more targeted small language models that are targeted. There's kind of three legs of the stool around, um, um, speed, um, accuracy, and cost, um, as you think about those three legs, and not every use case is the same, and there's a lot of levers you can pull to drive optimization, as you go through this. And then lastly on the summit is uh it's around uh that embedded governance, predictive elements and being able to plan capacity, especially on the GPU side in the right and effective way. Um, uh, we did include a bunch of the details as well, and I'll pull those up as you have them, uh, as you see them there, um, um, and it's a, it's an evolution you would expect, uh, as you go through this, which is, um, uh, what are the right metrics to look at, um, to assess this, how do I then evolve the right accountability model, um, and link this to business outcomes. Um, and then ultimately, uh, how do you have preventative planning and levers as you go through it? There's a lot of levers that exist to drive automation, I mean cost efficiency of using AI, um, uh, and, uh, and there's a lot of, uh, options, uh, that exist there. So taking advantage of those options is an important element as we go through it. OK, we're in the final stretch. So we're gonna do tips, then we're going to do educational recommendations and then advice from leaders, and that'll wrap it. So if you're gonna remember anything from today, it's these 5 things. You're not alone. 90% of organizations are still in the early phases. I know I'm a broken record with this, but data quality beats AI sophistication always. No amount of AI magic is gonna compensate for that poor data. Human judgment still matters. Use AI to accelerate, humans to validate. Solve for real problems. Don't deploy AI because it's cool or because everybody else is doing it. I know you mentioned it at the beginning. Deploy it because it solves a painful, expensive, or repetitive problem. And then, as I mentioned at the beginning, Never stop learning. And on that topic. From the leaders we spoke to, we started to just gather people that they follow, uh, podcasts that they listen to. We even got a query, this is from an individual on my team, Adam Richter, that he does almost every day, just to kind of stay up. With everything going on in AI, so take a look at these. Follow, I'll call out one of my favorites. Uh, there's a lot of greats one up there, but the AI Daily Brief, if anybody doesn't listen to that, they do a really good job as a podcast to highlight the the 5 minutes of insight that you need to learn of what's happened in AI that day. Then they go into a deeper dive element. So I know time is sensitive for everyone. So if you could devote 5 minutes, the AI Daily Brief under Listen is a must listen to from my vantage point if you want to stay informed of what's evolving and happening out there. Awesome. Great, so we'll, we'll wrap with some advice, uh, that we heard, and this, these are direct quotes from leaders that we met with as a part of it, um, in a part of the conversation. So, um, the, the first one is, uh, around the data side, uh, and the quote was, we thought we could kind of skip this and just go right to it. I know I get a lot of structured unstructured data questions. Um, how do I leverage this? I know the gold inside of companies is their own data. Um, it's not just the large language model, it's pairing that with the insights and the information you have in your organization and spending time and in integrating those two together is, is key. The second is, uh, around humans in the loop, um, make sure there's good fact checking, good frameworks. I loved your, uh, the loop you had specifically around the process that you can integrate on top of it. Um, third is, uh, it's all about painful problems and repetitive problems. Don't just assume a dashboard solves issues. You need to make sure it's integrated and being shared in a way that's actually solving, uh, customers' problems. Um, the whole buy verse build, I know there's a, there's kind of a. Hybrid answer which is you're likely gonna buy an LLM. Some companies here, many large enterprises may build a lot of their own. Most are buying, but it's integrating that with your own data is the key, and that's where you can have an in-house solution to integrate that together. And this is all about experimentation, um, and taking time to do that. And then lastly is, while the dirt words are different, don't overthink it. FinOps basics exists. I know, um, uh, I don't know what your viewpoint is, but, um, I would say FinOps has matured a lot over the last several years, but I still meet with a ton of customers that haven't gotten started yet in this regard as well. So I think learning the basics here, you're at a really good time, especially if you're maybe in the early stages, to be able to leapfrog and mature your practices and FinOs by using some of the AI capabilities that exists. And just to bring home the summit, uh, analogy, um, reading through this, the base camps where you prepare, the climb is where you grow, and the summit's where you see the view, but you can't do it standing still, so it's an element, it's really important to get started, uh, in this regard. Obviously AWS can help you. There's great partners out there that can help you. So if you're interested, I definitely would reach out to your account team to engage on this, um, uh, to learn more and to hear more about it. And then I just want to wrap. I was talking about my son at the beginning, and I share this because this is from like 23 weeks ago now, and I share it because he's thriving in my humble opinion. And I don't think we could have gotten him to this state without the subject matter experts, the community that have really helped us, and we want to be that for you. And as Chris mentioned, reach out to the account teams, engage us, let us help you. And then, of course, this is our QR codes for our LinkedIn if you want to connect. We will also stay outside for any questions after, and then please, please do the survey. Feedback is a gift and we want to continuously get better. So, with that being said, Happy reinvent. Thank you.