---
video_id: zmWCkoi-Qlg
video_url: https://www.youtube.com/watch?v=zmWCkoi-Qlg
is_generated: False
is_translatable: True
---

Good afternoon. And welcome to reinvent. Imagine tracking. A developing hurricane. In the Atlantic Ocean. Many lives at stake. As coastal communities prepare for the impact. Behind every forecast. Behind every evacuation order. Behind every decision that saves lives. There is massive amount of computational power that is at work. Today, along with my esteemed colleague, David Mishoff from National Oceanic and Atmospheric Administration, And Rayatullah Abdullah from AWS. We will explore. How high performance computing. Is evolving from traditional supercomputers. To AI-led forecasting. Transforming the way in which we predict, track, forecast dangerous weather events that impact our planet. The convergence of high performance computing. And artificial intelligence. It's an evolutionary technology. However, it is revolutionizing how quickly And how accurately we can respond to Very dangerous events. And therefore save more lives. We're excited to take you through this journey with us today. Our presentation is going to be organized as follows. First, let's talk about. What the market is doing in terms of this convergence of AI and high performance computing. According to Hyperion Research back in 2023, The HPC market was about $37 billion. And it grew it grew by 24% in 2024. We expect the numbers to be even higher in 2025. By 2028, 1 3rd of the HBC market. is expected to be on the cloud. And by 2029, the converged AI and HPC market is expected to be $49 billion. That is huge validation of the market by some of the early results that we're seeing. And also It's kind of following the innovations like agencies like NOAA are. Actually coming up with and executing with HPC at scale. So then I will invite David Misha at that time to come up to the stage, and he will do a deep dive into a NOAA case study. He'll talk about NOAA's mission. We'll talk about the important work that they do, how it impacts lives, and how massive amount of computing at scale is used at NOAA to do various mission outcomes. Then Raat tells Abdullah will come to the stage. As a principal architect at AWS she's been leading the capability development and service development for supercomputing and other cloud opportunities at NOAA. She will do a deep dive into these architecture capabilities that power NOAA's missions. And then finally, I will come back to the stage and I will discuss some of the new investments that AWS has committed in this space as well as give you a couple of additional examples of innovation that we're seeing in the converged HPC and AI space in the cloud by some of our other partners. So with that, I would be honored to have David Michaw join us on the stage. Thank you. All right, good afternoon, everyone. My name's uh Dave Michaud, and, uh, and I work for the uh National Weather Service. Uh, my role at the National Weather Service is, um, essentially, uh, essentially responsible for managing the, uh, the operations of the, the networking that we have, uh, enterprise-wide. Um, all the supercomputing, all the data dissemination systems, and, uh, also, uh, support locally for all the, uh, national centers that we have in, uh, in College Park, Maryland. Um, I've been with NOAA for about, uh, 30 years. It's my life and my passion, so I'm very excited to be here today to talk to you about, um, what we do. So first a little bit about the mission of the National Weather Service. So, um, often times when I talk to people about, uh, the National Weather Service they don't really realize the diversity of what we do um a lot of people think about, uh, OK, what do I need to wear the following day? Do I need to bring an umbrella? um, we're much more than that um in the National Weather Service we have a very diverse mission. um, our, our domain space covers anywhere from. Um, the entire Atlantic basin to the mainland US to the entire Pacific Basin in the northern hemisphere in terms of the domain and scale of what we're forecasting, um, and, uh, the, the bread is, is, uh, astounding, so we do everything from the, the surface of the sun to the bottom of the sea. So, uh, we'll monitor the activity on the surface of the sun. And, uh, forecast, uh, electromagnetic, um, interruptions or storms that are produced through coronal mass ejections and monitor those those as they approach the Earth, um, again we have the traditional forecasting, but then we also have, um, like Mickey mentioned, uh, the tropical forecasting, uh, with the National Hurricane Center and, and watching those storms, um, aviation weather, um, again the entire Atlantic and Pacific basin. When you think of aviation. You're we're producing data that's helpful towards having the airlines calculate flight efficiency and how much fuel to put on planes. Every pilot needs to have a certified weather brief before they take off, and all of our data and forecasts feed those sorts of mechanisms. We have tsunami warnings and we monitor seismic activity and those impacts on the. The surface of the of the ocean and uh what impacts those would have as they approach land um and uh fire weather we have actual meteorologists that uh reside um and are embedded with the fire chiefs at the site of uh uh forest fires and wildfires um so there's many more pieces um that I I didn't hit out and one last piece the ocean Prediction Center so. When you think of the Ocean Prediction Center, um, think of all the economic impact of all the vessel traffic traveling across the ocean and these massive storms that come, uh, creates the need for ship avoidance to to uh circumvent or move around the storm and so we're really focused on life safety and the economy there. So we operate across the United States, uh, anywhere from America, Samoa to, uh, Hawaii to Guam to Alaska, uh, Puerto Rico, the mainland US, um, we're, we're, we're really embedded in a large number of communities across the United States. Um, we have 122 forecast offices, those are probably what you would traditionally, uh, see as or. Recognize as like a weather forecast office, but we also are embedded with the FAA and air traffic control centers providing some decision support um we have uh uh forecast uh river crest and uh the the river rise and falls and flooding. We have specialty national centers that are looking at a unique kind of niche type of weather, um, and then, uh, uh, a water center and looking at, uh, the hydrology so. We're really geared to be out in the community um with the the people who need to make those decisions. So today we're gonna talk about high performance computing and the associated workloads and um how we generate a large amount of uh forecast information that helps with the forecasters um producing the forecast and the forecast process but I don't want you to lose sight from a mission perspective that that's the only thing that we do. Forecasts are one piece of what we produce. But we've really been focused over the past decade or so on making sure that we're working with decision makers in the community to understand what the impact of the weather is going to be so what do I mean by that? Um, so give you an example, right? um, I'm from, uh, Washington DC. If we have the slightest bit of snow that wreaks havoc on traffic and that's a massive impact, right? Um, where I grew up in New Hampshire we could have probably 1 ft of snow and people would still be out moving around, uh, with ease, right? So, so impact wise, uh, different places have different tolerances for different weather and different weather types of, uh, of events, so we're really focused on delivering the forecast and making sure that we're providing a risk assessment. To those decision makers in the community that are um providing um those life saving sort of direction to the community so like evacuating for a hurricane or closing highways and and so on and so forth so we're really based um. What what we're really looking to do is create that trust relationship with the community decision makers so that when things happen um we don't have to learn who we're working with we have that trust relationship so that we can just move on the dime and uh and get those warnings out. Right, so high performance computing so let's talk a little bit about that, um, so in, in my career when I look at this, what, what gets me excited about high performance computing is it's one of those areas where any sort of improvement in high performance computing or technology with high performance computing directly correlates to improvement in the mission performance. So we see improvements in high performance computing capacity or the way our software works and the efficiency on a high performance computing that results in better uh advancements, uh, more accurate forecast, higher integrity science that's going out to our forecasters to allow them to perform the mission better so really that direct correlation in. Improving HPC with improving the forecast, which then improves those trust relationships. So let's talk a little bit. We talked about the mission, right, and the diversity in the mission. The way in which we operate within the mission provides certain constraints for the requirements and how we operate the HPC and how we think about our workloads and the HPC environment. One of these metrics that we we chase is product timeliness. So on a daily basis we have a cycle of models that run. Um, and they run everywhere from every 1 hour to every 6 hours, um, and we'll, I'll show you in a bit what the diversity of that workload looks like, um, but it's very, uh, time driven and cyclical in nature, so you can imagine for a, a landfalling hurricane there's a certain cadence where our forecasters have to interact with decision makers and those decision makers are looking for updates. And people in the public are looking for updates, um, and, and there's a certain timeliness and rhythm to that that creates a certain level of trust. So, um, from a product timeliness perspective we create 14 million products a day out of our supercomputer and we look at each one of those and then we judge ourselves to say, OK, within was it produced within 15 minutes, um, uh, on average, uh, of any previous day. If if it is we're good. If it's not, we, we kind of check our mark against ourselves and we reduce our timeliness numbers and stats. So we try to achieve a 99.9% on time delivery. I think we have a metric of 99%, but we really achieve on a regular basis about 99.97, 99.98% on time delivery on a daily basis. Um I think We'll kind of move on and just keep that in mind in terms of the, the timeliness, uh, the way in which we, we generate our, uh, and, and architect our solution we have two high performance supercomputers that are, uh, about, uh, 14 petaflops each or about 40,000, uh, cores per system. That run in uh different power grids, uh, one on the east coast, one on the west coast, we're able to switch operations between those within a 10 minute period so I can move my entire operation of uh uh forecast operation in 10 minutes from one system to the next and uh with with no, no real issue, um, so that's the type of availability and, uh, uh, data movement that we're looking at on a, on a regular basis with our systems. Let's talk a little bit about work flow. So, uh. A modeling system generally we have data that moves into the system and so we'll stream on a consistent basis billions of observations a day into our supercomputer so we'll stream it one feed into our operational primary supercomputer. We'll have a separate parallel feed into our backup supercomputer. And then when the time cut off comes to generate a forecast, um, we'll we'll dump that data out we'll quality control that data and then uh we'll run what we call an analysis state so we essentially use a little bit of magic through the physical based forecast models to look backwards at the observational data. Run it ahead 3 hours in a forecast, recheck it with some data. Run it ahead 3 hours, recheck it to its data. Run it ahead 3 hours, and then recheck it and do a final analysis state and that gets us to an atmosphere that's very continuous that the model, uh, recognizes, right? So you can imagine in. The real world with all the data observations, it's not always continuous and even where we have an observation point at exactly. Every data point that our model has, so we have to kind of reconcile that through a data, uh, analysis process. And then we run a model forecast which will go out into the future, so we have some forecasts that will run a few days into the future. Our global models will run 16 days out into the future, and then we have other models that will run 2 weeks and then a coupled climate analysis, a system that will run 9 months out. Um And then as the model's running we need to output and prepare, dump that model um memory out in each time step out to kind of post that information out to uh a file format that people can download and and and look at and our forecasters can look at. And then we have a certain amount of post processing whether it be statistical post processing um or just um uh adjustments to the model or adding derived fields we have post processing so the interesting part of this from an overall workload perspective is as you can see we have certain parts of the forecast model. That require a large amount of cores that are massively parallel, right, so our global forecast model will require about 25,000 cores over an hour and a half hour period to operate a physical based model. Our model analysis will be quite similar, but then we have a lot of serial post processing that runs around these jobs, um, and the way that we currently construct our, um, our architectures, we have 40,000 of the same. Kind of HPC cores some will have higher memory than others, but we have a lot of processing on there that's really really not HPC per se type of processing, but it, it runs on our HPC because it needs to be in proximity to the data where we run it, um, so this, this kind of presents an interesting use case when you start to look at cloud and the flexibility that you can get with the various types of instances versus, um. Trying to forecast ahead when you're buying an on-prem system about exactly what the architecture should be and what exact balance should be with the um the the multiple core um workloads versus the the the the serial workloads. This is what our what our production suite looks like. It, it looks like a mess, right? It's, there's, uh, 80 different models. Um, we have it all scheduled, and it's, it's a bit of a Tetris puzzle to fit everything in, but you see the peaks and valleys generally 4 times a day. Um, so if you look at the bottom of the scale, that's a 24 hour period, um, from, uh, zero, Greenwich Mean Time to zero Greenwich Mean Time. And each one of those layers represents a different type of model workload that we're running that's layered on top of each other, so. Where this gets kind of interesting is, OK, we have a timeliness metric in general on a daily basis things run pretty smooth everything works out pretty well. Certain days you might have a glitch in the system. Um, where your workload might get backed up, so. On an on-prem system when your workload gets backed up um we have to do a bit of traffic control because you can only peak so far on your on-prem system to catch up. So oftentimes to catch up you may have to load shed certain models or certain model cycles to to make that workload work um whereas you know if we were to contemplate a cloud environment that could mean a quicker catch up cycle because we could burst differently so these are the types of things that we're looking at when we're trying to weigh the benefits of working with an on-prem versus a cloud um sort of set up. And just as a side note from an AI perspective there's a lot of talk about these uh inference models which we'll get to in a bit um uh a bit later here um you know it's interesting when you look at the inference models it's really easy to generate these uh niche kind of bespoke AI models that have a specialty. One might be your standard global forecast model one might be geared towards tropical forecasting another might be geared towards uh. Certain domain space or you name it right you could get into hydrology or different things and so in a production environment when you're trying to run things on a routine basis and uh also manage the implementations that go into operating a system that's reliably um you could really easily get yourself into uh a situation where you're running so many different models that it becomes a bit of a mess from a change management perspective so. As we look at AI, um, it's exciting, but I think, uh, from a change management perspective it's, it's an interesting thing to think about as well in terms of, um, you know, OK, we're gonna have like 500 models to manage, or are we going to focus more on gain the integrity in, in a, in a lower number of models. All right, so What I showed you in the last slide was the general. Layout of the modeling suite and that's the bulk kind of regular routine schedule work that we have so you have kind of 80 different models uh they're all dumping data they're all doing their initial state to the atmosphere they're all forecasting into the future, um, they're all posting data and they're all kind of stacked on each other, but we, we also have some other different workloads that, uh, that, that lend well to, um, a more. Kind of on-demand elastic environment one of those uh is uh. Tropical storm prediction or hurricane storm, uh, prediction, um, so we have in our production suite right now, uh, reserved slots in our production environment where we can run up to 12, um, different hurricane modeling runs. So you see each of these, uh, yellow boxes represent. The domain of that particular model and that model moves with the storm itself and it's using the global model for boundary conditions as it's moving through. So we can run up to 12 of these in our operational environment at any given time 4 times a day. The problem there is that we have to reserve that sort of compute capacity from a production perspective to be ready any time we need to run those storms, which means that that's space on our system where we can't otherwise expand the the bulk kind of production environment that I showed in the in the slides before, so. Um, this is, this is an interesting case, so. The other interesting piece of this as well is that. When we need to run the models, we need to spin them up quickly. Um, so we can't wait around for the, for the, for the environment to spin up when we need it. It, it needs to be there when we need to start running it. So that's another kind of interesting piece in architecting the workload with these sorts of, um, on-deman runs. We also have uh development workloads which I'd like to call like surge work development workloads so we have our developers working on constant improvements in the models, but then there are also cases where you might wanna look back 30 years at the observational data and apply that to the latest technology with the models. And uh create what what I what we call a a reanalysis data set so you're using the current model to fit it to a continuous sort of atmosphere fitting it to the grid to make it nice and neat and tidy, um, and you're doing that going back 30 years and you're doing 4 forecasts a day for 30 years moving forward which creates a massive amount of computational workload needed. But we don't need to do that all the time. That's an example of a surge workload. Another example might be taking all of that numerical weather prediction, kind of physics-based model and applying that to an AI model and training the model to do that sort of workload. So it's kind of an example of a. One time surge, right? And so on an on-prem environment you really have to either reprioritize your, your workloads or you have to go get more money, take the time to revision that, get that on the floor, and then maybe like 6-8 months later you're able to start running your workload, um, so that's, that's an example of, uh, of development surge workloads. Shifting gears a bit, this is newer to us is these AI, uh, workloads for partic in particular the, the inference, uh, workload. So the things that we run operationally in production, um. We're, we're experimenting with these, um, we're in the process of getting those running in real time, uh, as we speak, and so currently, um, we have, uh, a graph cast based model that's trained with our global forecast model, um, and, and there's two types of models, modeling systems. You could have a deterministic system which means you make a, a single model run out from one initial gas field. Or you can run it in an ensemble, which means that you take an initial gas field, you perturb it slightly. And then you run it out into the future and what that does is it helps us with uncertainty calculations, right? So if you look at a suite of 31 different forecasts starting from slightly different initial conditions with the same model that gives you an indicator of the type of spread in the forecast that you could have in the future. So we have inference runs that run out to 16 days. So I told you earlier that a run like this on a physical based model would take about 25,000 cores for an hour and a half. This model with just a handful of H-100s can run in 7 minutes. It's a game changer, right? Um, and our, our global ensemble forecast very similar in, in nature, so, uh, we're very close to running these in real time operationally. Um, we're pretty excited about that and we're, we're embarking on this mission. Um, so kind of a word of caution though from an AI perspective, uh, versus, uh, physical based models, right? Are we gonna go out tomorrow and just switch straight to AI models and do that straight from observations? No, no, it's, it's a bit of a codependency at least for now, um, so we need physical based models. Um, to train the AI models, statistically based model, um, and then. Um, we can run those really quickly from an inference perspective, um, so the model runs that we're talking about, um. Are performing In a very similar performance level to our physical based models, but just keep in mind that those are trained off of the physical based model so that's a codependence that we have. Um, so we still need significant resources, um, to run these reanalysis or these large training data sets with the physical based models to train the, the AI based models, um, but it, it's an interesting context, right? So now from an operational perspective you saw the amount of compute capacity that we needed to run our physical based models on a regular time frame, um. Now think about that with a bunch of inference runs that run in 7 minutes versus an hour and a half, uh, it really changes the nature of how we think about, uh, our high performance computing and even those inference runs, um, for ones of those scale really. Are they really like kind of the true HPC or or or not so we we're kind of trying to figure that out um another interesting aspect of this that's uh it's something that we're we're trying to figure out is. What's the impact in terms of moving that data out to the public, right? I'll give you some statistics in a moment about the type of data movement that we have. But now you're going from outputting a large amount of data over an 1.5 hour hour period with a physical-based model to dropping that same amount of data in 7 minutes, right? And from a customer perspective, they're all expecting that data to be out and and to be available. So now you have a bit of a data streaming issue, right? So you have, um, uh, uh, a lot of customers looking for the same exact data at the same time and all of that's compressed in a 7 minute period that's not really fun to think about, um, from a, from a data distribution perspective. Um The other piece of this as well is just looking at the advancements of these models. There's, it's really fast paced in terms of advancements, right, but. The other issue we have is that uh the integrity of our data and the scientific integrity is is super important so there's a large. Validation process that we have to go through before we implement a model you want to look back retrospectively to understand how these perform in different seasons. Um, uh, if you're updating a hurricane model for instance, you might want to run that on 90 to 100 different storms to make sure it's verifying. You need to make sure that if you're improving the the troposphere, um, or the mid latitudes, you want to make sure that. You're not impacting the tropics, which would then impact the performance of hurricane runs and so on, so. This pace of advancement and the model improvements we have to kind of come to grips with adding potentially more automation um and insight into the validation of those models as well to ensure that we have the scientific integrity and that's a challenge for us. All right, just gonna finish up real quick here on data dissemination out, um, so this is, um, not sure if a lot of folks understand the type of data that we produce, but um we'll, we'll send out about 12 to 15 terabytes a day, um, and make that available to the public. Um, we'll serve out uh about uh 300. terabytes a day. So about 10 petabytes, 11 petabytes a month of data that we're, we're streaming out and you can see what these model implementations as we get to higher resolutions, it's not a steady amount we have to plan and plan for growth in that as well, um, so we. Offload about 50% of that data distribution to content delivery services, um, about 50% of that will, will stream out from our on-prem systems that we have from a web kind of services or an API-based, uh, perspective, we get a little over a billion hits a day on our services. Um, so it's, it's pretty tremendous for us, and about 80% of those we offload through content delivery services. But it's not just That on a on a steady basis our load will change based on what we have going for weather hurricane events, snowstorms, uh, tsunami events. So this is an example of a tsunami case that we had recently where there was an earthquake the coast of Russia and that created a large amount of warning and advisories for tsunamis across the basin. Uh, and we typically get about 2.5 million hits a day on our web services for tsunami. In 12 hours that surge to about a billion hits. So that's just a good example of how we have to be ready to surge as we move through. Um So now I'd like to hand it off to. Um Rayette and we'll get through, excuse me, we'll go through some of the more, uh, practical aspects on the technical aspects on the cloud. Thank you. Thank you, thank you. So welcome again my name is uh Raya Tos Abdullah. Um I started supporting NA as their solution architect in April 2020. Um, I was excited, um, to, to become someone who's supporting such a wonderful mission at NOAA to help save life and property. Um, certainly took it seriously, um, as you can imagine, um, and also, uh, David might not know it, I'm a NOAA employee who sits at AWS, OK, that's, that's what we have going on there, right? Um, to explain Noah's journey with HPC here. We started off, uh, early 2020 late 2020 to start to look at, um, benchmarking some of their models that they run on prem in the cloud, right? um, we started looking at proof of concepts and actually porting in their MPI applications which proved to be the longest clip um for getting this done. Once we got the benchmarking done for uh models such as GFS, uh, GEFS, things of that nature, and, and getting the metrics and stringent, um, information that we needed, uh, for how those models were run in the cloud, we certainly, um, were in a place where we started to talk to AWS partners, some of our force multipliers that I'm gonna get to in a second. Um, to help grow their offerings at NOAA for their scientists and their researchers, we flattened the learning curve along the way, right? There's a bit of a black box, um, for some scientists and researchers, researchers who are doing HPC in the cloud, so we set. Enablement sessions to help everyone understand how this works in the cloud and how does it look um there's less knobs for you to be turning um in the cloud which you know we've been able to do on prem and found to be unnecessary as well we work with our HPC specialists, um. To help enable that as well, uh, kind of an unshameless plug for Aaron Boucher, who's the HPC specialist who's not here today to helped us to actually enable, um, what we see now at NOAA and the cloud in terms of HPC. So There's two offers offerings at NOAA that they offer their scientists and researchers in the cloud in AWS. They can either run their self-managed HBC clusters themselves, build up the clusters, interconnect those clusters, have their head nose in their compu nodes as they do on prem. Or they can leverage the HPCA as a service offering that they have there. In that case we partnered up with Parallel Works and GDIT as the prime to basically provide. A unified, simplified interface for scientists to run their jobs. I'm gonna get to that in a second, right? But either way, the architecture looks the same. Um, NOAA standardized on purpose built HPC EC2 instances. These are HPC 7A6As and introduced at, um, supercomput 25 and 2026 HPC 8As will, will be released, right? Um, so they standardize on that. These are tightly coupled clusters as they are on prem. So cluster placements groups are, are, can be used so that those clusters are in close proximity to one another, right, um, in, in AWS um they standardize on using elastic fabric adapter. This gives you sub microsecond connectivity between the nodes that we have as cluster nodes as you see on prem. They standardize on using FXX for luster as a file systems that give them high latency low throughput with their storage system, but key to that is also the ability. To integrate natively with S3 to import and export, export data, meaning they don't have to have that expensive storage. Consistently, persistently in the cloud, they can also export that data, export the data that they, uh, the models have output and actually do post processing with that data or, uh, future analysis with that data from S3, less expensive object storage instead of having that persistent FSX for Lester storage. And then like David was saying, after their runs are done. This cluster is basically gone. It's terminated and they move on to the next job that needs to be ran, right? Um. Some of the on-prem workflow capability exists, you know, they still use PBS, um, for job management. They still use SLEERM, or you can use, um, AWS step functions and, and lambda functions to kind of handle that workflow management as well, um, but they tend to like to stick with some of the tooling that they use now on prem for workflow management and job management. So going to the HPC as a service offering. This was key. To enable the scientists and researchers at NOAA to one. Accelerate research to operations, right? They have a simplified web portal. They just go into the web portal. They select, I need this many cores. I need this much memory. I need these EC2 um uh instance types. I need this type of storage, and they actually run their jobs. So what this do is it extracts. The, the scientists and the researchers from worrying about the configuration of clusters and how, you know, they get the network going and how they uh are able to You know, tweak the knobs for the configuration in the back end using infrastructure as um code basically from what they're selected, they get deployed an entire cluster to do their jobs, right? Um, and those jobs really run at performance um capability that David talked about earlier, for example, when he was talking about the hurricane analysis forecast system. Also with this um um uh offering, there came a time where we were able to see the convergence. That I'm gonna talk about in a little bit, right? So they come in, the scientists were coming in and saying I need HBC clusters. I need them now. I can't wait for the on-prem cues, right? I can't wait to do my R&D because I wanna get that done now. We're trying to go operational in XYZ year, right, or a month, and so being able to have this capacity and capability was a big success at NOAA. Um, we have a couple of use cases to talk about in that perspective, right? The biggest one and the same one that David was talking about earlier on PRm is the hurricane analysis and forecast system, and we've ran this a couple of years in AWS real time during hurricane season. Real-time hurricane season, a 21 member ensemble would run twice a day. The output of that data was exported to S3, shared with scientists so they can collaborate. And they can look at that data and see where you need refinements, and this helps to help them solve problems and even improve the model over time, right? Without waiting again for when they can run these on prem, um, as well as when they can, um, you know, get the analysis that they need done after those runs, right. Um, we ran 399 HPC 6A 48X large at that time on those twice a day, um, doing those twice a day, uh, lab results and, um, lab runs that we was running as well, and this was beneficial when they were doing a test bed and trying to and analyze in real time the data that was coming out of the models, right. Another good Or I should say great case study that came up was what we call RFUS, the rapid uh refresh forecast System. Now, my understanding is Rufus is slated to go operational in 2026. And the reason, and one of the reasons why they've been able to do so is because they were able to leverage AWS to run the model again. This too was a model uh ensemble run that they ran consistently during the day again. They standardize on the luster file system, give them that high throughput. They standardize on the the purpose built HPC instances again, and they also again exported the, the data from those models to, uh, AWS open data so that further analysis and understanding of how the model is run and how it can be tweaked can be leveraged as well. They also found that it ran 15% faster than on-prem. Um, and it also was, uh, 25% better from a cost performance standpoint. But like I said before, there was a convergence so we worked through a few years with the HPC as a service offering and then we found something interesting. The scientists and researchers were like, we want access to juniper notebooks, we want access to AIML services. We also need to have the capability to, to, to analyze large data sets, um, that's outputted from these models. So we've seen the convergence from using the big tightly coupled HPC clusters. To AIML service within this particular offering. Now while we was enabling that, we were like why? And David explained it earlier. We didn't know well early on why so, but we figured it out later, is that We were seeing the convergence into AI weather prediction coming. So David talked about earlier with the traditional numerical weather prediction models. These are physical based. They take the data from the sensors, from the buoys, um, from the satellites, and they take that observation data and they run those computations on a supercomputer. On prem to get the analysis and the forecast and the reanalysis with AI weather prediction though. is slightly different in the sense that we have the training of the data using that numerical weather data output, right. And then there's inference that happens with that data, um, with that, that training data that we have there, so. With Astounding to me, and when I looked at it, is that from a sage maker juniper notebook. In minutes instead of hours. Your scientists and your researchers are able to do those weather forecasts. And they get the output and the data that they need. This democratizes weather prediction. Um, We have the challenges that David talked about earlier. Now we got more data coming in, um, I think it's a good problem to have, but also we can see how this is, this conversion is also helping us to solve problems and do other scientific advancements with AI weather prediction because. This is a single GPU instance that I'm using here or that I'm showing here on, on this slide instead of. A tightly coupled large HBC cluster, right? Um. David talked about the models that's available for with NOAA and they're working on. These are some other models that are out there, uh, forecastnet, Pungu Weather, Graphcast, Aurora all can be ran on AWS with a single or if you have a with a single GPU or if it's an ensemble like David was talking about earlier, you're essentially gonna be looking at. Several GPUs there, but the idea here is again from a juniper notebook you can run an AI weather prediction in minutes instead of hours. To prove that, because the proof is in the pudding. I'm a technologist, I'm not a scientist, right? Um, but I was able to run this forecast net version 2 small AI model. What you see here is Hurricane Helene coming through Florida into North Carolina. And you can see my name up there that's, that's my notebook that's running there based off of some code that I found, um, uh open source and available out there. So even myself as a technologist is able to do these runs just imagine this this researchers and scientists having this capability to be able to run these at will with the capacity that they need and the compute options that they need available in the cloud. And so with that, Vicky's gonna come up and talk about the future of HPC and AI. Thank you, Rayat So you heard a great case study from our colleague from National Oceanic and Atmospheric Administration, some great examples of the use of supercomputing uh to solve their mission issues and mission and to get to the right mission outcomes that they're looking at. And then you heard from RA about how AWS is powering that infrastructure and providing support to National Oceanic and Atmospheric Administration. So what is the future of HPC and AI look like? We at AWS believe it is bright. And if you're not aware of this, about a week ago, we announced $50 billion in infrastructure investment to build new AI and supercomputer centers for the US government. This $50 billion investment. Is comparable to 1.3 gigawatts in compute capacity. And this will serve the missions of various government agencies. Including those that have their workloads in the US gov cloud. In our secret regions, in our top secret regions, as well as across all data classifications. Now this investment is based on real world innovation that we've been seeing our customers like National Oceanic and Atmospheric Administration as well as many other agencies demonstrate. So what I wanted to do was kind of take you through a couple of these examples just to kind of spur the innovation. So first of all, ah, we have a a partner called S2 Labs. And uh they do subsurface mapping. What does subsurface mapping mean? Basically it means that without doing any excavation you're trying to figure out what is buried underground. Could be an infrastructure, could be uh cables, could be pipelines, could be construction material, but you have to figure out what it is underground, especially when you cannot do excavation. So in 2004, there was Hurricane Ivan. Anybody remembers that? OK, so Hurricane Ivan actually destroyed an offshore oil rig. And all the critical infrastructure fell into the ocean. And it was buried under meters of sediment. Now it took 18 years of trials using traditional acoustic methods to try to figure out. Where that infrastructure was buried because it was hazardous waste as well, but there was no success. Until S2 labs. Used deep learning. And applied physics and inferencing along with supercomputing grade APIs to try to solve that problem. And they were able to do that with unprecedented clarity. So much so That they developed a workflow that can actually map an area of the size of 400 m by 400 m by 60 m. In under 5 seconds, that's 29.5 football fields. So the key innovation here is that this has applicability in many, many different industries. Oil and gas is one, energy, utility, urban development, construction safety, as well as many others. So this is exactly a great example of what we're seeing with the convergence of HPC and AI and the results it's delivering. Let's look at one more example. Now, many of you may have encountered Some challenges when you're trying to provide decision making that involves multiple expert teams that need to come together and collaborate and then provide some kind of an analysis. This happens a lot in the engineering domain, in the science domain, even in the financial services domain where you're trying to approve a loan, for example. So, uh, we have a partner called Senera, which is a process automation company. And they took uh they worked on a domain of engineering where in order to provide a request for quotation for an engineering component that is actually comprised of many, many other components that need to be assembled together, each one of which has to be. monitored And for which the information needs to be provided by a different engineering expert and all that information needs to be synthesized and they wanted to try to model that and create a solution for that using HPC performance grade APIs as well as AIML. So they developed a solution based on Amazon Bedrock. EC2 instances running on Nvidia. As well as additional AIML-based inferencing. And what they were able to do was take this assembly of complex engineering parts from multiple different experts, take all of their expertise, and run a multi-agent AI system that gathers this information, reasons through it, and then provides the result. From 3 weeks to several minutes now. Again, you know, great example of how We applied with our partner. HPC supercomputing to solve a very, very complex problem. So these two are just, you know, scratching the, the surface. There are many, many areas of, uh, of innovation that are possible with this convergence of HPC and AIML and certainly with the investment that we're making, uh, especially for the US government, uh, we see that this will become uh more advanced in the future as well. So let me just do a quick recap. You know, we started by talking. Talking about the convergence of HPC and AIML in the cloud, we talked about, hey, back in 2023, there's about $37 billion worth of market for HPC. We see this progressing quite rapidly, 24% growth in 2024, and then by 2029, the analysts are telling us that the conversion market, it's going to be about $49 billion. Then we had David come up and talk about Noah's mission. It's not just about forecasting, remember that, right? They do so many other things which are extremely important. And then we had Rayat come and talk about the infrastructure, the compute, the capacity, and the architecture that is supporting NOAA's mission. So we hope you enjoyed it. Thank you very much for your time.