---
video_id: vQ8P-_u2tH4
video_url: https://www.youtube.com/watch?v=vQ8P-_u2tH4
is_generated: False
is_translatable: True
---

Awesome. Uh, thanks a lot, everyone for coming today. Um, my name is Viraj Parte, and I'm a senior solutions architect at AWS. Uh, today, we are here to see and hear, uh, an interesting story from Robin Hood Markets on how they've used fine-tuning of models for uh uh for improving accuracy and latency for various use cases that they have today. But before we get there, I'm assuming a lot of people in the room today who are looking at generative AI adoption, and especially generative AI adoption at scale in for production use cases, are looking at either Improving their accuracy, reducing the cost, or reducing the latency when it comes to using LLM models in, especially in critical production workflows. So, various approaches we've seen for customers doing all three of these. We've seen customers improve the curation of data. We've seen customers' right-size models are fine-tuned. And also we've seen customers uh used optimized deployment options that AWS provides for reducing uh G AI latency. In this process of productionizing use cases with generative AI, AWS has been partnering with Robinhood for a while. We have helped Robinhood with creating really interesting offerings within their product, for example, uh, the Corte Digest, uh, a really revolutionary customer experience, uh, CX agent which helps resolve some queries for customers, and we have also been working with Robinhood for a future, a, a lot of their future releases that are going to be coming up and with this. I want to take this opportunity to quickly introduce to you, uh, two of our strongest, uh, champions within Robinhood, uh, Nikhil and Davidde, who are going to today tell about the story of how they're using fine tuning for improving efficiencies across various generative I use cases when it comes to Robinhood Market. Thank you. Thank you, Raj. Thank you. Thanks a lot. Hey, uh, my name is Nicil Singhal. I'm a senior staff ML engineer at Robinhood. Uh, I lead our agentic platform initiatives, everything from LLM evaluations to fine-tuning infrastructure to inference systems at production scale. Today, uh, I and David Day will walk you through how we have adapted models and we are continuing to adapt models for latency, cost, and quality in mission-critical workloads. And hi everyone. I'm David De Giovanardi. I'm a senior machine learning engineer at Robinhood, same team as Nikhil. I work on, uh, developing agen agentic apps as well as, uh, model optimization. Uh, this is, this means fine tuning, uh, like Laura DPO all the way to, uh, building our evaluation framework. Uh, very excited to talk to you about our fine tuning efforts and also building out our evils. Thank you. Uh, I'll start with democratized finance for all. So Robin Hood began with a bold question from our co-founders Baiju and Ballard. What if finance is for every everybody and not just for ultra wealthy? That simple but powerful cushion. sparked a movement, and this is where we started our journey and we broke barriers with commission-free trading. And we did not stop there. We entered into crypto, cash management, credit cards, and recently stock tokens. We gave users access to market in ways once unthinkable. Throughout our journey, we stayed true to our mission, which is democratize finance for all, and we enabled it by building tools and capabilities which are er ergonomic. Intuitive and empowering for users to take control of their financial future. Coming to Robin Hood's AI vision. We believe for us to truly realize on our mission, which is democratizing finance, we believe that we need to give users the same level of support and insight as ultra-wealthy. And for that, we need to harness the transformative power of AI and machine learning. So for us, AI isn't just a feature, it is one of the cornerstones for us to fulfill our mission. Thank you. Realizing that mission though means that we need to build an agents that work for millions of users simultaneously. Let's take this out of the abstract and talk about one of our agentic use cases at Robinhood. This is Cortex Digest. We've all been there. We open the app, we see a stock jump 10% or maybe like tank 5%, and our first question is why? To answer that, usually you kind of have to be a detective. You have to read summaries, you have to scour ana analyst readings, uh, all the news possible, uh, and that takes a lot of work. What we did with Cortex is that we are doing that detective working work for you. But finance is a hard domain. We can't have hallucinated summaries. We can't have vague or um not clear uh digest. So this is where fine tuning becomes our, uh, our power. A fine-tuned model is usually better at, for instance, vocabulary. It knows that advice is not just a traditional word, it actually means guidance, uh, in the financial terms. It's also really good, fine-tuned model is also really good at being more objective, uh, if fine-tuned properly, uh, and for instance, giving a balanced view, uh, that is compliant. And finally, it it learns importance. So it may learn that an analyst uh report is more important than a random blog post. So, back to Cortex Digest here on the right, we see that it's processing all this data and it's kind of telling us why a stock may be up or down. But the next question is, OK, we know why, but how do we act uh on this knowledge? This is, uh, this brings us to the second frontier of our, of our work, uh, and this one is, uh, called custom indicators and scans. This solution deals with translating natural language user queries into actionable, executable trading logic. We announced custom indicators and scans at HUD Summit last September, actually here in Vegas. Uh, usually, if you want a chart to light up, uh, maybe like you want a golden cross, as, as we see in this animation, you have to be a programmer. You have to know how to write scripts. Uh, for instance, you have to know how to, uh, you have to know how to code in JavaScript. Uh, with cortex, we are removing that barrier. First, custom indicators. With custom indicators, we, uh, are giving the option to the user to just ask in plain English for an indicator, for instance, golden crossover, and then under the hood, our agent will write the code, uh, and our aim is also to fine tune a model that uh knows and learns indicator syntax and writes the code for you and then automatically displays that indicator on the chart. We custom scans, we scale this logic to the entire market. Um, we have, um, a scan that will, uh, scan for stocks, ETFs, and any asset in real time depending on the filters, uh, that, uh, our, our coding logic, uh, will spit out. Now the technical win here is that we are democratizing algorithmic trading for all and just letting users use plain English to build trading logic and custom scans. Now, this brings us to our 3rd and maybe one of our most complex use cases and which also turns out is the primary focus or use case for this talk, which is the CX AI agent. To serve millions of customers, we had to build a solution that is more advanced and goes through different stages of maturity, and it's not just a general chatbot. The first stage of maturity is what we call the foundation. We leverage Amazon Bedrock to handle the heavy lifting here. This is the heavy lifting of inference. We needed state of the art models that take user questions and translate these messy user questions in tool calls, in planned actions, in high quality responses. But having a state of the art model is usually not enough for uh answering questions in a personalized way. So this brings us to our second pillar, which is knowledge and tool access. We gave our bot uh knowledge of our internal tools, uh, knowledge of the account history. Uh, so that when customer ask for where is my tax form or uh what is the status of my latest crypto transfer, our bot will have access to those tools and will actually be able to learn you, to, uh, learn how to help you in a very personalized way. And this brings this brings us to our third pillar, which is fine tuning. This is how we scale ourselves. Now, fine tuning, uh, for fine tuning we use Amazon Sage Maker, uh, to, uh, scale up our fine tuning effort, uh, with methods like Laura. We will deep dive, uh, on that later, uh, and, uh, we, we, we, we were able to improve on, uh, uh, access like latency, uh, at great length. But as we move from a prototype to deploying to millions of users, uh, the next question is, OK, how do we make this sustainable as we keep adding intelligence, we are keeping increasing our price tag, and this may not be sustainable if we don't take action over time. So this realization forced us to confront confront ourselves with the fundamental constraint of our industry. In the, in the world of generative AI these three players, cost, quality, and latency, are always fighting with each other. We call it generative AI trilemma or a problem triangle. The, the idea is simple here. If we go after quality and use a large frontier model, we will definitely get quality, but it blows up our latency and cost budget. But if we go with the tiny model, To improve latency or cost, then the quality often dips and it goes below our safety threshold and that gets blocked. The answers do get blocked by our cartrails. So, The problem here is even harder when we talk about agents, because an agent isn't a singleton conversation. It's a, it's a, it's a multi-tier pipeline where we are making end numbers of model calls. So if one of the calls in your entire agent flow is either slow or producing an inferior quality, it may jeopardize your end to end user facing results or responses. Either they don't get to see or the guardrail blocks or the latency is high, and that causes a dip in the customer satisfaction. So How do we go about it? And that's where we want to be very methodological. We, like DD just now covered our CX uh block diagram where we have 3 stages. One is the intent understanding. The second is the planner slash tool selection, where we understand that which tools to be called, to retrieve and to answer a particular user question. And then the third is the final answer generation. We can't just put a large-sized model in all three stages. We need to be selective. And that's where we need to be methodological, and this is what we will be covering in detail today that we'll be, we, we apply prompt tuning, we apply trajectory tuning, and we are selective when to apply uh fine tuning to improve on the quality and getting the best of both worlds of the quality and latency and also the, the cost. I think it's not work. OK. So with that, I would like to share the agenda for today's talk. Now we have context contextualized it. Well, the, the first is the foundations, will, the DVD will cover the evaluation and data processing. Workflows and then I'll be covering the tuning roadmap which I just discussed, the prompt tuning, tray tuning, and fine tuning, and how we uh have built infrastructure capabilities over AWS Sagemaker and Bedrock, and then David Day will be doing a lot of deep dive and with that, uh, uh, at the end, I'll be covering the lesson learned and some of the examples where we have solved these problems. Awesome. So, before we talk about learning rates, ranks, or loss curves, we have to talk about evaluation. Sometimes there is a temptation in Gen AI to just vibe check the model, maybe like ask it a few questions, see if it looks good and then just ship it. But when you are fine tuning for especially a financial application, vibes are not a magic. Uh, we adopted a philosophy at Robin Hood of walk before you can run. If you have, um, if you cannot reliably measure performance, then we don't have a baseline. And without a baseline, you have no idea if our fine tuning is actually improving the model or just making it different. So we realized that we needed to move beyond a simple vibe check and actually build a framework that measures evil both on the end to end scale and also at the task-specific level. So let's talk first about the end to end evaluation system. Um Training the model might actually be the easy part sometimes. Uh, the hard part is is answering the sometimes uncomfortable question of did we actually improve the model or we just, did we just make it different. So to answer this with confidence over time, we implemented a three-layer uh evaluation system, uh, and this is especially valid for the end to end evil. At the top, we see uh what we call the unified control plane. So. Yeah, at the top we see the unified control plane. Um, this is important because evaluation is not just an engineering task, it's most importantly, also a product requirement. So we want to make sure that we are aligned, uh, between engineers, product managers, and data scientists, and especially we are aligned on the success criteria. Now, this evil framework is also powered by brain trust, and this brings us to, uh, the core, our second point, which is our hybrid evaluation. Now it's not sustainable to evolve millions of chat logs, uh, with, uh, for instance, like by yourself or like human human graded, so we heavily leverage LLM as a judge for end to end, uh, automated evaluation. But we don't stop at LL as a judge. We also backstop it with human feedback and also hand curated evil dataset which we will cover later. And finally at the bottom of the slide we see 3 types of model. One is our fine-tuned model, but also closed source and open source model. This is our competitive benchmarking criteria. Whenever we want to ship a fine-tuned model, we want to compare it to the baseline and especially to big models that are either open source or third party. So, this approach gives us uh what we call system level visibility, allowing us to sometimes catch regressions before uh we actually go to production and uh the model will actually be served to customers. Now, in addition to system level, uh, we are, we are also talking about task specific. Um, so, this is where I want to zoom in and use our CX, uh, uh, agent as an example. And in particular, we will talk about the CX, uh, planner. Uh, now, to give you some context, uh, the planner in the CX architecture is sitting between the user question and the rest of the agentic system. Uh, so, in this case, its job is to create specific, uh, tool calls or calling downstream agents, uh, and more importantly, it's not a creative job, it's not necessarily a free form generative job. So, for this particular task, uh, it was very useful to identify some metrics that are not necessarily LLM as a judge. So if we see the first type of metric on the, on the left-hand side, uh, this one we call categorical correctness. Uh, we can think of this as kind of like the, the routing check. Um, so in other words, did the planner select the right downstream tool or downstream agent? So for example, if a customer is asking about what's the price of Apple and then the planner is actually also invoking the crypto wallet, then that would be a hard fail. So, we see that this is a classification task and it's very natural to use precision recall F1 uh for for for this task, uh, which are traditional classification metrics. On the right hand side we have semantic intent. So in semantic intent we are actually dealing with the input argument to the downstream agents. Uh, this means that the planner, in addition to routing to the downstream agent, is also gonna give it a query or an argument. Now, comparing those, uh, planner generated queries with a reference set, we want to make sure that the similarity is high enough. In this case, we use semantic similarity, uh, and make sure that, uh, we over time, we have high similarity compared to the reference set, uh, and we are never in the case of low similar, uh, input query to the agents. Now, the key point here is that we saw the end to end evaluation system, we saw the task specific. So, the question is, OK, do we use both at the same time or when does one come into play? Um, so, by isolating the planner for the task specific metric, uh, we actually unlock the ability to do very rapid hyperparameter tuning and model comparison when we are fine tuning. So, we are able to zero in a model candidate, uh, very fast. And then once we have a model candidate, we deserve the end to end, uh, a little bit more expensive and time consuming metric for the final acceptance. So we'll talk about metrics, uh, but metrics are not super useful if we don't have a very high quality evil data. Um, and in this case, we'll stick with the CX chatbot. Uh, if we look at the left hand side, we have our, uh, sampling and an audition strategy. So, uh, with sampling and an audition, we start from real escalated cases which will end up giving us our gold answer, but let's see how this happens in our case. We don't just pull random chat logs, we actually use our internal platform called Optimus to specifically sample escalated cases. So in this case, escalation means moments where the CX chatbot failed at giving an answer and the case was actually escalated to a human. So in these cases, uh, it gets sampled and it goes to the QA team, which helps write gold answers, which then will be contributing to this gold data set. But we can sense that this process, although it's optimized for negative examples, which are usually the, the ones that are most important, is usually a little bit slow and doesn't scale too much. So on the right hand side, we have explored some synthetic data generation techniques. At the top we see self play or a coverage expansion. In this case, we have our goal data set and we generate variations of the same question to just expand the coverage in the representation in the data set. And then at the bottom we have some active sampling strategies, and this is very useful because we, the, the chatbot, especially in CX, the chatbot will have different intents, so we want to make sure that we are sampling in underrepresented areas or areas that where the bot has higher uncertainty or areas where we deem our higher impact based on the feedback data that we get. So by combining the high quality signals from Optimus and the emerging synthetic, uh, synthetic generation strategies, we are building a roadmap where our evaluation data set evolves along our models as well. So this brings us to the so what of evaluation. We don't just build evils to just get a score, we actually build evils for velocity. This means that when our evils are reliable or when they correlate with human judgment, we can move fast. And one key takeaway here is that we actually stop guessing and start defining our problem with much higher precision. So, instead of saying the model feels off or the vibe is off, we can say we have a 5% improvement in quality or the latency increased by 2 seconds. So, once we have this data-driven problem statement, we actually have to decide how to fix it. And that decision actually uh is exactly what we want to show you next. So, at this point, uh, we have a good conviction that how our evals are working. We know what is bleeding and we know what is working. Say for example, in our use cases, in the customer support use case, questions like, how does the instant transfer work? What are the fees for instant transfer? are typically the easier questions and our CX agent can just perform good in for such cases, but for questions like, hey, Why my incoming ACAT transfer has failed. These such questions require passing through a lot of error logs, joining multiple data sets, and then coming up with a, with a high reasoned answer to and to give to the user an insight that why their incoming transfer might have failed. And this is a typical workflow, how a human agent would have uh operationalized while attempting to answer a user question. So The, the question here is, Isn't that. Whether we can fix it or not, the question, the question is rather, how can we fix it in an efficient, in, in an efficient manner, because you will always find a smarter model, uh, um, uh, a furnitureer model which can answer more complex queries, but are they the right model for you to productionize? And this is where, uh, we are methodological in our approach. We. We, we believe that if we go, if we look at every problem as a nail, then we will go after over utilizing fine tuning hammer. We will be like all the time like, hey, can fine tuning solve it or not? But that itself is a big undertaking and that is where we want to have this hierarchical approach, uh, in operationalizing which which tuning method is more appropriate for a particular problem. We start with the base, we call it prompt tuning, with. With prompt tuning, uh, the idea is that we, we hill climb on prompts, whether we, if we want to move from a larger model to a smaller model, that if it gives the lower quality, can we tweak, can we make changes to the prompt which elicit best results. If the prompt tuning doesn't work, then we move to the trajectory tuning. In the trajectory tuning, the idea is you'd give the model some few short examples, dynamic few short examples which carry higher fidelity with the user cushion and. It works. It has uplifted the quality by a big margin, but it has its own issue. Now you inject more dynamic future examples in your context, your context grows up, and with that, your input token goes up, and that has an impact on your latency and also the cost. Now, when we see some uptick with the trajectory tuning, then it becomes a good question for us, hey, now this is we are seeing like if we give more data to the model and more uh examples of the model, model is able to learn, and that's when we jump into the fine tuning. And this is what we'll be talking about because higher quality means higher cells solve rate, we call it like uh higher satisfaction for the user. Uh, bot is able to successfully answer the user and with lower latency, we, uh, we, we, uh, we are able to answer user question in a, in a, in a quick manner. So, the first is prompt tuning. Before I walk you through, uh, this block diagram, like, as a user, if I were to, I, if I were to tune a prompt, I will basically, uh, pick some uh pick some examples. Some are easier, some are tough, and run my prompts against it. And see whether I'm getting the desired results or not. If I don't get desired results, then I mutate my prompt. I see what could be the gaps here, and then he'll climb on my prompts. This is a, this is a natural cycle we typically follow while tuning a prompt, but when we look at. An LLM agent, which is multi-stage, you are not just dealing with one prompt, you are dealing with end prompts, and now optimizing all those end prompts itself is a big, big hassle and it's a big manual work and this is where we, we are leveraging a lot on the prompt tuning, uh, tooling which we have built as a platform capability at Robin Hood. There are existing uh prompt optimization. Uh, there is another name for it prompt optimization which are there and uh offered by many, many, many, uh, companies, but for us our use case was a little distinct because we were looking the implication of a prompt change at a particular stage to the entire agent response. So because our application is the uh is not a single turn or single interaction application, is a multi like a multi-stage pipeline. So, this is a split into four sections. The first is the base prompt with the foundation model, uh, the, the left bottom, and then the evaluation, and then the optimization loop. And then the final output, which is an optimized prompt. So we start with the base prompt and a foundation model. We see with the eval how it is doing. And if the base and the Eeval data set has to be well stratified, I think this is what David Day has covered, uh, in length, that the importance of the stratification and importance of the diversity of your Eval data set. We will also talk about its importance in the context of fine tuning. So now you have the EAL dataset. With this prompt, if your EAL score is good, ah, and hopefully your Eval is well diversified, then the problem is solved. If it is not, then you throw it in this ah Optimization loop where you utilize a frontier model to critique your prompt and you generate more candidates. You basically mutate your prompt and generate more candidates. It is your choice. There are some configurations we offer to our users internally that whether you want to include. Short examples or not because sometimes that is an overfitting, sometimes that is OK, sometimes that is not, depending on uh how you, uh, like depending on a use case. And based on the number of epochs, what we have seen, like if we keep a fan out of 10 or say 16, 5 epochs are generally good enough. And. With this feedback loop, with this optimization loop, the prompt gets mutated multiple times, and every time you run the evaluation, depending on like 10 to 50 rows, are good enough for the evaluation. And you pick the top, say, 5 or say top 4 candidate at every epoch, and then finally you are, you, uh, you have an optimized prompt, which gives you a higher quality than the prompt you started with. And the other benefit with this approach is you understand the impact of the change on the entire agent because sometimes it's a multi-tier, uh, multi-stage pipeline, uh, That interconnection across stages is equally important. But at times you would see that. There is a limit on what quality uplift you can get through this, and that is where, which I was talking about previously around dynamic few short examples. With that, We call it trajectory tuning. Why we are calling it trajectory tuning? Because of the agentic nature of it. If, like, as we have a planner and we have at the, at the end a communicator, uh, also known as like the final answer generation stage, if you add or you inject some examples into planner, which is teaching the model how to answer what is my balance across multiple portfolios. You are basically teaching the model a domain-specific information which carries a high fidelity to the user question. You will not be injecting that example if somebody asks a question around credit cards. So, we call it a dynamic few short example injection, and the way we, and that is why we call it a trajectory tuning because by changing the planner itself, you change the entire trajectory of your agent workflow. How do we build it? There are 4 pillars to it. One is annotated data set. This is where the real magic lies that. You, you build a strategy and that strategy could be based on your LLMS judge or your evaluation, etc. which works as a filtering logic. You build a data set for humans to review and those humans do the annotation on the data set that hey, what, whether it's good or bad. If it is bad. Generate a golden answer and that golden answer is the main key here because now you know that this is, this would have been an answer provided by the bot but uh bot failed to provide that answer and this is and uh uh so what the delta or the success criteria you have defined. So annotated data set, you have your agent. And then the eval loop, the eval loop here is pretty simple, which is, could be just a similarity or a like an LLMS judge that, hey, whether the golden answer or the agent answer are really similar or not, or if there are the some figures like for example, account. Balance or or or a stock price, etc. are they similar or not, the factuality, and then a vector DB which is where you store your updated or high quality few short examples. So, to walk you through, we start with the annotated data set. Uh, labeled by human. We put them in the agent. The, like, uh, sorry, we have that annotated data data set that has input and output. We give the input to the agent, agent generates an answer and that answers get eva evaluated against your golden answer. If the gold, if it doesn't match with the golden answer, we go in the analyzer loop. The analyzer loop 2 weeks the planner, the execution phase of the of the CX agent, a customer support agent, and then we keep doing it till we find a match. Once we find a match with the golden answer, we nailed it. We really found one. Modification to the input prompt with one few short example which has resulted in a golden answer. So that few example which we were injecting into the prompt itself is our golden futured example which we then put in the vector DB. So at the real inference time for a user question, we do a similarity check or the embedding similarity and then we retrieve those 5 or 10 few short examples from that vector DB. And that uplifts the quality. So, this overall is called trajectory tuning, and this takes us uh beyond what the prompt tuning could uh really yield because of the dynamic nature of it, but it has its own challenges. Your context length grows and your uh input token grows and also it, the latency grows, and also like, there are limitations to what you can do just because of the number of examples you can inject into the prompt. With that, We finally, uh, jump into the top of the pram, which is fine tuning, where we are not just optimizing the context, we are optimizing weights of the model for our domain-specific use cases. The one thing which I want to underscore in the fine-tuning context that It doesn't really require too much machine learning or the AI expertise. The real magic here is in how you create a data set. The magic isn't in the recipe because res in the magic is there in the recipe, but there are a lot of standard recipes which are out there. For an example, we do utilize uh AWS Jumpstart. They have recipes for some open source models and they are ready for use and you can just play with that. So provided with a golden data set, you can get good results and it doesn't really require much of machine learning or the AI domain knowledge. We sometimes talk about hyperparameters. The real magic, again, is in the data set. With hyperparameterss, you can try some and then you will see that you typically get a good quality in in say 3 or 4 iterations. And these are serverless iterations, so you don't, you are not really paying too much cost in in terms of training. You are only paying for the cost for the training time. So here The first part is the training data set. The way we create training data set, we go after quality, not quantity. It's not that we throw everything to the model and let it alone. That is a failed, uh, approach. Sometimes it could work if you are lucky, but that's not the right approach. We basically employ If we first find a strata, which is Which can work as a dimension to cluster your input data set. For an example, in the customer support use case, intent. Is one of the strata which we have used. The other stratas are the number of tons, like whether it's a single tone conversation, multi-ton conversation, or whether the user is just asking for like, hey, agent, agent, agent, right? Such cases, so these becomes our stratification dimensions. Once we split our data set across these dimensions, you can use Kine clustering. And sample, say, 5 from each cluster and create your data set. Maybe say 5K or 10K. For our use case, 15K was a good spot. Once you have created a data set, query the same uh certification or same level of uh sampling you use just to create uh evaluation or a validation data set. Which is 10% or 20% your input training data set. You give it to, you pick a foundational model. Um Depending, like, suppose you see 40% quality and your goal is to get the 80% quality, you pick a foundation model, we use Laura heavily. This is what we have productionized and this is what we believe that are uh easy to adopt and easy to roll out in production, and there is good support which we receive through AWS, Bedrock, and Custom Model import CMI. There are other techniques, DPO, PPO, uh, and in the RFT space which we are, which are at the moment exploratory and something as we go more in the reasoning model era, uh, I see uh we adopting them more in the future. And once Everything works, we get a fine tuning model which Should work in the, like, we should have that quality bump on our EAL data set. With that, uh, Uh, I would like, uh, deputy. Thank you. Awesome. So, we saw our tuning roadmap, prompt tuning, trajectory tuning, and fine tuning. Uh, now we would like to do a deep dive on Laura. This is the method that we use the most. Uh, so what we'll do here, we'll, uh, define Laura just as a refresher and, and then we'll go deeper into the why we use Laura at Robinhood, uh, how it works under the hood. And then we'll also see how we implement uh our fine tuning platform uh in more detail as well. So, what is Laura? Before, um, what was the standard way of fine tuning was just a full fine tuning. If you see the left hand side of the diagram where we have the regular fine tuning, we have the pre-trained weights matrix W. And then if we wanted to fine tune for a specific task, let's say we are using a 70 billion parameter model, we have to learn the delta on all the 70 billion parameters. So that means tracking gradients and optimizer states for all 70 billion, which is very prohibitive in terms of cost and in terms of GPU usage. So, the real question is actually, do we always need to do full fine tuning? And the answer sometimes and actually oftentimes is no. Um, and this, this is where we get to, uh, low rank adaptation or Dora on the right hand side of the diagram. We still have the pre-trained weights, the W, but we keep these frozen, so we don't, uh, change them during training. And then we introduce, uh, the green boxes here on the right-hand side. Uh, two, more learnable matrices, A and B. And one of the key things about Laura is the uh inner rank, so the rank that you see between these two matrices in the visualization, uh, and this is the basically the inner dimension of these matrices. And if you force that rank to be small, very common values are 8 or 16, uh, then we are essentially reducing the trainable parameters by a factor up to 10,000 depending on the model that, uh, you're fine tuning. Uh, so, instead of, uh, learning all these parameters, we have, we end up with these two, small matrices, and then at inference time, we actually have two options. One is keep them as is and swap them out, so we would have the base model plus the, um, learnable matrices depending on the task, or we can actually merge them in and then deploy the model, uh, in full. So this is what we look at uh in the next few slides, uh, and also we'll give examples of how we implement this at Robinhood. So let's uh dig deeper a little bit into the why of Laura. You've seen the math, we've seen the diagram, uh, but, um, what, what, what are the main factors that uh let industry, uh, really adopt this method? The first one is cost. So this one is the most immediate and straightforward. We've just talked about it. We are freezing up to 99% of the model. So by doing that we are just uh reducing the, uh, training parameters massively. We don't need to start optimizer states, and this means that we can fine tune most of the time, uh, on a single GPU, and this was the case, uh, for, uh, RCX Chatbot. Second is latency. So this is a little bit more nuanced. So when we actually introduce more parameters or like a different, uh, matrices, then we may think, OK, we may need to, we are, we are adding some extra latency at inference time, uh, but it turns out that this is not the case. Because the math is linear and at inference time, especially if we just merge these weights with the base model and we deploy the model as is, there is actually zero latency overhead at inference time. And then finally accuracy. And this one is more like the natural question, like if I'm fine tuning just a small size, like maybe the 1% of the weights of the model, will my performance, uh, take some hit? And the answer is usually not. There is a lot of empirical research that points that to Laura achieving performance that is very comparable to a full fine tuning run. And this is also what we found out, uh, on our site, uh, for the CX chatbot. So in conclusion that we're getting the performance of a full fine-tuned model at a fraction of the cost and at no extra, um, latency and at inference time. So let's dive deeper and see how does it work under the hood. Um, the first concept here is the integration. Uh, so the question is where do we put the Laura matrices, uh, in the tran in, in the context of the transformer architecture. As we see in the two diagrams here, we have the two different types of blocks. One is the multi-head self-attention and one is the feed forward. Uh, so if you see the blue box here, um, these are the frozen weights that we were talking about in the previous slides. This is the big W. and then you see the Laura green boxes. These are attached to each of them, and these are the ones that are learned during training. Now, the second part is the training strategy. Uh, like if we can attach this to every layer and every, uh, every, uh, matrix, does it mean that we need to train all of them? Um, this is a trade-off. So on the limit, if you attach it to every layer, you should probably get the best performance, but this also comes with, uh, cost, uh, and the training time and compute. And on our side what we found out, uh, was that the sweet spot, especially for the CX chatbot planner was to only target the multi-head self attention, uh, weights. So this means that. Um, once we train these weights, then we end up with the multi-headset of attention, uh, matrices, and on our side, uh, especially using Amazon Sage Maker and Amazon Bedrock with CMI, uh, it, it was a seamless, uh, deployment because the base model plus the adapters are merged and then we have a final model that is identical in architecture to the base model. We can just, uh, deploy a smarter version that is optimized for the, uh, planner task. So finally, let's recap why it matters. Um, on the left hand side we have some practical benefits. We talked about scalability, uh, we talked about, um, training way less, uh, parameters, uh, and this also means very short training time, which means we can train for many, many use cases instead of just focusing on one, and this, uh, lets us scale our fine tuning to multiple use cases at Robinhood. And this also leads to fast iteration. Uh, so fast iteration means that we are training, uh, on the same use case many times and then comparing and seeing which one, which model is performing better. And finally, portability. If you train the entire model, it's many gigabytes, but, uh, Lora matrices are usually a few megabytes, so it's very portable. In terms of use cases, this unlocks use cases that were prohibitive in terms of cost before Laura. For instance, we have domain specialization. We can have a model specialized on SQL language, and another in Python, for instance. Uh, and then we have Persona or tone tuning, for instance, for CX chatbot, it would be very useful to have a more, um, soft tone versus a more objective tone for, uh, let's say, financial writing. And then finally, AB testing is very effective here because we can train many, many different versions, maybe like doing some hyperparameter tuning and then test this directly on AB testing setup and see which one is received better by the end user. So let's get to how do we integrate Laura into our fine tuning platform. The first block here we see is the goal and success criteria. This ties back to the end to end evil slide we were showing before where we have the unified control plane. The first step is always defining the goal and success criteria in partnership with product team, data science team, uh, so to make sure that we are aligned on the goal. Once we do that, uh, then there is the base model selection. We want to choose a base model that is aligned with the goal. So if the goal is latency, we will choose a model versus quality or maybe a mix of those. Once we have the base model, um, we, evil come in very quickly, uh, and this is because evils are very important for to establish a baseline. We want to, we want to know the base model of the shelf, how good it is at this task. Once we have baseline evils, we work on creating the training data set. Uh, this may employ some synthetic data generation depending on the task, uh, or it, it may mean just accumulating enough data, uh, so that the Laura, um, uh, Laura can run. And Laura usually can run on very, um, like relatively small data set quantity, which is, uh, another advantage. So when it comes to training, we actually developed two paths depending on the use case. At the top we see uh standard Lora recipes. This is what we call the fast path, and here, here we leverage Sage Maker Jumpstart, which is a great tool if you want a quick, uh, experimentation and testing your hypothesis, and it allows you to apply standard Lora recipes and, um, choosing the most common and popular hybrid parameters like rank for instance and target target weights as we saw before in the other slide. For cases where maybe the data, uh, is a little bit more messy or or or when you want to try something a little bit more custom, then we have what we call the power lane, uh, at the bottom, and this is more for custom loaded recipes. This is where we leverage StageMaker, um, studio and StageMaker training jobs. Uh, this is more like what we call our lab. So this is where engineers spend their time, uh, testing different iterations and different flavors of, uh, loaded recipes. No matter what lane we choose, either the fast lane or the power lane, we unify the deployment with Amazon Bedrock, uh, and specifically with custom model import or CMI, and this allows us to seamlessly deploy our model, uh, which also, uh, connects to, uh, Robinhood LLM gateway, which is very important because some engineers which don't work on fine tuning, they don't care where the model is coming from, they just want to eat. Uh, hit, uh, our API or endpoint and actually use the model. So, this is like providing an abstraction layer for, for them and making it very easy to use. And finally we have the evil based iteration loop, so we don't stop evil at the baseline. We make sure that once we have the model, we rerun evil. We make sure that it's better than the baseline, and if so, we ship it to production. If not, we do iterations until we see improvements. With that, uh, we discussed a lot about technical details and the ideas, etc. Let's talk about some numbers and what we have received, uh, in production. So, with our, uh, a lot of fine-tuned model, where one of the stages NCX agent, we have received more than 50% latency savings. And to put it in more perspective that what 50% is. Our previous model was giving us 3 to 6 seconds of latency and with the Laura fine-tuned model, we cut down and dropped it and brought it within 1 2nd. The major gain was On the long tail because we were seeing like P90, P95, the latency was upwards of 55 seconds, and that was causing some dissatisfaction at the user because they were like, oh why is it taking so long? And then as we have follow-up stages, the P90 multiply P90, the the, the, the, the impact gets just amplified and uh and uh and, and we run out of uh like the time to serve, serve a particular request. So, the other important aspect here was that we maintain quality parity, which was very important, which I, which we mentioned in the beginning that we don't, we can't really compromise on quality. We received or we were able to get the, the, or we were able to match the categorical correctness of the trajectory tuned frontier model and that was very essential for us to really productionize this. Uh. With this success, we plan to extend it to other set of agents under the Cortex portfolio, and we have seen early trending positive results on how we like in terms of adopting the fine tuning. Jumping to the last slide, like the lesson learned, like, the first is like I have 4 cards here, one by one. The first is evaluation. I think evaluation was very critical and there is a flywheel here. I talked a lot about prompt tuning. The prompt tuning isn't just to improve your agent prompts, it is also useful to build LLMS judge prompts. And I'll give you two examples here. One example is with the CX bot. When we were doing, when we do a lot of human evaluation, it was difficult for us to scale the evaluation flow. But Our approach to LLM judge initially was, can we just throw all the account signals in a prompt and ask the model to do the evaluation, but it overwhelms because there are just a lot of account information of like there is like account data is just too much and it overwhelms the model. We built a two-tier approach leveraging prompt tuning where we first collect the necessary signals. Which are needed to answer a particular user question and we use that in the second step we just use that information to evaluate and this helps has helped us in scaling our evaluation and also it has helped us in calibrating the human reviews because we have seen that some set of intense human reviews were just lenient on the other they were just more strict. So this has helped us there as well. In one of the other use cases, Fin crime use cases with our evolved driven development, what we have seen that we were able to get the same quality out of the box with a smaller model. As matching the uh frontier model. And we could make it happen because our valve first driven development. Otherwise, there is just a tendency to just go after and adopt the frontier model. With that A nice segue to the data preparation. Data preparation is equally important. The question is not about the quantity, it's about the quality. And understanding that what data set needs to go in evaluation versus training is important. For an example, if a model is performing well on some set of questions or some categories of questions, You don't necessarily need to include in your training data set. You can, if you, if you want to include it, just reduce its footprint, but you can just, you should definitely include it in your evaluation data set so there are no regressions. So with that approach, you just apply the same approach to create eval data set and the fine-tuning data set. With that, we covered the tuning. The methodology, which is, which ensures that we are using the engineering resources efficiently and not using the fine-tuning hammer all over the place. Prompting And then the trajectory tuning uh for the dynamic, few short examples, and then the fine tuning for the additional quality gain. The last piece here is the inference. We work a lot with AWS Bedrock and CMI where we have customized our inference capabilities. We pick hardware whether it's H100, A100, or some other hardware, uh, based on our needs, whether we want to optimize latency or cost. And there are other techniques including, uh, prompt caching. We definitely leverage them, leverage them. So for an example, if there are prompts in your agents, move your static contents towards the beginning. So the model isn't building the attention KV cache on every user question. It Reduces your cost and also reduces the latency. So it has uh multiple benefits leverage prompt compression. There are, like if we study the input data or the input prompt which is going into the model, there are a lot of opportunities on compressing the data. Can you. Can you change the way you represent your data? Is tabular a better way to represent your data? Can you remove some UU ID from the data? Can you remove the null values or the columns from the data? And this overall helps in ensuring that the input token counts are low, which brings two benefits at once, which is the latency and the cost. Uh, with that, uh, I want to say thank you and thank you for attending this talk. Yup, Raj. Thanks a lot, Yuan. We appreciate uh you all spending the time here. Just one last note is if you can see Robinhood, which is working in, you know, financial services, regulated industry, they can be so sophisticated with using all the various AWS services for shipping more generative AI powered workloads to production, you all can do it too. So I hope you got some really good lessons and some ideas on how you can use this more uh reliably in your production workloads. Thank you.