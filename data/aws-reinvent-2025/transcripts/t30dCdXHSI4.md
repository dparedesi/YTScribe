---
video_id: t30dCdXHSI4
video_url: https://www.youtube.com/watch?v=t30dCdXHSI4
is_generated: False
is_translatable: True
---

Financial institutions aren't choosing between AI speed and regulatory trust, they're engineering both. The question isn't whether to automate risk intelligence. Hm, but how to to to architect AI that augments human expertise without compromising accuracy. That 260 million daily transactions depend upon. Good morning everyone. Um my name is, I hope you're enjoying Reinvent. And I hope you're looking forward to the week ahead. Uh, thank you for taking time out of your busy schedule, uh, to spend some time with us today. Um, my name is Richard Chester. I'm a principal solution architect with AWBS based in London, and I'm delighted to be joined shortly on stage by Chris Hughes, Director of Engineering, and Louis Duert, CIO from the Risk Intelligence division in the London Stock Exchange Group, which we call LSEG for short. Today, if we can, we'd like to start by giving you a little bit of background about LSEG. Then Lou, we'll move on and Lou will give you some er information about risk intelligence as a business um and where it, it's place fits in the financial services ecosystem to fight financial crime. And then the heart of the presentation will be to dive deep to explain how LSEG Risk Intelligence are accelerating the World Check platform. By employing AI. So, we'll make a stop. Founded as a regulatory exchange in 1801, and with a rich history going back through coffee houses and the Royal Exchange. And London Stock Exchange Group. is a financial markets data and infrastructure provider headquartered in London. Among a number of trading platforms, as the name would suggest, it operates the London Stock Exchange. And manages international equity, fixed income, and derivatives markets. The group though, has much more than exchanges in its portfolio. It also offers. Real-time reference and market data. Through FTSE Russell curates category defining indices. Develops capital market software used by other exchanges globally. Provides extensive post trade services. And helps fight financial crime. And through risk intelligence, and that's where we're going to spend time today. So software engineers like hopefully many of the people in the audience will find coding solutions enjoyable. Working with new technology, particularly AI can captivate the imagination, as hopefully most people would agree. But using these technologies together to fight financial crime, now that's a very compelling story and it affects absolutely all of us. So let's now find out more from Louis. Thank you, Richard. Good morning everybody, pleasure to be with you. Financial crime impacts us all, whether it's through fraud, scams, uh, corruption, money laundering, and even terrorism financing. All of these fuel some of the world's biggest challenges. Europe alone lost $100 billion in 2023 to fraudulent fund flows. And that's just the tip of the iceberg. Globally, we're looking at money laundering. Between 2 and 5% of global GDP is um is laundered each year. Uh, and that's in real terms, that's $5 trillion in 2024 terms. And then there's financial fraud. Where we have global fraud losses estimated at $5 trillion as well. Account takeover fraud is a big growing category, as is synthetic identity fraud, growing 31% in 2024 alone. But this isn't just about money. It's about trust. It's about the health of the financial system. We know that the villains are getting smarter all the time. They're getting faster, they're getting more sophisticated, and the question is, are we? The real kicker though, 78% of financial institutions are admitting that they don't have the adequate people or technology to deal with the challenge adequately. For 25 years, ESEG Risk Intelligence has been a trusted partner, helping organizations across the world to detect, assess, and minimize financial crime risk with confidence. Our human expertise built on global intelligence and our adoption of the latest technology helps customers uncover some of these hidden risks. Because we know that behind every statistic, behind every number, There lies a human story. And we know that helping protect the financial system is also about protecting people. Our two key focus pillars for our product strategy. Firstly, customer and third party risk, and this is where we screen individuals, entities, and even vessels all around the world against known risk factors. And then digital identity and fraud is the other big pillar, and that's verifying accounts, and it's verifying identities to establish both confidence and risk. In terms of the actual product suite below those pillars, firstly, there's screening services. And this is where we have structured intelligence on heightened risk individuals, entities, and vessels. And this uses our data, but also our solutions that we build in-house and the human insight that we combine that with. Then there's due diligence products. And this is where we use detailed integrity checks and advanced background checks on any entity or individual. There's account verification where we enable organizations to transact together in confidence knowing that the accounts that they're moving money between are of known good repute. Identity verification, verifying that people are who they say they are, not just because they've said it, but because we've taken what they've said and checked it against the world's identity databases, in many cases uh governmental databases. And then lastly on boarding, and this is where we help our customers to digitally onboard their prospects with confidence. Knowing that they're not on boarding bad actors. Today we will focus on our screening services and our use of AI in that space. Just to understand our business a little bit more in numbers, we are a trusted partner to the world's largest banks, but also non-bank financial institutions. We have 10,000 customers, everything from banks, as we say, to FinTechs to um non-bank payment service providers, and many corporates as well have a lot of need for our services. And we help our customers de-risk their financial actions and to achieve regulatory compliance, and in total, that's making the financial system of the world safer. And this is hard work. It involves thousands, 10s of thousands of data sources constantly being monitored. And we combine that with our unique subject matter expertise and tech innovation. So that's our mission, and we're energized by it, we're inspired by it. But looking beyond that, we also want to grow, and we also want to succeed in our market for our stakeholders. We have several key growth levers. One is efficiency. One is leaning into the market's demand. For SAS-based solution delivery over traditional deployment models. Another is serving our clients' need to consolidate their vendor dependencies and their complexity. And most relevant for today, we need to increase automation. We need to deepen insights through the responsible adoption of AI and Chris will talk to us about that in a moment. In particular, our strategy is to move from manual data collection to intelligent automation. From rigid data distribution to tailored real-time distribution. And to scale customer need and offer broad-based solutions to our customers. And Chris will now help us understand how we accelerate with AWS AI products in this key data curation layer that you see at the top. Right. Thank you. So what I'm gonna be talking about tonight. A content curation platform. And the idea behind the curation platform is really how do we accelerate our data collection services within WorldCheck. Every person in this room who's opened up a bank account has either been screened against our data or screened using one of our services. And that data is critical to how we operate as an organization, how banks, financial services operate in this world. Now, when you look at WorldCheck as a product and that sort of thing, You're looking at this, it's a global database, high risk individuals, Peps, adverse media data, sort of known terrorist groups, those associated with nuclear proliferation. It's a phenomenal amount of data really dealing with the world's biggest challenges today. It's quite cool. You get to think you're Jack Ryan in the middle of a spy movie with all the stuff that we do at the moment. And when you look through the key features here, we've got over 200 analysts who are curating data from thousands of sources. And you've got to keep in mind that this is not just Western English speaking locations and that sort of thing. We've got analysts who are covering everywhere. They're using that data. They're understanding what is there. They're curating it and they are making it available to customers. So whether you have somebody who is based in Kazakhstan, Peru, Bolivia, wherever, you've got that data, you're able to utilize it and actually make Intelligent decisions through the process, OK? 60 plus languages, rigorous quality control, and it's used by banks, insurance providers, and governments worldwide. So if you look at the WorldCheck ecosystem today, the number of applications that make up the ecosystem, we've got WorldCheck 1, which is our web-based screening product. This is used by, as I say, banks, financial services, social media companies to screen their data, screen their customers as they are on board through the process. We've got our on-demand API which aims to deliver data to the hands of our customers to allow them to make intelligent decisions quickly. And then we've got products like WorldCheck Verify which are specifically designed for low latency screening in a payments type of use case. And then when you look at the content facing side of the process, this is where we're really going through and accelerating that data. We're pulling it through the process, getting it through the process, and making sure that the data that arrives in the hands of the customers is accurate and is being reviewed. One of the biggest challenges that you see here, and a lot of startups are struggling with it at the moment, is they simply use AI straight through processing and you end up with a lot of very low quality data, OK. I'll give you a good example of this. Somebody goes through and you're using an AI agent to go and review a website. Lots of flags that pop up. It's all about a terrorist attack and some nuclear explosion that's taken place, only to find out it's actually a movie, OK? And there's a review about that movie. But when you look at it from an AI perspective, all the flags tick all the boxes. Oh no, no, this is important, OK. Ben Affleck, he's a bad guy. We definitely need to put him in our database as a sort of a known terrorist. OK. So this is where the whole world check Curation platform is really understanding a lot of that data, but it's also combining that human expertise that we can start sifting through that and working through the process. So why generative AI? As I said, we need to go through and accelerate curate this data quicker and faster. We need it in the hands of our customers. It needs to be accurate, but at the same time, it needs to be timely. OK, you can't sit there and have a situation where some event has taken place and you're only telling your customers about it weeks later. OK. By that point, the money is gone. It's been moved around to 6 different accounts. It's in 6 different countries. OK? It's a huge, huge problem. We want that data in the hands of our customers as quickly as possible. OK? So escalating data volume, we're going to be pushing the limits in terms of what we're wanting to pull through the process. OK. If you look at it, you know, everybody's got a mobile phone nowadays. The amount of news data that's coming through that process is absolutely phenomenal, and Need to be able to keep up to date with that. You can't just simply scale that using human beings. It's unsustainable. And at the same time trying to find individuals with the sort of language knowledge, the expertise in that particular space is a huge problem. So how are we going to go through and actually do it? Key opportunities here. Now if you look through what's involved here, where you're looking at things like summarization, extracting of data, drafting of curated records. It allows you to really accelerate a lot of what we would classify as toil. So these are the sort of, you know, low value automated tasks that require a lot of time and effort, but don't necessarily add a lot of value. What you want your researchers to be focusing on is that real high intellect tasks, the stuff that they're really good at, and that's the area of expertise. But then at the same time, you go through and actually accelerate the actual work that they're doing. OK, how it works, you know, basically we built an AI that performs a series of micro tasks, and the concept of a micro task is really very similar to what you would think of as an agent in an engentic AI framework. OK. This is an individual step within a business process that allows you to go through and analyze that particular action, OK, decide what you're going to do with it. Now that micro task can be done either by a human. Or done by an agent, OK? And this is the key thing to think about when you're building out these processes and that sort of thing. It's not all about a fully automated capability. It's not about stringing together a series of agents and completely reinventing the wheel in terms of how you process it. It's about understanding where you're delivering business value. In that process, to be able to step through that and say, OK, cool. This particular step, this is a high impact decision. We need to be able to do this step faster because it takes a long time. Once that step is done, I'm gonna present it to a human because it's actually quite high risk. I want somebody to go and eyeball it, OK? But, OK, I want the work to be done very quickly. OK? And again, technology-wise, it's built on AWS. We're using advanced models and scaling architecture provided by AWS. Now if you think through the actual journey that we take here, you've got typically exploration, experimentation, so this is teams that are going off, they're running a POC, they're trying to build something, put it together, then they look at this and go, OK, hold on, time to operationalize it. I want to take this. I want to put it into production. Then they try and scale it up and then they decide that they want to transform the organization to really utilize a lot of the new technologies. The problem that we find with this is a lot of organizations start at the transformation piece, OK? And they sit there and they look at this and they go, Oh great, we're going to completely transform our organization with AI as the basis. And that is typically why large organizations fail at AI implementations. It's because they think about this in these big transformation type things. There's millions of dollars that get invested into these programs. And they typically always fall flat. Why? Because people do not fundamentally understand how to build AI components that augment what humans are doing in the process at the moment, OK? And typically when you think about this, you want to start right at the left-hand side. Yes, you've got grand visions about where you want to get to on the right, OK, and you want to transform your organization, but that should never be step one in the process. So if you think about Big Bang, you know, why do these projects fail? You know, Overshoots reality, you know, data ops, governance, they're not ready for it. Lots of organizations out there at the moment fundamentally do not understand how to govern AI projects, OK? Whether it's the legal team, it's the risk team, it doesn't matter. They don't fully understand it. So either they overregulate it and make it impossible to get anything done. Or they are under regulated and create huge amounts of risk within the organization. Demos are not production outcomes, OK? And this is typically what you see with a lot of consulting type projects or things done within certain pockets of the organization. They'll put together a cool demo, OK? And it looks awesome. OK. You can do a couple of things and that sort of thing, but it's very much a happy path. There's not a lot you can actually do with that process and that's the thing. It doesn't scale, it doesn't deal with any of the edge cases that are associated with it. Then the next one there, which is critical, when you're dealing with people and you're fundamentally changing how they operate within an organization, trust is fragile, OK? You can't come along and go, hey, we've got big, big plans. We're going to sit there and automate the entire journey here. We're going to change the world, do all of this sort of stuff. You put the AI system in, you deploy it, you get it up and running, and it falls flat a week later. OK. As soon as you do that, you lose credibility within your organization to be able to deliver this, and it takes a very long time to rebuild that, OK. And the key rule around this is ship us a working slice per level. So work through the application. What are you actually trying to build out, OK? Take that single use case and then ship it to production. So the reality of what you're looking at here, and this is the sort of journey, and I'll start talking through each of these steps in a lot of detail, but you want to think through how you step through this process of maturing within an IT AI organization. So typically what we do, and when we talk through this with the teams and that sort of thing, we say, cool, start prompt only, OK? Nice and simple. And I'm gonna go into a little bit more detail with each of these in a moment. But you're starting at prompt only, you're moving to the single agent action. You're then implementing something like retrieval augmented generation, tying it back together with real data in the process. You're incorporating sort of a genetic orchestration, multi-agents all tied together, then creating this interoperability layer to be able to really scale it across the organization. And then you're looking at how you're wanting to fine tune a lot of the models, OK? These are the key steps that you want to follow in terms of how to really get solutions into production. OK? Again, the key mindset here is how do you take solutions to production. OK? Not how do you build something that's cool from an engineering perspective, but how do you deliver solutions that add real value to an organization. So if you look at the first one here, we sort of, we spoke about this idea of prompt only, OK. A lot of organizations skip over this, OK, but why? Because the idea behind a prompt-only scenario here is it allows you to prove out something very quickly and very easily with relatively no risk. So if you take part of your business process, there's an activity that's being done there, it could be summarizing an email or content that comes through from a customer support person. You want to be able to go through and look at that and say, hey, I want AI to do one action. OK? And that could be pass the content to an to an LLM, get a response back and display a result to the user. Now that user is still the person who's making the key decision here, OK. You're not automating this entire process. You're simply accelerating it. You're augmenting what's already in place. And the key benefit associated with that is you massively reduce the risk associated with it. Because it doesn't matter if the AI is 100%, OK? It doesn't even matter if it's 90%. If you've got an AI that's accelerating a single action in a business process, OK. And saving that person 80% of the time in that particular action, you have a, a clear win in terms of how you take the whole process forward, OK? And it allows you to deliver it very quickly. You don't need complexvals to be able to do that, OK? Especially not in the first stage of the process. Now you want to get to that point, obviously, but for the first step in this particular process, all you're particularly interested in, OK, is how do I sit there and prove value using AI solutions, OK? The biggest problem that we're seeing at the moment is a lot of organizations are going out there. They're using tools like co-pilot, chat GPT, you know, some of the anthropic models that are there. They'll go through that and they'll say, Oh no, look, I'm a business user. I can do all of this cool stuff. But the engineering department takes too long. Oh no, you're telling me this is gonna take 18 months to build out this capability. I'm already sitting there, I'm sticking all this highly sensitive data directly into chat GPT and it's saving me all this time and effort and that sort of thing, OK? It's creating risk in the organization by actually delaying a lot of these processes and that sort of thing. And again, the key thing here is you can deliver this type of use case very easily within an organization. Now, I'll give you a nice real-world example here. Taking an article from a news source, come through the process, summarize it, OK? I want you to pull out all the names of the people that were listed in that article. Tell me about any relationships, any facts associated with it, any key events. If don't do anything else, just pull that information, display it on the screen, OK? What that allows you to do is to use it to actually see that information. And they can make a quick informed decision about the content of that particular record without necessarily having to review the entire article. One of the big challenges we've got within the World check space at the moment is our researchers are spending a significant amount of time. Simply reading the news. What's going on? Let me read the news. Let me understand what's going on in terms of global politics and events, OK. And as I say, the key thing here, you know, proof of value. So if you look at this from a sort of a technical implementation perspective, this is a relatively simple implementation. You know, you've got an API gateway there, you've got a lambda which sits in the front of that, and you're simply connecting to a bedrock model. Nice, simple, very easy to implement in any organization, OK. Delivering business value very quickly. So, then you start looking at the idea here of single agent action, or single action agent, sorry. And The idea behind this is to sit there and actually start leveling up in terms of what you're wanting to do. But again, keeping this whole solution very, very simple, very, very straightforward. What are we doing here? OK? We're defining the bounds of what this agent is doing. We're not trying to solve an entire system, OK? We're building a capability that solves one task, OK, and one task only. It's not complex. If it takes you more than a page to describe what the agent is actually doing, it's too complicated, OK? You want something that is nice and simple that you can call or, or potentially listen to an event that's taking place in the organization. Analyze something and generate an event, OK? And you want these components to be integrated into an existing workflow process to be able to sit there and go and analyze that and say, hey, What's going on? How do we actually drive it? So as I say, how do we use this from a world check perspective? This is specifically where we're extracting event data. We're classifying the article for relevance. If you've got articles about somebody who is making a donation to the local church, that's not really something we want our researchers spending time and effort on. We want to look at that. We analyze it, and we say no, no, that's not relevant to what we want to include in the world check system. So we'll sit there and go through. Push it to one side, OK. Allow the teams to actually go through and continue working with what they are doing at the moment, OK. Again, you can classify the article relevance and you can focus on the decision, OK? Then you start looking at that and you say, OK, fine, let's gets a little bit more interesting, OK? And you may sit there and say, cool, I'm passing it through the process here, OK? And I'm adding a database query that's going to be involved here. OK. Now, this is typically where you start thinking about this is, again, very single agent action, OK? It's not a complex rag implementation here where you're wanting to query large volumes of data and you're setting up vector databases and a whole bunch of stuff that's associated with it. You want this to be very quick, very simple, very easy to use, OK? Cool. Then you get into the next level here, and this is typically where a lot of AI type solutions start because I think this is the first step in the process. Everybody is extremely concerned. Oh no, I'm gonna have hallucinations in the model and that sort of thing. I'm not quite sure what I'm gonna deal with. How do I know? Oh no, we've got to start with, with RAG, um, and we want to start implementing that as the first step in the process. Yes, it's important. Yes, you want to make sure that your data is grounded in reality, but no, you don't need to have a full blown RAG solution where you've got vast amounts of data sitting in a vector DB, OK? You don't need to start there, but that's where you want to build up. So once you've proven out the use cases where you've got Agents running in production, delivering business value, that's when you can start looking at this going, OK, cool, what's next? How am I gonna take that process forward and start delivering more value on the process? OK. So if you look at the sort of situations here, we're using things like, uh, you know, the Titan embeddings model, you know, you, you sit there and you build out embeddings, vectorize the thing, store it in a vector database, and then you can start querying and interacting with some of the data that's available to you. Now, this is really relevant to what we're doing at the moment because it allows us to cross-reference news articles, OK? You've got an article coming in, you may have summarized in individual articles, but what happens when you want to start clustering articles together? You know, there's 50 articles about John Smith. OK. Let's pull all that information together. What, what details can we pull down from John Smith? OK. There's an article that mentions his wife in this article, OK, and mentions his children in a different article. It mentions where he went to school, uh, mentions what he does for a living. You can start cross-referencing this data together to pull it together, and that's where it starts becoming really important. To have capabilities like RAG, you know, being able to do semantic search on the actual data that's associated with it, it's really, really critical to be able to pull all of that together. And the nice thing about doing this using RAG, is you can pull out of that data and vast sums of data without worrying about the size of the context window, without worrying about what's involved in the process. It allows you to combat hallucinations because everything is grounded on real data and really allows you to get significant value out of the whole process. You can Then you start looking at sort of real step up in terms of that, and this is where you start thinking about things like sort of knowledge bases, information that you're putting together. You're going to land up with multiple data sources that you're putting in as part of the process, multiple steps on that particular journey, looking at that data, how we tie it together, OK. You know, standard flow that we've seen so far, you know, API core into a lambda, and you've now got Bedrock working through it, but at the same time, you're now looking at this additional layer at the bottom there that allows you to start mapping out that particular process. OK? We're talking about, you know, generate the query embedding, OK, passing it through there, retrieve some documents from the vector store. You know, augment the query with the retrieved documents. You're able to analyze them and then you can generate a response that's associated with that. There's real value associated with how you fit that together. Now we use this all the time. OK? We tie all this information back together. We're cross referencing data on existing World Check records, stuff that's already been processed by our system. It's made available to customers. Potential relationships that exist. So you've got sort of graph rag that you're incorporating into the mix there. What does that tie back together? How do you utilize that data and make it available? Sorry. And then the key thing here is moving into the next level, and this is where, to be honest with you, things get very interesting. It's all around the idea of agent orchestration, OK. This is what levels up organizations in terms of utilizing these capabilities. Now, if you think through the sort of curation process and typically what individuals are doing at the moment, you've typically got multiple stages within a business process. OK? You want to understand what's involved in those business processes. How do I step through what's happening there? What events am I doing? What's the ordering? What's the prioritization associated with it? What actions am I needing to do at this stage of the process? What agents am I needing to do at the next stage of the process? Have I got quality assurance? How am I assessing risks that are associated with it? And having an orchestration of agents step together within a business process allows you to do that, OK. The key thing here is this is not simply agents passing information on to other agents. It's understanding the flow of that business process. OK? As I mentioned previously, one of the key parts of this whole process is the idea of um Human in the loop. So as I step through the series of agents here, I want to understand what the system is doing, OK? Am I evaluating this to understand the risk associated with it? OK. Have I done any sort of quality assurance checks on that data? OK. Can I sit there and look at this particular piece of the process and go, you know what, I'm not happy with the quality of the content there. I'm either gonna kick off a different agent to go and enrich that particular record, or I'm gonna pass it to a human to say, hey, I'm worried about this. Or do I sit there and say, go off, trying to enrich the record first. Here's a token budget of events that you can actually go through and do. If you've exceeded that and you still haven't met the criteria presented up to a human being to go through and review it. All of these steps are critical to the process, OK? But you want to be able to seamlessly switch, OK, between a human-curated task or human action. An energetic agent-based process, action, OK, and you need to be able to switch between the two as you're stepping through the process. Now as the platform itself evolves and you start sort of utilizing more and more of these processes and that sort of thing, you're going to reduce the number of human steps required in that particular process. But to start with, especially in an organization with a sort of a low IAI maturity in terms of how they take it forward, you really, really want to make sure that you can embed both of these components together to allow you to step through. And pop up to different points in the process. And this is often the key thing a lot of people forget is they focus very much on all of the technologies associated with the gentech. We need to implement MCP servers. We want these components all tied together. They forget the human aspect associated with it, and that is really, really critical to make sure this is a success. Remember, it's not just an engineering product here. We're wanting to deliver a capability. That are people in the organization are actually going to utilize this technology to be able to pull this information together. That typically requires large amounts of change management to teams that are typically sort of not necessarily up to speed with everything that's happening in AI, but they've probably heard things about AI is going to take my job and all the rest of that sort of stuff, and you want them to be champions of this type of solution to be able to really drive the capability forward. This is very much an augmentation solution, not an automation solution. OK, and it's key and important to understand how that ties back together. So, if you sit there and look at it in terms of, you know, wheelchair context and that sort of thing and the type of stuff that we're doing from our side, you know, the agent performs a search, it does the identification, it does the extraction, it potentially proposes a draft, sends it through to quality assurance to go and review that particular process, sends it back, presents it to the human to say, hey, I've done all these checks. This is where it is in the process, OK? It allows you to massively accelerate that journey through that particular process. But one of the key things that you haven't done is you haven't added risk, OK? Why? Because you humans are involved in all the critical junctions through the process, and this is really important when you're looking at implementing AI solutions from a regulatory perspective, OK? You want to be able to explain why the AI has done all of these key steps and how it's taking the process forward. And by having a human involved in that process where they are the ones actually making the decision, effectively, the human accountability is the key thing in the process, OK. Cool. So, if we move on to the next one, now this is where things get fun and quite exciting is when you start looking at how do you interoperability between your agents, OK? And this is where MCP is a really cool technology and a really cool way of actually being able to tie your agents together. The big challenge though is building out MCP. In an automated way where you've got a whole series of agents being tied together is actually quite difficult to do, OK? And it's quite difficult to do it well. And this is typically the key reason why I always push MCP type adoption, interoperability of all those components right to the end of this journey, OK. Because it's difficult to get right, OK, and I'm not saying you've got somebody who's got lots of experience building up these components or isn't able to do it straight off the bat, OK. But most of your engineering teams are going to struggle with this type of adoption and being able to build it out correctly. So starting at this level one and moving up through the process, they're able to mature the process. They're able to understand how they need to adopt these solutions within the organization. They're able to prove out that value and then they can start building out these bigger, better solutions in terms of what's there. But the stuff that you can do with an MCP server now and how you can tie everything together is fantastic. I mean, the world is really our oyster in terms of what we're able to pull together. OK, so if you start looking at level 6 now, and this is the one that gets interesting and possibly a little bit controversial because Everybody comes and they start looking at sort of, oh no, we're going to implement an LLM, OK. And every techie at heart, and myself included, comes along and goes, no, no, no, hang on, we want to start fine tuning models and we want to start doing all of this really cool stuff, OK. It's more often than not, not required, OK? You don't need to fine tune a model to get value out of it, OK? And especially with the types of use cases a lot of organizations are putting together, using the standard models and that sort of thing actually allows you to deliver everything that you needed to do. Now, where the, the fine tuning comes into play is very much understanding your cost rationale associated with it, because it's very easy to go and use the latest and greatest model. And it costs a fortune, OK? And this whole piece of work that you've put together that works really well, just doesn't make any sense. There's no ROI associated with it. So what you want to start understanding around this whole fine tuning is starting to think through how we tie all of these components together, OK? What's involved in terms of optimizing latency and costs? You know, are you choosing a really high, high-end model? Which may be a little bit slower, but gives you far more accurate results, or you're going with a much smaller model that allows you to deliver things quickly, um, and your responsiveness is key to how you want to do it. You're wanting these low latency models associated. That's a key decision that you want to make, depending on the use case that's there. It's often not a key decision that needs to be made right at the beginning of the process. It's something that you can do as part of that thing. OK? Again, your model selection and that sort of thing. Select the model that's appropriate to what you're needing it to do. Where is using an AWS Nova model appropriate? Where is using one of Claude Anthropic's models? OK? What's involved there? And again, you then allow you to select the models based on cost, latency, performance, ability, and that sort of thing. Then you start getting into your specialized models. And I typically, this is always the last step in the process because you need quite a robust data science and AI type organization to be able to really get benefit out of this, OK? It's very easy to fine tune a model, get it wrong, make things worse. Now you've just wasted a whole bunch of money associated with it, OK? So you really want to look at that as a final step in the process. Not to say there's no value associated with it, that is really where you start getting real value at the end of the process. But think about it as a sort of like 80/20 type rule. 1st 80%, you just don't need it. And 80% of the use cases out there don't even need to think about it, OK? It's only when you get to that last section of the, of the pie that you really want to start thinking through what that looks like. OK? Now, If we start thinking about the fine tuning process and that sort of thing, you know, you're going to look at this and you're going to say, OK, cool, I want to define my use case. Let me think through it, how this all fits together, what's involved, and you really want to understand the whole enter in process. You're prepping the data, you're formatting it, you're feeding it through the customization process, and it allows you to monitor it. And then you've got the emails that are associated with it. Now, one of the key things that you would have noticed is I haven't spoken a lot about evals. And the reason why I haven't spoken about it is that a lot of organizations fundamentally don't understand evals and the benefits associated with it. They keep thinking back on evals as a golden record, golden data set. Now think about it from a sort of traditional QA type perspective. I'm going to test out the model, you know, using old school QA practices. I want a series of automated regression tests to check stuff that's working. And that's part of what evals do, but it's not the, not the whole part of the process, OK? You want to make sure that you've got a lot of that sort of stuff in place, but at the same time, you need to be monitoring what your solution is doing, OK, when it's in production, not just sitting there going through and saying, well, I've tested it, it works perfectly, it's now in production. OK, and I've deployed into production. So emails are super important, OK? But they typically come much later in the process when you have an organization that's mature enough to really understand the benefits associated with it. And it allows you to do things like upgrade the model that you're using, downgrade the model to a different version of the model, revisions to your prompts, that type of thing. But you really want to be able to look at how you incorporate that into the process. Sorry. So, if you look through the adoption playbook that's here, OK, and the sort of key things that are there, OK. The first one is pick a painful workflow and define done as in production, OK? Just to really re-emphasize what this looks like. If you take an organization, OK, and you start thinking about the business processes that exist. Sit there, look at the business process, break it up into sort of 100 steps in terms of thinking lowest common denominator. What's the easiest way to implement some of those components. Once you've broken it down into those sort of 100 different steps that exist there, OK. Pick the most painful one to your business organization. What's, what's the number one thing that is a pain point for them right now? Build something that solves that problem and take it to production. Don't worry about the rest of the process around the rest of the organization. Just build something that helps solve that one individual problem. Get it into production, get it delivering value in that process. That's the key part of this, OK? Especially at this initial stages of the process, you want to start thinking about things in 2 weeks, um, spikes, OK? And I know it's quite, kind of a bit of a cliche talking about sprints and all the rest of that sort of thing, but that's really where you want to define it. You want something that you can define, you can get into production and deliver real business value associated with it. So get it there as part of that. Then the, the third piece here is very much this idea of keeping humans in the loop. Now I've spoken about this quite a lot in terms of this presentation. For me, that's absolutely critical at the initial stages of the process is to have humans in the loop by default for any business process that you implement. OK? Your organization will mature and be able to implement these capabilities and have, you know, automated straight through processing, but it should never be the starting point of the conversation. You always want to use the AI components, present stuff out to the human, get them to review it, feedback in terms of, of, of. That process so that you've actually got real feedback there. That then ties into your monitoring and emails because you've now got the humans making a decision. The LLM got this, or the AI capability got this wrong. I'm marking it as it got it wrong. It's feeding back into my emails and LLM ops type monitoring processes. And then I can start looking at that and going, oh, no, hold on, I'm seeing an uptick here on that particular process and it's taking too long, OK? Or It goes to the human more often than it should, or the human is rejecting it more often than it should. What's going on there? Let's go start deep diving into what's going on as part of that process. Now, when that happens, OK, you've then got a team that's able to look at that data, really understand what's going on in that particular process. Whereas if you rely on a fully automated end to end capability, when things go wrong, it's typically because customers are phoning up and complaining, OK? You want to be able to monitor that. You want to be able to effectively be able to understand what's going on so that you can be very proactive in terms of how you report through that, uh, and sort of be able to analyze and look through that. You know, I sit there with a lot of the data science teams and people talking about, you know, how we're monitoring precision and recall and that sort of thing. And a lot of data scientists by default will think about those terms in terms of a way to test for that, OK? Not to monitor for it, and that's the key distinction associated with it. You need to be able to monitor this effectively so that once you're in production, you've got a real view as to what's going on, OK? Rather than it just be something that you say, oh, well, I've tested it, I've got my golden data set, it's all working well, life's great, OK. And the last piece in terms of this adoption playbook. Communicate wins with metrics, not demos, OK? Everybody likes doing demos, OK? They want to build some cool widget, demo it to Xco and go, wow, look at this, this is amazing. Demoing metrics is what really allows AI adoption to really accelerate because that's the key win that you get. OK, and then just talking through some of the AI content curation architecture that we've got here, OK, and, and how a simple flow allows you to ingest records through the process. You know, you're really able to sort of have data flowing through there. You know, we've got content coming in through the platform. It's being acquired. It's putting it onto an SNS queue there, and then we can start processing that data to be able to step through it, OK. And the entity extraction, the fuzzy matching matching, the inclusion criteria, that sort of thing, those are all key things that we do using LLMs running on Bedrock. OK. We're able to process them. We're able to pull all that data together. Query them, feed them back to the user as part of the process. So that is a very straightforward ingestion workflow to be able to process inbound news articles, but it's extremely effective at delivering real business value, um, to the teams and the organizations that are involved, uh, within ELSIG. Cool. So I want to just hand over to Louis now in terms of talking through sort of some of the outcomes that are associated with this. But let's look at the, so what, let's look at the impact and the outcomes that we're achieving with all of that uh great work from Chris and his team. On speed and quality. We are moving from hours to minutes for updates while maintaining accuracy and quality. And in a world where financial crime moves at the speed of light, this is really important. On efficiency, we are more efficient with time freed up for our, the valuable time of our, our analysts and our SMEs to focus on deeper analysis, higher value add, deeper judgment problems. And then on scaling, we're looking to scale more effectively and we're seeing this. We're able to expand content capacity without proportional headcount growth, the holy grail of scaling. And then we're detecting content issues earlier in the life cycle, and this really builds trust with clients. Altogether, it adds up to a material acceleration of the value add in WorldCheck, our flagship product. So as we wrap up, I wanna leave you with a few critical insights from our AI journey. At ESEG Risk Intelligence. Firstly, AI augments experts, it doesn't replace them. Our world check analysts remain at the heart of our product and at the heart of the trust that our clients place in us. What has changed is that AI is now handling much more of the heavy lifting of content curation, the toil, as Chris said. And this frees our experts up to focus on higher value tasks, on nuanced judgment calls, for example, that require real deep domain expertise. This all results in faster, smarter, risk intelligence advice to our customers that they can really trust. And then secondly, Successful G AI adoption. Follows a maturity roadmap. We didn't start straight with fine-tuned models as Chris said. We started with simple prompts, validated the results, and we progressively added capabilities. Each level was built on confidence, it was delivered. And there was value before we moved forward, and this incremental approach manages risk while it maximizes learning and value over time. And then finally, in regulated industries like ours, like financial er financial services, AI must be grounded in trusted data with human oversight. We've maintained our commitment to accuracy and compliance. By keeping humans in the loop of that decision making process, which is really critical for us. AI accelerates our process from hours to minutes, but our analysts still validate every output before it reaches customers. The bottom line Is that AI transformation in financial crime detection, it's not about disruption, it's about evolution. By combining Amazon Bedrock's capabilities with our domain expertise and our rigorous governance, we've scaled our impact without compromising the trust that our customers place in us. And Richard will now wrap up for us. Thank you. Right, thank you. Great, so Louis, Chris, thank you ever so much, um, that was, you know, particularly interesting, er both insightful and relevant. So I really appreciate that. Um, the final slide we have, um, we could probably have called it references or a summary or something like that, but the, the call to action is because if you're gonna take away one thing today, what we'd like to convey, um, other than the content. Is what are you gonna do? How are you going to use this technology, will you use the same process, the evolutionary process that Chris has described in order to achieve your own business outcomes, whether you know for the the organization you work for or for your customers. So what we've put on here, we've got some references to bedrock obviously because that's what underpins some of the work that we've done. Um, we also have a reference to the, the use of, uh, knowledge bases for, for RAG. Um, at Amazon we like to talk about their security being job zero, so there's, there's some good references on there with guardrails. And then the final thing, um, in the, in the cluster of four at the top, uh, concerns that we have some GitHub code samples for you, uh, if you would like to try out some of the things that we've talked about. Um, there's also a blog post which talks about the work that's been done today, uh, that's on our partner website, Mesh AI, because that's the, uh, the, the other thing probably to stress, uh, within AWS is that we have, you know, a very, very rich partner ecosystem, and one of the partners, um, has helped EEG. Now, um, we, we've got roughly sort of 8 and 87 or 8 minutes left I think, um, and one of the things that we were quite conscious of is to sort of try and give you some time back because we're conscious that people need to go between venues, uh, and also, you know, you might want to get a cup of coffee or take a nature break or something like that. So, um, all that really remains is for me to say on behalf of the three of us, uh, thank you ever so much for your, your, your time today, uh, we hope you've enjoyed it. And for those people that took your phones out to take the QR codes and some photographs some of the slides, if you wouldn't mind at all, uh, giving us some, some feedback on the, on the survey, um, simply because we tend tend to try and use the data to improve what we've talked about. Um We will, we need to get to other sessions ourselves, um, so we, we won't take questions from the audience, but if people wanna, um, stay back afterwards for those people that do want to ask a question, um, we can hang around at the side for a few minutes. So thank you very much.