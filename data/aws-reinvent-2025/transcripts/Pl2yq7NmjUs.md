---
video_id: Pl2yq7NmjUs
video_url: https://www.youtube.com/watch?v=Pl2yq7NmjUs
is_generated: False
is_translatable: True
---

Hopefully you're all here for this session. If not, there's still enough time to leave. I'm Anna. I'm a developer advocate at AWS, and I'm very honored to be joined by David here. He's an AWS community hero. Thank you, David. Thank you for the for the invite. My name is uh Victoria. I'm from Mexico. Sorry for my bad English. So today, uh, we're going to talk, uh, what are these agentic workflow means and how we can take advantage on the local LLMs. So. First, let's talk about the pain point, the cost of downtime. Aberde research told us that in the average cost of unplanned downtime industrial operations are more than 2,060,000 per hour. So there are a lot of industries affected by this, um, problems. So and I will take the stage on that. Yeah, we'll cover some of them, but there are many others. So mining, you're underground usually. Without any signal, there's manufacturing where you're in remote plants with legacy infrastructure, oil and gas, usually on offshore platforms in many miles into the sea. Energy, government with data sovereignty requirements, and retail. Just a few of many industries that can be affected by this problem. If you want to run AI agents, you require cloud. So what's the cloud dependency problem that we've been talking about? We're at a factory and we want to analyze sensor data and if we have Internet connection everything's OK. We talk to agents through Bedrock and we're good to go, but once that connection goes down, then we have a problem. What do we do? So what do agents require to work offline? So first of all, agents are powered by large language models, so they need to think, they need to reason locally so that we're gonna be running those agents locally. Agents are powered by LLMs, but they need the ability to take actions, and agents do that through tools, so they also need to be able to use those tools locally, not through the internet. They need to keep context between different sessions so they have memory. And then finally once we have connectivity again to sync that information to and from the cloud, all of those things are required for agents to work uh offline. And the solution that the VNI found and we're bringing to you today is using strength agents SDK and Olama together to have the power of agents in the edge. Let's ask Ana to the audience who have worked with strength agents or an agentic orchestration tool. Show of hands. Great, fantastic. Who has used Olama to run locally. Perfect, fantastic. So we're all on the same page. So the hybrid architecture that we built and that we're proposing here today is using strands agent to do the reasoning using tools, having memory, and when we're offline we're using models through Olema and the custom local tools that we created for strengths and when we're online we're back into the clouds with Bedrock. So talking a little bit about Olama, Olama is an open source tool that helps us to run lightweight LLMs on our own computers. In fact, I can run in my old MacBook, uh, small models like 8 billion parameters, uh, queen. But we have a A catalog of LLMs that we can use with Olama. We have models from uh from Google, for instance, uh Hema 3 Mistra AI, uh, powerful models from Meta Lama, basically Deeps see 5 Queen GPT OSS, and many more other models that we can run, uh, in local, um, facilities. So Olama will be giving us the power of the model. But agents require not only models. There's this whole thing that we call the agentic loop where it performs a bunch of steps until it's happy with the response and it sends it back to the user. So how does it work? First part is the reasoning part where we receive the user input. We process that input with a large language model. And then the model decides if it needs to call additional tools or if it's ready to send a response or not. Then we have the actions, which are the tools that the the model can call. With these actions we are going to send and receive inputs that can accomplish the task that the agent is trying to finish and if the agent determines that it's ready to respond to the user, the agent responds the final output to the user. So it's why we call it a loop. It keeps iterating between reasoning with the model and actions with the tools until it's happy with the response. The tools help add additional context to the model and then it gives the response back to the user. And that's why I say that the the agents are loops with superpowers. So write these loops, superpower, super powerful loops, it's hard. So that's why we want to introduce ourselves as Strength agents. Strength Agents is an open source Python and and since yesterday also TypeScript, SDK for building agents using a few lines of code. Basically. Strength agents is a solution think for builders, so people that actually write code. We are not reinventing the wheel. We are only putting. The same capabilities as a builder or developer needs for agentic purposes. Yes, and not only it's easy to use trends, we're going to show you the code right now, but it also is extensible, so we can add tools, MCP, memory management, a lot of different things to make our agents more powerful, and we're not only using Amazon Bedrog LLMs. In fact, we can use Olama. We can use first party APIs like Anthropic API, Open API, and other vendors like Grok, etc. So it's a very extensible tool, and agnostic, let's say, for a single vendor. Yes, so we create the model, we give it the tools, all the information that it needs, the capabilities that it has, and we can simply switch to different models if we want to. So let's start reviewing a single agent, the most basic agent. First we have to import the library with import strands and then we can instance an agent with the default features. What is the default features basically, um, Amazon Bedrock with clotsonnet 4.0 in the Oregon region. That's all. So after that we can start, uh, working with an agent, sending a prompt, receiving an answer, accomplish this powerful agenttic loop. So this is the simplest agent that we can create using strengths. If we don't define a model, we're using the default one. But how about if we talk about an agent that works locally, so we need to import only the Olama model library. In this case I'm importing some tools that are um that I create before for my workload specifically for instance I create a tool for read sensors trigger an alert, etc. uh that that custom tools are created and we're going to talk about a little bit later about how the tools works but let's continue with the code. Great. So now I am creating an agent but setting the model, uh, isolama model where. Uh, we have running now Olama and our host. We define the local host, the port, and the model ID. In this case we are using IPSI in the 8 billion parameters model. So in the previous example we're not specifying the model. Now we're specifying this model through Ola. And for sure we need to include the tools that the agent will be able to use to accomplish the mission. The system from basically what the model needs to accomplish, what is the main task that the er models that the agent will work with. Yeah, I like to say that it's the model's personality. One important thing that this system prompt have is that we are indicating that this model is that this agent is it's working fully offline so the model will not try to reach internet or to reach external source or features. At the end we only start working with the agent sending a prompt in this case checking our sensor and alerting if uh anything is wrong, how using the tools that we have previously defined. So here with less than 20 line of lines of code we have a local model using Olema and custom local tools that our model can use. Perfect. So our agents need to take action. So how agents take action outside the world, that's the the meaning of the tools. So how a tool works basically. The agent use a tool to make an action externally. I always say that the uh tools are like the the small handies of the agents because have this powerful to extend the capabilities outside the scope of the, the single agent. So how, how, what kind of actions we can take, for instance, in this case, in this specific workloads, with sensors, analyze images, query some databases, create work, uh, work orders, send alerts, or anything that we can code, how it works. Here, we're gonna talk about things and then show you the code. So, here's how we create a Python function that we can turn into a tool. Same as before, we're importing the agents and now the tool from from strengths. Creating the the function to read sensors and then we have some important info here in the documentation. No, yeah, that's right. So this function, uh, this function Python function read a sensor we receive the sensor ID and we have a local endpoint to trigger to receive, uh, sorry, pull some data. So this gate to a URL we are using request, uh, as a library to get uh the data from the URL and that's all we receive the data from that, uh, mod bus TCP raise for status, wait for that. Have the answer. Create adjacent to return or a dictionary to return in the function, but the most important thing here is not the function itself. It's the decorator, the tool decorator. With a single decorator we are creating this specific, uh, function. We are converting this function into a tool that an agent can use. One important thing, Anna, duck string, it's the way that we define the agent, what is the purpose of this, um, of this function. In this case, the duck string says that, uh, you can use this for, uh, read real-time data for industrial sensor, and the arguments that you have to send is the sensor ID, and that's all. The agent with this dock string will identify how to create the invocation process, the execution process of this tool. So the dogs are super important, so the agent knows not only what the tool does but what information he needs to call that tool. And apart from custom tools like the ones that we just showed you. Strengths has a set of tools that are ready to use for the operations for the actions that we most frequently use. So, it has an HTTP request one that we can use to query APIs, talk to sensors, has system ones that we can also use to run local diagnostics. Workflows we can even give in to a human in the loop when we need more critical approvals, and this is super important because you know agents can work autonomously, but the best practice indicate us to use human in the loop to validate that capabilities. And file operations. So, here my personal recommendation to you is to check the strengths documentations to see the ones that you have available because I did create a custom tool to save files and then I figured out one already existed. And there are many more. I mean, there is only a subset of tools that we find valuable for these warlocks, but there are many. More community built tools and one more important thing about community tools, remember that red agents, it's an open source solution so you can make a for, create a tool, open a PR, and the the strength agentsten team can validate that tool and maybe include into into the next release. Perfect. So let's talk how we can interact with other caveman systems. So we have our agents running in the edge, and even though we're used to now having G AIs and chatbots and models wherever we are, there are still systems that require all systems, yes, more primitive data. So that's why we have the structured output, uh, as Ana mentioned, we have some ERPs, CRMs, MIS, etc. that, uh, talk with other with other systems using let's say APIs. Hopefully maybe a lot of these systems use rest APIs, but other other systems still use soap envelopes to communicate with. So we structured outputs how it works basically the agent can receive. Byanic model. Byanic model is only a thing to define a data structure. For instance, we have a a set of data that we we need to send to the API so we can define the specific um JSON format, let's say. And then we can send the Pythonic model to the agent and receive the specific Jason as our answer without any other data, you know, models are very verbose, yes, so we don't want to get a bunch of texts and send that to an external system that's expecting an API call with specific data. That's why we are using structure output andres has a really nice way of doing that. So we, we're using pedantic here as David mentioned, and we're creating this model for a work order. So here we're specifying the fields and their types, and we even have a description so the model can have additional information on what those fields are. So we have a part ID, the priority, description, and estimated hours. And that's our structured output. And now when we are interacting with the agent, we Send the agent that exact information so we can answer us with. The data type that other systems are expecting in this example, uh we are um talking with the agent that the compressor have an abnormal vibration that agent maybe have some tools to analyze that information and with the structured output we're creating adjacent or a dictionary with this with that specific uh fields in the exact order using the same uh um values that we are expecting too so. After that we can validate that results, get into that variables directly, uh, as part of the, the code, and at last we can send this information to an external system without any intervention. Automatically the agent is capable to create the JSON and send into the uh the API. So instead of sending a very large text saying probably excellent question, this is your, what you're looking for, we're sending the specific data or external systems need. Fantastic. So up until now we talked about tools which are how agents take actions. We talked about some community tools and we talked about how to structure our output so our agents can communicate with external systems. What else? What else do we need? So our agents, we don't want them to forget things that happened, right? So if a specific machine required maintenance or was showing an abnormal temperature or something. We want the agent to remember that. So we have session management out of the box session management with persistent memory with strength agents. So in the case without, without sessions, maybe we are, we are interacting with the agent and asking about check on pressure or the agent and the agent found some, some important data. But maybe the app crashes. The legacy hardware that we are using, uh, power off. There's a power outage. Yes, so if the app starts and you ask again, they just will say. What are you talking about, man? I don't, I don't know what are you talking about. So like me yesterday at the omnia, we don't want that to happen. So with sessions it's a whole different story. The user asks the agent to check on the compressor, and then if the app restarts. The agent can keep the conversation going. Great. So how it works basically, we only need to import the library. So, um, we import file session manager. It's important to notice that, uh, one agents have other ways to manage the sessions like Dynamo DB, S3, etc. But in this case we don't have connection. We are not going to use that tools. So we are using the file session manager. We only have to define the session ID that can be a unique ID. Uh, per user per unit per machine, etc. and we are going to store the data. In this case we are storing the data in JSON format locally in the same computer that the agent is running. And the agent will receive the session manager instance, and that's all. The agent will be able to uh communicate and remember things across the context. Yeah, and here we're using the local file system but we could be using SQL or Reddit there or if we're connected to the cloud other solutions. At last, model context protocol. This is a well known protocol created by Anthropic last year. We are now in the and Model context Protocol is basically a tool that, uh, other vendors can expose to you. You can, uh, install or instance in your strength agent, uh, agent, and automatically all the tools and data sources that that MCP, uh, describes are available for you. What do we have in the code? Let's check. So we have our imports as usual. And one thing that I appreciate about rends is that we can import those tools from the library and use them, same as we did for the session management or for the custom tools. We don't have to be creating things ourselves. We import our custom uh tools and in this case I am uh importing or calling an MCP server from MongoDB. Notice sorry that at the end of the arguments we have a local host URL URI, so we are invoking a a Mongo collection locally. So if you're using MCP servers on your coding assistant or on cloud desktop, you're used to that type of information. But if not, we just need to say which how the The model is gonna communicate with that information. Here's locally, so we're using UV and then the the arguments for MongoDB. After that we need to have to import the full list of Mongo MCP and then pass as a tool into the agent. So MCP is no more than like tools exposed by third party vendors that you can import as a custom tool. Yes, but here instead of David and I creating custom Python code that would connect to our local Mongo DB database and get that information, we're using the MCP server that already exists for that. And we can keep talking to the agent as we've been doing so far. So in this case with this Mongo MCP server we are able to communicate to the the database, uh, show collections, insert, um, uh, attributes, insert data, modify data in uh we can delete data too, so it's important to implement human in the loop process to validate if the action that the agent going to take it's the best path to accomplish the task. Yeah, and here we're saving log information to Mongo DB. OK, so we showed you the code. Now it's time for you to see it running. Great, who knows Quo? No. The people that know that don't know Kiro, it's under the rocks, or? Just joking. Quiro is the new agentic idea from AWS. It's an amazing tool to build solutions. Let's open. My Perfect. Demo perfect. So the reason why we talked about Hiro is because Hiro helped us build this demo today. 100%, but spec driven development, not vibe coding. So this is a Jupiter notebook. This is not the final demo. This is a baby step process. First we are going to. Review how it works. Every of these, uh, capabilities that we, uh, talked previously. This, uh, Jupiter notebook will be available in our GitHub repo, so you later will be able to, uh, interact with them. Yeah, we have a QR code for that on the next slide. Great. That's a start. So we are instancing a, uh, Olama model local host. Notice that we are using a custom model. A community model. Why? Because Quant 3 is an amazing model, but has the thinking process and think a lot, so thinking takes a lot of time. For demo purposes we are using a not thinking model to speed the capabilities and works very good. I mean, accomplished all the tasks that we have in the demo. Full disclosure here that David is off the internet. That's why his trends server is not working. So We are using this uh model blah blah blah. OK, perfect. The Olama model is configured first tool decorator. We have here, obviously this is mock-up data because we don't have IOT devices here, but imagine that you have some devices with some data that we you can pull in production like temperature sensors, humidity sensors, value actuators, etc. I am going to create a reed sensor tool uh sorry function but with the tool decorator now this function it's able to use by the agent. So, same as we saw on the slides, we give the documentation information to the agents so he knows what those tools are going to be doing. And how to call them. On the other hand, we have a control device, basically a tool that indicates a specific IOT device that we can interact with to take another action. So, I'm going to run this piece of code. We have these 3, these 2 different functions ready. And we have 3 devices available temperature sensor, humidity sensor, and valve actor. How we can use this tool with an agent. So basically we create, we create an agent, we set the Olama model that we previously instanced. We inject the tools, the two different tools that we created before, and in with a system pro the LLM personality we, uh, define what this agent will accomplish. So it's an IOT control agent for an industrial facility and it's gonna help users monitor the sensors. I can ask to the IUT agent, what is the current temperature of the production floor, so we indicate what which sensor I need to analyze. And now The agent is answering me. The current on the production floor, it's 24:19. Let's control tractor. The IOT agent will receive the open the cooling system, the name of the valve that needs to open. And now it's taking action. It's calling the tool to take an action and open. The system evolve. Another amazing capability that that agents bring to us it's the multi-process, uh, multi-step operations. For instance, in a single shot operation you send a prompt and the LLM response. That's all. In the agenttic loop, the agent receives a complex or maybe in this case two different, um, responsibilities or operations, and the agent will accomplish every task and after that will answer you. So with the multi-step operation, I mean, I ask him, check the humidity first task. And tell me if it's in an acceptable range. 2nd task. Reed sensor uh the agent used the tool with sensor uh got the storage area humidity and then validates with a second tool. If it's an acceptable range. Great, this is the first. Let's check the demo number 2, the structured output. There are a lot of Scada systems integrated in industry, but a lot of them creates this. Let's run this Create these awful responses basically text TXT files without any parsing process uh basically it's for printing and uh human readable but the system need to understand and get data from this scatter response so. We have here different Byanic models, sensor reading, uh, Byanic model, alarm info, equipment status, and production metrics. Every of, uh, every one of these, um, models have a specific data definition, uh, not only in naming pro in the naming but in the data type. I'm going to run this. So, we are simulating a scatter response. Basically this TXT file and then with an uh small agent er that have a data extraction task we can uh get. The structured output. Of the Scala report. Basically, with production metrics, we er um extract successfully the data and now. We can access individual fields from this structured output. We can use that structured output, as we mentioned to send directly to other. Systems. Can, can we go back a little bit on the, when we talk to the agent when we talk to the agent here, so when, when we say that the structure output model is the production metrics, that's when we're telling our agent to use that pedantic model that we defined and then give its answers on that format that we are, we are expecting. So, the 3rd session management. So, we talked about tools, we talked about structured outputs, and now how we're gonna manage the session and the memory of the agent between interactions. In this example, we are defining the session the session, the local storage directory. Cleaning if it's previous. We are simulating a session ID in this case each operator 001, We instance the file session manager. And then We can start interacting with the agent. You're an edge device diagnostic assistant blah blah blah, and we're giving the session manager when we're creating this instance of the, the agent so we know where it should be storing uh session information. We have a first interaction. A second interaction. And a third interaction. Requesting the summary. We simulate a failure. And then Let's recover From that error. We have a session ID, the total messages that we previously have, where are the sessions? Where are the messages. And we can Start talking again. Without any data loss. Continue the conversation. Operation continues until after we start and It broke. Do you want all the time. Do you wanna use hero to fix it? I don't have internet. Oh yes. So But the idea is that the session manager here you could be using a SQL database, but here we're using local file storage you're gonna be storing those messages and then you can restart your conversation with the with the agent. The last capability is the integration. Very quick, we are going to create a. A SQLite database edge telemetry. 2, because it's my 2nd test. And we are including the MCP server SQL Lite directly as a tool. Here Um, the list of tools. I will run this. So to the agent that we are creating with strengths that MCP server is another tool. Exactly. And now we are listing the tools that the MCP have like read query, write query, create table, etc. And we can start using those tools in a new agent. We simply passing the tools. In the instance of the agent. And we're creating a database. The agent is using the create table, the described table tool. And now we can. Insert each device data on this SQL lite. With natural language we are telling the agent to insert some data. And the agent It's able To write the correct queries, read the queries, try to insert. And at the end, 5 telemetry records have been successfully inserted. Let's query that data that we recently um write. We have an agent, the tools, we are asking the agent, show me all the telemetry records in the device, telemetry table. And we use the read quality to tool to review every uh row that are that are in the table. Analytical query. We're asking with natural language to calculate the average value for each device ID in the device elementary table. So if I don't remember what's the SQL query to get the average of something, 2 years ago I wrote my last query line code. So the MCP server was capable to read the structure of the table, create the specific uh query that we need to run to accomplish the task, and that's all we have the results. Ready to use. This Jupiter notebook is available in our GitHub repo, but we also have in the GitHub repo. Our edge operator agent, so that was the demo for learning purposes. Now we have, uh, another demo that's more interactive. Oh, it's using my same session so it's recovering the data from the last session. OK, at least here we're able to show the session management in in action. So I will ask I opened a new session ID check this. So I can start a new conversation. So the agent answered me that the temperature sensor value is 426 Â°C and the time that was recorded was actually now. So we can. Ask about Some other. The state Let's close some valves. And in the back. This agent It's, it's calling tools basically. But on the front end we are interacting with a natural language. What else we have? Perfect. The humidity sensor. So we are running locally with Olama, but let's check. Perfect, we recovered the the the Internet. What happens if we open a new session or no, the same session? Only New check, we're going to start working with cloth. And now we are talking with Bertrag. So notice the the latency. It's very different than the local ulama capabilities. So let's ask again. Which devices. You have. So it has access to exactly the same tools. It's the same agent with a different model. Before we were running it locally on the VS computer and now we are running a remote agent uh through through Bedrock in the cloud. And basically it's a single uh change of line. So instead of uh calling uh the Olama model we are calling the default er bedrock agenter LLM. So This is the demo that we have for for attendees, but. We have for you This repository not only the Jupiter notebook but the but the full streamlet UI and the agent running on on the edge. So what else Anna? I think it's since some of you don't know Kiro, I think it's a good opportunity to give a brief overview of Kiro for sure if you feel comfortable doing that 100%. He don't have to to have the capability of accomplished that. So Are we in the demo? I think one of the first things that we wanna talk to you about, um, usually when there's less Zoom you can see a little ghost here that's hero. And same as we're using some of you might be using MCP servers with cloud desktop or you're coding agents, we can connect at different MCP servers to to hero and there's an MCP server for strengths. So usually we have this problem when a library is too new or when it has constant updates if we talk to the models that have a cutoff date on their training. They're not gonna give us up to date or accurate coding recommendations for the library, so the strands agents MCP server really helped us in this 100%. In fact, without this strand agents MCP server, Quiro will be unable to accomplish the task. Yes, so most likely when the model that Kiro's using was trained, there wasn't strengths SDK information available. So highly recommend you to use it. You can enable and disable them so the model knows better what to do and not to do, and you can just ask questions about the, the strengths documentations for sure and not only for strength documentation, but for AWS AWS have more than 40 MCP servers. One of my favorite, it's the AWS documentation MCP. So basically we have the 100% docs available in our LLM Agentic uh ID. So we don't need to review the documentation because really it's up to date. It's query the the real time documentation. And I don't know if you use steering here, but steering docs we can create a steering dock just telling Kiro whenever you're creating strengths code, please check the up to-date documentation on strengths using the MCP server. So it's instructions that the agents are going to use whenever we make a request. It's directing Kiro on what to do. I have here some. Uh, steering This was for the road to bring the hackathon. So basically I prepare three different steerings to define what uh we are going to do, what kind of technology, the structure of the, um, my whole project, the technology that I want to use in this case I opt to use strand agents, of course, strand tools, agent SOPs, the MCP SDKs. And all the AWS services that I try, I, I'll try to, to use on that project. So similar as we were saying that our agents in the edge need session management and memory. This is the way of giving memory to your coding assistant so it knows what your project is about, the folder structure that you're using, the technologies. So just wanted to give them that quick uh hero intro since some of them didn't know and really really powerful agent hooks another capability. I always for every change that Quiro uh creates I am commit creating a commit automatically because Quiro handle all these uh capabilities that creates a kid commit, uh, in every uh in every task that Quiro a complete. So I think. We can return to our Session here. And as a wrap up, Download the uh ripple. Check the ripple if you have ideas, if you have uh some uh challenges that you are facing. Contact us. We are more than happy to help you. We are more than happy to, uh, describe these challenges for you and help to, uh, find the best way to create agents not only in the edge for, uh, uh, but in the cloud too. So Don't wait for your cloud to think, but Use the cloud to think bigger. Thank you so much, Tim.