---
video_id: DF8sK2xkhgQ
video_url: https://www.youtube.com/watch?v=DF8sK2xkhgQ
is_generated: False
is_translatable: True
---

So imagine this, you are part of a team that is responsible for maintaining a mission critical system inside your organization. And this system was so important for the company that enables the company to scale to hundreds of developers working together in this massive application. Obviously, uh, yes, there are some, uh, benefit of doing that and, uh, having, uh, uh, all these, uh, great work done by developers, but there are also some challenges. So now instead of deploying like you were doing in the past, uh, multiple times per week, now you deploy once a month and maybe the testing life cycle takes tens of minutes before are fulfilled and sometimes when you are, uh, deploying you need to roll back because there are many issues. Does it sound familiar to some of you? Cool. Um, it's for me and also for me, so I work a lot in, uh, uh, this kind of application, and I understood the benefit of leveraging distributed system where needed because, uh, an architecture is not forever. And during this session we are going to see how Booking.com have done through this journey of modernization leveraging serverless in order to overcome the challenges that I described before. My name is Luca Mezalira. I'm a serverless specialist solution architect, and today with me there is Sarah. Hi everyone, my name is Sarah. I'm a solutions architect based in Amsterdam, the Netherlands. I am passionate about observability, resilience, and surveillance. And. Hi, I'm Zishan Pervez. I'm engineering manager at Booking.com. My team was responsible for modernization of Legacy, uh, 20 years old Legacy reservation system at Booking.com. Perfect. So let's start, but before we start, we need to understand what distributed systems are. Very often you see a lot of great definition, but for me the best one is this one. Distributed systems are living systems. If we enter in the mindset that the distributed system will require attention, care, and we start to iterate over and over again, challenging sometimes our decision, we are halfway through to the success. Usually when you embrace distributed system, you want to reach specific goals. The first one is organization scalability. You want to have the possibility to plug a new team uh or a partner inside your system to accelerate the delivery of specific function potentially. You want to have business agility, so if something happened internally or externally or inside your industry that require a drift towards a different direction, you will be able to make it because you just need to change uh a part of the system. You also want to have a faster feedback loop. I think, uh, many of you have heard about Dora metrics, and these are metrics that enable to succeed in this very complex world. But more importantly, you want to reduce the external dependencies for, for teams because the reality is when you work in a monolith and you have hundreds of engineers working together, you understand that. At some point you have to slow down because there is a lot of coordination, so you want to reduce the external dependencies you want to decrease the cognitive load, and finally you want also reduce the blast radius. So when you deploy your application or service, if that service goes down, it doesn't impact the entire system like it happens in monolithic systems. And I think in order to achieve that, you need to leverage modularity, but let's try to understand what modularity means. It's the quality of consisting of separate parts and when combined form a complete whole. But most importantly, module a system lacks of modularity when a tweak to one of its components affect the function of others. This is not something uh that you can find on, on the tech side. This is from Cambridge Dictionary, it's the definition of modularity that is applied to everything, right? So modularity in our case can be expressed in different ways, maybe using code, so you leverage a strong encapsulation, you use design patterns that we have seen for decades working pretty well, uh, but then at some point when you have, let's say, a decade of software that is there up and running, these kind of things can start to be diluted. Uh, you decouple the business logic from the environment, maybe using hexagonal architecture, but you need to be disciplined in the development teams, and teams sometimes are changing. People are leaving the company, moving to other teams, and so on and so forth. Therefore, modularity using code can be done, but it requires a lot of discipline. On the other side, you can apply modularity on the infrastructure level where you have more option to uh express your intent as an architect. I want to use. Synchronous and asynchronous programming when it's needed because that enables me to have a more resilient system and also a scalable system. You, uh, configure more uh than than coding so because there are a lot of built in behaviors that are available inside server system but more importantly, you have more control on what you develop because you develop only a chunk of the system and there are certain behaviors that are already built in inside your service. Because what we think in AWS is that serverless change is not more associated to lambda, but it is a strategy, a strategy that enables you to uh remove the undifferentiating heavy lifting that we take the burden on and enables you to maximize your business value that is not writing code but generating value for your users and your customers, and that's for us what we do day in and day out. Because when you embrace serverless, what you focus is uh just a little bit of defining the API and defining your business logic. All the rest, it will be taken care by us. You select the right service for uh for the job and you will be good to go. So what we are witnessing with customers, and I'm uh let's say I usually touch between 50 and 100 customers every year, uh, a paradigm shift in order to win new customers for them, we need to build better products and therefore it means we need to innovate more often and be. Advanced compared uh our competitors, we need to release features faster because that's the only way to test our assumption and therefore the Dora metrics comes very handy in this case and therefore you need to focus more on the business lodging and the couple software systems so serverless seems like the perfect fit of what you need to to do. And Surless very often is uh uh thought in a way that uh oh you can do only the driven architectures, but it's not true witherless it's just a tool you can express whatever architecture you need. You want to be point to point using microservices and HTTTP response, you can do that. You want to use even driven architecture because a part of the system like we are, we are going to see later on with Sara uh is benefit from this approach. You will do that. You can. Use also for data architecture. Very often I have customers that are using serverless services for ETL or for, uh, let's say drawing data towards the data lakes, and you can also use it at the edge of your architecture and its integration patterns that I would say is one of my favorite ways to use serverless because you know better than me that SAS products very often have a lot of uh quotas that we need to be aware of, and you cannot just hammer it every second with the same throughput that you have inside your system. So these kind of things are, uh, let's say patterns and architecture you can use in servers. But now, without further ado, let me introduce you to uh Xian that is going to start the discussion around how Booking.com embrace all this goodness. Thank you, Luca, for explaining the foundational concepts of Serless, um, and the associated architecture patterns. So my name is uh Zishan um and for the last 3 years I've been working at booking and um the target of my team was uh only one thing to modernize our reservation system which was 20+ years old. It was a mission critical system, uh, but before we start. Let me ask one question. How many of you have booked a trip through Booking.com? OK, I see some pants. Thank you. Thank you for choosing our platform. Um, at Booking, we believe life is made of experiences, experiences we share with each other, experiences both big and small, and that is why millions of customers each day choose Booking.com as a platform to experience the world. Therefore, our mission is to make it easier for everyone to experience the world. At the core, it is powered by Booking.com's mission critical reservation system, which handles millions of reservations each day. At scale, uh, to stay competitive, we need to deliver features faster and the, and the features should bring some value to the customers, uh, but also quickly and reliably, um, so that they can enjoy their experiences. However, engineering teams are under constant pressure to improve, uh, the time to market or reduce the time to market, uh, and this puts them under a lot of pressure, apparently not so easy for the engineering teams. So before I go into the why part of why we modernized, I would like to talk about some of the things that were really stopping us from delivering the features faster, uh, at the pace that the business demanded. So let's start with the first one. Booking.com is a monolith as as it began in, uh, it was written in Perl. Uh, when it started, it was a small, cohesive, and easy, uh, uh, repository. Uh, it was easy to build, easy to test, and easy to deploy, but as features and teams multiplied, cross dependencies grew, uh, builds slowed down, uh, tests ballooned, and, uh, they were, the boundaries were blurred. Tight coupling made changes brittle. Deployments were becoming painful, and debugging became a nightmare. So overall, it culminated it into a big ball of mud. Teams then faced a choice keep patching a fragile monolith system or modernize by splitting it into services. So before we look into how our team solved the challenges they faced, uh, let's speak into a bit of what specific problems they were facing. So our legacy system had 90+ dependencies, which included services as well as database tables. More than 80% of those dependencies were hard dependencies. That means like if there was any failure. Uh, there was a cascading effect on other services as well. Uh, this oversized web of dependencies resulted in weak modularity that was, uh, that Luca was also referring to. So, also there was a leaky boundary in data layer that internal data model was exposed to other services and um this caused services to erode encapsulation. And since, since it was a tightly coupled Pearl monolith, uh, developers, um, added code wherever they wanted and wherever the data was available, so this introduced coupling creep. Um, also there were shared data models which were used across different services. As a result of this excessive coupling and wide blast radius, a single outage would cascade, uh, and the modules can't degrade gracefully. Another challenge that we had was domain divergence reservation workflows and data structures, uh, accumulated logic from different domains, for example, pricing, commission, cus customer service, messaging, fraud, to name it a few, uh, and the, the, the domain boundaries were blurred. For example, in our reservation workflow, commission was being calculated, uh, which could be done independently. Uh, and in another example, reservation system was calculating charges and then writing them into a database that was even not owned by us, so it was outside of reservation domain. So many of these problems that we discussed um gave rise to either shared ownership or no ownership at all. Uh, the team frequently came across problems in terms of finding the right owners, and we found a lot of dead code which was left uncleaned. This made it difficult for the team to perform root cause analysis during outages and find the owners who knew what the code was doing. Um, our system was running on bare metal servers which came with its own main points. It was a bundled logic in one giant service. Uh, changes in rollouts were risky, and, uh, there was a lot of time spent in maintenance of the servers, for example, applying security patches, faulty server replacements, or updates. Regular manual capacity checks were required to ensure that servers are configured correctly and they can handle the load. Well, though somewhat automated, it was taking a lot of time uh that we could easily use um in, in building some new features for our customers. So these were not just technical problems, they had a profound impact. On our team as well, so let's look into how our teams were impacted with this. Teams spent more time dealing with issues related to ownership, uh, which was not a fancy topic. This led to burnout, low motivation in the team, resulting in disengagement. Um, on the other hand, it took us 4 months to onboard a new team member, uh, and, and even before that, uh, it took us like 50% of the time to just give them 50% knowledge of the domain. And there was not enough, not enough documentation, and the system was highly complex. This high complexity, unclear ownership, mix up of domains, uh, resulted in bugs and outages which increased the KTL of the team. So yeah, there was high KTLO. All of this combined took significant energy of the team which could have been spent building new features. So the team reached a point where they decided this cannot continue anymore. Uh, Around this time we also received a big list of features that we needed to implement, and the team decided that the only way to sustain and the only way to to be futureproof is to modernize our system. So we started looking into our future needs and capabilities required to ensure that we remain fit for the purpose. So based on the team's learning from the hands-on experience that they had in the reservation domain, uh, we listed down some problems that the team was experiencing, and then we prioritized from those lists a couple of things that we think like we really need to work on. Uh, some of those things were, uh, isolating unknown code, defining boundaries, removing coupling, uh, and then with this prioritized list of, uh, things that we wanted to fix, we come, we came up with the North Star architecture. The North Star architecture or our target architecture was aimed at solving problems related to unclear boundary, coupling of code, and reducing unnecessary dependencies. We came up with a high level uh architecture which was platform independent, so we didn't think about moving it to the cloud at that time, but we but we designed an architecture which was mo modularizing our things, and we were, we were looking at different approaches for that. So once we had this architecture, this was reviewed by a lot of our internal architects, including cloud architects, as well as some principal developers. Um, during our discussions, there was an inclination, uh, that the future architecture of the, uh, um, should go through the transformation phases instead of going from the first one, and then stepping to the second one and then stepping to the third one. We should really move from the first one directly to AWS managed services and reap the benefits of the cloud. Um, one of our requirements was to have isolated flows and modular code. So when we looked at AWS, there were, uh, the, the step function and lambda offerings provided what we needed. Lambdas can break logic into smaller reusable functions, and step functions can orchestrate these reusable functions into clear isolated workflows. So all of this while providing out of the box functionality such as observability, uh, um, and monitoring. So overall inclination was to pioneer. Uh, use of serverless with a mission critical workload, uh, but the problem was that our team had no prior experience with the cloud, uh, and, and, um, let alone, let alone serverless. So hence it was not clear for the team how to proceed on this one. So despite promises from the leadership that they will support will be provided, the team was a bit skeptical about Serveus. So some of them said, let's not take the risk, it's too risky. Some didn't have the opinion about it, and some said let's go do it. Some of the assumptions that they had was it will take too long to to learn AWS lambda functions will create some performance bottlenecks. The cost will be too high, uh, it will be challenging to connect to our booking internal infrastructure, and why choose a mission critical system for such a, such an experiment. However, as an engineering manager, I made sure that I saw a big opportunity in this, uh, for learning for the team, uh, and I asked the team, let's, let's do something different and, uh, make sure that, uh, we get hands-on experience on AWS. So also it was a great opportunity for our internal cloud teams to learn from a highly critical mission critical workload, uh, and to see that how, how, how this, uh, workload works on several of us. So after a lot of team discussions, one on ones, uh, and inspirational speeches, I had to write the first lambda to inspire the team. And following that, the team started working on a proof of concept, which I'm just gonna show. Uh, for the proof of concept, we, we chose a very simple use case to test a request coming from AWS Cloud to one of our services on-prem. Um, as you can see, it's just a test call. Uh, the lambda calls one of our internal dependencies, and then it gets some data. What we wanted to test was how it performs in terms of performance and what is the cost going to be in, in with the volume of requests that we are trying to to create. Um, well, when, when this worked, this boosted team's confidence, and one of the things was clear that it would be challenging to connect to internal infrastructure of Booking.com. But we also saw very good output in terms of uh managed services um for um there were some things lacking in this uh this proof of concept that that we came to know later for example, we didn't we we were not able to test multi-region deployments, um, how it connects to other internal tooling, for example, incident management, service to service integration, but at least we got the confidence that if we move forward it's gonna work. So with the learnings from the proof of concept, the choice was made. We are going to modernize using AWS service list, and the proof of concept will uh give us this confidence that uh the selected, the selected solution is going to work for us. So the team decided to go ahead and and create a surveillance architecture, but before we look at the architecture, what were the key takeaways from from the proof of concept? We saw that we would, uh, the team was able to focus more on the business logic because a lot of the operational stuff was handled by AWS. Uh, we noticed that our team can save 5 to 10% of their time. Um, spending and managing servers updates and things like that that, uh, that we can save, um, and then integrating with other AWS services, AWS services was seamless. For example, we connected with stop function later on. So the team spent some time and formulated a final solution on Servers, uh, which was reviewed and approved by our cloud architects. Now I'm gonna explain what the solution looks like. On the left side you see, uh, booking infrastructure. On the right side you also see, uh, booking services, and in the middle is the new solution and the new reservation system that we built. Um, one of these lambda functions triggers a step function you can see in the middle, and, and the step function internally had 15 other lambdas which were doing different things. Uh, we also made use of dynamma DB to store the request temporarily. We put a time to live on that so that the database size is not increasing, but this made it easier for us that through, uh, within the step functions we were able to access data from Dynamo DB from uh from the request whenever it was needed. And we would only select the the the uh the part of the data that was required for us. Overall it created very flexible decoupled architecture. Uh, for example, we also see at the end that we have used SNS to fan out to different, uh, notifications to different SQSQs, and, uh, and then, then there is another lambda on the other side which is asynchronously reading from this queue. So basically decoupling the system, um, so over, uh, overall this was, uh, this was the architecture that we created. Also, I would like to highlight here a step function execution. Um, so if you just focus on the green blocks here, uh, the one, the blow, the big block at the top is a parallel execution of, uh, all these lambdas, and then you can see that after that execution is complete, then we call another lambda on the left side which further calls another parallel block. But what is also important to see here is all these lines which are catch blocks basically, uh, and this comes out of the box. So basically the error handling comes out of the box and step function which saves a lot of time because we don't have to maintain this code anymore. So it wasn't a smooth journey. We encountered a lot of problems, but we wanted to solve them, and, uh, we, we, we showed resilience, uh, and they were systematically resolved. For example, one of the biggest problems that we had was lambda latency and cold starts. So I'm going to explain a little bit about the chart that you see on the top. Uh, this is a latency chart that we, we did an experiment with different configurations to find which is the most suitable configuration for our use case. So on the left side you see provision concurrency with 0.5 GB of memory. In the middle you see provision concurrency with 1 GB of memory, and then on the right side we use snapstarts because we were using Java. Aliss offers snapstart as a solution to warm up the lambdas, and provision concurrency also offers the same opportunity, but we saw that the configuration in the middle was something that we, we can rely on. So we, we chose provision concurrency as our configuration with 1 GB of memory. But if you see the graph in the below that, you will see that results were still not good enough for us. We had, uh, we were, we started at 3200 milliseconds, which was, uh, which was what we were getting as a, uh, a latency in our lambdas. But then you see like in the, in the bottom it says SDK optimizations, Jakarta optimizations, and other optimizations. This is where we optimize the code to work with, uh, to, to warm up the objects, uh, in Java. So that we, so that the the Java initialization time is reduced and then uh then we get the same effect as if the lambda was warmed up this reduced the latency to 140, which was, which was very good. So when the team started working on modernization project, uh, we had, we encountered another problem. We had only one environment to work on, and that was just our development environment, um, and we had 6 people in the team, so we had to, we had to work with our cloud team to come up with a new, uh, solution for creating personal environments within AWS. Uh, they worked with AWS and delivered us a solution. Before that, the team was time slicing and basically there was a lot of communication overhead. Um, so we, because we adopted this solution early on, there were some problems in it which further, uh, created problems for the team to, to pursue further, but over the time the solution matured and we expanded from one team to 3 teams, and at the end 25 people were using this, uh, environment. Some of the internal booking tooling was not ready to to to to to set up the handling witherus. There were some key features missing. For example, there was no multi-region deployment. We didn't know how to do that, and there was no clear strategy on how to do canary deployments, rollbacks, hot fixes, feature flags. All of these are essential for a good operations. Um, also, our compliance team was a bit resistant in approving our requests for production environment because this was the first time we were doing surveillance, and they were, they wanted to make sure that there are controls in place. So this, that, that took us a couple of months of our time uh in back and forth explanations of how it works and what is our use case. So we had to address all their concerns to do that. At the end, we wanted to test what we build. We took a little over a year to build the architecture that we mentioned. Most of the, most of the time was spent on understanding the dependencies. Uh, we had some capacity constraints, and then there were some internal alignments, so it was not the development that took the whole time. But once we had the basic use case developed, we started thinking about how to test this, how can we make sure that this is the solution that is working, that's going to work in production. So we decided we will go with the shadow traffic approach. Has anybody here heard about shadow traffic? I see a few hands. OK, good, um, so shadow traffic is an approach in which you can find gaps in functionality, uh, when you modernize our system. So as you can see in the diagram, essentially we create a copy of the request coming to production and forward it to the new service and compare the outcomes. We also followed a similar approach as you can as you can see, we, we, there was a media request created which went to our modernized back end and at the end, uh, the output from the legacy and the modernized back end was compared by our data quality checker. We created some script for that, and that script generated an output report which was mentioning the mismatches and based on that information we were able to find where where there was a gap between the two systems, um, and, and there were so many upsides of this approach. Let's look at that. Uh, shadow traffic approach allowed us to do iterative development. Uh, it helped us to test new features, uh, with confidence, so it gave us confidence to go forward. Um, there were, there were data quality gaps identified during, uh, during this execution, and we were also able to performance, uh, uh, tune our system because we really put 100% shadow traffic on our new system. So we scaled from 1 to 100%. And we were able to see the whole load, how is it performing, how much cost is going to take, so we kept it running for a month to understand all the, all the, all the problems that can occur, uh, with 100% sh traffic, we also got a good idea about, uh, the, uh, the cost, and we, we were able to forecast the cost in the future. Um, as we went live, um, as a go live strategy, we used experimentation approach. Uh, this is something very common in booking. Um, essentially we pass a percentage of traffic to the new system and see how it, how, how is it, um, basically different between base and variant. Uh, we wanted to make sure that company metrics are not impacted, uh, and the business is not impacted, so we took an experimentation approach because it helps us de-risk, uh, the high stake migration. Uh, explore real world problems earlier and build confidence with stakeholders. Um, so as it was a mission critical system we use experimentation, uh, and we scaled this traffic in stages. You can see here 1, 1030, 45, 19, 100. These were some of the stages that we created, uh, but the question was how to divide this traffic in the first place. Like how, how do we, how do, how did we come, came up with this? So we worked with our data science team and we went over past data to see like how we can divide. Uh, the traffic into different payments, uh, payment, uh, payment timings, and this is what we use as a criteria, but of course in, in your use case you will have to find something else, uh, based on your own use case. So did we get what we wanted to achieve at the end? Uh, was it all worth the effort? First of all, we unlocked a lot of tech capabilities for the, for the whole organization. For, for example, we were able to communicate from AWS to on-prem service. Um, then lambdas were able to write to Kafka topics. Uh, also, we integrated with our internal monitoring, observability tooling, and incident management tooling. Uh, this is something that was not available before. And then also we had vault integration for service to service authentication, uh, especially when we were connecting with on-prem services. So some of the benefits that we saw were introduced on boarding time. Uh, new developers were getting productive very fast. Uh, basically the time we, we saw that the new developers are now producing their first MR within their first month, which used to be 4 months in the past. So this was something that we saw immediately. Um, and of course there were plenty of resources available. There was a lot of documentation, so it's, it's very standard how AWS works, so that was helping us on board people faster. Uh, it improved our developer experience. There was faster debugging due to all the tools that are available out of the box, for example, cloud watch, uh, metrics, X-ray tracing, step function execution workflows like we, like I showed you before, um, so it was easier for us to perform a root cause analysis super fast within a few hours and produce 1st 1st fixed MRs for any bugs that we found. All of this improved the developer experience and helped us respond to incidents faster. It also improved the time to market. This is, this is the, this is the main metric that we wanted to, to hit. The agility in the architecture provides the ability to move things around quickly. In one particular example, we had to move the whole parallel block of lambdas to, to, to synchronous, uh, so it was asynchronous, and we were moving it to synchronous, uh, and that, that took us just 2 weeks. Uh, it wasn't a big deal for us because the system was very flexible and it was easier to move things around, um. So we, so for, so we say, so we see amazing results when it comes to improvement in terms of uh reduced time to market. So Etina, I would like to ask, how many of you have booked a trip to Las Vegas using Booking.com? OK, I see some hands So your reservation was 100% through booking modernized system which is based on AWS Serveus. So the team recently achieved this milestone of 100% traffic, uh, on the new system and thereby we have, uh, decommissioned the old reservation creation, uh, endpoints. So that means all the new reservations are going through this new era system. Uh, this milestone paved the way for other teams to also use our platform and delivers features faster. This is the creation of reservation. We also have modification and cancellation use cases, and they are delivering their use cases very fast on the same platform. And to wrap it up at the end, I, I would, I've shared our modernized journey and what we set to achieve and what were our targets, but I hope you, it gave you some insights into how AWS services can be used to for transformation, um, and that you will explore what fits your needs, uh, and what are your use cases. So I will leave with a quote at the end. And invite Sarah to share more about service patterns based on broader uh industry experience. Sarah. Thank you, Jason. We just heard this inspiring story about bookings, ARB, accommodation reservation backend, how they went through this transformative journey and modernize their legacy system on-prem into a fully surveillance architecture. They took the bold choice of doing this leap, but in fact, there is, this is not the only way in which AWS customers can migrate and modernize in AWS. In AWS we have identified what we call the seven R's, migration pathways. We have the retain. We have a retire. We have relocate. We have a re-host. We have a re-platform. We have repurchased. And we have a refactor. Refactor is the main topic of today, as you can imagine, you have understood, uh, understood so far, and this is essentially taking the leap from migrating from on-prem to AWS and immediately quickly adopt surveillance technologies and, and unlock the benefits of adopting cloud native capabilities. What is important in this slide is that there is not one that fits all use cases. Company all around the world, a different size, they need to choose what fits the use case and what makes more sense for uh for their company requirements. This, what the slide that I just showed you uh speaks about technology. But when we think, when we think about software modernization, like modernization of our architecture and so on, it's not only about technology, it's about technology processes and people. Think about it as a triangle. If any of these triangle the sides of these triangles are weak, this has an impact on the other aspect. And the other pillars and it might be that in your case, in your company's case, not all, all of these three pillars are perfect from the beginning. So when I say people processing and technology, what I mean, we need multidisciplinary teams that have a deep understanding of the technical and business context in which your company operated on prem and will operate in the cloud. You need processes that empower people to uh uh have a um fast decision making as well as uh iterative process, uh, progress. Lastly, you need the technology that best fit your specific use cases. There is not one size fits all, and the successes or issues that you have in one pillar may have a ripple effect on all the others or one of the other. Let's have a look at an example, a practical example. There is customers sometimes have, uh, the, uh, choice, the technological choice of choosing between a mono wrapple for their system or a code base versus multiple rales. On the surface, this sounds like a technical use case, right? But let's have a look at it a bit deeper. So if we think about this decision, what are the trade-offs when you decide, if you decide to adopt a mono repo, this comes from a technical standpoint with the consequence that you may as a company or as a team have. Less availability of tooling out, out of the box to manage this large code base. Practically what it means is that you have to build your own specific tooling to manage this code base. You have to maintain it. You have to build it from scratch. You cannot use easily maybe something out of the box. So this is the technical part from a processes perspective because you have a mono repo. It might mean that you can adopt a trunk-based development. This informs how your processes work. And on the other side, from the people perspective, because you have a mono repo, uh, this can come with the consequence that you have frequent or higher touch point of merg request cross teams. So this is the mono repo choice, but if we go with multiple repo from a technical standpoint, we have more, perhaps more tooling out of the box to manage these different code bases more independently from a processing standpoint. This means that now you may need to enforce strict API contracts to. Communicate between the systems and from a people perspective because now these teams operate more independently in different code bases this uh uh comes with the um the consequence that they have not very often the opportunity to uh uh do merger requests to be cross functional or uh cross team requests within your organization. There are, as you can see, there is not one size, one size fits all, and it's about choosing what fits for your use case. So Keeping this triangle as our guiding principle to modernize our application, how do we transform a highly coupled monolith into a loosely coupled collection of several services? How do we do that? There's different ways to do, to achieve this. Let's start with a very famous one which is the strangler fig pattern. This is a pattern that takes its name from the use case of this plant that slowly replaces its o tree. So, as the name suggests, this is about incremental modernization. Let's take an example of an e-commerce organization. That wants to uh move away from their monolith and then move uh uh to AWS. How would they, uh, the strangular, let, let's have a look at the first phase. In the monolith, they have a different core functionality. There is a product functionality, product management. There is order management functionality, and then there is invoices. At the beginning, they are all on-prem. After that, what happens in the next phase in the straggler, strangler fig pattern. We, uh, the company identifies a use case, in this case, invoicing for modernization, and in this coexist, uh, phase, second phase, what happens is that you have a routing layer that enables and allows the two systems, the legacy system and a new modernized system to coexist. And once the invoicing, uh, components be modernized and moved to the less, you have, uh, these two existing, uh, system, um, uh, the route, the traffic is routed into these two coexisting systems, and, uh, yeah, once the first, uh, phase is successful, you incrementally move all the others until you have all of them into the new, the new modernized applications. The next pattern that we typically identified is the branch by abstraction. Uh, this is particularly valuable when modernizing shared capabilities. This can sound a little bit similar to the one that we have, uh, just discussed before, but the first one was more about routing and coexisting on two different systems. This is about abstracting the complexity of having two implementation at the same time. So in this case we have this um e-commerce company that have again the product management, order management and invoicing management functionality. All of these functionalities we identified that there is a common functionality that all of them rely on, which is the notification functionality. Think about, oh, once a customer, uh, buys something that is the order, uh, email that. Uh, summary that, uh, tells the customer about the order. It can be about different things. It can be about invoicing, and it can be also telling the customer about new products that have been, uh, now in the catalog, right? So this is a shared functionality from all of this. What this pattern does is hides the complexity of having the two implementation at the same time. Then you have uh the decomposed by business capability pattern. So in this case, the capability could be the order service or order management as well as functionality or so on, and in this case, you decide to modernize and your architectural boundaries are informed by your business functionalities and capabilities. So in this case, your business capabilities become the map of your modernization. Then we have decomposed by subdomain capability which sounds, could sound a little bit like the other one, but it's slightly different. So in this case, think about the order notification. So, we have an order service that publishes an event and then this event triggers some uh uh notifications in your internal banking systems related to different things. And while you decided to modernize your architecture, you realize that actually the business functionality varies in this case depending on where your order is uh being processed. So in the case of orders that happen in the US you have orders that need to have different, uh, business logic depending on the state. So California may have. Different regulation compared to other states and there is some uh things that need to be done differently based on privacy notice and customer data and so on and so forth and in Europe for instance you have different business logic because you have a GDPR compliance and so on other compliances uh that are related to Europe so you during your modernization process you notice oh this is quite different. And what this means then, uh, you decide as a, as a company that your subdomain will inform the architectural boundaries of, uh, of your system essentially. So in this case you have an AW lambda order service that publishes an event and then you have uh Amazon SNS that uh Uh, that, uh, propagates this event to lambda based in the US and that has a different business logic and on the other side, you have a similar architecture on the, uh, for customers that are based in Europe and then you can encapsulate this logic and easily change this logic without impacting the one from US customer or the other way around. Then we have decompose, decomposed by transactions. So think about your order functionality again. As you try to modernize, you realize that mm the user experience when you, the customer tried to create an order is slightly different from the user experience of customer that tried to cancel an order. So as an architect or a builder, you realize, OK. When customers are trying to, for instance, create an order, they think the customer wants and needs, uh, immediate an immediate feedback loop check. It should be almost instant whether when the customer decides to cancel an order that triggers, uh, um, a chain of, uh, actions they take more time and the customer doesn't need to know right away what is the outcome of that orchestration. But they just need to know, OK, that's, uh, the order has been submitted. So this, this different user experience informs your architectural boundaries and by, uh, decoupling, uh, by, uh, uh, implementing different architecture based on the transaction, create or cancel, you can now handle the two things differently. So in the first one on the top, create order, you can have Amazon API gateway AW slam and MDB. The user creates the order. You store the data in NMODB bank, and we have, uh, the response very quickly. Whether when you want to cancel the order, you have to process, uh, a lot you have to process the data, talk with different banking systems, so potential with other third party systems. That will take some time. So in this case, we need an EPA gateway as well as a step function workload to manage the orchestration of this. Then we have team-based service decomposition. In this case, the, remember when I talked about the pillar, uh, the three pillars, people, processes, and technology. This is an interesting, uh, pattern because in this case, the people inform your architectural boundaries. So you have another team that is familiar owning the um. The order code base and the order systems, then you have a product team that uh manages the product capability as well as code base and invoicing team and payment team. So this is an interesting way in which you can also uh decompose your monolith. So we have explored so far the seminars. We have seen how the strangularic pattern can help you, uh, or the branch by extraction pattern can help you decompose the monolith, but there is also another crucial aspect of modernization that deserves our attention, which is how our services communicate with each other. So think about as we break our monolith. Um, how, how can we rethink the way that our components talk to each other in a way that improve our customer experience as well as our team communicate to each other. Remember, the triangle, it's about processes, people, and technology. One of the key, uh, aspects about adopting surveillance is that you can challenge your own assumptions about user experience and how your system should communicate and how your team teams could communicate to improve and, uh, your overall, you know, customer satisfaction and, uh, uh, time to market and so on. So let's go back to the example of having an order, uh, sorry, um, e-commerce organization. Let's say that once, uh, that your product team comes with an idea that you want to implement a new gift code functionality. So your teams need to implement a gift code service. This gift for gift gift code needs to, uh, um, be, uh, communicating with other third-party gift code systems within, uh, other sauce vendors. You have to, uh, this gift code needs to be attached to internal systems like for instance, your user account needs to be attached. Uh, to be redeemed. And then to validate this gift code in real time, you need to have like the CRM system, external system that validates this gift gift code. So in a, a synchronous world, what could happen? Think about the holiday season, high season, a cus from the customer perspective, a customer goes to your website, an e-commerce website, they try to redeem a gift code. And this process of validating the gift code and redeeming the gift code is usually synchronous, potentially synchronous, but this is a problem in uh, in high season or uh like a a peak period because your CRM might be external CRM that you have no control over, might be a bit in difficulty, so the response time is a bit longer, which has an impact potentially in a synchronous world on your own system. As well as an impact on the user experience of your own customer. That is not great. How can we rethink, how can we leverage AWS surveillance services to rethink how this could, uh, could be uh uh architected? So here is where several shines with asynchronous patterns. When a gift code is entered, we immediately save it to Dynamo DB and Amazon Dynamo DB. Dynamo DB streams then trigger the event bridge pipes which handles the event propagation. Then this uh system needs to validate the code across multiple external systems, our loyalty program provider to check the point balances, think about also fraud detection and so on. We need to verify legitimacy and so on. After that, we can rely on eventbridge pipes to then transform the payload to make it uh uh uh compliant to the um CRM format that the CRM needs, right? And uh if the code is invalid, then we can notify the customer through the existing notification service and the customer in the meantime, which is very important, can enjoy the instant response and continue shopping because at the beginning we have the synchronous feedback loop. In this case, and this is crucial, we're not saying that everything should be as synchronous. This is very important. The beauty of surveillance is that we can mix these patterns, right? So we use Eventbridge, Event bridge pipes in this case for a synchronous gift code validation and enrichment during the shopping part, but then we switch to synchronous API calls during the checkout part, right? And only when we absolutely need it that immediate conversation. This is not only about implementation. This is about rethinking our assumption about user experience and system integration. So sometimes the best solution is letting go of immediate that immediate consistency in favor of eventual consistency when the business context allows it. And then lastly, pipes and enrich the event with user data from our internal user management service. So again we going again through the flow we have Amazon APA Gateway, AWSlambdano ODB that handle that synchronous part improving the customer experience and then we have that event propagation to event bridge that handle the synchronous experience and that allows us to rethink again how our system works, how the customer experience should be, how our teams should cooperate with each other, what they own. And how can modularize these architectures so then we enable faster development and better experience. So As we wrap up today about the discussion about modernization, let's kind of recap what we have discussed so far. So As Lucas said initially, distributed system, a living system, what it means is that they are not static, they're not immutable, they evolve and grow because your company, your organization evolves and grows, your processes change, your people evolve as well. So in this case, we have realized through the seminars and also through the inspiring story that session shared about ARB accommodation reservation back in that we can think beyond lift and shift. We can think, uh, we can think about surveillance not only as a technology but also a key strategic, uh, decision in this case. And uh in this case, we also saw while decomposing the monolith and the patterns, what are the house, so for instance stranguller pig fig, and what are the what that we can decompose and modernize, uh, think about decomposed by uh business capabilities. And at the end we have understood how we can unlock a synchronous communication in our systems and challenge our own assumption by using AWS service and services like Dynamo DB, even Bridge, Lambda, APA Gateway, and so on. Lastly, I want to conclude this session with something really important and I want you to bring with you back home. Grow, not build software. Software is non-immutable. Software evolves. Thank you so much.