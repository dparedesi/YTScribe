---
video_id: 1DgGZ__dw1g
video_url: https://www.youtube.com/watch?v=1DgGZ__dw1g
is_generated: False
is_translatable: True
summary: "This session, \"Designing local Generative AI inference with AWS IoT Greengrass (DEV316),\" presented by \"Max\" Kohei from Soracom, explores the concept of \"Physical AI\"—AI that interacts with the real world through sensors and actuators. The talk focuses on the trade-offs between cloud and local inference, demonstrating that while cloud AI offers powerful models, local inference is crucial for low latency (responsiveness), safety (working offline/danger), and privacy. A live demo using a robotic arm with a Raspberry Pi shows the significant lag introduced by cloud round-trips (500-600ms) compared to the near-instant response of local processing. The speaker introduces \"VLA\" (Vision-Language-Action) models as key for physical AI. To address the challenge of keeping local models up-to-date, the session details using AWS IoT Greengrass V2 to deploy and manage AI models as Docker containers on edge devices. This approach treats hardware like software, ensuring \"updatability\"—a core requirement for physical AI systems."
keywords: Physical AI, AWS IoT Greengrass, Local Inference, Generative AI, VLA Models, Robotics, Edge Computing, Latency, Updatability, Docker Containers
---

Thank you for joining uh this session. Uh, my name is Kohei, not Shohei. Unfortunately. So, uh, I have a nickname. Uh, please call me Max. Yeah, take it easy, yeah. So, I work uh as uh an evangelist for IOT and AI at Rocom and uh I'm uh I'm AWS hero, yeah. So today, uh, I will talk about a simple idea of running generative AI uh on local device. So, I think so. You came here uh because you want uh running a generative AI on your devices. So let me ask, uh, raise your hand, so has anyone tried, uh, so running, uh, generative AI model, uh, on your device? No? No one? OK. So another question. So, it's a, did you think uh it's harder than uh I expected the running generative AI on local device. Raise your hand. Thank you, thank you, thank you. So, good, uh, I feel some, uh, me, so me here. The pinpoints, so the reason. Uh, I'm talking today, yes. Yeah. So one example is robotics. Uh, in this session, I will show design and operation at Local Inference architecture. I will so uh also show uh when local influence works well. With a live demo robot arm, yes, uh, you will, uh, compare the, uh, the trade-off between uh cloud and local influence. So It works, yeah. This is a soluba, so traditional uh robot teaching. And So they're all teaching now. And so latency. Local device and the cloud and connectivity. And a model updates. Uh, I will also show a, a practical way. To deliver and update AI models. At the edge. So to running uh generative AI safety and continuously. So here's today's agendas. So first, we will uh clean up the keyword, physical AI. What is physical AI? So as a power of AI in the real world. And next, uh, I will show the latency uh difference uh between local and cloud. Uh, influence. So we live them, yes. And after that, I will show how, how to update foundation model in uh local device with AWS IOT Greengrass. And finally, uh we will uh wrap up. So let's start, uh, so what is physical AI? It's physical AI means a machine can sense the real world with cameras, sensors, and can decide what to do and can act to change the environment. So with the growth uh and the AI and robotics, AI moving from the digital world into the cyber world, uh, real world. AI is no longer just, just on the screen. It can move things in the physical world. We also use uh keywords, H, so local and uh cloud AI. So, bottom. Their ideas show how AI runs places. AJI runs near the data source. And cloud AI runs after the data goes to the cloud. So physical AI is a concept name. I think so, yeah. Physical AI is HAI and cloud AI, so different AI. It's not about about the location. Physical AI is needed for 3 main reasons. The first Is responsiveness. AI can react fast to what it sees and feels. It lets machine is moving quickly when things change. The 2nd Is autonomy. A can more than follow orders. It can think, uh, uh, it can think, act it by itself and can run the, uh, create new value. The 3rd Is collaboration. AI can understand human intent and work naturally with peoples. Humans and machines can act as one team and one goal. Here is one example of physical AI. Our customers. A company in Japan called Linku gives a robot arm vision by adding a 3D scanner. Robot arms are used in factories and production lines. So traditional robots arm use teaching. What means teaching is an after slide. So they repeat fixed movements again and again. So if the uh uh so if the object is in a different position. Or uh angle. The robots can pick it up. For example, for, so if the robots uh set, so uh pick a flat object, flat object. So it cannot pick its tall one. Then human turns object, so for the robot. Humans works, uh, so for the robot, uh, a little, a little bit sad, yeah. Link with solution. Uh, use a 3D scanner to find the sharp shape and angle of the object. So use a camera, yeah. Yeah I understand it and adjust the movement of the robot's arm. This China. Works as a uh robot's vision, yes, and AI works as a robot's brain. This is a good example of responsiveness and uh autonomy, autonomy, yeah, sorry, autonomy and collaboration, working together. Here, uh, so I will explain the teaching. Teaching means, so teach a robot, teaching a robot. How to do that task. You can write a program, but so today's robots uh need complex moves. So as you see, So human uh move the robot arm by hand and the robot records. Uh, Red Cross is a moving. The robot plays back, uh, repeat task automatically. This is a traditional teaching. It's a core uh rule-based. This robot arm is trained to pick an object on uh one spot. This one spot is a very rare wax. Oh, OK, yes. But Uh Object moves a little bit. Move. The robot cannot pick it up. So it examples so fails. Why? Uh, traditional teaching does not have vision. So rule-based method works well when the same task, uh repeat many times and a short time. In the case, manual positioning is required. So, but uh if the tasks uh is a little different, uh, or if no human helps, we need a different approach. One solution is a BLA. So vision vanish action model. A bigger model. So see the world. Uh, with a camera and sensors to understand the scene. And makes the correct reaction. It expands uh generative AI from the cyberspace into the physical world. VLA is a key technology for physical AI. So the link with case is a good uh example of the uh VLA, sorry, VLA approach. By the way, so a model that understanding of the text and images is called uh uh BLM, visual language model, yeah. So. Here, uh, BLA model finds a black object and put it back at home position. You can you can imagine as camera just uh so outside picture so uh this is, yes. So 8 cameras to capture a cable. And for the BLA uh model. The first saw black object placed in front. Of the arm. Oh, oops. Oh yeah. I put it in front of the arms. The BLA uh use a camera and sees it, pick it up and put it back to home. Next, So, I uh put a little bit uh Different position. Solu-based systems failed before, but Uh, VLA model is works. These tasks, uh, so, uh, run, uh, raspberry Pi and, uh, a simple web camera. So, with AI, so physical AI is a, so. Already possible in a very simple way. So Let's compare, so a large language model, you know, yes, better understand the better way. The first LLM. It takes, uh, so text and images input, so even called as text. And it output text and image, so you know, yeah. So good examples, so Amazon Nova uh cloud, cloud, and GPT open source. Now VRA. It so also takes uh text and images, but the data comes from sensors and cameras uh from the real world. As the output, the VRA. Is a movement generated. For examples, so RT2, so little robot, so and small uh VLA. Here is the architecture of the VLA. A VLA model takes a real-world input from the sensors and cameras. So it makes uh inference and generates and so action plan. So action plans uh send signals to actuators. Like a robot's arm, yes, this one. And the moves uh device. Some BLA uh can take a prompt text. For example, so move the red apple. Uh, so another one, so pick the white and black. With the correct terrain. They can do different tasks. Now, this brings a new question. Where should the AI run the inference? For a robot arm, if So it feels natural to run so near. Yeah, On the local device. But Is cloud AI not useful. For physical AI. Is that really true? So let's take a look. First, so let's look at so architecture. The difference between local and crowd is a side that BRA runs uh inference on. With cloud inference, the device must send data to the cloud and wait for it to come back. So this round trip makes the latency higher. So, what is far mean? So, let's check it. So this demonstration. So here, 2 robot arms. So when one arm uh moves, so other arm moves in synchronize. Right now, there are linked, uh, so USB. Directorly, yes, uh, via controller, so latency is very low. So next, via cloud. So, today's demos, so um let's make latency. The synchronized signals goes out over LTE network and server sends it back. So I use uh so, so chat command, uh, simple uh round trip. So switch to demore. Yes, uh, uh, I will. So this is, uh, Raspberry Pi 5, so I, uh, SSH to the, uh, Raspberry Pi. This is so, uh, remote access service on Soracom. So it's a TCP port, uh, for the service. Yeah, so. Connect Yes, uh. And At I tried so raspberry pi connected. Oath Yo My fingerprint is, oh, it's OK, right, uh I mean, it's, uh, so. Local operation fast run. Uh, so. Shortcut makes teleoperation local, but also runs a robot tele operator command. That now works so. It's OK. Hello. Yes. So via USB connect very uh low latency. Just stop Next. We are cloud operation, so it's a Bottom architecture. I built So I prepared reflector server on US East. It's OK, yeah, this one is a reflector server. I tried socket command on reflector server. So connect, it's easy, uh, great, and uh so what should cut I forget is uh reflector, yeah. It's a, so get command, uh so incoming TCP uh and uh so outgoing uh TCP port. Uh, it's OK. So next, Uh, so raspberry pi, so connects, uh, so USB signals, so the TCP port, uh, I, uh, run the, uh, socket command, uh, on, uh, raspberry Pi, and so to connection I need. Oops. And the, this is so upstream uh terminal and This is a good downstream terminal. It's so what, what command I forget is make file. So, uh, so, so catch upstream. Upstream SO and the reflector server address is where is SSS uh. Reflect the Oh, it's correct, yes, and I run the Socalled downstream command. OK. So let's run the remote. Axis. So let's try it now. So there is. It's already slow, slow, so car. Let's go, let's go. So. It's OK. It's OK. I'm, OK. So, so let's move it again. This is a, uh, so LTE networks, uh, USB signal, US East one, and LT network and roundtrip. It's a Or delay. Yeah. Clearly delay, so compares uh on local, yeah. It's OK to stop. So wrap up. So this is a backup video. So if my demo failed, so you, you see the this uh screen. So I'm happy. So uh my demos works. So, anyway, anyway, so as you can see, when latency appears, it lose real-time performance uh quite a lot. In this video, clearly delay. The latency is around 500 or 600 delay. Milliseconds, yeah. So of course, so if we, if we use a nearby region, the latency can be much lower. And not Not all crowd calls take 55 600 milliseconds. Its key point is to measure it. So, what does that latency really mean? 500 600 is what is. Some say, so there is a 100 millisecond limit. When latency goes past that point, humans feel that something is wrong, wrong. For examples, so you know, uh, vending machine. Uh, the response time after pushing button. On the vending machine. This is not everything. But a good difference for you, yeah. If your systems acceptable this latency, then crowd inference is still possible. But You also need a stability connection to the crowd. So when do we need local, so, so HAI when should the device do the inference by itself? There are 3 main cases for local, so HAI. The first, the camera and the robot combination. The system must make a fast, fast inference and a quick action. On, on a production line with one object, uh, for example, per, per second. So very fast. If the inference runs far away. The object is already gone. Second, safety. When there is danger, we cannot wait. For long latency, and we want it to work offline. This must be local. Third, privacy. As you know, so sensitive. Is a top security, sorry, Security is top priority, yeah. For example, camera data, so it's very sensitive. So it must be handled with care. One way, uh. is to keep the processing on local. If you think 3 cases similar to edge computing. You're right. There are so almost the same. So, up to now, talked about VLA and local, so HAI. And You may ask. Which AI models good for local AI inference? So has anyone uh so watched uh CEO keynote yesterday, right hand? Oh, great. So, it's exciting and uh so funny uh keynote. So she said in a keynote. Modest choice is critical. Yes, yeah, I agree. In the Alo, the Amazon bedrock gives us many mothers. Yesterday, so Google's Gema 3 was released. So in cloud, choice and switch is easy, but on the device, model choice is too hard. Because the devices have Limits. For example, so storage size, memory size, and so on. We can install only 1 or 2 models, so what should we do? The answer is updatable. Today, latest AI models may be outdated tomorrow. So we must build a way to keep AI modelsdable and always use the best one. So don't lock your system to one model. Updable. This is a core idea. In the physical AI era. As you know, in the cloud, we already have uh Amazon, Sagemaker, Bedrock, and the CICD uh pipelines, but what about the device? For real edge device, you are So must staydable. One of, so answer is AWSIOT Greengrass. It brings crowd style. Uh, updates straight to the device. And it turns a difficult challenge, hard where. Into software. And updable platform. Let's take a look. Uh, so, uh, review, review, uh, at IOT Greengrass. So the latest version is V2. is a Java-based uh software. It runs Linux and uh some other operating systems. In IoT Greengrass, applications are deployed to devices as components. And in V2, Doca contier can be used as a component as well. This LED is, uh, this lets you deploy and manage containers on your devices, much like so Amazon ECS. Now, let me show you uh demos uh of updating uh VLM model with IWIT Greengrass. Here is overall uh architecture. My VRA models, so store a hugging face, so bottom uh light, yes. Uh, so in the development environment, uh, so, I learned uh uh Dockard built to bring the model into the container images. Next, I pushed a container image to the uh Amazon ECR. Then I sent uh deploy command through uh AWSIOT Greenglass. The devices run, uh, to a pool and starts the container uh locally. Let's vote, so. Now, change a screen demo. Now, please look at my Docker file. It's a do file Very simple. So 8 line, it's downloaded a BRA model from hugging face and bring into the container. This one. So the tag, BB-3. What means BB-3 is a black box object. Detect black box or object. So I have already, uh this uh container image uh pushed uh Amazon ECR this one. In green grass. This black object model is already deployed. It's a deployment and What's deployed is a success. In the Component is this one. This is a container on the local device. So. I'll show you the demos, but I would love to show the full demo with the. Here's low down, but so uh actually the part is we show the video. Sorry. So I appreciate your understanding. Uh, anyway, uh, anyway, anyway, uh, so fast, you can see the black object BLM model uh running right now. Start. Of course, VLA model uh finds a black box object and put it back. Right. And next, So Another position Of course. So, right. VLA model, find a, a black box object, uh catch to the, uh, and, uh, uh, put it back. This is before video repeat, yeah. So let's update uh model, uh, so white, white, black, uh white box uh detect model. So hanging face, uh, I, uh, my, uh, another, uh, another, uh, uh, VA model, uh, stored, uh, hanging face too. And I already saw. This is so uh Docker container too, and I edit uh Docker file. So Black box instead. White box detect model. Right. So I am a dog of build, build, a black, not, not black, white. Beard. As you see, so great. Toka image, yes, nice. This one. And push to Amazon ECR. So build command so. Uh, Push, yeah. Oh great, fast. It's a OK, damn, yes. And I create so Iotic green grass recipe. Use white box object model. So component and Now it creates bargium and Now using black box objects. 2 White And this one. White And update Oops Oh, conflict, uh, version. So I increase uh version number and create version, and, oh, it's OK. So, This recipe deployed via AWSIOT Greengrass. Deploy next and To the uh version increase, yeah, deploy. They check the raspberry pi. Now Process is now black object detector model runs. And OK, it's a dockapo, nice uh white object model container and uh Uh, so slow, uh. So, so, uh, a little bit. Go for it. Yeah, Well, What happened Oh, OK, it's OK. Uh, let's stop the black box object detector model and, uh, uh, so change the white uh white box object detector model. It's easy, yes. So active part is a video. After the update, it works like this, yes. So it Detects the object in front of front of arm and of course, so it's detect it. Even, even in a different position. Yes, this is a success, and Uh, same as black box, uh, demos. Different position. So, VRA finds it and uh put it back. The another one This new VRA model is designed to detect white object only. So even with other objects around, it picks up only the white one. Look It's a so detect white box object and catch. OK, oops. It's very cute. And so put it back. So of course, so change so object position. It find. And So catch So put it back. It's OK, yeah. So you can update the model or switch a new VLA model if the system is updatable. With Docker. Uh, you can, uh, export uh device files, so you can use the cameras, so, and, uh, so sensors, you can uh insight on use container. So, and please remember, so an AI application uh that cannot be updated. Becomes old very fast. AI needs not only code and data. It also needs the modifiers. So using IoT greengrass instead, instead of making your own systems helps the system stay stable. In IOT device for development, we used to work only with program and data, but now we also work with AI models. There are 2 important points. First, the network environment. You don't need so constant connectivity, but so deploying AI model and uh so update uh AI model. So fast and large data transfer network. So indoors, uh, you can use Wi Fi, but outdoors, so on when uh Wi Fi not available, one option is LTE. So my company, Soracom provides some mobile connectivity for IOT and AI and today, I shared uh our experience with you. Another one is so you can uh use broadband satellite network, uh, so like Amazon layer, yeah. So connectivity is important. So, 2nd is file size. AI models are very large. So we need new knowledge to handle them. So, even, oh yeah. OK. Even a small model. is about 1 gigabyte. It's a huge So open VRA model over 15 gigabytes. So We consider this big large model deployed. In AWSIOT Greengrass, there are 3 ways to deploy large files. First, you can use IoT Greenglass artifact files. It's simple, but uh uh it has limit, so 2 gigabyte limit. It's at 2. You can put the model file inside Docker container. This is a method, uh, so today, er I use. So the the limit depends on Amazon ECR. So 3 You can download the file, so with the script and the IOT Greengrass recipe. This is a, so avoiding uh most limits, but you must check the download and so execution yourself. So choose uh what to it uh right method. So based on the file size. So my recommendation is to bundle the AI model in the container. As you saw in the docker file, is a very simple bundle the model this way. Containers makes so development and testing. Much easier. So AWS IOT Greengrass can also manage them well, and this helps, uh, so avoiding file size limit. We still need some skills to keep the container, so containers layer small. So, today, so showed you many things. We looked at the of Idea of physical AI. And how to choose between local and cloud AI. Physical AI is not only edge, so local only. Cloud AI is useful, so physical AI. If the latency is acceptable, the cloud works very well. The key point is a measurement and requirement. So you remember, so 100, 100 millisecond barrier. And so for physical AI the most uh important idea is keeping it updable, not just choosing. Mothers You don't need to build an updatable systems from scratch. We have IoT green grass. Today you saw how ASIOT Greengrass makes devicesdable and reliable. So AI will continue. AI will continue to grow and maybe so new normal. Let's use its power in the real world. So with adable local and HTAI two together, uh AWSIOT Greengrass. That's all from me. So my bad uh English, but that's all. Thank you so much for joining this session. Thank you.