---
video_id: nHaLbyLPK7s
video_url: https://www.youtube.com/watch?v=nHaLbyLPK7s
is_generated: False
is_translatable: True
---

Alright, thank you so much, uh, for, uh, being here, and this is, uh, day two of Reinvent. Hope you all are having, uh, lots of fun, and this is almost a week left, so I hope you get to learn as much as you can. So I'm, uh, Vive Gangasani. I'm a principal Jenny. I'm a specialist solutions architect. Uh, in my role, uh, as a lead specialist architect and Sage maker, I've been working with, uh, hundreds of startups and enterprises in helping them train, deploy models, and scale them up. Uh, I'm also responsible to build, uh, solutions, uh, for running models, and I also partner closely with our product and, uh, engineering teams to help, uh, shape the, uh, features for SageMaker inference. Uh, so today we are gonna talk about how you can efficiently and effectively deploy. Uh, LLMs and JAI workloads on Sagemaker and manage them, uh, and these are learnings based on our testing, feedback from customers, and, uh, our, uh, new features that we are launching, and I wanna introduce, uh, I, I wanna give it to Kareem to introduce himself. Thank you. Thank you all. You guys can hear me right. Awesome. So, Kareem, uh, product manager with Sagemaker AI Inference, uh, happy to be here on stage with you guys. Happy to, uh, explain to you about how Sagemakeria Inference has evolved and the new capabilities we're bringing. Um, and we will be here after the session ends. If you have any questions, feel free to drop by and ask us. Uh, now, I'll hand off to our partner here. Good afternoon. Can you hear me? Yeah. My name is Richard Chang. I'm from Salesforce. I'm a software architect. Uh, it's my great pleasure to work with Vivec and the Korean to tell you what we, uh, what product we are building and how we build it. Thank you. Awesome. Thank you, Richard, for, uh, sharing the stage with us. We're super excited to have you. All right, so, uh, let's get, uh, right in. So in terms of, uh, agenda, we, we'll start off with, uh, going through what are the trends in 2025, what are the new things that we've been seeing since, uh, 2024, and then, uh, we will talk about, uh, what are the steps involved in deploying models to SageMaker AI and what are the features you have just like for, for you guys to. Uh, know about SageMaker. Uh, and then we will talk about, uh, the three important pillars, uh, that will enable you to run models efficiently and cost-effectively and, uh, uh, you know, uh, at production scale. So those are optimizing price performance. So we'll talk about how you can optimize your models on SageMaker AI. Then we'll talk about how uh you can use or deploy any models or frameworks of your choice for giving you the flexibility with SageMaker AI and then we'll talk about uh How easy it is for you to uh deploy and uh manage your uh cluster of, of nodes with SageMaker. We'll talk about that and then uh we'll talk about uh once you deploy models in SageMaker, how do you consume it? How do you build agentic workflows with it, uh, and then we will hand it off to our uh. Partner Richard to talk about what is uh Agent Force and uh how they're building their platform and how they're using SageMaker AI to power their platform. All right. So let's uh jump right in. So, uh, in terms of trends, we, since the launch of Chart GPT in, uh, late 2022, uh, we all have used Chenya in some way or the other, like text generation, summarization, chartbot. So that is, you ask a question, you get a response. So this year in 2025, uh, agentic workflows have picked up, and by that what I say is enterprises are, uh, seriously trying to integrate, understand how to build an agentic workflow, how to integrate it with their. Uh, existing workforce and gain productivity from it and agency care is pretty interesting because it's not just gives you a response but it takes actions. It goes and figures out what to do to accomplish a goal that you give it and you give it some tools to go do something and it will find out, uh, it'll break the problem first. It'll find out how to execute it and execute it on, on your behalf. So this is a super uh interesting. A new development that is taking place in 2025. And another key development that has taken place is the rise of test time compute or inference time compute. So what this is, is, it is the computational effort a model needs to generate an inference response. So a year ago, a year and a half ago, the general models like GPT 4 or cloud models, you just ask a question and it will respond. But as you see in the latest. Set of models, the reasoning models, they go through a chain of thought process before giving you a request so they look at your question. They break it down, uh, plan, and identify different ways to answer your question, and then give you the right response. We've seen this with Deepeek models, the new Queen reasoning models. Basically all the models, uh, open source models and even cloud that are coming these days are using reasoning models with chain of thought. Now this is great and it gives. Better accuracy and it's proven that it works better because it not just uses the knowledge it has gained through training, but as it goes through a chain of thought it improvises its answer. But the trade off is that you generate much more tokens per inference response than you would do in just giving a straight response right? So if you're. Prompt is 100 tokens and your response is expected to be 100. The reasoning takes another 200 tokens, so it's 300 tokens. So the demand for computation power has gone up with reasoning models. Uh, so this is a prediction from Gartner. Over a third of, uh, uh, applications will have some kind of agentic workflows by 2028. That's the expectation, and enterprises are moving towards that. And, uh, also a lot of, uh, day to day tasks will eventually be not if not totally automated, they will have some percentage of, uh, automation in there with agents. So this is all great, but the challenge that we hear from customers is going from POC to production. Right, so it's easy to, uh, spin up a studio account and, uh, spin up a GPU, deploy a queen model, uh, use land graph to just write up some framework, and then, uh, invoke the model, uh, to get responses. It works. It also works if you have a team of 5 people, but if you look at scaling up. From, uh, like 5 people to 100,050, I mean 500,000, these these challenges that customers face, right? So let's talk about what these are and, uh, how we are solving those with Sagemaker AI. First is performance. So LLMs, like I said before, they take time to generate a response, and as the concurrency increases, the load increases. So it's important for a good user experience to have the ideal performance response within the time they have. So customers are finding it challenging to identify. First benchmark, what is the right number of GPUs? And once you have the GPUs, how do you optimize performance to get the best responses as you scale up? So that's the number one challenge. The second one is related to it, which is costs. We all know that GPUs are expensive, they're scarce, and it is super important to ensure that you have the maximum GPU utilization. And squeeze maximum efficiency out of a GPU to keep the costs in check, but it is not clear how do you, uh, optimize costs. And then scalability again, it's unlike CPU workloads. You cannot just keep scaling up, scaling down, uh, because of the cost reasons. So you need to plan within a cluster how do you efficiently manage your resources, your deployments, your workloads, and scale up and down, uh, within your resource within the number of GPUs you have. And the fourth one is GA is still evolving and there is a lot of new frameworks out there. There's a lot of environment variables that you have to adjust different models and optimization techniques, so it's very complex in uh how do you. Uh, set up your infrastructure. How do you set the drivers, your containers, your load balancing, routing? So there's a lot of, uh, minute details that come into the picture when you're going into production, right? And it is all of these steps are different in the, uh, non-AI workloads like in a web app, you have to do things differently versus in JAI you have to do, uh, things differently. So, uh, before I hand it off to, uh, Kareem to talk about this, I just want to spend one minute, uh, for a question that we get asked a lot. So Bedrock also has inference, and Sagemaker also has inference. So what is the difference? I think a lot of you might have, uh, this question. So this is like, uh, to keep it simple, there's like 3 differences. Uh, one is if, if you wanna access our proprietary models like cloud or, uh, anthropic cloud models or Nova models. Or coherent models, they are available only on Bedrock as APIs, so they're not available on SageMaker. That's a difference one. The second one is Bedrock has a certain set of models that are available on them, and they support certain open source model architectures so you can import and Use them whereas Sagemaker you get full flexibility in deploying basically anything you want. Like today a model releases you can just download it from hugging face, download a container, and deploy right away. You also get lower level access so you can configure your post processing scripts, your Your metrics, how do you want to process your input output, it's totally customizable. You can also bring in your trained models of any architecture, whether it's custom or open source, and you get full flexibility there. The third difference is Bedrock is a serverless, so you pay on a token basis, and Sagemaker is a server full where you. Pay for the GPU you use and that is why you pay for the GPU. So it is important to ensure that when you pay for the GPU, you squeeze maximum efficiency out of it and that is where we're gonna, and Karim is gonna talk about how you can do that. Thank you. You guys can still hear me? Awesome. So thank you, thank you, Vivek, and thank you for also setting the stage here. Um, now, what we are going to do at this part of the talk is, for all the capabilities or all the 2025 trends that Vivek spoke about, how Sagemaker AI inference makes it easy for you to do the deployments of your models, any model. Um, get good price performance, get flexibility, and get ease of use, so that you get the best out of it. And that's what we're going to do in the next, uh, next part of this talk. So as Vivek had hinted that there are 3 main pillars. The 3 main pillars that we will be focusing on is, as you can see, there is price performance. Price performance is because it's the computer, you want to get the best performance out of out of your models, so high throughput, low latency, and auto scaling. Auto scaling is very important because production traffic is never constant. Yes, you can prepare for what is steady traffic. But there can be a burst. You guys all came off from Cyber Cyber Monday, Thanksgiving Friday, and so on, right? That you guys would have seen the peak in the traffic. How do you plan for that? So quick auto-scaling is important, but at the same time, you need to have low latency and high throughput. The second pillar for us is flexibility. One thing that Vivek was hinting towards is, you can bring any model. A model releases today, if those weights are available in hugging face, download it, do the deployment. You can deploy with any framework. You can deploy with any GPU instance that you have access to. And last but not least, ease of use is very important for us. We want to give you all these capabilities, but if you don't have a good ease of use, that's going to be a big problem, right? That's a big, big hurdle that you have to cross. So ease of use is extremely important for us. And how do you go from a model weight all the way to invoking the model or getting responses back for us that flow is extremely important, and we have we take we take a tenet to keep it as simple as possible. Now let's go through these one by one. So first, what I want to talk about is, this is, this is a pretty Interesting slide and a cool slide because this takes you through the journey of what you can do on Sagemaker AI inference and what do you get with it. So let me explain. On the right side you can see that you can bring models from S3, Jumpstart, which is the Sagemaker studio, hugging face, or even your VCR image. Then you create an endpoint, you deploy the model at that endpoint, and you start invoking. That's simple. What do you get with it? You get instance choice. It can be a G series. It can be a P series, whatever series that you need for your needs. You get your weights, choice for weights. You get your choice for instances, auto scaling. You can decide to have on demand. You can decide to have a reservation, or you can decide to have a reservation for your because you know what your steady traffic is, and when you're scaling up, you can get it from on demand so that you have continuous operations going on. Built-in observability, of course, we need that because we have to make sure that our instances and our models are performing well. If there is a failure, you need to know why, where, when it happened so that you can take the corrective measures. Bring your own container, which is what Vivek was hinting at. Bring your own containers. There are no restrictions there. Um, I'll be diving a bit into multi-model. Because you're going to be consuming the GPU or the instance itself, we have the ability for you to have multiple models run on the same GPU. So that you're getting the best out of the GPU then, right? You're squeezing as much as possible out of it. And streaming response. We'll be talking about a feature that we launched which enables much, much real-time experience for streaming. And before I move on to this, if you guys do want to take a picture, please do. What I do want to call out is that Sagemaker inference gives you the flexibility for the models you bring, containers you bring, instances you use, uh, performance, scalability, um, and getting the best out of your GPUs. So once your setup is done, this is how the deployment looks like. Your bottom is your infrastructure layer. Which is where your GPU CPUs live, your containers that you bring or any open source container, and then your models. You can do a single model deployed on the instance because maybe you need, you have the use case for that. You can do multiple models on the same GPU like I was talking about so that you get the best out of it, and that also enables you to scale. For example, if one particular model is getting more traffic, you can scale that model on the same GPU. As a separate component, so that you are able to meet the traffic needs. And if at all, and that's the auto scaling model copies, and if at all you need to actually scale out to an on-demand instance, you can. So now, I'll be touching upon some releases that we have done uh leading up to reinvent. Um, one of our uh focus areas has been price performance. So let me walk you through a, a bunch of releases that we did there. So Really happy to announce that we have Eagle 2 and Eagle 3 speculative decoding available on Sagemaker AI inference. Um, this is well explained with the use case, so I'm going to walk you through a use case and a sample so that it, it makes sense to you guys. Uh, and you can also see how the, the claim that we are making that throughput increases with this happens, so that the dots get connected. So, normally, what do any customer expect when they are working with a chatbot or with an inference? They want fast inference. That's like, that's, that's like table stakes nowadays. But in reality, LLMs generate a token at a time. Which actually is slow. Hence, your throughput in general is not at the performance level that you want it to be. Here comes eagleheads, speed without compromise. So you increase the throughput without compromising on your quality. The way it works is you have your foundation model. And you also have your eagle heads as drafts playing the roles of the draft model that are actually generating the next end consecutive tokens. They generate all the I don't know, maybe the next n number of tokens all together, and foundation model then evaluates every token that has been generated and sees whether it is actually the token that it would have generated. Instead of it generating, it's just evaluating all these tokens at the same time. And accepting what it thinks it would have generated and rejecting what it thinks it wouldn't have generated. So from a foundation model generating one token at a time, it has now accelerated and accepted 3 tokens at a time, increasing your throughput. Let's play it with an example. So let's say that my LLM is writing a book on a cat, and it's about to finish the sentence. The cat? And my draft model says that jumped over the moon with the given probabilities. And the draft model generated this entire set of tokens together. The foundation model then looks at every token. Together and sees what is the probability that it would have generated with and if its probability is lesser equal to the if its probability is within the range of the draft model's probability it's going to accept these and it's going to reject the one that is so with this your inference speed has increased. This is equal to. Eagle 3 makes it even more cooler. Eagle 3 generates, draft model generates multiple variants. Of the tokens, it can say jumped over the moon, climbed over the bed, hopped over the fence, whatever, all together at the same time. And the foundation model evaluates all of these together and selects the best and gives you the throughput increase. So last but not least, it leads to latency reduction. We have seen in our test 2.5x throughput, and there is no trade-off in your accuracy. Now how did we implement this? So what you can do is we run it as an optimization job today. Uh, you can bring your own data set, or we can bring a data set. If you bring your own data set, then the eagleheads get trained for your specific use case, or you can use an open source data set which does improve the throughput. Um, we create an optimization job and then we run the evaluations and when we Share the evaluation reports with you guys, right, as a, as, as a chart, and you can see the values, and you can see if your throughput has actually increased or what is the rate at which it is increasing. And if you are happy with the output, you can deploy the new weights, which is your optimized model, or you can update an existing model with these new weights so that your same user experience now just got faster. This is another capability that we have brought, brought to you guys. So one of the trends is that you can fine tune your model with multiple adapters, right? And what you can do in that case is you can deploy your base model on an instance, and you can deploy multiple Lora adapters on the same instance and based on which Lora adapter gets invoked. The responses are handled by that lower adapter. What we have done here is we have made this even more dynamic. Let me explain to you what that means. So let's say that you have your S3, your CPU and GPU. Initially all your lower adapters that are trained are sitting in a nursery at the time of initial invoke. The adapters move to your GPU and then they start serving the traffic based on what has been invoked. And if your GPU memory is getting filled, then the adapters that are actually not in use get offloaded so that the next adapter that is invoked, which is not yet available in your GPU memory, comes back up. So in a way it gives you like there is limitless load adapters, but in all honesty, it is able to serve the right load adapter and be ready for you to load it from memory, from your storage to your instance memory, and start using it pretty quickly. And this is where you can actually be able to deploy multiple models or copies of the same model on the same instance using our inference components capability. So, our inference components enables you to have, so if you look at the picture here, Model A is our pink model, if you will. Model B is our blue model, if you will. In the same GPU, which is 8 GPUs, you are able to deploy 1 copy or 2 copies of the Model A and 2 copies of Model B. What has this done? This has not only enabled you for scaling up of models each, but also enabled you to maximize the utilization of your GPUs. So we are getting the best out of this, uh, with respect to utilization. Um And we have also we have also brought up the caching capabilities into our NVME volumes such that your containers are now already available on the NVME volumes, and as your instance is up, the containers are downloaded from the NVME volumes, and we have seen actually upwards of 50% improvement in auto scaling when this happens with your with your containers. And last but not least, um, we also have the improvements that are done for latency. Specifically, we have the loader routing and session routing. Think of loader routing as where It's OK for you to get a response from any available. Accelerator or GPU. It's stateless. I just need to get the response. Example, what is the weather of Seattle? I just need the response, right? So load routing enables you to get your prompt being sent to the GPU. To the place where there is not enough traffic so that you get the maximum or the quick output back. Session routing is more stateful so that it's a multi-tone chat or you want to have a proper context with the past history so that you're able to get more information or more more personalized data back. That's where session routing comes into the picture. You enable a session and then you use the session to keep sending your prompts back so that your chatbot or similar applications are able to get good good experience there. Now with this, I want to hand off to my friend here, Vivek, so that he will walk you through what are the flexibility capabilities that we are bringing in. Thank you. Thank you, Kari. I, I. Alright, so, um, yeah, so Karim talked through about two things. One is, uh, how do you optimize performance, increase throughput, uh, and how do you. Pack more models into a GPU and uh manage it so you get maximum efficiency out of it. So now, let's talk about the second pillar, which is how do you get choice? Like how do you uh Deploy any framework of choice, any model of choice, and how you can experiment quickly, right? The first thing is Sagemaker has managed containers, so we build containers using open source frameworks and we do some optimizations to make it run on Sagemaker. We have those, but some customers have security. Requirements or they do additional customization on top of the containers to, you know, to be better. So we support anything bring your own containers and then inference scripts. So a lot of customers have uh pre-processing, post-processing scripts where when I, when you send an inference response, you take a part of the inference response, you put it in some database or anything, right? So. You have complete capability to customize your inference scripts. Third one is, uh, multi-modality. Text generation is the, uh, is the old, the one that started with the original OG. But then right now we are seeing, uh, uh, video generation, video understanding, audio generation, voice agents. So basically we are launching features to support all use cases, multi-modality. So that's, uh, that's a key, and we're gonna talk about a launch there as well. So, um, when we say flexibility, right, uh, this is a screenshot from artificial analysis which has like become the industry standard to look at performance. Now, if you see here, the key point I want to make here is open source models are on par with closed source models, and we have many customers who want to use open source models for multiple reasons because. The open source ecosystem with like hugging face because of the ease of customizability that you can have with open source and the cost efficiency that you have. So we have benchmarked and models like GPTSS are pretty good in tool calling function calling which is required for agents and are. And achieve pretty good results in some testing that we have done, and these are very cost efficient compared to the The bigger models and are also much faster. The smaller the model is, the faster it responds to inference requests, and these are also reasoning models as well, so. The GPT OSS model can run on a single G6E2X large, which costs around $2 an hour, right, and it can handle like 40 to 50 concurrent requests at the same time. So it's super, super cost efficiency and at some point, uh, when you are scaling up significantly, uh, maybe GPU based, uh, infferencing will give you better price performance. Uh, so you can basically take any open source model from here. Uh, choose the right GPO instance and deploy it on SageMaker using the same code. You don't even have to change the code there. Uh, I'm also, uh, excited to introduce a new capability, uh, bi-directional streaming. So bidirectional streaming is super important for, uh, audio transcription, right? When you speak, it needs to be transcripted in real time. Audio translation or, uh, customer service agent where you're, you're, you're sending a, a streaming. Request and you expect a streaming response from the model. So this has been implemented for reinvent and the way it works is we have to give some technical details we have. A router and we have the Sagemaker endpoint and you can bring your own container to work with the bi-directional streaming or we've also partnered with DeepGram to bring their models to work with Sagemaker so we are implementing HT 2 protocols whenever the client. Uh, connects to the SageMaker router. It, uh, connects through using HTP2, and the router will forward the request to the, uh, uh, the API which uses a web socket connection, and the web socket connection takes care of, uh, establish establishing a connection between the model and the, uh, router, and it takes care of streaming the responses using the same connection. Uh, so it doesn't keep opening new connection, but it uses the existing connection to stream the response in, uh, both the ways, uh, and this is, uh, super useful in say cases like when you're talking to a, a customer agent they don't have to. Wait till you send the entire request. They can the the response keeps coming in as, uh, I mean, the, the real-time tran transcription happens as you're speaking, right? So it's super efficient and we also have a demo on how this works. Uh, we also partnered with DeepGram to bring their models natively into SageMaker, sorry, into SageMaker, so you can just go to Marketplace, go to DeepGram, and select some details and directly deploy to Sagemaker. You're speaking with our virtual pharmacy assistant. How may I assist you? like to check the status of my order. OK, let me look that up for you. Could you please provide your member ID? 1001. Can you hear the audio? Thank you. Let me check your orders. You have one order, ORD 001, currently processing. What medication is in this order? Amoxicillin 500 mg, 21 pills. Is there any refill remaining on my prescription? You have 0 refills remaining for RX 1001. When will this be ready? Your order ORD 001 should be ready for pickup on December 6th at 10:00 a.m. Thank you. Thank you for calling. Goodbye. OK, so yeah, as you can see, the assistant and the customer, they're both of their conversation is in a streaming way which enables the call to, uh, you know, the whoever is acting on it to take actions quicker because it is streaming instead of waiting for each request, right? So this is super important and this is, uh, this is available only on Sage Maker in AWS. It takes a lot of effort in building this, so we are offering it as a capability right on SageMaker. Um, The next one Uh, the next, I want to talk, talk about the ease of use. Um, so one of the common asks from customers is, uh, how do they know how many instances they will need to handle production grade capacity? How do you know if a specific GPU is a good fit or not? Or how do you know a specific framework is a good fit or not? So customers want an easy way to spin up GPU capacity. And experiment with it. They they want it for shorter periods. Some need it for a week. Some need it for a month and 2 months. So right now, until now it has been a manual process to work with your account teams to get the capacity you need, but we're launching this capability now where you. You can go to the AWS SageMaker console. Just do the request, reserve your capacity in a self-service fashion, so you can just go there, select what instance type you need, and select the time frame, number of days, and you have the capacity starting from the day when you selected and uh. Once it's approved, you can start using the capacity right away. You don't have to create any request or raise any limit request. That's been a challenge in the GPU space uh for a while, so we're solving that using this. Uh, so again, end of the day we wanna make it easy for you to get started and, uh, do your experimentation deploy. So you can use it both for benchmarking. Uh, testing or if you have a planned release, uh, where you want some suddenly like 20 instances, you can come here and uh uh use it. Uh, another thing is, uh, GPU. The way you know that you're maximizing the GPU utilization is by having an observability stack. So we built an observability stack into the SageMaker where it tracks per, uh, instance, GPU metrics. So every GPU that is there, what is the level of utilization, what is the CPU level of utilization. Uh, the basic idea is based on this you take necessary action either scaling up or ensuring that you, uh, process more requests through these GPUs. We also track like number of invocation. Our router mechanism exports metrics where it calculates. How many errors are there? How much is the latency for these errors? So basically the idea is that you don't have to build all of these yourself. We are having it and you can just deploy your models and get these out of the box in a cloudWatch. Uh, the other one is, uh, what is the, uh, one of the questions we get a lot is what is the ideal container or the best framework to get the best performance? The answer is, uh, it depends, right? So VLLM has been a pretty popular option. Uh, and, uh, it is always supports the latest models on day zero. so whenever a new LAMA 4 or LAMA 5 comes up, it will support on day zero. So VLM fits most of the use cases. Uh, SG Lang is, uh, has also been good for some models like mixture of experts, uh, and if you're having extreme. Long context length SGLN sometimes performs better, so it's for you to benchmark and know, but the idea here is we have our containers which is LMI container which is based out of these frameworks and offered as a managed container. But again, open source container, we have solutions in our GitHub repo which I'll share at the end. Uh, where we walk through Docker files on how you can just take open source container and do some minor modifications to run it on SageMaker basically to allow the health checks, to allow the port and all of that on, uh, SageMaker, so that is there. And then we also just launched a new container, so Kareem has been talking about Laura earlier, speculative decoding, but that is the what to answer the question about how you can start using that is through this container, right? So this container is built off of VLLM which we shared earlier, and then we have the. Additional optimizations on top of it to run efficiently on on uh SageMaker plus we have the capability with Lora hosting where you deploy a base model and you add multiple Lora adapters. You just configure them in uh using the Python SDK and then start deploying it. Even speculative decoding, uh, the entire optimization job after it's run, you can, uh, use the LMI container to. Host your model to get that performance benefits, uh, and we also support chart open AI chart completions that's been the industry standard so, so that you don't have to change your client code. You can use the same client code to deploy any model whenen open AI GPT OSS and use the same uh. Format to use it and since it's a managed container from AWS, we want to ensure that it is free of vulnerabilities. It is production great quality tested and used. So that's another thing. The customers don't want to build containers. They want AWS to do that heavy lifting for them, which is why these managed containers are super useful for the customers to get started and deploy. Um, So we cover all the three pillars price performance and um uh ease of use, flexibility. Now, OK, so you do all, you follow all of the best practices and you deploy your uh models on SageMaker, uh, JDI models on SageMaker. What do you do next, right? So since the theme is Agentic AI, uh, but by the way, gentic is not the only thing you can do on SageMaker. You can also do like personalization, computer vision, anything. But how do you build an, uh, agentic workflow? So all of these frameworks have connectors uh for SageMaker. So if you deploy a model on SageMaker endpoint, you can use any of these frameworks and uh have the SageMaker connector uh to connect to the model directly and use it for your uh frameworks, uh, use it for uh your agentic workflows. So this is a sample use case uh on how you would uh do that, right? So you have yourageMaker endpoint, you deploy like a coin model, and then You take land graph for example. You write up your agent workflow like an SRE agent or a customer support agent in your land graph, and most of the flow would be similar on how you would do anywhere else except that the connector part instead of using like OpenAI or Bedrock, you just use the SageMaker connector so that it connects. Authenticates to the sage maker and then uh gets the response. So once you write up that code in the form of Python scripts, then you can uh then deploy it into agent code runtime, right? So that's a new launch for July that we did earlier and that is a scalable platform where you can run your agenttic workflows with features like memory gateway, uh, authentication, all of it. So even for Sage Maker you can use that. So for that. Once you write a Python script, you have to create a docker file. You create a Docker container with that, push it to your container registry, and then go to your agent core and deploy that container, right? And then, uh, yeah, once you deploy the container, it becomes available in a healthy state. Your applications can start connecting to the agent core runtime to invoke an agenttic workflow, and the land graph code that you have will connect to the sage maker endpoint to, uh, to give you responses. Now we have a workshop tomorrow for this exact same thing, uh, so you will get, if you are interested, you can come and, uh, have a hands-on experience on deploying a model to Sage Maker Endpoint, a Queen model, and writing up a, a Python code for a deep research agent and deploying it to Agent Core so you can, uh, stop by tomorrow for the workshop. Uh, and then these are, uh, some of the takeaways we have. Uh, the, the, the, the reinvent session tomorrow, it gives you details about where it is. We have a workshop that you can do in a self-service way on deploying models and doing auto scaling and all of that. The third one is it has a lot of examples on deploying different models, um, and, uh, doing optimizations for, uh, the, uh, for your workflows. Uh, so yeah, take a picture. I'll just give a minute. All right. And then I'll hand it over to our uh friend Richard to talk about Salesforce. All right. Thank you. Can you guys hear me? All right. Thanks, Vivek. Thanks Karen. Um I'm really excited to share our journey together with AWS how we build a genetic platform. Now before I started. Has anyone used Salesforce heard of Salesforce? All right, I see a good amount of people, all right. So, first, I'd like to tell you briefly what Salesforce is. Salesforce is the largest CRM company in the world. We were founded in 1999. And since the beginning, our mission is to innovate and reinvent how business manage their customer relationship. Starting from there, we have Salesforce platform. In 2016, we introduced one of the most important innovation, Einstein platform. Which predict brings the predictive science and the machine learning automation for all the applications across the board on Salesforce platform. In 2024, we achieved a new milestone, which is Asian force. Now in the agent force, we build autonomous assistive agents natively on Einstein platform. All those agents, not only they can assist, they can also reasoning, take action, complete. Complicated workflow. This transform how our business manages their customers. All right, I'm going to play a brief video, tell you about what is agent force. Agent McCourt. A year ago, Agent Force was just a whisper. An idea Today it's the fastest growing product we've ever launched. Let's talk about Salesforce unveiling Agent Force. Tell us how, uh, big companies are using Agentforce. It's not just what is possible, it's what you are going to make possible. Humans with agents driving customer success together. This is what AI was meant to be. This story isn't just about scale, it's about something more profound. It's about a choice. Is AI going to replace us or are we going to be in command of it? The sales force, we've made our choice. We're building AI that elevates people, where agents handle the busy work so we can focus on the work that matters. Because AI when built with trust, it elevates your people, it helps them move faster, think deeper, and connect more meaningfully. Agent Force has now handled more than 1.5 million customer service requests at Salesforce, and that's not just a milestone, that's over a million questions answered with precision, a million customers understood with empathy and care, and most of all, more than a million times our people were empowered to drive greater value. All right. Now we are very excited to talk about agent 4th. Our vision is human beings and agents can work together to deliver success stories. All those agents are seamlessly integrated with our ecosystems, and they can perform very complicated jobs. Next, This is a slide showing a comprehensive architecture of agent force. We start the foundation layer, which is the data layer. We build Salesforce we use Salesforce data, customer data. On top of it, we build knowledge base vector database. Those data layers, this data layer served a single source of truth to enrich AI and agent. On top of it, we build a model ecosystem. We have Salesforce training models. We have our customer trained models, and we also allow customers to access state of the art large language model such as anthropic through Bedrock, Chat GPT, Gemini. After that, we build the trust layers where Salesforce can help customers to securely retrieve the data do the groundings. Now the core of the architecture is basically the agent force. This part we have autonomous and systems agents. They are built on top of our reasoning engines protected by the guard rails, and they can do the classifications and tool use. The entire agent force is integrated with every single product from Salesforce such as sales service, uh, tableaus, and you name it, Slack as well. Now this was last year. This year, we introduced voice. Agent force's voice is basically a nature extension of agent for core. It allows human beings to directly interact with the agent force. Now there are 3 main features of Asian forces voice. First, we designed the voice to be ultra-realistic and conversational. And we introduced a wide variety of voice to represent a customer's brand. We have audio cachings. Interrupted handlings and activity detections to make the entire voice experience fluid and in nature. On top of that, our speech to text and text to speech models will perform activity, entity detections and reduction for any securities issues. Second is, we want to make sure the voice can do the real-time agentic reasonings. This would require low latencies, intent detections, topic selections, and tool use. It is fully integrated with our knowledge base. Including the the rack systems, rack pipelines we have built. Last but not least The agent's voice is multi-channel and multi-platform ready, which means any time if we need to require a human being to get involved, the entire context and a conversation with the agents can be seamlessly transferred to the human being. All right, that's the features and what are the requirements? There are 3 major requirements we like to fulfill. First, the entire end to end voice flow, we want to perform on within the Salesforce security boundaries to achieve the maximum securities. Second, we want to make sure the voice experience is fluid and conversational. That means we need to support bi-directional low latency streaming support. So this is where Vivek and Karem has just shared with you. We choose SageMaker. It provides that features so that we can perform very low latencies bidirectional voice support. Second, 3rd thing is we want to make sure we support low latencies, open source models. We use open source models built in-house tailored for Salesforce user case. This is the architecture of the voice. On the left side we support different uh uh channels PSTN traditional PSTN, digital channel, web RTCs. The Majority of the work is going to be tested is under testing and it will be released very soon. In the middle, which is the voice call, we connect to the age maker which is supporting the bidirectional uh low latency streamings. The agent force core itself has two major connections. One connect to the external large language models. The other one connected to the in-house build fine-tuned model based on the open source models. On top of it, that's the salesforce knowledge base we build through the rack pipeline. All right, we have talked about the voice at the Salesforce now. I'd like to step back, look at the overall model servings. At Salesforce, we serve 22 regions at this time global-wise. And by the beginning of next year, we will expand to 28 regions. We use varieties of combination of sage makers, model servings, strategies for the G AI models. So I will name several, uh, first is. We use the inference component which allows us to share the endpoint and utilize more efficiently of the instance. Second, we support multi adapters. So basically we use the same models, same baseline OSS model, but tailored for a different user case which Korean has just shared. We also have single model endpoint which is for the traditional deployment cases. Last but not least, we have the the CMI support from the Bedrock so that we can upload customized model image. Now, on this slide, I want to show you some of the, the thought process and practice we have at Salesforce. We use SageMaker AI and LMI image to basically optimize the different model serving strategies. This is because StageMaker AI and LMI integrate multiple state of the art inference framework. So those work includes several items I'd like to kind of touch base. First is we choose what is the right framework VLLM, TensorT LLM. Second is, we do a comprehensive instance scanning to pick up the best instance to serve our models to achieve the best latencies and the best throughput. The next items we use is the the rolling strategies, the batching strategies. On top of it, we have quantization. We have wide variety of quantization based on the eventually the accuracy of models and the latencies requirement. Last but not least, we have speculative decodings as well. Overall, with this combination of a strategy, we have been able to serve real-time traffic as well as large volume batch traffic. On this slides, I'd like to touch base how we do model fine tuning at Salesforce. At Salesforce, we host uh almost 10,000, uh, 10,000 of training pipelines. So here I'm only going to focus on the fine-tuned part of the models. We start with open source model such as GPT OSS models or other open source models like LA 4 LMA 3. And we use a good amount of uh data uh for the pre-training or pre-fine tunings. Those are typically much larger volume. After that, we get a baseline of the models. And then we use very high qualities labeled data as well as IL based data to fine tune the model further which achieve the highest throughput and highest accuracies for Salesforce user case. Over this process, we reduce the hallucinations, improve the output format, and the, as well as for the specialized the customer user case. Overall, at Salesforce, um, we are looking for integrated solutions and infrastructure for security scalability and high availabilities to empower agent force which include our agent builder, prom builders, uh, trust layers. We also want to assess the latest models such as anthropic. AWS has been partnered with us to provide very integrated solutions for us. Let Salesforce really focus on the innovation, uphold our core values with customers so that we can serve the customers in a very high, highly regulated industry. I'm going to pause here. Thank you. Hi, thank you.