---
video_id: c_1FhdXNUSE
video_url: https://www.youtube.com/watch?v=c_1FhdXNUSE
is_generated: False
is_translatable: True
summary: "This deep technical session on AWS Trainium 3 UltraServers features Joe Surchia (EC2 PM), Ron Diamant (Chief Architect, Annapurna Labs), and Jay Gray (Trainium Inference Lead, Anthropic), explaining how AWS builds AI infrastructure for next-generation enterprise workloads. The session opens by positioning AI as a tectonic technological shift, with software engineering itself becoming a scientific domain where models complete 80% of GitHub issues on SwiBench Lite. AWS has spent over a decade building the most comprehensive AI stack: compute (Trainium/Inferentia, GPUs), networking (ultra clusters with 10s-100s of thousands of chips via EFA), storage (FSx for Lustre, S3 Express One), security (Nitro system), and management (CloudWatch). Key 2025 trends driving Trainium 3 design include post-training emphasis with reinforcement learning, reasoning models with extended thinking, agentic workloads with concurrent autonomous agents, context lengths exceeding 1 million tokens, and mixture-of-expert (MoE) models requiring heavy all-to-all communication. These demands require balanced systems: more memory, more bandwidth, and larger scale-up domains—not just compute flops. Trainium 3 Ultra Servers deliver 360 petaflops of micro-scaled FP8 (4.4x over Trainium 2), 20TB HBM capacity (3.4x), 700TB/s memory bandwidth (3.9x), and 2x faster interconnect. A key innovation is Neuron Switches—new components connecting compute sleds in full mesh topology for single-hop latency and high-performance all-to-all coms essential for MoE models. Ron Diamant explains that peak vs. sustained performance matters more than specs—like betting on a marathoner over a sprinter. Trainium 3 includes micro-architectural improvements: hardware-accelerated microscaling (quantization/dequantization for FP8 without compute overhead), handling outlier distributions in tensors; and accelerated SoftMax instructions running 4x faster at the same precision, preventing tensor engine starvation when attention computations are FP8-optimized. Benchmarks show 5x more tokens per megawatt versus Trainium 2 on GPT-O3 (120B parameters). For scale, Ron notes ML adoption curves differ—customers demand giant clusters immediately. Trainium 3's modular design enables robotic assembly and rapid field servicing. Trainium 2 deployed 4x faster and 33x larger than any previous AWS AI instance, with Project Rainier now running 1 million chips for Anthropic in production. For ease of use, three customer personas are served: ML developers get integrated third-party libraries (PyTorch Lightning, vLLM, Hugging Face); researchers get PyTorch native support via Private Use One with eager execution and torch.compile; performance engineers get NKI (Neuron Kernel Interface)—a Python DSL combining tile-based and assembly-level abstraction—plus Neuron Explorer for nanosecond-level profiling with zero performance hit. NKI compiler is being open-sourced. Jay Gray from Anthropic reveals that the majority of Claude traffic is served on Trainium 2. He walks through optimizing a fused flash attention kernel: converting BFloat16 to FP8 for 2x speedup, rewriting memory movement to use fewer larger vector operations for 13% attention speedup, and implementing SRAM-to-SRAM collectives to eliminate HBM hops for lower decode latency. On Trainium 3, the same kernel achieves over 90% tensor engine utilization versus 60% on Trainium 2. Trainium 4 is in development targeting 6x FP4 performance, 4x memory bandwidth, and 2x memory capacity."
keywords: Trainium 3, UltraServers, Annapurna Labs, Anthropic, Microscaling FP8, Neuron Switches, MoE Models, NKI, Neuron Explorer, Flash Attention, Tensor Engine Utilization, Project Rainier, PyTorch Native, AI Infrastructure, Performance Engineering
---

Welcome everyone. My name is Joe Surchia. I'm the EC2 product manager for our In French and uranium chips, and I'm super excited to have everyone here. Just a quick show of hands, how many are familiar with In French and ranium? OK, what about anthropic quad models? OK, a few more. Well, today I'm super excited because we have two experts on both of those things. We have the chief chief architect of ranium, Ron Diammont, and we have Jay Gray, who's the ranium inference lead for Anthropic, thinking about optimizing cloud models on ranium. So quickly what we have in store today, I'll first walk through how AWS builds and thinks about building AWS AI infrastructure. Then I'll have Ron walk through ranium and how he built it for performance, scale and ease of use, and then Jay Gray will come up and look at how he actually optimizes different kernels to run on ranium effectively. OK, great. So let's get started. So first, why is there so much uh news around AI? Why is there so much excitement? And I think it comes down to it is really a tectonic shift for how we build, deploy and interact with the world. So this isn't just incremental change, we're seeing new capabilities pop up because AI has enabled them and Andy Jasse. He said this most recently in a in a quote, we are at the beginning of the biggest technological transformation of our lifetime. And so I think one of the areas that I want to step back and take a look at, you know, where is AI really reshaping is scientific domains, and we look at this and things like protein biology, where models can now predict and design new proteins in minutes, which traditionally took hours to do, or even in mathematics where models like alpha geometry are competing at an Olympiad level and also solving formal proofs. Another area that's become its own scientific domain in and of itself is software engineering, where AI is now a breakthrough force of its own, and it can solve, it can resolve, develop and deploy its own code, solve bugs within the code, and even reason across large code bases. And this is just the beginning. But together these innovations are no longer just supporting scientific discovery, they're actually becoming the engine driving that scientific discovery. And so let's take a look at how this, how this is happening in practice with, as I mentioned, software engineering. So over time we saw traditional programming and over the past few years we've seen things like software chat-based programming or even collaboration with vibe coding start to take off. But as we've seen. These start to take off in the same time frame. We've also seen benchmarks exceeded on things like Swi Bench Lite, where models are completing up to 80% of real GitHub issues or even on harder benchmarks like Swi Bench verified, where some models can complete up to 50% with full correctness. And so what that enables then is the next phase of AI in software engineering, which is deploying agents or agent fleets or agent clusters so that they can autonomously operate and solve software problems. And we don't know the exact shape this future will take, but we can envision a world where we have software developers collaborating closely with an entire fleet of agents, and what that does is it opens up a new set of speed and scale for delivering software features and functionality. So take a step back, why does this all matter? And, and the important part is really that none of this happens without the infrastructure underneath that's powering all of this AI. And at AWS we spent millions, we spent more than a decade, building the most comprehensive, deeply integrated AI stack, starting at the top with a compute where we offer a broad portfolio of AIDA GPU instances or our latest influential and trainium. which offer cost efficiency for AI workloads at the network layer where we're deploying ultra clusters that are capable of scaling up to 10s or even hundreds of thousands of chips all connected with low latency, low jitter elastic fabric adapter. Then you have storage which where we've increased high throughput storage options so that you can keep those GPUs fed with FSS for luster or even S3 Express one. You now have access to your data at 10 times faster speeds than before and. Importantly, security, which is important for all of our infrastructure here at AWS, but also important for AI, where we have the nitro system that allows isolation of your workloads to protect customer data. And at the very bottom we offer management services and observability tools like CloudWatch, where you can monitor and watch your nodes to make sure that they're healthy and operating efficiently. And all of this comes together as a full stack platform for training and inference frontier models. And the reason that we can do this is behind the scenes we've been developing silicon for over a decade, right, whether it's in our nitro system at the top here, we have over over 6 generations available now for offloading that virtualization to dedicated hardware hardware for higher performance and more isolation of your workloads, stronger isolation of your workloads. We built Graviton, which is. Now supporting a multitude of workloads and over tens of thousands of customers and then we also anticipated the the growth of AI and we started building our inferential ranium chips early with the release of inferential in 2019 and we continue to continue to innovate across this full stack which is really important to drive the next phase of AI infrastructure. And starting with last year when we announcedranium 2, we talked a lot about the chip and the specs on the chip, but we also showed that it wasn't just about the chip. It's also about the innovation that we're bringing at the server level and the network level. So at the chip level you have innovations that are pushing compute and flops like 1300 FB 8 dense flops, T-flops. Or you have at the server level where we released our first tier and 2 ultra server capable of scaling up to 64 chips across a neuron link which has 1 terabyte per second connectivity, or at the network level where we deployed tens of thousands of these chips all connected with our elastic fabric adapter. And really when you look at all that engineering and that end to end to end design, when it comes together, it enables us to do things that we had never done before. And some of that is like you see here, shrinking the time at which we receive chips from our manufacturer to when we can put them in customers' hands. And here you can see over the course of ran ranium 2's life, we shrunk that by 70%. And the result is that it allows us to ramp ranium 24 times faster than any other prior AWSAI instance and to a footprint that is 33 times larger in capacity than any other instance, and all of that capacity is fully subscribed. Really, that is the end to end innovation that's required to build something like Project Rainier, which is the world's largest publicly announced uh compute cluster. But as we build more scale, we want to keep our eye on what is scaling next. And so we look at the trends continually with customers and we look at the industry to see where they're going. And here we'll start to look through some of the trends that we saw over 2025, more emphasis on post-training. So you have reinforcement learning is becoming more important as customers look to put their models in or as model developers look to put their models in real environments and get feedback, whether they're virtual, virtually generated or actual real environments like robotics. Then you have reasoning models where these models are taking a little bit more time. They're reducing the latency, but they're reasoning over multiple steps so they can generate a more accurate response to a deep question. And then last coming back to what we talked about with software with agentic workloads where we see multiple agents kind of collaborating autonomously outside making tool calls to really drive independent independent solutions for a wide variety of problems. And so Digging a bit deeper, what is the impact? How does this impact what we're building next for AWSAI infrastructure? And I think it really comes down to a few different new system requirements that we're looking at. And I say system, not just chip, because this is about the bigger picture here, um, and these systems now we see context lengths as we see reasoning models. Over longer contexts, context lengths reaching over a million tokens, right, that we need support, the systems that support that capability. We need support for a mixture of expert models which are communication heavy where you have sparsely activated mixture mixture of expert models communicating across the scale up domain. You also need support for infrastructure that can be used for pre-training, post-training, and inference so customers can really optimize the compute that they have available to them as they scale each one of these independently. And then the last one is you need support for really high batch size, high throughput systems that can support kind of concurrent, lots of concurrent agents operating autonomously on their own. And so the key theme here is that next AI infrastructure isn't just about compute flops. It's about more than that. It's about having balanced compute, which means having more memory, having more memory bandwidth, also having a larger scale up domain that you can support a wide range of experts, expert parallel designs as those models scale. And that's really why we're we're happy to introduce Tranium 3, which is the chip built for these next gen agentic workloads, reasoning workloads, as well as video generation workloads that are going to drive the demand, the compute demand for these next AI systems. And as I mentioned before, it's not just about the chip, it's also about the system. And so here we, if you caught Matt's keynote, we recently announced our TierN 3 Ultra servers which scale up to 144 chips, and I won't walk through all these stats, so I'll leave that to Ron, who's our chief architect here, but the key thing to remember is that there is innovation at each one of these that drives the capabilities of our next AI systems. So with that, I'll pass it off to Ron to walk through. Training 3. I think you're a monk. Alright folks, uh, so Joe, thanks, thanks a lot and folks, thanks for being with us today. For the next part of the talk, I'd like to go a little deeper into how we built Trinium 3 and specifically how we built it to be performant, ready for scale, and easy to use. Let's start with performance. As Joe kind of hinted, performance is actually not a single metric, it's actually a combination of metrics. Uh, of course there's compute floating point operations per second, but you also care about memory bandwidth and memory capacity, and the interconnect that connects between these chips, and all of these need to be balanced in order to achieve maximum performance. We actually touched on that in detail in last year's talk, and you have a QR link at the top right and by the way, throughout this talk every time there's an opportunity for offline self learning there will be a QR link at the top right. Trinium 3 Ultra servers made significant leaps across each one of these performance dimensions. We got 360 petaflops of micro scaled FP8 compute. I'll explain exactly what that means in a sec. That's 4.4x more than what we had with the trinium 2 Ultra servers. We have 20 terabytes of HBM capacity, 3.4x more, and 700 terabytes per second of HBM memory bandwidth, 3.9x more than the trinium 2 Ultra servers. We also have a 2X faster interconnect, and I'd like to draw your attention to these switches in the middle of the rack. These are new components we call them neuron switches, and they connect between the cranium 2 compute and cranium 3 compute sleds in a full mesh topology. Each sled is connected to every other sled within a single hop. And these are the sort of system optimizations that don't come through in the top level specs, but they absolutely impact real life workload performance. That's because they give us more flexibility to deploy different topologies. They cut down the latency between each pair of training 3 devices, and they give us really high performance for all tool coms. The reason we care about performance in all to all coms, or at least one of the reasons we care, is what we call a mixture of expert models, MOE for short. In such models, MOE models, we tend to place different experts on different chips and then route a token to the relevant expert in real time in order to do the compute just in, in that specific in that specific chip, and that requires blazing fast all to all communication, which is exactly what the neuron switches provide. That brings me to my next point, which is peak performance versus sustained performance. In real life we don't, if you think about what I just quoted to you a slide ago was spec performance numbers or peak performance numbers, but in real life that's where the performance story only begins. It's not, it's not where it ends. And one of the, the a nice. Analogies for that that I could think of is who would you bet on winning a marathon race, a sprinter or a marathoner? So obviously a marathoner, right? But if you think about it, the sprinter has a higher spec speed or peak speed. It just can't sustain it over the entire marathon race. So you can see that there's at least some situations where we actually care more about sustained performance rather than a short, short peak performance. And in AI chips, it's actually the same. We care a lot about achievable and sustained performance, more than specifically some spec number. So when we started developing the rimium 3 chip, our software team posed a challenge to us. What would it take to build a chip where the sustained performance is as close as possible to the peak performance? You get every single floating point operation that you paid for. And that led us to a list of micro architectural improvements that are aimed to give you every last percentage of performance, and I'd like to walk you through a couple of those just to give you a sense of how this looks like and how we're really optimizing optimizing this workload end to end. Let's start with microscaling. The motivation for low precision training and inference is very clear, right? It's pure physics. If you use a smaller data type or a lower precision data type, you can run the compute on smaller circuits, and you can move smaller data around the chip, which leads to higher performance and better energy efficiency. But like many good things in life, it comes with a little bit of fine print. For example, if you just naively cast with from a high precision data type, for example, B low 16, into a lower precision data type, for example FP8, then it turns out that you completely destroy your model. And the reason for that is that B float 16 has a much higher dynamic range, a range of numbers that it can represent compared to FP8, and that means that large numbers overflow to infinity and small numbers tend to be squashed to zero. We can fix that by a technique called quantization, and here I'm showing you a quantization technique called ABSmax where we calculate the maximum absolute value in a tensor, and then we scale the entire tensor such that it exactly captures the entire dynamic range of FP8. And this actually works quite well until we reach an interesting case of outlier, distribution outliers in the tensor. Imagine a case where one of the elements in the tensor is 100x larger than the other values. That's the green element right there. So we would scale the tensor such that the green element will will map to the maximum representable value in FP 8, but then all other elements will be squashed to zero or near zero, so we completely lost the representation capability after casting or quantizing to FP8. We can solve that as well via a technique called microscaling. With microscaling, we do abs max quantization one more time, but this time we do it in small groups of elements. Here we have one group with the green and yellow elements, and another group with the orange, pink and blue elements. And you can see that the green is an outlier. The green element is much larger than any other element. And that's because and and what that causes is that with the first microscaling group, after we quantize, green goes to maximum representable and yellow gets squashed to zero. But in the second microscaling group, we quantize from scratch with new distribution, and you see that the blue, pink, and orange elements are quantized quite well without any impact from the green element, from the outlier. That's exactly what microscaling does, and it has been shown that it's very efficient in preserving model accuracy in low precision training and inference. But micro-scaling is hard to do, because you need to take a tensor, break it into groups, then compute the smacking in each group, calculate the scale, apply the scale, and then do all of this around when you dequantize in reverse order. So what we did, I should go back, so what we did in trainium 3 is that we built hardware circuits in order to completely offload microscaling quantization and dequantization. You can basically get all the accuracy benefits of micro scaled quantization without any overhead on your compute engines, and that drives, even though it doesn't appear in the peak numbers that I show you, that absolutely improves your end to end workload performance. Let's do another example. In this case, it's accelerated softmax instructions. To give you a background, at the core of every modern or most modern AI models, there is an operator called Self-attention. It was one of the breakthroughs in the transformer architecture that make make models like Claude and others work as well as they do today. And at the core of the self-attention computation, we multiply two matrices, Q and K here, and then compute soft max on this result, and finally multiply that by another matrix, V. So if, if I show you show you a timeline, we do a matrix multiplication followed by a softmax operation, followed by another matrix multiplication. And if I pipeline that over multiple tiles of computation, you can see that we can get a very clean pipeline where the tensor engine, the engine that is doing matrix multiplication, the most precious resource in the system, is constantly busy, 100% utilized. We love that. Now let's apply the previous optimization that I told you about, micro scaled FP8. So all the matrix multiplications now run way faster, but the overall self-attention computation didn't accelerate by nearly as much, and that's because SoftMax doesn't leverage FP 8. It actually runs at a higher precision. We need to do that in order to keep the accuracy of the model. That's a well known secret of trade in the ML space. So if we weren't paying attention to that, we could have gotten a couple of problems here, right? First of all, despite the nice optimization that I showed you before, the end to end speed up is not as much as we wanted it to be, and the tensor engine, the most precious resource in the system, is now. Luckily our team saw it a mile away, and as we worked on the micro scale FP8 optimization, we also introduced another list of optimizations to make sure that we always keep the tensor engine running. In this case, it was a SoftMax, accelerated SoftMax instruction that is able to run SoftMax 4x faster at the same precision with zero loss of precision accuracy. So that's how it looks like with the accelerated softmax instructions. Now we get the end to end speed up that we wanted, and now the tensor engine is constantly running 100% utilized again. Achieved performance is as close as possible to peak performance. Alright, now we have a, a huge list of these optimizations. We actually document them and you can do self-learning online, uh, again, a link at the top right, uh, and all of these optimizations build on top of one another in order to make sure that you get to use every single floating point operation per second that the trainium 3 device offers. Let's put it all together. Here we benchmark a model called GPTOSS with 120 billion parameters. This is an open source open weight model by OpenAI. On the x axis, we measure what we call interactivity. That's the per user experience, how quickly we can generate output tokens. And on the y axis we measure overall throughput. If, if the server is serving multiple requesters at a time, what's the overall number of tokens that it can generate per second? And to make it really fair, apples to apples comparison, we normalize the y axis to to a megawatt. So now we're comparing trinium 2 and trinium 3 on even ground. Which one is more efficient? And I think the results are beyond impressive. So we can generate 5x more tokens per megawatt with trinium 3 compared to trinium 2, and at the same time we're also improving interactivity, OTPS. We're really proud of these results. We think it will generate real value to you guys. Let's move to scale. Last year I showed you this graph, and it shows us, it demonstrates that the adoption curves in the ML space are very different than the adoption curves that we're used to from other technologies. This is a typical adoption curve. We have the early adopter phase, then we start ramping up and eventually we get to mass mass volume. And with ML when we introduce a new technology, just like we're doing today with Training 3, we immediately get customer demand to build the giant clusters with this new technology, with this new generation. And this required us to build trainium 3 to be ready for scale from the very first day. If you think about it, that's exactly where Annapurna and AWS meet each other and complement each other. We at Annapurna built ranium 3 that is built for scale from day one. I'll show you exactly why. And then why and how, and then we marry that with AWS that has decades long of expertise in deploying the massive compute clusters faster than anyone in the world, and we build projects like Crinier and what we're going to build with Trainium 3. So what you see here is that we have, this is a training 3 compute sled. It's a very modular design, and that's not just an elegant design choice, that's important. It means that we can test every component independently and then plug it into the system. And every single component is top accessible and replaceable, and this is critical because it allows us to automate the the the production line and make the assembly completely robotic, and that means that we can scale much, much faster versus manual or complicated assembly. It also means that when we need to serve these cards in production, we can do it very quickly and efficiently and keep your infrastructure infrastructure running, which is what we all want to do. Let's break this down. At the back of this computer sled, you see 4 trainium 3 devices. Then in the front, you can see 2 nitro devices for scale out networking with the FA. And there in the middle you see the graviton CPU that is responsible for input, output, and management as a whole. Now all these chips were built in-house with deep expertise. We know how to optimize them, we know how to debug them, we know how to serve them, and with deep co-optimization between them. And that's critical again in order to give you maximum performance. Achievable performance needs to be as close as possible to peak performance, and we need to optimize across the entire stack to do it. Joe show showed you these graphs. With ranium 2, we deployed 4x faster and 3x larger capacity than any other AI chip in AWS. And for example, he mentioned Project Rainier. So let's talk about Project Rainier. Last year, actually, when we were on this stage, we talked about we announced Project Rainier. We said that we were going to build a giant AI cluster for Anthropic, and now we're 12 months later, and we have 1 million chips running, training and serving state of the art cloud models in production. I'm not talking about some future announcement. This is running today, and this happened in 12 months. With what I just showed you with ranium 3, we expect to scale faster than we than we scale with trainnium 2, actually much faster and to much larger quantities. Lastly, let's, let's talk ease of use. We're building a very sophisticated infrastructure here, and we need to make sure that our customers can easily use it and get the maximum value from it. And we knew that if we want to optimize for ease of use, we needed to deeply know our customers, so we talked to them a lot, and what emerged is that we have actually, we actually have 3 customer personas and they kind of have different needs. At the top here we have the ML developers that are building AI applications based on existing models, and what they value the most is very strong and robust third party library integrations and ready to use pre-optimized models. Then we have researchers, and researchers are inventing new models and new operators. They want to iterate quite quickly, and they care about a robust, frictionless experience much more than they care about performance, actually. They care about developer cycles. The experimentation needs to happen very, very quickly. And finally we have our performance engineers. These are folks like Jay Gray that breathe and live hardware optimizations. You'll hear from Jay Gray, uh, in a second. He's one of the best in the field, by the way, so I think he'll explain very nicely how he's optimizing, uh, uh, for training 2 at this point, training 3 coming, but, uh, but specifically what they value the most is tools that give them full control over the hardware. We'll talk about that as well. So let's go one by one with ML developers, with deeply integrated neurons, with third party libraries like Pytorch Lightning, VLLM, and Hugging Face, and you get to just take models from these libraries and seamlessly, frictionlessly run them on training. We're also engaging the community via uh university courses and hackathons. You can see one example there when folks take a hugging face model, for example, fine tune it to to to the to do a certain task. You can see a QR link there for a hackathon that we have for fine tuning models to play chess, um, and eventually we serve the models and training as well, and the feedback that we're getting so far is overwhelmingly positive. For researchers, we have deep integration with Pytorch and Jacks, and what I'm really excited to share is that Trinium is also becoming Pytorch native. So let's talk about that. With uh with recent advances and advances that the Pytorch team did here where they introduced something called Private Use One that allows you to integrate a custom AI backend into Pytorch, we made Tranium natively supported by Pytorch, and that means that code that you write on Pytorch that can run on a CPU or a GPU can seamlessly run on Tranium, the same exact code, and I'll show you that in a second. That means that you get the eager execution experience that you know and love from Pytorch on the training devices and devices, and it also means that you get the automatic code optimization that PyTorch introduced via Torch.compile, also running seamlessly on training. A nice side effect, a positive side effect here is that all the tools and libraries that you know and love that run on top of Pytorch also come along for the ride. So if you, if you're using FSDP or DTsor for distributing your workload, that will run seamlessly on ranium as well. And if you're using libraries like Torch Titan to do to do large scale training, that will run seamlessly on ranium again as well. Here's how it looks like in code. On the left we have Pytorch code that runs on GPU, and on the right we have the corresponding Pytorch code that runs on Trinium. It should be hard to spot the differences because there, there are not many differences. It's one word literally, instead of saying two, instead of writing to kuda, you write two neurons and we take care of the rest. It just works. Uh, Again, we, we wanted to give a lot of credit to the Pytorch team here. The way that they extended the Pytorch framework allowed us to, to do what we're, uh, showing you here. We're already piloting this capability with a select set of customers. We're getting very good feedback and we plan to make it generally available in Q1. Last but not least, let's talk about performance engineers. For this category of customers or customer personas, we introduced two new capabilities. Neur internal interface, uh, yeah, ne, sorry, neural internal interface, we call it NKI or NII for short. I'm, I'm used to calling it Niki. Uh, that's a low level programming interface to directly programming the training devices. This existed last year, we evolved it quite a bit. I'll tell you more about it in a second. And the second piece of it is the neuron Explorer. That's a toolkit for doing performance optimizations on top of the training devices, and that that's built on top of the neuron profiler and gives you a really deep insight and observability into your workload running on training. And with both of these together, you get in full control over optimizing your workload on training. Let's go through them one by one. Niki is a Python embedded DSL. But it's, it has something that is quite unique. It, it combines two levels of abstraction in a single programming environment. You can implement your code in a tile-based language, just doing computation between submatrices, and this would be very easy to ramp up on, especially if you're coming from Nampi or Triton. But then if you identify an area where you really want to optimize, you can go all the way to the assembly level with very similar semantics to the tile-based semantics. And that combination allows you the ability to ramp up very quickly and to optimize very deeply. This year we're introducing a couple of new capabilities in Niki, including scheduling and allocation API that allows you to do fine-grained control over the scheduling of different instructions that are running on the machine, as well as where do we allocate the different tensors, and that allows you to build the very structured pipeline that I showed you before in the self attention example. This is actually a a feature request from some customers. We listened. This is already available. You can start using it. In addition, we also also introduced a new front end with much improved error messaging that basically allows you to self-serve and iterate much, much more quickly on the on the Niki on the Niki programming experience and improve your time in order to get an optimized kernel on training. And last but not least, I'm, I'm actually pretty excited about that one. We decided to open source the Niki compiler. It's coming in the next couple of months, and the the the reason we decided to do it is because Nikki is all about giving you control and observability. So now we give you full transparency on how your code is compiling to the Trinium hardware, and we also welcome industry and community contribution across the entire Trimium stack. Here's one nice example. So this is a company called Descartes. They have a, a, a cool application where they do real-time G AI video generation. Uh, they can ingest the video, edit it, and generate the video back to you. You can see examples here. They decided to build their entire model based on Niki, and they achieved phenomenal utilization numbers actually beyond what I expected the team can do in 3 to 4 months. Next, let's talk about the neuron Explorer. If you ever wrote highly optimized code, you know that your best friend is a strong profiler or tracer that tells you what's running on the hardware and where the bottlenecks are. With ranium we have the industry's leading neuron profiler that allows you to get instruction level trace of what's running on the hardware with zero performance hit on the actual workload running. This is nano nanosecond level observability without slowing down your your workload. So we extended the neuron profiler a lot and built a suite of tools that we call the Neuron Explorer on top of it. First of all, it's 4X more interactive. And that means that you can just debug much faster and get and and get a better overall debugging experience. But on top of that, we, we made it available via web applications for easy sharing between developers and also we deeply integrated it with IDs like VS code, and that's actually quite important. What you see on the screen here is that I highlighted one of the lines in my Niki code and the neuron Explorer automatically highlighted the relevant instructions in the profiler. So this gives you a much tighter connection between the code that you're writing and what's actually running on the hardware, and gives you a sense of what's worth optimizing. We're also introducing, this is not ready yet, it will come in a month. We're also introducing system level profiling where we where we allow you to see a full run on multiple devices and you can see if they're tightly if they're tightly synchronized or if there's one slow machine, etc. etc. It really helps you when you debug highly distributed code like a big training run. We did a couple of more things. We introduced hierarchical view, so when the neuron Explorer is brought up, it shows you framework level operators, stuff like self attention or fully fully connected layer, and then you can click and drill down all the way to the instructions. So that makes your debug experience much more incremental. You can, you can start at high level and try to understand where the bottlenecks may lie, and then when you really want to zoom into something, you can just drill down through it. It makes the debug experience much nicer in my view. We'll also give you a summary page that shows you how the different engines are utilized. Here you can see the tensor engine at the left. The tensor engine is utilized very well here, 60 or 70% MFU, and the other engines are kind of lightly utilized, so that shows you how the workload is running. At the top right you can see how we're utilizing our memory bandwidth, how what, what portion is used for reads, what portion is used for writes, and what portion at what portion of the time the memory is actually sitting there idle. And when you look at this top level view, uh, you can really get a sense of how well your workload is running on the hardware. We also give you stats and visualizations. The one on the bottom right is the one that I particularly like. So here we're showing collective communication throughout the execution of the of the inference run in this case, and we're we're showing you a scatter plot of them. So what you see here is. Good. Most of the coms are happening almost exactly for the same duration, which means the performance is very consistent and predictable. But if you ever see a large spread here, you kind of get a sense that there's an outlier and you need to go to debug and try to understand what happened. And lastly, this is, this is a cool one. We introduced something that we called performance insights. So on the summary page you'll see, you'll see a bunch of boxes that show you where we think the performance button Xs are and what you can actually do to solve them. And we do it via a combination of AI-based techniques and just human-based techniques. So if we debug something a couple of times, we'll introduce a rule here and try to give you a hint that this might help improve performance. All right, we show this to, to the folks at Anthropic. Uh, there's a brilliant performance engineer here, uh, there named Tristan, and we, when he saw that, he said, uh, this is the, the dream of every performance engineer. So that's one of the quotes that I love the most, uh, uh, in recent years. Especially from someone like Tristan, by the way. Alright, wrapping up, we, we provide this ease of use across different customer personas, and most of what I showed you today is going to get open sourced. I talked about the Niki compiler. In addition, the Torch native training backend is going to be open sourced, and we're also open sourcing a neuron kernel library, which is a suite of pre-optimized kernels that we built for our use cases that we want to make available to the world. Just before I pass it to to Jay Gray, as you can imagine, we're deep into implementing training 4. It's a little early to share the detailed facts, but we're just accelerating over time what we're shooting for and we'll probably. Exceed is 6X performance uplift in FP4, 4X memory bandwidth uplift, and 2X memory capacity uplift. The energy efficiency uplift is going to be tremendous, but I'm not ready to share that just yet. Jay Grey, why don't we talk about how we're actually using these chips? Thank you guys. Let Awesome thanks Ron, thanks Joe. It's a pleasure to share the stage with you guys. I'm, I'm super stoked to be here. Uh, hi everyone, my name is Jay Grey and I'm the training inference lead at Anthropic. So Anthropic is the fastest growing business at its scale in history. Our cloud 4 and Cloud 4.5 models are the most trusted AI by enterprises all over the world, and especially with our release just last week of Cloud Opus 4.5, uh, Cloud is the best coding model in the world and the best model for agentic workflows, and the key to all of this is that across all of our product surfaces across our first party API. And AWS bedrock, every usage of clawed code, our web apps, our mobile apps, the majority of our traffic today is served on Tranium 2. So what enables us to scale model inference like this? My team's job is to provide the core inference technology on ranium that enables us to scale at such an unprecedented rate. And today we're gonna take a deep dive into the kind of performance engineering work we do that enables the scale. So, what is it that we actually do? Our, our work is fundamentally about running our models as fast as possible while serving an exponentially growing set of customers as efficiently as we can. Simple job. Every time we shave 10% off the pre-fill time of our models, it opens up new product use cases. Uh, more ergonomic uses of longer context so you can put your entire code base in the context, uh, faster response times to enable more ergonomic, uh, interactive use cases, and every time we increase the token generation speed, it enables Claude to think a little longer, your code to get written a little faster, or perhaps it enables us in the back end to increase the sampling batch size and silently serve your traffic a little more efficiently. At Anthropic, every operation and every kernel of our model inference is designed to get the best performance out of ranium chips. So today I thought it would be fun to take you all the way deep on a dive into the kind of performance work we do on a day to day basis. So to start, let's have an overview of Anthropic's custom model architectures and custom kernels. OK, this is a bit of a joke. I'm not, we still do have some trade secrets, and I'm not going to literally run you through our model architectures, but I am gonna take you through some real optimizations that we've done, uh, on a realistic large scale LLM inference kernel. So this is going to be our playground for the next 5 or 10 minutes. This is a real fused flash attention kernel in 3 parts. It starts with uh a large scale matrix multiplication that generates the queries, keys, and values, uh, that are the inputs to the self-attention operation that Ron described earlier. There's the actual self-attention op, and then it ends with another big matrix multiplication that projects the outputs of attention back into the residual stream space. Um, before I really get into it, I'm just gonna give a very quick overview of the neuron architecture. Uh, if you're already programming in ranium, this is a review for you. Uh, if you're more familiar with programming other architectures, then this is hopefully just a, a quick and interesting overview of the neuron core architecture. So at the core of every ranium chip is a set of neuron cores, and in each core are a number of different engines which specialize in different linear algebra operations. So at the heart of this is the tensor tensor engine which does small tiles of matrix multiplication. And if you take just one takeaway from an ML performance or a kernel optimization talk, it should be this. The goal of a kernel and the goal of a kernel engineer is to make sure the tensor engine is always doing matrix multiplications. Uh, everything else is essentially auxiliary data movement and extra operations to ensure that when the tensor engine is done with one matrix multiplication, the data needed for the next one is ready to go in, and we densely pack our mammals. The vector engine is an engine which specializes in doing reductions and processing over streams of data like a summation on a vector, and the scalar engine specializes in doing element wise operations like activation functions or the exponent part of of a soft max. The last engine here is a fun innovation on, on the training architecture called the GP SIMD or the General Purpose SIMD engine, which basically lets us write arbitrary C code to operate on our data and uh basically fit in whatever weird operation into your custom architecture that doesn't fit into the other engines. Uh, all of these engines read and write to a set of fast SRAM memory banks near the engines, uh, called Sbuff and PSum, and I won't get into the difference between the two of them here. And there are a set of DMA engines which shuttle data back and forth between the fast SRAM memory close to the engines and the larger HBM on chip. So back to our flash attention kernel, what you're seeing here is the actual profiler view of a real kernel, uh, and every row here corresponds to one of the engines that I just described, and every, every line is an actual operation happening on one of those engines. Uh, and what you can see here without even diving into the numbers is that we're doing pretty well here. Visually you can just tell the tensor engine is densely packed with these blue matrix multiplication operations. This is, this is looking pretty good, uh, but how did we get there? So for the first optimization we're gonna dive into the first of the big matrix multiplications which is the QKV. And let's start by looking at a single operation happening on the tensor matrix, and I'm gonna pause here for a moment and really belabor this point because I think this is maybe my favorite thing about programming on ranium, is that what you're seeing here is the actual ISA readout of a single 128 by 128 matrix multiplication operation, one of many that happens within a kernel, uh, within a, within a full forward pass. And what you're seeing is the full readout here down to the nanosecond, the individual like bytes of memory space, uh, that are being read from and and written to. This is exactly what is happening and this, I mean if you, if you're used to programming on other Checo architectures, you understand immediately how cool this is. This is a level of visibility into the performance of your kernels that you really just don't get anywhere else. Every flop, every nanosecond, every byte of memory in every operation of every kernel can be traced to this level of detail, and this is what enables us to get the maximum performance out of cranium chips. So here we're starting with a well densely packed matrix multiplication in the standard B float 16 format, but a lot of modern LLM inference, uh, especially in decode, is about using smaller, more efficient data formats which which Ron alluded to. And ranium 2 is designed to get twice the speed out of the smaller FP8 formats that that you get out of a full width B float 16. And so by moving these operations from the slower B float 16 into a faster, in this case FP8E4M3 format, we immediately get twice the speed up or 2x speed up on this matrix multiplication. Next, let's dive into the actual self attention op. So this is, you can already tell a bit more of a complex kernel, and optimizing attention is one of the most interesting problems, I think, in modern LLM in modern LLM inference. It's a much more complex optimization than uh working with a single, single matrix multiplication because there's just a lot more optimizations, or a lot, a lot more operations in there and a lot more opportunities for bottlenecks, uh, that stop your kernel from spending all of its time in, in matrix multiplications. So what you can see here visually is unlike the matrix multiplication that we were just looking at, if you look at the green tensor matrix, the third row from the top, uh, we are not densely packed doing mamos the entire time like, like we want to be. What we see is these bursts of matrix multiplications, but then interspersed with these gaps where it seems like we're actually doing a large number of these small vector core operations. And when we dive in here using the profiler view that that that I was just showing you and reading the ISA view, what we can see is that the bottleneck here is not doing matrix multiplications like we want. The bottleneck here is actually in shuttling the results of the matrix multiplications between one memory. Bank in another using an inefficiently large number of these small vector core operations and when we realized that, we rewrote the tiling such that we move uh memory from one bank to another using a smaller number of larger vector core operations, amortizing the instruction launch overhead and making better use of the instructions so we spend more of our time in the matrix multiplications and just by touching this, we get a 13%, 13% speed up in attention. Maybe it doesn't sound like a lot, but at the scale at which we operate, this is a huge amount of chips saved, a huge amount of extra traffic that we conserve. Let's talk about comms. So it's been many years now that that modern LLMs are large enough that they don't fit on a single chip, and a lot of the interesting design space that we have as performance engineers is how to split up and shard the uh data and the computation of a full LLM forward pass across multiple chips and then communicate between them using collectives uh to, to arrive at the correct results. So ranium, like most chip architectures, operates, as I said earlier with a smaller amount of a fast SRAM memory bank that communicates with a larger amount of HBM. And by default what happens here is in order to do a collective operation you take the result of one of one of your operations, you shuttle it from the fast SRAM down to HBM, you do a collective from HBM to HBM of different chips, and then shuttle the result of that back up to SRAM. And especially in token decode when you're trying to stream tokens as fast as possible, this 3 step memory movement is terrible for latency and uh and if you're unable to overlap your comms with with other computation, uh, spending time in comms like this is just the death of a low latency kernel. So what ranium allows us to do in this in this cool optimization is take use of, of one of the cool hardware features that allows us to do direct collectives from SRAM to SRAM along different chips and saving the extra hops of memory between SRAM and and and HBM. Um, and so what you can see here, it's, uh, not super obvious, but I've, I've, uh, notated, uh, with the red circles the GPSMD operation which on the left is spending all of its time writing descriptors for the memory movement DMAs, uh, between Sbuff and, and, and, and HBM goes away in the right, and we, and with the faster SRAM to SRAM collectives, uh, the amount of time that we spend in comms is lower and the, the latency of our decode is faster. The last, uh, last optimization, of course, is to run this kernel on ranium 3. Every operation that I've described today gets faster on ranium 3. The double speed FP8 mat molds that we looked at in the first optimization are 4X speed and trannium 3 and make use of the microscaling architecture that Ron described to do more efficient blockwise quantization and and dequant. The vector and scalar operations that can so easily become the bottleneck of a complex real workload like attention are made faster in ranium 3. the comms are made faster. The amount of HBM capacity per ICI domain is larger, which lets us serve larger models on, on a single ICI domain. Uh, I could go on and on. And uh in this case, the kernel that we've been working with for the last 5 or 10 minutes, uh, which achieves after the optimizations about 60% tensor engine utilization on tranium 2. Gets to over 90% on ranium 3. And so I'll leave us there. Um, a year ago we announced Project Rainier and Anthropic announced its initial use of, uh, of ranium chips. A year later, we're serving anthropic models on nearly a million ranium 2 chips, and we're so excited to see in 2026 and beyond where we can get with ranium 3. Back to you, Joe. Hm Great, just another round of thanks for both Jay Grey and Ron did a great job really deep diving into the details on, on how they think about optimizing ranium just broadly across the stack. Um, a few takeaways before we leave. ranium 3 is generally available. Um, the chip was announced yesterday at Matt's keynote. Um, you know, think about tranium 3, not just in the chip, but also the system. You saw how important the system is for the optimizations as Jay Gray talked through kind of not just the flops, but also the ICI bandwidth or what we call neuron link, um, and we're building systems that really scale with 144 chips, the tranium 3 ranium 3 Ultra Server. And then the other part, uh, you know, it's easy to get started. We're making it really easy, so we have a lot of information available about neuron SDK, um, and, uh, we wanna make sure that you can get out there and learn. Uh, so, uh, if you have time tomorrow and you're interested in learning more, there's a few workshops available on Thursday, uh, so definitely recommend checking it out. You can scan this and see the full list. Uh, to learn more and if you don't have time on Thursday, uh, you can always, uh, scan one of these, get started, and quickly, you know, ramp up on your own independently. Uh, we have a lot of tutorial, tutorials available, um, and we're really excited about, you know, having folks develop on, on training across the board. So with that, uh, just to thank you again to everyone for being here, we really appreciate it, um, and uh if you have some time, please complete the survey.