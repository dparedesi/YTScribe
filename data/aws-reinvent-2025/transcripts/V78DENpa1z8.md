---
video_id: V78DENpa1z8
video_url: https://www.youtube.com/watch?v=V78DENpa1z8
is_generated: False
is_translatable: True
---

Today's session, you're gonna get to hear from two large enterprise customers about how they use SAP as well as AI to transform and to innovate for their business. You're gonna hear from Gustav Hilding, who's the chief technical architect at Ax Food. Ax Food is a large grocery retailer that's headquartered in Sweden. You're gonna hear from Varda Reddy. Uh, Varda is the director of SAP Technology and Transformation at Harman International. Harmon is a, uh, pro and consumer audio company that owns well-known brands such as JBL speakers and Moran stereos and many more. My name's Eric Kammon. I am a member of our worldwide SAP on AWS uh team. We have more than 7000 customers today that run SAP and AWS. Uh, I lead a team of solution architects that are focused on building, uh, solutions and services and guidance to support all of our SAP customers on their journey to the cloud. So, from an agenda today, I'm gonna give a brief overview of some AI services and solutions and use cases for SAP. Then we're gonna hear from AX Food about how, uh, how they're using AI machine learning to analyze SAP data and their business processes. Uh, and then we'll hear from Harmon. They're gonna talk about how they're starting their S4 journey and how they're using Gen AI services to analyze and transform with, uh, their ABEP code. We will have some time for Q&A. It's not gonna be live, live up on stage. It'll be kind of behind the seating area after. So please stick around. Love to, love to talk after if you have time. OK. Quick show of hands. Uh, who's first reinvent? First time reinvent? OK, quite a bit. Uh, what about, uh, more than, well, more than, more than one? Quite a bit as well. Good, good, good. So, for those of you who attended last year or even in 2023, if you attended any sessions about GENEI for SAP very different message last year than what you're gonna hear this year. Last year, uh, we had a lot of questions from customers. What can I do with Gen AI? What are the possibilities, and we'd show, uh, that you could do with Gen AI for SAP. And we showed a lot of demos and Art of the Possible. Fast forward to 2020, end of 2025, and we now have real customers like you're gonna hear. With Harmon and Ax Food sharing real stories about the, about the value that they are bringing now. So 2025 is a very pivotal year where we started the year with um a few customers that were early adopters. We had a lot of customers that were, were doing proof of concepts or evaluations. But you fast forward to the end of 2025, we're exiting the year with a lot of customers that are moving, uh, Gen AI solutions into production. We're starting to see mainstream adoption of many Gen AI services, um, and a lot of, you know, other kind of production-ready services that we have ready to go. This was made possible by a rapid advancement across a few areas. So, first is the maturity of large language models. Um, I'll, I'll, I'll call out anthropic. We use anthropic cloud, their sonnet model quite a bit, does a great job of, of analyzing and interpreting SAP data as well as SAP. ABA code, um, the beginning of the year Anthropic Claude Sonnet was on version 3.5, then it went to 3.7 to 40 to 45. Each of those releases was a major step change in the maturity and the capability to analyze and and interpret SAP business context. We also saw industry adoption. For some, uh, some kind of open standards and some and some protocols across the industry and adoption within AWS services. So for example, you see MCP up there. MCP, if you're not familiar, that's model context protocol. This is an open standard that when you build AI agents, it, it, it shows you how you can communicate with back end data sources. We're starting to build solutions. We have some available that use MCP to communicate with SEP using. Uh, the O data interfaces or, uh, the ABAPT test cockpit. So, you know, overall, very pivotal year 2025. I'm really excited where we're exiting and seeing a lot of customers adopting Gen AI for SAP, and this will accelerate further as we go into next year. OK. Uh, we have more than 200 services. You've probably heard that before. When you start to narrow it down to say, what are the main services I should be looking at 4Gen AI for SAP, here's the shortlist. There's more services than just these 4, but these are the 4 core ones. Starting on the upper left is Amazon Que Developer. This is an AI coding assistant. It works across almost all programming language, languages, does a very good job with all of SAP's programming languages and frameworks, including their, their, their cloud application framework cap and wrap also works really well with ABAP. We released an MCP server called the ABAP Accelerator, uh, just recently. The ABA accelerator allows you to do some really interesting use cases, uh, with Q Developer. For example, it will automatically generate unit test cases for your ABA code. It will also take legacy ECC code, automatically convert it to, to the S4 compliant code, right? So this can, so Q Developer overall can really help you on your S4 journey, uh, if you're moving to Clean core, and there's a lot of interesting use cases that can help, and then a lot more beyond that. Upper right is Amazon Quick Suite and that family of services also is Q Automate and Quicksight. Um, QuickS Suite, this is an AI business intelligence platform. It allows you to aggregate data from multiple sources such as your, your own emails, your own files, as well as interface with enterprise data like SAP, Salesforce, ServiceNow, and a lot more. Once you gather the data, you can chat with it agenttically, um, you can drill down, you can, you can build your own, uh, dashboards. You can even build your own agents with QuickSuite. And we're seeing some really interesting use cases customers are starting to build to automate accounts payable, accounts receivable, month-end close processes using QuickSuite. The bottom two Amazon Quiro. You'll probably hear a lot about Quiro this week, um, at reinvent. This is our new Agentic IDE or integrated, uh, developer environment. We're seeing some really interesting SAP use cases with Quiro such as building BTP applications or Fiori or UI 5, front ends, um, and then Bedrock and Bedrock Agent Core on the bottom right. Bedrock is. Our main AWS service for integrating with uh large language models like Anthropic or Amazon's Nova models or even OpenAI now um and then Bedrock Agent Corp, this is a runtime environment for AI agents so if you, you could build AI agents, run them in in Agent Corp, it allows you to deploy them into a highly scalable, highly secure, highly performant, uh, runtime environment. Um, we have sessions and workshops this week on all of these services. We have sessions that are SAP specific and of course, we have some that are not SAP specific as well. So if anyone has any questions about any of these, come find me after the session and I'll, uh, point you in the right direction. Um, OK. So without further ado, I want to get into our customer success stories. So, Gustav, I would like to bring you up to the stage and, um, hand it over to you. So Eric, thank you for the introduction. Um, Ax Food is a leading food retailer located in Sweden, Europe. We're on the other side of the ocean, back where the time difference is 9 hours, so I'm here at midnight right now. We have around 800 stores. And meet millions of customers every week. Axford is one of the top 60 companies on the Swedish stock exchange. Our company consists of different brands, and our customers, they recognize us as supermarkets, grocery stores, convenience stores. With different logos, different prices, different concepts. We operate both within B2B and B2C businesses. We are a family of brands, but we share common operations when it comes to purchasing, logistics, business development, IT, and other supporting functions. Our market share is continuously growing. We currently sell about 25% of the food that is being sold in our country. This means that both growing volumes and keeping a tight focus on our costs is key for our success, since retail is a business, especially food retail, is a business with high volumes and low margins. So we try to standardize whenever possible. Our SAP landscape consists of a vast amount of systems. Mainly we're running SAPS4 and SAP car. In SAPS4 we're running the industry solution retail as an add-on. This means that we can keep much of our most data in S4, so we keep our articles, our assortment, our prices, our promotions, our sales orders, purchase orders, inventory, and point of sales data in SAP, besides the traditional ERP data, of course. For maintaining our forecasts, we're running SAP EWM and SAP FNR. Besides traditional systems, we are also running. Sub-commerce cloud and sub Success factors. Here we keep our e-commerce orders, our employee data, and our content services. Looking at a process view, we can categorize our systems into different application layers. S4 is the basis of many well established flows. We do a lot of custom development in S4 as well. Subcar has got more unique capabilities and is used for retail specific processes such as merchandizing and assortment planning and promotion planning. CAR is the abbreviation for Customer Activity repository, by the way. So it's got most data objects besides customer data, it doesn't have customer data. We keep our customer data outside of SAP. AWS is used to enhance our systems with analytical AI. AI machine learning and composite applications. We also keep our data stack on AWS. The systems on AWS typically have a shorter application life cycle. I've also depicted some other recipe systems to the left, depending on their uniqueness level. So we manage our SAP suite ourselves. We're currently running the sub as for Hana on-premise edition. Without the involvement of any third parties, we've got our own staff, own employees, and do everything ourselves. Our business processes that need differentiation are custom developed in Abop and have been so for quite some time, over 15 years. We've got full control over the code, the logic, and the development. We're building integrations on SAP BTP with integration suites. To give you some numbers, our Hono database is over 6 terabytes in size. We're running over 4000 CPUs. And 100 140 virtual machines. So going into our use cases, well, in retail, many flows are about moving goods from point A to point B. We also handle fresh items, so we really need to have the stuff at the right place at the right time. Therefore, a lot of our focus is on optimizing forecasts and volumes. We currently have more than 100 machine learning models in production. We anticipate that we can get around 30% better accuracy in our custom built models than we get in our standard systems. Especially when it comes to irregularities or deviations in our flows, this is because we can build them up with more history and more data. We're currently running around 20 different use cases in forecasting for both stores and our warehouses. Furthermore, we use simulations for our assortment planning. And use forecasted volumes in combinations with customer data to get the desired outcomes for both us and our suppliers. In Acom, AI helps us to deliver the relevant products for our customers. By using AI we get better stability. Better sustainability, less food waste, and a positive impact on the environment. A few examples of our optimized flows are being realized by traditional machine learning. We use AL for campaign forecasts, seasonal forecasts, sales forecasts, and e-commerce forecasts. A simplification is that all forecasts that require more data than we keep in SAP, they are custom built. In our warehouses, we optimize where to place items depending on how much they are sold. In that way we can get the shortest picking routes for our manual pickers. Regarding e-commerce, we personalize the user experience online. And in our customer journeys. This is done by building models with article relations, for instance, related items. And predicting the next best action. This can be used to drive cross-sell, upsell, and other forms of recommendations. Our shopping baskets are normally around 50 items, so clicking add to basket 50 times is quite a hassle for our users. So everything we can do to simplify the user experience is of big value here. We also present users with their most commonly purchased items and inspiration on what their peers are buying. In our assortment planning, there is a constant tug of war between different parameters. We use simulations a lot here. For instance, we simulate what happens if you change the price. Your volumes are probably going to go up somewhere, but then again, another item will sell less. So we try to optimize for the optimal outcomes with machine learning models in our assortment planning. We improve our customer offers by creating clusters of customer groups, and we also allocate personalized offers for our customers to bring them to our stores. In our support functions, we make data-driven decisions and we share aggregated sales data with our suppliers. We never share customer data, but we share it on an aggregated level. This helps our suppliers plan their flows better and it also helps us get our flows better in turn. We also use a couple of AI models for our internal decision making here. Of course we're also using Gen AI for employee productivity tools such as chatbots and similar applications. Another example of Gen AI usage that we built is a design tool for creating images. It's a fine-tuned stable diffusion model on bedrock. Trained on our design language of our own brands. We gave this to our business and they played around with it and actually created package designs for milk cartons. These are actual items that were sold in our stores. Well, this was all about bringing attention to what can be done with AI and also got our leadership on board with what can be done. So awareness in departments outside of IT was a key benefit here. So how did you get here? Well, our SAP history started way before 2010. We started building our current data layers in 2012 when we introduced a loyalty program. And an enterprise data warehouse. A few years later, Axford decided to go into e-commerce. We needed a solution that could scale up and scale down according to our purchase patterns. So this was when we introduced AWS back in 2014. Being on AWS made us see amazing launches like Sage Maker, Lambda, Fargate, and other cool technology within AI. So we decided to place our entire AI platform on AWS a few years later. However, running AI in the cloud and having a warehouse on-prem wasn't really the perfect solution. We got into scaling issues and decided to move our entire data platform to AWS back in 2022. Since then, we've been building lots of innovative solutions and composite applications on top of both SAP and AWS. As you can see, our cloud journey started in a small corner of our company with e-commerce and currently covers most major processes. So, since we introduced our loyalty program back in 2010, we had needs for creating traditional reports. Well, we introduced the principle to export all data, SAP data and non-SAP data into our data stack. On top of our data stack, we run AI models and return the results either as machine learning models or through APIs or as result sets back into our enterprise systems. It's a simple picture, but I'm going to go into more details. So one key part of our AWS AAI stack is a self-service platform that we call Memmer. Mimma is a suite of AWS tools, mainly Sage maker. And airflow. We both create and run ML models in Memur. We also use AutoML to choose the optimal machine learning models and produce the optimal results for each use case. With Mimmer, we can build data pipelines, either through manual files, through third party data, from our warehouse, or even through web scraping. All data is imported into S3 where it can be used for further AI development. Our data scientists work in Sagemaker's Studio and can explore large amounts of data. We prepare inputs in Sagemaker. We train models, evaluate the results. And deploy artifacts. When results have been calculated, we return them either as machine learning models through APIs or as result sets into our source or into our target systems. Currently our data stack has grown from being just a data warehouse into an entire data ecosystem. Data is generated in our application layer, mainly in our SAP systems. After that it's integrated through the integration layer, either through EventBridge, Event mesh, or integration services from CPI as messages. When data lands in our BI stack, we use DBT and glue. Amongst others, to calculate our ETL and data layers. On top of the BI layer, we place our machine learning layer. Where we can do traditional ML with Memer or generative AI applications with our AI gateway. On the bottom of the picture, I've depicted our analytics layer where we use micro strategy to analyze data from our warehouse and sub analytics cloud to go directly to our SAP systems. So our data stack is continuously evolving. Some of the key points for future extensions are better focus on data integration with SAP. We always want to have more data in our data warehouse layers. We also need to improve our business with more AI models, more real-time and new use cases. A lot of our focus right now is on building data products. Building data products also requires data governance and has an impact on our semantic layers, which are continuously evolving. Furthermore, we need to support the world of Agentic AI better than we do today, so we have an initiative to look at MCP servers and other technology to have our data AI ready. Finally, we need to do some rearchitecture of legacy systems, for instance, sub PW. Finally, I would like to introduce some of our lessons learned. We've treated data as a foundation for improving our business since 2012. This has led to a long-term focus on building multiple layers of data. Data needs to have a single source of truth, be clean and ready for AI. We also introduced some high value use cases early on. This made our leadership introduced into what can be done with AI. When our leadership was on board, we got a lot easier to get funds released and to create the teams for the necessary machine learning development. Last of all, I would like to thank AWS for the amazing AI technology that we're using and for the enablement services and good collaboration. I would also like to reach out to my colleagues and say a big thank you. Thank you. OK, thank you, Gustav. Stay up here though. I have one question for you. Uh, love to hear you. I love your story. Thank you. You were early adopters in SA in Gen AI for SAP or AI for SAP. Uh, a lot of the work you did started before mainstream adoption of large language models. Could you just share a little bit about your thoughts about transitioning from, I'd say some of your homegrown. Uh, Machine learning models and what you're thinking about with adopting some of the large language models in the future. Yeah, so we're, we're already looking into Q. We're using that for development in all our teams, be it SAP or non-SAP right now. We're also looking at the world of agentic AI like I said. Uh, we really need to get more AI generative AI use cases on board with agents. So that's really an area for exploration for the future for us. Awesome, awesome. Well, thank you again for coming here and uh sharing your story with everybody. Thank you. OK, so next year we have, uh, we have Harman International. So I'd like to bring Varda up to the stage. Good afternoon everyone. Um, my name is Varda Reddy. I work as director of SAP Technology at Harman International. So thank you, Eric, for the introduction and Gustav for a nice presentation. So first I will start with a quick summary of what my company does, and then I will explain what are the SAP products we are using at Harman International, followed by our S4 HANA implementation approach, and then I will highlight one of the key challenges we faced during our S4 implementation and how we solved it with AWSA capability. So a brief introduction about my company, um, so Harman International is a global um company specializes in connected audio technologies, um, providing. Speakers, audio systems in automotive and consumer divisions. So it is headquartered in Stamford, Connecticut, and the company was founded about 75 years ago, and the parent company is Samsung Electronics. The company was acquired by Samsung Electronics in 2017 and since then it has been. Part of Samsung Electronics. So some of the well known brands of Harman International are JBL, Harman Gordon, AKZ, and also there is a recent addition, Bowers and Wilkins. You might have been familiar with some of these products. And the company has two major business segments in automotive solutions. Uh, the company provides infotainment systems, um, ADA systems, and connected car platforms, and in consumer audio it has headphones, speakers, sound bars, and multi-room audio systems. And also, um, Professional solutions is a subdivision of, uh, lifestyle division, so which provides audio equipment for studios and live events and enterprise environments. So let me, um, quick, uh, give a quick overview of Harman's SAP applications. So we have two SAP ECC landscapes. Both are running on ECC 6 and EHP 7, and we have SAP BW for data warehousing, and we have IVP for SIOP supply chain, uh, planning and optimization, and we have SAP Ariba for indirect procurement, and we have BPC and BW 4 Hanna for planning and consolidation. And then, uh, we have several manufacturing integration intelligence and manufacturing execution systems for, uh, managing the operations at global plants. We have manufacturing plants and then we have BTP, a Business Technology platform, um, on Business BTP we have integration suite enabled and also we are using, uh, some of the other modules for build process automation, build work zone, etc. And we have Bob the Data Services and information steward for data profiling and cleansing, and we have SAP GRC for user provisioning and governance and risk compliance, and we have supplier business network and some of the SAP technical solutions like solution Manager for change management, custom code life cycle management, and NWDI for Java development and portal systems. And let me zoom a little bit on our ECC landscape. So we have two ECC landscapes, as I mentioned. Both of them are running on ECC 6 EHP 7, and as I'm going to highlight a problem very specific to custom code, so I'm only covering the details related to customizations we have. So we have 30,000 custom objects in one of the ECC landscapes, and on the second landscape we have nearly 20,000 CBOs and. As part of for migration, these are the key considerations we have, so we wanted to improve some of the business proceeds running on. Environment as part of S4 migration to improve the efficiency of those business processes and also we have a finance transformation plan. So as part of this finance transformation, the goal is to improve the enterprise architectures and as part of that. We are planning to consolidate some of the company codes and harmonize some of the chart of accounts and also reduce the technology debt by getting rid of some of unused or low used third party solutions and replace it with standard S4 functionalities and also get rid of some of the unused or less efficient interfaces and improve the code and configurations. And also another goal is to reduce the database footprint significantly, so even with Regular archiving on the current ECC environment, we are unable to reduce the footprint significantly beyond a certain extent. That's mainly due to a lot of third party related tables have grown rapidly over the period of time and also several custom tables which are not part of the archiving scope contributed significantly to the size of the database. So reducing the database footprint is another objective. And then another objective is reducing. Streamlining the custom code we have by eliminating unused customization or replacing with standard functionalities wherever possible. And one of the drivers to reduce the database footprint is downtime minimization during the go live. So as our manufacturing plants are operating 24/7 downtime. Causes business impact, so reducing the downtime to a weekend is necessary. So coming to Harman's S4 implementation journey in specific, so as I mentioned, as one of the key objectives is to reduce the data footprint and also do the finance transformation, improve the business process, we selected uh selective data transition approach. So as part of this, we are migrating only the open transactions along with master data and customizing data. And as part of the finance transformation, like I said earlier, uh, we are planning to consolidate the company codes, harmonize the chart of accounts, and adopt SAP best practices wherever possible. And then coming to the custom object racialization, um, the objective is to streamline the customizations by eliminating the unused code and replace, uh, less efficient custom code with as per standard functionalities are. Improve the efficiency of the code, etc. And the landscape build is being done with shell approach, so we are using SAPDMLT services to migrate selective data from ECC source to S4 environment. So as I just mentioned, one of the key challenges we have is, um, as our systems, ECC systems were implemented more than 25 years ago and over the period of 25 years, we have added a lot of customizations in the system, and the majority of those customizations have very poor documentation and in some cases very minimal documentation. So it is extremely important to clearly understand what are the customizations we have today in the system so we can efficiently migrate it to S4 and also we can understand what to test and where there is a possibility to replace some of those customizations with S4 standard functionalities and also where we can. Adopt some of the innovations available from S4 so this determines the need um to document all those customizations which are in place today. So some of the key objectives, like I said, eliminate unused custom code. So after doing the initial analysis, we determined that nearly 40% of today's code is not even in use, so that. Requires a thorough clean up and then um we have a requirement to analyze the interfaces and dependencies as a lot of programs which were developed as part of building the interfaces. For the systems connected with the ECC environment, we need to clearly understand the dependencies so we can take care of all the interface mapping. And dependencies, etc. when we migrate the code from ECC to S4. And also another objective is to identify what are all the simplifications happening in S4 and where we can eliminate some of the customizations we have. So as we build these customizations over 25 years, over the period of 25 years. So meanwhile, SAP might have released a lot of those functionalities as standard functionalities with subsequent ECC releases itself, and some of them may be released as part of S4. So this. Having good understanding of what we have today will help us to understand what are the standard functionalities which can replace some of the customization. And another advantage of documenting this customization is we can limit the testing effort by clearly knowing what is really important for business and what is being used, so we can focus on the testing and also. When we are troubleshooting some issues as part of SAT and UAT, we clearly understand the dependencies and the purpose of the code, so we can troubleshoot it quickly. So not doing this documentation will have a business impact because it can bring in operational inefficiencies and escalating costs as we have to focus on more number of objects without clearly understanding the purpose of the current code and also missed opportunities for innovation without knowing what we have today. We cannot adapt what is available in the future releases and there are also integration risks like I mentioned earlier if we don't understand the dependencies and. Mappings in place so we cannot take care of those. Also, it has a strategic importance. So with fully documenting and eliminating unused code or replacing with standard S4 functionalities wherever possible, we can optimize the system performance and reduce the cost of ownership even with future upgrades because we have less custom code footprint to deal with and we clearly understand where there are risks and what are all the key areas to focus. And then um Even with future releases with clear documentation, we can quickly adopt whenever there are new standard functionalities available and replace the existing custommisations. So with all this clear understanding, we know that now we have to document all 30,000 custom objects, so there is no alternative for it. So initially we started documenting the custom code manually. So there are some key challenges with that. So we started with 6 consultants and started documenting. So the performance was very poor and it was very, very time consuming. So in order to meet the project timelines, so we initially planned to achieve this in less than 6 months, and as we were not meeting the goal with the rate of speed. With 6 consultants working on the documentation, we ramped up the size of people working on the task, so we ramped up to 12 consultants. Even with that, there is minor improvement in the speed. Still, the estimation to achieve the goal was 15 months. And the cost of paying 12 consultants over the period of 15 months is expensive, as you know. And also the consultants were collaborating with several functional teams to understand the purpose of the code developed over the period of 25 years and trying to document, and it was very inconsistent and missing the uniformity because each person was documenting in different format and it's not providing the value we were looking for. So these were the practical challenges and then we thought this is not working at all and also. We were unable to meet the timelines for the subsequent tasks planned based on this documentation like mapping this custom documentation with business process, documenting or linking it with the L1 to L4 process like order to cash, um, record to report, etc. So the plan was to use this documentation and association with different business processes um during uh the unit testing also during SAT UAT, etc. But this was not working, so then we looked at alternative opportunities, how we can solve it with less manual effort and how to bring in. High um consistency. So then, uh, we started exploring the AWS AI capability mainly the we started with AWS Bedrock tool. So the initial output from the tool was not up to the expectation even though it was doing automatically and it was only producing the technical details like it scans the program and provided only the tables that program was utilizing and few technical details. Then we collaborated with AWS team and significantly improved by improving the prompt and outputs, etc. and then we were able to achieve what we wanted. So by utilizing AWS Bedrock to quickly scan all 30,000 objects at a rapid speed significantly helped us to achieve the goal within a short period. So the speed compared to the manual processing was much, much higher, so the speed is improved by 6 to 7 times and reduced the total timeline from 15 months to 2 months, and the total cost reduction was more than 70%, and the outputs are very structured, easy to interpret, and very consistent. So this, this is all achieved with AWS Bedrock and later we tried. Amazon Q developer with a latest version of clots on it and it was even providing better output. So let me show you a quick sample. So on the left side what you see is uh the manually generated output by the 12 consultants. So this is one of the sample program output. As you see, it is very difficult to understand unless a technical person. Looks at it and completely puts effort to understand. It is very difficult for a business person or a functional person to understand what is the purpose of the code and what you see on the right side is the a generated output. It is very structured, so the top 4 or 5 lines, if you look at it provides a key summary and then. In the bottom it provides business scenario description of the program so when a business person takes a look at it, can quickly understand what is the purpose of this program and understand as it is written in business language and then what are the key functionalities. So if one of the SAP functional team members looks at it, he can quickly understand he or she can quickly understand what are the key functionalities this program. And then in the bottom as you see. So this is a detailed step by step documentation in technical perspective. So if an above developer wants to understand each snippet of the code, he or she can take a look at this and understand. The complete functionality of the code in technical terms, so you see the difference. So the right A generated output is very easy to consume and also. is very consistent across because the same prompt was used across all 30,000 custom objects, so the output was very structured and very consistent. So one of the key lessons learned based on this is, so whenever you are dealing with some task which is highly manual. And highly repetitive and consuming a lot of your resources, think AI as the first option and see what are the possibilities available from AWS and evaluate thoroughly if you can automate it completely or at least reduce the manual effort to some extent and then. Think about manual effort. So in our case, as it It took some time for us to explore. I want to say this lesson learned with you so you can if you are. Going through S4 journey, whatever may be the task you're dealing with, if it has high manual effort and repetitive in nature, think of AI as an option. And that's all for myself. OK OK, thank you. Thank you. OK, thank you, Varda. I have a question for you as well. Uh, I know you're on a multi-year journey for your S4 journey, and what you're doing with Gen AI analyzing EAP code is is part of your kind of assessment, your early phases of the project. How are you thinking about use cases for Gen AI for the next phases of your S4 journey? Sure, um, some of the use cases we already started eva evaluating is unit testing in development environment. So after we do the code remediation, um. Using SmartShift, we are using a tool called SmartShift to automatically remediate the custom code in S4 environment. So once we do that, and also as part of our finance transformation, we need to do some functional code remediations as we are consolidating the company codes and harmonizing the chart of accounts, etc. So we need to do some functional code remediations as well. Suppose that we need to perform certain level of unit testing in development environment, so which is highly manual. So to reduce the effort in unit testing, we would like to use MCP capability of Amazon to automate some of that, so to quickly scan the remediated code and see. It is consistent and efficient and compatible with S4. That's one of the use cases. And also another use case is whatever the new code we develop as part of S4 implementation, we'd like to use Bedrock or Q Developer to document it. So manually documenting the new custom code. Can lead to inconsistencies and the developers sometimes if they are lazy, they don't fully document all the details required and later it becomes very challenging to troubleshoot. So to maintain that consistency, I would like to introduce those tools and make it as a mandatory step for all the developers to use those tools with a specific predefined prompt so that way all the programs will have mandatory details in place. So which can be easily referred later for troubleshooting or making further enhancements in that code. That's another use case. And the third one, we are thinking of is um using agentic capability of AWS to scan historical transactions in ECC environment and capture the data used in different transactions and use the same data in automating the testing. In S4 environment, so manually generating those test data is very, um, big task and difficult. So if we are successful in doing that, it will significantly help in automated testing. Yeah, got it. Cool. Well, hey, thanks for the answer. Thanks for presenting here today, and I look forward to hearing, uh, how the next couple of years of your S4 journey go. Thank you. Thank you. I would like to take this opportunity to thank AWS team for their significant help on, uh, utilizing some of these AI capabilities. Thank you, thank you, Verda. Please do uh go into the app and complete the survey and hope you enjoy the rest of your reinvent. Thank you for joining today.