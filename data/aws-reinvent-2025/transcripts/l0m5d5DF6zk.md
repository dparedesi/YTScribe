---
video_id: l0m5d5DF6zk
video_url: https://www.youtube.com/watch?v=l0m5d5DF6zk
is_generated: False
is_translatable: True
---

Thank you all for being here. Uh, I know Werner's doing a keynote right now, and so I will channel him a little bit and talk about one of his famous quotes. Uh, everything fails all the time. Uh, I want to highlight the second part of that quote that build for failure and nothing will fail. Um, having your applications not fail is pretty, you know, standard, uh, for, for all of us that we wanna build, uh, and it's really important in, in a lot of domains, but it's, you know, most important, I would say, uh, when you're building humanitarian workloads, things that are meant to help people in need, uh, during a catastrophic event. Uh, in our world today, you know, unfortunately, we live with constant humanitarian crises, um, and so we want to be able to build our applications to meet the needs of our users who are suffering in one of these events. Um, it's not OK for a 404 or a timeout when someone is, uh, you know, calling for help in an emergency like a flood. And so having high reliability and resiliency is really crucial in these humanitarian workloads and as builders and operators of these workloads, we need to prioritize building in resiliency, uh, when we go to develop our systems. And so you don't need me to tell you that building resilient systems is hard. Things that are highly available, self-healing, maybe even multi-region, these are complex technical, uh, tasks that need attention and effort, um, and so. So what we want to talk about today is how we can leverage tools like generative AI and in particular agentic AI to help make that easier for us. How can we take these new tools to help improve the resiliency of our workloads? And so before we get into the actual building of this agent, let's first go over some basics of resiliency so that we have a, a common lexicon as we're going forward. Um, as we think of resiliency at AWS, we have this mental model broken into three different parts. Uh, the first is around high availability. This is the concept of how can our, you know, primary, uh, application, our active site of our application withstand common failures that might impact the availability of our service. Uh, these are things like a server going down, uh, a network interruption, things like that, that, you know, if we want our applications to be highly available, we want it to be able to be resilient to these common failures that happen, you know, pretty much every day. The second topic is around disaster recovery. This is on the, there is a major impact that is influencing our primary site and so we need to fail over to a secondary site and how do we do that? So this would be a, you know, a disaster hits your primary site of your application where it is no longer able to serve traffic and so you need to fail over to a secondary location. Um, and how quickly are you able to do that, and how are you able to then with, uh, serve that traffic from your secondary site. And then also the fail backwards, right? Fail back over to your primary site once the event is over. And so then the third component is the continuous improvement of your applications over time. So we all know this is true for, you know, new features, uh, improving performance, but it is still as critical for resiliency as well. Once you launch an application, it's not done. It doesn't, it's not static. You need to be able to also improve resiliency over time based off of, you know, learnings of failures that will happen in your application. So taking the, the learnings and the hard lessons from failures that you withstand, putting it back into the application, uh, and then just over time as you add new features, new problems can arise and just building over and over, iterating on that to improve it. Um, with that mental model in mind, we then have different categories of failures that we define. So these five categories are those, um, buckets where errors will occur, failures will happen, and so we bucket them into these five categories for ease of, um, uh, you know, identification, but also for remediation. So starting with the first one, you know, most people are familiar with this. This is single points of failure. So in your design of your application, if there's any component that is by itself or, you know, running on a single box, anything like that where if that component were to fail, it brings down your application. Uh, and you have a bad day. So, uh, when we're looking at this, we're looking at how do we, uh, expand that single point of failure to be not a single point, but have, uh, redundancy in it so that it can withstand any individual failure. Next is excessive load, so looking at is our application able to withstand a sudden increase in load and not collapse under its own weight. So this could be anything from making sure you can scale elastically either ahead of the load or in response to the load. Uh, and also making sure that you have the headroom, whether it be, um, service limits within AWS or, um, whatever capacity you might need to start to, start to accept and, uh, handle that traffic. Next is around excessive latency. So is our application built to withstand any kind of network interruptions? Um, are the timeouts that we're building within our code able to accept, um, you know, sometimes it's 5 milliseconds, sometimes it's 200 milliseconds. Uh, will our application handle that increased latency of different calls between dependencies and be successful in handling that kind of, um. unknown or uh not start to think that other parts of the application are down if you have an increased latency somewhere in your chain of dependencies. Next is around misconfiguration of bugs, you know, pretty self-explanatory. Is there a bug in your code? Is there a misconfiguration in one of your dependencies? Um, these things, you know, always happen, right? And so are you able to either roll back, roll back a deployment, or are you able to, um, uh, put out a patch in time to, uh, prevent any kind of issues from cascading as you move along. And then the final one is shared fate. So do you have any, uh, common dependencies across applications such that a failure of one of those common dependencies starts to impact outside of its fault isolation zone where you, um, multiple applications will start to fail because of a single component having an issue. Uh, so with these categories in mind, um, you know, if you took notes, great, they will show up later, uh, as we start to look at how can we identify areas of improvement in our applications, these five categories will be, um, you know, the framework for which we're looking for improvements and identifying identifying remediations, uh, where there are problems. So as I said earlier, we are looking to build an agent for us to help identify these, these improvements, um, and so just I know you've been beaten over the head with it all week, but, um, just in summary of Agentic AI, you're building a, an agent to have agency, uh, it will. Be able to take autonomous action on our behalf to investigate our workloads, um, using the foundation foundation model for reasoning and then a list of tools that we're giving it, um, so that it can take action within our account, uh, look at what we're look at the applications that are running, um, take action to find the documentation that is necessary for it to provide recommendations, and then have an assessment of our workload overall. Um, so, a quick architecture diagram of what we're building, it's, uh, fairly straightforward of, uh, we are building this agent, um, uh, and Mike will go through, uh, how we're building it in our code, but as a, as a quick, uh, summary of it, we are leveraging a foundation model from Amazon Bedrock. We're calling in our, our inference is coming from Bedrock. And then we are creating a couple of tools, um, that Mike will go through, but we're also leveraging existing tools by AWS. So, uh, the two that I want to call attention to are the cloud control API. If you're not familiar with this, this is an API that is just exists for AWS where you're able to do crud operations against your account and against your environment for creating any resource, um, uh, reading a resource, anything like that, um, and then the AWS documentation. So both of these are, uh, tools that we're leveraging existing. MCP servers or model context protocol servers that AWS runs to access these resources, uh, and Mike will kind of walk through how that works within the code, but, uh, we are just reaching out with our agent to these existing tools, um, so that we're able to look at our, um, an existing cloud formation stack within our account and. Um, take, you know, uh, evaluate that stack and then use the AWS documentation to provide specific recommendations on how to improve. So being able to look at the current state of our account and our, of our applications and then give us specific actionable things to do, uh, based off that feedback. 01 last thing before we get into the code, um, we'll be looking at an application running in Mike's account. Uh, this particular application, not really important what it does, but, uh, as you start to see the recommendations that the agent is giving us, I just wanna give you a couple, a little bit of context. Um, this particular application is for a, um, an agent that is providing, uh, food assistance, um, uh, based off a blog post that Mike, Mike wrote a couple months ago. Um, but this isn't super important for the talk, but just give you context for what we're looking at as the agent goes through. And so with that, Mike, over to you. All right, so let's build something. All right, I'm gonna build this in Strand's agent. And uh by, by show of hands, have any of you built anything in Strands agent before? Nobody. Awesome, and you are in the right place. So I, I, I'm, I'm gonna step you through the, the build of all of this, and I'm gonna explain what we're doing and how, how we actually built this out. And I'm gonna be creating two pieces. The first is the actual agent that goes and uses the cloud control API that uses an AWS documentation MCP server. It will go to my AWS account. It will pull back resources. It will reason about what's in my account. We'll pull back relevant documentation around those, uh, those 5 areas of resilience we talked about, and it will then generate a report for me, kind of like a report card where I can see, OK, what is the resilience of this workload, and as I change my recovery time objective and recovery point objectives, I'll be able to get insight into whether the workload does or doesn't support some of those requirements that I have. All right? So the first thing I've done is I have created a really basic agent here. And if you know, if you look at the agent. You can see that my prompt is, you are an AI powered AWS resilience expert analyzing and enhancing fault tolerance and recoverability for mission critical applications. Provide actionable, insightful guidance to identify vulnerabilities and implement robust solutions aligned with AWS best practice. and specific business requirements you generate all output in a GitHub markdown format. OK, so you know you can have your output in lots and lots of different ways. I arbitrarily decided that for, for this solution I wanted it to output in a GitHub markdown format. Great. Now when you create an agent with strands agent. There are really two methods that you have to have. And let me show you the easiest one first, that really the smallest one is you need to have a method that returns the status. And in this case, I'm calling this my ping method, and what this does is it just returns healthy, and I decorate that with this decorator app.get ping. And so that tells strands agent whenever a request comes into the slash ping URL it will return status healthy. That's the first method I need. The second method I need is an invocations method, and, and this method is a post, and it returns on slash invocations. OK. And this method is really the, the heart of my agent. So, anytime a request comes in, I look at any JSON that's being passed in, I'm expecting a JSON element called message. That's my prompt that I'm passing in, I'm going to take that, and, you know, that's my prompt. And I'm creating an instance of an agent. An agent is a built-in class in strands agent, OK. Allows me to create a, you know, essentially an agent like we've talked about. And my system prompt here is just the system prompt that I showed you up here earlier. OK, yeah, you are an AWS resilience expert. And all I'm doing is I am creating, uh, I'm, I'm, I'm passing that agent, my prompt, whatever prompt is that I'm, that's, it's, it's receiving here in that JSON messages object. It's generating the result, and I display the result in a JSON format. OK, so right now, this doesn't do anything particularly useful, but this is, this is really the building blocks of a, a much, much more complex agent that we're gonna be building through this. All right? Now when I invoke or when I run this agent, this will be running, you know, if I run this locally, this will be running on port 8080 and you know if I wanna, you know, invoke anything like if I wanna call that ping method I've got to call, uh, I have to curl to, you know, get slash ping if I wanna actually invoke this I have to. Perform a post to that slash invocations, and so just to make this easier, I am going to create a Python script that allows me to easily interact with my agent. And let me show you, actually let me just save that file there. Let me show you what this file looks like. OK. Pretty straightforward. I've got one method to clear the screen. Uh, my, my main method here, when I start it up, it's gonna say resilience advisor, it's gonna have a bunch of equal signs. It's gonna give me some instructions around how to quit, and I am creating, uh, basically I'm gonna be running in a continuous loop, OK, while true, and, uh. It's It's going to ask for some user input. Whatever input I give it, it's going to create that JSON object with that message. So, so really what this user input is, is what's the prompt that I wanna send to my, to my agent, right? It's gonna send that in, or create a JSON object with that. I've, I've got some kind of administrative things here and then it's going to perform a post to my agent that it's running at slash invocations and then if I get a 200 back then I'm going to clear the screen and I'm gonna print out the results and remember. That I, I set up my agent so that it's printing the results or it's generating the results in a GitHub markdown format and so I'm just, I'm using some libraries here to nicely print out that, that markdown format, OK? So let's just run this and see what this looks like. OK, so I've got 2 consoles here now in the real world, you know, if I was running this in production, of course I'm not gonna run this in a console, uh, you know, we'll talk more about how to actually operationalize this in a bit, but just so that you understand how this is all working, you can see how all the pieces work together on the right hand side, this is my agent. So I'm gonna run my agent, and you're gonna see it is running on port 8080, and on the left-hand side, this is my client that I'm using to interact with my agent, OK? So you're gonna be able to, because I've got both of these side by side, you're gonna be able to see me issue commands through my client. You're gonna see those go to the agent, you're gonna see the agent process, do some stuff and return the results back to the client, and then the client will display the results. OK. OK, so I've got a prompt. Well, you know what, this is a resilience talk. I want to improve the resilience of my workload. So let's ask it something. I have a. Workload with the tag of AWS cloud formation, stack name and value of food agent help me understand. The resilience of my workload. OK. So that request is going off to my resilience agent, and you can see here how it's processing. It's going to be doing some stuff, it's reasoning about my request, and in just a moment, we'll get some results back. Now All right, so here's my results, so let's look at this really quickly. So it says I'll help you analyze the resilience of your food agent, and if we look through this, OK, it tells me what to do here, you know, you can look at cloud formation. You can here's some things to look at here. Here's some, yeah, think about a multi AZ deployment. This is not useful, right? This is not looking at my workload. So I built a really simple agent, but it doesn't actually do anything super useful right now. So let's take this to a next step and actually build something that's kind of useful. OK, so let's clear this out. Let's go back. And see if we can improve our agent a little bit. Oops. OK, so I've, I've made some updates to my, my agent here. Let, let's look through and let me show you what I've changed. Notice my system prompt is a lot bigger than it was before. So I've got the same basic system prompt as you originally saw, but now I've got a few other things. Perform only non-mutative actions when using the use AWS tool. OK, we'll talk more about what that means in just a moment. Critical when analyzing a workload, provide your response in exactly this format. I want it to look like this RTO RPO analysis classify as high, medium, low, and brief justification. You remember those 5 areas that Matt talked about for resilience? OK, you remember it was the acronym SEEMS. S stands for, anyone remember? Single points of failure. My first E was for. Excessive load, next E was excessive latency, the M was misconfiguration and bugs, and the S was shared fate. I'm calling them slightly different things here, but I'm, I'm asking this resilient agent now to generate my results according to those five categories that, that are really the properties I need to have a resilient application, and I'm asking it to create, to give me a letter grade, because really what I wanna be able to do is I wanna be able to look and see, OK, for redundancy. Is it an A, B, C, D, or F? And that helps me be able to really quickly express to my leadership whether we need to invest in engineering work or not. OK, the next thing, now you may remember from the previous example, I hadn't actually specified the large language model that I'm using. I don't know if you noticed that. Well, so here, I'm actually telling it what LLM I wanna use. In this case, oops, in this case, I'm using Sonnet 45. And I'm setting this up as a Bedrock model. So I'm using Bedrock, I'm using the Sonic 45 model through Bedrock, and I'm setting my temperature, and there are lots of other parameters that I can set through this as well. OK, I'm gonna skip this letter grade for just a moment, and we'll talk about this more in just a moment. OK. All right, so let's get down to my invocations method, my, where I actually invoke my agent. OK, same kind of thing as before. I'm getting some input from the user. I'm creating an agent here, and this part right here is a little bit different than what you saw before. Uh, when I create my instance of an agent, I'm telling it to use my bedrock model that I set up earlier. So that's how it knows to use Sonnet 4.5, that's how it knows. You know, my temperature and all those other configurations, it's using the same system prompt, but now I'm giving it. Access to a set of tools, and I'm telling you my tools I want you to use are the use AWS tool and the calculate letter grade tool. OK? So, this, this is the fundamental part of how agents work. I give it access to an LLM and I give it access to a set of tools. And in those tools, the agent can unders if the agent understands what the tools are, it can reason about what it needs to do to solve the the problem that's in the prompt. OK, so let's now talk a little bit about these tools. OK, so the first tool is the calculate letter grade tool. That's this guy right here. I created a method called calculate letter grade, and my idea, my idea behind this calculate letter grade is I wanted to, like I said, to be able to really quickly see for shared fate, is, is my letter grade A, B, C, D, or F. So then I, I have a, a quick sense for whether I need to invest some time and effort to improve that. And you know, honestly, this is probably kind of a naive method, but it works well enough. You know, you can see that I am asking for a number of critical vulnerabilities, and these are critical resilience vulnerabilities. The number of high resilience vulnerabilities, the number of medium resilience vulnerabilities, and the number of low resilience vulnerabilities. And I'm, I'm telling, I'm giving some definitions here, and I'm doing these definitions so that the agent can reason about what this method is and how to actually use it. So you can see, I'm saying this calculates a letter grade based on the number of resilience issues in the workload. A workload with a low RTO RPO will have more resilience issues than that that same workload with a high RTO RPO, OK? In other words, if I have a recovery time objective, a recovery point objective of 4 days for each of those, I can have a lot more resilience problems there than if I have a workload with a 2 minute recovery time objective and a 2 minute recovery point objective. All right. I'm, I'm defining what each of those inputs are for the critical, high, medium and low resilience vulnerabilities, and then my method is really, again, pretty naive. If I have any critical resilience problems, I'm saying the workload has a D. If I have any high resilience vulnerabilities, it's a C. Any medium resilience vulnerabilities, it's a B, otherwise it returns an A. Now the only thing I need to do to make, to mark this as a tool for an LLM to start for my agent to start using it, is I simply. Put at tool. and that's using the, the strands STKK so it makes it super, super easy to, I, to, to write tool definitions and get those to work with your LLM. All right, so now remember, when I look at my agent and the tools I'm using. Yeah, so I mark that method with at tool, and then I give it the name of, of the method here. Now this next tool, the use AWS tool. This one is kind of interesting. If I look at where this is defined. If I look at the libraries. In in my, my, my imports here this is built in to the strands SDK. So there is a use AWS tool as part of Strands tools and so, so you know you think about all the stuff if, if you had to connect to your AWS account, all the, the gunk you'd have to build to do that, you know, you'd have to. You know, pull out your. Your credentials you'd have to go to the cloud control API. You'd have to then determine which method of that to call to find each of the resources in your account. Well, I don't have to worry about that because they use AWS tool does all that for me. All right. Should we run this and see how this actually operates? OK, so I'm gonna run my agent here. Should look the same as before. And I'm gonna run my, my. Client, and let me ask the same question as before. I have a workload. With the tag of AWS. Cloud formation, stack name and value of food agent. The RTO Is 24 hours and the RPO is 12 hours. Help me understand the resilience of this workload. All right, and what we're gonna see now. You can see that the agent now says, all right, I'm gonna help you analyze the resilience of your workload with this tag. Let me start by discovering the resources in this workload and assess them, and now you can see tool number 1, use AWS. Tool number 2, you, so it is actually going out to my account. And retrieving results based on that tag, and it can, it's smart enough to realize that, hey, I, I used a tag that's used in cloud formation, so it's intelligent enough to realize it needs to go to cloud formation to figure out that workload and inspect the stack. And then once it's retrieved those results, then it's gonna start reasoning about that workload and. You know, calculating the letter grade, and you can see through the, the output here that it has called the calculate letter grade tool a couple of times, and again, it's continuing to reason about that and we're gonna, should hopefully get results here in just a second. And so Mike, as we're going through this and we're looking at um that use of use AWS like you were saying because it's built into the strands agent you don't actually need to say use the cloud control API it knows if I'm looking at a cloud formation stack I need to use the cloud control API to be able to list resources. Understand the stack itself, um, and in your prompt you'd mentioned that you put in, uh, only use non-mutative, uh, calls to the cloud control API so as we said earlier, the API itself is full crud operations so you could create a resource you could update a resource, but you're specifying, hey, only do read only. Um, so that we're just reviewing our application and we're not actually making changes in this, in this exact agent, exactly, exactly now, and one of the reasons I, and I think that's a great point that you bring that up, one of the, the things that. You know, one of the reasons that I said use non-mutative API actions is, well, I'm running this locally and I'm using a profile that has administrative access, right? I'm, I'm sure many of us are a little bit sloppy in our engineering and we're full admins when we're running locally, right? But when I actually run this, if I were to actually deploy this in my account and have it be an agent that I'd want to interact with over time, I definitely don't wanna give it admin access, right? So I could make sure that over time when it's running in my account, I assign a role that only has read only permission. So I, I think that's a great point that you bring up there. Alright, so you can see that we've got some output here, let's look at the output. Let me scroll to the top here. And you can see it's, it's done some analysis here. It redundancy, it gave me a D. Sufficient capacity, gave me a D. Timely output, it gave me a D. Correct output, it gave me a C. Fault isolation, it gave me a D. And, and it gave me some, some problems that I might need to look at here, but you know what? This workload has a recovery time objective of 24 hours. Now if you don't remember what RTO or recovery time objective means, that really means how much downtime is acceptable for this workload. And when I talk about my recovery point objective or RPO, that's how much data loss is acceptable to me. So you know, I'm saying that my. I, that I'm essentially willing to have up to 24 hours of downtime and up to 12 hours of data loss. So this feels like a pretty uh. Pretty low risk application, right? These grades don't feel quite aligned with what I've got here, so let me just ask it, you know. Do these grades really reflect the RTO and RPO? that I provided. OK, and this is my result here. I don't have any context about previous grades or RTO RPO values that you provided. What? Oh, what's going on here? Anyone, anyone have any ideas? Is there a session state in our agent? Oh, we don't have any session state, so that's probably the issue, right? OK, so what that essentially means is that every request I'm making to this agent endpoint is independent of any other request. So I asked it to look at my workload, it gave me a lot of results, but if I want to ask any follow-up questions, I can't do that. Because it doesn't remember the previous interaction. OK, so we obviously need to add some session state here. So let's add that as the next step. All right. OK, so let's, let's look at this and I'll show you what we ended up changing. So we have the same system prompt as before. We didn't make any changes to the system prompt. We've got the same LLM, the same bedrock model thing we've done before. We are calculate letter grade is the same. Let's see. But there is, there is something different here, and I'm, I'm trying to remember, I'm trying to find it in the code here. OK, there is one thing that we added here. I added a bucket name. OK. The bucket is where I wanna store my session data, OK, so in this case, I have a bucket in my account called reinvent session data. That's where I'm gonna be storing my session state. Now, if I go into the actual agent invocation here, you're gonna see now. I'm gonna try and. So I'm, I'm getting the prompt from the user. And then I'm going to try and get the session ID from the user. I'm expecting that that client to pass in a session ID. I'm gonna get their session ID. If they don't give me a session ID, I'm gonna raise an exception and return a 400 HTTP code. I want to write out the session ID and then I'm creating an instance of an S3 session manager. So this is a built-in object to Strand's agent. And you can see here that I'm passing in the session ID, that ID that's coming through from the, the client. I'm telling it the bucket name, and there's a prefix in that bucket where I'm gonna be writing my session data. I've created a, I've got a prefix there that I'm just calling prod. That's where all my session data is gonna be going. And if you look at my agent, all I'm doing is I'm adding a session manager. And that's it, and I should hopefully have session data by doing that. Now the, the last thing I really need to do. Is I need to go to my interactive file here, and I need to update this to provide a session ID. So all I've done here is I've created a session ID which in this case, I'm just using a UU ID. And then I'm just adding that to the payload that gets passed in. To my agent. All right, so let's try this again and see how this works. All right, so let's clear this out. All right, so let's try this again. I have a workload with the tag of AWS. Cloud formation, stack name and value of food agent. The RTO is 24 hours and the RPO is 12 hours. Help me understand. The resilience of this workload. OK, so we're gonna see the same thing kind of happening, but one thing you will see here though, is now I've got a session ID that's being admitted to. You know, emitted from my, my agent, so I can see that I'm getting in a session ID. We're gonna see the same thing where it's gonna it's gonna pull resources from my account. It's gonna reason about my workload, yeah. We're gonna get the, hopefully the same result as before, and then now I should be able to ask it follow-up questions to validate what's what it's actually producing. OK, so you can see here now it's, uh, you can hopefully see some calls to, it's moving too fast. Calculate letter grade. You can see now it's reasoning about that workload. It's already pulled everything back about it from my account. It's reasoning about it. It is providing some, well, yeah, it's reasoning about it, so it's providing some recommendations. It's now creating the output and I should see that output in just a second here. And do you all remember what my grades were before? Seems like it was mostly like C's and D's, right? They weren't great. Yeah, it was not, not great. And uh, you know, if I look through this. OK, looks like C's and D's, you know, redundancy, D. OK, well, let, let's ask it a follow-up question. You know, Given my RTO and RPO, are these grades accurate? You can see that it is printing out my session ID so, you know, it, it, it's using the same session ID so I've got that context now. And I don't know if you, you know, this, the. Agents moving pretty quickly, but. If you're able to see that, you know, it made a comment, hey, you're absolutely right, those grades don't make sense. Let me recalculate for you. And something we don't do here, Mike, um, but you can totally do depending on how you want your client, how your user to interact with the agent would be to, uh, change how you stream back responses to your client, right? I think that's a great point. You know, I am waiting for the, the complete result to be done before it comes back to the client. Yeah, but you're suggesting probably streaming would provide a much better user experience. Exactly. If, if we wanted to have that, you know, more interactive feel, uh, like in a more chat box. Way being able to stream tokens back as soon as they're being generated would help to kind of improve that user experience over time, but for our purposes here, not super necessary. Yeah, no, but I think that's an excellent point. Definitely streaming that would provide a much better experience here. All right, so let's look at what this, what it returned to me. Gave me a lot of results. Let me just go to the top here. OK, you're absolutely right to question these grades with a 24 hour RPO and 12 hour or 24 hour RTO and 12 hour RPO. Many of these issues I flagged as critical or actually nice to have or optimizations. All right, so then it said, now my redundancy grade is a B. My sufficient capacity grade is a B, my timely output is a C, my correct output is a B. OK, so it looks like most of my grades are now B's and C's, which I think seems more aligned with reality. OK, so now let's take this a step further. So you, you think about. There's a couple of things going on that we, we've talked about here. So first of all, in terms of tool use, I've, I've shown you how to use a tool that's built into strands agent. They use AWS tool. I showed you how to use a tool that I created myself, that calculate letter grade tool. The next thing I think is valuable to use is there are a lot of tools out there that are MCP servers. How do I actually integrate an MCP set of tools into my workload? So let's look at doing that. You know, we also showed you how you can have more interactive conversations through things like session ID. But you know, I, I feel like there's still some problems with my prompting here because I don't wanna have to ask it, hey, are you sure that these grades are right? Are you sure that the way you analyze this is correct? That's not a great experience, right? I really want it to give me the correct results the first time without me having to really quiz it and, and, uh, challenge it on every single thing it brings up. So I, I. In terms of next steps, I definitely want to add an MCP server. And I definitely want to improve my prompting so that I don't have to, you know, challenge it on everything. So let's go ahead and try to fix those kinds of things. All right, so let's stop these and clear these out. So let's go to my agent. And I made a couple of changes here. Let's look at the changes that I've made. The first change I made, remember before I had my prompt embedded in this file here. I've now taken it out and I put it in a separate file. I'll show you this prompt in just a moment. There, can you think of any reasons why I might have done that? It's easier to maintain. Maybe, I mean, there, there, there could be a lot of reasons, you know, for me it's easier to maintain. You'll also are gonna see that, you know, as your prompt gets bigger and bigger and bigger, the, the one thing that I, I see a lot of organizations deal with is they'll complain about how, you know, I got my agent and it does OK most of the time, but then it'll sometimes. Do things I don't really want it to do or you know I don't see maybe some of the consistency I want and oftentimes when I see that if if they're using a model that has good direction following ability, usually when I see that it means that maybe you've got things in your prompt that are vague enough that the the model needs to fill in the gaps. And so if it needs to fill in the gaps, then it's going to do things you may not expect or may not anticipate. And so that usually requires that you add more specificity to your prompt, which means that your prompt itself is going to get larger. As your prompt gets larger, if it's embedded in your code, then, you know, after a little while, most of your agent file here is going to be prompt. So, so that's why I broke that out. We'll see that more in just a moment. OK, so then the, the next thing I did is I wanted to, I want to use the AWS documentation MCP server. Any of you familiar with this MCP server? It is like my number one favorite MCP server. And the reason it's my favorite is because all it does is it returns documentation and blog posts on AWS content. OK, it's, it's curated, managed by AWS. This went generally available about a month or two ago. It's awesome to use. And if, if you want deep curated documentation, this is a great way to go. So. This is my code to use that MCP server. I create a variable called AWSc MCP client. I create an MCP, well, I use an MCP client object, and I give it the URL of that MCP server, which is knowledge-MCP whoops. So you can see it right there. And this MCP client object. Really defined right here, so it's built into the strands tools. All right. So that's How I get my, my MCP client for that documentation server. Then to actually use it. I've, I've made then a few other changes here, you know, the first time somebody runs this code, I really wanna be able to ask, OK, what's the, the tag of the workload that you're interested in? What's the value of that tag? What's your RTO and RPO because those seem like the, the natural first questions. This agent's gonna want to know. Once I understand that, then I can actually process and and do some other interesting things. So I've updated this code to retrieve a tag name and tag value if they're there, to retrieve an RTO an RPO if they're there, and then create a prompt, something like, hey, this user workload is running in AWS, it's defined by the tag, defined by tag name, the value is tag value, the RTO and RPO are this and that. The only other thing I've done here is, if you look at my agent, well, uh, before my agent, I have this AWS doc tools variable that I've created. And remember that variable before, the AWS doc MCP client, that's my MCP client. All I do on that is I call list tools sync. And what that command does is it goes out to the MCP server, it pulls back the tools that that server supports, and puts them in this AWS Doc tools variable. And then I just add that AWS Doc tools variable to my list of tools. And that's literally all I have to do. It's so stupid simple, OK? And I love stupid simple. All right. Now, the next thing I want to show you. Let's show you my system prompt. This prompt now is 145 lines of code. Or 145 lines of text. So let's read through not everything here, but just give you a sense for what I'm doing here. Uh, first thing I give it a purpose. Whoops. You are an AI powered resilience expert. OK, we've kind of seen that before. Uh, I give it a persona. You should embody a seasoned cloud architect with 10 years' experience designing and optimizing distributed systems, communicate with authority, precision, approachability, ask probing questions, offering tailored recommendations that inspire confidence. I'm giving it a grading philosophy. So you saw some of the problems I saw with my grades, right? Where it generated grades for me, they didn't seem consistent. I asked it to, well, I asked it, are you sure? And I said, you're right, I'm not sure, let me try that again. So I'm, I'm giving it some directions on how to actually grade because if you think about it, I, I say give it a letter grade, A, B, C, or D, but there's a lot of things that the model has to assume in all of that. So I'm, I'm providing it that specificity of what I want. So I'm giving it this grading philosophy. Grades reflect how well the current architecture meets the stated RTO RPO, not absolute architectural quality. Give it some core principles. The same architecture receives different grades based on RTO RPO, and I give it some examples. I give it a grading scale. What does an A mean versus a B versus a C, versus a D versus an F? Again you can see how, as a human, you know, when I, I wrote those, that first set of prompts, there's. A lot of things that I'm taking for granted where you know this model is having to make some decisions on its own about OK what does Mike actually mean when he says an A? Well, what I mean now what I'm saying is A means significantly exceeds requirements it may be overengineered. That's much more specific and actionable by an LLM versus give me a letter grade. I'm giving it some requirements for my RTO and RPO. You know, and high RTO RPO means I have hours to days of tolerance. I give it some examples. A medium means I've got minutes to hours of tolerance, a low means I've had seconds to minutes of tolerance. I give it a grading matrix. For high RTO RPO these are things I would expect to see. For a medium RTO RPO, these are things I would expect to see. And for a low RTO RPO, these are things I would expect to see. I give it some directions for, whoops, for tool usage. When calculating letter grade, you must always include the RTO RPO in the context of the assessment. Frame the evaluation as for X hours or minutes of the RTO RPO requirement. This setup meets, exceeds, falls short. And I'll give it an example here. Uh, and I say never call the tool without explicit RTO RPO context. Then I have a grade verification. After calculating each letter grade, internally verify is grade X appropriate for the current architecture for the given RTO RPO. And if it's inconsistent, recalculate. Now again, I'm I'm hoping you see all of the areas that the LLM was having to. Essentially guess what I meant. You know, resilience definitions. You know, I asked it before to, to categorize my resilience posture in those five areas. But you'll also remember that I never bothered to explain what those five areas actually meant. So here that's what I'm doing. Redundancy means I'm eliminating single points of failure through component or system redundancy. You can read through all of those. Now that I'm using that AWS documentation MCP server, I have specific directions around how do I want it to generate documentation, you know, for me. I want it to give me essentially the, the, the, what I'm trying to achieve here is I wanna have good recommendations and if, if it's telling me that I need to maybe set up lambda reserve concurrency. I don't want it to give me a link to the lambda landing page. That's not useful, right? But a link to the documentation on how to set up lambda reserve concurrency, that's actually useful. That's what I'm trying to get at here. So that's what I'm, I'm saying here. So provide specific AWS documentation links to support all recommendations. Links must provide deep insight into the topic, not generic product pages. I give it, uh, an analysis process. So do this, you know, accept the AWS workload tag value, classify the RTO RPO as medium high, low, execute only non-mutated API requests for each resilience area, state, you know, these things, calibrate all recommendations, format output according to the response format section. Now this response format section, this is probably what you remember, this is what we've been looking at. This is the same as before, except I've now added a section here around where I want my AWS documentation links to be part of this output here. And then I'm giving it some output rules. Be concise. Start immediately with formatted analysis. Don't include thinking tags. Present only the structured response without preamble or conclusion. Every grade and recommendation must align with the stated RTO and RPO. If the architecture exceeds the requirements, note potential cost optimization opportunities. And then I've got some final critical reminders here. Before finalizing any grade, ask yourself, did I classify the RTO RPO correctly? Does this grade make sense? Would the same architecture get a different grade with a different RTO RPO? Am I grading based on requirements fit, not architectural sophistication? All right, so the, the, really the thing I want you to take away from this is. Well, by raise of hands, are any of you surprised by the complexity of this prompt? Few, few people, yeah. And I hope you see that as you look through this, this is designed to take away the ambiguity that we originally had and, and that's one of the key takeaways I want you to think of when you see your agent not operating the way it should ask yourself how much is the agent having to come up with? all right. So let's actually run this. But before I do that, I need to update, excuse me. Uh, I, I've just updated my interactive page just so that it includes a few more things here. Let's actually just run this. And Mike, as you're on that, I wanna highlight as well that the complexity of the prompt, um, and talking about, you know, breaking out into its own file, all of that gives you the ability to really iterate on that prompt, you know, we're, we're showing you the finished product here, but that took a lot of time and, and execution and. Uh, work to get it to that point and, uh, as we're, you know, talking about the iterative process, it's not gonna be a one and done with your prompt, um, but also keeping it out there as you try different models, for example, you know, we're using Cloud So at 45, but if you use a different family of models, how you structure your prompt will be different, the kinds of tags that you give it. Will be different and um by treating it as just another asset within your code uh and within your development pipeline you're able to more easily test things out and iterate um as you move along to build out your agents. I think that's a great point and you know when you're building a prompt like this that can be a little more complex, a, a good approach is say you know you're using Cloud Sona 4.5. Say you know what you want it to look like, you can, you can go into an LLM like you, you can even go into the Bedrock console to the, the text playground we have there, and say, hey, I'm trying to write this prompt. I'm doing this in cloud sonnet 45. This is my prompt, this is my output, and this is the output I want to get. Help me get there. What I'd recommend if you do something like that, don't pick a clawed model. Pick a different model, a model from a different model family. You do that and it can give you really good direction on how to do this. You know, hey, maybe you need to change your prompt to look like this or that. OK, so now I've got my results here. These are now aligned with, you know, what I want it to look like. Hey, um, I've got my redundancy, my sufficient capacity, you know, my, my letter grades here, this looks just like I want. Now, let's see if we can do something a little more interesting. Let me ask you, how do these grades change if I change my RTO to 2 hours and my RPO. To 30 minutes. We're gonna, we're gonna see here is if you look at the agent processing, it already understands my workload, so it doesn't need to go back into my account. It's gonna be recalculating those letter grades based on how it's reasoned about my workload. It's going to now go to the documentation, it's gonna search and retrieve documentation, uh, and in just a moment we should see some results here. Now, one thing that's important to note is now you've all seen my prompt. I never gave it anything specifically to reevaluate grades based on a different RTO RPO, but it's able to do that because it can reason about what's already there. And you're gonna see in just a moment how I'm gonna ask it some things that truly aren't in my, my prompt. OK, and you can see, you know, through here that most of my grades now are C's and and D's for a lower RTO and RPO. Let's try something even We're crazy. So I know that I have observability problems with this workload. I would like to know what are maybe the top 3 things from an observability perspective that I should work on fixing to improve this workload. So let me ask you, what are the top 3 things. I should focus on to improve the Ability of my workload. Yeah, now You, you've seen my prompt. I'd, I've never mentioned anything about observability in the prompt, but it's able to again reason about my workload. You can see that it's searching the AWS documentation. It's going to reason about what it's searched in that documentation. It's gonna pull back results, and hopefully soon I should have something that I can actually action on. OK. So give me some, some good information here. So if I scroll to the top. Tell me 1, I should implement cloud watch alarms with automated alerting. Great tip, I don't have any alarms here. It tells me some ways I can do that. Enable X-ray with distributed tracing. Great idea, I don't have any tracing. Even gives me some code I could add. Hey, that's pretty cool. Uh, 3, implement structure JSON logging with cloudwatch logs inside queries. Cool. I do have a log file, but I'm not really doing much with it. These are all great tips, and it gives me some, you know, quick wins, um, some, some quick implementation steps, a lot of really interesting things. Let me just do one last thing here. I'm gonna ask it to build me a runbook that I can use to recover from an operational event. Now, again, remember, there's nothing in my prompt about building runbooks, but because it has access to these tools, it already understands my workload, it's able to search the AWS documentation, it's able to reason about that, it's able to reason about my workload and generate something that I can use it, that's actually pretty useful. And as it goes through here, you know, very much like we're talking about getting more specific with the, the core part of our agent looking at doing that evaluation and, uh, and recommendations, you know, this run book if we're not happy with how it's displaying or the recommendations it gives like Mike was saying we can be more specific within our prompt, give it that option. Of hey if someone asks for a run book, you know, format it in this form in this way or provide these specific, uh, components of that run book, um, but we'll see here like it does a really good job without that but if we wanna get better, more specific, um, we're able to provide that within that prompt to, to help refine those outputs. And you know if, if you're following along with what the, you know, the, the agent is processing through pretty quickly, but if, if you're able to quickly read what it's doing, I mean it's looking for, OK, maybe there is downtime, maybe somebody accidentally deleted something. I mean there's a lot of different scenarios that it's thinking through how to recover, you know, uh, you can see like what if somebody accidentally deleted my stack? How do you recover from that, you know, so. It's pretty interesting. And so I could take this, this run book that's generated, I could then add that as, you know, add that to my recovery actions, my, you know, add that to my documentation. And, and that gives me a good first step on, you know, how to have better resilience here. OK, so let me, let me just scroll to the top here and we'll just scan through this quickly so we can wrap up. You know, Gives me a lot of scenarios. OK, what if you have S3 data loss or corruption? Here's some things to do. Let's scan through. What if you have um. With your lambda function, how do you validate that your lambda functions running and, you know, gives me what, when should I escalate to AWS support again, really, really good information here, uh, and we've got just a, a couple of seconds left. So Matt, let's, let's wrap things up. Yeah. So, uh, you know, we talked a lot about all those different steps in building the agent itself and we also alluded to we're running all of this locally on Mike's machine, great for him, not great for everyone else in our organization, and so how do we make that more usable by everybody, uh, and so one really good way to do that would be to deploy this agent to Bedrock Agent core to use that runtime as a hosting platform for having this agent be applicable or be usable by everyone in our organization. Um, and so we can just take the, the strands agent that we've built, uh, bundle it up, and then send it up to Bedrock Agent Corp, uh, to be, to host the agent itself. Uh, everything that we've written can still reach out to those tools that, uh, he defined, but also his MCP. Servers and then things like session can be held handled by uh Bedrock Agent Corp things like observability and tracing of the agent itself handled by agent, uh, Bedrock Agent Corp. So it really helps us to refine or, uh, host this and make it more applicable for everyone else. Uh, and so from here, uh, if you want to learn more about Strand's agents, you can, uh, go up to the documentation using the QR code on the left. Uh, and if you want to get your hands on this code in particular, uh, the QR code on the right will take you to our nonprofits sample page on GitHub. You can see all the code, uh, get your hands dirty with it, and, um, extend it for whatever you want to do. Maybe you wanna include, uh, cost, uh, or performance optimization as a separate agent or separate tool, uh, for helping to improve your workload over time. Um, and so I know this is the last session of the week as far as our nonprofit, uh, track goes, but if you do not know who your AWS account team is for your organization, uh, use the QR code on the right here to get in contact with your account team, uh, and have access to, um, essays like myself and Mike if you need any help building this out or just wanna talk about, uh, what you guys do and how we can help. Uh, one last resource is the AWS Skill Builder page. If you wanna get your hands dirty with any other kind of resources that we may have talked about here today, um, go to SkillBuilder, and, uh, there are workshops and other learning resources that that are available to you for learning all about AWS. And with that, thank you so very much for your time, um.