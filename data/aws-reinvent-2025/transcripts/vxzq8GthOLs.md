---
video_id: vxzq8GthOLs
video_url: https://www.youtube.com/watch?v=vxzq8GthOLs
is_generated: False
is_translatable: True
---

COP 326 elevating application and genitive eye observability. My name is Matilda Hayes. I'm a worldwide tech lead for Cloud Ops based out of Dallas, Texas. This is my 6th reinvent. I'm very excited to be here. I have Peter with me. Hi, I'm Peter Gang. I'm a product manager in CloudWatch, my first reinvent, but very excited to meet you as well. Awesome. So let's start with this picture. So imagine if you were in a store, like if you're driving down an unfamiliar highway and you have very limited visibility. So it's quite nervous wreckons and dangerous as well. You don't know what exactly what's the, the road's gonna do or what's gonna appear in front of you, and you're just starting to grip the steering wheel even harder. Your palms are sweating, you just wanna make sure they drive safe home. We'll all be in this, you know, this familiar situation, driving with limited visibility, and we don't wish it to repeat it again. So this is like operating in today's complex applications powered by AI. We often lack of visibility and observability. And they just wanted to understand how the systems are driving forward, and without it, it's quite dangerous. Even our uh CTO Verno Vogas, he once said. That you needed to, you needed to uh uh have a visibility, because without it, you're just guessing, you're guessing what the system's gonna do next, how the systems will react to the dynamics of, of your end users. So in this, in this presentation we're gonna help with that. First of all, we're gonna understand how observe uh applicational observability monitoring is today. What are the challenge that you have now that we have generated AI in place and how that comes and play along with that, how to gain visibility, uh, and observability to AI power applications. So our commitment, myself and Peter, that you're gonna walk for this session. Um, going back to your, your, your places, your homes with actionable insights for how you elevate application but also generative observability in a single pane of glass. Let's start with John. So John here is our persona. So he's a SRE, he's a developer, he's someone that is using call watch in a daily basis, and as you can see, his application right there is quite complex. That is many layers. That's backend, database, infrastructure layer. So, John's using just the simple monitoring, like CPU metric, stuff like that. That is many layers on that application that John can leverage, as you can see on this diagram. So each layer that you see on the screen is a possible layer that it can be observed. So networking, infrastructure, application, database, users, and so on. From the, from the bottle up, AWS offers many choice for you, for instance, on the infrastructure. So you can observe on the ECU perspective, containers perspective, server on-prems, and so on. Application layer, it is one of the crucial ones because you can use application logs to understand how the applications traces the interconnection between your applications moving from the CPU metric perspective to a service level objective perspective time back to your business outcome. Last but not least, you have the user perspective where you can create. Uh, understanding of your user behavior and using car watch for that. So what John had asked for us, it is, I wanted a native integration. Because he's using either of us today, he wanted faster time to attack problems and also to resolve them. He wanted to cost optimize, uh, doesn't want to move data from, from here to there, you know, doing due duplication is, is quite complex, um. And also it should be a single pane of glass, it should be a simple user, simple service to use that has actionable insight that is smart, is using, for instance, uh, machine learning to understand anomaly. And for each one of those layers, uh, Amazon call Wach offers John a perspective and an option to monitor his uh application cover to cover. So from the bottom up, uh, in the internet, in in in network perspective, we have, for instance, internet monitoring. Can analyze uh internet performance, understanding if it is uh also your provider, internet provider has problems, some outages, uh, you have the infrastructure level, the blue one, where you can have uh CPU container services, some pres, and you can leverage containers sites, lambdas sites to understand. The system level um of, of that particular infrastructure choice. Uh, we have database inside too to collect monitor database performance in the real time so you can leverage, understand which SQL queries is taking longer than others. Um, the green one, which is the application leverage is, is the most crucial one, like I said before, because You can automatically detect your KPIs, your application KPIs, and use SLOs to maintain server quality commitments that you have with your customers. Last but not least, uh, the top level, which the user's perspective, uh, you can use run real user monitoring and synthetics to produce canneries to simulate what the user's behavior in an alert, if you have any availability issues or performance degradation, all of that in a single tool, which is Amazon call Watch. So, going to the three pillars of of of suburability, you got everyone might be familiar with, with this, but That's the journey that we wanted to to discuss here. Every monitor mon observability, uh, strategy will come along with metrics, logs, and traces. This is a call as a 33 pillars of observability. Match matrics, it is, I wanted to understand, uh, I have the information that my application response rates increased 35%. logs, I have the information in a, in a, in a the data structure or not a structure that you know my authentication, uh user just authentication in my application and trace. I can ask questions or answer questions uh about distribution processing time of my microservice across the board. So, as you saw it in the beginning, John has a quiet. Uh, complex, uh, architecture, and then of course some challenge also arise with that. Uh, microservice like a complex, complex, uh, architecture needs constantly performance over time to satisfy the end user. You have, uh, also John needs to manually stitched together telemetry data with the service to understand the degradation of availability and high latency. Also it's very hard to uh understand what is the priority for the business, right? Understand those anomaly which ones that matter most to the business and how he he prioritize with that. Last but not least, this joint uh experience because, uh, correlating that telemetry data from, from the perspective of metrics, logs, and traces, you also needed to put in place real user monitoring, synthetics monitoring. If you put it all together, that can extend it in a very long time to attack the real issue. Now, In order to offer a technical perspective how John should use and what he should use and look specifically for uh uh elevate his monitoring, it is the golden signals. So the golden signals are volume requests, which is directly impacted the demanding place of your application directly impact also. In latency, latency is speed, right? How, how much time a specific request will uh expanded to be resolved, and for that, if you have higher latency, or you also have the uh directly impacting the user's experience. The two in the bottles, you have fault and errors. This is a tie back to requests. There are mal form or application issues, so you needed to understand those in order to understand your application monitor as another level. Now this is a technical perspective. Uh, what John needed to, to do here, it is also to connect that from the business perspective, right? So, for instance, business impacts directly to revenue per minute or car if you have a car abdomen, what, what is the level of that? What is the user experience signal, for instance, page load time, uh, what is the API errors code and session durations, so on and so forth? You have service health indicators like latency. Uh, P90 P95, you wanted to understand that that particular piece to understand the overall health. And last but not least, connecting with reliability, um, um, objectives, tying back each signal with an SLO, a service level object to ensure that you're maintaining the optimal level of the application. So in order to answer those questions, you know, John should focus on that particular layer, because on that layer, it's where John can tie back all those knots, all those dots, the application level. And, and Amazon offer an APM solution uh called Amazon called Watch Application Signals. Uh, from usingation Signals, John can elevate his application monitoring. Because first of all, that is uh application seekers automatically discover which applications you have in your account using open telemetry SDK. You don't need it to do anything, right? It's already automatically discovered for you by call by call watch. Second of all, there is previewed dashboards that will standard metrics including those golden signals, uh, will have that so we can understand. Uh, that technical, uh, the technical metrics that I just mentioned a few seconds ago. Third, you have easy understanding with a few clicks the root cause if you have a HTTP, uh, malform, uh, if you have an exception, you can understand where this, uh, was the deception, which line of code, and so on. Last but not least, SLO service link object. So this is uh related to goals on your reliability and tying back to the business objective. This is crucial for any application monitoring strategy. And that's why when we talk about application monitoring monitoring, we talk about SLOs because it is a comprehensive understanding of application health from the user perspective and this is how it works. You have an application like or or an API called Gap Research API like, and I, I wanna, I have a particular internal goal of 99.9% of uptime of this particular API. Well, In order to to have that, I already select a particular SLI which is service link indicator to measure that goal. In this case it is up time. But as you see, there is a 0.1% that can create something called error budget, which means if I'm evaluating this particular SLO in a 30 day period, I still have 4 43 minutes of downtime and still meet my SLO. Why is this important? Because you can tie all this back with your SLA. And SLA, everyone know it is an agreement that you do with your customers. So when you put it in perspective, what are you gonna have? You're gonna have happy customers because if you are looking for all those APIs, establishing all those SLOs tying back to the SLIs, customers are able to understand that you take care of the application. In this particular case, John will, uh, achieve happy and happy and, uh, users too. OK, so let's go for the demo. Let's go to the fun part. Um, in this demo, I'm gonna show, um, specifically two scenarios, uh, of fault and also latency, uh, problems that I have in my application and how we can use application signals to monitor my entire application observability, uh, in a single, um, console. So first of all, what I'm gonna do it is to show uh SLOs. So I have multiple SLOs as you can see here, and some of them of course are unhealthy. I wanted to show how to create an SLO. Um, SLO can be, uh, I need you to choose first the SLI. SLI can be a service operation and called watch metric or a service dependency. And again that service means that you know is automatically discovered by open telemetry SDK. In this case, I'm, I'm gonna use my front end, uh, pet clinic front end and a specific operation of posts and the interesting part right now I can select the calculation metric method for this particular SLO which can be by request or by period, uh, and I can select also the condition of the particular calculation method that can. Either by latency, I'm choosing 100 milliseconds right now and or also availability. One of the cool things, it is called watch put that little phrase over there which is exactly determines what I'm doing, you know, in in that configuration. From that, I select what is my SLO. The I define my inter interval of period that will be assessed by, by this SLO, uh, the attainment goal, and also I have the little phrase over there. Of course, none of this makes sense if I don't tie back alarms. Alarms can be directly related to the SLI or the SLO attainment goal. And and the overall health of this SLO everything there, it makes sense for that particular API. Now I wanted to show also something beyond it which is application Mac. We launched this like a couple of weeks ago. This is the, uh, what I mentioned related to auto discovery. From the open telemetry perspective, not just instrumented but also uninstrumented applications are capable to be discovered by application map. I can filter by groups. There's multiple filters that I can select over here and then all the applications that I. Have in this account we will automatically be select on those little boxes on the right hand side even if I wanted to create my own custom attribute, uh, I can use configurations directly on open telemetry uh config file and will show up directly here on the console too. Now going a little bit deeper on the path clinic that I mentioned. When I click, double click, uh, I see the polish. So this is my polish for my application. All the connections that I have with the front end, with the back end, all the, the micro service that I have connections I can see. And if I click on the thin line, I'm able to see the interconnection between one server to the other and understand the, the top path with fall rate with high latency, so on and so forth. If I click in, in that particular front and back again, what I have on the right hand signs, it is the golden signals that I mentioned before. So requests, volume requests, latency, errors, and faults are out of the gate for me over there. I can enlarge, uh, to see this over the time and select P90, P99, or P50. And one of the cool things that I, that I, that I believe here that call watch give it, it is, is operational audits, auditing. So as you can see I have some drops, some spikes. So call what's already given to me over there, what are the indication that the problem should be? So what exactly happened in that period of time and what exactly what's gonna happen, you know, you needed to assess even further. So to assess even further, when I click in the dashboard, I, I can see the overall. Uh, metrics for this particular application, and again, uh, uh matrix, uh all the golden signals are here. As you can see, but one of the things that I mentioned in the SLO creation, it is the service operation. So, uh, using open telemetry, I'm, uh, I'm able to understand all the posts, the gaps, the puts that these applications using behind the scenes and understand, as you can see, I have SLIs that are unhealthy. Uh, if I double click in the dashboard, uh, because I have a spike of a fault, it will automatically show the correlation of spins that is necessary for that request to be served. And if I click in one of those spans, it will show me exactly what is the tracing map from, from the front end to the back end, what exactly the whole trace and and the spans for this uh request to be uh resolved, and I can even see the same visualization, but now in a timeline perspective, in a span timeline perspective, where is this host. And as you can see, there is a fault in the visit service Java is throwing an exception, and the exception, it is a DynamoDB throughput problem. Uh, I can see the message and also the line, specifically the line that in my code I needed to go back and fix the problem, and even in the Dynamo DB's perspective, I can see the message on the right hand side too. One of the, one of the interesting part, if I go back to the, the previous dashboard, I can also see because this is the EKS cluster, where is the node that is having most problems with this particular fault. Um, and what exactly are the pods that are having this, this pod, and then I can prioritize by looking to these notes and pods. If I, even if I click to container insights on the, on this little button, it's, it's gonna guide me to a console where I can see my container insights performance across the board. This is uh in the single pane of glass. So dependencies is another tab on, on again, same application that I can see all the dependence related to this application, and, and also, of course, the dependence related to. To the service, so I'm, this is the 2nd use case that I talked about latency. So as you can see there is a spike in this specific time, and I'm just gonna click on one of the tracings using the same methodology on the previous, uh, use case. We're now looking for, uh, latency, and you're gonna see that now because it's a different API that I have high latency, it's being served for different service including for instance Bad Rock. Uh, I have the same visualization trace map, but, uh, and also the timeline, uh, with all this pans, and as you can see, the Badrock runtime has an error specifically when I click into view, uh, it will show me exactly what the section is being thrown in my code. In this case, the problem is the Badrock foundation model is being deprecated, so that's why I have high latency because it's been deprecated, so it's taking too much time. To be resolved back to to my user and even if I click in the event, I'll be able to see that more properly. Now this is the application level. I wanted to also show the user that that top level uh layer. You can use uh synthetic canneries to create um and simulate users' behavior. In this case, uh, of our persona John, you can create to simulate what exactly uh steps that needs to be uh used and simulate what is, what a user we would use, uh, his applications for, and we'll take a little screenshots as well if you're determined to do so. Uh, you can also use the user experience, the run that I mentioned before, uh, to understand page load. If there are any errors, uh, you can see, uh, when I increase that, that, the time on the, on the right hand side, I can see the errors over time, uh, understand what exactly the page that I have from the user perspective that are having the problems with load specifically. And when I click one of this, uh, that's giving me the entire information regarding that particular error, and this is a user perspective, so I need, I, I, and I can see uh when the session happened, when this happened, which type of browser call watch used to simulate uh the user behavior, and I can, you know, understand my uh user uh experience across the board. Now, this is the application part and now I wanted to move over to the G AI because now John, Peter needs to include G AI in his application. Thank you so much, Maths. I'm so excited here to talk about to you about how observability has changed to monitor AI workloads. So raise your hand if you are currently building an AI application or workload in your company. OK, wow, most of the room, so you're in the right place. So looking back this crazy year of 2025, um, we all know that AI power applications are going to transform the way that your business and your users interact with your business. Every tech leader here is seeing in real time how easy and powerful it is to build these AI workloads on AWS. So I'm here to help you navigate through this time in a lens from observability and showing you three things. Number one, how observability has changed. And the new tools that you can use, and second, how observably is still some of the same things that you, you're already familiar with and show you the existing tools that you can use and lastly, showing you all that together with a demo of that workload. So, walking back in memory lane, so we started this AI talk back when we had these um question and answer chatbots, uh, but that is still 2023. What we then had these are AI assistants or what kind of the AI entity that walks the user step by step through a particular um business process. But now today, what we have is AI agents. These entities are more autonomous, that can do certain tasks by themselves and make decisions independently to achieve specific goals. What we think in the future will be is fully agentic systems, where the entire systems behave independently, achieve open-ended um requests and goals, and help, help the entire business make its own decisions and drive those outcomes. And How do you even build these AI applications? Um, in AWS we offer a full stack of tools for you to build those workloads, and monitoring is across the entire stack. Starting from the bottom, can help you build, uh, run, train and deploy those AI models on StageMaker and also our on our uh infrastructure. And the middle layer I think is gonna be the meat of it. It's gonna be first is the Amazon Bedrock, which is a er fully managed service that offers you um direct access to a slew of foundation models through a single API so you can scale up your application really easily. And then the more recently launched Agent Core was the place for you to build and deploy highly scalable and capable AI agents. It comes with a suite of tools to augment those agents with memory, so the agent knows the context with gateway, so the agent has control over what kind of third party APIs it calls and identity for authentication and control. And going higher, even in AWS we've used these things to build up those fully agentic systems with Qiro, the agentic IDE, Q Agentic business intelligence, and also Amazon Connected with Agentic contact center. We've been loved with millions of customers. So wherever, however you're building AI, we've got you covered, and again, in every layer observability is there with you. Now, here comes the question is, how do you have the right observability on these AI applications? How do you see, how do you track what the AI is doing? And then what's new, what's different? So let me help you navigate through this rapidly changing time. Um, with these, uh, AI applications, what we've talked to customers and they've unanimously come back to us with some common challenges. Number one, these agents can be indeterministic. Their actions, uh, can differ from time to time, even from similar past scenarios is very unexpected. They're like teenagers, sometimes they say brilliant things, sometimes they wish they didn't say anything at all. And second, root cause analysis is focused on tracking the sequence of calls that the agents made, but it's very difficult to trace. Analyzing those invocations uh at scales very difficult because every model provider has a a slightly different formats and tracking that that volume across different regions and accounts is harder than it's supposed to be. Now, and also lastly is assessing system health. It's still the same goal that Matteos talked about, but what's new is this word quality. Right now you need to answer why did the agent do the thing it did? Why did it route itself in that way? What context did it have? Well, sort of the traditional monitoring tools can tell you the performance, uh, surfacely performance parameters, uh, like. Latency errors, uh, but it doesn't tell you that that the reasoning doesn't explain to you the AI decision making process. So you are no longer just observing whether your AI is working or not, going up or down, but you're trying to observe reasoning and intent, and we believe this will be your new operational reality. Observaly it is going to be the control point for trust, safety and quality. Now, going back to the same stack layer that Matteus showed you, we believe that AI workload is gonna ride on top of all of this, and observability is there to, is going to be operate at the highest level with you as well, connecting from all the way from the infrastructure signals, stitching the telemetry throughout throughout the stack, and coalescing from those different models, agent actions to observe uh the entire end to end interaction for you. That's why from those challenges, we worked that was introduced to you um Amazon G AI observability already in GA has a slew of capabilities. Number one, it's a 360 degree view of your agent, no matter what model they're using, uh, on frameworks such as strands, crew AI, lane chain, and with out of the box, uh, dashboards on those performance parameters. Second, it's very simple to instrument your, your AI workload, um, as long as using Otel, which is open telemetry format, we'll touch on that a little bit later. And very importantly is the end to end prompt tracing, which traces across the LM calls, the agent actions, the tools, the memory calls it makes for you to quickly identify issues. And in the data protection world, now in these AI interactions, it's very likely that these, what the AI receives or the AI outputs can contain PII content, and those are stored in the logs, and we have data protection features to mask that content and protect it. And lastly is the evaluations feature for you to monitor the quality, and these are using LLM as a judge to constantly assess how your AI is responding, whether it's saying the right things, whether it's routing itself correctly and solving your customer uh problems. And to build these agents is fairly easy on AWS Agent core. Agent Core allows you to deploy, operate highly capable AI agents scale securely at scale. It offers infrastructure purpose-built for these dynamic agent workloads. As you can tell, it coalesced from different aspects of what the agent would use. For example, any model. Uh, LM model offered on Bedrock, and they will use agent core memory, which is the context of polls and uses, uh, to operate identity. Looks at the controls of security, uh, and also gateway, which is a central place for it to make third party API tool calls and all of that. And as you can see all of the tools and the primitives around the agent send telemetry to the observable platform which is Cloudwaatch genetic observability. So you can have that single pane of glass to monitor all of this. One thing I do want to highlight that has been resonating with a lot of our customers is that. You can host your agents on uh Agent Core runtime service, and that telemetry comes through Cloud Watch, you can get all the features I talked about out of the box. Also, if you're hosting your agents elsewhere, ECU, AKS, on-prem, or other clouds, as long as your data is in the Otel format, we can also accept it and you'll get uh very similar capabilities in cloud watch as well. So giving you that flexibility wherever fits your needs. Going into a little bit deeper on the agents running an agent core runtime, when creating and running these agents, uh, we provide even more flexibility on how you create them. We support agentic frameworks like very popular ones, uh, strands strands agents, crew AI lane chain, and the instrumentation. Supported instrumentation libraries are very broad, are all open source, open inference trace loop, and when you, once you've been instrumented, connected with the Amazon Distro for Open telemetry ADT, and we collect that instrumentation, send it to the Cloud Wash Otel endpoint, and we'll be powering those screens I've talked about before. More specifically, agents hosted on runtime. Remember that A dot I talked about? Plus, once that all that telemetry is coming in, in the cloud watch telemetry config with a single click, you can turn on all of the telemetry from those individual primitives like agent core memory, agent core gateway. A single click from entire account, all that comes to Cloudwatch and power those views, and 4 agent agents have posted elsewhere, still the A dot, but I need to emphasize that as long as in in the hotel um formats, we also accept it and turn on uh Cloudwatch transaction search to stitch stitch together uh those actions and and to calls I talked about. Uh, in there, and that's, here's a very, very easy quick start guide you can even start it working on it right now on your, uh, laptop. So with all that groundwork we laid, let's focus on what John is doing with his genetic workloads in more prac practical terms. Here I think this slide looks very similar to you of what what what you saw before, and you already know it, it's the metrics, the locks, and the traces, and that's still powering the genetic observability. That's the same old tools you're using today. So with metrics, uh, these things are like the token consumption, and you're tracking how those have changed, the volume of work the AI has done. Well, the logs is where you keep. Those verbose inputs and outputs from the user and from the outputs of the uh LM models and the agents. And lastly, tracing, very important. The tracers now can understand how the response propagates to the entire through the entire system, and you can analyze it at the aggregate level, and you have the capability to drill down on every single interaction. OK, a little bit more detail on how it really works, uh, metrics, these are some common metrics you'll be, you'll be able to monitor on is number of invocations, how fast these implications are completing itself, any throttles, and any, and the volume work like inputs, token counts, output token counts. And the logs coming from the model invocation logging and also the sam logs which contain each individual step and the contents of any tool calls and agent's actions. And lastly, the trace, as long as it contains the same session ID or trace ID will be and stitched together, we'll be able to surface those. And an interaction that I talked to you about before. And with those on the metrics we've put it into the same set of golden metrics that you we believe you should be monitoring. In the first bucket is the token usage, the volume of work that the AI is doing. And help you to not only forecast the demand that you will have and also cost. And then latency, how fast is your AI AI responding to your needs, throttles to keep a track about how close you're you're going against your limits and quotas and errors, seeing anything that's going wrong in your request. And we'd be able to surface all of these things in a pre-built automatic dashboards without any configuration, so you can diagnose troubleshoot from here. Additional filters include um uh filtering by the model itself so you can deep down into any specific model you're using, anything going wrong there, and also it's definitely fully integrated with existing clones capabilities like alerting if you wanted to keep track of your total consumption patterns. Now, here's something that's going to be a very different, relatively new concept to the world of observability that's relevant in AI now. Like I said before, the new operational reality for you is going to be looking at the quality of those AI responses and agent actions. So often this LMs hallucinates, or the agents take on a path all of its own that's not desired. And in the past, how teams have kept an eye on this, on this qualitative issues is the science teams take a very small sample of the AI workloads, manually look at it and assess it, and only when that's good, that's good. They are, uh, they're just OK with deploying the the AI out in the real world, um, almost hesitating and timid about what it's really going to do. So very. Uh, there's lacks of lack of trust and also very burdensome in the entire process. So that's why with evaluations we can you can leverage LLM as a judge to assess how faithful an agent, um, it's saying adhering to its context, um, how well it follows the instructions you've given it and how helpful it it was in helping your customers achieve their tasks and questions, and this, this is done continuously. Automatically on the entire traffic of your AI workload, and you have the ability to sample on full sampling or only a proportion of the samples, so it really removes the labor intensive and manual assessment process that our customer teams used to do. So evaluations metrics was just launched yesterday and it's available now in CloudWatch, powered by Agent Core evaluations. We talked about the metric side of things, coming back into the logs, some of my favorite because this is the most densely packed information they'll be using. I'm gonna be talking about the invocation logs, how to query on them, and protecting that data. So the invocation logs can come from both the LM model itself, uh, and also the agents. They all can, you can choose to come to a cloudWatch and or S3, and once it's in CloudWatch, it's stored in the familiar cloudWatch log groups concept that you can use with your integrate with your existing workflows. And once those logs are in in Cloud Watch, you can use the Cloudwatch log insights to drill down deeper, which is a powerful coordinating tool. We we support powerful languages like SQL, OpenSearch, PPL, um, and launching powerful create commands, uh, regularly. And we can, you can use power, uh, sorry, you can use pattern analysis to detect common text structures within the log events for faster insights and also automated real-time anomaly detection. To identify changes in these agent behaviors and performance changes. And then I said before, these, these, these customer interaction can contain PII information and we've got you covered when those logs are in CloudWatch. Uh, the CloudWatch data protection feature can identify and mask sensitive cus customer information like credit cards, names, addresses, uh, and. Put basically redacted for you. Also we offer um granular IAM ro based controls so that if a super user needs to see the content they can, but most users won't be able to, and we generate automatic audit reports uh for compliance um and. Uh, and reporting. Now, coming back to the last part, we talked about metrics, we've talked about loss, and we're talking about traces now. This is probably the most important part in in the GI observability. With tracing, the first thing we'll be able to do for you is show you the detailed information of the every single individual API call and the function calls that agents made in a very neat list view. You no longer need to query on them on multiple databases to stitch them together. It's done for you. And second, we'll show you those actions in a timeline, you, so you can know which one's taking the longest, which one's taking longer than I expect for you to troubleshoot faster. And a trajectory map showing you all those different interactions, API calls, tool calls for this particular trace, and each step here is a span and aggregated to this trace. This map is automatically visualized for you, so you can easily understand the hierarchy and the sequence of those two calls in a very easy. To understand fashion. Now I, I talked about what's different, what's the same, the logs, metrics and traces, so there's a lot. Let's tie it all back together with a short demo. OK, let's go to the demo part then. So in this demo, uh, now we're gonna see kind of the, the, the patch clinic that I, I showed you before, but now in the observability general observability use case, and this is a perspective of, uh, an administrator, a manager that is, you know, using, um, uh, fully. A fully uh uh shaft um hosted in uh in agent that is hosted by uh agent core and then behind the scenes that is multiple microservice that you saw it before this is the same old, uh, uh, jaunt, um, application, uh, so you have database and so on and so forth. So, uh, here I'm just typing a couple of, uh, prompts related to to this path clinic and this, this particular prompt from here. You know, it is showing me like informations related to the owner ID as you can see there will be uh some PRI information as Peter before this is of course is an agent that's been used for a uh uh uh administrative perspective and in this last prompt it is related to uh billing information for this uh uh path ID number one. And what you're going to see is the agents will not be able to capture that information, to fetch that information for some reasons. Now this is of course it is the uh the front end. Let's go ahead and and actually see in the console. So 1st, 1st of all, in the left front side on on Amazon called Watch, you're gonna see a new section called a GAI GI observability, and this is the model invocation that um uh Peter showed before. You have the golden signals, the GII golden signals for invocation latest and so on. And all the invocations that you have in the foundation model, you will be able to either filter, query them and call logs inside and also see the whole JSON file. You can even filter by model IDs that probably are using multiple different foundation models that you can also see. Now going to the Badrock agent core, um, not just for the runtime, but we all call watch also observe other, uh, parts of agent core too, so, uh, memory, you know, gate and so on. Here focus on the, the agent run time specifically. I have 3 agents. I see all the overall information regarding these agents, how many sessions, how many traces going deeper in the path cleaning because it's the one that I've been using. I wanted to. Show a couple of information here in this overview tab. I, I, I have out of the gate information related to the evaluation. So I'm, I'm using evaluations and I can see the evaluations score and also configuration metric. I can see the error, um, uh, in latent by span specifically, and I have this table, as you can see there's many errors related to the Dynamo DB, and I can, you know, uh, see that information, of course, double click it if I need it. Um, scrolling a little bit down, still, still on the overview page, I see the how many sessions this agent had, how many traces, and also how token consumption, very important for cost optimization and how it the best charge you, it's, it's information that is useful, um, how many, how many sessions this time has and the latency, everything that you're seeing, it is already prebu. For you, last but not least is was the CPU, um, um, and memory utilization. Now each, each interaction with the agent, it creates sessions, and sessions contain traces and traces contained spins. So going deeper on the, uh, on the session, uh, part, I just, uh, reduced the time for 5 minutes. So captured the last prompt that I did. And what I can see it is, uh, kind of the same, uh, information that I have, but now looking for just for this particular session so it's allow you to go deeper on this particular prompt. If I click one of those traces, what I can see out of the out of the gate, it is how many spans latency token consumption. If I have an errors, I already see one of the errors. Remember that I. I was not able to fetch that data. I also see the trajectory of, of the this whole prompt cover to cover what exactly the agent had to do in order to produce the information back and all the tools that the agent have to call on my behalf. I, I'm able to see also the pans in the same in the format of a timeline, a tree, and also in the timeline here I'm showing. The token consumption on that particular invocation input and output and this is this, uh, same visualization but now in a timeline perspective. So as you can see I have multiple ways to understand how how this particular session is doing looking for the, the event itself here is where I will be able to, to see the prompter that I did, right? It, it is trying to get information regarding, uh, uh, payment and then a tool internally will be, will be a trigger. And then what is gonna happen because that information is in the Dynamo DB table, the Asia doesn't have permission. So it is throwing an exception access deny exception over there because the agent itself doesn't have a permission to fax that information. That's why I was not able to, to see that information on the, on the agent. So here in order to fix it, I needed to, of course go in the IM role of the agent and give her the permission to do so. Now, I wanted to show, uh, because we, we mentioned, uh, related to evaluations, right? I have a, a LLM as a judge, uh, in front of this particular, particular agent and I'm just gonna change the time here to to 12 hours to cover how many sampling, uh, information was used to analyze by this LLM. And first of all I can see the the evaluation configuration across the board. So filtering, uh, is, it is available and I can see like related to to stereotyping to, uh, session and so on and so forth. Uh, so each evaluate the evaluation is offered that same granularity session trace and spans do. So as you can see all the, all the tracings that I have in this particular agents. It it is right there and it can filter by many, uh, many options that is being used by the evaluator. I also have, uh, the dashboards, right, dashboards, building dashboards created by, uh, called Watch. So each one of those elements, those metrics that is being used by, uh, the evaluator, it is right here. So instruction follow if if the LMLM use the instruction that I gave, if it's harmful, if it is helpful, if it is stereotyping, all, all, all that information, it is right here so I can understand, uh, the, the, the agent behavior itself and of course I have that as in a log format to using call watch logs. Uh, I just wanna go back to, to the demo and to the session. Um, specifically to show one last case, 11 last use case, um, going back to the same, uh, session, but now one of the prompts that I, that I did, if you remember, was, uh, related to getting information for a specific. For an specific uh uh own and again here's gonna I I'm gonna have all the information related to the trace, uh but I wanted to show because I have data protection rule in place uh for my call watch and that that specific rule is masking, uh, birth dates and also telephone numbers as you can see it will be masked, uh, in the logs too. Right directly here on the on the console. So Uh, we're closing, um, to our, our session today, so I wanted to make sure, uh, you, you leave with a few takeaways. So first of all. Uh, getting started today enhancing like your observability stack as you saw in the first, in the first demo and also in the first part, uh, we talk about application monitoring. You can use uh APM. To help you enhance your uh overall application, understanding more the interconnection, the dependencies that you have in the application, it is can use application map, for instance, to see the topology of your application and is, uh, automatically discovered using open telemetry. Second of all, you are building agents. You should build agents and use call watching AI observability to. To overall understand and observe your agents if they are hosted by agent cop or not, you can still use CaWatch to generate observability and fully integrate it with Amazon CallWatch and use the assessor evaluation. assess programmatically your agents using the evaluator. We just launched that, uh, two days ago. So set up the evaluation in order to understand your overall, uh, structure and following for your agents, and you have all that in a single pane of glass. Last but not least, I wanted to give a, a couple of, uh, resource QR codes, uh, here that's useful. The first one, it is, uh, demos, uh, demo codes. We have demos, uh, in the GitHub available not just on, on Bab Rock Agent Core but also agents hosted in, uh, EKS, ECS, EC22. Uh, the QR code in the middle has all the, the links related to a bras or observability. So we host a show in a YouTube channel called the Coloration show that we talk about observability coloration as a whole, um, we have a workshop, best practice, everything that I mentioned, it is in that single QR code in the middle. The, the last but not, uh, but not the least, the third one, it is, uh, a blog post of this particular launch that I, I just mentioned related to generative AI observability. If you're interested in this topic, I wanted to invite you. We, we are, we have a lot of kiosks, uh, on call-ups in the village, and in the best village, um, and just, you know, swing by. We have, uh, one on one demos. Uh, we have SMEs, uh, across the globe. So if you're interesting to ask a question, you know, uh, myself and Peter is gonna be there outside. Uh, we also can answer your question, but if you wanted to. To go to the kiosk, please do so. We have swags. We have stickers, we can do demos, so just, just go there and we, we're gonna help you. Um Last but not least, uh, I wanted to thank you. Thank you for your time. Uh, appreciate it, you come to this presentation. I, I, I really wanted to ask you to please complete the survey. We use that data every year to improve our sessions. So if you go to your, uh, ERs events app in your, uh, cell phone. You'll be able to go to more and then surveys on the top. This is COP 326, so please give your uh uh data points related to this session to so we can improve next year. So with that, I wish the best of luck to the rest of the reinvent and thank you very much for being here. Thank you.