---
video_id: fdEizoLj5tU
video_url: https://www.youtube.com/watch?v=fdEizoLj5tU
is_generated: False
is_translatable: True
---

So, good morning everyone. Thanks for joining us today. Um. For this session, we're gonna be predominantly talking about streaming at scale, um, and focusing really on Amazon Cloudfront to do that delivery. So my name's Jamie Mullen. I'm a specialist solutions architect here at AWS and I'm joined by my colleague John Councilman, who is also another specialist leasing architect. Um, and with Jess Palmer, who is from Netton to talk about what scale really means to them. So we're gonna be thinking about, you know, those live scale, er large scale events, sorry, those critical events that just must be delivered without any issues, um, and also those events with many, many viewers globally. But it's easy, right? You have a contribution feed, er, you do some processing on it, and it just. Goes to a viewer. That's it. What else do you really need? Um, well, it, it's a little bit more complicated than that. Um, and we're gonna look at, you know, some of those design decisions that kind of help you influence that, that delivery. But scale can mean different things to different customers. For example, it could be millions of concurrent viewers, or it could be billions of minutes consumed. But no matter the scale, we frequently get asked by customers, like, how do I monitor my distribution? How do I protect my content at the edge? How do I scale to that global audience with that high uptime? And finally, can I monetize that live content? You might be wondering, like, what architectural decisions do I need to make to make sure an event is successful. Um, glad to say you're in the right session this morning. But what happens if we get our job wrong as video architects or software, uh, uh, solutions architects and video engineers? Well, viewers might get pixelation, degraded viewer experience, and, or arguably, it's my personal, or most frustrating experience is that spinning buffer wheel. So, we're gonna be focused predominantly on delivering content through Amazon Cloudfront today. Um, if you don't know, Cloudfront is our content delivery network, or CDM. Cloudfront is a large global network with over 750 edge locations. All the edge locations are connected over a redundant and private backbone, back to AWBS regions. And Cloudfront can be used to accelerate applications of all types, whether that's static assets like video files, image files, um, CSS, JavaScript, and they can be cached in edge locations and delivered really closer to those end users. Um Cloudfront can also be used for dynamic traffic requests, too. So the talker's gonna kind of revolve around this steel thread architecture today. And we're gonna look at different design decisions through, throughout the workflow. Advance warning to all the video folk in the, in the audience, we're gonna start downstream this time. So, first off, we're gonna look at delivery. You know, some OTT providers deliver these streams for a cost, whether that's subscription or pay per view. So you naturally care about, you know, security, protecting your content. And next we're gonna look at encoder origination. So for big events, or big sporting events in particular, this is really, really critical and important. You want to design your workflow to ensure the event is successful delivery. Delivered successfully, sorry. And then finally we're gonna look at ingest. Simply put, if you have no feed, you have no event, right? Um, so we'll explore different ways to make ingest more reliable. But the first thing that we're gonna kick off with is monitoring and ask that question, how do we know we're actually delivering that event successfully? OK, thanks, Danny. I'm in, um, I'm a solutions architect with AWS. And We wanna look at how can we gauge whether our live event was successful, um, as you can see here, there's a lot of data available to us from our platform, things like exit errors, video start failures, rebuffering, a lot of this stuff either comes from the client itself, the video player, or it's coming from Cloudfront. So this might be. Presented as logs it might be beacons. It's coming from a lot of different directions. We have a lot of data, which is good, but it's also bad. We have a lot of data. How do we deal with it all? So we're gonna look into uh some features on Cloud Front and, uh, uh, some other ADBS services to see how that we can actually make sense of everything here. So first off, every event uh needs uh needs good data. We need a dashboard. We need real-time access to logs, and uh we need to be able to analyze the event after the fact. So first off, uh, with metrics, uh, Cloudfront actually uses CloudWatch to, uh, to deliver metrics. So I like to use CloudWatch dashboards, uh. Kind of I compare it to an actual dashboard like on my car, uh, when we go camping, we actually pull a trailer and I'm constantly looking at two gauges on my dashboard. I'm looking at the fuel level because I'm getting like 5 MPG on my car and I'm also looking at the transmission temperature. So if there's an issue because I don't normally pull a camper every day, I know exactly what to look at. I know if it gets too hot, I don't need to put gas in the car. It's actually the transmission I need to pull over. So the same is true for a streaming event. Most of these only last a few hours. In the case of like a sports event. Uh, we need to know right away whether we have a problem with an origin, whether it's call front, or if you're using something like DRM or advertising, whether there's a problem with that affecting our stream. So dashboards are the first line of defense, so we know where to look, where to dive deeper. So speaking of diving deeper, uh, Cloudfront, it supports real-time access logs, and as you can see here, it actually uses Amazon Kinesis to deliver this. Delivering, uh, real-time logs for millions of viewers is extremely difficult. Kinesis is designed exactly for this, so when you configure real-time logs in Cloud Front, it's gonna ask for a kinesis standpoint. And the nice thing about Kinesis is you can have multiple consumers pulling that data in parallel so we can have real-time dashboards. We can have Um, observability platforms, analytics platforms, we can also share this data in real time with partners whether they're looking at viewership in real time or if you're looking at advertising data and they need that information in real time. So then with this real-time data we can see issues before they become a big problem and react to it and we can adjust our platform in real time as needed to handle these largest events. Then lastly, um, access logs, uh, these are stored in batches on S3 as you can see here, the delay is usually in the minutes, um. Nice thing about S3 is we can tie uh we can tie Athena into it so we can query all these logs using standard SQL and we can set life cycles on the S3 bucket so we can have, uh, events from weeks or even years ago stored in archival, uh, so we can compare this previous event to one that we did pre uh, you know, maybe a week or a month ago and see how we've improved and find out lessons learned. Uh, we can do some troubleshooting after the fact and we can pull reporting easily using Athena. Then lastly, console reports, uh, this is in the Cloud front console and it's useful for just, you know, figuring out trends, viewership, uh, cash flow ratio between events and monthly reports so that's always available as well. OK, um, next, um, I spoke a lot about, you know, Cloudfront and its logs, uh, but typically Cloudfront only has access to things that are in the HTP header or geo information, uh, such as, uh, user agents, the client's IP address, maybe the geo region, the ASN cache status. This has always been logged, whether it's in real-time logs or in the S3 logs, but I mentioned earlier, um, about the client. The client has a lot of really valuable information. Uh, such as like the Thoroughput, uh, the client being the player, uh, like dash. JS or HLS. JS, it's making decisions about which video bit rate to pull based on what it knows your home internet bandwidth is and having that kind of information along with session ID and content ID is extremely valuable for figuring out what's going on. But in the past this information was only available using like SDKs from third parties and it was sending it using beacons to some other platform so you had Cloudfront or CDN logs in one bucket, then you had all the player information in another bucket and really no way to tie the two together and uh trust me it's extremely difficult to do that. Uh, it can be done but it's not easy. But what if there are a way to combine all this client information. Such as, you know, the bit rate being used, the session ID, which rendition, whether it's HD or standard definition content, and combine that with all the, uh, information that Cloudfront knows about, such as HTP headers, uh, you know, user agent IP address, and that's exactly what CMCD, uh, common media client data attempts to solve. It's actually a standard now, uh, that was created by CTA Wave which has created a lot of other standards that Jamie's gonna talk about later. That, uh, pretty much all CDMs adhere to and the players know about, so using CMCD all this information. All this information is kept in the same log line, so all this, uh, content ID session IDs transmitted as query strings or headers to the CDN. And since it's standardized, we know what to do with it already. And a lot of partners, uh, that you might be using for a monitoring platform already know how to deal with CMCD. So, uh, Cloud Front has supported CMCD for quite a while. Uh, if you look here on the left, this is a real-time log configuration, as you can see here we have additional fields that are standard CMCD fields. They're all prefixed with CMCD, and you can just choose one, some of them or all of them, depending on what your client's sending, and all this information will be added to the real-time logs that we're sending through to Kinesis. And again, a lot of partners already support this on their logging platforms. And we actually have a reference sample that already knows how to deal with this additional information in the log lines and it has a real-time dashboard, uh, that can, uh, it actually refreshes in real time based on all this data to show exactly what's happening with the client and with the CDN so you can scan the QR code here or just search for the uh video observability with CMCDN Cloudfront and try it out on your own distribution being a reference uh sample it's free to use, um. And you can edit it at well it's open source. Excuse me, so it is easy. CMTD is easy. Uh, again, uh, you saw it on the real-time logs, uh, in Cloudfront. It's just adding those additional fields and having a consumer dashboard that knows how to access it. Uh, this is an example of some of the players you might be using today. Uh, these are two popular open source players, HLS. JS and Dash. JS, and, uh, as you can see here, um. Configuring CMCD is just a matter of adding a few more key value pairs, uh, when you instantiate your player, uh, in the case of HLS. JS, which is one I'm very familiar with, it's just a CMCD structure, uh, but notice specifically on the left, uh, we have a session ID now that's actually, um, a common or standard CMCD field, uh, which is extremely useful if you are using CMCD and not using session ID today, um, highly suggest you do it. Uh, that's usually pulled from like your content management system for that particular user when they start a session. When you, when you include this session ID in a field, you actually can find all the logs that are tied to a specific user so you can see individual users journey from starting to play back your stream till end, how many times they switched bit rates, rebuffered. Stalled all that so you can you can figure out individual user journeys and then use the same information to find other users' journeys as well based on session ID which wasn't possible before since the client information and the CDN information were stored in separate locations so it makes it super easy. OK, so now, uh, this is, uh, security requirements. Uh, we talked about logging, um, we've added visibility to our cloud front distribution. We wanna make sure all of our users, uh, are valid users are receiving, uh, the best possible experience, but we don't want our, um, uh, invalid users that are trying to steal our streams, uh, take down our, uh, you know, steal our content or take down our streams. We really wanna stop them from, uh. Receiving our content and harming the experience that other users are experiencing. So what do these users look like? Uh, well, ideally in a perfect world you'd have your origin server behind Cloudfront. You'd have Cloudfront delivering a standard URL. In this case it's HLS and index. M3U8, and all of your paid or subscriber subscribed users would pull the content. That's not the case as we all know. Uh, we have, uh, we see users they're trying to re-stream the content, so they're a paid valid user coming from maybe a particular IP address pulling our content and then re-streaming it to their users who are probably paying them. Obviously we don't want that. And then invalid regions uh. Not only have we not really designed our infrastructure to maybe operate in a different country for scalability purposes, uh, when we have viewers from invalid regions, they're messing up our stats, so they might be having a bad experience and it's going to mess up our analysis of how know whether our event was a successful or not. If we see viewers coming from outside of the country with high latency numbers, that's going to affect all of our reporting. And then lastly, uh, Jess is going to talk about it in a little bit, uh, with rights holders, uh, there might be even legal consequences of delivering content outside of your country where your, your contract, uh, states that you can only deliver in a certain region. You only have rights to deliver in maybe a specific state, uh, in the case of sporting events or maybe a specific country. Um, I know in the past, uh, dealing with some customers when the Olympics are involved, the contract for their is extremely specific when it comes to. Regions and the security required to deliver that content. So again, rights holders are very important as well. And then lastly, unpaid users that have found their way around any cloud front restrictions and are able to go to records to origin. So, uh, obviously we don't want that, but also if you're able to go to the origin, it's going to affect the experience of everybody because their origin should be protected behind Cloudfront where Cloudfront serves 99+% of the traffic from its cache. So the origin server should never be delivering content directly. OK, so what tools do we have available to us? Um, so first of all, my, um, son, he's 7, he's actually been home from school, uh, recently, uh, on vacation, uh, for break, and, uh, he tends to find ways to stream YouTube when he's not supposed to, so I've tried the, um. You know, the firewall rules in my home internet and um eventually ended up just locking the remotes in the office and networks uh so that's the tools I have but obviously on cloud front we don't have physical locking keys, but we do have some ways to um restrict traffic. So first off, uh, for anybody who's been in the cloud front console, you've probably noticed the geo restrictions built in. So that's just built into the distribution in the console or through the API for using that. So this is a way to quickly allow or disallow specific regions. So if I'm delivering content that's only allowed to be delivered in the United States, for instance, I can go into Carfront and quickly add geo restrictions to only allow the US or I could block specific countries within there. But the next layer we can add on top of that, the next tool in our tool kit chain is AWS web application firewall, WAF. So in this case we can inspect anything coming in whether it's headers, IP addresses, um, against a um a list, and we also have a list that you can subscribe to so we have to manage lists so um we are able to allow specific regions with geo restrictions, but what about users who are using maybe uh VPNs or proxies to access our stream. Using WAF we can actually subscribe to lists of known VPN endpoints, known proxy endpoints, or in the case of the re-streamer, uh, we found there that their IP address, uh, in the case of the example was 1234. During the event we can actually add that IP address to our own managed list to block that user that's a re-streamer. So WAF is really useful for before, during, and after events. So if you, if you notice something specific, you can add rules after the fact for your next event or in the case of the re-streamer, you can actually modify it during the event. And then lastly for anything above and beyond uh geo restrictions and WAF we have functions. So Cloudfront functions run as JavaScript actually on the ad server. They're extremely low latency and high performance, um, and you're able to analyze the request and the responses coming from, uh, uh, Cloudfront or to and from the origin, and we've added key value stores as well. So you're able to add data to the key value store using the API and query that on edge function. So we're going to go into detail about what this can be used for when it comes to security in a minute. So how do we secure our origin? um. So as we all know, Cloudfront's origin agnostic, so the origin could be anywhere really. So first off, if our origin is on-prem, it's most likely on a public subnet. So we obviously don't want users to access that directly so again it affects the experience for everybody. So, uh, the first thing we can do is when you configure your origin in Cloudfront, you can leverage HTTP headers that's in the origin level. You can send a secret header to the origin in which the origin can validate that this is actually in fact coming from Cloudfront, and then you can also, we also publish a list of known IP addresses of Cloudfront edge servers that you can add to your firewall on prem or on the origin itself to allow only Cloudfront to access it. And then second, uh, if you're using something like an application load balancer or network load balancer maybe EC2 instances behind those for your origin. Uh, CloudFront now can leverage VPC origins. I don't know if, uh, everybody's aware of that. And recently we've even introduced cross, uh, cross accounts, sorry, cross-coun VPC origins. So if you're using multiple accounts, CloudFront can access these origins on a VPC from other accounts, whether they're your accounts or even, uh, partner's accounts, uh, which is huge because if your origin is not even on a public subnet, it can't be accessed in the first place. And then lastly for some of our managed services delivering media, a lot of you use S3 for live and on-demand media delivery as well as elemental media package. Uh, these can use words and access control. So those are, uh, rule sets that specifically allow only Cloudfront to access, uh, there's content using signed URLs. So I mentioned functions uh to take uh to basically expand upon security above and beyond WAF and uh geo restrictions. We see a lot of customers use tokenization. Uh, within functions to further secure content, so a token, um, it's, it's actually typically we see JWT used, but there's quite a few formats out there and using functions we're flexible. You can basically implement any sort of function, uh, mechanism that you'd like, whether it's JWT or some custom function, uh, uh, or some custom token that you use across other CDNs. So inside the token, it basically describes who you are, uh, what you're able to watch like a content ID. And it would have a time limit. So in the case of JWT, it's very easy to set a lifetime. This token is only valid for 1 hour, 2 hours, a full day, and then it also provides, you know, instructions on where and what you can watch. So ideally a user with a valid token, they're sending all this header information and IP address information to cloud front. We're able to look up their geo information using geo headers that are configured in Cloudfront, and the token contains all this information that they're able to watch from a specific maybe US state they're tied to this particular IP address or user agent, and the function is going to validate all this against the token and what Cloudfront knows about the user. And if everything matches up, the token wasn't modified in transit from the user. And the token hasn't expired, then we will just return a 200 OK within the function. And then we have users with invalid tokens. Uh, maybe they modified, they tried to modify the token and create their own token, and it's invalid because it wasn't signed correctly, or they're coming from an invalid location, uh, you know, they, they maybe traveled with a valid token. We signed it for their home use and they're on their cellular network and they didn't receive a new token. We'll actually return back or manufacture a 401 unauthorized response from the function itself and immediately deny that user before they even hit Cloudfront in the origin server. And then lastly we have a user here with a shared token again all these all these examples are using JWT, which is really common, uh, which is easily implemented and functions. Uh, this user has a shared token, so they received a valid token from the first user and uh they're trying to reuse it and they might be living in the same neighborhood and using the same device, so it happens to be valid for some reason. We noticed that this user, uh, this token is being used multiple times and is not valid. We can actually add this token value into the key value store. So that's for us at a good use case for key value stores and functions is token revocation. So you can insert this token into a key value store when you notice something wrong with it that's being used too many times, and then you can have the function return of 403 forbidden if that token's in that key value store. So again, uh, this is a common use case that we see for tokenization JWT tokens, uh, but actually we've recently, right before reinvent, uh, released CBO token support which is, uh, needed by the Common Access tokens which again was created by CTA Wave. So instead of having to figure out how to write. A full token authentication workflow in functions you can just tie directly into CBOR, which is supported as a module in functions and the code, the code is extremely minimal. I was using it earlier and it's just a few lines, so that's natively supported now and against, uh, common access tokens uh seems to be where the industry is headed because it has been standardized now. So, um We do have guidance for secure media delivery at the edge on AWS, so this takes, um, all the work we've done on functions and does it for you. So you can uh simply deploy this on your cloud front distribution and we're going to deploy all the functions supporting JWT token support and we have a step function available that's also deployed that can automatically revocate uh revocate tokens that are being used, uh, incorrectly either multiple times or from invalid users and, uh, we've even added a um. A full, uh, playback application, sample application with video player so you can try it out so you can just deploy this, uh, using cloud formation on your own distribution and being open source you can modify it at will. We even included SDKs, uh, for your content management system to create tokens and sign tokens when you create playback URLs. So check it out if you haven't, um, and we're actually enabling support for common access tokens in this really soon, uh, that's coming shortly. Uh, so yeah, just keep an eye on it for the updates. OK, so I'm gonna hand him back to Jamie for a talk on last mile capacity. So large scale large scale events can generate significant internet traffic spikes, say, for a really short period of time, with strain network links that connect viewers to end content. However, there might actually only be a certain amount of capacity between an internet exchange itself and an ISP or a smaller city. For example, ISPs continuously optimize their networks, but scaling capacity for, like, those last minute big events is understandably challenging, right? Um, especially those events that, you know, you only need that capacity for that short period of time. When a network becomes saturated or congested, um, both packet loss and increased latencies can occur, and the result on the viewer experience is things like quality drops, rebuffering, or even those playback failures. Remember, you can lean on that monitoring approach that John spoke about earlier to try and help you identify some of these issues. But unless you own the network or the underlying network as an ATT provider, this is something that is really largely out of your control. Last year, however, we introduced embedded Pops. And they overcome this challenge by getting you closer to those end viewers, caching content directly inside an ISP network and removing some of the dependency on the network links between the viewers and the internet exchange. If you didn't see it on the Pop map before, we've got over 1140 embedded Pops across over 100 ISPs in more than 20 countries. Because that distance to end viewers is reduced further, customers have already, uh, customers that have already adopted end pedipops have seen performance improvements too. Because we're able to remove some of the network dependency, they're really great for highly cacheable content. So we're very focused on the media and entertainment space today, obviously, um, but you can also use them for things like, or content like game delivery and also software downloads. So, as part of the onboarding of embedded pops, we validate the workflow that you want to use them for. If it is a good fit, uh, the distribution is enabled up by us in the back end. However, there is some additional work required on your side to make sure your client application can route to embedded POPs. The way that we recommend doing this is integrating the client routing SDK into your Ur vending service. So how does this work? Well, firstly, you need to do that integration of that client routing SDK into your Euro vending service. The QR code on the bottom right is actually the link to the, that, that SDK in GitHub, if you're interested in having a look at that. So when the client application requests a URL from that vending service, the client the client IP is encoded into the URL and it's annotated as CR label in this diagram. The client then performs a DNS lookup, um, and Route 53 and CloudFront is able to decode that client IP from that CR label itself. Then Cloudfront returns the best pop based on that client IP, whether that's an embedded pop, if appropriate, or a traditional pop. The client can then start fetching content from Cloudfront. So in summary, embedded POs deliver cached content directly from ISP networks, improving performance and overcoming those potential last mile capacity constraints that you might have. If you want to make use of embedded Pops, I really urge you to contact your account team to help you validate, evaluate your workload to see if it's suitable. So typically, the next bottleneck that we see is the origin. Our aim really is to offload as much traffic as possible from the origin, so that it has enough capacity to serve requests at scale. This is to prevent our origin from becoming overwhelmed and failing entirely. Caching helps with this, of course, and Cloudfront uses tiered caching, so we have edge locations, we have regional edge caches, but you can also use something called Origin shield to further reduce that origin load. Instead of regional edge caches hitting the origin directly. They go through Origin Shield, and this provides a better cash hit ratio, reducing demand on your Origin. Especially for those events, you know, we've got a lot of scale. So we've covered capacity, we've covered reducing load and ensuring viewers can get a great experience. It's time to really think about how you design the rest of the platform to provide just that, right? To quote Werner Vogels, Everything fails all of the time. Lots of customers approach us by thinking about how many nines do you want to design for. But, when you're thinking about a media world, this can also be very different if you're designing a 24 by 7 channel, for example, and those event-based channels. And the other important part is, you've got the task of really balancing the cost and how much resilience is really enough for you. If your platform fails entirely, ah, users will be unable to consume your content. And this might impact you in other ways, such as increased customer contact or maybe even financially too. For live events especially, there's no real second chance or try again later. That moment's just simply gone forever. Like we've discussed, things can go wrong. Um, so redundancy is really critical to keep your stream up and running so that you can deliver, ah, to your, to your end users. In Nick's example, in particular, we have two ECTs. Into availability zones to try and help combat this. We do see customers deploy the Rain encoders and origins in AWBS, um, but we do offer managed services for media workflows. They offload the challenges of managing your own customized infrastructure and gives you capabilities and features that we'll actually talk about in a little bit. These features are such as like helping you provide that best viewer experience possible to viewers. So if you're not familiar with ADS Elemental Media Services, this is a typical workflow for an ATT stream. So we have ADBS Element of Media Connect, which is our secure and reliable transport service for live video. That's then fed into AWS Elemental Media Live, which is our cloud-based broadcast grade video processing service. Then have ADS Elemental media package, and that is our just in time packaging and origination service where you can implement DVR-like functionality and things like DRM. And finally, we've got Amazon Cloudfront, which is our CDM which we're predominantly focusing on today. And then viewers, depending on, you know, what they want to stream, what manifests they need, can pull different origin endpoints from media package. So with this, we've kind of moved from ECT and we've gone to AWS Elemental Media Services, so we no longer have to worry about, you know, those underlying instances and get all the benefits of ADBS Elemental media media services. For example, Media Live supports cross AZ synchronization and redundancy and for failover scenarios. Customers running large events tend to implement more redundancy than just one region, though. Uh, so we're gonna look at what that multi-regional architecture might look like. To help kind of speak this through, I'm gonna look at a typical, uh, failover scenario that you might see, but looking at where manual intervention might be required. So, we have the same architecture as what we had, uh, but obviously we've got Region 1 and Region 2 with Media Connect Media Li and Media package. We also have the addition of Elemental Live, or RDX Elemental Live, which is our on-prem, uh, encoding appliance, which in our case, is doing that venue contribution encoding. So let's say something goes wrong with uh one of the, the contribution feeds going to region one, where the viewers are currently streaming from. Well, Viewers connected to that might start seeing black frames instead of the actual video content or real content, but hopefully you've got a real, uh, uh, an eyes on Glass operations team, and they spot this issue early on, uh, and decide that they want to do that failover to region two manually. Remember there's a time delay, right, between. The issue occurring, somebody spotting it, and then also er proactively rectifying that situation. So, the fail failover occurs, uh, our viewers go to the URL vending service and say, I need a new URL and it returns one for region two. However, there's a few challenges here to kind of think about. Number one, predominantly, is the, you know, it requires somebody manually to do this, right? And there's also a time delay in detecting it and then er remediating it. But not only that, when a viewer rejoins, are they going to have to go find where they were viewing in a stream, for example, and then the timing might be misaligned between the two anyway, two regions anyway. So how do you keep things in sync? How do you do these, these things automatically? So we're gonna explore this further, but we're gonna start with time code. Ah, time code is a time reference that's embedded in each video frame. Time code can be really important in the media distribution world. Um, we wanna use it for synchronization in downstream components, such as the encoder and packager, ah, to provide that frame accurate alignment. So time code is part of the answer, but how is it actually applied to a failover scenario? We have a feature that can help manage this, and it's called seamless cross-region failover. So, we're gonna start with the Elemental Live encoder on-prem, ah, and that's gonna use the same time code source, so both regions receive an aligned, ah, contribution feed. Media Life is then configured with epoch clocking, ah, and uses the embedded timekeeper, that timing source being that input clock. And then CMAP ingest on the output of media pack into out into media package, sorry, and when you combine CMAP ingest and epoch blocking, it results in this regular segmentation cadence based on the EPOch time. Media package can then use stateless synchronization to help predictably package the output content. So what happens if something does go wrong, such as slate input, incomplete manifesto, and a missing DRM key? Well, Media Package actually has the ability to 404 its endpoint. And Cloudfront is configured with something called origin groups, where you specify a primary and a secondary, and also a failover criteria based on a HTTP status code. When it picks up the media package has indeed 404 to 1 of its origin endpoints, it'll automatically send that traffic to the secondary. But what if it's not such a fundamental problem, for example, what if it's black frames, frozen frames, um, in the source, and it's only impacting one of those contribution to go into one of the regions? Well, building on the previous architecture, we have a capability called Media Quality Aware resiliency to, to solve this, or MQAR. So Media Live actually has the ability to also produce something called an MQCS score on each segment, and MQCS stands for Media Quality Confidence score, and that's based on those input parameters of being black frame detection, frozen frame detection, uh, or input source loss, and that score is out of 100. And each score is independently based on its own input. Media Package actually has the ability to use this in 2 locations. It's able to use it on the input to provide in-region failover. And also on its output via um CMSD or common Media Server data. John spoke about CMCD earlier. um CMSD is also part of that CTA spec. And then finally, the, you continue to use origin groups on cloud front, um, but instead of it being default, we use media quality score, uh, being the origin selection criteria instead of default. And the way it works is there's a get ahead request into both regions, and it basically picks the segment that's gonna ultimately provide that better viewing experience to interviewers or clients. So in summary, you can leverage AWS media services and Amazon Cloudfront with MQAR or Media quality or aware of resiliency, to automate your eyes on glass and minimize the duration of disruption events. Got long left. All right, thanks, Jamie. Can you hear me? OK, good. All right, so all, all these measures we've taken to analyze our stream using logging and to make sure our origin and encoder are available are useless if our contribution stream into the cloud is not stable. So we need to make sure our contribution stream is reliable uh just as the rest of the chain on the encoder, the origin and cloud front. So how do we make sure our source is just as reliable as our delivery? Well, some of the things that we require in our source are consistency. We need a reliable source. We need coverage of that source of wherever our event happens to be. And we need it to be private because usually the source coming into the cloud is our highest quality mezzanine feed, so we don't want anybody to be able to access that as it's the, um, you know, again it's the highest quality content and we don't want that to go into the wrong hands. So we do have a solution for that that a lot of customers use, and that is Direct Connect. Direct Connect is used heavily for stream contribution, uh, for consistency. Uh, it is a dedicated connection into a region, uh, offering up to 100 gigabits of connectivity. So a lot of you all are probably contributing, uh, content now through, uh, some of the higher quality lossless formats like JPEG XS and using 2022-7. And, uh, if you're doing things like that, Direct Connect is, uh, extremely helpful for things like that. And also it is reliable. You can have multiple paths into the region using direct connects. And uh coverage is in over 100 locations worldwide in 30+ countries so it's probably uh nearby or at the uh facilities that you're broadcasting from like stadiums and things like that already and again it is private uh it is a private network and we are able to add things like media encryption on top of it, uh, which we'll talk about in the next slide. So this is where uh Jamie spoke a little bit about Media Connect. So this is, uh, one of the elemental services for entry point of live media into the cloud, uh, whether you're using Direct Connect or, uh, Internet contribution, you can use Media Connect for both of those use cases. So if you're using Direct Connect you're probably ingesting over a VPC uh if you're using uh internet connectivity, you could certainly use a VPN endpoint and also ingest through the media connect over a VPC as well. So uh utilizing VPC origins and VPT ingest, the whole media workflow can be private soloudfront supports pulling from a VPC and we can ingest to a VPC as well. And uh we mentioned encryption earlier. A lot of the protocols that we support on Media Connect offer encryption such as SRT and 60 and Wrist. Uh, SRT I see you use quite a bit and enabling encryption on that's trivial and pretty much every encoder out there that publishes SRT supports encryption. So whether you're using the internet or direct connect, uh, encryption is an option. Especially over internet contribution, uh, packet loss is a thing, uh, she, uh, you know, uh, drop packets are a thing. Uh, most of these protocols like SRT and X60, uh, and RIS and RTP FEC, uh, support error correction and for, uh, and ARQ automated repeat request. So this is all adjustable on media connect on the and on the encoder. So if you're doing contribution over the internet. Um, adding a little bit of latency, uh, in exchange for reliability is available on Media Connect, so you can just, uh, you can add a little bit more error correction for known spotty connections, and both of these protocols, uh, Dixie and SRT that I mentioned, allow for fellover, uh, of the stream. So you can have multiple ISPs contributing over Media Connect, and if one of them has an issue, uh, during the live event, it can automatically switch over to the other one. And then I mentioned earlier about metrics. Uh, they're super important. Uh, the dashboard that we create before the event has got, it has cloud front on there so we know if there's an issue with Cloudfront, uh, most likely it contains, uh, metrics from the origin server to know how the origin's performing and if we're using DRM or, uh, ad insertion. Those metrics are also available. We can also add ingest metrics to the dashboard as well for, uh, you know, ART recovery drop packets so we know we have a quick view on whether our ingest is having issues, whether it's craw front of the origin, uh, for SRT, which is what the one of the most common protocols for ingested, uh, over the internet, uh, these are the recommended metrics that, uh, we see, uh, being used for dashboards for Media Connect. These are the ones that, uh. You know, from the largest live events that customers are actually monitoring really closely for ingest quality. So next, uh, Jess Palmer is going to come over. She's from New England Sports Network, home of the Red Sox, and she's gonna speak a little bit about her platform. Thank you. Um, so Nelson is not the largest RSN, but we are actually the first RSN to offer a direct to consumer, uh, live streaming app in 2022. Um, as a regional sports network, NSA has a small digital team, um, mostly me and my boss serving a growing user base armed with basic streaming knowledge, we were able to easily learn and manage the stream flow through AWS cloud services. RSNs like Nessen have unique streaming challenges. They must deliver high quality, low latency, live and on-demand sports content, enforce regional broadcasting rights, and handle unpredictable traffic spikes, all while maintaining strong security and compliance. Additionally, unlike many national distributors who receive SRT feeds and then retransmit those, at Nessen, we control every step of the broadcast. First, the game is captured by cameras which are connected to our mobile production unit, otherwise known as the truck. It's an actual truck with the production equipment inside it. Uh, we have 4K cameras and HD cameras at Fenway Park for the Red Sox and at TD Garden for the Bruins. For away games, we have our own HD cameras that travel with the teams. The feed is then relayed over the Internet from the truck or from the opponents stadiums to master control at our Boston office. These feeds are shared directly with our linear distributors or fans still watching sports the old fashioned way through their cable subscriptions. And additionally, the SRT feed is routed directly to AWS through AWS hardware encoders. About 50% of the Red Sox and Bruins season's games are home games which we offer in HD and 4K. AWS makes it easy for us to manage the availability of our 4K stream using an AWS elemental encoder via Media Live for transcoding and streaming management. We installed Media Live anywhere on our AWS elemental encoder. With John's help so that we could administer the feed through Media Live the same way we manage our HD and UHD links in the Media Live dashboard from the 4K source we configured a Media live output group that contains 4K, 2K, 1080p, and 720P video outputs as well as stereo and surround audio outputs and closed caption outputs. We kept the traffic. Uh, sorry, we kept the audio and video tracks separate because we found that mucking resulted in issues with some of the OTT players. We used the media package V2 channel group to create dash and HLS origin endpoints again for compatibility on different systems, um, and those origins are then used for cloud front distributions. A major advantage to using Cloudfront is that it automatically scales to accommodate traffic variations. So for example, if the Sox are playing the Yankees, we don't have to worry about edge capacity. Cloudfront takes care of that for us. We also occasionally get third party feeds or one-off direct feeds like batting practice directly from Fenway Park or morning skate from Bruins practice arena that we offer as digital exclusives and we have an HD link that we. Use for those one-off events we use the same basic flow for those events. The live or third party feed is rolled out through the HD link, and the HD feed is transcoded into lower output tiers to accommodate CTV and mobile devices. Then it's the same media live to media package to cloud front flow. Additionally, cloud front logs across all services provide metrics and alarms that help detect performance issues early, maintain stream quality, and ensure smooth delivery. Having these logs easily accessible and queryable using log insights is essential to a lean digital team like ours. In summary, Cloudfront empowers Nessen to deliver reliable, high quality streams for every fan, scale seamlessly during live sports peaks, protect media rights, and maintain regulatory compliance, and gain real-time insights into audience behavior and performance. Right, thanks, Jess. It's really cool. I always uh love everything you're doing over there. Uh, Jess spoke a little bit about Media tailor. Uh, it was on her diagram, uh, earlier when she gave a demonstration. Uh, Media Tailor is a managed service under the elemental umbrella that does, uh, ad replacement and ad insertion for live streams or on demand streams actually, um, and, uh, this workflow here again, it uses media tailor, but a lot of the other ad, uh, insertion platforms work in a similar manner. Um, when you're trying to monetize your streams, uh, it's a whole new set of challenges. Um, in effect, uh, This is called dynamic ad insertion. It is by the name, uh, you add a dynamic element into it. Every single manifest for end users are personalized because they have ads destined for the end user, whether you're using linear ads or whether you're using some of the newer formats like squeeze back or pause ads. So before we've always talked about scaling and cloud front scaling the origin when we talk about ad insertion. The origin becomes that much more important because every single request from the end user, whether you have 1000 or a million users, you're going directly to the origin and the origins modifying the manifest in real time. So it's really important that your origin can scale with your viewers, uh, appropriately. Uh, when you configure a cloud front for advertising, you're gonna have a couple more behaviors being configured, uh, for your manifests you're no longer caching that you're sending them directly to the origin server which is then modifying every single manifest going to the end user. Then the media tailor in this case uh is going to make a call to an advertiser upstream uh whether it's Springserf, Free Will, uh, any of those ad platforms, and those must scale as well because every single request is going to be a unique request to an ad server. So, uh, it's really important, uh, when we add dynamic ad insertion into a platform that we, uh, look into that as an origin as well as DRM platforms as well, uh, you know, that they're typically not behind Cloudfront. Um, Media tailors actually recently added some features, uh, to help with the ad platform, uh, because if everybody's, uh, going to an ad break at the same time and we're hitting a million viewers to an ad platform all at once, the ad platform, whoever it might be, can't handle that. So Media tailor actually has the ability to prefetch ads before an ad break and stagger requests to an ad server. So these are some of the really important considerations when you're adding uh monetization to your live stream again whether it's uh linear ad replacement, um, overlays, squeeze backs, or any of the newer formats, uh, there's a lot more considerations uh that you have to look over specifically with the origin server, the origin server being the ad insertion provider. So I wanna hand it back over to Jammie for the summary and uh yeah yeah cheers thank you cheers. So, um, To wrap things up We wanted to aim this talk about, you know, thinking of ways that you can, or to help you deliver those events successfully and at scale. We want you to remember these key things when you start planning for your next large scale event. So firstly, observability is critical, right? Whether that's dashboards, data collection through real-time logs and kinesis. But think about how you could use other data points such as CMCD and CMSD to also help you with your observability. Architecture actually, oh sorry, architecture design can actually also go beyond just the incode and the origination, right? We covered AWSL and media services, but also remember how, like, how to leverage other infrastructure to help you absorb some of that traffic, such as Embedded Pops and Origin Shield. Security is also a really layered approach, there's not really a, you know, one mechanism that fits all. So think about your end to end security strategy, covering, you know, tokenization, IP blocking, um, GA blocking, and using mechanisms like cloud front functions and AWS WAF. And then finally, Monetization can be just more than a subscription payment, right? You can use server-side ad insertion to help you monetize your content using services such as Media tailor. So if you wanna learn about or continue reading on some of these topics that we've discussed today, or want to deploy the samples, here are the links to them. And finally, if you're interested in leveling up your skills on network and content delivery, I thoroughly recommend you look out, er, check out SkillBuilder. So that concludes our session today. Thanks for joining us. Hope you found it useful, um, to kind of think about, you know, the next things you need to look out for those large scale events. Um, just a quick reminder, please go to the Reinvent app to, to submit your feedback on the session today so that we can help improve it. Um, but yeah, we'll be hovering around if you've got any questions for. For what we spoke about today, but enjoy the rest of your time here at Reinvent.