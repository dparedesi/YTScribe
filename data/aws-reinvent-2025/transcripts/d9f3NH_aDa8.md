---
video_id: d9f3NH_aDa8
video_url: https://www.youtube.com/watch?v=d9f3NH_aDa8
is_generated: False
is_translatable: True
summary: "This Clumio (now Commvault) session demonstrates how to make GenAI application data resilient using a fictitious 'Clumio Flick' movie app with LLM chatbot features. Three recovery scenarios are covered: 1) **DynamoDB**: Canary deployments can corrupt partitions at different timestamps. Clumio Backtrack for DynamoDB (launched June 2025) enables in-place, granular, point-in-time recovery to the source tableâ€”avoiding 'recovery hell' of full restores per partition. 2) **S3 (RAG Pipeline)**: Lost S3 objects break vector embeddings, causing LLM hallucinations. Clumio Backtrack for S3 (announced at re:Invent 2024) offers in-place, object/prefix/bucket-level recovery with continuous change tracking for zero data loss, avoiding vector recomputation. 3) **Apache Iceberg (S3 Tables)**: Industry's first Iceberg-aware data protection (launched September 2025) preserves table structure during backup/restore, supports snapshot-based recovery, and enables migration from AWS Glue Catalog to S3 Tables. All three use SecureVault for air-gapped, immutable, cyber-resilient copies. Best practices: protect entire data pipeline, prioritize speed, and architect for scale. Demos show Python-simulated corruption and recovery via the Clumio UI."
keywords: Clumio, Data Resilience, DynamoDB, S3, Apache Iceberg, S3 Tables, Backtrack, SecureVault, RAG Pipeline, Cyber Resilience, GenAI, Point-in-Time Recovery
---

Hey everyone, welcome to Reinvent 2025. Uh, thank you for choosing to spend the next one hour or so with us. We're gonna make it worthwhile. Um, in this session we'll discuss how you can make your application data resilient using Clumeo, and these applications can be Gen AI or otherwise. We'll go through a host of scenarios ranging all the way from NoSQL databases to rack data pipelines to data lake houses. Quick round of introductions. My name is Akshay Joshi. I'm a senior director of product management here at Clumio, uh, and Clumio is now part of the Commonwealth cloud portfolio, and I'm joined by Jeff. Good morning, everyone. My name is Jeff Cascio. I am a principal engineer in our cloud sales organization focusing on Clumio and AWS. I'm also a former Amazonian. I was a storage specialist solutions architect. It's great to be speaking with you today. Cool. All right, so, um. Let's get started. Before we begin, let's take a look at why we need to care about resilience right now. AI is everywhere, and 52% of the data leaders that were surveyed by AWS said that their data foundations are not ready for AI implementation, and that's horrible, right? And that's why we're here to talk about resilience at the data layer. It's absolutely foundational. Oh. We want to acknowledge something right off the bat though, resiliency can mean a lot of different things, but I think we can all agree it's essential to your cloud infrastructure. Aksha and I speak to customers all the time. We speak to companies and we get many definitions of what resiliency is, but one thing that's consistent is that there are common occurrences that affect resiliency in the cloud in a pretty negative way, and we've put a few of those as examples on the screen. The first being an accidental deletion. This can be somebody just accidentally making a mistake and delete deleting a directory in your cloud infrastructure that has some critical data on it. It could be a cleanup script that perhaps wasn't just fully dialed in. It's basically a mistake, and mistakes occur. Humans are mistake prone. They're not perfect. That can happen. That's definitely one reason why resiliency is is essential. Another is something that we probably have all heard in the news lately where code pushes maybe go wrong and it becomes a front page of the Internet type event. Software is not perfect. It can create havoc. It can create a scenario where resiliency does suffer. And lastly, it's almost an hour by hour thing. It's definitely a day by day thing where bad actors act with, uh, negative intent on your cloud infrastructure. Uh, prime example is ransomware, but that's not the only example. So we wanna establish what we believe resiliency to be and why it is absolutely essential in your cloud infrastructure. Let's get some, uh, participation from the audience if you don't mind. We have a couple questions that we would like to see you raise your hands in response to. The first is how many of you are using the AWS cloud today to build your applications. I already got one hand to build your applications in a generative AI way with a generative AI or large language model pipeline. I see a bunch of raised hands. That's great. Uh, like, like I mentioned, Aksha and I speak to a lot of companies very frequently, and this is extremely common these days in the AWS cloud. AWS is very effective at making that a reality. One second question that I would love to get a raise of hands about as well, similar and related to the previous question, how many of you are actually protecting the data layer that powers these Gen AI cloud applications? Is that something you take seriously today, or is it an overlooked factor? Less hands, that's actually very common. It's very related to what Aksha and I hear from companies. They're really building some incredible stuff in the AWS cloud that powers their chatbots, but it's interesting how often data protection is overlooked. We're gonna paint a real world world picture for you today. We created a fictitious app. I wanna emphasize this is not real. We didn't get permission to use any actual real apps. It's actually, uh, you know, a little bit of humor here, lightheartedness. We, it was vibe coded by our CTO in about a weekend. It's really kind of cool. I'm very impressed by it. It's called Cluo Final Cut. Think of it as an app, a smartphone app that you probably similar to probably what you use today. It's using an openly available data set of thousands of movies, actual real movies, that typically is used for machine learning type experimentation. And here's the picture we wanna paint. We'll call this user Sharon here. Let's go ahead and press play. We have a video going here. This is our user Sharon. She's using the Clumo Flick app. She's loving it, but all of a sudden something happens. Resiliency is suffering. She tries to go use the AI chatbot as a part of the app to provide her with recommendations of what she wants to watch next, and it starts to fail. There's her using the the movie assistant chatbot as a part of the app that we built. And you can see errors are starting to pop up. It failed to load. She has a watch list. It's not finding her entries. She's frustrated. She went from happy to unhappy. Like a typical smartphone user who have come to expect extremely good performance from AWS because AWS has set the bar very high with that, she goes to her favorite social media app and starts complaining. Perhaps her friends see her complaints, the people she follows, they start noticing the failures. This is a bad day. This is a, this is a failure of the cloud application. Let's, let's dissect what's going on here and in order to do that I'm gonna pass it back over to Akshay to explain the first part of the cloud app. Thank you, Jeff. All right, we all saw Sharon struggle with the application. Um, now let's see how we can dissect the application and see what's happening under the hood, right? Um, so let's, let's go feature by feature. The first feature that Sharon struggled with was the user profile data and the watch list data. And the application uses Dynamo DB under the hood to store user profile data and the watch list data, and Dynamo DB is actually a perfect choice to store that kind of data because it's inherently partitioned and it's multi-tenant in nature. And finally it's also highly scalable, right now, a couple of our customers actually use um Dynamo DB for the very same reasons to store user data. While scalable, it's also absolutely vulnerable, right? So, uh, imagine a standard Canary deployment where you're, uh, updating your application in tranches, right? Users get the updates in tranches, and if a code push goes wrong, this may be due to a bug, for example, the users won't be able to access the data at all because those partitions would get corrupted, right? And that's what happened to Sharon in our example. She couldn't use her, um, you know, user profile or watch list screens because the underlying Dynamo DB table got impacted. Now, uh, let's visualize the problem, uh, a bit more. Let's see how the event unfolded. As I mentioned, the Canary deployment starts at 1 p.m. here. And then partitions are being updated in tranches and at 2:30 p.m. somebody reports a bug that you know your uh Dynamo EB partitions have been corrupted, right? Uh, thankfully though, in this example only a couple of partitions were, uh, impacted but then, um, there could be a scenario where thousands of Dynamo EB partitions may get impacted because it's highly scalable and consumer apps usually have like millions of users, right? So, um. The complication here is that every single partition gets impacted at a different time stamp, so the last known good copy of a Dynamo DB table is actually not a single time stamp. It's, it depends on the partition that's that's impacted, and that's the complication that we're gonna explore and learn about how to recover from in this session. All right, so in case of such an event, let's wear the hat of an SRE. SRE has 4 recovery imperatives to deal with at that point, right? First, they'll have to recover to a known good point in time, and the challenge, as we all know, is that every single partition can have a different last good known, um, copy, right? And then number 2. You need to also ensure that the reconfiguration at the application level is as little as possible, right? Because you don't have to deal with reconfiguring the application to talk to a different Dynamo DB instance, and that can be a time, time consuming operation. And number 3, of course, PCO, right? You need to ensure that the resources that you end up spending to get the app back up online is as minimal as possible. And finally you need to ensure the best user experience, which means little to no downtime and um we don't want Sharon to get frustrated and go on social media at that point, right? So let's take a look at how you can recover from such an event. I'm gonna talk about two different ways of recovering from any event one, the hard way, and two, the simple way, which is the Clumo way. So in the absence of Clumo, it's an arduous process, right? For each partition's last known good point in time, you have to do a full restore. And mind you, this is for each and every partition that's impacted you'll have to do a full restore. And then you need to cherry-pick the pieces for each partition from the full restores, copy it back to the source table so that you don't have to reconfigure the application at a later point in time. And finally, delete all the temporary tables that you've created. And finally, I want to mention that you have to do this over and over again for every single partition that's impacted, right? If you have 1000 partitions impacted, you'll have to do this 1000 times. Now imagine that, right? That's recovery hell. And now let's take a look at how simple it is to recover using Cluo Backtrack for Dynamo DB. Um, it's literally a two-step process. Step number 1, you pick a point in time, choose a partition that you want to recover, hit recover, and that's about it. It recovers only what's needed for every single partition to the point in time that you've selected. And directly to the source table, so you don't have to reconfigure, reconfigure the application at all. And step number 2, there is no step 2. Simple as that. All right, so. Um, let's learn a bit more about Clumo Backtrack for Dynamo DB. We launched Clumo Backtrack for Dynamo DB back in June of this year. It's essentially a recovery modality with three distinct features. One, you can recover to any point in time, uh, that's available in your backup. 2, it's in-place restore. It means that it goes back to the source table directly so that you don't have to create temporary tables, repoint the application to talk to a different restored Dynamo DB table or none, none of that, right? And then 3, granular recovery, right? You can perform a recovery at any granularity, and this can be in the case of Dynamode EB, it can be at the record level, partition level, or at the table level itself. Now, let's see the feature and the event in action. Uh, I'm gonna hand it, hand it back to Jeff to show you a quick demo on that. Thank you, Akshay. We uh pre-recorded a demo of our, uh, vibe coded app called Clumio Flick, and, uh, what I'm gonna demonstrate today is what Akshai just explained about what Clumio Backtrack for Dynamo TV is capable of doing. What you see on the screen right here is what we call the movie watch list, but this particular part of the app is a watch list that allows you to search for a particular customer's watch list. And it is all entirely based on Dynamo DB. You can see I've highlighted the section that all of these records are coming from Dynamo DB items. What I'm gonna do is I'm gonna search for customer ID 3. That takes me to customer ID 3's specific watch page, so very similar to what you might experience in your favorite movie smartphone app. In that customer watch list, I have two things that I can do. I can use the AI movie assistant as a chatbot to interact with our, uh, large language model pipeline and ask questions using natural language prompts. In this case, I'm gonna ask, tell me more about the movie Clumio to the Moon. I mentioned we're using real movies in this openly available data set, but we did insert one just a little bit for a little bit of humor that is related to a fictitious movie that Cluo created. And then in the watch list, it's actually mentioned in there, and then I'm gonna show you in the AWS console where all of this is coming from. We have a Dynamo DB table that has all of the user watch lists in there. I'm gonna filter the table by just the customer ID 3 items, and you can see the corresponding items in that Dynamo DB table. This is the critical data to ensure a good user experience is resilient for customer ID 3. You can see in the movie AI assistant chatbot, it has responded with some really interesting rich information about Clumo to the moon, things like the plot of the movie, some interesting metadata about revenue, some even some catchphrases when the movie was created, uh, and the genre as well. This is a happy state. This is Sharon loving the app, having a great time, a user that will come back, a user that will be loyal to you. Let's simulate an unhappy situation. That simulate a bad day, a bad user experience. We're gonna do that with a Python script that we created that will purposely but simulate corruption in customer IDs, Dynamo DB items. Essentially we're removing them, simulating perhaps an accidental deletion scenario like I mentioned a second ago. I refreshed the user IDs, customer ID 3's watchlist page, everything's gone. This is absolutely a demonstration of what we showed that Sharon experienced a second ago. There's nothing there. Unhappy customer, unhappy user. I'm gonna refresh the Dynamo DB table. That simulated corruption has completely removed the customer ID's entries, and there's no matches found now when I filter for that particular customer. The good news is we are taking a backup of this table, and we have been for some time. What you're seeing on the screen is the CluosA interface. Our calendar view, everywhere you see a dot in that calendar view is a point in time we can recover back to. Usually, like Akshay was mentioning, this is a very difficult process to restore resiliency and faith in your customer with Cluo. All I need to do, and I have different options here, is choose a recovery modality. The first one I'm gonna show you is our cyber resilient recovery modality. So unlike the sim the simulation we're creating where there's been accidental deletion, that's ideal for if there is a, a compromise of some kind, a cyber-related threat. You're recovering the Dynamo DB table or individual customer ID's item from an air gapped immutable secure copy that will be unaffective in the event of a cyber-related incident. I have multiple different points in time and just that particular day in the calendar, so I'm, I have a lot of choices here. But what I want to demonstrate is going back to a point in time in place, something that we announced like Akshay mentioned a couple months ago with Cluo Backtrack for Dynamo DB. What you can see on the screen is I picked a point in time. I know the text is a little small, but I did pick a point in time. Where I know that there's a good Dynamo DB recovery point to go back to, and I'm given three options. The one I'm highlighting is the in-place option, that is the Clumo backtrack for Dynamo DB that is avoiding all of the app reconfiguration that Aksha mentioned just a second ago and recovering customer ID's specific item entries alone in place. All I need to do is search for the customer ID string 3, which I'll do at the bottom here. When I press preview, it'll query that point in time to make sure that what I want to recover is actually correct and there. I've successfully found the recovery, uh, items. I'm given a summary of what I'm about to do. Just to confirm that I'm doing the right thing, I'm gonna initiate the restore that has created a brand new task in Clumeo that I can monitor for completeness. The very beginning of the task, it is queued, and through the magic of video editing, we're just going to skip ahead to when the actual task is completed. And in order to prove that it has been completed, and this is a very similar scenario to what you might do, I'm just gonna refresh the Dynamo DB table. All of the entries have recovered and they're back to where they should be. And just like what a user would experience like our user Sharon, close the app in my smartphone, reopen it, it refreshes the page, all of my movies are back. We have successfully restored resiliency in our application because of a Dynamo DB issue. We're gonna give you another scenario. This has to do with S3 specifically, and for that I'll pass it back over to Akshay. Alright, thank you, Jeff. That was a great demo. The second feature that we're gonna talk about today is the concierge chatbot, right? You would have seen Sharon interacting with a chatbot where she's asking, you know, recommendations about movies and then more details about the movies that she watched, right? So that's the chatbot using, I mean, that's built using a, a large language model and again this is a typical LLM feature with a rag data pipeline. Um, that we're all familiar with, right? It has three components. One is, um, it has a bunch of objects with movie details. These could be like text files, PDFs, or JSON files, all of that storing movie related information, and it's stored in NS3 bucket. And then there is a vector store, uh, where we've computed all the vector. Beddings from the S3 bucket data and we've stored it in S3 vectors for this example and then finally the LLM layer connects to the vectorized database, right? And um that that is what enables the LLM to answer the questions that you ask about any movie uh that's available on the app, right? Um, now, again, if the S3 data itself is lost, then it's gonna be, uh, it's gonna be, um, the, basically the vectors would be pointing nowhere, right? So, um, in that case, the final cut chatbot won't be able to talk to the vector store because the vectors themselves are pointing nowhere. And then finally, um, Sharon won't be able to get the answers that she needs from the large language model, right? So now. Let's see how the event unfolded here. You can see that um as the objects are lost, um, the LLM starts going rogue, right? You can, you can see that the LLM is not being, um, not answering the questions satisfactorily. We all know that Love Actually is not an action movie, so the LLM has basically gone rogue and the vector store is essentially useless if the uh bucket data that it's pointing to is not available, right? Now, um, let's take a look at, um, the recovery imperatives once again, right? I mean, coming back to the SRE persona that we keep talking about, right? So there are a few changes. It's again the four recovery imperatives that the SREs deal with in this scenario, but then there are a few changes, right? Um, first one is that there should be as little data loss as possible because the vector data, uh, is pointing to the S3 bucket data. Even if there's a small amount of data loss at the S3 bucket level, then the vectors would essentially be useless, right? So we don't want that situation. So, uh, the first imperative is that the last known recovery point, last known good recovery point should have all the data and should have zero data loss. Number 2, there should be as little reconfiguration required at the LLM layer as possible. Again, um, it's a complex process to rewire the LLM to the new vector data store, so that's why you need to avoid this as much as possible. And number 3, you need to keep your costs in check, right? Uh, recomputing the vectors on all of your data is a time compute and, um, cost intensive operation. You wouldn't want to undertake that kind of effort, right? And then finally, user experience matters once again. Uh, there should be little to no downtime for users like Sharon, right? She shouldn't get frustrated asking simple questions as, hey, is love actually a comedy movie or an action movie? So. Now again, um, I'm gonna show you what recovery hell looks like, right, and then we'll talk about Clumo. In the absence of Clumeo, again, what you have to do is you have to do a full restore of the S3 bucket back to a last known good copy, and then you need to recomute the vectors on the restore bucket, and you need to remove the old vector store. You need to clean up all of that data, and then you need to clean up the old bucket as well. And then finally you need to point your LLM to the new vector store because your vector store itself has changed now. Your S3 bucket was restored to a completely different bucket than it was, so the vector had, uh, the vector embeddings had to be recomputed at that point, and the S3 vector store would also be a different bucket altogether. Now the LLM has to be pointed to that, right? Now again. Let's go back to Clumo and take a look at Clumo's two-step recovery process, right? So this is possible due to Clumo backtrack for S3. We'll talk about Clumo backtrack for S3 in just a bit. But again, step number 1. You select a point in time, choose only the impacted objects. Mind you, you don't need to restore the entire bucket here. It's not all or nothing with us. You restore only the impacted pieces. You choose the impacted objects and then click restore. And step #2, there is no step 2. No need to recomput the vectors, no cleanup, no changes at the LLM layer, simple as that. All right, so we launched Clumeo Backtrack for Amazon S3 last year at reinvent, in fact, um, and Backtrack for S3 is basically the three same features packaged together. It's in place recovery so that you can recover back to the source bucket itself without having to reconfigure the application layer, vector, uh, store, or LLM, none of that, right? And granular recovery. Again, in case of S3, granularity can mean different things, right? So in S3 granularity can be at the object level, prefix level, or at the bucket level itself, right? And finally, point in time recovery. You can go back to any point in time that's available in our backup, um, using Clua backtrack. And note that Clumeo Backtrack for S3 works with or without SecureVault. SecureVault is our backup offering. Again, you may be using versioning um for S3, and that may be your idea of data protection, and we don't judge you for that. That's totally fine. You can still roll back your versions to any given point in time using Backtrack without the need to back it up with us. And finally we have continuous change tracking for S3 objects, which means that every single change that happens at the object level or at the version level is continuously tracked and we back it up, right, so that there is absolutely no data loss whatsoever. If you remember this ties back to the very first recovery imperative that the SREs deal with. All right, so now let's see this in action. I'm gonna hand it off to Jeff for a quick demo. There you go. Great stuff, actually. So I wanna acknowledge again resiliency is important, but also what we've experienced and what actually just explained is accuracy in your chatbot is also extremely important. It's very common and actually really, really easy for LLMs to spit out gibberish, to hus hallucinate, to give inaccurate information that doesn't necessarily that that can happen very easily by just having some sort of incident or event that has data loss associated with it. So let's go back to our Clumoflix vibe coded app in the AWS cloud. We're now back at customer ID 3 just like we were before. And I'm gonna give it another prompt. I'm gonna ask it to tell me more about Clumio to the, I'm sorry, it's, I'm gonna, yeah, I'm gonna ask it to tell me more about Clumeo to the moon just like we did before in the Dynamo DB example. While we wait for that to think, I'll go ahead and highlight that we have an entire part, part of the app, just like what Sharon was experiencing, that is dedicated to just a specific movie, and in this case we're highlighting our lighthearted Clumio to the moon movie. You can see there's a lot of rich, cool, interesting information about that particular movie. I could totally imagine a business wanting to do a research on a particular movie, this could be really helpful for them. Information like the plot, the revenue that it generated, movies that are similar to it, this could be very helpful for researching the next project, for example. You may have noticed that in the address bar, and this would be uh the same thing with a smartphone app. That that movie was represented by a number. This entire openly available movie database is filled with S3 object files in JSON format that are represented by a number. You can see that on the screen. I'm highlighting the actual S3 bucket that we're using for this app called Clumeo Demo Movie. Each one of the objects is a number followed by dot JSON. Just to show you the specific object for Clumo to the moon, I'll do a search for 7,000.json. That's what's connected and associated with Clumia to the moon, and highlight that we do have S3 versioning enabled. Like Akshay said, it's a great choice. Doesn't necessarily provide 100% resiliency, but it's very fantastic that AWS offers something like that. What Clumo has done is enhance the ability to leverage it at scale. I also point out that we're also using S3 tables for this, uh, providing this information and then S3 vectors as well. And again, like the Dynamo DB, the good news is we're backing this S3 bucket with Clumio. We have multiple different recovery points that we can go back to in the last month. Just like with the Dynamo DB uh recording we showed you a second ago, lots of cool, interesting information in response to the natural language prompt that we put into the chatbot about Clumia to the moon. Sharon's happy, it's a good day. Everything's good, no recovery is needed. Let's turn that around. Let's let resilience suffer in a simulated way by executing another Python script that will harm that S3 object. It'll essentially remove it. To prove that that actually occurred, and we're now in a bad day. Scenario I'll go ahead and submit the exact same prompt I gave before where I'm asking the chatbot to give me more information about Clumia to the moon while we wait for that to think, I'll refresh the actual page would be which would be representative of where you would go in the app to find it. The movie is gone. In that, and so now we're creating a scenario like we did in the video with Sharon where she's frustrated, she's going to social media and complaining. I'll refresh my S3 bucket, the 7000. JSON file completely gone. There's no matches found, and that response that I that that I'm now getting from the exact same natural language prompt to the AI and movie assistant is different. It's inaccurate. It's shorter. It's not as interesting, not as helpful. It even says at the bottom, unfortunately I don't have all the information. Like Akshay said, the pipeline's broken. It's now no longer able to access that rich, interesting information from the dotchason object in that S3 bucket. Let's do like what we did with uh Dynamo DB. Let's use Clumeo Backtrack for S3 to bring the resiliency back. But again, just like with Dynamo DB, we also have a modality for S3 to recover from Secure Vault. Like Akhay mentioned, that's our immutable air gapped copy. So if instead we were simulating a cyber-related event, this is what I would recommend to, to my customers and companies is to recover from that immutable. Air gapped cyber resilient copy of the object. In order to do that, I just need to search for the 7000. JSON in my example. I don't have to do that, but in my example, I just need that object. It's querying a backup from the point in time that I selected. It found the 7000. JSON object. I now have the option to recover just the latest version or all versions. Remember this object in this bucket has versioning enabled. There's a lot of benefits to being able to do that from a cost and ease point of view. And like the Dynamo DB examples, I'm given a lot of really advanced selection options because I may wanna customize this restoration. I can absolutely see a scenario, especially in a cyber related event, where I don't wanna recover to the same AWS account. If the object in that AWS account had been compromised, it's very likely that other parts of that AWS account are also compromised. We give you the ability to recover to a completely different AWS account, maybe even one that you've created brand new and that is completely clean. We also give you other options like recovering to another region. Maybe there's a large scale event occurring and you need to go to a different region, and we even give you the ability to, to add tags to the object or, uh, store it in a prefix, an S3 prefix, very advanced selection options to make sure that your particular needs are met for the specific and particular recovery scenario that you have. But instead, what I want to demonstrate is what Akshay mentioned and what we announced at AWS reinvent last year, and that's S3 backtrack for uh backtrack for S3. I'm given the bucket name. I'm given the point in time. I'm choosing the point in time. I apologize, my mistake. I'm just gonna go ahead and choose to pick an hour that's earlier in the same day. I'm gonna filter what I see by the 7000. JSON object when I press preview. It's going to query the object, I'm sorry, the S3 versions at that particular point in time. It found the the 7000. JSON for that specific point in time, just like I showed you before, lots of interesting selectivity options to customize. And be very specific to your recovery needs. In my example, I'm just gonna override the source. I'm gonna override the most current object. I've started a new task in Clumeo to get that done through the magic of video editing. We're gonna fast forward to when it's completed and to prove that it's been completed successfully, I'll go back to the AI movie assistant, provide the exact same natural language prompt to tell me more about Clumia to the moon. While it's thinking We'll go ahead and go back to the movie page for Clumio to the moon. Just like I would if I was to close and open the app, I'm refreshing it. It's there. The data is there. I'll refresh the S3 bucket. The 7000. JSON has returned. I'll go back to my AI movie assistant, and I'm back to a happy day. Sharon's happy. She's getting the exact same consistent response with the rich metadata and rich, interesting information about Clumia to the Moon movie. We've successfully restored resiliency in our, uh, cloud application. We have one more scenario to cover with you today. This is specific, and I'm actually very, very excited about this one to Apache Iceberg and our new support for that. For that, I'll turn it back over to Akshay. Thank you, Jeff. Another great demo. The third and the final feature that we're gonna talk about today is the box office analyzer. Again, this is a feature that allows you to add movie budgets and the revenue performance. This can actually be interesting for any Hollywood exec out there, right? The data layer for this feature is basically a data lake house that's powered by Apache Iceberg, right? Iceberg has gotten really popular over time and it's the most preferred open table format for data lake houses. And uh for this specific use case we are using S3 tables to store the Apache Iceberg data. Right? And not only is this available as an external feature for uh users like Sharon, but also, um, internal employees of the company can also use the, the same data set to create dashboards internally to analyze, um, the, the, um, movie financial performance, right? All right, so, um, Iceberg data is actually vulnerable to rights, right? We all know this. Whenever you run an overwrite query that changes the schema or drops a column, for example, your visualizations don't, don't really match the schema, right? And that mismatch causes all of your visualizations to go empty, right? And in this case what has happened is all of the um data in the table, all of the genres for the movies are overwritten to comedy, right? And that's that's what we're gonna talk about here, um, and because that final cut is not able to pull the information satisfactorily and give accurate, um, responses on the visualizations, right? Um, and this would also impact any dashboards that have been created internally by the employees outside of the application, right? So we'll, we'll fix both of those issues, um, in this scenario. OK, let's take a look at the event. Uh, let's see what unfolded, right, that led to the situation. Uh, we are using Athena query here to pull the data from S3 tables, and we're using Quicksight as our dashboarding tool for this example. And as you, um, notice here, as the, you know, iceberg tables are getting impacted, the dashboarding. Tool is malfunctioning as well, right? So the uh dashboards are, uh, showing wrong information, which should not be the case. And let's talk about the hard way to recover from such a scenario, right? But before that, let's take a look at some of the recovery imperatives once again. Um, we see a constant theme here with the SREs dealing with the same recovery imperatives over and over again. Uh, but there are a few things that you need to keep in mind here, right? With Iceberg data, the known good recovery points are actually snapshots or point in time. It cannot be, you know, a single source of truth, right? It, I mean, uh, you can have the data admin who's well versed with iceberg snapshots, and they may know the snapshot ID. And the um S3 admin, for example, may know um the point in time where the data was cleaned, right? So it could be either or. So you need to be mindful of both of those situations and deal with that, right? And number 2, there should be absolutely no need to recreate dashboards or reconfigure the application whatsoever, right? So all of the um recovery should happen in place. And finally, of course, um, we know that PCO and time are absolutely essential to keep in mind. And with that, let's take a look at recovery hell, right? It's a long process. If you're backing up only the S3 data. That's powering uh the iceberg tables, right? So in this case, um, let's assume that you're, um, backing up only the general purpose S3 bucket data under the hood because there is no other iceberg aware data protection offering out there in the market today, right? So you have to, you know, back up only the, uh, S3 data that's available, uh, to you. And first, you need to do a full restore of every single S3 bucket that's holding the iceberg data. Right? And then you need to restore the iceberg table structure once the full restore completes. This would mean you have to reconfigure the manifest files, metadata files, and the data files to preserve or bring back the iceberg table structure. And you need to point the application to the new tables once you've restored it and then finally reconfigure the application. To talk to this new table that you've restored and finally you need to reconfigure the application and recreate all the dashboards that you would have created with the uh previous data set, right? And that's what recovery hell looks like again. Let's take a, uh, let's take a look at the two-step recovery process with Clumeo. Again, Clumeo is the only, um, offering out there that's, that's, uh, that provides iceberg aware data protection. Um, and with that, step number one is the backup is absolute backup and restores are iceberg aware in nature, which means that you don't need to uh restructure the table once the data has been recovered. So you just pick a snapshot. You pick a point in time and then you click on recover, and this recovery happens in place, so you don't need to deal with any sort of upstream impact. And step number 2, there is no step 2. As with all of our other recovery modalities that we covered, right, the application doesn't need to be reconfigured. The dashboards don't need to be recon uh recreated, and none of that, right? It's simple recovery, um, just a single step. Now I can say that it's not a two-step recovery anymore. It's just one step recovery. All right, so we launched the industry's first icebergwa data protection offering back in September. Let's learn a bit more about that, right? The iceberg table structure is preserved both during backup as well as recovery so that you don't have to restructure the table once the table has been recovered. Or you don't have to do anything under the hood to, uh, preserve the table structure even during backup, right? And you can, um, you can back it up and recover it, um, without any hassles and without, in a, in a very transactionally consistent way. And the data itself is stored outside of your enterprise security sphere as with all of our other secure world offerings, which means that your data is safe, air gapped, and immutable, and it can actually help you recover from any sort of threat vectors um that that we talked about earlier in the session. And then finally you can retain as many snapshots as possible with Clumia Secureault, and this is especially useful if you need to be compliant with compliance standard standards like FINRA or HIPAA that require you to, you know, store every single transaction for about 7 to 8 years. Right. And finally, another thing that you can do with Plumeo secure word for Apache Iceberg is that you can modernize your, uh, iceberg footprint on AWS. You can actively migrate your tables from, uh, general purpose S3 bucket to the newly launched Amazon S3 tables feature. All right, so Enough about the feature, now let's see it in action. I'm gonna hand it off to Jeff for 11 last demo. Thank you, Akshay. Like I mentioned before, actually I started speaking, this is, I'm very excited about this. You heard it correctly from Aksha. Clumio is the first Iceberg and S3 tables aware data protection solution in the AWS cloud. Uh, as many of you probably know, Iceberg is extremely popular with us, and when combined with S3N and, and various different data pipelines, it's so popular. AWS themselves came out with a new bucket type S3 tables at this time last year at AWS Reinvent 2024. So we're very happy to be helpful and being able to participate in increasing the resiliency of those different services. Let me, uh, let me show you another demo. This is our last demo, and what you're seeing on the screen is what we call the movie insights page. So think of this as, you know, sharing using the app and going to a different part of the app again to probably do some research, but I can also imagine businesses using something like this to do research on maybe what movie they wanna make next. They wanna find out that out of the several 1000 movies that are in our openly available data set. Which ones were the most successful, which ones made the most money, maybe which ones had the, the worst return on investment. Like Akshay said in the app, we built this with a pipeline that included Quicksight to show this chart information in a visual way. I wanna highlight this chart at the bottom, revenue by genre, that's what we're gonna demonstrate as failing today. And as you can imagine, that's some really cool data that you wanna keep resilient to keep your users and maybe even the business users of your applications happy. It's broken out by $1 per genre, and you can see that it's very diverse. There's a lot of genres listed there. I'll go ahead and highlight that we do have an S3 table powering this, and that S3 table is in the Apache iceberg format. I have it highlighted here in the AWS console, and just like with S3 and Dynamo DB, we're also taking backups of this S3 table, and we have been for some time now. We have multiple different places and point in time, points in time to go back to should we need to. It's a good day. It's a good thing. Let's go back to our AI movie assistant chatbot, but this time I'm gonna give a prompt to to have it tell me the specific genre for Clumio to the moon. I no longer need to know the information about the movie. I just wanna know what genre it is. While it's thinking. Well, I'll go ahead and explain how this is querying that pipeline that I actually mentioned and explained a second ago. It has successfully responded with the correct genre. It is action and adventure. That is the genre of Clue Me to the moon. Again, another scenario where Sharon is happy or your business users are happy of your app. Let's make it a not so good day. Let's make it a day where something bad happens, resiliency has suffered. We need to go back to these imperatives that Akshay keeps mentioning to ensure that we increase resiliency or return back to a resilient point in time for this application. I'm gonna simulate that with our Python script again, but this time instead of actually destroying data, I'm gonna change the data. I'm gonna change all the genres in our uh uh study table to be comedy. When I go back to the movies insight page, and I refresh that, instead of the data being gone, the data is incorrect. I'll go back down to that chart from Quicksight that showed the revenue by genre. Now all $400 billion plus dollars in revenue is comedy, and all the top movies of revenue are in the genre of comedy. This is obviously not correct. This is bad information. You're this is creating a negative user experience. Like, let's make it worse. Let's do a second suffering of the application by going back and doing what we did in the previous video and completely deleting the 7000. JSON object again. I'll go ahead and simulate the the negative experience that your customers or your businesses might have in this app, by giving the exact same natural language prompt to show me the genre of Clumio to the moon, and in this case, after it's done thinking, it'll respond with incorrect information. It now thinks that Klumio to the Moon is a comedy movie. We all know that that's not true. We've looked at the data for that movie very regularly in these demos, so recovery needs to be performed. Resiliency has suffered. Again, like before, I have multiple different recovery modalities. The first one I'm gonna show you is again that secure vault, immutable air gapped cyber resilient copy of the data. But in this case, a nice little added benefit actually touched on it a second ago. Not only can we recover from an immutable air gapped copy, but we can also convert the, the catalog type from a, uh, AWS glue catalog type to S3 tables catalog type, a nice little added, almost migration related benefit to the functionality that Clumia provides with the resiliency that we're showing you today. But for my demonstration instead, I'm gonna go back to a very specific point in time. Using the built-in native Apache iceberg snapshotting, I'm gonna go back to October. I'm gonna pick October 30th just because I know that's the point in time I wanna go back to. It's successfully found a snapshot from that point in time. Again, just like before, I can even give the option to convert from an AWS glue catalog type to an S3 table catalog type during this recovery. I've executed the restore. A new task has begun, but remember, I have, I have multiple things I need to recover from in this example, not just one like the previous examples. I need to go back to Clumia Backtrack for S3 and do what I did in the previous video where I recover back to a previous version of the 7000. JSON object. Just like before, I pick a point in time. I do a filter for the 7000. JSON object. It's going to query the versions for that bucket. If there's one that exists, it'll come back with a result showing that there is a point in time for that particular object that I can go back to. It successfully found it again. I have additional options, but in this case I'm just gonna override the existing object, execute the restore task. Now I have 2 restore tasks running simultaneously. Through the magic of video editing again, we're gonna fast forward to when those tasks have been completed. I'll go back to that movie's insights page, refresh it, which would be just like a, a user or a business closing the app in my smartphone and reopening it. Go back to that same chart for revenue by genre, everything's back to normal. I've successfully restored the resiliency for my app from in this case, a multiple failure scenario. Again, just like before, ask the AI movie assistant the same prompt I asked before. I want it to tell me the genre for Klumio to the moon. As long as everything was restored correctly and successfully, it should respond with the correct and accurate information, no longer providing a negative user experience for Sharon or for the business, and it has. It's now showing Klumio to the moon back to what it was, which was an action adventure genre movie. We're in a happy place. Sharon's a happy user. Your businesses are your customers are happy users. We're in a good, a good spot. Let's look at Sharon again, our trusty user for our smartphone app. She's now, uh, looking at it again. After we've done our restorations. Her expectations are high, which she should have high expectations because AWS has set the bar so high for this type of stuff. She does the same search, she found a great movie she wants to watch and she started watching it, eating her popcorn with a smile on her face. We're back to a resilience, a resilient state for our smartphone app, our cloud-based app. Now that we've gone through all of these scenarios and we've explained to you how Clumeo can help increase your resiliency and why it's important to do so in the cloud using these Gen AI pipeline based cloud-based apps. We want to leave you with a and summarize a few pieces of recommendations that we have. The first is what we think are requirements for when you design your data strategies, data backup and protection strategies, and maximize your potential for resilience, eliminate the risk of those three examples I gave at the beginning of the presentation. I'll start with the first one. You need to protect your entire data pipeline. The Clumo Flick app, what Sharon was using was simple. Like I said, our CTO vibe coded it in a weekend. Most likely you have applications running in the cloud that are more complex, that are multifaceted, that have detailed pipelines that involve maybe a dozen or a dozen more services than AWS. It's very challenging to be able to recover from something that occurs to those services in that pipeline if you don't have, don't cover the entire pipeline all at once like Clumio does. That's what we recommend as the first requirement for architecting for resiliency in the AI world that we live in today. The second is speed. Sharon expects fast responses from that smartphone app. AWBS has been around for a long time. They've done a great job at raising that bar to make sure that customers have speed in what they're doing in the AWS cloud. Klumo recognizes that, and so we have built an architecture that is built for that so that you can have a good user experience as often as you possibly can. Lastly, scale, we recognize that our app was very small. The watch lists were only about 20 Dynamo DB entries. We had about 5000 or so S3 objects. We only had 1 S3 table. We only only had 1 S3 vector. It's odds are your, your, you're a magnitude's order larger than that. So Clumo has built their product and their application and their solution along with Commvault to keep pace with that scale. We know that it's dynamically changing every single day. We know that you're adding and subtracting every single day. You need a solution to increase that resiliency and maintain that resiliency that will scale. Those are our 3 recommendations for architecting, for best practices for the G AI world that we live in today. But more generally speaking, we also, Clumeo is an 8+ year old company, we've come to know what best practices mean for the cloud. So we want to provide you some guidance in 3 different particular areas today before we leave you. The first is recovery in place matters. Not having to go to the work and the effort to reconfigure your app after you only recover partially will slow things down and make your resiliency suffer. We wanna accelerate that availability of that data so thats Sharon has minimal interruption or your businesses have minimal interruption. The second is, like I mentioned a second ago, the cloud is dynamic. I've been working on it for about 6 years now. I was at AWS. I observed it firsthand. It changes every single day. It shrinks. It grows. So we wanna be able to help you with discovering what already exists now, but also what has not been yet created. We have a part of the product called Discover that will help automate that protection, that level of protection. And then lastly, like I keep saying, scale is huge in AWS. I don't see that slowing down. If anything, that'll accelerate in the next 1520 years that AW plus that AWS exists. So you need a product that will be elastic and scale with you. Clumo is a 100% serverless architecture that is built for scale. So that is our, that is our third best practice that we wanna provide to you today. I really enjoyed speaking with all of you on behalf of Akshay and myself. We don't want this conversation to end. We want it to continue even when you walk out those doors. So we wanna offer a few other things that we're doing before Reinvent ends this week. The first is an AWS game day that starts at one o'clock tomorrow. It's gonna be a little bit of fun. It's gonna be a little bit of sci-fi. It's we're calling it Race to recover, but it's based on a zombie apocalypse or apocalypse is I guess how we're pronouncing it on the screen there. You'd see the QR code. I'll be there if you wanna, uh, talk to me more there or after the session. I'm available 100%. I'll be there like I said at 1 o'clock tomorrow and then right after game day we're gonna do a happy hour just before replay just so that you can have some uh relaxation and comfort and enjoyment before going having some fun at the concert. That's all for me today. I really enjoyed speaking with you. Like I said, I'm gonna go ahead and turn it back over to Akshay to wrap us up. Thank you, Jeff. Um, thank you so much for joining us today. If you wanna learn more about everything that we talked about, number one, you can sign up for a free trial on 8. Marketplace, it's really simple. It's a 14 day free trial. And then if you wanna learn more about Plumeo Backtrack for Dynamo EB, you can, uh, scan the QR code to access the blog. And then if you wanna learn more about Pluo for Apache Iceberg, you can use the 3rd QR code here, and it's gonna tell you all that you need to know about it. Thank you so much for attending our session and have a wonderful day, and Jeff and I will be here for a few more minutes to answer any questions that you may have. Thank you.