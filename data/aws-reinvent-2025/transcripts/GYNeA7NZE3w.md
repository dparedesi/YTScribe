---
video_id: GYNeA7NZE3w
video_url: https://www.youtube.com/watch?v=GYNeA7NZE3w
is_generated: False
is_translatable: True
---

Uh, welcome everybody. Uh, my name is Philip. I'm a global solution architect, uh, working for our financial customers, and next to me is Tivida. He's one of the builders, uh, behind the platform we are going to talk about, uh, in this, uh, session. Um, Thiboda is from Allianz Technology, and I will start with a small introduction about what the actual challenges, uh, we currently see when building, um, AI and G AI based um platforms at scale. And when I'm done with this, I will hand over to Tivida who will give a brief introduction of who Allianz actually is and then walk you through how we tackle some of these challenges um at Allianz. So when we look back over the last, let's say 5 to 8 years, um, AIML has ever been, I mean it's, it's always been a hard problem, right? Was came always along with a lot of heavy lifting. But the point is the capabilities we had to provide to the actual users, they were pretty stable, right? So we always knew, OK, like my folk, they need they need a place to train models, they need something for experimentation, monitoring interference, right? Although it was complex, um, it was stable, and I mean for that heavy lifting we invented things like Sagemer AI to make this simple, but. If we then look back like the last 2 years, there was an explosion of capabilities and along with that of complexity when Gen AI entered the stage, right? And I mean this is just an illustration, it's not, uh, don't name me on this, it's not a 100% complete list of all the capabilities you will need to provide such platforms. But I think what you can easily see is that there are a lot of new capabilities, and the important part here is these capabilities, they're just stacking up to our platform, right? They're not replacing what has been there. It's a simple addition of new stuff we have to take care of. And if we dive deeper into this problem and uh look at the thing from the application side of things, we'll find that um the complexity on the application side is even more complex, because for each of these capabilities, um, people are creating frameworks to help us to abstract these things a bit. But the point is, again, there is not a single framework or the single solution which solves all of these problems, and indeed it's more of the opposite actually that all of these frameworks are pretty good and you might want to use all of them because all of them solve a certain problem for you might be doing this better than the other framework for this specific. Problem and I promise you if your company is large enough, you will most probably find all of them somewhere and this is just an example of a genetic framework. Same holds true for let's say memory reck whatnot, right? So trillions of these frameworks. If we change perspective a bit, um, what we can also see, which is in fact a good thing is that GEI democratized a lot. Um, who can work with, uh, with AI these days, right? Like in the old days, old days is funny because it's actually 2 years back, right? We had a very strict and narrow scope, um, who was building and working with AI, right? So we had like our researchers, their scientists, I mean they'd even. Degrees or PhDs in what they were doing so we could assume they pretty well know what they are doing right and then we had some ML engineers who were building the infrastructure around the platform so it's a very well defined world if you will, right. Nowadays, nowadays, I mean these people still exist, good, but like if we look on the bottom level, we find a lot of more and different and um highly diverse user groups, right? We have now business users using like visual orchestration tools and starting building agents all over the place. um we have people who are interacting with chatbots, all these things, so the user groups are highly diverse and they want to build AI agents, right? Problem is. Given their, their, their diverse skill sets and educational background, um. They need support in different stages in this or over this process. So if we take all of these things into account like the proliferation when it comes to capabilities, the amount of frameworks, the different user groups, uh we need to build our platforms for, The question is like how can we provide a platform, an operating model for these users that on the one hand side does not lock them in by being prescriptive and saying, OK, from today on you use this framework to build an agentic application or a GEI based application. But on the other hand side, identify, let's say the standards, these stable elements you can provide as part of the platform. To first standardize your operations and your processes and on the other hand side, use these capabilities, these stable elements um to to scale, um to to accelerate the adoption of AI within your company and to say it more in an architect's language. So what we actually want to do is identify the stable elements, make them part of our platform, and decouple them from these fast moving elements so that these fast moving elements do not, do not force us to rebuild, rebuild, refactor, refector again and again. And by this, handing over to Tivoda, we'll now start with the introduction of Allianz. Thank you very much, Philip. This is really great to be here. When I was a child, I have seen a really great movie. Casino was the title, with Robert De Niro, with Joe Pesci, and I am here in this city. Unbelievable. Yesterday I was in the desert. I really would like to tell. I really would like to share what I was thinking in the desert, but first, who we are, what we are doing, what our purpose. We are alliance. We have a really, really strong numbers. We are stronger than ever. But Behind it. We have a lot of people. And for such a developer like me, It has a meaning Two years ago I visited Croatia and I was able to see the Allianz office and I talked immediately. I am at home. It happened also in Greece in the summer, and that mics demeaning. 435 years, 4 135 years we are here, and how we managed it, we always realized how important to change. And we are changing and we are still here stronger than ever. I belong to the Allianz technology. And here we can see also really, really strong figures, but behind all those figures we have a lot of stories. For example, we have a really strong feedback culture. If I would have, if I have the one on one with my chef, with my boss, the first question always, what can I do for you? How can I support you? And in my life I have never seen so much PhD in one place, so 1000 engineers, and we have just one purpose day by day. We would like to do it better and better. We are full of ideas, but we are in the people industry. We are doing insurance. I am sure you already heard Allianz. But you would think insurance is boring. Let me prove this is not true, but we are doing the people industry, not just for external customers. We are also doing for us, for internal customers. A couple of months ago we were developing with one of our internal customers the new tool. We were really, really excited and he thought to me. A couple of years ago I was really, really sick. I was fighting for my life. I was really confused. I really didn't know what should I do, and I was already worked at Allianz at this time, and Allianz came to me and Allianz said, You really need to put focus on your fight. You really need. You really do not need to think about your position, your salary, your living. We are behind you. We are supporting you. And he won. He survived, and we were working at 8 p.m. and he told me the only thing that I can do right now, I am trying to give something back. We have the best continent in the world. Then if you ever have a chance to visit us, please do it. Best service, best food, and we had the tribe review in this building, and I was standing next to this building. And I think I belong in this company. This makes a difference. Really important, I already mentioned, we need to change. We need to move, and we realized, and we already started it. We have vision. We have different groups, and with those different groups, we would like to support all those changes. We would like to bring the AI everywhere. We would like to bring to support the process, support the development, support every aspect. We have a target. We would like to achieve this. Until 27 and we already started. We are not doing it from scratch. We did the first step, how we would like to achieve it. We would like to provide training. We would like to provide documentation. We would like to provide tool that you will be able, you could use out of the box. First one, Jenny, I love unbelievably good. You can imagine you have immediately access into one URL and this URL, each employee which belongs to the technology has access to it. You can choose which model you would like to use. This is not just technological people. This is also for the business. You would need to translate something. You can do this. You would like to compare a couple of things. You can do this. But it can provide more, not just for the technology, and we have this out of the box, but we have another aspect also agents. We have hundreds of agents with it. We can make it easier the onboarding. We really do not like to read documentation. We would like to get immediately the results. The journey I love can bring us. And we organized another group, Gen AI Focus group. With Gen AI Focus group, we can bring to the people the coding assistant, the coding assistant such as Amazon Q, really, really great tool. This is the lower hanging fruit. You would like to accelerate, if you would like to accelerate the speed of developing, you would like to achieve. Higher efficiency, then the coding assistant would be the first step, the easiest way, and our focus group providing really important because we need to measure it. How does this look like we need to measure how many users are using adaptation. The coding assistant, we need to check the productivity. How does this look like, what we achieved with the productivity. We can make interviews. We can check it, and we need also to check the enablement. How many users we trained. We are also providing training for them. We have champions, really great group. You can imagine we represent different teams. We represent approximately 200 people, but from 200 people we provided. 1216, 19 people, but they have the background. They know what their team is doing, and we can provide a forum and in this forum we can show new technology, new process. And yet now I would like to talk about allianz in Germany. We have our tribe where my team or team also belongs to it. This tribe is Vandia, Data Insight Analytic AI. And this tribe for data. The data without process, without extraction, has really not a lot of meaning. We need to process the data and for it we have the tribe, a really strong tribe. And here we can provide for our customer platform as a service. We can provide data as a service, data product. We can provide AI related things, for example, data science workbench. We can provide training. You can imagine you are the use case. You are such a use case team and you have a lot of ideas and you would like to bring your ideas in the production. You can come to us. You can ask us, and we can provide the solution for your ideas, and we did it many times. We have a really, really strong relationship with AWS, and the AWS has really strong tools. And we provided the platform for our customers to bring everything into the production, and my peos, my lead architect, they have a really strong motto do it one time, make the documentation, and after it reuse it, repeat it, and we did it many times. I would like to talk about Data Science Blanch 2.0 because as we have seen in this picture, this is already the production, but we also need a place where our developers are able to develop things. And this is the place that the science workbench is based on Sagemaker, a really, really strong tool. With Sagemaker, we are able to provide instances for our customers. For instance, with they can use model training. They can use AI related things. Machine learning and this is decentralized. That means each team can make their own boarding and can start immediately. It's really, really fast. It requires 2 days, you know. We know the feeling. We would like to develop something and we would like to put everything together. It takes days, weeks, months until you will be able to install all the drivers, until you will have connectivity in the infrastructure with the databases, but we have here everything out of the box and really easy to handle, really easy to get it, and really strong. We have different personas because I already mentioned we really do not want only provide solutions for technical people. We also would like to provide solutions for not technical people, for business analysts, because they can better understand the business and for different, different persons we have different solutions. You are an engineer. You can take sage maker. You are a data scientist. You can use Bedrock. This is also available out of the box. Or you are the business. Analyst, you can use Sagemaker canvas, such a strong tool. You really do not need really strong knowledge. You can use drag and drop. Really great And How you can imagine these things we can provide here Jupiter Lab for Envi for developers. They would like to use Python. Jupiter Lab best things you would like to use coding assistant. We also have coding assistant possibility in code editor. You have knowledge about air, our languages. Talk about machine learning. We have two possibilities. One possibility is Python. Another really big possibility would be air, and we can provide it. We really do not say sorry, you need to learn Python. We have Air studio and we can provide it for our customers. And we have CAS, already mentioned, a really strong tool. But we have another solution. But why we are doing these things? Why we have all those things? Because we would like to be more efficient. Amazon tax. The measurement is really easy. We have a really long history for company, and we can measure it. How many times did it take to process 1000 documents? How many hours behind it, and we can measure it. We can compare it. How many times does it take with text track? How many times does it take with Pedrock? And we did it and we already brought a lot of things into the production. We have access to secret manager. You know it. You need to open a ticket in the normal case. You need to go to the support, but we can provide direct access to the secret manager. They are really small pieces, but all those small pieces make difference. And it's already happened. Our customers, they started to develop something here in the data science workbench, and after it, we were able to bring into the production. We provided for, for the whole thing. Governance, we were able to provide pipeline, frame, infrastructure. And it's worked and this is still working. I prepared some demo because I just would like to show you how this whole process looks like, but really these are who does the magic or the customer. It is not the same as what they are doing, but maybe with it you will be able to see how much possibility do we have with sage maker. And first, we have a lot. A lot of forms. This form could be such a doctoral form or such things, and we need to process and we can use text for it. And after that we have the data, but we also need to process the data. We can use bedrock for it, and after it we can make the machine learning. Let me start the demo. This is the code, but I wrote for it. And I already started. The first step would be tax tracker, and with Tatractor we will be able to extract the information, the data from, from PDF. And after it, after we finished, we will be able to see immediately the result, a really strong future, and after it. Maybe our customers, they really do not understand German. They would like to use English. It's also possible we can also do it. Send to Bedrock, and the bedrock will be able to translate from German into English, already happening. But after it, we would like to somehow summarize all those things. The bedrock can also provide for us, and this is already happening. I created the CSV. We can see the CSV here and based on all those information, we can do the machine learning and the business analysts, they can do their job with it. Also really important about the lesson learned. Because We are working in the enterprise scale, and into the enterprise scale belongs the need to do everything. Infrastructure is a code. We need to use 4 eyes concept. We need to follow what happened, why it happened, and I struggled about one of the pieces. I was working. I just wanted to write the terraform code for the code editor, and at the end of the day. New release happened in the terraphone side and it solved immediately a problem. This isn't the product. This is a process which developed, which is moving always, really important, the governance point of view, because we are in the enterprise scale and we need to save our customers. We need to save the data, custom image. We are working with custom image because we need to put What certificate we need to put Or proxies inside the custom image and the scalability is also really important because we need to provide a solution for 20 people and we also need to provide a solution for 200 people or 300 people. This is really great. We discovered a pattern. Now I just would like to talk about different pieces. In the end, all the pieces are coming together. We discovered the pattern. Or platform engineers. They are doing the infrastructures. They are doing the governance. They are doing the pipeline. But for business users, they are doing the prompt. They are closer into the business. They can better understand the business, but we need to provide for them such a tool where they will be able to use it, that they will be able to use the evaluation, that they will be able to check. They can iterate their prompts. We already started 3 years ago. We already started the work with. We know that most of the time we are going and we are changing the prompt again, again, again. We are fine tuning the prompt and our business users, they would like to also do the same, and they need the platform. They need the portal just to do this. Second, and we can provide the platform for them. We can provide a really strong platform. And we also need to make the evaluation in this platform. And for our technical teams, we need to provide the pie test, the integration, and it is crucial, we combined the prompt management with evaluation. But how we can do these things. We have ideas. We have prompts, and we would like to somehow scare what happened in the prompt, how we can do all those things. The first We put all our ideas into the box. Maybe we can try to use database for prompts. We tried it out, it cost a lot. It requires extra database knowledge. OK, then we put what we would need for this purpose. We need versioning. OK. We need traceability. We need tag. We need branch. And after we put all those pieces together, we realized the geese already know all those things. We really do not want to reinvent the well. We would like to use what is available already in the market. We have versioning. We can check what happened 2 or 3 years ago. We can create another branch for it. We can do all those things already. Is already provided and this is crucial. The GI is a low cost standard. It does not cost a lot and all our developers all know how can we manage it, what we need to do, but we know it. As a developer, but or business user, they really do not know this. But we have found the solution for it. The automated. They can only see the visual interface. They can only see this. But what is happening in the background it is already automated. If they would like to save in this visual interface, then this will automatically trigger the Git commit. This will automatically trigger. The git, the pull request and everything git push, push, everything which belongs to it and with it they will be able to work and they will have such an interface, such a place that they will be able to evaluate immediately and they can do this job and they can go back and everything is automated. AI ops, really good things. But how can we bring all those pieces together? What will be the step about all those pieces? We recognize these aren't problems to solve, but tension to manage or solution dual pathway. The first, data science workbench. I already thought about it, really easy to get. You can start, you can test, you can use it. The second would be the flexible, the self lane. You can do your job there. In traditional IT in enterprise scale, we already realized if we are getting bigger, larger, then it takes more time to move in any direction because you can imagine tanker ship we heard already in. Another aspect, the small IT companies can react really, really. Really, really fast, but somehow we also need to find a way where we can also react. We can move faster. The goal isn't to eliminate the security aspect because we have more than 100 million users and we need to save our data. We need to protect our customers, but somehow we also need to find a way where we can react faster. We have happy way, happy happy path. This will be not a science work branch, but we have another possibility stage governance. You can imagine such a funeral. It's really, really wide at the top. You can go to a science workbench. You can test it out. You can make your experience there. You have immediate access to bedrock, texttractor. You can do. AI related stuff you can build LLM. You can train it, but All our things which are going into the production, all those things need to meet with security standards. Absolutely important because we need to save our customers. We need to secure our customers, and all those things need to meet with compliance and such things if they would like to go into the production. Selective standardization. The platform team, we are going to create the infrastructure. We are going to create. The pipeline for it. And all those things standardized, but it's really important to give them the flexibility at the application layer. And they can use what they would like to use it, but we are protecting them. And the next one. We are not forcing everyone. Into the same box, into the one box, we are providing a platform where they will be able to use what they would like to use it. First step, going to the science workbench. Happy, happy way. They can start fully integrated. They have connection everywhere. Second, self-managed team. We have defined clear handover standards. They can put their application into the container. After that we can use open telemetry because we need to check, we need to keep an eye about the cost, what is happening there. We need to trace it. We need to follow it and after that we can do they can do the prompt already talked about it. And it You are not really doing from scratch because behind it you can imagine you would like to bring something to production as a use case we can provide the agent for you. We already created the repository where all our template infrastructure as a code is existing and you can talk with this agent. Hello, I would like to meet London. I can do. I am going to do immediately. I will need bedrock. Then this agent is going into this repository and grab the template and provide it, and we can automate a lot of things, but at the end we have crystal clear handover. Because we need to use our standards. We need to use security standards, and what we can win it, what is the main purpose for developers, they can react faster. They can move faster, but we are also saving the environment or environment. Are We also need to talk about decoupling, maximize flexibility. The first would be prompt from application already mentioned. They can use, they have a portal. They can create infrastructure, and they can do their own. The second one, really important, open telemetry, trace, follow what is going to happen. The second one, run time. They can use for a long running job. They can use for event driving lambda for orchestration. They can use EK up to them what they would like to use for it. And we can provide this business interface and they can work for it. We already have it more than 20 years ago, and this will be also here 10 years later, 20 years later. This will outlive a lot of things, and we have open telemetry. Why we don't use it. We have containers, really strong, really great solution. Do you still think insurance is boring? I don't think so. Yesterday I was in the desert. And do you know what I was thinking? I was thinking this is amazing. Thank you very much for listening. Thank you very much, Allianz. Thank you very much, AWS. Thanks a lot, Philip. I am handing over back to you. Yeah, thank you Thivada um, to sum it up a bit, the key takeaways of, uh, what we wanted to show you, um, is first of all build on what we call low regret standards, right? This what I mean with low regret standard is not necessarily. That they cost you less money. It's more like standards that are already anyhow in place in your organization which do not require, let's say, a lot of change to get installed and build on them because they're there for like 20 years. They're stable, they won't change. And basically we we could have done a talk about MCPA2A and all this stuff, but the idea was to like show you a bit the standards which are there which aren't maybe not super obvious like like open telemetry, and these are things which are worth it um to build on. I mean Tivoda had showed you how you can use Git to decouple basically the prompt management, the UI from the actual application, and in this way make sure to create portable AI applications where you can then decide on your runtime. We've decoupled the let's say the code, the runtime by open telemetry. Don't forget open telemetry is more than just tracing. There is a GAI standard built into open telemetry. You can look this up and And check it provides you things like the Amounts of tokens consumed, right? The LLM you're using, you can use all these details to build a lot of governance um around your LLM just by building on these standards, right? Um, Another super simple obvious standards, right, juice containers to, to, to encapsulate these fast moving fast changing chain um sorry changes uh frameworks right from your actual run time, um, super important, um, and this is also important. Don't treat like the the streamlining of the operation as a platform capability in that way and what I mean by this, if you remember the the picture from from the beginning, right on the right hand side everything is like standardized by decoupling the actual application with the low regret standards and then we can apply the same operating model no matter which tool we are using to build or create. these AI applications. And don't forget the what we call this dual pathway. This is also important, so we provide two entries into the system, like two ways to get into the system like this fully automated thing which means, OK, like we have a pattern. We found a pattern, right? It works well, so then we invest into, OK, this thing now becomes kind of a standard we build a pattern which is then later reusable for our teams um and it will become part of this of this fast um heavy pass right on the top, but. We also give the people the freedom to experiment, to play with with new technology, as long as they comply with these lightweight low regret standards, right? Give us our prompt in the GitHub repository. We don't care which which prompt management tool you were using to create these prompts in the beginning, right? Same holds true for the framework. Same holds true for for the runtime. Yeah, and by that, uh, also a big thank you for uh from my side, and uh if you wanna discuss or have uh some more questions, uh happy to meet us outside. Thank you very much.