---
video_id: MMdXKVcSH-o
video_url: https://www.youtube.com/watch?v=MMdXKVcSH-o
is_generated: False
is_translatable: True
---

All right. Make sure everyone can hear me. If you can't hear me, raise your hand. Awesome. All righty. My name is Ken Beer. I'm the director of crypto cryptography at AWS. Uh, I've been in AWS for a little over 13 years. Helped launch the key management service back in 2012 here at Reinvent. And now owns sort of the full stack from open source cryptographic libraries and algorithm implementations up to value-added services like KMS, cloud HSM, AWS payments cryptography. So, today, uh, you'll hear a little bit from me to describe what AW's cryptography is up to, what we've done in the past year. What we're thinking of doing in the coming year. You're going to learn a little bit about. Uh, sort of where we are focused. Where we think you might want to be focused in the area of cryptography and securing your data. And then the real treat today is you get to hear from Rajiv Sharma from Vanguard, who's gonna talk about their approach to how they're looking to get to a state we call crypto agility. In an effort to migrate to post quantum cryptography. So this thing that all of you will eventually have to do, the question is how soon? And I think it's really useful to learn from some of the early adopters in the space to figure out how you're going to strategize. So One of the things that we have asserted at AWS is let us do the undifferentiated heavy lifting for you. And certainly when it comes to running compute and storage. When you use a cloud service provider, you no longer get to care about the physical security of the data center. And you don't really get to care about all the front-end networking equipment. That is in front of APIN points, so we take that away from you and we ensure. That that is always patched and upgraded to sort of the state of the art when it comes to protecting your data. You can look at other tasks related to data protection. Whether it's digital certificates, managing encryption keys, uh, deploying software that handles PKI or encryption. And again, we want to provide solutions for our customers when it comes to software libraries that you would use to encrypt or digitally sign data. We offer those for free. They're all open source. We have a very strong commitment to transparency. About what we're doing in the area of cryptography, even when we are on the bleeding edge and in inventing some of the new areas of cryptography. So talk a little about uh the services that we have offered and how it's helping customers deal with undifferentiated heavy lifting. So I talked about the key management service, which is a, what we call a foundational root of trust at Amazon. So all encryption of customer data, the keys ultimately chain up to keys that are managed inside KMS. If any of you have used KMS, we have a resource type called a customer managed key that gets to be your root of trust for whatever data you're protecting across however many AWS services you want to use, as well as your own applications running anywhere on the planet. And KMS we believe is the largest lock and key service in the world, just in terms of raw numbers of keys that we're handling, as well as the number of encrypt, decrypt, sign, verify operations that we do on a daily basis. So we average around 30 billion operations per hour. Depending on the time of year, we can peak north of 30% of that. And we're able to scale to meet your needs. So if you need to do more cryptographic operations faster because your business is taking off, it's fairly straightforward for us to add capacity. One of the ways we did that is we invented our own hardware security module 12 years ago. And we designed it for the cloud. Highly cost effective, highly scalable, scalable, so we can meet you as you grow your business. One of the things we did with KMS is we changed the game a little bit on how you think about managing encryption keys. For years, developers had to know where is my key located? How do I know that the physical and logical security controls around the location of my key are up to snuff. It's a hard problem to solve, especially when the key at the top of a hierarchy must always be in plain text all the time for high availability. With KMS we said, you don't get to worry about that anymore. The only thing you have to worry about is access control to use a key. And the access control mechanism is the same access control mechanism used for every other API at AWS. So we found that most customers can reason about this. You think of it. KMS key, kind of like an S3 bucket. I've got a resource policy. This controls who can use this key under which conditions, in which services can it be used. Data tagged with what tag can be encrypted and decrypted under this key. So, KMS has scaled nicely for us. We've got a host of other services to solve slightly different problems. The cloud HSM service, in many ways, is a stepping stone. A lot of customers, especially in banking, government, some large manufacturing firms, had been using commercial off the shelf HSMs. Their application and client code. Talks one of the arcane cryptographic protocols to KMS, excuse me, to, to the cloud HSM. Uh, so this is where you want to do a lift and shift, and you have to use either a client that you don't want to change or a client you can't change to talk to an HSM. Certificate manager, again trying to simplify PKI. Your certificate is a AWS resource name. You can configure where that certificate gets deployed across any of the AWS service load balancing services. Or Earlier this year, we launched what we call take-home certs or exportable certificates, and so you can have us issue a cert for you and you can use it wherever you want. And you'll see a lot more work in this space to try to simplify the issuing and deploying of certificates because we think it's going to be a linchpin for this explosion of machine identities, part of which is being driven by the explosion of agentic AI. Certificate Manager is really designed for those public facing, publicly trusted certs that you would attach to your website, for example. Private certificate Authority is where you manage your own trustor. Your client knows how to validate or verify a certificate. Because it's an internal mesh network. You don't rely on the browser's trust store to validate a certificate. You can do it yourself. And then finally, Secrets Manager, where maybe you don't have an encryption key that you need to manage because KMS does it. You don't have a certificate to manage because ACM or PCA do it, but you have a random database credential for some application stack or a SAS provider, and you don't want to bake that into your code. So you put it into Secrets Manager. And that string that is your secret, your credential, in itself is encrypted under KMS keys. That's important to know as well, even the certificate management services in PKI. All of the private keys and key pairs are generated in KMS vended through ACM and PCA. All the signing operations for the issuing of certs happens through KMS. So that's why we call KMS a cryptographic root of trust. So Talked about certificate manager launching exportable public certs earlier this year. Uh, Secrets manager made an announcement here in the past week that talks about some deeper integrations with third party systems. So, uh, for the past couple of years, for example, if you use Cyber Arc, they have an on-premise secrets management system. You have a way to synchronize secrets that are in Cyber Arc with AWS. That's what we launched 2 years ago, and you'll see a growing number of additional partners where if you're using. Well you have to use some kind of secret in Salesforce to manage all your accounts and your access to your Salesforce apps. You can now synchronize that with Secrets Manager. So we're trying to make it easy for you to look at Secrets Manager as your single pane of glass for arbitrary secrets management, where those secrets are specific to your application stack. Private CA doing more and more integrations where certificates are needed, certainly spinning up large numbers of Kubernetes clusters. You've got to get the right certs because each of those clusters and those nodes have their own identities, and those need to be based in PKI. So, uh, I want to shift the conversation to the main theme of this talk, which is post-quantum cryptography. So what is post quantum cryptography? Well, it's a new type of cryptography, a new set of algorithms that are designed to protect you from. A threat that doesn't exist yet. Which might sound kind of weird. But it's something we all need to be thinking about. So, a quantum computer, there are certainly prototypes of quantum computers out there, you read about them every month. There's a breakthrough somewhere. At some point, a quantum computer will be what we call cryptographically relevant. And may be strong enough to brute force attack the classic asymmetric algorithms that we rely on today for all of our transport security. And a lot of other digital signature applications. RSA elliptic curve. These are things that have been in use for 30, 40 years. If a quantum computer that is strong enough and stable enough appears, there is already an exploit in the wild. It's called Shore's algorithm. So We don't know when this quantum computer. Is going to appear. Nobody really knows. I would encourage you when you're reading articles that say it's right around the corner, it's a matter of months, to take it with a grain of salt. Uh, because there are still a ton of material science problems to solve, and the stability of these computers is the critical part. Cause without stable quantum computing, you can't run Schor's algorithm for as long as you need to, to potentially crack an RSA 2048 bit key or higher. So, if we don't have a quantum computer today, why do we care? Well, here's the theoretical risk, which we call harvest now, decrypt later. The problem is Asymmetric keys and RSA elliptic curve algorithms are used in every TLS session that secures a channel between an arbitrary client and an arbitrary server across the internet. That network connection. In almost all cases, You, as the owner of the server component or the owner of the client component, have no ability to prevent somebody from copying packets, even though they're encrypted packets. Right, there's just too many places for someone to plug in a network sniffer. Pull down a PCAP file. And say I'm going to sit here and wait. For there to be a quantum, a cryptographically relevant quantum computer, and then I'm gonna go take this PCAT file and brute force attack it. Why? Because I might be able to get an underlying secret. That was used to encrypt very sensitive data that was sent across that wire. So the question that we're all asking ourselves is where are the network connections where Data that's sensitive today needs to stay sensitive and secured for many years into the future. That's kind of a hard question for a lot of companies to answer. But there is one company or organization that's pretty clear about that threat model. And it's nation states, so the US government. A couple of years ago said. Hey, we need our secrets to stay secret for like 50 years. And if anybody were to intercept a transmission over the wire. And decrypt it in 1020, 30 years. That's a very bad outcome. And so the US government has been doing a lot of work over the past 5 to 10 years to try to find a mitigation. And uh. I'm just going to go back one slide. The, the mitigation here is that you have to be able to create a ciphertext so that if it's captured, you have confidence that it is sufficiently protected against, again, this imaginary quantum computer that does not yet exist. Now, the good news Is that NIT has spent a lot of time and there are some quantum safe algorithms. You'll hear a term called lattice-based cryptography, acronyms like MLCCM or MLDSA or SLHDSA. So this is a lot of alphabet soup, uh, and it's stuff that you're eventually gonna learn and it will be standard and typical things the same way that we talk about RSA and ECC today. So, these algorithms are available, they've been standardized, and now vendors like us are in the process of implementing them. So the question is, how should you be taking advantage of this, right? So, the harvest now decrypt later threat. I've, it's really addressing what we call a confidentiality risk. The confidentiality of your data tomorrow is at risk if you don't encrypt it. Correctly or with quantum safe cryptography today. The other risk is what we call an authentication risk. So as we all know, it's one thing to be able to crack crack encryption and discover the underlying secrets. Perhaps another scary attack vector is that somebody can impersonate you and pretend to send a message as if they were you and fool the recipient into believing it's true. And this is where digital signatures today mitigate against those impersonation attacks. But we need to be thinking about how do we ensure the integrity and the authentication of signed data into the future. So if you are a company that signs firmware that gets deployed into devices and you simply cannot update those devices with a new digital signature for many, many years, you're concerned about this problem. Amazon is concerned about this problem with the Leo satellites that we're putting up into space. Lots of set top box vendors, lots of other manufacturing firms are concerned about this problem. So, this is another area where, while the cryptography is a little bit different, and it's not about securing confidentiality, it's about ensuring authentication. So We put together a blog post on the AW Security blog last November. That kind of outlines, here's how we at AWS are thinking about this because we have to go through our own migration. Uh, we don't pretend like we have the perfect checklist that everybody should follow, but we know though that there are different steps, and one of the real challenges is that most organizations have not had to worry about a migration in cryptography for 30 years. Right when the old AES standard was invented in the mid-nineties, it was replacing the defense encryption standard. And people are like, well, we've got to re-encrypt stuff with this new symmetric cipher because the old one is just flat out broken. These newfangled Pentium 2 chips can brute force attack Des. So there's a lot of work in the 90s. You fast forward, those people aren't around anymore. Nobody really knows how to do a cryptographic migration. So you cannot get away from doing some kind of inventory. And you have to identify, especially for that encryption and transit to address the harvest now decrypt later, what are the most important. Connections, where is my most sensitive data coming from, going to? You've got to prioritize where you want to make changes. That's why we think that integrating quantum safe cryptography into your public endpoints, your public facing endpoints, is the first place to start. Next is this authentication problem. Code signing, right? Where you want to make sure that you're not going to be forced to replace a long-term digital signature in a device that you can't access. And then the third area is actually also an authentication problem. But it has to do with short-lived roots of trust. And the best use case to think of here is actually the digital certificates that are used to Express the identity of your web server and or your client. So you get a digital certificate for your public-facing website. It says that you are example.com. And anybody who gets that certificate can say, yes, I trust the certificate authority that issued this certificate because they are Amazon or they are DigiSearch or they are Setao or whoever the CA happens to be. And your client says, I can trust the thing that I'm connecting to. Right? The reason that trust works is that there's an RSA or an elliptic curve digital signature inside there. And both parties can trust. That nobody's impersonating somebody they shouldn't. Right, but in a world where RSA and elliptic curve can be brute force attacked, somebody could potentially spoof you. So we have a solution. There are algorithms. MLDSA is an algorithm that can be used here. Part of the problem, however, is that clients and servers in a world where you might own the server, but you have no control over the browsers who are the typical clients, interoperability is critical. And interoperability is a thing where everybody kind of has to move at the same rate, and right now the browser community is still trying to figure out how they might want to deploy MLDSA. They're looking at alternative algorithms because these quantum safe algorithms create larger signatures, so more bytes on the wire. They also take a little bit more CPU to do the math, and so it's going to add latency. So that's why we think PQ for server and client authentication. Is sort of the final step, the final area, mostly because. You can't do anything today with public certs. Nobody is issuing a public certificate with an MLDSA signature. Because you can't Right, the browsers have got to build support for that first, and then all of the SDKs that do client-site authentication have to support that. So Uh, one of the things that we realized, we talked a little bit about NIST and the development of the core algorithms. They have been issued, they're standardized. You'll see things like PIPS 203, PIPS 204, PIPS 205. These are shorthand for the underlying algorithms. Uh, and that's step one. But then step two is, all right, how should these algorithms work in protocols like TLS or SSH. Or IPEC or MacSec. There's a whole raft of protocols between clients and servers to try and build an encrypted and authenticated session. The IETF tends to be the place where you define protocol implementations, and you'll see recently AWS was a part of a new RFC to try and standardize how MLDSA should be used in digital signatures. The telecom industry has their own set of custom protocols that they use to secure all of voice and data across telecom, and Etsy in Europe is sort of the leading standards body. We've been involved with them for 10 years. To also make sure that for example NIT, IETF and Etsy aren't doing different things because that's the bane of any new standard is that 4 different companies decide to choose their own or 5 different countries decide to go specialize. So we're trying to avoid that from happening. So, what have we done internally? Well, where we started is, well, we have core algorithm implementations. AWS Lib Crypto or AWSLC is our offering in this space. And uh AWSLC is open source, completely free. Uh, has runtimes that work in almost every major development environment. Uh, we have some optimizations for X86 as well as ARM 64. The reason we decided to build our own cryptographic library. is about the time that Amazon got in the business of building their own hardware. Because we now have ways to use less CPU to do AES RSA ECC operations. The less CPU we use for cryptographic operations, more CPU for your applications to do its thing. So we start with AWSLC and then we work up the stack so our. Open source TLS implementation called S2N. Highly optimized to try and minimize the latency for TLS connections. It uses AWSLC and supports the MLCm algorithm. So you'll see more and more of our open source tooling. Supporting PQ by default, and then those can be deployed inside your apps, in your software vendors' apps, inside our own AWS services. So, this diagram is trying to explain sort of where does the work need to happen. And where might you instead delegate work to us, instead of you coming through all of your source code to try and figure out how to upgrade the crypto. So what we're trying to point out here is that if it's your own custom application code, You're just going to have to go through it and figure out where you might find examples of RSA or elliptic curve being used. And how you're gonna swap it out. Sometimes that's working with a third party vendor, sometimes that's swapping out a new open source module. Your mileage may vary when it's your own application code. But customers also think in terms of the The endpoint that clients are talking to, we call it the TLS termination endpoint. Right, when EC2 first came out, people said, oh great, I can run my own web server. I'll run a copy of Engine X on EC2, and that's where my web server will live, and that's where I'll put my digital certificate, and I'll terminate TLS inside an EC2 instance. Over the years, we've offered more and more what I call reverse proxy services or load balancing services, where you let us handle TLS termination for you. Application load balancer. API gateway, cloud front. There's plenty of solutions in this space if you're relying on one of those services. To handle the TLS termination on your behalf. As of today, you already have PQ. It's there. You didn't have to do anything Like you may have to make a selection to upgrade a particular policy, depending on how much curation you do on your endpoint. But all the low-level coding and cryptographic change we've done on your behalf. Same thing with other managed services. So when you talk to the S3 endpoint. You don't manage the S3 endpoint. We do. Uh, as of a couple of weeks ago, it now offers MLCchem and a PQ option. So if your client can talk PQ to the S3 endpoint, you'll get quantum safe cryptography and protect your data that you're uploading to S3 and downloading. So this is the story going forward is every AWS service that exposes a public endpoint will support MLMm. And I'd love to tell you a date by when we'll do that, but we're talking about hundreds of services. And just know that it is my number one priority in 2026. And I've got most of the company on board with me, so. You'll see a later slide where we go through the number of services that already have it, and they're the ones you would expect. The long tail of services will be coming soon. So one of the things that people have said about PQ is like, well, it's new, it's different, it's gonna break somewhere, it's going to be extra CPU, it's going to add latency, and a lot of these people who have lived through the migration of the TLS protocol from 1.0 to 1.1 to 1.2. They have some battle scars. Because when you change the underlying protocol and all the steps required to do a handshake, you do see breakage. A lot of old clients just aren't going to work. But the good news is when you're taking an existing protocol like TLS 1.3, which is required for PQ, and you're just adding a new cipher suite up at the top of the prioritized list. You get very little breakage. The good news here is we are seeing an insanely low amount of issued reports. If you follow Cloudflare, they're very good at blogging about sort of their adoption of PQ. Over half of the traffic that's going through Cloudflare is now using MLchem. No complaints. The latency issue is a non-issue. Breakage does not appear to be happening. So, that's good news. It's gonna make it easier for you to convince your site reliability engineers that it's OK to upgrade to this new algorithm. OK. So, a little timeline here to review what we've talked about. Um, so NIST certifies the MLCm and MLDSA standards as PIPS 203 and 204, August of last year. Uh, we release a version of our core library AWSLC that supports initial implementations of MLCm late last year. We then deploy it in some of our core security services like KMS and ACM and Secrets Manager. We also make it available in a few flavors of our AWSS SDK because we know that that's the number one client that our customers are using to talk to AWS services. Right? Uh, KMS then launches support for MLDSA as a native cryptographic operation. So if you want to do firmware signing using an MLDSA key, you can now do that using the KMS sign operation. Cloud Front So Amazon's CDN. And outside of S3, the largest fleet in terms of, uh, connecting accepting TLS requests, it supports MLCm by default in September. We then released a new version of AWSLC that we're submitting for PIPS validation. This has optimized implementations of both MLchem and MLDSA. So again, these optimizations mean less CPU on your clients and servers to do cryptography, even though the algorithms are more CPU intensive and more bytes on the wire, we think we have done enough optimizations to make that transition negligible for you. So that brings us up to more or less. Today in the past month, so let me talk a little bit about the things that we have done here most recently. So in the past couple of weeks. We've exposed MLCm PQTLS to the. Load balancing reverse proxy services where it's your endpoint, it's your FQDN, you manage it, it's just we're managing the web server on your behalf. ALB NLB API gateway. So you can now configure your endpoint to use a quantum safe security TLS policy. On top of the services that have already been supporting this. We think that When you look at the most important TLS connections into AWS, as long as you're using an AWS service to terminate TLS. You're in fairly good shape. You might argue, well, I use a lot of Dynamo DB. Don't worry, they're coming, right? So this is where you inventory and you say, all right, where are most of my TLS connections across the public internet to infrastructure that stores customer data. You have your own list. We have our list based on what we know about all of our customers. We're working through it on your behalf. Private CA launched support for MLDSA. So if KMS lets you do a raw MLDSA signature. Private certificate authorities said, oh, we'll take that raw signature that KMS gives us and we'll use it to issue a cert. So this is a new kind of certificate. Instead of a RSA signature or an elliptic curve signature, it's now got an MLDSA signature. Now, we could only launch this in private certificate authority. Why? Because there's no such thing as an MLDSA signature that is publicly trusted, because the browsers don't yet support it. But if you control your own client. And you can control your own trust store on that client. You could issue a certificate through PCA that supports MLDSA and now you have quantum Safe authentication combined with the quantum Safe confidentiality of MLchem. In your say private mesh network. OK. So another pretty important primitive, uh, and one of the things that we want customers to start to experiment here to understand, OK, how does this affect my private mesh network? Are there performance issues with MLDSA? Because MLchem. Is really a nothing burger when it comes to added latency and performance. MLDSA might create some issues because of the certificate validation process. You've got to have the right client side code to do that. So it's there's going to be some experimentation. It's one of the reasons why the public browsers are taking a little bit longer to support this, they're working through those issues with this core primitive. We think that our customers can also start to experiment in this space. And then when the public browsers announced support for some type of PQ algorithm, likely MLDS. Then we will start issuing public certificates with this. So, in summary, Don't just say, oh, I need a budget for PQ migration. And it's like a one shot deal. You've got to break it down. You've got to prioritize. Protecting confidentiality of data in transit is your first job. And if that's all you do over the next 3 years, that's OK, that's winning. Right? Long-term routes of trust, whether it's code signing or other long-term digital signatures, maybe that's applicable to you, maybe not. And with digital certificates, especially the public ones, you're gonna have to wait anyway, till the browsers support it and the overall infrastructure is able to handle quantum safe signatures. So one of the things that we are doing at Amazon is we're using this sort of The pressure to adopt PQ as a way to go back and say let's think about how we build. Pipelines that are. Cryptographically agile, so the next algorithm that we have to deploy is easier. Right, and we don't have to go grip through a billion lines of source code and try and figure out like, alright, where is this arcane algorithm being used and how do I upgrade it, how do I test it? How do I know whether it breaks, how do I roll it back? These are all important issues. So we would strongly encourage you as you go through your migration to think, when is the next time I have to do this? Because while I asserted it was 30 years ago when we had to do it when DES was broken. I'm fairly certain that we're gonna have more algorithms that we want to migrate to. Why? Because the actual attack vector does not yet exist, and when it does, we're gonna learn about other potential risks. And there's a lot of work already happening, especially in the digital signature area, to try and optimize. And make the digital signatures smaller and faster. So while MLDSA is sort of the preferred implementation today, there will likely be 3 or 4 more options. And you're gonna see vendors adopt those you may choose to adopt them, and that's gonna be yet another process of how do I swap out an algorithm inside a mission critical system to a new algorithm without breakage. That's really the task. So Shared a little bit about how we think about the problem, what we're doing at AWS for undifferentiated lifting. Uh, I'm going to bring up Rajiv Sharma from Vanguard to talk about how he's applying some of these concepts specifically to what Vanguard needs to do. OK. OK, Thanks, Ken. That was great. Um. So I am, I'm super excited to be here to talk to you about uh Vanguard's journey to quantum safety, um. And here at Vanguard our mission is really to take a stand for all investors, to treat them fairly and give them the best chance of investment success. And so let me give you just a quick snapshot of what Vanguard is. So we're a global asset manager. We have over 50 million investors and $11.9 trillion in assets under management today. Our internal crew, which are our employees, is around 20,000 strong. The one key point here is that we operate without any physical branches, so all our interactions with our investors happen over secure encrypted digital channels. And a few years ago, like Kenan mentioned, We identified quantum computing as a material threat to that communication. And since then we've been actively involved in mitigating that threat. Also around, uh, you know, with the, uh, governments, uh, Kennon mentioned in the United States, NIT had released their, uh, last-based encryption algorithm standards in August of 2024. Um, in, in the, in the EU, uh, there are existing regulations today like NIS 2 and Dora which actually specify that you must adhere to the strongest form of encryption, uh, for your communications, and so that can, and, and, you know, it might include, uh, quantum safety in the future. Uh, the Canadian government has also issued their, uh, their plan to implement quantum safety in the Canadian government. So one thing to take away from this is if you, if you really haven't started your or or your PQC migration or looked at PQC in any way, this is really the tap on the shoulder like this is, this is the, the go, right? We're, we're ready to go. Um, just for a little technical context, um, so Kennon mentioned a lot of Amazon services, and, and we are heavy users of Amazon, but we also live in an ecosystem of things like data centers, uh, other cloud providers, as well as our third party SAS partners. So integrating within AWS as well as outside of AWS is going to be an important thing to cover. Um, for, for today's discussion, I'm really going to be focusing on. What we're doing within the AWS environment to support this. So how do we plan for our journey? There's really 4 pillars that we're looking at here and I'll, I'll get into each one of these. The first is around the strategic response. Uh, so, so really framing this problem up is not just, hey, we just need to swap out encryption algorithms, but this is really a, a business problem. Um, the second part is, uh, embedding that quantum risk in, in how we look at our enterprise risk, uh, horizon. So build those internal initiatives, build those center of excellent uh center of excellence. And then, uh, 3rd and 4th, let's look at some of the visibility tools. Where is all our cryptography today? That's a hard problem to, to answer and, um, and then work with our IT partners to start to migrate those into the newer lattice space algorithms. So really, um, like PKC isn't just a solo sport, it's not like one team in an organization that's gonna do this we have a lot of teams that are doing this as well as getting executive buy-in. So going to strategic response. So really, you want to speak in terms of the business risk and opportunity. You want to look at this problem a lot like we looked at resiliency, um, or any of these other, these large IT risks, um, this is gonna hit us in a regulatory compliance way at some point in the future. Also, more importantly. Our investors and our clients expect us to have the highest level of security. So they are going to be looking to see that are we implementing uh quantum computing um in our in our websites. And really it's to stress that urgency of early action. So the work that we have in front of us could take many years to do across a large environment. So starting now and planning now to get there well before what we call Q day or when a quantum computer is available is, is urgent. We need to start that that work today. And then also recommend some of these actions to executive leadership as well as the board. Hey, we've got to allocate resources. We have to allocate planning. We gotta think about things like change management. How are we gonna talk to our vendors? How are we gonna talk to our 50 million, uh, investors? OK, the second part of the journey is around the internal initiatives. So really this is about building the cryptography center of excellence. And I would say we shortened this once to crypto Center of Excellence and it got confused with cryptocurrency in in the org so don't do that spell it out. So we're gonna talk about the we're we we call it now the Cryptography Center of Excellence and this is a group within our organization it's a small team, um, but we are we're starting to grow on it. They're looking at this as an enterprise-wide transition. They're holding out the roadmap, the plans, organizing the, the IT teams together, working with our third party risk management teams to, to understand like what is the scope of this, um, post quantum, uh, cryptography problem. They're also building a lot of things like what what does good mean? How do we know that we've completed. Um, the cryptography, so looking at the, let's say the percent of traffic that's TLS encrypted with, uh, MLCm algorithms or the number of, um, uh, third party vendors or third party partners that we interact with that also support MLchem. And, and part of that is like, OK, let's, let's take a look at this, uh, this little framework here, discovery, prioritize and transform. So whenever you do any kind of large transformation exercise, you need to understand what that problem is. So that's what we're doing today. We started this effort, um, about a year and a half ago to discover and prioritize all the different areas where encryption lives in our environments. And then as services uh uh start to implement MLCm or implement quantum algorithms like like Ken was talking about Amazon services are starting to adopt these we we then start to transform those and start to enforce uh MLCm as needed as you know as we go through the the environment. The goal is to hit this by sometime late 2029 is to have a complete uh MLCm implementation throughout the environment and be quantum safe. You know, one of the ways that we're doing this is because of Amazon's commitment to that shared responsibility model where they are taking on that heavy lifting of building the cryptography into the services and it's really just up to us to configure and use that service. OK. Talking about the visibility and IT collaboration, I kind of put these two together. The way that we're framing up this problem, the Cryptography Center of Excellence is framing up this problem is in two dimensions. The first dimension is around posture and monitoring. OK, so this is one dimension. So the posture is talking about what is possible in your service, what is possible in the environment. So for example, what is the, how's your cloud front configured or how's your load balancer configured? What are the different algorithms that are available on that um on that service? The second part of this dimension is the monitoring. So if I have a list of TLS cipher suites that are available, one of them is is PQC enabled, and the rest may not be. We look on the wire for that TLS handshake, for that client hello and server hello to understand which of those protocols actually took place. And so that gives us the um a, a high level inventory of what's going on. And I'll, I'll, I'll dive a little deeper into each one of these. The second dimension is around ingress and egress, and why we made this important is we need to understand which party is initiating that TLS connection. So from an ingress standpoint, this is gonna be your external browsers, for example. They're the ones initiating giving the cipher suite. To a CDN, let's say Cloudfront, which also has a list of cipher suites available, and they're going to start negotiating the most optimal cipher suite. Then from the edge location, it's gonna go into a load balancer. These two hops are over the Internet today, so that load bouncer is exposed on the Internet. It's getting traffic from Cloudfront over the Internet. The browser is connected to Cloudfront over the Internet. From a prioritization prioritization standpoint, this is one of the most important places to start our work. As of September 5th, if I get the date right, Cloudfront supported, um, MLKM as one of its options, and it was, it was backwards compatible, so we instantly got advantage of having any browser that supported. That, uh, quantum protocol, uh quantum safe protocol was now able to, to use that and we did see it in fact we, we're seeing at that time somewhere between 75 to 80% of the browsers actually supporting or capable of supporting that, um, that type of connection. Um, the low bouncers have just come recently, so the, um, uh, cloud front to low bouncer, uh, will be the next thing that we're gonna start to tackle. From the egress side this is where our systems are initiating the connection. So think of this as your EC-2 or your containers inside the VPC are gonna make an outbound connection. They have a cipher suite, uh, on their software as well. So in our architecture they go out through a proxy and then out an Internet gateway and then to a partner, whatever that partner may be. It could be a, um, like a login service or an authentication service or or anything like that. Um, So in this flow when we're talking about ingress and egress we're trying to understand. From that compute what is configured on that compute or how's that proxy or firewall configured or from a third party risk perspective. We have hundreds of partners that we interact with. Which one of these partners has gotten the tap on the shoulder and is actually working on a PQC implementation? Like this is a, this is a partnership across everybody. So when you take those two dimensions and lay them out, you get this, this floor box where you have ingress posture, ingress monitoring, egress posture, and egress monitoring, and this is all around the crypto discovery and prioritization patterns. Each one of these boxes has a different tool or different technique of how you accomplish this. OK. All right, so let's talk about the ingress, and I'm gonna focus mainly on the Amazon services side of the house here. So when you're looking at the ingress posture, When we're talking about that cloudron distribution. We use uh AWS config to understand how Cloudfront is actually configured. So what is the posture of it? And this is uh an example query and then you get this result of what all the different uh SSL policies are available on that. Um You know you can do this through config. You can do this through other CNA tools as well. If you have a third party CNAP, you can provide that same type of data. Um, the second part of the Angus posture here, this is from the cloud distribution into the low bouncers, so it's a very similar thing. You can go ahead and just get the posture just by creating config or or using a CNA tool to, to understand what's out there. From the monitoring standpoint. What we've done is we've turned on cloud front logs and logged into CloudWatch. From there you can then query um CloudWatch and understand what are the different um. What are the different uh SSL cipher suites that were actually connecting on that wire? This gives us some idea it gives us um. You know, it, it gives you, it gives you an idea of like what CyberSweet, uh, actually it tells you what CyberSweet but. Um, whether if it was MLCAm or not is, is still not in the logs today. So that's one thing that we're, um, that we need to figure out is, is how we can get the, the actual, um, key exchange algorithm in here, not just the cipher suite. So, but what we do know is that Um, these are browsers based on, let's say their user agents and things like that that that do support the MLC MLCm Cypher Suite. Um, same with, uh, the low bouncers. So, uh, we've used, uh, ItalyS Athena to query the logs, and then from here we can actually tell what the cyber suits were, were used here, and then we can. Um, infer that, uh, which ones were actually PTC or not. All right, from the, the egress side. So if you remember we have a a proxy architecture today that's making calls out um through an ENI out the Internet gateway into our partner um infrastructure. So in this model what we're doing is we're just simply capturing the, the, the logs that are coming out of that that firewall uh to understand what is um. Uh, what is the cipher suite that was used there? The other part of egos posture is so that if you. If in some of our environments we have a transparent proxy where that proxy slash firewall is not breaking the TLS connection but is going straight out, so in that case this is where we actually have to look at the source code that's sitting on the compute and how it was written. Um, to understand what that, what the, the ciphers was used there. Um, a couple techniques here you can use static code analysis. Um, there are throw party SCA tools, like, uh, other static code analysis tools. Um, there is also from, uh, from PQCA, the Post quantum Post Quantum Cryptography Alliance, uh, an open source tool called, uh, COM kit. Uh, which right now covers Java and Python, I believe. We've experimented a little bit with that. That, that's one option. Another thing that we're experimenting with is, uh, just using something like a code QL or maybe even uh like GitHub Advanced Security to understand what are the, the different ciphers that are actually configured in that environment. Um, From this monitoring side, um. This is where we're actually capturing that that uh TLS handshake here. Also, one thing that we experimented with in monitoring was. We tried this, OK, in the lab, so if you don't have a proxy, this is something you can turn on for a few minutes just to understand what's going on is really using um uh VPC mirrors. So when you take, when you look at a VPC mirror, it will actually capture the traffic off that ENI. Um, it is still TLS encrypted, but you can see the handshake, and then you can send that those, those logs over to um. Another, uh, compute instance which will then capture the logs and you can write it to your, you know, a bucket or a log source. Um, I, I probably wouldn't recommend this in a live production environment. It's, it's pretty heavy, um, but it's something that you can. You could do to to try out and actually it it's interesting education to to see how the the ciphers are actually being handled. All right, so, some quick takeaways here from our side you really wanna establish that, uh, cryptography center of excellence that that's really where we wanna go and then have them define the quantifiable outcomes. What does it mean to succeed? How do you know that you're done? Um, how do you know that you've, you've gotten every piece of cryptography out there? And then um focus on that muscle, uh, that muscle memory to uh to build that uh those cryptographic assets because we may need to do this again one day. And then um start the transformation when MLM is ready. So as these services in AWS start to implement quantum safe algorithms we can start adopting them. OK. So this is really a very high level of uh how to adopt uh quantuma and quantum and crypto agility in your environment. If you wanna dive deeper, there's a couple of resources here from from A AWS. The first one is around uh PKC migration plan. Um, and it's, it's about how to, how to actually build this roadmap. Uh, the second one is, uh, hands-on workshop where you can actually experiment with some of the services and, and understand like how to, uh, turn on PQC. Um, there's also a, uh, quantum hub here, uh, to get the latest, uh, latest news and information on, on, um, on PQC from AWS. Pictures, OK. OK, and then, um. Later this week on Thursday, there's two other sessions that dive deeper into more technical details around this. So, uh, there's SEC 404, um, which is actually a workshop I believe, right? And uh 331 which is uh talking more about the actual algorithms in place and uh OK you wanna talk yeah I I wanted to. echoes something that Raji brought up which might Be perceived as a gap. So Raj you talked about how do I prove that my last request was protected with MLchem or some quantum safe cryptography, right? When it comes to Detecting whether or not an endpoint supports a particular algorithm, there's plenty of scanning tools that will tell you, right? It'll say this certificate signed with an RSA key or an ECC key, right? And you'll see more and more of those tools that will say this endpoint is capable of MLMm and PQ. But actually showing evidence that yes, my connection from my client to this endpoint was in fact done over MLMm. And that means the trillionth connection will also be protected under EmLcam. That's an important thing you wanna hear. So, uh, one of the things that we're doing is if it's an AWS service, if it's our endpoint, a managed endpoint, where you're just gonna get whatever crypto we give you. What we're in the process of doing is adding a field inside the cloud trail log. Called TLS details, and it will very specifically include the string that includes MLchem. It's a long complex string X25519 ML chem 768, right? That means good. The absence of that might mean bad depending on what you're looking for. Uh, but the thing that you're gonna wanna do is your application stack. Where it emits its logs to something like cloud watch. You've got to get your application stacked to emit the right thing to cloud watch. So this could be under your control, might not be under your control, depending on whether you've delegated to some other SAS provider or some other software provider that you know, you're running their software inside your EC2 instance. But it's a critical part of this, because this brings things full circle. At some point, you're going to be asked by an auditor or regulator to prove that you're using MLchem. It's probably going to be somebody who is representing a US government customer you might be trying to sell to. Right, and being able to point here in this cloud trail log, there it is. That settles the issue. But it's definitely an area that we need to work on throughout 2026 inside our managed services. Every, uh, ISV partner that we work with at AWS, we're reminding them, trying to get them to be out in front of it. Um, but I thought it was worth coming up and reiterating that because that's a very important part of closing the circle. OK, so I think we're done, uh, and we've got a 30 minute break. I think Rajiv and I will hang out up here if anybody has any follow-up questions. Thanks a lot for your time.