---
video_id: Jk49dLj6ZL8
video_url: https://www.youtube.com/watch?v=Jk49dLj6ZL8
is_generated: False
is_translatable: True
---

Hello everyone, thanks for joining the session. Uh, so today we're gonna talk a little bit about one of the growing performance challenges with AWS customers and, uh, really the hybrid cloud customers in general, and that has to do with Agentic AI and AI, uh, applications in general, which, although they're very powerful, they tend to be, uh, unpredictable in, uh, usage from a resource perspective. So we're gonna spend a bit of time talking about. How this problem kind of manifests a little bit more about the problem itself and then as well discuss some potential solutions because ultimately you want to make sure that you maintain the performance of the end users who are using the agentic AI but also make sure that you are as efficient as possible when it comes to the cost allocations so you can avoid that blow. So all right, let's get into it. Uh, so before I get started, would love to get, um, some feedback from the audience, just a quick poll in terms of where you are with adoption of agentic AI. Um, raise, raise your hands if you're just exploring, like you're just getting started with learning about Agentic AI and where it might apply for your environment. OK. Makes sense? How many of you are running proof of concepts, where you actually have some workloads and, and some, uh, it's in development and you're seeing that. OK, great. How many of you are in early production, where you actually have some real users, um, and a number of, of uh work streams applied to that, great. And then last, uh, business critical, where like you've got real users, you're using this at scale, it's a big part of your ROI you're seeing that, anybody in that category. OK, alright. It's not quite yet that at that maturity adoption level, but a lot of that kind of makes sense given what we see from our customers too. OK, uh, so just expanding on the hidden cost of agentic AI and a lot of times what you see is overprovisioning and resource bloat, and primarily this comes down to because, uh, people don't make performance uh or resource decisions based upon averages. Usually they'll take the worst case scenario because they're worried that performance might suffer. And what that leads to is really conservative capacity decisions because ultimately they might be afraid of this performance degradation which will lead them to oversized GPUs, overallocate kind of ram farms, overprovision storage, and this leads to. A lot of idle costs for these expensive resources and so what you'll see from a lot of teams is that for fear of creating these performance buffers, they will overallocate these expensive resources and then many times just because they're afraid of that peak that might happen, right. There are static scaling policies which are in place, but some of the problems with these scaling policies is they tend to be reactive and so if you have like a peak that happens, then it's already too late for this for the scaling policies to take effect because like I said it's. It's er with Agente AI and AI apps, a lot of times the usage is unpredictable, right? And it also requires a fair amount of human intervention to stay on top of that, so er you're consistently looking at those scaling policies or taking remediation to deal with that. All right, so talking a little bit about how Agente AI is changing how workloads behave, um, a lot of this comes down to, you know, with traditional legacy apps, uh, there's, they typically follow like a linear process. A request comes in, it gets processed, and it consumes resources, and this typically follows a straight line, right, as more users show up and hit the site, for example, then resources get consumed in a, uh, linear fashion. With Agenic AI and AI apps, um, it doesn't necessarily work that way. It's, it's very unpredictable. It can plan, uh, branch, and then spawn, uh, many times like a lot of different, uh, hundreds of micro workloads. You could have Gen AI talking to another, uh, agentic AI talking to another agentic AI, Agentic AI talking to a number of APIs, and all this is kind of happening in the background, and it's creating these resource spikes, um, that can sometimes bring applications down to its knees, right? Um, one user request, for example, can come in and then create hundreds of downstream tasks, and this leads to overprovisioning of GPUs. You see issues with throttling and spiking, and as evidenced here, like we've seen customers who um. Have to deal with overprovisioning and sometimes utilization of 30% of what they should be, and 10 to 20 times the GPU hours and really what's needed, and ultimately you end up with a situation of where these idle resources just sit there because they're afraid of this performance risk and then ultimately become very wasteful and a hidden tax upon the organization. So you really need to consider. How can I make sure that my resource usage is dynamic, just as the Agente AI and AI apps are, are dynamic themselves based upon their nature. All right, so, uh, talking about the gap between insight and action, just elaborating there, you know, you do have observability solutions which will do a great job to, to give you information such as is there CPU throttling happening, is there GPU contention, are there latency spikes. So you can see observation there, but it doesn't necessarily do anything for, OK, so when that happens, like what do I actually do? Maybe I can do an RCA and after the fact go and fix that, but uh it doesn't actually take action. when you're dealing with these types of agentic AI applications on the FinOs side, FinOs does a great job in terms of, OK, well how much am I spending here or if I need to do reporting for showback or chargeback, I can see that and allocate it appropriately for these applications, uh, but again, it's, it's primarily around reporting. It's not meant to do resizing of GPUs or to handle these dynamic scenarios, right? It's really more about the cost allocation, uh, versus actually like solving the problem. So again, you have this problem where even with some of these other solutions, in general, just oversizing becomes the default, where many of our customers who come to talk to us about this problem will essentially say, you know, because I'm worried about performance and maintaining my SLOs, I'll just oversize what I have in the public cloud or in even certain cases in the private cloud. And then again back to static scaling, this is in place, but because of the nature of these apps, they're so dynamic, as I mentioned, in terms of the way they do their work, they can't keep up with the demand. And so again, making sure that you really look at the problem from a resource perspective and have dynamic resources for the platforms, resources that support it to meet that ever changing demand is critically important. Um, we're gonna skip this poll just based on timing, uh, so I'll move forward, um. So in our particular case, so what Turbboonomic does to deal with these problems is we will deliver real-time optimization to agentic AI workloads, and what we will do is, uh, we will provide actions that look at the continuous optimization by analyzing data that comes from the GPU instances itself, uh, VCPU and VMM saturation. Throughput for storage and network and look over time at these metrics and then ultimately make decisions across your environment to right size based upon what we see from these metrics. So not only will we analyze that information. We look at this entire supply chain from the Agentic AI app, but also look at the resources that are supporting it in real-time scale up and scale down, and we look across AC2, the GPUs themselves, EKS and hybrid environments, um, of course AWS, but also if you're, if you have on-prem workloads as well or in other public clouds, um, and as we do this, we do, we make sure we maintain performance and make sure we meet your SLOs, but also keep things efficient so cost is, uh, under control. And as I mentioned, GPU optimization is one of the most impactful use cases that can help with gentech AI given how much of the resources come from there. So as you see peaks come in where more of the resources are needed, we will right size them and bring in additional GPU instances if required. Or and and leave them there, but as demand goes down in real time, we'll right size those instances and reduce them as well as other resources like memory to reduce that overprovisioning and deal with some of the challenges that our customers face by doing this on a continuous basis. It takes a lot of the human error out of it and allows you to know that these gente AI apps are going to run in a performant manner but also as efficiently as possible. All right, so I'm gonna take you through an example. This is a screenshot from our product and what we're looking at here is a, uh, a pending action, which is essentially an action that the user can take, so this is an example of like a manual action. And what Turboonomics has been doing in this scenario is it's been analyzing the GPU usage, uh, the GPU memory, the instance sizing and utilization in general, and it's looking at this particular GPU which is a P3DN 24X large, and which is one of the more expensive GPUs for AWS, um, and then it's essentially saying based upon the utilization that we're seeing. Uh, you can scale this down to a P3.8X large, and we're able to do that because we're looking at the utilization data and and matching this up with the SLOs and knowing that this is going to be a safe action that can be taken, and as you can see just from one single GPU you can get a savings of $13,800 per month, right? Um, and this is still keeping your performance, uh, uh, maintained in the SLOs. So just getting down to a little bit of the details because you know it's a great recommendation, but our customers, our operators want to know, well, how do I know that I can kind of trust that that action is going to maintain performance. So I need to see an additional level of details. So what you see here on the left hand side in some of, in some of the charts are, OK. Look, starting with the GPU count percentile and utilization, what it's saying is that utilization for this observation time period is around 13%, so it hasn't really been used that much, um, even though it's tied to this EC2 instance, right? And then you look at the GPU memory utilization, and that's hovering around 22%. Um, again, not really something that is speaking to a lot of, a lot of usage in general, and it's kind of hard to see with a VCPU, but I think that hovers around 3 to 4%. So not a ton of resources are being used for this app. So if you get into the resource impact, the red box on the right, you can see the current and then post recommendation for this action. And uh Turboonomics is recommending to take the current GPU count from 8 to 4 and uh predicting that this will lead from 13 to 26% utilization, that's a projection. And then on the memory, it's going from 32 to 16 gigabytes and also predicting that the utilization will go up to 44% after taking this action. Again, very safe, you're not going to see a performance degradation. And then VCPU it's around I think 13%, and then you see storage throughput, network throughput. In a similar situation, so again, the nice thing here is that as an operator, I can look at this action. I feel good about it because I know that the performance is there and we're still going to maintain our SLOs, and I feel comfortable doing that. So it's not just like, hey, you're going to save, you know, the cost on this action, you should just take it. It's giving that user the confidence that if I take this action and potentially even automate it, I feel confident that things will move forward in a positive manner. So I'll just wrap it up here on on this potential action so you can see kind of overall the summary, the on-demand rate goes from $31.21 an hour down to $12. It does take into account our eyes and savings plans, um, and then you see that on-demand cost, which again rolls up to around $13,800 savings per month. And this is just from one single GPU instance. So imagine if you have this, uh, quite a big investment, then you're going to see even more savings. All right, so let's talk a little bit about, um, some more of the details in terms of how intelligent automation can really transform your AWS environment. Talked a lot about smart GPU optimization, so what, so what we will do is automatically tune those GPUs. We will right size, like I said, based upon, uh, as demand spikes up in real time, uh, and then we will bring it down to make sure we're as efficient from a cost perspective as much as possible so you can eliminate that idle capacity. We also have real-time visibility, so our application looks at all of the resources running in real time, and we have a supply chain that will map the business apps themselves all the way down to the resources, and we look across those resources and how they work together ranging from EC2 and EKS and then the GPU environments as I was mentioning, a ton of different resources we're able to correlate those metrics. Make sure that no harm is done and that ultimately those applications are getting the resources that they need and we'll also take into account there might be a potential issue or a bottleneck in that supply chain. It may not be hitting one specific resource, but we'll identify that and we can take and take account to be proactive about what we're doing. Uh, we also have orchestrator um, integration which will understand things like pod placement, affinity, resource quotas, and scheduling. So we'll optimize the container infrastructure, um, and, uh, the traditional infrastructure layers together for those business apps, and we also have proactive automation as well. So that the previous example I gave was like a single action where you can completely automate. The actions that we're taking, so no no human intervention is required, um, and like I said, if, if we are identifying a potential issue, we'll go ahead and take care of it. So you don't have to have those incidents that happen that require like an RCA and trying to problem solve after the fact. At the end of the day, uh, like I said, part of what we do is around efficiency and the cost savings and cost allocation is a big part of that, and uh we sell the businesses and so we will sum up individual actions, we will sum up the overall ROI of all of the automation that's happening and present it back to our customers, and that's available as well. Um Wanna go through a couple of case studies that apply directly to some of this work that we've done, and, uh, one of our internal customers is the big AI models team, or BAM, short for BAM, which supports the LLM uh behind Watson X, and really what they were looking at is they needed to. Improve their environment so that there was less manual tuning required to look after their environment, and they have, they have hundreds of containers, they're running Kubernetes, around 100 A100 Nvidia GPUs, and really what they wanted to do as well was also like, how can we potentially make these GPUs more dense and get more ROI out of what we were doing. And so after using Terminomic they saw some great results, uh, 5.3x times in terms of idle GPU resources, this took the, the headroom from 3 to 16, so these 16 GPUs were, um, these resources were available for other workloads. They saw 2 times through throughput improvement without impacting latency, so that was a, a big benefit for them. And again. They had 13 fewer GPUs that were needed, and they were able to allocate these GPUs to other workloads, so huge savings for them, just by making the other GPUs they had denser and, and being able to allocate those new GPUs as well as some of the additional resources from the GPUs they still had allocated to other new AI workloads. You can scan the QR code if you want to hear more about it or read more about it, um, but again, great example of us working together on that. Alright, so 3 things to remember today, I'd love to leave you with, um, Agentech AI, again, it, it's because of its nature, it's very unpredictable, right? It requires a lot of resources, it doesn't scale in a linear fashion, so remember when you're, um, thinking about projects, uh, along your maturity curve of adopting agentic AI, uh, keep this in mind, that you're gonna have to be, uh, sensitive to this because they're bursty, they're unpredictable. Uh, and visibility alone is not enough, right? Uh, just having access to the resources, um, behind it and seeing some of the issues, uh, can still create a lot of manual work for you to have to support it. So, um, being able to have a solution that takes those insights and creates, uh, continuous action to right size your environment and make sure those applications have the resources that they need is incredibly important. Um, you can really start with one high value workload, so, uh, look at for a potential application where you want to prove that you can still meet performance, um, but have GPUs be more efficient, uh, look to target that and then scale it out and see some of those results. Uh, as far as working with us, uh, we'd love to work with you in terms of. Let's look at, uh, solution together, identifying a pilot workload, and, um, whether it's a gente AI or a GPU service where performance or cost matters most, we can balance both of them. We also have a broad set of integrations across, uh, the hybrid cloud, so we have a ton of AWS services that we optimize. We also do private cloud with VMware and Newanics and others. Uh, Open shifts and, um, hybrid environments and other CSPs that we support also an opportunity to combine observability, uh, and automation. So our sister product Instana provides even better metrics to power our, our automation and, uh, make our right sizing actions, uh, even better and, um. I'll just leave you with uh thank you for your time today, I really appreciate um all the feedback and responding to the poll. Um, our booth is over there if you wanna come and get a demo and see us in action, so please stop by. Um, and again, uh, thank you for your time and have a great rest of your show.