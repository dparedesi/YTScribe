---
video_id: zs6-dPhtKB8
video_url: https://www.youtube.com/watch?v=zs6-dPhtKB8
is_generated: False
is_translatable: True
---

Alright, thanks, thanks everyone for coming. Um, today, uh, we're gonna be talking about how Zooks, uh, uses machine learning infrastructure, uh, built on AWS to train our foundation models. Um, so my name, oops, excuse me, so my name's Jim Robinson Bonslav. Uh, I'm the tech lead for foundation model training here at Zooks, and my co-presenters Anando and, uh, Avinash will introduce themselves when they come up. All right. So here's the uh overall agenda. So first I'm gonna give a short introduction to Zooks. Hopefully you all know already, um, and then I will give an overview of the use cases for foundation models, uh, here at Zooks, and my co-presenters will go over, um, AWS Sage Maker Hyperpod and how we're integrating that, uh, into our training and evaluation stack. All right. So what is Zooks? Uh, why did, uh, we build this company? So Zooks aims to make personal transportation safe, uh, clean, and enjoyable for everyone. So we, to do this, we built a a purpose built robotaxi that hopefully you all have already seen, uh, driving around the strip, uh, that is full electric, fully autonomous, and designed for riders. Why did we decide to do this? So we believe the model of individually owned human driven vehicles is broken. So you can see here, uh, human drivers are actually quite dangerous, uh, and there are, you know, many fatalities every year, and human driven vehicles are idle the vast majority of the time. They're not actually taking you, uh, from place to place, and of course, uh, they, they also put out a lot of pollution. So, uh, I hope you all know this already, but Zooks is live in Las Vegas, so I'll let you see this, this little demo. Um, so you can see our robo taxi going on the strip. It's very beautiful. I hope you agree. Um, Yeah, so I encourage you to take out your phones, get the QR code, get the app riding is free in Vegas right now, uh, and so that's very fun. So I, I strongly encourage you to take a ride and, and let us know what you think. All right. So here's a high level overview of Zuke's autonomy. So it drives itself, hopefully that is not a surprise to any of you. And to do this we have sensor pods all around the vehicle. Um, you can see that, uh, here in this actual image, uh, these are the pods on the top corners, uh, that have LDARs, radars, cameras, thermal cameras, microphones, uh, tons of stuff on there. And we have of course 360 field of view so we can see all around the robotaxi and uh they are redundant in in you know in case of uh hardware failure. And we can see up to and over 150 m away from the vehicle. Alright, so the three pillars of autonomy are perception, prediction, and planning and controls. So perception's job is to take raw data streaming in from all of our sensors and turn that into a structured understanding of the world. The core of that is, of course, the 3D bounding box, uh, where we are going to detect and track every agent in 3 dimensions around the vehicle so we can tell if it's a car, a pedestrian, a truck, um, or we detect traffic lights in addition to using our, our HD maps. All right, so prediction's job then is to take all these agents that are moving about and predict where they might be in the future. So these predictions are multimodal, uh, which in this case means we predict multiple possible futures of every agent, uh, so. Um, so that the planning stack and controls can turn that into the actual, you know, volts that end up controlling the vehicle. So planning integrates our perception stack, the outputs of prediction, the map that we've prebuilt, and of course the mission. You have to know where the rider wants to go, uh, in order to make the high and low level plans and controls. Alright, so most driving is easy, but some driving is very, very hard, um, so here we have a log, uh, where we're going at a, at a pretty good rate, I think, I, I forget, between 25 and 45 miles an hour, and there's two fire trucks in the right lane, and note that they don't have their, uh, their siren on, their, their blinking lights, so even though they're, it looks like they're actively responding to an incident. So we obviously take uh active uh fire engines very seriously, but um even in this case they they had put a fire hose into the drivable lane and our autonomy stack is capable of detecting this uh and making a plan to to uh safely, safely move around. So I, I've been driving for 18 years and I've never seen anything like this on the road, but our robotaxis see things like this all the time. OK, here's another example right from here in, uh, in Las Vegas. So we're going down the strip at 30 miles an hour and somebody decides to walk across all 6 maybe lanes of traffic. Uh, again, not something I've ever seen and I don't think, uh, that was a great decision, but our, our autonomy stack has to be capable of handling people making poor decisions basically, um. OK, so. Uh jaywalkers are. You know, important to deal with, but they're not particularly unusual, and we do absolutely see very unusual things in our robotaxis. So going uh across top left to top right we have a child dressed up as a construction zone flagger, uh, and spoiler alert, we do listen to construction zone flaggers, but in this case it's a little unclear if we should or not, um. One time we saw a convoy of tanks going down the strip, which you might imagine for our machine learning based uh perception stack is a little unusual to to deal with. uh, we don't have a a specific tank class in our uh ontology. And the next image is, uh, you know, we obviously pay a lot of attention to traffic cones. They tell us where construction zones are, incident zones, and other things, and somebody perhaps on a fun Vegas night out decided to put one of those on top of a sign. Uh, again, not clear that we should listen to that. So we see cars on fire, which is maybe not that unusual, um. We see handwritten construction zone signs saying keep right, uh, that we might actually have to have to obey. um, here in Vegas there's a half tour bus, half shopping cart called the Cartzilla, uh, that we see often, and our, our, our autonomy stack has to be able to handle all these long tail cases. So we even have to be able to handle when people are behaving badly, uh, so this particular image, the right side, it's a little dim, uh, it was, it was at night, is somebody's foot as they climb on top of the robotaxi, um, and, in the left panel of that is, uh, their friend, uh, videoing them. So, um, and of course, uh, we have somebody riding around with a dog in the backpack, um. And unclear if we should maybe call that agent a dog or a pedestrian or or bicyclist or what's going on there. So this is the long tail of autonomy and our system has to be able to deal with all of these but you might imagine the edge cases proliferate, um, and it becomes increasingly hard to deal as we as we scale and. One way to to handle this and and the way that we on our team have decided to handle this is with uh you know foundation models. So when you have a really difficult problem, you go back to theory and principle, and in this case we turn to the bitter lesson which should be familiar to all of you, but this is the famous blog post from Richard Sutton, and the, the key quote is the biggest lesson from 70 years of AI research is that general methods that leverage computation are ultimately the most effective. So I, I sort of adapted a graph I had seen on Twitter of here's 3 possible ways that a model might improve in its performance over time. So the first way in light blue is where you add a lot of human engineering and expertise. So you might have a small model designed to do one specific thing like uh is this construction zone flagger a kid in a costume or not model, uh, and that's gonna be great in the short term and it's always the best in the short term. And then in the middle maybe you have a more general model, uh, but we're, we're really betting on this long term approach which is we're gonna use a lot of compute, uh, that my, my co-presenters will tell you about and we're gonna have a very, very simple approach, uh, and we're not gonna put a ton of human knowledge or biases in so we're just gonna train, train big models on, on big data. So and specifically we are going to combine many, many different uh tasks and and data sets and approaches into one multimodal language action model. So our goal is to train a model to zero shot these long tail scenarios. So 0 shots mean means that we don't have to see 1 or 2 or 5, tanks before we can accurately deal with it, or we don't have to see 10 kids in flagger costumes. Uh, we will, our, our stack will accurately handle this the very first time we ever see an event. That's what it means to to zero shot, and that's our goal. So our approach uh that we're taking is to make a multimodal language action model. So the core of it is an LLM, uh, and what we're gonna output is robot, uh, robotic controls, so you know, controls for the robotaxi like acceleration, braking, steering, and so on. Um, as well as 3D detections like our perception stack does currently, um, and also other, you know, other more generative techniques like answers to questions, uh, am I supposed to listen to this flagger, um. Is the is the uh. Hotel employee gesturing at me or at the guy next to me, um, so answers the questions like these and of course descriptions of scenes, uh, captions are very useful for for many, many applications. So the input to this like any LLM is going to be a text prompt, as well as a a pre-trained uh embedding layer in this case, and those are gonna go. As embeddings into our LLM. So in this case the prompt is just uh a dummy prompt. You are the driver of a Zook's robotaxi. Um, what should you do in this scenario? Think step by step. As well as the text prompt, we have images or videos coming off of our uh sensors, and those go through pre-trained encoders, uh, that we then pass through a projection layer. This is a standard, uh, multimodal LLM approach into embeddings that the LLM can process. As well and a little uniquely you might imagine at Zooks we've been driving around for a long time we have nearly infinite LDA and radar data so we encode that and project it into the LLM as well. Lastly, the one of the main benefits of a. Using an LLM as the core is its flexibility so you might imagine we can encode and and pass many, many, many things into our model including the outputs of our perception stack right now if we wanted to, um, so something like the 3D boxes, uh. That we that we generate we can encode those and and pass those into the model. OK. So how are we actually training this? This is a very high level slide, um, but I'll, I'll walk you through. So we start with pre-trained models, uh. Yes, uh, didn't wanna get into the pre-training game for reasons you might imagine. So we take state of the art vision language models like Queen 3 VL came out, uh, a few weeks ago, um, that we were basing our model off of. In our first stage is really large scale supervised fine tuning. So this includes behavior cloning from tens of thousands of hours of human driving, where we train the model to do what the human did. We have millions of 3D detection labels so we can train them all to understand the world in 3 dimensions. And then of course we have more standard LLM uh techniques like visual question answering, spatial understanding, and more. So the next stage in our pipeline is a smaller but higher quality uh SFT stage. So this includes rare object detection, driving, particularly in difficult scenarios, and synthetic chain of thought reasoning to help the model, uh, learn to think. And uh and reason its way to to an answer. Which leads to the last stage in the in the training pipeline which is reinforcement learning, so we are fine tuning for robotic controls or driving the robotaxi on our most difficult uh scenarios. And we use techniques like GRPO DAPO, um, for this. And the last stage for for deployment is uh for offline purposes we use VLLM and for for um we're working towards potential uh online integrations with something like Tensor O T LLM. OK, so what are the challenges in training a model like this? There are many, as you might imagine. So first is that we have petabytes of data. So we have multimodal sensor data again cameras, liars, radars, all of the structured data that our autonomy stack already generates, um, which numbers in the petabytes. So, and of course accuracy and performance is a challenge we want it to be as good as possible and as low latency as possible, which is a fundamental trade-off. And for our researchers and engineers working on our team, we want rapid and scalable iteration, so we need to be able to launch an experiment, track it, get metrics, uh, run our evaluations as quickly and smoothly as possible so that they can run their next follow up experiment quickly. And there's a whole host of difficult uh infrastructure that goes into this managing data sets, allocating compute, debugging the model training pipeline, and so on. And so for this section I will now turn it over uh to my co-presenter Avinash from AWS uh tell you about the AWS uh Sagemaker Aropot. All right. Thank you, Jim, for covering extensive details about ZOOCs. That's so exciting to hear, as well as discussing about the large language model challenges. I'm Avinash Kolui. I'm a senior solutions architect with AWS supporting Amazon and its subsidiaries on their cloud journey. How many of you here are building foundation models or doing distributor training, as well as fine-tuning with the large language models? I see a few hands raised. Thanks for that. I'm pretty sure you might have run into certain issues around the hardware. And you end up spending more time debugging and fixing the node failures. And guess what? When you do that, your capacity or the GPUs, they stay in idle time. Which end up paying in a huge cost. For these reasons, we have Sagemaker Hyperpod. It's a resilient cluster that was purpose built for distributor training as well as running your machine learning workloads. So that it can take care of the heavy lifting in terms of bringing down, bringing the efficiency as much as possible and reduce the cost. Let's look at the hyperpod's versatile environment. I'll try to unfold it and make it so simple so that you'll get to understand about what Sagemaker Hyperpod is about. At the very beginning, on the foundation layer, we have storage and computer. Well, as Jim mentioned, there's a resource optimization and resources extensively that are needed for a lot of large language models. SageMaker Hyperbot supports Cranium, which is AWS based GPUs, as well as Nvidia-based GPUs, which are P4s, P5s, and P6s. In fact, we have seen clusters the size of more than 500+ nodes running on SageMaker Hyperpod for distributor training as well as ML workloads build-up. And then comes the storage. Just for an example, today's with the Zooks, there's a lot of data that's being covered across the liars, cameras, and sensors. In fact, uh, in, in fact, it is petabytes of scale. So you need a persistent storage and a better efficiency storage mechanisms like Amazon S3, FSX for leisure, and EFS so that you can load this data directly onto the GPUs in order to do any sort of distributor trainings. Well, these nodes do not come with 1 or 2 in number because when you load this amount of data, you need multiple nodes, tens and hundreds of them, and when you do that, it has to be high performance computing and efficiency in order to make sure the data transfer speeds and the efficiency is at high scale because you have to load and train the data. So for that, we have elastic fabric adapter, which is a network layer physical device that is attached to these instances or the nodes of the GPUs to give you 3200 GBPS capacity for each and every node. In order to make sure you're running at a high performance compute mode. And then comes the runtime environment. So SageMaker Hyperpot supports two types of runtime environments now. One is NMedia-based, which is KUA libraries, and then it's AWS-based neuron runtime. Just right about the runtime, we have various set of ML frameworks and tools. I know this is something that might be interesting to a lot of ML engineers because at any given day, many ML engineers deals with many set of tools and frameworks. For example, Pytoch, Nemo, Megatron, as well as Cube CTL or Ray for job submissions. And one among my personal favorites at this this entire section is observability. I strongly believe that you need to measure something to improve, and having a single unified experience always helps in terms of getting the right operational excellence. So Sage Maker Hyperpod is easily integrated with Amazon Cloud Watch. As well as Amazon managed Graffana and Prometheus so that you can have one-stop observability section or an operational excellence for your workloads to make sure that you are monitoring and building the alerts on top of it. And then at the surface layer, we have the orchestrators. We have seen many customers using SLRM as well as Amazon EKS. All right, let's check what's Hyperpod sample architecture is. So, either you take EKS based or slum-based, this is a generic architecture, which helps you to understand how SageMaker Hyperpod really behaves. So on the right side, Sorry, on the left side of the screen, you're seeing the compute nodes. Typically all of these compute nodes are in the customer's, sorry, are these compute nodes are on the Amazon Sage Maker service account and they are mapped to the customer notes. In fact, The reason for that is the health checks towards the bottom of the compute nodes so that Sagemaker Hyperpod can take a control on them and make sure it's governing and guarding those nodes, and whenever there is a failure, it automatically replaces with them. So that is one least job to worry about for a lot of ML engineers. You could also see there is a control node and a login node. The control node is basically the head node, which is used for orchestration and also handling the jobs where your ML engineers usually submit the jobs. And then the login nodes are basically to protect the control nodes and carry on any sort of admin activities. So SageMaker Hyperpod also do come with a lot of built-in flavors. For example, use uh life cycle scripts. So the scripts that run when uh instance is booting up and make sure the cluster configuration and setup is correctly done. And at the same time, it also provides capabilities for the checkpoint mechanism. And we just discussed on Amazon Cloud Watch anyways, uh, the observability platform, it's directly integrated with it. All right. 11 of the primarily or the most important, uh, The pillar or the aspect of uh SageMaker Hyperpod is resiliency. So let's take a look at why resilience is important, and I'll try to break it down in such a way that it, it is easily understood. So on the screen you're looking at a 16 node cluster where you have 8 jobs, job A, job B, and job C. Let's assume that job A has taken 8 nodes and job B B and C has taken 4 notes each. So when an ML experiment or when an ML workloads are running, And you happen to find a failure of instances. It could be either because of a software or hardware. The usual software failures are something like hyperparameter misconfigurations or even missing a semicolon, and that's pretty usual in the case of an ML engineer life cycle that goes, debugs, fixes it, resumes the job, and picks it up, right? But when it comes to the hardware failures, and this is physics. It can easily happen It could be because of an overheat or some other reasons, and these are, so when that happens, ML engineers do end up spending a lot of time debugging it, but ultimately all that it happens is replacing the node, and when it, it doubles up from 8 to 16 to 30 and all. You'll end up spending more time in debugging and fixing these nodes one after the other. And that costs huge because you're having an ideal capacity or idle usage of your GPUs then. So here is a typical scenario. If the failure is of the model issue, it ML engineers usually goes and debugs it. We just discussed about it. But if it is with the instances, that is when you have to investigate, replace the node, and if it occurs again, you have to repeat the same process again and again. And this is where the Sagemaker hyperpod comes into the picture. The biggest value proposition of auto resume feature based on the health checks. And how this is happening is Sagemaker Hyperpod has a set of reserved nodes in this in the Sagemaker Hyperpod service account for the reserved category, and it automatically checks and replaces the instances so that you don't have to do that heavy lifting for you. And you can just focus on building the models. Uh, uh, we discussed on some of the capabilities, and here's a quick recap of some, a few more capabilities on SageMaker Hyperpot. Uh, resilience is something that we have touched base just now. And the next one is about the scalability. In fact, when you deal with hundreds and hundreds of nodes, EFA is something that really stands out in order to give you that level of bandwidth for all these nodes to intercommunicate and exchange the information. And at the same time there is a Sagemaker flexible training plan which uh Anando would be discussing it next, but this is also another capability for you to have instances reserved and have uh continued operations at times. So SageMaker Hyperpod also supports a versatile environment. In fact, within the stack that we have just seen, a lot of frameworks and tools that are supported. So this brings down that opportunity for ML engineers so that they can just focus on writing the code and building the models and least worry about a lot of frameworks and tools that are integrated. So the another one is efficiency. For example, with the integration of deep learning AMIs and the built-in scripts, you have a lot of frameworks and tools that are available so that you can immediately get your cluster spin up and started and build the models. And at the same time, the integration with CloudWatch gives you the capabilities of building an observability platform on top of it, so that you govern your resources and make sure that you're taking the right decisions at the right time. What's a typical life cycle within the Sagemaker hyperpod when you build? Large language models. Jim just mentioned that they collect a lot of data. So, anyone, anyone that building foundation models need to collect a lot of data. And once they collect the data, they need a mechanism to store, and this is where we have discussed about S3 and storing the data. Now you have the data with you in S3. So what are you going to do next? You need to load this data and bring it to the GPUs so that you can continue doing the model trainings. So once you have this data, you can load and then do the trainings. For that, you need an efficient mechanisms and distributor training mechanisms of your GPUs to continue on the training process. And towards the end, once the training is done, you deploy these models. With that, I'll hand it over to Anando, and he would be discussing about the implementation side on SageMaker Hyperpod and also the challenges. Thank you, Jim and Avinaj for setting the context, why, what we are trying to do and why we need SEMa Hyperpod. I'm on in the, I lead the distributed training efforts in ML platform at XOOCs. So there are many features in SageMaker HyperPod, but I will just highlight a couple that really caught our attention. The first one was we wanted to do large scale training, which means we want to do FSDP, HSDP, DDP. So we want a framework or an ecosystem where we can do it freely with our own tools. And we also want EFA to be enabled because we want to train large scale models, and it should, it should be across nodes and GPUs. And the, there are other features like checkpointing, which I'm going to talk about in a bit. The, the most important was uh training plans because since we are working with the Queen 3 VL or similar models, so we'll be using instances such as P5 and P6s. They are costly instances and we cannot get them on demand. So how can we guarantee upfront that we are, we are, we will surely get these nodes when we need them. So hyperpod training plan offers that flexibility. So once we decide what we need, uh, it is guaranteed that it is available. Let's Understand that whether we have hyperpower or not, what are the fundamental requirements of any training platform for foundation models? First of all, distributed training jobs. I should be able to run jobs on multi-GPU nodes sometimes like we have a vision to run jobs on 64+ GPUs and also run FSTP, HSTP, and tensor parallelism. Is there a way to do it? Sagemaker, Hyperpods. Gives us that opportunity and ecosystem. We can bring our own code, own libraries. They also have their own library. It's, we are not bound to use them. We can freely switch between the implementations. terabyte scale data streaming, um, Sage Maker Hyperpower does not mandate us to use any particular data loader. We can bring our own data loader and we'll talk about that. We use Mosaic data streaming for our data loading strategies, so this fits very well. It's a win-win situation because. We are using SageMaker Hyperport as a compute platform, but we are bringing our own data loaders and solution on top of it. So both systems coexist together. The Third one is production resilience. Jobs, our jobs typically run for 2 days, 3 days. If one of the node crashes due to a GPU failure, uh, it should automatically recover. The node should be replaced. So traditionally in our, in Zos we had a SLRM-based ecosystem. Uh, we started with the, uh, in that case, we had to manually come and, uh, restart the nodes. But SageMaker Hyperpot already gives the auto regime capability which we want to leverage. Unified observability, uh, when multiple engineers are training many jobs, we should have EFA GPU, and FSX metrics available, and we do not want to build all those observability in-house, so we use the Sage Maker hyperpods, uh, add-ons for observability to do that. And checkpointing, we, we, we want distributed asynchronous distributed checkpointing and once the job, uh, if any failure occurs, we should be able to resume from the last checkpoints and it should be automatic. We do not have to. There should not be any human in the loop to come and do that, uh, job for us, so we want to automate and so we looked uh onto Sagemaker Hyperpower and they already have built the ecosystem so the hyperpower ecosystem is really geared towards large scale training and they have thought about all these, uh, challenges and problems and they have imbibed them into the product itself so it was a natural fit for us. So, uh, Avinash told about the generic architecture. This is a Zooks architecture. It's very simple, straightforward, driven from there. On the center we have, uh, multiple partitions of P5 and P6N nodes. Again, Sage Maker hyperpod runs on a managed VPC. So all these compute nodes are in the managed PPC. On the left we have the ZOX VPC where we have ZO specific components. For example, we have FSX and other components. Uh, we, we have the FSX in our VPC. On the right side, these are additional components from SageMaker, uh, uh, apart from SageMaker Hyperpod. For example, we track our experiments in Comet ML and we also have cloudWatch integration. Uh, I will talk about it in a moment. So we talked about the architecture and the placement of the components. Let's look at the user workflow, like having, when we have this, how do we use it? It's, it's the user journey. On the center, which covers the green box is the FSX, which, which is like the heart of the system, and on the bottom we have the compute nodes login and slum controller, uh, we already discussed. But what the diff the, the, the beauty of this ecosystem is the FSX is a shared storage. It's mounted everywhere, not only on the compute nodes, but on the controller as well as the login node. So if I make any change. Onto the FSX file system, every compute node should be able to pick it up. On the right hand side, we have a section for model checkpoints and evaluation. It is not related to SageMaker Hyperpod. We do the evaluation once the checkpoints are created, we fire off the evaluation as a separate job, but to highlight the ecosystem that both Sage maker and non-Sage maker can coexist at the same VPC layout and same infrastructure setup. They both collaborate with each other without competing with any of them. So how is the workflow looks like? First of all, the user logs in to the login controller, uh, login node. And then, uh, step two is he creates a virtual environment. It's a Python virtual environment. Why we do that. We want to give users the flexibility to bring your own libraries. For each experiment, you can have your own virtual environment. So if you look at the folder structure FSX user 1 VNV, you can have VNV12 up to N. So you decide what you want to bring for your experiment and create the virtual environment on the FSX. Once created on FSX on the login node, since the FSX is mounted on all nodes, the compute nodes will naturally see those virtual environments as well. Then we get to check out a particular branch. Or a Shah that you want to run it on SageMaker Hyperboard, you can also make changes on the fly, on the training code, and submit the job to the SLURM controller. Uh, submit means we use SLURM batch. Slum controller takes it and schedules it at the compute node. This is where the step number 4 comes in. The distributor training job is orchestrated. All the health agents from Sagemaker kicks in, monitors the job. If any node has to go down, it will replace that node and make that job into a pending set up and re-kick off that job when a new node is spin up. So everything is orchestrated by SageMaker Hyperpot here. Step 5 is the output of the model are written as checkpoints, and step 6 is, after the checkpoints are written, the, we kick off the evaluation jobs. So this is our entire uh workflow, uh, how it exists right now. So now it's time for, to check a little bit what are the techniques we try, usually. Uh, to scale the model training process. Just to uh address the training processes, we use, uh, mostly we use HSDP, you know, a hybrid charter data parallel. So because we have a partition of P5 and P uh P6s, we can apply DTP across nodes, but FSDP in inside the nodes. And some of the models are too large because we are working with Queen. Um, they come in multi-billions, so we have to do the shard, the matrix multiplications as well. So you apply tensor parallelism too. And then the third thing we also apply some training, other training optimizations such as use BF 16 to make the computation much faster and memory memory uh uh consumption uh even half than the flow 32. And then gradient checkpointing to increase the batch size as well as we use torch compile to compile the torch graph. And the scalable hydroput is governed and or dictated by the EFA fabric that governs across nodes. Uh, we have been talking about training, but we think a little discussion, uh, primer on the data plan is also critical. Uh. Because we use our custom MDS Mosaic data streaming, that has helped us a lot in achieving high throughput and efficiency because direct streaming and local caching is a feature like a a native feature of Mosaic data streaming. They, they are aware of the node, the topology. They understand what shard is to be needed or downloaded on a particular GPU or a node. And then we also have a deterministic sample, uh sampling and resumable iteration, which means that if a node dies or a GPU crashes, and when it restarts, the mosaic data streaming allows mid-epoch resumption. And it also maintains a global state of the samples that it has seen, so that when it resumes, it knows to skip the samples that it has already seen and only uh go through the samples which is unseen yet. And we also uh MDS also supports batch prefetching because we are working with video data. As, as the current as the GPU is working on the current batch, we need to prepare the next batch of data and uh we have complex preprocessing uh logic there so we need to prefetch them so that the GPU is not starved. Uh, challenges every. Every system goes through a series of challenges when we implement it for the first time. I will only talk about two which are broadly applicable, the first one and the, the last one. The, the other three are very specific to Zooks. The first one we started with, uh, Docker and Pike is an en route combination to run Docker images on SLUM. It was quite complicated because we have to first of all make a docker image to run it on slum. Developers could not iterate. They have to make docker images many times. So it was uh hindering uh velocity. The last one is like, when we started having more P5ENs and P6 ENs, we did not do a good job initially calculating the CIDR or the IP ranges, but P5 and P6 comes with EFAs. Each EFA consume one IP extra per network card. In addition to the IP that is consumed by the node itself, so when we when I mean it was a good problem when we started to grow having more nodes and more training, we ran out of subnet IPs and nodes are not available, so we had to redesign our CIDRs and VPCs and things for to fix that. Other things like enabling certificates and con uh connectivity to get it's like. We have Zooks. Inside Zooks, it's very secure. It's uh quite complicated. The SRE set up and we just cannot connect to any RA system just from any node. So it has to be properly secured. We have to install certificates on all compute nodes. Here also Sage maker's, uh, LifeCycle script came to rescue because we can add. A life cycle script when the node gets created, so we can add our Zos specific logic there. It may be applicable to you, may not be, but in our ecosystem, we had this challenge and we had to fix this. Another part is like. Well, Sagemaker already comes with so many features. Can we build our own? Of course we can. So here is an example like. We deployed uh NVDR DCGM exporter on each compute node because we wanted to see the GPU matrix that's coming out of these nodes. So we, we built our custom exporter, we created our custom events, and we created our own cloudWatch matrix. And once the matrix were available, we could create CloudWatch dashboard. Here I'm showing uh GPU utilization for P6 and P5, but the point to drive here is. Well, we use Sagemaker hyperpowered, but it's still extensible. You can customize it to your own needs. You, you do not need to use each and every feature. So this is very important. Like, uh, we are in a transition phase now. We started with SLERM because we had experience with SLERM cluster before. We have our own, uh, high-performance compute system in SLERM. So we initially started with SLERM so that the learning curve is low, and we could bootstrap and unblock our science team faster. But later, we are now currently moving on to the EKS orchestrated SageMaker Hyperpod. And the reason we want to move there is uh because of more flexibility and uh scalability. And one of these examples is, these are uh metrics that comes from one of the transition that we are making a real job that we are currently trying to test out on EKS. And we want to show, uh, in the previous slide I showed you, uh, custom cloud matrix, but this time, we don't want to build anything in-house. We want to leverage what is provided by SageMaker Hyperpod itself. It boils on the question that whether you want to build it or reuse it. Do you want to focus on building observability by yourself, or you want to focus on using the observability to fine tune or make a better decisions what should be your bat size? Are you using the EFA correctly or not? For example, if you look at the EFA metrics, um, EFA RDA read. Uh, rewrites. What, what does it show that do I have EFA is supposed to be used with RDMA. If you use RDMA, then the communication between the GPUs is skipping the CPUs and other kernel, uh, levels. If the matrix are high and stable, it means that we are using EFA in the right way. If it is low. We need to fix it. Probably there is room for improvement. Similarly, EFA received bytes. Are the GPUs receiving bytes at a standard steady, higher level? If it is yes, standard, which means that everyone is working as desired. If there is a sudden chop or drop, which means it's wrong, something is wrong in our setup. Similarly for GPU we know about GPU memory used, it's pretty straightforward. Am I using, consuming the full memory or not? Uh, if I'm not, then I need to increase my bat size. Another one is a tensor core utilization, why it is important. Well, I mentioned that we also use BF 16 data format. If we use BF 16, uh, if the nodes come with tensor cores, BF 6, that it can be, the tensor cores can be utilized to do the BF 16 computation. And uh if you do BF 16, it is half of the um uh the speed is double and it's half the memory. So having this tensor core utilization gives us the confidence that, oh, OK, so we use BF 16 and it is really being implemented and used. If the graphs are low, there is something wrong we need to fix. So the fundamental point to drive here is. We would not be building all these metrics by ourselves. It's already part of SageMaker as um we should focus on the business problem at hand that we want to train large scale models and efficiently and Sage Maker Hyperbot helps to do that. So I will, um, so this is a slide which is pretty much self-explanatory, like what was earlier and what was new. In our existing setup, we did not have a good experience with EFA and RDMA. With Sagemaker hyper because we used managed instances, installing EFA is like a 4 to 5 step process. So you have to install EFA installer, AWS Lea Fabric, then AWS OFI Nickel plug-in, CUDA drivers, the versions need to match. There are many combinations where we can go wrong. Instead of focusing on doing that ourselves and where we can go wrong, we just reuse what SageMaker provides. They come with everything already working. We, we focus on using the technology to improve our models and efficient and train more efficiency. So that's why we have the GPU utilization also bump up to 95% after using SageMaker and. Because we are using EFA and RDMA, we get, uh, almost linear scalability with the multi-node setup. We were training somewhere around 400 million parameter models, but now we train almost regularly 7 billion and we are also going towards 32 billion parameter models as well. And the recovery time, as I mentioned, that we want to reduce the human in the loop. We do not want, uh, errors to be there and uh let's say if the error was there and the person who was running the job, it was in the night, he comes back next morning, we lost 6 hours, 7 hours of P5 or P6 times. These machines are costly. We cannot afford to do that. So Sagemer Hyperpod is geared towards, uh, first of all, saving, uh, first of all, detecting these failures efficiently and quickly. Secondly, they resume it, uh, as part of the product feature. And, and the third is, like, we can also integrate these alerts with our observability. OK, lessons learned. There are many, but I will just highlight 4. for simplicity. Use data loaders and data sharing everywhere and don't download everything on your disk. Do streaming processing, leverage HSTP the best you can, and then you go to uh tensor parallelism, but try out the HSTP gradient accumulation, uh, Pytouch compile, and other, uh, other techniques first. And try to run on EFA enabled devices. Currently we do not have multi AZ infrastructure. We only run on USS2. We have to graduate and grow from there to run on across regions. It is hard to get nodes in one particular region, but if you enable this multi-region, it is easier to get compute nodes across regions. Yes, and the 4th point is. If you don't see what you're doing and if you have no clue how things are performing under the hood or in your bag, you just cannot fix it. So you have to have a greater visibility and observability for sure. You should be matrix driven. This is a quote for uh from our director. Um, uh, I will not go through it line by line, but fundamentally, SageMaker Hyperpod has unblocked large scale training at Zooks, and we are using it like to, to full extent and we have like great visibility into the utilization and performance. Road ahead. What are we going to do? So we are not complete in the journey yet. We're just somewhere in the middle. We started 8 or 9 months back with slum-based ecosystem. Now we are transitioning into an e-case-based ecosystem. The reason being. Kuberities is quite popular. It's very flexible, it's very scalable, and there's lots of open source framework, uh, built on top of Kubernities. And we can uh utilize them along with Sagemega Hyperpod. We also want to transition to SageMaker Hyperpod training operator, which is very customized for Hyperpod. We currently use QFlow Python job operator on the EKS. Uh, that, uh, HPTO also gives us, mm, log, uh, sorry, regular, uh, regular expression-based log scanning. For example, If your job is not progressing fast enough, or if the validation loss is not uh decreasing, you can catch, you can have expressions that will detect job hang or like your training is stalling. Implement task governance. Currently we have a like a first come, first serve basis and we make a like a manual roster like who will be using which machine. We want to move away from that. It should be. We should have teams and teams should have compute quotas, and they should be able to lend and borrow between teams. For example, Team B has, uh, there are so many jobs running in Team B, they have consumed all. Now I need one more job to run. Should I just sit idle? No, I can ask Team B, uh, Team A, can you lend me some resources for the time being so that I can run my job. So task governance of sales maker, sage maker hyperpo will give you that feature, and we want to leverage that. And the 4th point is we want to continue leveraging FSTP HSTP, and uh do more experiments. So those are technical. What is non-technical requirements on the road ahead? Which is not um the non-technical is can I ask for any resource? Can I ask for the entire cluster? Can I ask for all nodes in the cluster? No, obviously no. So we need to implement guard rails and resource quota and we have name spaces so every user or every name space should have some. Uh, restrictions so that we do not, uh, go with the runaway clusters. Everyone is asking for everything and uh we will be having the DDS attacks. Uh, expand to multi-region, which I mentioned that it's in our radar, which, but we have not implemented it yet. And Zooks has its own log monitoring system. Sagemaker Hyperpower does not mandate or does not have an opinion. How should you manage your logs? Logs are created from slum jobs or EKS jobs. It's the user's responsibility to have a system on top of it. So we already have an established robust system of monitoring logs. We will bring that, uh, system into the SageMaker Hyperpod. Uh, similarly, I showed you some matrix on Grafana. Zooks has its own observability stack, their own grafana, own Prometheus. We will integrate that. So as you can see. We are leveraging Hyperport for the uh compute nodes, orchestration, recovery and resilience. For the other things which are already in XOs, we are just integrating them and plugging it, so both ecosystem coexist together and play well together. So all this talk, my previous two authors and me, ultimately culminates to this. We want to build the future of transportation. We want to build a robotaxi, which is, uh, which should be uh should be safe, who should be, uh, it should be reliable, and it should be scalable also. And in order to build such a system, we need better models to drive the robotaxi. And in order to build such models, we need a better infrastructure to drive building those models. So that's why we have partnership with AWS on SageMaker Hyperpod. If you want to learn more about Zos, please visit zos.com or um we also have a one Amazon Lane on Caesars Forum. You have been hearing a lot of innovations in, uh, in all other uh aspects as well. You can come and try out hands on each of these innovations in this one Amazon lane. It's on Caesar's Forum. We also have a Zooks booth in the Amazon lane. You can check out the robotaxi, uh, get into it, take pictures, and yeah, just come talk to us and, uh, love to explain more. Yeah, so thank you for coming to the talk. This is the 3 of us representing the talk, but understand behind these 3, there are many more people who have supported this infrastructure and, and they're, they're working in the background behind him. Thank you all.