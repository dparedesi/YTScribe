---
video_id: _akY6uISm4o
video_url: https://www.youtube.com/watch?v=_akY6uISm4o
is_generated: False
is_translatable: True
---

Welcome. Uh, we are here today, uh, to talk about Nova 2 Lite. How many folks here have tried out Nova 2 Lite since its launch, like less than 7 hours ago? OK, you didn't leave the keynote and do it immediately. This might be the first workshop where we do that. Um, I'm Pat. I'm a solutions architect with the, uh, the, uh, WWSO Bedrock go to market team, and I've been working for the last year with Gene, uh, on Nova. And my name is Gene, so I'm on the Amazon AGI team. I am a senior solutions architect, uh, so happy to be here with you guys today. Yeah, and I'm glad the badges work too, so congrats on getting in. Um, we're gonna cover a lot today, uh, and we, we have a, a, a, a rough agenda, but if we're doing this the right way, we're gonna be doing a lot of coding, uh, and unfortunately some of it will be mostly code review given the difficulties here, but we're gonna go through a lot. Um, when Gene and I were first planning this, we had a very tangible, uh, bite size idea, and we quickly realized this is a huge problem. Uh, so to solve this at scale, to build a fully autonomous coding agent. Has a lot of nuts and bolts we're gonna talk about all of that, but I wanna set the right expectations here that um we may not get through all of it, but what we want you to see is how you can prompt Nova 2 light, the major considerations to make if you're gonna do this at scale, uh, and any um. Any of the any of the stuff Gene's going through or I'm going through, make sure you ask questions. We want everyone engaged. I know it's a large audience, but, uh, that's always the best way to do this. Yeah, a question, show of hands, how many people have actually done a code talk before? Are you familiar with the the format? The goal for this is we're gonna be in the code and we're gonna be very interactive. We wanna be talking. We wanna hear, you know, the questions that you have as you have them. This is not a 10 minutes at the end Q&A. This is a, you know, conversation we wanna have with everybody, um, so it might be a little bit different than all the breakout sessions you've gone today. It's really, really scrappy. We're gonna be in the code and we're gonna be talking about the model. Cool and I have Leon and Cesar who, who I've volunt, uh, I can call on, which is great. Does anyone else wanna be called on? I figured that was the answer. We'll get you talking in a second. Don't worry. So let's talk about Nova, um, Nova 2 Light. So Nova 2 Light was announced today. It's a large context window reasoning model. Uh, it comes with built-in support for our system tools. You'll get to see some of that at work. Here, um, it's our next generation of light, uh, we've also announced Pro 2, and Omni that are on the horizon in preview, but, uh, today we're using Nova 2 Light exclusively and we're gonna walk you through how to use reasoning, how to set budgets, and how that impacts, uh, co-generation. You didn't want either? No. Um, so where Amazon Nova 2 Lite fits in, uh, like I said, it's got a 1 million context window. It's actually got 64,000, uh, token output, so, uh, if you are using reasoning, you will eat through some of that, um, but the output, uh, of the model we've seen perform really well. Oh sorry, I'm on the wrong screen here, um. And it supports uh all the languages we've supported, uh, thus far uh we're seeing it do really well at translation tasks uh you can use it to fine tune we'll talk about why you might wanna do that, um, for, for code generation use cases, um, and yeah, we'll go ahead and get started in the in the code. Gene's gonna get started, uh, on building out this coding agent. Feel free to follow along in your notebooks if you want. Awesome, yeah, you wanna switch me over to, OK, OK, nice, we'll walk through some of that today, yeah, um, another question, this can just be a show of hands. Is everyone familiar with strands? OK, some people are. I'll give a little bit of an overview, but I won't, I'll make sure to cover what's important here. What about MCP? Model context protocol, nice. That's a good amount of the group. Everybody's, has anyone built an agent using an MCP server before? OK, pretty good. So we're gonna be using MCP, uh, today as well, uh, to talk through this. So to set the scene for where we're kind of gonna start and how we're gonna build, um, we are gonna have a GitHub issue that we're working with. The problem, it's a little bit meta, but, uh, we're, yes, question. Yes, I absolutely can. OK, so I am in. A repository, um, so lane chain AWS, so Lane chain, we have our own, uh, dedicated kind of sub repository for the AWS integration. Um, new model was released. We wanna make sure we're integrated well and that we support all the new features of the model. This is kind of our meta pro uh problem we're gonna talk through today. Um, so starting with an actual kind of GitHub issue in, uh, using a real repository, right? We're not one shotting an entire application. We're in a real repository with real code that serves a real business function. And we're gonna start kind of iterating on, you know, what it looks like to actually interact with that, how do we get the model to understand the context of the space it's working in? How do we get it to solve real business problems in a space that, you know, has been around for a while, it's, you know, it's a real code app. So I'm gonna be using strands, the, uh, kind of high level if you're not as familiar, it's kind of like Lang chain except it's our version and has really great integration with AWS services and tools, so it means that our models and kind of other uh things that we've been building on AWS, they all really play very nicely together uh and so I'm gonna be using that today uh to. Supercharge really get started with building this agent, uh, the nice thing is you see with like a lot of these agentic kind of, uh, frameworks that you can use, they do a lot of the code pieces for you, so I can get an agent going in a couple lines of code. And what I wanna start with is just what happens if I prompt the model, right, with, you know, look at this issue and comment on the issue with your implementation plan. This is extremely vague. I'm not giving any context at all. Um, all it has is the GitHub MCP client. Now We're gonna clear this out And I'm going to spoiler, I will be going to a different script to show you a little bit later, um, but we're gonna run this and we're gonna see what happens and I have some boilerplate code that should be, um. Making this a little bit nicer, and I'm gonna zoom in cause I'm sure you can't see it. It's not as pretty when I do that, I apologize. Um, but what's happening here is the model is starting to call tools. So the GitHub MCP client. If I go and just open up. My python I wanna make sure you can see it. I'm going to. Import The client and we're going to initialize it just in a Python execution. And the goal here. Is To show you what the model is seeing behind the scenes. So We put a one line prompt and I initialized a GitHub MCP client. So GitHub, right, they provide a couple ways that you can use their MCP server. We're just using their remote URL one, and we are, you take 1 15 seconds to explain what the MCP is? I'm still, uh. Yes, absolutely, yeah, so MCP stands for model context protocol. It provides a uniform kind of standardized way to interact with arbitrary, um, servers and things like that and to allow agents to really build with them very easily. So instead of you having to go and implement every single API that's gonna interact with GitHub, you can use their GitHub MCP server, and now I can go get issues. I can, uh, make changes to files. I can create a branch. I don't have to implement any of that myself. They've kind of created this standardized interface for me. And and GitHub released their own MCP. Uh, many companies are, uh, it, it, we, we're gonna see how you can use it, how you cannot use it, uh, but it's gonna be one of the first tools you're gonna wanna consider when you're rolling out a coding agent of any sort. AWS also has MCP servers for like all the announcements today will go into our MCP servers so if you're building agents that need to use the latest and greatest, you can make a tool to go look that stuff up for you. But OK And you watch, OK, so basically if anyone was really watching me stumble for a second, um, when I am interacting with the GitHub MCP client, I, it took me a couple lines of code, and this is all of the tools that become available to me. I can get a comment, add a comment to a pending review. I can get team members. I can get a commit, um, I can assign a co-pilot to an issue. Maybe we don't even wanna do this code talk today and we'll just say get a co-pilot, go do this for me. Right, and so that you have these standardized ways that you can um start interacting with GitHub. But what's kind of tricky here is this is really nice and this is really easy for me to get started, but I did a couple lines of code and now I'm passing my model 40 different tools it has to choose from. That is a lot of context. The model immediately has and has to, you know, decide what tool to use, um, and there's a lot of kind of things that come into play here, where it's important to be intentional, it's a very nice thing, but. It's easy to um get things a little out of hand just to begin with. Because remember the model, the model gets passed like a giant tool config, right? And the model has to decide of these tools which is the most relevant for what I'm trying to do. So if you give it 5 very well selected tools, it's gonna have a better chance. You're not gonna eat as much context window. If you give it 20 or 30, you're really making a big bet on the model's ability to traverse long context and pick the right one. Nova 2 Light does it wrong. Just gonna say, uh, but what she's trying to say here is. That's a consideration to make, yeah, and I think you'll see a lot of guidance for like all model families and once you get above 20 tools, they usually recommend breaking things into sub-agents, exploring multi-agent architectures. It's just, it's a kind of a really important consideration that you wanna make, um, and what you definitely need if you're gonna do that is a good system prompt to give some guidance, right? I didn't have any system prompt. I just said go analyze the issue and I am not gonna lie to you, it did OK, right? It. Uh, read the issue, it added a comment, it made an implementation plan, uh, and I'm gonna even, I would like to. Pull up the issue, make sure my Internet, let's see what it said. Right, I'll implement support for this. I'm gonna, um. Make a couple of steps, this is all pretty reasonable, right, with no instructions, just providing the context of the tools. It did something that I think is like pretty fair. What might stand out to you is, this is all very vague. It didn't actually read any of the files, right? It didn't actually go and start exploring the repository, it read the issue, made a comment, decent plan, but it's not what I want it to do, right? If I'm gonna build a real agent, I want it to look at the files. I want it to actually be interacting with what's real. So what we can do next. So if I go back To my strand agent, I wanna use this new template, and I'm gonna take a second here to talk through just some best practices with Nova 2, and I think a lot of kind of complex agent prompts that you'll want to build, um, overall, and I'll I'll zoom in. You need more than just a simple prompt, and the first line might look pretty familiar. We're providing a role. You do this for like every, uh, agentic application or any LLM application, um, but we need to take it a step further. So a couple things that you know we have trained our models to be good at are these specific sections that you can use to draw attention to specific pieces and workflows and business processes. So the first one is core mandates. This is, you know, the important business rules that you're, you need the model to follow, um, so one, right, I don't want you to update the original issue. Don't edit it, just add a, add a new comment, um. Another one is the actual workflow you want the model to follow, right? I want you to get the issue, but I want you to explore it. I want you to, uh, go and look at the files. Um, Nova does really well when we pass the tool names that we want it to use. Um, it really draws attention, right? It links to the, the tool configuration it's seeing, uh, and then I'm having it return the plan, um, in a specific format. I have in the output formatting. Output formatting does really well when it's at the end of the prompt. Uh, the next section is tool usage. So you might wonder, OK, we're passing it, the tool configuration is gonna go to the model. What else would we really have to specify? Uh, and tool usage. Is where we want to put the important business rules for us for how we call the tools. So MCP is great, it provides a standardized interface, but how we, sorry, how we want to use the tools is gonna change based on every single use case that you're gonna have. So this is where you kind of bring into, you know, some of the, the specific things that are the most important for you. um, I always wanna make sure that we're reading the issue, um, providing some kind of tips around how to interact with the file paths. I'm encouraging parallel tool calling. Parallel tool calling is when, uh, we want the model to call multiple tools in parallel instead of waiting for one after the other, so kind of encouraging that behavior. Actually, what I wanna pause there for parallelism. Is this, is this a parallel invocation of the model that's occurring when you're asking it to do that, or is it? So what'll happen is the model will, uh, in one single assistant response it will have two tool calls in it. And so this might come up and where I would hope to see it. If it's necessary, is, you know, let's see the contents of multiple files at the same time. Awesome, uh, so that's kind of a behavior that I'd like to encourage, uh, and then. The next thing is around error handling, right? It's very nice to assume that everything will work perfectly all the time, but when you're using MCP servers, when you're using tools and agentic workflows, things break a lot, right? Something's gonna time out. Uh, I might do, I don't know, something else wrong, right? And so you just wanna start to, uh, kind of that's the focus of that section is how do we want it to respond. If I, uh, time out, do I want the model to try again? Do I want it to, uh, respond gracefully and give up, fail quickly, right? So this is kind of where we would start to think about that section. So I am going to now run. This, again, I'm just gonna pass it in the system prompt here, um, no other changes necessary, right? I'm gonna keep the prompt exactly the same. And we're gonna run it again. And we're gonna look and see how the model changes. What I'm hoping to see, we still should read the issue. Uh, I'm hoping the model will start actually looking at the files, right? I'm hoping it's going to actually start kind of exploring the repository. And fortunately, that is what I'm seeing. Something you might notice though. It's going to be a little bit slower than it was the first time. Why is that? Right, it's gonna have to make a lot of tool calls because it doesn't have any context about what this repository looks like at all. It has no idea. So what is it gonna do? It's gonna guess or and then try to like start exploring and figuring this out and. A lot of models are very good at that, and it's, you know, something that if I am doing my own kind of day to day job and I wanna connect to arbitrary repos, that's fine. But if I am building a coding agent for a repository I know it's mine, we can take advantage of that, right? We don't have to make the model explore by itself all the time. You can see like this is just. Slow, and I don't blame the model at all. It's gonna be searching for a repository and exploring and iterating. It's fine. So this is where we get into this, this, uh, optimization opportunity for context. So the Nova models, um, like Pat mentioned earlier, support a 1 million token context length. So, I wouldn't blame you if the first thing you thought about was, why don't we just dump the entire code base into the model context. That seems easy, right? I'll pause here because we got the output, but you can see like 8 tool calls. It did great. It read the it read the issue, uh, it explored a lot of um different files, it added the comments, and I can even like we can see here the quality that has improved just on our system prompt. Like we're actually looking at specific code changes we want to see happening. It took a long time to get there, but it got there, and now we have real code that is um actually gonna be usable and uh relevant to our own repository. But I wanna improve on this because I'm not a very patient person and I think that we could do it better. So, real, real quick though, any questions on what Gene just went through there, uh. Yeah, yeah, so when you've got. Yeah, I might have missed it, but you specify it. So That's a great question, and it's actually gonna be one of the optimizations we're gonna make a little bit down the line. At first, I am just passing all of the tools. I'm passing all 40 of them, and you can see, like, like Pat mentioned, the, the Nova models, we like tune them for agents. It does well with 40 tools. It did the right tool calls. I'm like happy with this, the decisions it was making, but. That's a lot of tool calls and the I think the, the first prompt where I just have one line in all of the tools, it adds immediately uh over 8000 tokens just in the tool config. Which is helpful context, but it's a lot and it's unnecessary and so that's one of the thing the steps we'll take is kind of shortcutting that and being really intentional, yeah, and the, the quantity of tools is actually a good place to start if you're trying to now make multiple agents, uh, because then you can compartmentalize your tool use is associated with the behavior you want out of the agent, um, so it's a good place to start if you're moving into to multi-agent architectures. Did someone else have their hand up? Yeah. So I'm using GitHub's hosted MCP server, um, which everybody, so they have one that you can self-host. It's just a docker container you can run it locally, um, but you, they also have one that's available just through an API endpoint, uh, so that's what I'm using today. Um, definitely recommend if you try this at home and you make your GitHub token, be smart about the permissions you put on your GitHub token. I've done some crazy things, so, uh, learn from me. Um, we're gonna, we're gonna push to the Lang chain repo today, right? Just kidding, we're not gonna do it. Um, so I. Yeah, you got suspicious when you asked me not to ask. Yes, I did. You caught me there. And then one time I wasn't following my own like the our own security best practices and it was starting to uh push to the main repot and I was like no. And so I made a protected branch and then I was, you know, intelligent, um, but it's fun to play with this stuff and see what sees what happens. It's fun to break things. So and tools are an implicit security boundary too. Sorry, I know you wanna move on, but like if you don't want an agent to ever do anything associated with a tool, you don't give it the tool, uh, and when you're first iterating on things like we're doing here, you'll, you'll find those rough edges really quickly, yeah. So I think there's some. I guess I, I, I went a little bit on a tangent there with the, the tools and everything that was happening, um, going back to the how we take advantage of the context length. Um, I do wanna show what it happens when we pass the entire code base. I think it's a very natural, uh, path to take it on, especially when we have repositories that can handle that much context. And, so I have a formatted, I wrote some, you know, a script. I dumped all of the files, um, I did, I'll call out, I'm using some of our best practices we have for how to format for long context, so I'm gonna be breaking up, um, the. Uh, the documents in a specific structure, um, I will call out, I, um, did not do something very nice to the model which is instead of passing the path that is gonna be in the GitHub repository I use my local file path, so not really making it easier, but, um, at least providing some context right to the files, uh, and the, the content in the repository. So what we're gonna try to do now is I'm gonna. Actually, uh, put this in the prompt, and I believe I, yep, I called it, uh, code base. Next. I'm gonna say format repo content and we're gonna just put this now. At the top Which is another best practice for a long context. You'll want to put that at the top of your prompt. And we're going to. Now run it. And we're gonna see what happens, and I'm gonna call something out that you might pick up on really fast. One, it is a lot slower. We're passing, uh, absurd amount of tokens through. Um, I was smart. I ditched the UV lock and some other stuff that just threw it right to the context length. It's gonna be about 500,000 tokens, uh, but that's gonna be on the higher end right of our token context length, um. And There's a lot of papers around this topic, so we can take advantage of the context length. Uh, we train for long context, uh, but something that is kind of seen across all models is performance of the model degrades as the context increases and a lot aligned with that when we do a long context, a lot of times it's usually best when we're looking for something very specific, right? It's a needle in a haystack. I wanna go search for something in this long, uh. A huge amount of context models are very good at that, but what's important for when we're doing code is semantic connections and how are things playing together and that you don't get as much when you're just dumping everything into the context and so the benefit. It doesn't really, uh, of having all of it. Uh, it, it's a little bit tough, right? What we're probably gonna see is still very good outputs. It's probably gonna call less tools, uh, but it's a lot slower and it's gonna cost a lot more. And we've, we've seen customers now with, you know, navigating the dependency of a, of a software project take this in a, a, a whole direction unto itself, building a graph of dependencies, uh, so you can look up dependencies using, uh, a query. Um, there's a bunch of rag efforts that can go into making this specific part of a coding agent more efficient. Uh, but yeah, hopefully you get the results I don't, yeah, it, um, we'll notice a couple of things, right? Less tool calls, so that's kind of nice. I had to do less steps, you know, maybe that saves us a little cost, but I'll call out what stands out to me when I see this immediately is that it didn't add a comment like I asked it to, right? The, the performance of the agent decreased with all of it, with, uh, with all of that context and the quality of the code. It's still great, but for agentic work flows like there is a big trade off here and you wanna be like really intentional because of this about managing the context and what we see a lot with coding agents is you can run through your context so fast and you don't want to. Basically shortcut that by adding a bunch of context at the beginning. You want to allow for the context to build up naturally and have more proactive compaction strategies throughout the, the period of the agentic workflow, right? We don't wanna just kind of, you know, basically handicap, right, put the, uh, model at a disadvantage just from the beginning. How many folks are using codesist in their IDEs? Just any sort of, oh, you're willing to admit it. That's awesome. I, I do too. Uh, so we, we, what you noticed a lot with Quiro with, uh, with like Continue with any of these plug-ins, they're very surgical about how they look at files. So they'll they'll, they'll go search for something in a file and they'll take the line numbers and they'll put that into the context, um, and so, uh, just to highlight what Gene's explaining here, that's for context management, uh, they're getting very specific about the things they put into the context window, yeah. So, Um How does it not and the whole I was able to search and look. So. So what I, my proposal to you all to do and what you'll see if you really get into the nitty gritty of how a lot of these like coding assistants will do is. Well, they will use the repository structure in the context instead. So, one of the benefits, right, of the fact that we as humans have been working in code bases for so long is that we try, hopefully, to design them in a way it's intuitive and easy for us to navigate. So now we have the ability to just take advantage of that. It's designed for us to be able to navigate it. I hope if you're lucky, I've, you know I've been at companies where maybe we're not as good about that, but. You have, you know, human preference, you know, tuned file paths that provide context out of the box for what is probably gonna be happening in the file, and you can provide that to the model, and it gives it a starting point of where to look. And so what I have now is a separate. Text file that has all of the file paths in the entire repository. I'm not at this moment providing any additional context to it, but I'm providing the file paths. And you can see here, Like the, I think the the thing I called out in the, in the issue itself is add something to the chat bedrock, uh, converse class. We can see here we have this file path that says bedrock converse. It probably is going to give a hint of the model that oh, that might be a really good place for me to start looking. And so if we. Switch this out now and we just instead pass this new text file that has the file list. And try to run that. We should see a couple of things, ideally, the Wi Fi holds up, right, it's going a bit faster, uh, we should still see a pretty high quality output, um, from the model and so this kind of really easy tuning, to be honest, it wasn't a very hard script to write, um, and so this helps us kind of very easily provide context to the model and while that's running, I will show the fact like the next step that you can take it. Is actually sorting the file paths and stuff and if you are, you know, are willing to put the time in, this is another way that you can be a little bit more intentional about providing the context of the model, uh, and a lot of, I think, uh. And what I've noticed a lot of repositories that want to be really easy for agents to work in, they'll provide this kind of stuff, uh, in the, in the, you know, agents MD file and things like that. And so it's being a little slow, I will say. I'm gonna blame the Wi Fi, uh, but so far we've only had two different tools that are gonna be called, um. We're getting another file contents and so we'll see what the output looks like and what I'm hoping for right is we'll get a high quality output, um with the implementation plan and and so one thing that to keep in mind because uh agents in particular are token hogs right? we know they use a lot of tokens code bases will eat through those tokens faster than you can blink an eye so. Um, the other reason to be conscious about how many tokens you're using in your time to this window, how good your tools are, is because that's a, that's a measure of cost in the long run. So if you're, if you're running this agent for every single issue that gets submitted and it's running autonomously, you know, there's a consideration for cost. Uh, we'll see the output tokens here, but it's considerable. We're only doing a pretty, a pretty straightforward, uh. Yeah, yeah, and so. We can see the output now we had a significant reduction in the tool calls from the, the first one where we provide no context to the repository. Um, we also in comparison to the one where we're passing the entire code base, right, this is a lot more reasonable on the input tokens that are going into the model, uh, and then the quality still of the output. That we're getting is still very high. We're still going to be, uh, making very specific code changes that were targeted for this, the files that will, you know, have the impact that we want for the, the GitHub issue. So This is all good things if we're thinking about a, you know, autonomous coding agent in terms of planning, right? We're gonna do a long horizon coding task. Uh, we want high quality planning at the beginning, right? If you're using a coding tool, you wanna plan and then you act. It's the same when you're building it yourself. Uh, we put a lot of time into, uh, building a high quality plan. This is only so helpful because at this point nothing's actually writing any code. I still have to go implement the plan, right? We want to build on this and we want to actually build something that can, uh, take actions in the code base. So The option for us, we have two basically we can either continue to build on the current agent that we have, or we can start to expand upon a multi-agent uh implementation strategy now. There are a lot of cons with a mono agent architecture here, and I, we're being really repetitive, but a lot of it comes to the context. We're gonna put all of these tool calls and context, uh, just to create the plan. All that really matters from the plan is the output of the plan. And then if we build on top of that we're still tracking all of the tokens that we use to get to the plan to then act and actually build it. So what you'll see in a lot of the coding agents and kind of the direction we wanna go now is a multi-agent architecture and. We will start with. Using uh strands still and we're gonna implement it using a graph-based approach where where we will have, uh, a, a planning agent and then an act agent. Our plan agent, right, it will do all the tool calls to look at the files and determine what needs to happen. And then that output will go to our act agent. The act agent will take the the implementation plan and it will actually start iterating and writing the code. So we're the direction we've been going so far. We might, uh, you might assume we're going to, uh, use the GitHub MCP. So one thing you might pick up on and I'll call out is when we use the GitHub MCP server, every single file, uh, every single tool call where I'm. Getting the file contents I am getting the entire file contents from their API. It's a lot it's quite slow, um, and then if we, you know, do the create update changes we're also outputting the entire file. Through a tool call back to the server, so it's very, very slow, uh, and. Not necessarily the most practical way to do it, um, but, we'll get to that a little bit later, a little bit of a teaser, um. So we'll try it out. So I have two agents like I said. Um, I'll just show very quickly in strands if you've done a graph, uh, approach before, um, what you'll do is you'll create this graph builder, um, you'll add your nodes. And that's really it. uh, it's quite easy. Is everyone familiar with like the, the basic agent orchestration patterns? Uh, I'll go over those really quick if, if that helps. So, so the most ubiquitous pattern right now in agents is the manager agent and the sub agents. The manager agent dictates when and who it calls to get work done and it decides when the work is done. Um, absolutely a thing. Uh, there's also graph agents you can think of it as a, uh, a left to right graph in our, in our case for coding. Uh, where every agent is a dependent on the previous node, they call them nodes, um, that's also, uh, there's different names for it. Curry I has its own terminology. Strands has its own terminology, but the idea is that the agents are moving left to right, and once they move to the right, they don't, they don't come back. So, uh, in, in code, uh, projects you typically have that if you've used Quiro. Um, that it feels like a left to right process, right? They have a requirements doc or design doc requirements and then implementation and then unleash the agents and do the things, um, so those are like the two most ubiquitous orchestration patterns, yeah. I'm sorry, I missed that. Is trans doing like a group chat of the. A group chat That'd be cool. Um, no, it essentially what, what's happening in a graph, uh, agent orchestration is the, uh, the, the, when the agent's done, uh, that you can send the results of that agent to the next one, and then it picks up where it left off. You can send state to it. Uh, you can propagate the input from the original all the way to the last agent. Um, but no, they're not commingling and talking about things that would be more of the manager agent orchestration. You can ask it to do a white boarding session, update a markdown file with your ideas. Let's take votes based on, uh, what you think of, of what's been proposed, stuff like that. Um Yeah. Yeah, the manager agent says, and this would go in your prompt, right? You'd say you are a manager agent, you're assessing the work here you have an implementation agent, it's responsible for this, you have a coding agent is responsible for this, and you're relying on the reasoning ability of the model to decide which agent to work with, so you're leaving that completely up to the manager agent. So I have it running. I'm actually not even gonna let it kind of continue all the way just because we only have 20 minutes left and we're gonna get to some really cool stuff, um, but the, the message I wanna give here is we're able to switch into this multi-agent architecture. We still have a disadvantage. Because it's still really slow and so the, the path that we wanna talk through now that Pat's gonna, we're gonna, you know, take over a bit is how can we build this more locally centric, how do we create an environment that we can actually make the code change and test the code changes, um, because that's gonna be a lot more practical for these kinds of applications. Let's do it. Let's do it. I'll have to show you what I wrote, which I know isn't as exciting, but we've had some. My laptop doesn't connect well, so, um, I've neglected this side of the room. I hope you're all getting as much out of this as I, as I hope you are, um, so let's talk about a couple quick concepts. So when you write code, you never just write it and then send a and then push it. I mean some of you might be that good at at writing code, but. Uh, you've gotta make sure it actually executes and it actually does what it's supposed to do, and we take that for granted as developers. We run the code, we write a test, uh, and, and if it works it works. If it doesn't, you, you write more unit tests and you keep iterating until it's done. So how do you get an agent to, to do that on its own because it doesn't have a workspace? Well, you give it a workspace, um. What I'm gonna talk about now is how to maybe do that with code execution. How can you use Docker to create a workspace for this agent to, uh, put some files in it, mount it to the image or the container, run it, see what happens, get the exception, and keep iterating, um, and this is something that we're seeing a lot of folks do, uh, they might have a container image that they've baked for their pipeline that they wanna use. Those are great candidates, by the way. Um, we're gonna show you a really simple example in Docker, um, before I go there. Let's take a look at that tool, uh. So I called it the code execution tool, by the way. In strands, as long as you're doing things and following the rules, anything you put in the tools directory can be sucked up by the agent without question, right? That's good and bad because if you've got hundreds of tools that gets to be a problem. But just if you're if you're building agents for the first time, it's a good way to get started, uh, code execution, um. I'll walk through a few things here. Uh, oh, you have your scroll reverse, OK, not reverse, it's the right way. It's the right way to scroll. Um, so what I've done here, and I've just hard coded it hard coded a default workspace path, but what's gonna happen here is I'm gonna tell it put all your work in this Docker workspace. If you're gonna clone anything, if you're gonna write any code, make sure it goes in there and put your test somewhere else, and, and you make sure that we're keeping these things separate. File system security and um network security are also a thing, right? You don't wanna if you go deploy this and you let it. Ping the Internet. Go pull down any library it wants. Completely unvetted by your security team. Uh, these are all super important considerations to make. Anthropic in late October released, uh, Sandbox Run Time. I would, I would, I would highly suggest taking a look at it. They've, in a, I think, in a very thoughtful way. Offered a great tool for solving that you can send all your traffic to a proxy if you want. If you don't want this agent doing anything else, um, you can lock down the files or the directories it has access to. We're not gonna get into that today, but I wanted to, I wanted to put that out there. There's also agent core, um, code interpreter, uh, that is designed specifically for this use case, um, so my execute Docker tool, um, just like all tools. Uh, you wanna give it good descriptions again, remember the context. All of that goes into the context like when Strands picks up the tool, it's picking up the description to help it decide what it's doing. I got a little carried away here, um, but it, it's important to remember that, uh. I'm sending it back a uh a return error if I if I don't uh if I don't have the right information um dependencies. So how are you gonna tell the the agent to like if it needs to do a PI install or it needs to like get the environment ready before it executes the code you need to tell it where to put it, um, so requirements.text is gonna be a really important thing. I'm, I'm using Python so of course if you're if you're using Node or or some other language that's gonna be any number of steps to complete as we know. Um, but here we just got a simple docker command. So if I have all the requirements, I'm gonna include the, the requirements dock. If I don't, then I don't, uh, but I've, I've pulled down, uh, a container image. I'm telling it the workspace to use, and I'm gonna, I'm gonna pip install everything before I do a single thing, before any code runs. So I have parity with my test environment, my prod environment, so you can imagine you probably wouldn't do this in for reals. You'd probably have a container that's hardened and everything. Um, but we're just, we're moving fast here. Any questions on this approach? Any worries about doing this this way? Leon's like, yeah, what, what do you think? You wanna watch it just explode, um. Yeah, so, uh, file, files, right, go ahead. Oh, yeah. Yep, the question was how do you prevent the agent from downloading packages that are either like not approved or are not from your artifact repository, which is what most enterprises are doing, um, that's where you'd wanna give a lot of thought to locking down which addresses it's allowed to use and that's why using a proxy, uh, is good. Sandbox runtime lets you say only allow anything to this domain, um, if, if it's going anywhere it has to go through this this address. Um, that's one way, um, you could create a separate tool or a separate agent to like prepare the dependencies, do it, do it in a secured manner, get the trust of your security department, um, and may maybe they manage it, um, and that that's a, that's a way to do it. There's a lot of angles you could take, but, but yeah, it's worth thinking about. Yeah. It Yeah, she asked, you know, it's just in Docker. It's, it's probably not gonna go anywhere. No, it's probably not, uh, but Docker is leveraging my network hardware, right, so we get to hand wave that, yeah, but if you're launching or spawning a container in like a, an ECS cluster, that's gonna be managed, um, for good reason, but you wouldn't want it reaching out through your net gateway and just like Pip installing whatever it feels like Pip installing because I can tell you it'll, it'll end poorly. Did you have a question? OK. Cool, um. And so This executes, um, runs the docker command and then returns, um, and again I'm sorry we had this while working but we we can't get this laptop working, um. Uh, it returns errors if it doesn't work. OK, so code execution, um, I'll, I'll show you the Docker file I've, I've written for this, uh. If that helps clarify um. It's, it's roughly what you would expect, uh, you're setting the work directory to Docker workspace, um, if your code like has to like create things and like. Has to persist things, um, that's a consideration too if, if it's task is to actually output more code or output artifacts you need for some other process, telling it where to put it is also important. Go put it in S3. You could put it locally, but then when the container dies you lose it. So these are the same considerations you'd make in any surveillance architecture. But, um, yeah, in our case a very simple, a very simple Docker file. Uh, OK, I'm gonna show you how uh we've proposed uh in this case at least you build a a multi-agent that does this. Is this multi-agent one mine, the red one, the one with errors, um, no, no, I, I cleared them. I staged the commit, got it, got it, OK, um. And I don't know if you've seen this yet and I, I actually, why don't we go through this, uh. Yeah. All this has been Our Willie. In the PSO What Yes, for this very trivial coding agent example we're just using the Docker, uh, runtime on your desktop, yeah, but, uh, where might you wanna host it on AWS? Yeah, you might wanna host it on ECS or Fargate or, uh, whatever favorite container platform you're using, um. Uh, remember that when the agents, when this is running in a CICD pipeline or it's running in GitHub actions, you've got to orchestrate where that compute and that run time occurs. Uh, yeah, does that answer your question? OK. Yeah, because I think like when I imagine kind of some like practical applications of this kind of use case, right, like we have coding agents that run locally all the time, right? So it's, you know, why would I wanna do this? I see the benefit here actually running in your like a GitHub action or things like that. Like you want it to be able to kind of spawn in a place that's not local all the time. Obviously we have one hour, um, that would be crazy cool if we could talk through all of this in one hour, but, uh. Yeah We're gonna try And but we have 10 minutes left, so, um, I'm gonna keep going and please, uh, raise your hand for questions. Um, I wanna show you how, how to actually, I don't, I don't think we've gone through how to actually make a request to, to Nova yet. So we have a, we have a model, uh, helper here where we're wrapping the strands bedrock model class and putting, putting Nova, uh, too light in there. Um, for a lot of heavy reasoning tasks, like if you're traversing a big context, you're gonna wanna actually use, um, reasoning on high, right, because I mentioned this in the keynote session today, the Nova 2 light model we're specifically talking about and, um, you know, pro and AI that are announced in preview, they're extended reasoning models, so you can turn reasoning on or off. It defaults to off, uh, and then you might, if you're familiar with using other extended reasoning models, you might be used to setting. Budget tokens or something like that we do it a little bit differently, um, so we have, uh, reasoning, uh, budgets and so we group them into three categories low, medium, and high and so they'll use a different amount of tokens in their reasoning, um, and so this is kind of t-shirt sizing that you can use to control the amount of reasoning that's done, um. And don't always think that more reasoning is better for obvious reasons. Output tokens, uh, reasoning high will use can use up to 32,000 output tokens. So we can imagine really complex problems. It's a really cool thing, um, but it's cost and time. Yeah, and we've actually found if you use too much reasoning that is a possibility and the, the, the, the outputs can degrade, um, so something to keep in mind. Um, yeah, question one. Yeah, these are just different configurations of the same model, um, for helper just to make things a little bit easier. The question was just if we had 4 previously I was just using, uh, reasoning medium, uh, and so I was using the same model. Right, yeah, and now like when you can also I think it's a really good question too when we think about multi-agent because you can tune it for the actual task. Maybe you wanna, um, have more complex reasoning for the initial planning phase and you wanna do less reasoning for the act or the validation. You don't need that much reasoning if you wanna add that at the end, right? Sorry, I'm trying to make sure I get all the hands. OK, um, let's talk about, uh, where was I with the the actual agent here, uh. Yeah So we understand roughly um how how the tool would be orchestrated so you can get a code execution. Um, in this example I've taken the liberty to make a, um, a, uh, create enhanced plan agent. So some of the bottlenecks that, uh, Gene's example ran into was how does the model even know what the repost structure is like? Well, um, now we're getting into like basic file system tools, right? So any of these IDE tools that you're working with, uh, they have, uh, show me a directory tree, uh, show me the 1st 3 files, show me the 1st 100 rows of the file, look for anything in the file that has this in it. Uh, then expand and give me like the previous and following, you know, 20 lines, uh, so you very quickly find yourself building, you have to build those tools for the agent to use, um, because like we said, you can't just dump the 15, you know, 1000 line Python files into the context and expect it to perform well. It's gonna be expensive, uh, and it's just not gonna be accurate, uh, so you very quickly find yourself building tools that the agent can use. Um, and so what we show here is, uh, we have some file system tools called, um, show directory tree, uh, get directory summary, how many files are in this thing. Um, we are using some get tools, so, um, as you move into, uh, uh, off of GitHub MCP to, uh, to your local workspace like any developer actually does, you're gonna want get tools, and that's scary because you can do dangerous things with Git. But in this case all we're doing is clone, so we're just cloning the repo, um. Uh, we're cloning the repo down and then we're telling it, hey, use these tools to like explore. Tell me, do a code analysis, read the issue, and tell me where I should be making changes and how, um, we're all, oh, like one question I'll throw in there too, I think it's like worthwhile to. Exploring, I'm just gonna call it out there, right? We're no longer, uh, passing the entire GitHub MCP tool config, all 40 files. We're narrowing it down, but you can see he's saying issue read at issue comment. There is still value to using the MCP server. Those tools, um, provide a lot of utility, but the ones that are gonna require a lot of data, you know, and everything that's passing through and can have, you know, this benefit we can take advantage of the local code environment, right? It's better to use the tools, right, that Pat is talking about. How many have built file system tools or anything for their agents and gone gone that far? How has it gone? Yeah, I thought about Emotive, so the first step was always clone the. I, I, I, I didn't even think about, you know, grabbing specific parts because the modern, I thought the model wouldn't have any context. Yeah, yeah. Yeah, and, uh, even so the other step you could take is we were talking before like a lot of repos have a contributing MD. Take a have tell the agent to take a look at that um until super intelligence is here, all of these agents will be coupled very tightly to the repository they're associated with that's just inevitable. Um, you're gonna have to prompt it on guidelines for contributing. You'll probably have more information on where to find things in the prompts. Um, it, it's, you're gonna have to couple this agent tightly to your repo. Um, unless you've completely, you know, your, you, your development practices are completely ubiquitous across all code bases, um, which is rarely the case. OK, um, I'm gonna quickly get through here, uh, lastly, we, um, I guess I'll show you the, the agent prompt if that would help, but we're, um, we're creating a strands agent we're using reasoning low, um, you could use high. High is gonna take a while. It's, it's, uh, it's also gonna be generally more expensive because you're using more tokens to arrive at the answer. Um, so let's take a look at the prompt and then we're at like just under 4 minutes left, so I'll try and go through this quickly, um. So, and I'll do word wrap here so we can actually see, yeah, so, uh, you are an enhanced planning agent with GitHub issue validation uh capabilities. I'm sort of telling it what an issue is, um. Determine whether it's a bug or a user error because right a lot of issues are like yo go check the the read me like this is the wrong way to use it so validating whether this is a legitimate issue or if it's a feature you can imagine too these are tracks you might go down where you're gonna handle features completely differently, right? Or you're gonna handle bug fixes with some level of urgency in assessing the urgency of the issue. Uh, but again, and I don't, yeah, I wanna call this out too, like we might know models really love the users, right? And if we have an agent that's just gonna go and change the repository every single time the user makes a mistake and it's just not using things correctly, that'd be way out of hand, right? We don't want it to do that. And so, you know, it is like an important consideration, uh, because a lot of times people, I don't know if you guys are managing any open source repos, but they might be commenting things all the time that are just that's not how you do this. We have some repositories from SAs at Amazon who are already managing their repository in this manner, so, um, I can, I can send you an example. Wait, we had a question too. Oh yeah. Basically, what Going another. Yeah. responsible for resolving some specific task that Uh Right. Yeah, uh, the, there's some dynamism here though that changes over time, right? Like if the, the things that you bake into your prompt, you're gonna have to keep revisiting this. We haven't even talked about agent evaluation, which is analogous to employee performance evaluation, right? So, uh, not sidestepping all that, but yeah, a lot of it is very analogous to that. Yeah. You steps So the question is if you can do this when a PR is submitted. I think I would see a different application maybe when a PR is submitted doing a code review or things of that nature. I think what we're looking at here, maybe is like almost reactive to specific issues that are open on our repository. Something is reported as a failure. How can we go and create the the PR that would have the fix and then have the human go and kind of do the review and merge it. Yeah. OK, um, we are out of time. Yeah, we're, we're more or less that time, I think probably a good, a good place to stop, um. Any questions? Yeah, go ahead. Sorry, say that again. Uh, yeah, we'll, we'll publish the repo for this code, yeah, yeah, we'll make it available. Yes. Yeah we can do all of that. We're, we will do that. We're just gonna push it to the lang chain repot and just, yeah, no, just kidding, kidding, um. Yeah, well, thank you for joining everybody. Uh, it's been an absolute pleasure. I'm sorry we had the technology issues, but, um, if you have, we didn't, the, the point of this wasn't to go too much into the features of the Nova models, right? It's not a breakout session, it's talking about the code. If you have questions about any of the new releases, the quick PSA, we have a booth at the expo, people there all the time that can answer questions, um, but yeah, I hope that you got, you know, some value out of this, um, and had a good time. I appreciate that.