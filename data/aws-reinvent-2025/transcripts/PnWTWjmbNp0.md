---
video_id: PnWTWjmbNp0
video_url: https://www.youtube.com/watch?v=PnWTWjmbNp0
is_generated: False
is_translatable: True
---

Well, thanks. And good morning. Um, I'm gonna just start us off with a quick scenario here. Can you imagine it's uh 2 a.m. and your phone erupts with alerts, thousands of business transactions are failing. And your applications are down. Are you wondering, is my failover gonna work? Or will my application even run in the other region? Are your engineers frantically searching for recovery run books, half awake, executing procedures you've tested a year ago? For some of you this may sound all too familiar. I see a lot of smiles in the crowd. For others, you're waiting for that day to come because it's gonna come. So whether you're exploring what it takes to go multi-region or looking to improve your own multi-region approach, welcome to ARC 404, building resilient multi-region applications with Capital One. I'm Daniel Sill along with my colleague Eduardo and uh Prem from Capital One. And we'll show you how to deliver predictable recovery when it matters most. There's been nearly a decade of multi-region talks to reinvent. That shows how best practices and approaches have evolved over the years, and we're back again. Because these challenges haven't changed dramatically, but our solutions have become more integrated and sophisticated. Today, we'll talk about 3 of those challenges and how practical strategies, along with new AWS capabilities can help you solve these problems. Perm and I will start with the foundation needed for successful recovery. We'll talk about dependencies, often overlooked until it causes your recovery to fail. And share how Capital One identifies these critical uh dependencies. Then we'll explore recovery orchestration using AWS Application recovery controller region switch. And Capital One's journey with multi-region recovery. Eduardo will come up and wrap us up on data data consistency, still one of the hardest aspects of multi-region design. He'll demonstrate how Aurora DSQL and Dynamo DB multi-region strong consistency can enable true active active architectures. Hopefully you'll leave today with practical knowledge on identifying your dependencies, implementing reliable recovery, and making informed data consistency decisions. But first, let's talk about when you should consider going multi-region. We're up to 38 AWS regions and county, each resilient and designed to operate independently as fault isolation boundaries. For most applications, a well designed multi-Z architecture within a single region it's going to be sufficient enough for resilience. For applications with the highest availability. Requirements, multi-region architectures can provide additional protection, but it comes with increased cost and complexity. It requires ongoing investment, not only in technology but in people and processes. So I want you to carefully consider, does my business continuity needs require a multi-region approach? And for many of our customers it does, and we typically see two reasons. You may be in an industry that has regulatory and compliance requirements that dictate you need geographic separation for disaster recovery. Financial services, healthcare, and government customers often fall into this category. Or you might have applications that need predictable bounded recovery in the rare event of a regional disruption. These are typically critical applications where extended downtime can create significant business impact. We're fortunate to have Capital One joining us today. They've been on this journey for years and have developed innovative approaches to build and operate multi-region applications at scale. Thank you, Daniel. As Daniel just mentioned at chapter one. Thank you. Good, good. Can you hear now? OK, awesome. As Daniel mentioned, at Capital One, our multi-region journey was around. By both factors As a function as a financial institution, we have regulatory requirements around geographical distribution. And recovery capabilities. But beyond compliance, we recognize that our customers depend on continuous access to their financial information and ability to make transactions at any time. Hello everyone. Hope you're all having a great time at this reinvent. I'm Prem Kumar Dayalan, senior Distinguished Engineer for Enterprise resiliency and recovery at Capital One. I'm glad to be here and presenting this topic. With AWS our cloud-based environment operates on a massive scale. Managing over thousands of applications. Deployed across thousands of AWS accounts. This infrastructure includes millions of AWS resources and supports thousands upon thousands of critical workloads. Our approach has evolved from treating multi-region as a specialized DR capability to making it a fundamental part of our application architecture. We call it bond Platinum. This shift allowed us to not only meet regulatory requirements, but also deliver a more sophisticated and reliable experience for our customers. A key part of our success. Has been centralizing and automating best practices. We don't leave it to the individual teams to figure out. Multi-region solutions on their own. Instead, BB created platform capabilities and guard rails that make the right way the easy way. What I'll share today isn't theoretical. It's based on years of practical experience operating mission critical applications across multi-regions. Now I'll have Daniel and I will talk through the first challenge. All right. You probably have experienced this already, but dependencies can ruin your recovery plans. And I hear this often, and it usually is the same story. We try to recover or fail over to the other region, but we had a dependency on X and X was unavailable because it was in the primary region still. A single overlooked dependency, whether it's a hard-coded endpoint or a third-party API can turn into extended downtime. In our multi-region fundamentals paper, uh, if you haven't read it yet, it's a QR code here, we talk about understanding your dependencies. And you can think about them in 4 categories. AWS services that must be available in your target region because not all AWS services or features are available in every region. Internal systems such as shared services or on-premises components. These might include your authentication services or other business systems that your applications depend on. Third party services like identity providers or other SAS applications. I've seen it increasingly become really critical in most organizations today. And finally, configuration requirements, including service quotas and secrets. These need to be consistent across both regions to support failover. And so if any of these dependencies are not available, it could cause your recovery to fail. What I see often is the challenge is really how do you identify these dependencies because they're not core components of your application. If you think about your architecture diagram, they, they often don't show up at um on them, right? And so it's often out of sight and out of mind. So I like to use this mental model to think about these dependency categories. Either as run time recovery dependencies or run time dependencies. So think about the critical dependencies you need to recover your application in a different region. Once they're recovered, what runtime dependencies do they need to operate normally, such as upstream or downstream services that need to be in the same region. So I'll start with AWS services and knowing which services and features are available in the target region is crucial and we just made that easier. We recently announced AWS capabilities, a new addition to AWS Builder Center that gives you visibility into the AWS services across our global infrastructure. You can compare regions side by side, filter for specific services or features, API operations, and even confirmation resources. You can use this tool now to ensure that you have parity for your services and features across your AWS regions. For the remaining dependencies, let's take a look at this multi-regional architecture. Here we have an application deployed in the primary region. What you don't really think about is it's dependent components likely managed by a different team. And so when you're planning for regional regional disruption, it's easy to think about only recovering your application in that other region. But what about all the dependencies the app has? Are any of them critical for recovery? Is the application you're recovering in the secondary region dependent on any services that are in the impaired primary region. Now your recovery has failed because these dependencies were either overlooked, unknown, or assumed they'll be available because it always worked during DR testing. So how do you know what recovery dependencies are needed? Images to scaled containers, secrets or credentials to connect to databases. So one practical way you can do is start off with your recovery run book to determine what service or tools that you need to execute every step in your recovery plan. Then you can take it a step further. Once you identify those dependencies, you can do the dependency chain mapping to understand what dependencies does your dependency have. And so by doing this, you've at least identified the known but overlooked dependencies. But that's sort of the easy part, right? What's hard is like what's hidden. What about the unknown ones that you don't know about that might be tribal knowledge, uh, that somebody had known 15 years ago and and they've left the company since? One way you can identify these independencies is to block network traffic between both your AWS regions during a failover test. You can use a purpose-built test like AWS fault injection services cross region connectivity scenario which does just that. You can use this scenario to find hidden cross region dependencies and validate that traffic is not going from the primary region to the secondary region. During a failover test. What it does is deny cross region uh traffic, access to AWS public endpoints, and access to your workloads via load balancer and API gateways. It'll also pause cross region replication to see how you handle data reconciliation. So the goal really is during a failure scenario, there should be no dependencies between the primary region and the secondary region as we saw. So to achieve this design, to achieve this design for regional independence. Because what you want to do is ensure your critical dependencies are available in each region. For some organizations, these capabilities are often provided by a central platform team. So consider operating that platform in an active, active strategy so that it's always available. Couple of benefits to this is that you can always support applications that fail over during testing at any time without coordination or during an actual event, uh, you don't have to actually wait for the platform to recover first, thus extending your recovery time. An important one too is if you're using a SAS provider that's also hosted in the same AWS region, make sure they also have a multi-region strategy. To ensure this regional independence, you should continue to use FIS cross region connectivity scenario as part of your regular testing to validate there's no traffic going between both AWS regions during a failover. Now runtime dependencies are just as important, and I'll have Prem come back and talk about how Capital One does runtime dependency management. Thanks, Daniel. So the runtime dependencies represents the highest tier of complexity and risk. And the reliance on coordinated availability of Maltese services. Consider this example. Such as bill payment. The single process is achieved through chain sequence of transactions. The client requests bill pay. Which initiates the business process. Balanced APA acts as an upstream dependency to validate funds. Scheduling API manages the orchestration and timing of the transaction. Print check API executes the core business action. Update API and DB operation represents the downstream dependencies critical for state persistence and final record update. As you can imagine, the failure in any one single. Link within this chain, for example. Bill bill payment balance API time out or schedule an API fails to communicate or commit. Results in the complete failure of the business process. This highlights Just how complex the upstream and downstream dependency challenges can be. In the large systems Over the years we've found that the most dangerous dependencies are often undocumented. Existing only as a tribal knowledge or The implicit dependencies that were created years ago. And forgotten. In our early multi-region implementations, we focused heavily on the technical aspects of replication and failover. But underestimated the complexity of dependency management. In our recovery exercises, we discovered numerous hidden dependencies that prevented the application functioning correctly in the secondary region. Those humbling experiences taught us a critical lessons to evolve. Lack of lack of visibility leads to cascading impacts. There is no single source for the complete dependency chain or the precise recovery order. Which makes the impact analysis and appropriate remediation action harder. Furthermore, A significant hurdle in determining the appropriate actions to take on each dependency. The other significant challenge is the difference between the dependencies declared during design versus observed in the live traffic. Without proper dependency management, you risk major business impact and longer downtime during incidents. Incomplete visibility leads to delayed reaction time when responding to failures. Dependency information quickly becomes outdated as applications constantly evolve. These failed recovery exercises led us to develop a sophisticated approach. To dependency management using melddata. Metrics, errors, logs, and traces. Our process begins with collecting a high volume of diverse meld data directly from our AWS resources and applications. The data allow us to analyze live traffic in tracks to generate and maintain accurate application dependencies. We correlate this data. And apply the context enrichment to identify complex dependencies, achieving deep visibility. We map all the upstream downstream dependencies. Assigning critical and factoring in the resiliency tier for prioritization. This comprehensive data-driven. Map is persistent and used as our core intelligence and determine the correct and sequential recovery. During automated recovery. Sorry. This data-driven approach has been a game changer for us. Moving us from static, potential outdated dependency management to a dynamic, accurate view of how our system actually interact. This is a high level view. The solution flow look like. From an AWS resource to melt data collection and enriched with the context. Forming detailed application dependency tree. This platform offers flexible views including infrastructure, API dependencies, and component views, and many more. To summarize our key takeaways on the dependencies. Identify hidden recovery dependencies through testing and analysis. Don't assume your documentation is complete. Continuously monitor and update. Identify all the runtime dependencies for end to end operations. Look beyond what's needed and consider what's required to function over time. Ensure regional independence to avoid cascading failures. So that a problem in one region doesn't affect your ability to operate in another. Sorry. Based on our experience, I would emphasize that dependency management is a one time exercise. It's an ongoing process. At Capital One, we continuously monitor and update our dependency maps as an as application evolve. We found that automated approaches like melddata analysis, which we discussed earlier, are essential for keeping dependency information correct and accurate. Now that Thanks Brent. Thanks. So now that we've identified critical dependencies, I kind of think of that as a pre-recovery things that you have to do beforehand, before you actually need to fail over, right? And so now when it's time to actually fail over, how do you orchestrate recovery into another region? You need a reliable recovery mechanism to coordinate a sequence of steps for regional failover. Asprime kind of talked about, when we understand what these dependencies look like, whether it's applications or services, often they need to get um failed over in a particular sequence. This is where I see a lot of organizations struggle. Using manual processes still or repurposing deployment pipelines that weren't designed for recovery scenarios. So here you see a typical regional failover. It requires more than just flipping DNS to the other region. It needs an automated approach involving multiple steps that needs to be executed in the correct sequence. In the sample architecture, there was a disruption in the primary region. We would perform the following recovery steps in the standby region. Scale up compute resources. And scale up our data streams. Fail over our databases. And then we would probably do some application verification in between, right? Maybe application configuration or validating your, your service and your application works and then flipping over DNS traffic so that clients can continue to operate and, and go to the other region. Mm So maybe you have these capabilities already today, but they're painful for for a few reasons. Many customers still manually work through a list of steps from the recovery run book, whether it's click ops or executing handcrafted scripts to fail over. This approach is error prone and takes more time coordinating between different operators, making sure that they're going in the correct sequence. Reliability is critical as well. Your recovery tool needs to be available when you need it to be most, which means it shouldn't have a dependency on the primary region and not only that, but your recovery plan is only as good as the last time you tested it, very similar to your backup plans, right? So without continuous validation, you risk discovering broken permissions or configuration issues during the failover. Then when executing a recovery, it's often unclear where you're in the process, what succeeded and what's failed, lack of real-time visibility can impact your recovery time. So I'm excited to share that in August 2025 we launched a new capability for AWS Application recovery Controller called Region Switch. We built it to provide customers a fully managed, highly available orchestration service to switch between regions to address the very challenges that we just talked about. It's a reliable mechanism for two reasons. First, the, uh, it performs a planned evaluation every 30 minutes to ensure that your recovery plan will work when it needs to most. And second, the recovery plan is actually executed from the target region that you're going to, so there's no dependency on the region that you're leaving that may be impaired. It provides automation that eliminates the need to build and maintain custom recovery tools. So let me show you how to build an automated workflow in a region switch plan that reduces recovery time and human error. I've taken the application recovery process that we just saw and built this workflow for it. Here we use the term activate to fail over to the passive region and deactivate to prevent traffic from going to an unhealthy region for active active architectures. Every box you see represents an execution block. A step in your recovery workflow. And execution blocks can run sequentially like steps 03, and 4, or in parallel like steps 1 and 2. We start off with a manual approval to allow human in the loop to make the decision to fail over, but the rest is automated. Regions which plans are also flexible so you can build nested workflows for complex scenarios to fill over groups of dependent applications so whether they're groups of microservices that you have together or groups of applications that need to move over together, you can coordinate all that with the plan. So if we take a closer look at the execution blocks, the first thing that we did was scale up compute capacity in the target region to take on failover traffic. This execution block is an ECS service scaling action that increases the number of tasks in the target region. But how do you know how much to scale up in the target region? Regions which will frequently track the peak running capacity of the source region to allow you to match by percentage in the target region and that's one of my favorite features. And by the way, it also supports EC2 auto scaling groups and elastic Kuberneti service with these same capabilities. For specialized recovery requirements, regions which supports a custom action lambda execution block. This flexibility allows you to incorporate any API accessible action to recovery workflow. Addressing unique requirements while maintaining the benefits of centralized orchestration and monitoring. What you see here is a Python script I just wrote to scale up kinesis shards and using its API in the target region. But it's like a Swiss Army knife. You can use it to do the application configuration we talked about or do application verification prior to failing over. So whatever custom code you have, you can use with this execution block. Database is often the complex part of recovery. This execution block makes it simple to fail over the Aurora global database, so you no longer need your DBA to do it. You just need to provide the cluster name and the arms and regions which handles the rest. It even allows you to perform a switchover for planned testing or failover during actual incidents. And then lastly, the Route 53 health check execution block provides a reliable way to route DNS traffic using Route 53 health checks. Here you'll add a hosted zone ID and a DNS record that supports health checks to the execution block. In this example, we're using a DNS failover record that has a primary record in US East One. And the secondary record in US West too. Once that execution block is created, Region Switch is gonna generate health checks for each of the DDS records like you see in the diagram below. That you just copy these health check IDs into the corresponding Route 53 records and associate them. What's important to note is that these health checks are created in AWS managed account and they actually don't monitor the resource for your application. Since Region Switch owns these health checks, it can control DNS routing by changing the health check status to route traffic to the other region on demand as part of your recovery workflow. We use what's called a stop pattern or secondary takes over a primary and so that way there's no dependency on the primary. Everything is executed from the secondary. That's a quick overview of how Region Switch works. Hopefully it's gonna make your life easier and more reliably to uh recover applications in another region. I'll have Prem come back one more time to share about how Capital One's has gone through their recovery automation journey over the years. Thank you, Dennis. At, at Capital One. Our recovery journey begins with slow. Manual process and isolated component level approach that ignored the broader. Ecosystem. We recognize this critical gap. At that time, no market solution met a high standard for fast, reliable recovery for business and business continuity. Building an in-house tool was a strategic investment in our resiliency posture. It's been a multi-year journey and a continuous continuous investment. First we developed automated runbooks, converting static plants into automated operations. Next We created custom CLA based runbooks for total flexibility and teamwork. Then we introduced experiences with least privilege. This reduced our reliance on the. And cut down the human errors. Our plug and play fees allowed for quick onboarding and automation for common enterprise cases. Finally, we achieved full maturity end to end automation via. Low court and no court were closed. Today, the decision to fail over remains. Human in the loop But the execution is fully automated. Remember we talked about the dependency resolution in the previous section. Here is where we leverage those dependency management for smart, targeted and sequential recovery. Our preferred and most efficient method is the dependency group failover. Though we support individual application failovers too. Our evolution mirrors the maturity journey of many multi-regional organizations. That's why we are excited about the AWS Application recovery Controller Region Switch. It addresses many of the challenges we solved internally. Our recovery tooling efforts were transformative. Cutting recovery times from hours to minutes and our tools are required common capability across operation across Capital One. And help us to ensure Standardization This consists of behavior and eliminates thousands of unique custom solutions and standardized workflow across common tasks, which helps reduce mistakes during incidents and testing events. Second, it significantly reduced the mean time to engage and restore automation and led to faster incident response and recovery. And 3rd, It freezes more capacity. Deliver higher business value. Developers spend on the innovation. This transformation has fundamentally changed how we approach resilience. Making a core operation capability rather than an afterthought. Our technical recovery exercise. Provides clear proof of our tooling benefits. Our average minutes to recovery across all resiliency tiers has reduced by an estimated 70%. We have a long way to go. But it's a significant Where we started the journey versus where we ended, where we are today is a significant uh increase from, from a benefits perspective. This improvement comes from several factors. Smoother recovery process. Reduce risk from human errors. Making it easier for applications to test failover more frequently. This transformation, what were one static dependent document into functional compliance and recovery tools that deliver the real business value. AW's tools and services act as an accelerator for incident response and recovery minimizes. Impact during disruptions. The slide shows the comprehensive ecosystem of recovery tools we use. For DNS-based routing with the Route 53 health check to traffic management with the Global Accelerator. And elastic load balancing. For disaster recovery, we use a combination of AWS services to build. The in-house recovery tool which I talked about earlier. For testing, AWS fallen injection service provides resiliency testing capabilities. Although we built our own orchestration capabilities, we are excited about the continuous evolution of database services and looking forward to seeing how arc regionw can use to reduce our operational burden. The key takeaways Implement a reliable automated mechanism like arc region switch. Don't rely on manual process. Or repurposed tools which were designed for recovery scenarios, an example, CICD tooling. We went through that similar journey in the early phase. Ensure your failure process accounts for application dependencies. Understand which components need to move together and what sequence to maintain functional integrity. Test regularly under realistic conditions. Don't test in ideal circumstances. Simulate the chaos. and constraints of real incidents. By understanding these elements, you will significantly improve. Your ability to recover quickly and reliably during regional disruptions, whether planned or unplanned. At Capital One, our experiences has shown the investment in purpose-built tools and orchestration to orchestrations pay dividends. Just not during incidents, also in the regular testing exercises. The confidence that comes from knowing your recovery process works reliably is invaluable. Both from technical team and for the business stakeholders. Now, Eduardo will talk through the 3rd challenge, which is data consistency. Thank you, Prent. Hello everybody. Uh, my friend Pram talked about the fact that dependency map is the hardest problem, and I agree with him. Uh, my name is Eduardo Petrosino. I'm principal solutions architect here at AWS supporting Capital One and banking community across AWS, but I want to talk about a different kind of challenges whenever we are doing building resilience applications on on multi, uh, region, and that is data consistency. So, um. Let's go and let's look at some sort of requirement for a credit card authorization system. So this is a fictitious application that I'm building, but it's it's realistic there. So the requirement that we have here is that we want to. Process the transactions in less than 500 milliseconds and then we want to do card validations using the loan algorithm of validating also the CVV and things like that and we also want to have some sort of ability to do business logic for the authorization and then we want to be able to process at least 500 transactions per second with 99.9. Up time and we want to persist the data with encryption. We want to have restful API. We want to do PCI data and things like that. So this is the scenario that I'm going to use for to talk about data consistency. So, um, the solution that I built here is still in a single region, single AWS uh region, spreading across multiple availability zones, and so let me describe a little bit the different components that I have on that solution. So for the data layer, what I do have here is that I'm using Aurora to be able to store the kind of static information, so like the profile of the user, how much the credit balance or the credit limit that he has, whether his card is authorized to do international transactions and things like that. So the Aurora database is being used for mostly the profile information. The Dynamo D B I'm using to store the transactions there. So any time that there is some sort of transactions that the user used and the transaction was approved or rejected, I'm going to store on Dynamo. And as I mentioned before that I want this solution to be as quick as possible. So in order to make that even faster, I'm using elastic cache here in order to provide some way to do e-memory caching. That information. So I'm taking the most recent transactions and bringing from Dynamo DB to Elastic Cash. I'm bringing the profile information from the user to Elastic Cash, and my goal with this solution is to do the approval or the rejection as soon as possible. So in order to do that, I'm using Fargate that will come and look at that information, pull information from Elasticashna. If necessarily and it's just going to come and say yes this transaction is approved or rejected. Everything else I'm doing asynchronously in the sense that I'm going to come and say this transaction was approved or rejected and asynchronously I'm going to send a transaction to a kinesis stream and then there's a lambda function that's going to pull that information and be able to persist that information back to the Dynamo DB table. This is cool, but as my friend Dan talks that there are many reasons for you to consider doing multi-region, such as different kinds of requirements or different users' experience. So you might have users all over the world or all over the country, and you want to be able to ensure that every user has the kind of same experience. So how do I evolve the solution that I had before in terms of providing a way to do multi-region? So my first step in doing those things is that let me take that solution and hopefully you're using some sort of infrastructure as a code, some sort of cloud formation or terraform that I can just go there and deploy that solution across two regions. This is pretty cool and I'm using two kinds of databases here that's helping me to implement this multi-region solution. The first is Aurora global database that allows me to be able to replicate the data from one region to another. And similarly is that Dynamo B also has the capability, as you guys know, a global database, a global table with Dynamo DB that I can use that just to replicate the data. This is a pretty good solution. I have now about 53 that's there on the top serving the DNS as you see on the very top is that I'm still kind of an active passive solution here, so all the traffic is going to the left side to. Region A, but if there is some sort of impairment in any of these components of that solution, I can quickly go there and switch, maybe using the region switch that Dan described before to take that solution and move from one region to another. So this is pretty cool, but it is still kind of just one region being active, the other region is being passive. So how can I enhance that solution? Before I talk about how to enhance that solution, I need to talk a little bit about the C term. So the capterment is something that was created 25 years ago, and it tells us that whenever there is some sort of network partition, you have to choose between consistency or availability. Again, is that whenever there is some sort of network partition, such as when we are using multiple availability zones or regions, is that you have to choose whether you're going to focus on the availability side or on the consistency side. Yeah, so now let me move that solution to be able to do active, active. So I changed my Route 53 to be able to go there, send traffic to the two regions. Now the users across the different regions of the world can be served with the same kind of experience there. So the solution is sending traffic to both regions there. So we are pretty good there. And so are we done here? The answer is no. So if you look carefully at that solution there is. That even though all the components are running across the two regions, it is not a true active active solution. Why? If you look at the pictures that there is one component that is running primarily in one region and it has a reed replica into the other regions. So the aurora, aurora that we have there is that it requires all the rights to go to a single region. So you can have the reed replica into the other region there, but. Uh, with that solution, I'm still dependent on region to be able to go and write the records to my Aurora database. So how can I make this solution better? So what I want to describe is go and talk about Amazon Aurora DSQL. DSQL is a new kind of services that we introduced earlier this year and allows us to be able to do an active, active right to database across multiple regions. Yes, it does require a third region to serve as the. Witness so that we can have quorum across the three different nodes of the Aurora DSQL there but with this solution here now I can go there and be able to write to region A and region B. Pretty cool. So now I have a service that we at AWS are providing to you that helps you to be able to implement multi-region there. But are we done? The answer is no. So Aurora DSQL is that it is something that we are going to manage the infrastructure and the scalability of the solution. It allows you to really do multi-active active across the regions there. It does the automatic fail over. So Dan described a little bit about the capabilities of. Application recovery controller to be able to implement the enablement and activation of the different regions here. No, is that we at AWS are going to do all the failover of the database for you. It is post grass compatible and one thing that I find super curious is that transactions are welcome in the sense that we don't like to talk about transactions on the cloud. But here is the opposite is that all the transactions or all the statements that you have on the transactions are going to be resolved locally inside that region, and whenever we do the commitment, the commit statement, that's where we are going to sync with the other side. So the transactions on Aurora's DSQL are welcome, and I encourage you to do those things there. Now let's look at this diagram again. Latency is kind of awkward. Like, even if I stop here for two seconds and you don't hear any noise from me, like, OK, what's going on here? This guy is going, so, so what happened is that if you look at the Dynamo DB global database, is that it is still doing the replication between region A. Region asynchronously. So the traditional mode that we have with Dynamic B we call the multi-region eventual consistency in the sense that the Dynamo B is going to persist the data locally to one region and behind the scenes or a synchrony is going to replicate that data to the other side. As a consequence of that, there is a caveat whenever we use Dynamo DB, the fact that the last one wins. So let's go to a scenario that you decided to store, for example, the credit balance of your credit card is a single record on your DynamoDB, and by coincidence, me and my wife decided to do a transaction at the same time. I'm here in Vegas and I'm spending my. $10 on my coffee and my wife is spending her $1000 on something else that she's buying there and so if it happens to happen almost at the same time is that we have the chance that in the property with Dynamo is that the last one wins so whatever record has the most recent time stamp is going to be the one that's going to prevail that transaction. So you have to be careful whenever you're using Dymo DB with global tables because it still has this property of the last one record that's updating a certain record will win. So in order to do that, there is another flavor now for Dynamo DB, and it's a mouth. The name is super big. It's called multi-region strong consistency, which means that the persistency of that data is being done not only into that region, but we are going to synchronously take that data and replicate to the other region there. So similarly to Aurora DSQL is that Dynamo DB multi-region with strong consistency requires a third region to serve as the witness there. So we do require 3 regions for that. But this way is that we have the guarantee that whenever you are storing the data in one region, the data first is persisted to the other side before we go back to the users. So what are the benefits of Dynamo DB multi-region strong consistency here? So it allows you to be able to implement a strong consistency across the regions there. It allows you to be able to still have low latency, so Dynamo is famous for providing us low latency still with the strong consistency across the regions. We still have low latency. Certainly is that we have to wait until the round table, round time to be able to send the data to the other region and be able to receive that knowledge to that region, but DynamoB is still a multi-region, multi-regional strong consistency provides a low latency solution. And it simplifies your application logic in order for you to compensate for this idea of dynamo to be that the last one wins, you have to do some logic in your code. Maybe you write the data and then you read back to ensure that the data has been persisted there. Now with this idea of the MRSC multireg strong consistency, you can go there and have the guarantee that once you write the data, the data is going to be persisted into Dynamo DB. Yeah, so I started my part here talking about the capturing, the fact that whenever there is a network partition is that we have to choose between consistency and availability. So my initial solution was very simple using just Aurora global database and Dynamo DB, what we call. Multi-region eventual consistency. So in that case we are favoring availability there. So on the cap trade-off is that we are looking for availability and the consistency is eventual because it's done asynchronously there. The availability is high and then we do certainly support partition. There, but now with the new solution that where I'm using DSQL and Dynamo D B multi region strong consistency, I'm focused now on consistency there. So in order to have a stronger consistency, I have to sacrifice a little bit of the availability. So as I told you that DSQL and Dynamo DB multi regions strong consistency. Requires a third region, so you need to get quorum across at least 2 regions in order for that data to be persistent there with DSQL and Dynamo B multireg strong consistency. So the availability is reduced whenever there is network partition there, but it's still something that we will be able to focus on consistency instead of availability. Yeah, so the key takeaways from the data consistency is that implementing uh uh resilient multi-regional application is hard, and we talked about different aspects here is that DSQL and Dynamo DB multi-region strong consistencies help you with those things as they provide ways to do synchronous replication of the data across the different regions and it allows you to really do a true active active solution across those regions there. And so, uh, it certainly is that uh in order to increase the consistency, we have to decrease the availability, so we have reduced the availability in the case of network partition there. So one thing that Pran talk is about the technical recovery exercise is not really an exercise that my friends at Captal One does is that it is really running the show in the sense that it will take the entire workload and switch from one region to another, and likewise is that my recommendation for you you practice or. Things that you are doing as if you are playing into the game. You don't like practice football by yourself by just kicking the ball and running and doing touchdowns and things like that. So practice all your things and maybe using photo injection surfaces as a way to help you with those things. But our recommendation here is that you practice your solution as if is it game day. So in terms of the call to actions that we have is that as we talk to you here is that dependency management is a hard thing to do, but you have to do it. It's impossible for you to create any kind of plans if you don't know the dependency across your components there. You have to automate your recovery. Don't do manual steps on that, but automate your recovery orchestration. And uh the last thing that I I do that I talk is select your data consistency solution here whether you're going to focus on availability or if you're going to focus on consistency and if you are going to focus on consistency now we do have some AWS solutions such as DSQL and DynamoB Much region strong consistency that can help you with uh with that that solution there. Uh, we do have a few white papers that, uh, feel free to take a picture, uh, and they will help you to in this journey of doing multi-region and resilient applications there, and, uh, we do have some swags available, not here. Oh, first of all, is that there is a meet up this afternoon, right then is that so all the uh uh resilience experts are going to be there. Feel free to join us at 5 p.m. today. For the meet up on the resilience there and we do have some swags. They are not going to be given here because we're supposed to leave this room as soon as we are done here, but Prem, Dana and I are going to be available at the lounge area near the door. Here, so feel free to reach out to us and uh share your experience of doing multi region and resilient applications and see how we can uh uh embrace on that journey uh together with that, I wanna thank you for coming to our sessions um and I hope you have a great day.