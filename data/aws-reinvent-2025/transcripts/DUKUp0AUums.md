---
video_id: DUKUp0AUums
video_url: https://www.youtube.com/watch?v=DUKUp0AUums
is_generated: False
is_translatable: True
---

Hello everyone. Welcome to Reinvent. Thank you for joining our session today. We know that you had a lot of great options out there, so we truly appreciate you being here. My name is Aman Twari. I'm a solutions architect at AWS. In today's session, We are gonna talk about Ripple. And how have they built an AI powered operations platform to transform their platform operations using AWS? Now this is a 300 level breakout. Which means that we are assuming that you you folks are familiar with core AWS services and concepts. That being said, don't worry. We will take a moment to introduce each AWS service. Now we have a packed agenda, so let's jump right in. Here are the things that we will cover in today's session. We will start off by talking about what is Ripple, who is Ripple, their journey, their story, and XRP to set the context. From there, we're gonna talk about the challenges faced by the team. Next, we will dive deep into the solution architecture, and the Ripple team is also going to share the lessons learned. The journey they took to build it and scale it on AWS. We also have a demonstration for you folks, so I'm very excited about the real implementation of the workload. And finally, we will close it out by talking about what's next for Ripple. Now with that, I will now hand it over to Vijay. Who's gonna get us started? OK. Uh, thank you so much, Amman, for the, um, introduction. Um, Aman has been an excellent partner from the AWS uh side throughout this initiative. I'm really excited to be here to walk us through the journey that we've been, uh, in Ripple and solving this, uh, XRPL operations platform via AI solution. Um, coming straight out of the Thanksgiving week, uh, particularly, uh, when you talk with the friends and family, I always get asked, uh cornered by many folks, uh, and, uh, like the first question I they end up asking is like where should we invest, uh, on the crypto side? How much ripple is gonna go, um, uh, why is Ripple is spiking 5X since Thanksgiving? Um, I always take a moment by moment of pass, and then I tell them like, are you talking about XRP? Um, I think there is always this confusion about Ripple and XRP, uh, where people misunderstand, uh, uh, what is Ripple and what is XRP. So I just wanna probably set off the stage with like, uh, what is XRP, right? XRP is the native digital asset, uh, for the XRPL blockchain just like BTC for Bitcoin, ETH for Ethereum, Sol for Solana. Ripple, on the other hand, uh, is a fintech company. We build enterprise great solutions leveraging blockchain, uh, to move value faster, cheaper, efficient, and, uh, to solve the problems for businesses like financial institution, uh, banking, etc. uh, we do have, uh, sort of applications, uh, varying from a cross-border payment solution, which is our Ripple Payments, uh, and then, uh, Ripple Custody, which is primarily used for storing, um, a digital asset in a more securely and. And compliant way and recently we also launched a stablecoin which is backed by US dollar which which recently attained a billion dollars market cap. So we use RLUSD XRP as a bridge currency in some of the solutions that we use as you see, we've been operating since 2012. We serve customers over 90 countries. We have almost 1,000+ employees. It's a well established institution solving, uh, enterprise grade solutions for businesses. Moving on to the next slide. Um, I wanna give a bit of an intro about XRPL. Uh, XRPL is a layer one blockchain built for business. So what do, what do I say built for business? Like I, I don't know how many of you have seen the 2008, uh, the Bitcoin white paper. Um, it came up with an abstract that Bitcoin, uh, is a decentralized peer to peer networking system, solves the double spend problem, right? So this is probably the first of a kind where, uh, decentralized ecosystem helps solve double spend problem. Uh, one of the things about Bitcoin was it's more energy consuming. Uh, I think the architects of the XRPL, they figured it out. This may not be a scalable way, so they kind of came up with this most energy efficient, uh, proof of association decentralized blockchain, which is XRPL, which has a deterministic finality of 3 to 5 seconds, uh, which solves the double spend problem which can. be used for payments and other business use cases as you see, this is XRP is also one of the OG blockchain. It's been around from 2012, um, and it has its own first native DEX and whatever the stuff that you want to do on, on a, on a, on a financial institution use cases like payments, escrow, uh, XRPL has all built in as a native primitives in L1 blockchain. Um, looking at some of the statistics, um, if you look at it, we, it's a decentralized ecosystem. We have almost 900+ nodes exist. This means like anybody can, uh, become a part of the network. All you have to do is like you have a machine which has 32 gigs of RAM. Um, uh, you can spin it up by easy to, uh, instance and then download the binary and you can be a part of an XRPL network. Uh, at the 900 nodes I talk about it, it, it, it, it's located throughout the world starting from Japan to end in, uh, San Francisco, and, uh, there are different. Types of nodes available, so validators are the core node which is primarily responsible for the consensus layer of the protocol. If you look at some of the metrics like we recently closed 100 million ledgers in the last 13 years, that also talks about the 3 to 5 seconds finality. Uh, that being said, one of the biggest advantages of using XRPL is, is, it's, it's a, it's a transaction fees, right? It's so damn cheap. If you look at it, it's like 0.0004 cents to send a transaction from one place to another, right? So, so that being said, um, uh, XRPL is a battle tested blockchain, uh, built for business, and, and it is probably making waves in the, in the digital assets era. On the previous slides we talked much about XRPL blockchain, but I just wanna give a miniature version of the network, right? So this is, uh, as I, as I mentioned earlier, it's a peer to peer network, which means there is no central server, no headquarters, no single machine, uh, which is controlling anything. It's instead it's made up of many different types of nodes. If you look at it, we have validators. I already talked to you guys about it's probably the brain of the ecosystem which is part of the consensus layer, and then we. We have hubs which is primarily responsible for the connection of these peers. We have relayers which can send messages from one machine to another machine. So, and we have almost 180, uh, independent validators. So if you look at it, uh, the scale of the network is like it's, it's so robust and resilient, um, and it, it is definitely, uh, one of the, one of the best network in terms of if you wanna do any, any cross border payments or institutional, uh, grade solution, right? So, um, that being said. Us being in the ripple platform team, um, monitoring, uh, the resiliency of the network is one of the challenging, uh, problem, right? For example, in a, in a, in a centralized ecosystem, it's very easy for us to figure out what, what is going on when, when we, when we launch a new feature where like you can just go to a database and you can execute a query, uh, before the change after the change you can easily figure out what's going on in the network, but when you. Talk about a decentralized ecosystem, uh, particularly, uh, 1000 nodes run by many independent operators like universities, uh, blockchain institutions, wallet providers, uh, financial institutions, uh, it's very hard to identify how the network is behaving as we in the Relex platform team, uh, we, we, we are in the trenches all the time in terms of making sure XRP is more secure, resilient, and robust. Particularly in terms of solving these problems, one of the things we always end up having is like we rely too much on on C++ experts. For example, like the whole protocol is written in 2012. It's a huge code base. It's all C++. So when an issue happens or when, when we want to monitor the network, what we generally do is like we get, get this whole huge volume of logs and we try to make sense of these logs by looking at the C++ code and it's, it's so complex we have to rely on the C++ experts for us. Make the understanding of what the pattern of the issues, right? So for example, I'll give you, uh, 4 weeks ago there was a, uh, Red Sea, uh, cable cut happened and some of the node operators in, in the Asia Pacific, they were having reliability issues, right? So when, when, when we got all those issues raised to us, we end up asking them, OK, can you give us the locks? Then we get all these locks which are huge volumes in size, which is almost 30 to 35 gigs for a single node, and then we go to our machines, we take all those huge volumes of locks, we end up doing a cross comparison. And all these logs are debug logs, and if you know it's a peer to peer network, you get a lot of signals, cryptographic details, proposals, peer connections, and, and, and to make a meaningful summary of what's going on is one of the most complicated process, particularly when you're solving a production problems, right? So it could take almost. Two days even to just understand what's going on and even those cases like we have to rely on a C++ expert like Core Ledger team who's an engineering team who who does all this protocol development for them to get involved and to make us a meaningful summary out of it and and and logs are the gold mine of the information and it is very it plays a very critical role and we've been doing this forever. One of the things that we wanted to change was like how AI can solve this problem for us, right? Why do we have to rely on C++ experts where the whole world is heading towards English as a. Programming language, we started looking into AI as a solution where we can correlate between the locks and the code and it can give us a meaningful summary out of it. That's when the whole journey of uh log and code correlation of of solution that we started looking at, looking at. So that being said, this is probably the most lightweight high level design you guys are gonna look into it. Uh, my, uh, partners who are wearing the suits, they're gonna come with the big guns of whole deep dive architecture. So if you are engineers, you're gonna be in for a treat. There is a lot of good stuff that's gonna be, uh, talked about, but on a very high level I wanna call out these three boxes, right? So. The first, the middle box, the multi-agent platform. So this is the brain of this orchestration. So this is a foundational layer that we built leveraging AWS stacks. This contains two agents at this point, and we're also working on 3 more agents which Hari would be giving us more detailed walkthrough. So this is primarily the orchestration layer where the agents work when a when a user sends a request via a chat interface. They make, they make a code path of whether it can go towards a code analysis pipeline or log analys. This pipeline. So the second half, this is where the bulk of the data preparation happens since the code is an open source repository. So the log code analysis pipeline, what we do is like we have an everyday periodic code retrieval that is run from the GitHub repository. So every commit which goes into the developed branch, which gets prepared, which gets stored in a graph DB for us to make a meaningful summary out of it. Then we have log processing pipeline since we're talking about huge volume of log which is. Almost 35 to 50 gigs for a single load and Ripple almost operates like 40 plus machine in mainnet that constitutes around 2002 to 20, 2.5 terabyte petabytes of data. So to have all these data ready and prepared, we have this log processing pipeline which runs every day, which, which makes it prepared, prepared for the whole agentic framework to make a meaningful summary out of it. So that being said, uh, I'm going to call on Hary next to come and walk you all through a detailed architecture. Yeah, or do you hurry? Uh, thanks. To me on. Cool. Uh, hey all, this is, uh, Harivignesh Rajendran. Uh, in the forthcoming slides, uh, we will talk about the log processing pipeline, uh, the code processing pipeline, and the graph rack capabilities that we have built. Uh, first, let's, uh, talk about the log processing pipeline which brings the nodes, uh, logs to, uh, cloud Watch. Uh, raw logs from validators, hubs, client handlers, uh, they are first brought into S3 using a separate workflow which we have orchestrated through GitHub workflows, uh, using SSM. Once the data reaches the S3, we have the S3 even trigger triggering the lambda function there, which looks into the file, gets the byte start and end of each chunk, and which also respects the log line boundaries and also the configured chunk size. And once these messages are derived, they are then put into SQS for further distributed processing. Um, so these messages are then read by the log processor lambda function. Uh, they, uh, it actually, uh, retrieves only the relevant chunks from S3, uh, based on the configured chunk metata that it that it read, and, uh, it passes the log lines, uh, gets the metadata out of it, and, uh, puts these log lines and metadata to cloudwatch. Um, now, let's move on to the, uh, code analysis pipeline. Um, so here we have, uh, two main reports, uh, the ripple D repository and the standards repository. Ripple D repository holds a server software for XRPL ecosystem, and the second is the standards repository which describes the standards and specifications relating to XRPL ecosystem to ensure interoperability between the XRPL and other applications that are built on top of it. Uh, both repositories feed into our pipeline using Amazon Even Bridge, uh, which is a serverless even bus, um, using Even Bridge uh scheduler within it, uh, we automatically trigger repository things, um, on a cadence. Uh, so once the sync event arrives, the Git repository processor pulls the latest changes from GitHub, uh, including the code and the documentation, versions them, and then stores into S3. Uh, then comes the, uh, knowledge-based ingestion job, uh, the sync job, let's say, uh, which pulls this data and puts it into, um, um, Amazon Neptune Analytics Vector store. We will look into this in more detail, um, so knowledge base, uh, uh, uh, by, uh, default in the back end, it configures the chunkings, uh, embeddings, and, uh, um, acts as a storage as well. OK, so now let's move on to the next slide. Um, so we want to actually dive a little bit deep about, uh, the Graphrack capabilities, uh, that we have built. So we are not using the open search, uh, vector store here. We are using Graphrack that because that's because it's really good at storing relationships, um, across the code base. Um, so I will detail that here. Um, so we have the files stored in S3 now. Uh, the process starts by reading these files. Uh, the knowledge-based sync jobs, uh, reads these files in, converts these files into smaller manageable chunks. Beddoc also, um, as you would know that, uh, it allows multiple chunking strategies, uh, fixed size chunking. Um, and other LLM based approaches, uh, in our use case we use fixerized chunking because it works well with structured content like code and documentation. So once these chunks are derived, uh, they are sent into the Titan text embedding V2 model, which generates the embeddings, uh, which is the which is the actual semantic meaning of the text. Yeah, so what, um, also you would notice that there is a box underneath the Titan text as well. So this is the extract entity step that happens. So what it does is it looks into the chunks, uh, derives related entities, entities being, uh, function names, uh, code calls, um, and the classes, modules, and other domain specific identifiers. The cloud IQV1 model gets all this information. And this information is then used to build a lexical graph, so this lexical graph uncovers all the relationships across our code base. uh so this is what makes us, uh, makes the retrieval very fast and very efficient for us, uh, utilizing a very less, uh, uh, context window for the LLM. OK, so once these tongues, embeddings, and entities are retrieved, uh, they are then sent to the Neptune Analytics graph store. Uh, this is where the graph is built. Uh, the lexical graph I talked about is nothing but nodes and edges which has links to the entities, uh, source tongues which belong to, uh, which, uh, source document, and the cross-document relationships are all stored here. Uh, this, uh, Neptune, uh, graph database, uh, becomes a powerful retrieval layer, um, you know, after all these, uh, things, magic that happens in the back end forest which is managed by, uh, Bedrock. So what happens when the query comes in? So the code analysis agent using an LLM which is hosted in Bedrock first gets this context from the knowledge base along with the entities, then it is sent to the LLM for generating responses which is not only grounded in semantics, but also on the code relationships as well. OK, um, we also have applied another, um, uh, let's say significant improvement as well. So we have re-ranking layer, uh, built on top of this graph rack workflow. So re-ranking is a powerful enhancement to rag. It adds a second and a more intelligent pass over the retrieved documents. Uh, while the vector search, uh, gives us the top matches, uh, based on the embeddings, uh, re-rankers like, you know, coher re-rank that you see here, uh, evaluate the relevance dynamically at runtime. This means that the model reads both the query and uh each candidate document together, uh, allowing to allowing it to understand nuance um and uh uh give out better results for the LLM. So here's where uh Ripple applies it. We first retrieve like a broadest uh chunk of uh data from Knowledge Base. We then give that to the re-rank model along with the user query. Uh, the rankor model then calculates the relevant score from 0 to 11 being the highest score, and then it reorders the results, and, uh, let's say it returns the top 10 results back to the LLM. So now you get like, uh, fewer fewer results to the LLM, but they are all very high quality, so that the context window is not affected for the LLM, so you still have more tokens to burn, uh, essentially. So you, you can see this question here why we have had the screenshot. I've asked the question about what log messages are defined inside a function, you know, this uh actually forms the premises of the entire presentation here. So we need to find the right log messages to query in our log uh log files. So you can see the source chunk that it shows is #1, but originally without the rear rank model it was actually 4. Because of the user query that we gave it and also the chunks that were retrieved, it was able to combine them together and give us the top results. OK, um, I will now, uh, pass on to Amman for the multi-agent architecture. I OK. Hello? OK. So now let's talk about the multi agent platform that Ripple Team has created. Our multi-agent platform consists of 4 AI agents. Let's talk about them one by one. The orchestrator agent is responsible for taking the queries from the platform engineers, that is using a web UI. Think about Web UI as a clean chat interface. Every request goes to Amazon API gateway. Which is a manage service that allows you to create, publish, secure, maintain API endpoints at scale. The orchestrator agent receives this query. And classifies the query. It does intent classification. Based on the query, it then decides whether to invoke a single specialist agent. Or coordinate multiple agents in a sequential or thorough execution format. Once these downstream agents complete their task, orchestrator agent is also responsible for synthesizing their outputs into a single coherent response. Now the orchestrator agent is using Dynamo DB as a state management backbone. If you're already familiar with one of these components, which is Amazon API Gateway, it has a soft 29. Integration timeout. Which means that if the user asks a question, and if the entire multi-agent workflow takes more than 29 seconds, it's gonna time out. And that is the reason as soon as the orchestrator agent receives this query. It immediately creates a task entry in Dynamo DB. And from then It continuously updates the progress messages. Asynchronously execute itself. And once the results are back, it is going to write the final answer back to Dynamo DB. Now the code analysis agent is using Knowledge Base as the tool. So Hari went into deep details about how we have developed a graph rag application using Bedrock Knowledge Base. The code analysis agent is using the tool to derive the code insights. The log analysis agent is responsible for performing operational analytics on top of cloud watch log groups. Now, the process involves generating a cloud watch log inside S query. And in order to generate a syntactically accurate query, the log analysis agent is working closely with the query generator agent, whose sole responsibility is to come up with an accurate query. You can see that there is a tool, a static JSON file that is provided to the query generator agent, which it reviews and hands off the query to the log analysis agent. You can see Amazon Cognito, which handles the required API gateway authentication. Now, Before talking about the other components, you might see that there are 2 agents that are hosted on AWS Lambda. Whereas the other two agents are hosted on something different. You might recognize the symbol. This is Amazon Bedrock Agent Core, which enables you to deploy and operate AI agents at production scale with any model or any framework. An Amazon Bedrock agent core runtime is a purpose built serverless environment for these AI agents. Now we made this design choice, because at that time. Lambda satisfied all our constraints and Agent Core has just entered preview. Now that agent core is generally available, one of the next steps for the Ripple team will be to migrate these lambda-based agents onto Bedrock agent core runtime, so that it benefits with this purpose-built infrastructure for AI agents. The Secrets manager is storing the sensitive credentials required for agent core communication, as well as the SSM parameter store stores the configuration parameters for the services. Now let's dive deep and talk about certain aspects of AI agents. Prompt engineering, system prompts. Now, system prompt defines and gives the agent its identity, its responsibilities. And, more importantly, what the agent should not do. Ripple has practiced strict prompt hygiene principles. Every prompt is structured that provides the agent its role, its task. Strong explicit guard rails of what the agent is not allowed to do. We have literally copy pasted the system prompts onto this deck from the code base. You can see how the system prompts vary for all these 4 AI agents. The orchestrator's agent system prompt is geared towards task delegation. The log analysis agent system prompt tells us that you are an expert in analyzing the XRPL. Logs that are stored in Amazon Cloud Watch. The code analysis agent is powered by the GraphRg application. We are telling the code analysis agent that you, your responsibility is to understand the code dependencies, the XRPL code base, as well as Git commit relationships. And when it comes to the cloud watch query generator prompt. It's responsibility, the prompt is telling that you are able to precisely generate queries. That can be used by the log analysis agent. Now this was system prompts. It is important to highlight system prompts, because prompt engineering is the foundation of how these AI agents behave. Now let's talk about. What makes these AI agents so powerful? What makes these AI agents able to execute tasks on our behalf? Which are the tools? The framework that they are using. And, of course, the model. Now the foundation of this architecture is the Strands SDK. Strands is an open source SDK built by AWS for multi-agent coordination. You can see that all these agents are using strands as the multi-agent framework. Let's dive deep into three of these agents. The orchestration agent, that is using clotson at 4.5 via Amazon Bedrock, uses two strands tools. The the call code analysis agent tool is used to call the code analysis agent via lambda invocation, whereas the agent code log agent function talks to the log analysis agent over HTTP with Jason web token authentication. If you look at the code analysis agent, the first tool, which is the XRPL query code, is powered by XRPL knowledge base, whereas the get recent commits and get commit details are get-based sync actions. The log analysis agent needs access to cloud watch APIs, such as execute the queries, describe, as well as retrieve. Now, in summary, SRNs handles the communication layer, it handles the message passing, context management and multi-agent coordination, whereas orchestration agent takes the decisions, and the specialist AI agents do the deep domain analysis work. And communicated back to the orchestrator agent. Now let me walk you through an end to end query flow. The user uses a Web UI to ask a question. It hits the API gateway. I was running out of space, so sorry for that. API gateway sends the question to an orchestrator agent. It performs the intent classification. In this case, the orchestrator agent decides to talk to the code analysis agent. The code analysis agent then invokes its tools to get the exact. Log relevant code lines. Once this information is retrieved, it then passes along that question to log analysis agent. Log analysis agent, again, is using the cloud watch query generator agent to come up to get an accurate cloud watch query that it can then use to execute on top of these cloud watch log groups. Here is where model context protocol comes into the picture, an open standard developed by Anthropic. That helps these AI agents talk to external systems, which is Amazon Cloud Watch here. Over standardized interfaces. As you can see The log analysis agent is using two MCP tools. The first tool. Provides It's ability to execute query on top of these log groups, and we get a query ID back. We take the query ID and then we use the 2nd tool to get the actual log results. Once the log analysis agent has these results. It assimilates the information through the code analysis agent, does the Code log correlation, and sends the results back to the orchestrator agent. Which finally synthesizes the output. And sends it back to the user. All the AI agents are using the large language models. In this case, we have been using cloudson at 4.5, but using Amazon Bedrock, you get the model flexibility, and you can choose any model, you can mix and match. The orchestrted agent can use a light, smaller model for speed, whereas the big, heavy agents can use models that are better at code generation, like clots on at 4.5. Now, this is the entire current state architecture of Ripple, signifying the AI processing, the log processing pipeline, as well as the code analysis pipeline. Now with that, I will now hand it over to Harry, who's gonna walk us through a demonstration of the solution. Thank you. I'm back. So, uh, before we move to the demo, um, so we will, uh, uh, we will take like few brief pauses, uh, to understand the log lines because that's going to be text heavy, uh, slide, uh, you know, demo here. So yeah, so I will pause so that you can, uh, observe the log lines and get better understanding of them. So let me play the demo now. So this is a chat UI. So the question that we have asked here is, uh, for the given time range, how many proposals are UN a validator has seen from other peers. A proposal is a validator's suggested view of what the next ledger should be. So we have asked the, uh, code analysis agent to get the log lines from the code and pass these log lines to the log analysis agent, uh, which can generate the response and give it back to us. Yeah, so this is the orchestrtory agent logs. You can see here, um, this has received the, uh, user query and uh it'll be calling the code analysis agent, uh, to get the log lines from the code. OK, this is the code analysis agent. You can see here that the prompt is received from the orchestrator and the chat history, if it was available, would have been sent here too, and it uses the query XRPL code like uh repeatedly until it gets all the log lines uh related to the question. So you can see that the uh primary log line has been retrieved. Uh, this is the line in consensus.h file. Along with it, you get other log lines as well, uh, with the log analysis agent to derive further analysis. This is the log analysis agent logs that you're seeing right here right now. Uh, this has received the prompt. Uh, it would receive the chat history and also you will see that the code analysis agent does respond, uh, you know, um, uh, shown here as well. So this helps it to understand what was the previous agent's response and also, uh, give better results, uh, based on this. OK, so now the cloudwatch query generator agent is being called to get the cloud watch queries. Uh, you can see that there is a pattern tool which gets called now, um, you know, we get the static patterns from S3, uh, which contains the lock patterns as well as the estimated, uh, number of, uh, uh, patterns that we have seen. Uh, this gives us a better idea on how to form the limit queries for the cloud Dutch query agent. Um, so now we can see that the pattern tool has responded. The cloudwatch query generator has generated the queries. We have to look for four important aspects here. Uh, one is the cloudwatch query itself and what this query does, an estimated count of, uh, you know, these log patterns, and also the instructions back to the cloudwatch query generate to the log analysis agent. So the instructions contain uh how to execute these queries, can they be executed in parallel or not, and uh you know what are the time ranges that needs to be passed and others. So here you can see the MCP execute query gets called and the get results get called multiple times based on the instructions from the Cloud watch query agent. And you can see that the results would be displayed in some time here. Yeah. Now we have the results. You can see that the total of 267K proposals were received from other peers. Along with it, you get the hourly distribution as well, and also all the peer node IDs which send proposals which will also be displayed here. So not only the uh question uh that that we asked for, but other related information as well is being sent here. Yeah. So this is the uh uh uh response from the orchestrory agent. It's a summary response, combines both the code analysis agent response and the log analysis agent response. Uh, this gives a nice summary view, uh, but we can also have a detailed view by clicking on the view agent analysis button. Here you can see that uh the code analysis agent response as well as the log analysis agent response uh would be, uh, you know, uh displayed uh uh in detail here. So user can now ask uh more questions on top of this, uh, and get more uh results based on this as well. So this is the uh agent core observability. Uh, this was really helpful for us to improve our agent performance. Uh, this has the total number of sessions, latency, duration, token, uh, usage, as well as the errors that that uh that are present in the agent core containers. So this was really helpful for us. So, uh, this question, uh, this is the 2nd question we are asking. So here, uh, multiple log lines need to be collated to answer a question. In XRPL we have something called consensus rounds which happen every 3 to 4 seconds, and that's a sequence of events that happen for each and every phase. Without understanding this, this question cannot be answered. By this, what I mean is, uh, in the consensus accepted phase, canonical transactions attached would be formed, as you can see, you can find the log line here. And after several long lines. You can see that the built ledger message would be shown which contains the ledger hash for which we have asked for. These two things need to be collated and because it understands its consensus rounds, it will be able to correctly form the log lines. It has given the correct log lines to look for. It's asking us for the canonical transaction set message, individual transactions. Under them and built a ledger message and this is sent from the code analysis agent to the log analysis agent which then executes the queries and gives out the ledger hash for us and also all the events that you saw earlier in between these two events they're also nicely summarized here. Um, yeah, you can see when it was, uh, uh, seen and, you know, and the second message when it was seen as well, a few seconds apart actually, so bodes well with how XRPL behaves. Yeah, so let me, uh, hand it over for the concluding slides to Vijay. All right. The lessons learned, um, so definitely one thing that we totally, uh, evaluated or, or probably like we didn't, we didn't pay much attention was initially was context, right? So context engineering is the total key. I, I think the LLM has a lot of capabilities, but definitely the context is not. Right, I think that's where, uh, it can hallucinate a lot. So the context engineering is something that we have learned that it's pretty, pretty important, particularly when you wanna design any solutions which are related to the AI based and, and also the capturing the decisions, uh, reasoning, the tool calls. It's a clear auditable workflow. So, uh, when the year started, uh, when the platform team was doing all these stuff manually, uh, we, we, we decided, OK, we're gonna use the AI, uh, but then if you look at the final hint architecture, that is not something we envisioned when we started the whole design, right? So I want to give you the walkthrough of how, how this architecture evolved. So, uh, the first Q1 quarter, that is when we started writing the vision doc that how this platform team should behave. How we should use the latest technological capabilities like AI to solve some of the problems rather than doing everything in a mundane manual way. So the whole first quarter we were thinking about the vision and in terms of the strategy how this platform team should operate. Initially we were like, OK, let's go with the machine learning system. We can train it. We can have all those data in our hand and we can have all these inferences, uh, and using that we could come up with a solution. But that's when like we also noticed the industry-wide standards have started completely shifting from Q2 uh agentic AI came into the picture, so that's when we realized, OK, probably we should involve more subject matter experts. Uh, we may not be the right team to solve this problem. So that's when, uh, we started having those discussions with the AWS Space team. Uh, Space is a prototype initiative team, so they quickly came in. Uh, we, uh, kind of walked them. Through the use case, the problem scenarios, uh, one thing, uh, that gave us, uh, the more confidence was once the pace team got involved they could be able to build this prototype as quickly as possible, right? So this prototype was, uh, one of the key, uh, stepping stone for us to even conceptualize the solution, and we can have the dependency of thinking, OK, this is possible, right? So all these complexities that Hari and Aman walked through, uh, in the back end, uh, those things are all. Like, uh, done by the foundation layer of the Pace Team initiative. Huge thanks to the Pace team for helping us out. Uh, Q3 is where, like, um, we also noticed Amazon also, uh, AWS also evolved. They started introducing us Agent Core, so we were probably the first of a kind in terms of working on their previous future, and we noticed that Agent Core has a lot of infra capabilities that we don't want to deal with, particularly, uh, let's work on the actual business logic, the problem. That we want to solve all the infra aspect of it, the agent core definitely does us, uh, a lot of help where we don't want to solve all these problems. And Q4 is where right now we are working with the, the Proser, uh, uh, AWS Proserve team. We want productionize this. What do you mean productionize this? Like putting all the guard rails, the VPCs, and in terms of making it so secure, compliant way so that we can, uh, serve this to XRPL open source community. Um, Definitely you could all see the benefits, um, particularly, uh, in the demo, right? So what Hari showcased in the, in the UI in the chatbot that used to take at least 2 to 3 days for, for a platform engineer working with the core, uh, C++ engineer to have a meaningful summary out of it. It's not, it's not, it's not, uh, direct, uh, log lines which would, which would give you the answers, if you notice it's so noisy, it's a peer to peer network. You'll get. Too many messages in the logs and it can be jumpy out of sequence. Uh, this is where AI definitely helped us to make a good meaningful summary out of these log lines. Um, one of the biggest benefits that I noticed was the removal of the bidirectional dependency between the platform team and the C++ expert, right? So platform team always relied on the C++ experts, uh, to help us understand these logs to make sure that this is an. Issue or a non-issue C+ experts always used to rely on the platform team to give them the good logs because like it's it's storing them all in Graa and making them searchable is also an initiative where platform team always be in the bottleneck. So this definitely having this one chat interface where it kind of reduces the dependency between platform and the core engineers so they can automatically search for any kind of information, the logs that we have already pumped in. That being said, we are also noticing that the core engineers right now want to develop features as fast as possible, so we started pumping the devnet locks, test net logs, so anytime any engineer wants to build any feature, they can directly look at their logs, get a meaningful summary out of it, even they can compare their current logs to the main net and make sure that what they're developing. It makes sense? Is it going to introduce any problems so they can quickly understand what's going on in the development cycle as well. Definitely time reduction is one of the biggest wins and enhanced data troubleshooting and the data processing in terms of the scale that we're talking about, this AI chatbot is definitely helping us in everyday operations. For example, even next week we have a standalone release which is going on for. RPL Mainnet. I have platform engineers who are looking at the logs every day right now via the chatbot to make sure that everyday locks are looking good, and they're giving me a thumbs up. OK, today locks is good. I think we are good to go. We have 4 more days of 5 days of chestnut soaking that we want to do. Uh, I think this whole operations efficiency which you're talking about in terms of excellence, this chatbot AI solution is definitely helping us a lot. So that being said, we talked about a lot of this architecture. There are some of the capabilities that we see which we could definitely use. One of the things that we are noticing is agent core memory. The reason is like we don't want every new session, it cannot have the memory capabilities where the the people who use the chat interface. End up reingesting the context, the question again and again. So we definitely want to take advantage of agent core memory, which is definitely solving some of these problems. And also we're looking into agent core identity, which is also coming out of the box of agent Core where we can put in all the role-based access secure layer definitely is going to help us solve the problem. That being said, uh, we solve the problem of log analysis, code analysis. Uh, what else we can use with this chatbot is like one of the things that we want, we, we have noticed from XRPL community, particularly working on cryptocurrencies like, uh, forensics, AA forensics, right? Say for example, you, you as a XRPL account user, sometimes you might, uh, end up losing money, uh, because of scams, network scams, etc. So definitely this chatbot can help quickly identify the transaction layer, uh, where if, if a user comes in, puts in the wallet. Address this chatbot can quickly go through the RPC calls of different accounts and then could give you a meaningful summary of where the source of the fund and where it finally ended in the destination, right? So that means like anybody who lost the fund, they can quickly understand where the final destination of the currency is being stored and they can involve the relevant law enforcement to quickly pass or freeze the funds before it goes to the malicious actor. The second thing also we are thinking about is. Level monitoring. One of the things that we noticed since it's so cheap to use XRPL, people end up sending a lot of dust transactions, spam transactions that could be quite operational heavy for the UNL validators. So we can also quickly identify these dust transactions, spam transactions by looking at the overall network, and we can involve the UNL community so they can quickly be aware of who are the problematic accounts and what what are what are those accounts' intentions, and we can quickly identify how we can solve some of these network level transaction issues. So that being said, um, this has been a wonderful journey. Uh, I wanna, I wanna definitely call out some of the folks, uh, who were, uh, helping us out in building the solution, particularly the Pace team. Shri, Jim, and Ishan. They came in, they kind of changed the entire architecture in just 6 weeks. Uh, we never, we never expected this architecture is gonna be this robust, resilient in terms of how we wanna take it forward for the next steps. And Aman, being a wonderful partner from the AWS side, he's been working with us hand in hand every day to solve this problem, and huge shout out to Parisa for, uh, stitching us all together and involved. Us all the right folks to solve this problem and also ripple engineering leadership, uh, when we said, OK, we're going to solve this AI problem, they were very supportive of it and they, they, they were, they were always about going, uh, pioneering us to solve this 0 to 1 problem. So without all the support we wouldn't be able to be here at this stage, and I hope you all learned from our lessons so you don't have to reinvent the wheel in terms of your technological choices, uh, particularly if you want to choose a multi AI agent system. So you can definitely use some of these technological choices that we made. And if you want more questions, we're happy to answer. I'm thinking. Hm