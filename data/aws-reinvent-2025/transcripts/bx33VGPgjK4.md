---
video_id: bx33VGPgjK4
video_url: https://www.youtube.com/watch?v=bx33VGPgjK4
is_generated: False
is_translatable: True
---

Good afternoon, everybody. Welcome you all to the session, architecting compliant GII Systems for healthcare workflows. A quick introduction. My name is Vienna Vasudevan. I'm a principal partner solutions architect from the worldwide partner Go to Market team. I'm joined by my colleague Sudhir, as well as our partner Sanshid from Quantify. Sudhir and Sanshid, would you like to introduce yourselves? Hello everyone. Sudhir Gupta. I'm the principal partner solutions architect, data and AI specialist, uh, in the same worldwide data and AI partner GTM team. Hello, everyone. This is Sanshit from Quantify. I work as a practice leader, AWS hero, and an AWS ambassador. All right. So today we are here to talk about a problem that affects millions of us worldwide, especially if you're from the US, probably you're relative to it. How many of you have had your insurance claims be denied for a medical procedure you had or maybe a medical. Yeah. Hello, hello, hello, hello. Um, and that's exactly what we are trying to solve today with the help of AI. Here's a quick, uh, view of the agenda. We'll get you started with the business context, and we'll move on to the solution approaches we have taken to solve a medical coding problem with AI. This is the coding problem that leads to your insurance claims be denied or you receiving a fat bill at the end of your procedure months later after you've gone through it. Um, and we will give you a couple of architectures on how we solve this with AI. Of course, not one model fits all. There's always multiple approaches to a problem. And then we will do a corresponding code walkthroughs and demonstrations. This will take a bulk load of the session time today because that's what we are here for. And after that, our partner Sanshet will cover path to production for one of the customer implementations he has done for this particular use case, along with resources and time for Q&A as well. So let me quickly walk you through the business context here. Talking some numbers here, right? So in the US alone, 4.6 million, uh, insurance claims are being processed on the daily, and 10% of these claims get denied. And this denial is mostly due to a medical coding error. What I mean by medical coding error is now switching gears to ICD 10 code. ICD 10 code stands for International Classification of Disease. 10 is the 10th revision. And ICD10 could think of them as the universal language for healthcare billing. So, for instance, for pneumonia, there is an ICD 10 code, J18.9, I believe. And then there is an ICD 10 code for broken ankle. And believe me or not, there is an ICD 10 code for getting struck by a duck. Seriously, that is a real ICD 10 code, W81.82. So, there are about 70,000 of these standardized ICD10 codes for all of these diseases, right? And new codes get, keep getting added. So when you go to an emergency room, all of us have been healthcare consumers, of course, we have been to the ER at some point in time or the other. And if you go to go and get an X-ray done or a CAT scan done, maybe your radiologist writes. Uh, patient came to us with an acute sign of pneumonia on the lower right lobe. Now, this is perfectly comprehensible to you as a patient and to your healthcare provider or your doctor. But what about the insurance companies? They don't really understand this language. They want this to be converted into the corresponding ICD 10 code. And that's the day job of medical coders. They spend so many hours, so many cycles trying to convert each of these medical diagnosis into the corresponding ICD 10 codes. And they do that using a traditional process. So they use a combination of rule-based systems like reg X-based pattern matching. So, for instance, regular, regular expressions will extract words like pneumonia from the medical diagnosis, and it's going to translate that word into the corresponding ICD 10 code. Obviously, if it works well and good, but regular expressions may not work all the time because if you think about it, your medical, your radiologist or your doctor may end up making a typographical error instead of pneumonia, they may say something else. And also not everybody universally speaks the same language. It differs based on countries, based on cultures. Not everybody is going to end up writing the diagnosis in the same manner. So regular expressions are error prone. If you're lucky, you're going to get it accurate, maybe 60 or 70% of the times. And then your medical coder comes in and validates all the codes that were, that regular expression was not able to capture, and they're going to manually do that coding. And again, this is an error-prone process because it is manual. So you're ultimately, this is not a great system that is built for complexity. Now, what are some of the impacts we are seeing because of this? I already covered the patient impact, that is, you end up paying a huge medical bill at the end of the day because a lot of the insurance claims get denied and they don't get resubmitted, right? And also, you're looking at, you know, uh, the provider impact. So the medical providers also face an impact because of this. Because you may get complex coding requirements. For instance, patients can show up to the ER room with both signs of pneumonia as well as a broken ankle, right? So you need to map this diagnosis into 2 ICD10 codes, not just one. So as we go along, we are seeing all these complexities that the traditional process doesn't actually address, and we're looking at billions of dollars wasted in healthcare because of this. And this process really screams for the need for modernization. And that's where we are taking an approach with generative AI to fix this challenge. Not all solution approaches are created equal. There are pros and cons for each approach, but I'll walk you through the three approaches that we have evaluated. The first approach is the most straightforward one, which is using a foundation model to do the ICD10 coding for us. You can take any model from Bedrock, let's say anthropic cloud or maybe Novasonic Novaro. Um, you can take a model from Sagemaker Jumpstart, whichever model you prefer, and you can get the model to do the ICD10 coding for you. You can feed the medical diagnosis as a prompt, pneumonia diagnosis or whatever it is, and have the model do the coding for you. Now, again, this isn't the greatest or the most ideal approach because LLMs notoriously hallucinate, right? So they're not really built for this particular use case. This is an industrialized or verticalized use case. This is task specific, so not everything is going to be captured accurately by the foundation model. That's where you need more approaches with slightly more nuance. Coming to the second approach, retrieval augmented generation along with the LLM. Now we are talking, right? So, in this approach, you're grounding the LLM's response on with truth or the source data. You basically have a vector database and you saturate this database with known medical impressions along with the ICD10 code. And provide a few short examples to your LLM. So your LLM has all these context to do the mapping more accurately. It's going to ground the mapping and source data, and hence it's going to prevent the hallucination problem that the first approach has. And the last approach we evaluated is the fine-tuning approach. Now, This is going to be the most accurate of all the approaches because the answers are going to be baked into your model weight, right? So you take a base model, let's say Claude again or Noaro, and then you can fine tune the model, um, based on domain-specific or task-specific data. In this case, you're going to fine tune it with a number of labeled examples. You have the medical impressions plus the ICD 10 codes, right? So you're going to end up with a model that is going to be tailored to this specific use case. It's going to give you much more accurate result and lower latency as well, because like I said, there's no need for semantic searches, rag lookups. It's going to be much faster here. But there is a downside to it, right? Because fine tuning is a very resource intensive process. If you've done fine tuning, you know that it may take even up to days if you have a lot of data to fine tune the model on. Right? So, and the accuracy is really dependent on the quality of the data and the number of label samples you provide as well. So there's always pros and cons to all of these approaches, but we'll walk you through the last two approaches because the first approach is a no-brainer, right? Uh, we know, just prompting the model and getting a response. So we need more nuanced approach for this particular use case. So we'll cover the solution approach with the rag as well as with fine tuning in this code walkthrough. Now I'm going to hand over to my colleague, Sudi, to walk you through the solution approach rack with LLM. Thank you Rena. Am I audible? Yes, sir. Talk you through the rack-based approach to build the ICD-10 medical coding pipeline assistant. Before actually going into the dive deep into the technical nuances, let's see what you are building for. So for that purpose, what I have done, we have created, I mean, in, we have created already a setup where we have created the Streamlit app, and in the Streamlet app, as an end user, like the medical coder or the biller, you interact the systems, you give the three inputs, the narrations, impressions, and the procedures. So basically, these are the clinical records about the, uh, about the patient. And based on this information, you want to generate the ICD code. So you, your end user come into the click ICD codes and you will see the system performs. First thing that applies at guardrails because we don't want to deal the non-health related questions. We want to protect the sensitive data because we don't want the sensitive data to get in into the system itself because of the compliance and all. And then, uh, after the com, I mean, once you apply the PAI stuff and the guardrails, we send the prompt into the, to the bedrock, where it generates extract the phrases in the first phase and then it generates the ICD 10 codes. So you will see here, I mean, the pointer is Uh, so at the bottom, you will see there are a generated ICD 10 codes, um, for three phrases which is extracted from the user inputs. Now, let's understand this in more details. So first thing I will talk about what is the architecture, right? What you are building. As I said, this is a rack-based approach. It means you need to build a knowledge base. And for the knowledge base, here, we are using the Amazon OpenSearch uh serverless. And um for, for the knowledge base, we are going to insert the two types of data sets. First one is the CMS data which contains about the information about the ICD 10 codes and the descriptions. And the second thing is like the few short samples. So we want to give some good curated response so that LLM can do the better job. So we are going to create that. And for that, we have two data files, as you see, the latest CMS data which contains the ICD 10 codes and the descriptions. And the second data file. Contains information like the phrase, uh, and then medical history information, and then the relevant ICD code, similar ICD 10 codes. And this becomes actually the input for your, the system prompts when we uh do the next steps. So you have these data files. What do you do, you upload it to the S3 and then you generate the embeddings. In the, our solutions built, uh, we have used Amazon, uh, um, Amazon Titan embedding models to generate those embedding, but you can also use the other embedding model like the cohere, even the Amazon Nova embedding models. And once you generate the embedding, you ingest this into knowledge base for the similarity search. As an end user, as I said, the, either you are the medical billers or the coders who want to generate the ICD 10 codes based on the history. You want to interact the systems. Uh, in this setup, we have given the two choices. Either you can use the command-based, like the line, I mean, Jupiter notebooks or the command line tools, or even you, we have the basic streamlet app to access the same thing. So once user input, uh, provide the inputs like the narration, impression, and the medical, uh, the procedure's names, these details, once it submit, it kicks off the lambda functions. And what lambda does, it extract the input and first thing it applies the bedrock guardrails. And the guidance is important because we don't want to deal non-health related questions, and we want to have, let's say, uh, because the medical coder might be inserting the record, instead of the patient, he might write, let's say the patient name, the date of birth, and other information. We don't want to store this information in our system. So very first thing, we apply the guardrails and we mask the sensitive data. And then we send this data to the LLM. And then for LLM purpose here, I mean, again, you can use the different uh um large language models. In our examples, we are using the cloud, anthropic cloud, uh Sonnet 4. And uh we are also going to showcase the fine-tuned version with the Amazon Nova Pro. Uh, and you can use actually multiple models. So for us, we perform the multiple testing and we find the Amazon Nova Pro and the cloud both works pretty much the same. And the two different approaches, we thought, let's give you the demo of both the models, how actually they are helping to build you the system which is not hallucinating, giving the response what you are actually expecting. And once you submit this data to the LLM, the first LLM call what we do, we perform the semantic search, uh, the similar search from your Amazon OpenSearchn Base indexes. And then for your input, we, uh, get the top 20 best matchings for each of those phrases. And once you have those 20 matches from the Amazon OpenSearch Index, we do the another LLM call. OK. And why we are doing this? Initially, right, we have provided some 3 inputs, and the response can vary a lot, and we want to be very accurate. So what we do in the first, based on the few short sampling and from the knowledge base, we restrict the response of the top 20 matches. And once we have the top 20, we do another LLM call with the actual user input. And get, these are the best final output, uh, I mean, the ICD 10 codes for the user input. And once the output is there, we send it for the, for the processing because you want to, let's say, receive the output in a specific parameter, you want to send it back to the application. We do that and then we send it back to the end users. So now, I will, uh, talk you through the, the code part. Uh, let me switch it here. Let me see if the, I hope you guys can see the screen. So we have created the Gitlab repository, uh, that contains all the codes we have used to build the setup. And this code is publicly accessible as well. So, and we are going to share the QR code so that you can scan, you can access this repository, and you can access it at your end as well. And this has the basic rhythmic file. So first thing, what do you need, right? I have shown you the architecture like that you have the bedrock guards, you have the knowledge base, and the indexes. And uh the other setups configurations. So for that purpose, what we have done, we have created a file called deploy.sh. OK. So when you actually do that, I mean, run this script and it will set up the environment for you. I mean, all the necessary resources, the provisions and everything will be deployed. So in this demo, I'm not going to spend time on how to provision the resources, but I will talk to you about the actual part or how, how exactly you need to think about building the code, indexing, and running those NLM call. So, uh, what I have done, I have already built in a system which contains a bedrock, guardrails, IM permissions, the necessary indexing. And these are the data files I was talking about. So we have the two data files, the sample data files, which contains the information about, uh, the ICD 10 codes and the impression, history, and the procedure information. So we have done the indexing with this. And then the second file is the latest CMS data which contains actually the ICD 10 codes and the description. To make it, uh, so in this setup, actually, uh, let me talk about one more thing here. So when, uh, what we have done, we have created all the scripts and everything in the Python format, but we have also created some notebooks. And notebooks we have created just for this, uh, demonstration purpose. So everything what we are running it through the Python so that you can actually understand what the Python code is doing. Otherwise, I will just show execution of the Python code, you will not learn anything. So these are the, this is the first notebook. I will show you. Uh, we, uh, we have created for the index creation, the second index creation, which contains a few short sampling. So what I will do, I will go to the console and, uh, by the way, a lot of codes we have built using the Amazon Kiiro. So what you see here, it's actually things created by the Kiro. And I will be using the same thing even to run the code. So I have all the environment already set up, and I will invoke the Jupiter notebook. Uh, I don't want to use the new one, and I will run this. OK, so it will, Kick off the Jupiter notebook for index creation. And the very first thing I will do, I will run all the cells, OK? And then I will explain what exactly we are doing. So what, uh, see, the index size can be a really large. I know in this example, especially for the few short, uh, uh, I mean, sampling index, we have just 334 records. It can really run faster. But when I was designing this, I thought of, what if you have the hundreds and thousands of records and you want to index the data faster. So, and we have another data, right? I mean, the second index, which is called for the ICD descriptions, it has actually the more than 75,000 records. And if you load the data in sequence, it will run for hours and hours. We don't want to do that because you will generate the embedding, then you will ingest the data into the, uh, Amazon Open Search Serverless. So to speed up what I have done, I have created the multiple, uh, the parallel processing for the indexing. So first, you do the all, you load the necessary libraries. You load the configurations, like the, what is the name, name of the index you want to create, which region and all, all those information, you will load it from the config file. Just to show you the, how the config file looks like. This is like the config.json where we are specifying the project name. Project name is nothing but any identifier. I have given VY 2022. And let me zoom in so that you guys can see it better. Uh, so the project name, then environment, then some region, and the information like the bedrock. So we have created the bedrock guardrails. Here is the arm which I have provisioned for this demo, the open search the index name. These are just name index is really not created completely. And then these are the data files which we are loading, and then the fine-tune models which we have already deployed. So we have the, this file. Now, coming back to the index creation. So first thing, you load the necessary libraries and the configuration. And then, in order to do the multiple, you know, the parallel data loading, uh, we want to create it, the threading, how you want to create it. So I actually use the, you know, the leverage the some formula like based on your CPU memory and the things that's available to the system. Come up with how many max worker worker you required, what should be the back size, and because systems should be able to run it, right, with the parallel threads. So those configurations, you'll find it from here, and then you define the, the real part. This is a real thing, right, which generates embedding. So you create a class, and if I come here, right, if you see here, I mean, we are loading all the data. Uh, from, I mean, what you are reading it from the sample load files, and then you see this embedding generator config, right? So, all that, um, I mean, the configuration, max, parallels, regions, bedrock model ID like the bedrock model, I mean, I forgot to show you here. I mean, we are defining the configuration, like which model to use. Here, we are using the Bedrock embedding models, so we provide all this information. So first, I have created the class. And then we have, uh, we are doing this, the open search index creations, right? So we opened up the open search client. You see the code. I'm opening up the client, and then the client is created, then we are creating the index. And here is the index definition. So with a few short samples, we have the 6 column, and you will see the mapping of these 5 or 6 columns are written here. And we are creating this vector, uh, with the vector dimension 1024, which we have passed it as a parameter. And then the index is created, and we started the loading the data, right? And you see that 334 few short samples have been loaded into the memory. And then we started doing the parallel processing, right? So we just split the data, we create the indexes, we define the batches, and we actually kicks in the data loading. And it, at the end, you will see it will, it is creating the 7 batches, and each batch has something 50 records. And then you can verify the index definitions. You can see the code here. So document response, and actually, I'm doing the, uh, you know, the loop-based search. Because once you load the data, it will not immediately reflect into the status, you know, and we are doing this live runtime. So I thought it should stop only once all the data is loaded. So you will see here, once it finds all the records is loaded, we are showing the index is created successfully. Same way, you can actually create this, uh, another index called ICD index creation. And you see, right, because of the parallelisms and all, I'm able to create this index in less than 1 minute or 2 minutes. If I do the sequential, I'm telling you this index creation will take at least 15 minutes. And same way, we, you can actually load the, this is a bigger index, the core index, uh, which actually contains the 75,000+ records. Similar logic, you can import the functions, you can create the parallel processing and load the data using the mezant Titan embedding model. Now, moving on to the real part, right? Uh, let me zoom in. So, In this setup, I mean, the whole thing, right, you have seen in the architecture diagram, once you submit the user input, you kick the lambda function, and in the lambda functions does all the things like the calling the bedrock guard rails, LLMs, similarity sets, perform the postprocessing and everything. This is what I'm going to walk you through, uh, with, uh, this, uh, uh, notebook. So there is an actually Python code and we have created the lambda equivalent function so that actually I can show you everything. What I will do, let me, uh, run this also from the Jupiter, one second. Let me take a new session. OK. Oh, not this one. Lambda functions, OK. So I'm just uh going to run this notebook. OK. So you'll see the Jupiter notebooks, it will launch. And again, similar, I mean, we have the Jupiter notebook created from the lambda functions and uh here we load all the necessary configure parameters and the dependencies. And then here I want to focus on these two things, you see, uh, there is one modified prompt template and second one is the EXP 327. Before I do anything, let me run all the cells so that while I explain the things will be run and you will be able to see the output of each steps. So first thing when you load this data, uh, as you see here, uh, we are importing these two key necessary files. And what are these two files? So first thing is modified prompt template. This contains the information like the prompt information, right? You are sending the information to the prompt. So the prompt definition, you see, uh, we have created like the dynamics so that we just replace the procedures name, narrative, and impression from the input, and uh we have some text. OK, this is what you are like the expert of the IACD medical coding assistant and all. You inform you, so this information will be used as a system prompt. And then similarly, we have the definition from the prompt 2, we have the definition for output, which format you want to see the output. So those format is written here. Like if you extract the phrases, the phrase extract, this is how it should be. And when we extract the phrases, this is another key thing, you know, we are doing two things, positive and negative identification as well. Because you may have received, let's say the doctor says, uh, you have a particular, you are diagnosed with a particular thing X, but no symptoms for Y. It can have that both information. And but you want to generate the ICD 10 codes for the positive things and to negate the things. So both things are important. What you are identifying, is it as a positive or the negative sentiment? Both. So these things we capture as an output format itself. And there are two formats, output, output 1, and output 2, because we are making the two LLM call, and there are two different prompts, and those definitions are defined here. And then this is like the most important file in this setup, which is actually, I will say the brain of the lambda functions it comes from here. So what actually it does, I mean, we, it has a library of all the necessary functions like you are making the bedrock, let's say the invoke model call, or you are doing the similarity search, and you are doing the vector embedding. So all these configurations and the things are defined here. And everywhere else in this, uh, Jupiter notebook, we will be calling this, uh, you know, I mean, These functions, definitions directly from here and all the things are defined here. Now, let me switch back to, to the real code. So as I said, we import all the necessary files and the dependent, uh, configurations, and then we load these configurations. And this is again, config file because we want to extract the details like the open search serverless serverless endpoints, bedrock endpoint, cartridge information. So we load this information, and once this data is loaded, we initialize the uh clients. And here you see, right, the first two point is very important. Here, I said we are going to use the, I mean, clot Sonnet 4 model and there, there's an inference profile is required in order to make an LLM search. So these two informations, actually, uh, you can do it. Either you can do hard coding or you can provide it anything here. In this example, we are using the Sonnet 4 configuration. And in the AWS, uh, you know, and we are using the Bedrock renting, uh, the Boto client. So you will define this configuration, and then you move to the uh openSearch client setup. Again, just like uh the openearch client connections I want to open. And then let's see they're testing the data input, right? Based on the setup, whether you are able to at least invoke the model or not. So we have the procedure names, narrations, and impressions. Uh, so these three things are the input. You see, I mean, the data is able to load successfully. And then the, as a first step, if you remember the architecture, we did the guardrails. So let me, uh, tell you what exactly is the guard rail information, right? So in this setup, we are using the bedrock guard rail. And uh we have enabled a few information like, uh, harmful content, right? You really want to, uh, block or protect, you want to address it. So we, we defined all the content filter to address the hated commands or sexual comments or all the things. You can actually say you, you don't want to use the system. And then, uh, if there's a prompt attack, right, maybe malicious way you want codes are inserting, so you want to protect from there also, so you can enable the prompt, I mean, enabled. And then you see the denied topics. I have actually written the non-health, uh, let me maybe zoom in further. You see non-health topic, I really want to block it. So if you come to the system, uh, the streamly tab or the lambda function, if you ask, hey, what is my hobby, how to hack the system, I don't want to answer that. So those information we have written and we have defined what does it mean by non-health topics. And then the PI protections, right? I was talking because if by any chance, if the medical coders or biller, they enter the patient, the PI details, we want to, we want to mask it first. So the name, phone, email, all these 2, there are 31 parameters we have defined to mask it. We have also enabled the contextual ground check, grounding checks, so where before we send the output we validate the towards the accuracies to improve the accuracies, and these are like the blocking messages, right? So it's a very easy configuration to the bedrock address you can do it and then we apply it into the system. So, you have the prompts input and we call this Bedrock guardrails, right? Guardrails is enabled, um, and, uh, here you see, we are calling the validate and mask input, this function. OK. And, uh, we test it against all the parameters, and then we, if there isn't a mask data, we will mask. I mean, if sensitive data, we will mask. If it is a blocking data, we will send a blocking message. So you will see, I mean, because there's a very relevant records, we are not masking any information. Whatever the details you see here, the same validated results will come here. I will show you another quick demo if I write some PI details and it will mask the data as well. And then, uh, you have the prompt, now the process prompt. What we do, the first thing is a few short samples. If you remember, we are talking about the two index creation, one with few short and second with the real ICD 10 codes. And why we do that? Just to improve the accuracy. Uh, so what we, uh, we have the impressions, expression, uh, and the narrations and the procedure information, and we are creating the dynamics, a few short examples. Uh, from the index 2, and if you see here in this line that we are performing the similarity search to get the top 22 matches, top 20 matches, and, and, uh, for a few short examples, we are creating it from index 2, and this definition of the similarity search and everything is defined into. Uh, here, uh, and the EXP3_27 depi5, which is nothing but the brain of the the main the core, core library functions, we are using it for ICD 10. So once we have this, uh, we get the few short examples, we get the UC it retrieves the 40 few short examples, and it will embed it, and then it will perform the first LLM call, first phase extractions. So in the phase extractions, if I come here, we are invoking the, uh, get invoked bedrock models. This is actually, and you see this is a final prompt one and the model ID Model ID is a sonnet. And we send this, the system prompt, which we have created through the prompt template, we submit it, and then you will see, uh, the first LM call has been done and it has generated a top 20 matches. Just for demonstrations purpose, I'm not printing all the records. Maybe the top 5, I think 1st 500 lines or something, uh, we are printing. So we retrieved the context from the vector databases and Then we, here, right, I said the top 20 matches, top K20 matches because this is a parameter, we are passing it to maze that vector dB search to get the top 20 best matches for the clinical phrases and the index one. And why, what we are doing it. So we got the phrases. Now we want to also generate the ICD 10 codes. So you have the narration, you want to generate the top 20 matches, and then top 22 matches, you want the ICD 10 codes for all those matches. So this information you will get here. So for the one, you will see, um, here. I am just printing the 1st 500 characters, but actually, it's a very big, uh, file. I mean, it will create with top 22 matches. And then once you have the top 20 matches, you perform the second LLM call here. And for that, you have the prompt 2 and how you are creating the prompt 2. I mean, the output which you received from the previous steps, you send it to the LLM and, uh, You apply this. This is the key thing. Get in work, bedrock model with guardrails. So here you actually perform the 2nd LLM call. And the previous, I mean, the final prompter which we have created just before, and we apply the guardrails for the output. You may be thinking, right, why we are applying the guard rail again once we are applying. So maybe let's assume you have the knowledge base. You may by any chance, LLM may also create, you know, sometimes, uh, security information because it's available into the knowledge base. So we want to bypass that rule. So what we do, we are applying the guardrails again to address two things. If again, there's a PI information which was stored in. The knowledge base, we should not be passing it. And the second thing is to enable other things like the ground, uh, grounding, right? We want the grounding checks to happen. Maybe let's say if you want to enable the ARC you can enable that. so that more accurate response, I mean, you will get it from the bedra. So you call this and finally, you will be able to see, uh, the system's final response. You will be able to get the JSON format. So it has extracted the ICD 10 codes for this phase one, phase 2, phase 3, phase 4, and all, right? All the phrases it will extract, you will see the respective phrase and ICD 10 codes. And then because we want to see in the more readable format, we have done some extra processing to see these are the four phase phrases it has created. So this is all about, I mean, minimum core lambda functions which actually does all the jobs. So you have seen like, uh, let me show you again, uh, one more thing, the, the guardrail stuff. So what I will do, I will quickly run. The stream le App, uh, let me run the stimulate app here. OK. So it will take us here. And let's say if you, uh, you know, I mean, we have recreated some of the system prompts so that you can see. And let's say it's some harmful content, right? You are giving the narrations like how to make the explosive weapons or something like this. If you ask, you will see. Uh, you will get the output. Guardrail protection activated. Sorry, I cannot answer these questions. And this messaging is coming from what we have configured. And if we have, let's say, the PEI data and, uh, you are passing it here. So I can. So load the, let's say user has given a medical coder is giving the information like this SSN number and all, and it generates the code. So here you will see, uh, the PI data will be applied, and then you will see the output. Before I go further, if you have any questions, feel free to ask anything what you have seen so far. I'm more than happy to talk to you and team, yeah. Rough metrics on how well the system did, like how well the rack for LLM did, and then how well. Yes, I think that's a good question. And in the second part, when we are going to talk about, you know, the fine tuning, we are going to also show you some comparison with the accuracy, with the speed and the things, yes. And uh before even we build the systems, definitely we, we have the ground uh sample data sets, right? So where you can actually see, OK, this was the phrases and this was expected ICD 10 codes and the descriptions, we are able, we are giving these matches and we verified as well. So it's definitely it is working. Uh, you can actually create the evaluations, uh, to make sure this is really working, but yes, this is not part of the demo. But yes, definitely you can do that. But there are some comparisons across the two different approaches we are going to show you at the end. And as you see here, right, uh, the same thing, the PA masking was applied and So this was the original input, but when we submitted to the LLM, this was the input was submitted to the LLM like the patient name, we just make it anonymize everything and we send it to the LLM and then finally, it generated the ICD code for that. And I mean there are multiple options. I mean, at this point, uh, as you see here, I mean, I am using the rack-based approach that this option is there. There's one more option available, fine tune, which we have already deployed, and my colleague Vina is going to talk to you, uh, talk about the fine tuning approach of the implementing the same solution. So, with this, actually, I want to pass control to my colleague, Prina. Listen. I think it switches to a. OK. All right. Thanks, Sudhir. So, so they gave you a walkthrough of the LLM and, uh, RAG approach. Now we look into the fine-tuning approach. So this is going to be a little bit straightforward compared to the RA approach. It's as simple as fine-tuning a model and then using that fine-tuned model for running the inference. So obviously, it starts, fine tuning always starts with sample data set, right? Um, here, in this case, Sudhir already showed you the sample data set. I'll show one more time, but we have about 333 labeled examples. And these examples are ICD 10 coded for medical impressions that are ENT based. So our fine-tune model will be specialized for ENT cases. The reason is fine-tuning takes a lot of time, right? Even for this, it's going to take upwards of 3 to 4 hours to fine tune the model. Uh, if I were to do this for all, every single ICD 10 code, it's going to take me days to fine tune the model. That is the only downside of fine tuning, I would say. Um, so in this case, we're going to take the training data set and then split it into test, training, test and validation, 80, 80/20 split. We'll put those, uh, training files into S3 and then call the Bedrock's model customization API to fine tune the model using a fine tuning job. And once the model has been fine-tuned, it's ready to run inference with it. And similar to the previous approach, we'll pass the medical impression, which is the user input. We can either use the streamlet application, which, so they showed you. We can also use direct invocation. We can use, uh, Jupiter notebooks that are multiple ways, um, and it's going to call the lambda function. This lambda function is going to be pretty simple. All it, all it does is just invoke the fine-tune model and then you get the response of it. And similar to the previous case, we'll also have input and output guardrails to redact the PII so we get the sanitized user impression. And once we get it, we can use the fine-tuned model to get the response of the ICD 10 code. And as I mentioned, the model's knowledge is there in the model's weight itself. So it's going to be much faster compared to the RAG approach. Um, to one of you who asked the question, you will see a comparison of the performance between the fine-tune model versus the RAG approach. Um, and similar to the previous case, the output guardrails get applied, and then the ICD 10 code is displayed to the user in the streamlet application or Jupiter notebook, or wherever you call it. So now I'm going to walk you through the code. Quickly switch to the other screen here. You just make it a little bit bigger. All right. So start, starting off with the sample data set. Uh, so this showed Kiro ID I'm using VS code here. VS code also has QDev extension that you can ask to do like the similar, similar approach, right? So you can ask it to run commands on your behalf. It has agency coding enabled as well. You can turn on or turn off agenttic coding. So in this case, uh, we, first, I'll walk you through how we fine tune the model, right? So here's where you see the sample data, which Sudhir already showed you as well. Um, this has all the ENT cases. There are about 333 labeled examples. So it has medical impression along with the corresponding ICD 10 code. And this is the sample data with which we fine-tune the model. And I'll now show you the fine-tuning job, uh, how it looks like. So for the fine-tuning job, we are using Amazon NAR model. What they showed you was cloud model with RAG approach. We use Amazon Nova Pro model just to show variation. And also the fine-tuning is not supported in, uh, NA, I'm sorry, in cloud 4.5 yet. It is yet to come. Right now, fine tuning is only supported for cloud 3, which is an older model, which we did not want to use. So for a better comparison, we used Amazon Nova Pro here. So now, uh, I'm just going to clear the output of all cells. And then I'm just going to run this. So this is a notebook to run the fine-tuning job. It's going to submit the fine-tuning job to the Bedrock model customization. Um, and here what we're doing is I'm just submitting, I'm just like, I just have the custom model name here, which you can configure, um, the S3 bucket and all of that config stuff. Now, loading the training data, here's the part I talked about. I showed you the training data set here. This is the sample data set that we're going to use. There are about 334 rows that we are loading from this particular sample. Um, and then we're also doing a little bit of data massaging here. Uh, we are doing this to remove any duplicates because you cannot fine tune a model with duplicate data set. So, all we are doing here is we're just grouping it by impression so that we get one row per expression. And now what we are doing here is we are formatting this input for fine-tuning format, which is expected by NOAA pro model. Every model has its own requirements on how the training data set should look like, schema requirements, right? So all we are doing here is conforming it to the schema requirement that is expected by NOAA pro model. So that's what we are doing here. And then we are splitting the data into training and validation, 80/20 split. Um, and it's going to write the two files, training file and the validation file. And then we are just uploading these two files to the S3 bucket. So you can see here. So we have the training data, 80% of the data, and the validation is the rest of the 20%. And now we are ready to launch the fine-tuning job. And for launching the fine-tuning job, you would use Bedrock's create model customization job API. And this is the API you would use to fine tune the model. And we are, we are providing the model name and all of those, uh, uh, config stuff here. And the main thing is we are passing the, uh, fine tuning, uh, the, the data set. Our state doesn't have the closing plan. Oops. I think I may have. Done some Stuff here, but I'll show you the other one where I have the output story here. One sec. Yeah Never mind, but I'll show you the output first before running it. So here you can see, uh, the training and the validation, which we have already uploaded to S3. And you can see here's where we are passing the training data as well as the validation data to this create model customization job. This is what we uploaded to S3. And then we're also passing some hyperparameterss here. Here, we are just setting standard epoch count, batch size, and all of that. And then we are just starting the fine-tuning job here, right? So, this is the fine-tuning job we submitted. It'll take about 3 to 4 hours to complete. You can check the status through notebook, but since it takes 3 to 4 hours, you might as well check that, right? Um, now, if I submitted the fine-tuning job, you would see that job here, right? So this is the job that we submitted, which I stopped, but I already have the model ready for you so that you don't have to wait for 3 to 4 hours. Um, so this is the model. And you can set up inference. Once you fine-tune the model, you need to set up inference so that you can invoke it, right? And there are two ways to set up inference. You can purchase pro throughput. This used to be the only way earlier, you need to purchase throughput in Bedrock to be able to invoke the fine-tune model. But in September, we released a new feature where it allows you to deploy the fine-tune model using on-demand, uh, token-based. So that's the most recommended approach and that's what we recommend you do as well. So you can, uh, create like a deployment, right? So you just deploy using the on-demand inference and then you can create the deployment. But I already have a deployment created here. And for all of this, again, you can find the instructions in the Gitlab, uh, report, which Sude shared. And we already have the deployment, so I'd like to walk you through the, um, Uh, lambda for this. So for the fine-tune model, we have a separate lambda function, right? And within that, so once you have the inference, you will have access to the on of the model. So let me go to this one, for example. Here you're seeing the deployment on. This is actually the on you will use to invoke the fine-tune model, right? So basically, that's what we are going to do here exactly. So if you go to the lambda function here, let me make it a bit bigger so you can see. Oh, sorry, fine tune. So this is the fine-tuned model on which I just showed you. Once you create the on-demand deployment, you'll, you'll have access to this fine-tuned model on. And all we are doing in this lambda function is just simply invoking this fine-tuned model on with the payload. And what is the payload is going to be the medical impression, right? Basically, the whatever payloads, so they showed, we are basically sticking to the same format. You'll pass the impression. This is the prompt we are passing to the fine-tune model, right? So you're going to just ask it to extract ICD10 code for this medical case and you're passing the narration, the impression, and the procedure name, very similar to what they showed. Um, and then we are just calling the fine-tune model using the invoke model API, which is the API from Bedrock to invoke both base and fine-tune models, and then you're passing the prompt here and getting the response out of it. And we already showed the, uh, workflow of how you can invoke using streamlet. So I'll just show you how the comparison looks like for RAG versus the fine-tuned approach. So, as you can see here, uh, we just like said, we, I'm just going to directly call the lambda functions within the notebook so that you can see how the comparison looks like. So there is this lambda for RAG as well as there is a lambda for fine-tuned model. So we are passing 3 test cases here. Notice that all the 3 test cases are ENT based. That's because our fine-tuned model is only trained on ENT data set. If I were to ask a question on, let's say, cancer or pneumonia or whatever it is, it's not going to do a great job because the fine-tuned model is just going to default to the base model knowledge, which is again, going to hallucinate, right? That is why fine-tuned model is not great for all approach. Now, we are running a comparison test here. Where we are calling the lambda function for rag, here's where like we are calling the lambda function for rag here, as well as the lambda function for fine-tune, fine-tune model as well. Um, and we are just going to see how it compares in terms of latency, right, as well as accuracy. Now, this is the expected code. This is the manual coding. We know that for this particular diagnosis, this is the expected code. And it, both of the approaches did great, but I have run this a few times before, so I know that fine-tune model performs slightly better than RAG. Like if RAG, you get an accuracy of up to 97%, with fine-tuned, you would get up to 99% as long as you're sticking to the ENT cases, right? Now, going to the result, this is what I'd like you to focus on. Here. We are getting a summary of the accuracy as well as the latency for RAG versus fine-tuned approach. And you can see here, right? Um, accuracy wise, both did well in these tests, but With respect to latency, the fine-tuned approach was 86.8% faster compared to the RAG approach. And the reason for this is, again, the answer is coded into the model weight, so it's going to be much faster. Um, and compared to the rag approach, you do semantic search like twice. You first retrieve the top key results and then do a further refinement of it to get the, get to the top result, right? So it's going to call the LLM twice, of course. So it's going to be a little bit lower latency compared or higher latency compared to. Um, uh, the fine-tuned approach. So, and you, if, uh, and the last thing I wanted to show was in the streamlet application as well. So, So they already showed you the RAG approach with the fine-tune, we're just going to do something very similar. So if you click on generate ICD code with a fine-tuned Novaro model, Right? It's going to just redact the PIA information similar to before. And then it's just going to give you the extracted ICD 10 code, and it's going to be much faster if you do a comparison again with streamlet as well, it's going to be much faster. Um, so that's pretty much what I had to show in terms of demo. Let me switch back to the PPT and I'll hand over to Sanche who will walk you through one of the path to production implementations that he did for his customers. Thank you Sudhir and Vienna for giving the demo, uh, and giving us a walkthrough of the core perspective, how it looks like, what's the engine behind to work around. Now let me talk about what we have done, uh, and how we took this particular solution use case to evaluate or elevate to the next level from a production perspective. We implement the solution for one of uh healthcare, uh, medical school in the United States, and I'll give you some of the architecture, what it looks like, uh, code you already have it. I'll talk about the architecture, the production guided architecture, what you should do, what you shouldn't, and what are the learnings, what we had from some of that and some of the best practices, what we had. So from an architecture perspective, this is the architecture what we have implemented for the customer. There are two ways, the entire architecture is divided into 3 parts of it. On the extreme left hand side you see is more of an interface, UI layer. I will talk about the second, the green box is more our AI magic button where we'll have all the AI stuff of it, and at the bottom what we have is all the security, the foundation, the guard rails, the bells and whistles, what we need starting from the extreme left hand side, that's where from an implementation perspective the customer wants to allow both the mechanism A, allow me to embed the UI like how we show in the stream lit version of it. Also allow me from an integration perspective with an API that if you want to invoke from an existing workflow then allow me to allow to exist that also. So that's how we have implemented storing a website on an S3 bucket, a simple page where anyone can request or streamlate sort of a stuff backed by cloud front and Vv from a security perspective. Similarly, an API perspective, we had an API gateway and VAF, uh, to make sure that it's authenticated, secured, uh, it doesn't get compromised and all. Next comes the core part of the entire solution, which is the whole private thing around what we have is the AI processing layer. That's where the whole bucket is all the entire thing is sitting in a private subnet so it does not to make sure everything is within the customer tenant within private secure, safe because we're dealing with the PHI data. The start of the entire AI, uh, things from the S3 bucket, which is where the medical document lends into the bucket. Once the documents are available, Sagemaker AI process that document to generate the embedding and the vectorization of those documents, which is via open search then been converted into a medical context. Finally, the entire magic or entire big thing has been done by the Bedrock, which is what we just saw as a demo of what the model and how it looks like. The bedrock does the ICV code detection perspective and on top of that we have the entire guard rail to ensure we have the securities, uh, from both the ends, not just from the model perspective, but even from a prompt perspective that no one is not asking any question like how to make bombs or hack the system and nor the system is responding anything which is not making sense. So we have the bedrock or a guardrail on top of it and the entire result gets stored into Dynamo DB with help of lambda that you do some sort of a pre-pros post processing sort of a thing. What is what we saw in the demo that how that entire big Jason got converted into a very fruitful or meaningful friendly way of sort of a post processing. And not only just you can also take this to next level if you want to store it, send some alerts or display. You can customize this entire lambda from a downstream perspective. And at the bottom we have all sorts of best practices things such as secret manager for storing any sorts of keys or tokens, cloud watch for all sorts of monitorings, best practices, uh, KMS you have, uh, key management certificates managers, which is almost from an encryption perspective, encryption at rest encryption in transit. So that is what the implementation, what we have done from an architecture perspective. Now let's understand about what was the result, how it changed the life for that customer. So there are two kinds of results. What I'll talk about. One is the quantitative results. One is the quality results. Let's start from the quantitative results perspective, and this is what you can easily match back to what Vina just started the entire session about where are the gaps in the industries? How do you see what the regics pattern or manual approach, some sort of a manual human approach has a gap, how the solution has double click or targeted on the gaps perspective. So starting from the accuracy part, 92% precision on the real radiologic data, not the clinical data. So what we saw in the training, the 90% is not on the training data, it's actual data live in action, what we are talking about. 70% faster in terms of detecting the code. Then we have the AE faster time. What used to take in days now it can do in hours, and then it can scale 2000. The architecture what I just presented, it's completely managed and serverless, so we're not talking about auto scaling servers, which is all the best things to do, but everything is built on serverless, so it's scaled. So today if you talk about some clinic, some hospital which has maybe 10,000 patients, some may have millions of patients, it will easily scale. You don't have to do anything. You just have to focus on your configurations of your models and the guardrails perspective and train the model well. Don't worry about the infrastructure part of it. So that's about the quality part, quantitative part, quality part of it. Let's move to the qualitative things of it. Which is what we understood. There is some clinical list which is doing this manual mapping of it before the solution. There is some human in loop sort of a thing. This entire approach try to automate first. It was very flexible to integrate their system whether you have an UI or API base completely automated. That allows clinicalists to focus more their time to doing some strategic work better for mankind rather than trying to fix this manual overhead stuff on top of it, it also reduced things such as billing cycles, the medical billing cycles. It made the cycles faster which improved the entire claim processing and reduced the overhead which was having in case of rejections you file again you document again, it goes entire workflow again you start from a claim that overhead got eliminated, which is what how it not just automated the entire solution for it but it digital foundationally transformed the radiologist department how they operate with scale. Accuracy and speed so they can save all of their time to doing a lot more strategic stuff rather than doing this mapping of the exercise which is human dependent error prone and all. So that's about some of the metrics what we have captured from our implementation. Let me talk about some of the best practices. Anyways, we have talked about a lot about guard rail guardrail, but there are some more best practices also what we have seen on top of that, which is what I want to double click here. One is security and responsible AI. With great power comes great responsibility. Similarly, we need to make sure that Guardrail, my co-speaker, I've already talked about the Guardrail perspective. They have already shown you the demo, how it works in actual. But there are some more things what you need to make sure about the responsible AI perspective that guardal is not just for one end perspective. It has to be both and from the prompt engineering as well as from a model output perspective. The guardel needs to be imp uh implemented on both the sites for defense in depth. Next is the entire, the guardrail part of it, not just. From any uh random output but even from a PII data perspective or PHI data because we cannot rely on any data set. It, it's in healthcare, so we have to be very sensitive and crucial about that data. Next is anti uh next is a hallucination. One of the important thing people always talk about what if it maps to a wrong thing or hallucinate? Then in that case you can use contextual ground, uh, guard rail checking. To filter the hallucination part of it. So that's how not only just we uh focus on getting the right model or optimize it, but also implemented on top of that to be secured and responsible. Next is the cost and the performance part of it. There are certain ways you can do cost performance part of it. Quickly to talk about you can do use model evaluation to figure out what is your right model. I think someone already asked the question to figure out what is the right model, how you do bench comparison and all. So you can use the model evaluation for comparison. Uh, you can use batch inference if you have thousands, millions of documents. You can use batch inference. You can also use prompt catching, which I have not seen widely used a lot in many implementations to save the token and save the response and get more speed. So that's how you can improvise the cost and the performance. Robust application you can use cross region perspective you can use rag evaluation to figure out where it's going to fail so your cover up like as always been said plan for the failures and that's how you design so you can cover from a robust application perspective. Next is the operation you can. Use cloud watch, cloud watch metrics, uh, cloud trail. You can use the feedback loop again it's very important. Don't rely only on generative based system have human in loop. Make sure that the system learns constantly and there is a feedback given so it works as required. So these are the couple of things what we have implemented on top of the solutions which my co-speaker I just talked about today. Here is the material. I know everyone was taking up pictures, but if you want to go through the code, here are the materials, which is what you can scan the Gitlab repu, which is what Sudhir and Vienna were talking about. So it has all the access. This rapport, uh, scanner code has everything, the Bedrock workshop, the code what we demonstrated, and other best practices to make the things production. With that, With that, we want to thank you so much, everyone. I know afternoon's first session is not going to be an easy one, so thank you for taking our time from Venetian to win and helping us out. Here are the cue codes you can connect with us LinkedIn, uh, to all three of our speaker. I know there are some questions in the room. Uh, since we have a session, would it be OK if we can take all the questions? Uh, after the session we'll be outside the room. Uh, so if you have any questions, if you want to understand what else you can do with generative AI or how we can automate together, we are standing outside the room and we can be happy to help you all of your questions. Thank you. Thank you, everyone.