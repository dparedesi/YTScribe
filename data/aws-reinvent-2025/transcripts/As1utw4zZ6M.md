---
video_id: As1utw4zZ6M
video_url: https://www.youtube.com/watch?v=As1utw4zZ6M
is_generated: False
is_translatable: True
---

Hello everyone, we're going to get started. Quick reminder to please put your headphones on. Hope everyone is having a great reinvent. Welcome to what's new in Amazon Redshift and Amazon Athena session. My name is Imran Moin. I'm a senior manager of product management within the Amazon Redshift team, and I'm delighted to have my co-speakers with me Scott Rigney, who is a principal product manager with Amazon Athena, and Sean McKibben, who is a principal software engineer with Twilio. In this session today I'm going to kick things off, talk about what's new in Amazon Redshift. Then I'm going to hand it over to Sean who will talk about how Twilio is using Redshift and Athena to transform billing and analytics at scale. And then finally, Scott will come up on stage and talk about what's new in Amazon Athena. When we look at customer needs for analytics, we see that broadly things fall into 3 main use cases. On one hand, you have customers that want to store and analyze vast amounts of structured data from their business systems, run SQL queries on them, and get insights on that data by visualizing it in BI dashboards like Tableau, Quicksight, Looker, etc. These use cases are well suited for a cloud data warehouse. On the other hand, we have customers that have a vast amount of raw, unstructured or semi-structured data, and this data is often used for advanced analytics or machine learning types of use cases. These use cases are well suited for a cloud data lake. And then you have customers that want to bring data warehouse and data lake together. They want to combine the structured business data with the unstructured or semi-structured data so that their teams can work off of a unified data foundation and they are able to get richer insights into all of their available data. The industry typically refers to this as a lake house. At AWS we have built a portfolio of services that are geared towards meeting all these different use cases. Amazon Redshift is our purpose-built cloud data warehouse that provides high performance SQL analytics on your structured data. Amazon Athena is a serverless way for customers to query their unstructured or semi-structured data stored in S3 using a familiar SQL interface. And then Amazon Sage Maker brings it all together where you can analyze your data warehouse data and data lake data together in one place so that your teams can get richer insights. Mostly for advanced analytics and machine learning types of use cases, and all your teams can work off of a unified data foundation. Let's begin with Amazon Redshift. Amazon Redshift has had a history of innovation over the last 12 or 13 years. When we first launched Redshift in 2013, it was the first ever cloud data warehouse. Providing massively parallel processing, columnar storage, and a familiar SQL interface at a fraction of the cost of your traditional on-premises system. When we launched Redshift Spectrum in 2017, customers had a way to query their data lake data stored in S3, bridging the worlds of data warehouse and data lake together. More recently in 2022, we launched support for Zero ETL where you could bring your data from your operational databases and business systems directly into Redshift without having to worry about managing complex ingestion pipelines and data processing workloads. Last year we launched AI-driven scaling and optimization for Redshift Serverless where Redshift would automatically scale up your compute clusters if there is a sudden spike in demand or query volumes, so that you continue to get the best performance out of your underlying redshift infrastructure. Along the way, Redshift has also invested heavily in optimizing the performance of your queries, which is why Redshift has 2.2 times better price performance than its nearest competitors, enabling you to get faster insights on your data at a fraction of the cost. Each of these first two market capabilities demonstrates that Redshift continues to evolve with the changing customer needs, which is why today it is used by tens of thousands of customers globally for their business critical applications. It processes billions of queries and exabytes of data every single day. As we look at 2025, Amazon Redshift has delivered a number of key features and innovations that can be broadly categorized into three broad investment pillars. The first one is cloud data warehouse fundamentals. The second one is distributed warehouse. And the third one is Apache Iceberg support. And I'm going to talk about each of these 3 investment areas. Cloud data warehouse fundamentals remain at the core of Amazon redshift innovation because they underpin every customer's trust and success in the platform. This includes areas such as security, performance, global region availability, etc. areas that are crucial for any cloud data warehouse to work well. Let's start with performance. Historically, Amazon Redshift has invested heavily in query performance, delivering up to 2.28 better price performance than its nearest competitor. In fact, for specific workloads such as BI dashboarding, Redshift actually offers seven times better price performance than its nearest competitors, which is one of the hallmarks of its leadership in cloud data warehousing. This performance is an area where we continue to invest, and we're not done yet. You can expect further improvements from Redshift in 2026 and beyond. Let me dive into some specific areas underperformance. First one is materialized views. Materialized view is an area of ongoing innovation for Redshift, where we provide you pre-computed results, query results stored in a database designed to provide you the best performance for your commonly used queries. This year we have delivered a number of key capabilities for materialized view, starting with in June we delivered incremental auto refresh of materialized views triggered by DM commits where any time there is a change in the underlying base tables, redshift automatically triggers a refresh of materialized view. This provides you with near real-time updates on your data without having to go through manual tuning. In July, we also released cascading refreshes across nested materialized views so that you don't have to go through complete refreshes every single time. And in September we delivered materialized views that are built on top of materialized views on shared data. Providing you the ability to optimize and pre-comput queries on shared data sets between clusters. Each of these advanced materialized view capabilities are designed to provide you with near real time access and insights into your data with minimal manual effort. I'm also happy to share that in September this year we announced general availability of a feature called Multidimensional Data layouts, or MDDL, which is a query-aware sorting mechanisms where Redshift automatically sorts your tables based on your query patterns and query volumes. I Unlike fixed column sorting, MDDL provides up to 10x better price performance for selective repetitive queries, and it far outperforms tables that are sorted based on single columns. What's even better is that once Redshift determines the right layout for your table, it automatically applies that layout using automatic table optimization. Thereby minimizing the amount of manual tuning you need to do on your tables on an ongoing basis. Security has always been job zero for Amazon. It is our highest priority, and it is the foundation of our customer trust. This year, in January, we enhanced the security defaults of Amazon Redshift. Now your redshift clusters are private by default. They are fully encrypted and they require SSL for every client connections coming into Redshift. With these new settings, it's easy for you to adhere to the best practices in data protection. And also minimizes the chances of misconfiguration that might happen on your part. Let's take a look at the 2nd big investment pillar for Redshift in 2025, which is distributed warehouse. As big enterprise customers continue to scale the redshift environment. We had to reimagine what the future of cloud data warehouses would look like. Last year we outlined a vision where customers would have different analytics workloads such as BI dashboarding, ETL ELT ingestion. Real-time analytics, etc. where customers would run all of these different analytics workloads on their own dedicated clusters optimized to provide the best performance for each of those workloads. All of these clusters also were designed in such a way that they can share one common data set. They could either read or write to a single copy of data. This year, we have enhanced this architecture even more, and added a bunch of new features and capabilities. This architecture is available in two different flavors. The first one is hub and spoke, where Different workloads run on their dedicated clusters, but they read and write from a single copy of data that is tied to a main cluster or producer cluster. The second architecture is a data mesh architecture where each cluster has its own copy of data, but it securely shares that data with all other remote clusters that might need access to that data. This distributed warehouse architecture is designed to provide best performance for each of your analytics workload. It provides workload isolation and it also provides more granular cost visibility and chargeback capabilities so that enterprise customers that have different business units or different teams can deploy this in a secure and scalable manner. Next, I'll talk about Amazon Redshift Serverless. Ever since we launched Serverless in 2021, we have seen many enterprise customers adopt Serverless for their business critical needs. This year we have made Redshift Serverless more cost effective, more flexible, and more easy to use. In March we launched support for trailing tracks where customers that required more stability and more time to test new releases before they are deployed in production are able to do that. All you have to do is simply set your surveillance clusters to be on trailing track, take your time testing those features out, and when you're ready, you can deploy those new versions into production. We knew that a lot of the government and federal agencies required a high degree of security and compliance standards, so I'm very happy to share that in May this year, Redshift Servalless achieved FedRAM authorization so that your federal contractors, government agencies, and regulated industry customers can now deploy redshifts servers in their production environment with confidence. In June this year we deployed support for 4 RPUs, which is a very small form factor, down from 8 RPUs which we used to have. So customers that have smaller workloads can easily get started with redshift serverless with lower cost and still get the benefits that serverless offers. Four RPU costs as little as $1.50 per hour, so it's easy for you to get started for smaller workloads. And then in July this year we also made Serverless easy to use in environments that have constrained networking. So Servers now requires only 3 IP addresses to get started, and you can deploy it with only 2 availability zones. It is fully compatible with IPV6. So if you have regulated or constrained networking environments, it's easy for you to get started with Servers. Now while Redshift Servals has seen tremendous growth since we launched it in 2021, we heard from a lot of our large enterprise customers that had predictable usage. They were asking us a way to get discounts and lower their overall cost of Redshift. So I'm very happy to share that in April this year, We launched support for Redshift serverless reservations where you can commit to a certain number of RPUs every year and get up to 24% discount off of your list price for Redshift serverless. This option is available in two different flavors. You can pay all upfront or you can pay partial upfronts. What's even better is that these reservations are applied at the payer account level and can be shared across your multiple different AWS accounts, thereby giving you a lot of flexibility if you have if you're running a large enterprise architecture where you have different business units or different teams that might want to benefit from the er reservations. Centralized identity and unified governance. We knew as customers continue to scale the redshift environment, centralized identity and unified governance are very important. So I'm very happy to share that only a few days back, Redshift launched support for centralized identity and unified governance by integrating with AWS Lake formation and AWS Glue data catalog. Now you can define your, you can log into any redshift cluster using your IAM identity Center credentials or simply your IAM credentials. Define fine-grained access control permissions like row level security, column level permissions, or dynamic data masking, and have those permissions apply to any cluster or any warehouse across your entire redshift environment. With these features, You can log into your users can log into any redshift cluster, define permissions, and have the confidence that those permissions and identities will apply to any cluster in any warehouse across across your redshift environment. Another area of investment for us as we build out the vision for distributed warehouse was concurrency scaling. Ever since we launched concurrency scaling in 2019, customers told us how much they value the fact that their redshift clusters can automatically and dynamically scale up any time there is a spike in query volume or demand. This year we have enhanced the concurrency scaling feature so that all of your ingestion, transformation, and consumption workloads can easily burst across different clusters using concurrency scaling. I'm happy to share that very soon you will have support for additional workload types on concurrency scaling such such as autocopy from S3, 0, materialized view creation, spectrum queries. Refreshes of streaming materialized views. All of these different workloads can scale across clusters using concurrency scaling. In fact, these features are already available to you in all the regions except ID, and we are rolling our support for I very, very soon here, after which we will make public announcements around this. Related to concurrency scaling, also happy to share that in October this year we launched support for D and D commands. So you can now use create table-like and alter table commands on concurrency scaling clusters, which makes it very easy for customers to do complex table operations and manage injection pipelines across clusters. The 3rd big pillar for Redshift this year has been around iceberg Apache support. We are seeing an increasing number of analytic customers adopt Apache Iceberg as their open table format, as it provides a high performance open source solution for customers to build their analytics architecture. Across AWS we are fully behind Apache Iceberg, and we are adding support for Apache Iceberg across a variety of different data and analytics services at AWS. In Redshift, we have also added support for Iceberg, and I'm very happy to share that Redshift now has support for Iceberg table rights where you can create and insert iceberg tables directly from within Redshift. So building and last year if you remember we added support for Iceberg read capability. So building upon that, now Redshift has support for both Iceberg read and write capabilities, giving customers maximum flexibility to build their analytics architectures on top of Iceberg. We have also made a number of enhancements in terms of improving the performance of data lake queries from Redshift, and I'm happy to share that based on a series of query optimization techniques, Redshift now delivers 2 times better price performance on data lake queries that are run on data lake tables built on top of iceberg. The last thing I'll mention on this is that we have added support for auto refresh of materialized views built on top of iceberg data. The way this feature works is that it periodically pulls iceberg buckets to see if it pulls S3 buckets to see if there are any new iceberg files, and any time it detects a new iceberg file, it auto triggers a refresh of materialized views. The benefit of this is that US customers get access to near real-time updates, near real-time insights into your data without having to do this work manually yourself. So so far we have discussed why it is important for customers to have a platform that has very, very strong cloud data warehouse fundamentals. We've talked about performance. We talked about security. We talked about distributed warehouses. We talked about global region availability, etc. But what about the end customer use cases? So when I look at the landscape of what customers are doing with Amazon Redship, I see that. These use cases fall into a variety of buckets. If you look at the left hand side of this slide, Redshift has historically been very strong on batch analytics. So when customers want to run end of day sales report or end of quarter regulatory reporting, right, a lot of customers are using Redshift for that, and it's very, very sort of tailored for that particular use case. Another strong use case we see customers using Redshift for is BI dashboarding, where they want to visualize their structured data inside of BI dashboards, whether it's AWS Quicksight or Tableau or Looker or any BI tool of your choice. We are also seeing an increasing number of customers use red chip for complex decision support or ad hoc analytics. This includes things like market forecasting, customer segmentation, those kinds of use cases. And if you remember, all of the performance enhancements I talked about that we focused on this year are basically geared towards reducing your latency for first time queries as well as repeat queries. And as we continue to make improvements in that area, this use case is going to get even more stronger. Moving on, we also see Redshift play a very critical role in data processing workloads for and pipelines, where customers want to ingest this data from their operational databases directly into Redshift, and they want to transform this data either using AWS Glue or custom SQL commands within Redshift or use a third party tool like DBT. Redshift plays a very critical role in the entire data processing pipeline. And all the enhancements we've made around 0 ETL and the distributed warehouse architecture that I talked about earlier makes this use case even stronger. And then last use case I'll talk about is near real-time interactive analytics. We're seeing an increasing number of customers use Redshift for this particular use case, and for this use case, both your ingestion latency as well as query latency becomes very important. And so on the ingestion side, we have made a lot of improvements on zero ETL and as a result, we can bring the ingestion latency down to single digit seconds. And on the query performance front, we continue to improve query performance for both first time and repeat queries, right? So all of these enhancements will make this use case even, even stronger, and customers can use it for things like click stream analytics, IoT monitoring, etc. Now we have thousands of enterprise customers that use Redshift daily for their business critical applications. One of those customers is Twilio, that provides a cloud communication platform for you to embed your email, voice, and video directly into your applications. To learn more about how Twilio is using Redshift to transform billing and analytics at scale, please welcome Principal software Engineer of Twilio, Sean McKibben. Thanks, Imran. About a year ago, my leadership on Tulo's commerce platform charged us with reimagining our billing engine. The business needed new ways to deal with challenges of scale and flexibility, and our finance teams needed to have analytical views of our data that were not possible with our classic architecture. What came out of this project helped us modernize and streamline how we process analytical customer financial data to empower the business to imagine new ways of delivering our products to customers. Tullio's billing engine processes billions of usage events every day. Every text message, every voice call, every video session from customers like Toyota, Salesforce, and Shopify needs to be accurately priced and billed. Our legacy billing engine was built when Twilio was much smaller. It was laser focused on operational speed, ingest an event, price it, and move on. But that architecture created two critical gaps. The first was flexibility. The business wanted innovative pricing models like package deals, committed use discounts with overage pricing, but our system couldn't adapt without major rearchitecture. Unwinding pricing misconfigurations was a multi-day engineering effort, and backdating a discount was a bespoke, time-consuming process. The second gap was with analytics. Finance couldn't answer basic questions like what would revenue look like if we changed this pricing tier. Our BI teams couldn't build dashboards showing customer usage trends without impacting the production billing system. The operational system and the analytical needs were stepping on each other's toes, and neither served our business use cases very well. We needed a distributed data warehouse architecture that could handle both transactional billing integrity and analytical flexibility. We evaluated several platforms, traditional data warehouses, data lakes with query engines, even a split architecture with with separate operational and analytical systems, but our workload is unusual. This system generates invoice line items, so we needed financial grade consistency, guarantees against duplicate charges or missing events, but we also needed analytical speed. Most data warehouses. Optimized for analytics at the expense of consistency. Most operational databases can't handle analytical queries at scale. Redshift bridges that gap for us as an operational data warehouse. We get seriallizable transactional isolation, the strongest consistency guarantees possible, while maintaining analytical performance. That's the unlock. We can ingest events, aggregate with transactional correctness, and serve BI queries all in one system. Plus Redshift serverless gives us this without, without infrastructure overhead, no provisioning, no capacity planning. No, no failover management and price performance scaling gives us this means that our our compute adjusts automatically as our workload changes. We pay for usage and not for capacity. The result is that Redshift server list cost us 75% less than what our previous system did, all while processing more data with stronger consistency guarantees. And enabling analytics that were impossible before. Let's look at the ingestion layer. Billions of events daily flow from MSK into Redshift serverless. Every usage event across Tullio's platform lands here. Two architectural decisions make this work at scale. First, seriallizable transaction isolation for item potency. Events can have can arrive multiple times, whether it's from network retries or system resubmissions, but Redshift's consistency guarantees ensure that we only count each event once. No duplicate charges, no custom deduplication logic, the transaction model handles it. Second, auto refreshing materialized views for real-time aggregation. We're not batch processing overnight, as events land, they're incrementally aggregated by customer, product, and day, and our rating system downstream gets near real-time data. We've stored over 0.5 trillion events in Redshift so far, all deduplicated and immediately queryable. That's the foundation, now let me show you how we built on top of it. We didn't build just one big cluster. We built what ended up being a distributed data warehouse, multiple redshift work groups sharing data through data shares. Our AWS folks call this the golden architecture, and honestly once we understood it, we couldn't imagine building the system any other way. So we have multiple work groups all working off of the same data set. Ingestion is writing billions of events. The pricing engine is running transformations and aggregations on those events. Analytics is querying the results, but they're all looking at the same tables through data shares. We're not making copies or ETLing data between them. This gave us workload isolation and cost visibility. Heavy analytical queries don't slow down invoice processing. When we need to reprocess data, it doesn't impact dashboards, and we can actually see what each workload costs. For a financial system, live data shares are critical. Everyone sees the same version of the data in real time. So let's talk about what's actually happening in that pricing engine work group, because this is where it gets complicated. How complex are we talking? Our DBT pipeline manages over 200 million price points across Twilio's product catalog. We're dealing with multi-tiered volume discounts, promotional discounts that stack in specific orders, multi-product bundles where one thing affects the pricing of another, bifurcated taxes. It's genuinely intricate. We use DBT Core to break this down into manageable pieces. Each pricing rule is a discrete sequel model, and DBT orchestrates how they run. Redshift paralyzes what it can, serializes what it needs to, and DBT manages the dependency graph between them. The bonus here is that DBT auto generates documentation as the pipeline evolves. We can actually trace the lineage to see exactly how an invoice line item was calculated. That kind of transparency was impossible with our old system, but it's critical for customer trust and compliance. And here's what surprised us. Redshift handled these complex joins and window functions with very little optimization. We budgeted weeks for performance tuning and barely needed it. We can recalculate an entire month of billing in 30 minutes, where our old system took 6 hours or more. Midway through this process, we hit a bit of a snag. Our product catalog and pricing configuration data lived spread across about 7 or 8 Aurora databases. And we only had the current state of our configuration stored in them. The old system just priced everything with whatever was current at the time of processing. That's fine if you only want to go forward, but we needed the flexibility to re-price historical usage for corrections, promotional adjustments, contract changes. To do that accurately, we needed to know when prices changed and what they were at any point in time. Zero ETL with history mode, which launched in April of this year, fixed this for us. We integrated that with Aurora databases and got full change history flowing automatically into Redshift. Every price update, every configuration changed, time stamped and captured, no custom CDC to manage. The result is that we can reconstruct any bill from any point in time. We can reprice historical usage with exact configuration that should have applied, or we can model what invoices would look like under different scenarios. That's only possible with complete changed history. Let's talk about outcomes, starting with cost. We're running at a 75% lower cost than the previous system while processing more data with stronger guarantees. Four people on my team are managing almost 100 billion events a month. Speed. 6 hours down to 30 minutes for full month recalculations. Finance dashboards went from daily batch updates to subminute response times. But the real win is flexibility. We can now do retroactive pricing changes, accurately recalculate last month's usage with today's promotional rules. Product managers can model new pricing strategies against real-time usage, answering questions like what if we change this pricing tier structure. Billing ops can audit any invoice and see the complete calculation lineage in minutes. We're also generating new data products like an effective pricing model. That represents what any customer will pay for any product at any time. Direct rights to S3 and soon to iceberg tables eliminate Spark ETL jobs for data lake delivery. The previous system served us well, but it wasn't built for this kind of scale or flexibility. This architecture fundamentally changed what's possible for the business. The power of this architecture is that it doesn't end with the billing engine. We're about to start writing our curated financial data sets to iceberg tables. Indexed with AWS Glue data catalog, this opens up the data to broader organizational use without having to give everyone direct access to Redshift. Which brings me to the next part of our data story. Teams at Twilio have built what we, what we call the Odin system, a query infrastructure layer on top of these iceberg tables and the Glue data catalog using Athena. Scott's gonna walk you through how you can leverage Athena's latest features to democratize data access across your organization. Scott, Appreciate it, man. Thanks. Good luck, man. Appreciate it. Oops, sorry. All right. Thank you, Sean. Uh, thanks again, Sean. Really loved to hear the outcomes that Tulo has had, especially the 75% cost savings, which is really impressive at Tulo's scale. So as Imran mentioned, I'm Scott. I lead the product management function for the Athena service. Delighted to be with you today. Uh, like many of you, I'm a, a data nerd and I love working with data, so I wanted to gather some data from you all. Uh, so raise your hand if this is your first reinvent. All right, cool. And raise your hand if this is your first what's new in Athena session. All right, great to see some friendly new faces and, and, uh, previous ones alike. So as a refresher, uh, Athena is our serverless interactive query service that's designed to make it dead simple to run SQL on your data lakes. Customers love Athena's ease of use. Uh, we talk about it often. It's kind of what we really focus on, uh, on the product team, um, in every feature that we build. There's no infrastructure to set up or manage. It just works with the tools that you have today. Customers love how Athena's optimized for interactivity, meaning you get queries that start in under 1 2nd and a fast engine on top of great out of the box performance. Last but not least, our engine just works with the data that you have, so you can bring your parquet, JSON, Iceberg, CSV, and other data and bring that to Athena and start querying right away. And best yet we're able to pair Athena's query engine capabilities with really deep fine grain access capabilities through lake through our lake formation sister service. So we launched in 2016 and today we have customers from every industry, from startups to large enterprises who are running lots of queries per week. In fact, over billions of queries a week in our most recent kind of metrics. So customers are kind of getting a lot of insights and value out of their S3 data lakes. So let's talk about the customer journey with Athena and how that often starts with Amazon S3. So customers choose S3 for its unmatched durability, scalability, and availability. Because of those foundational capabilities and unique differentiators, customers have been able to build millions of data lakes on top of S3. But as Imran mentioned earlier, there's a big shift happening in data lakes, and that's open table formats. Folks love how these open table formats bring familiar SQL functionality to kind of where we're all working today, which is in our S3ba data lake stores. Athena actually launched its open table format support back in 2021, and since then we've seen Iceberg emerge as a leading choice amongst the open table formats that we support. Iceberg is unique amongst OTFs for its ability to handle massive scale data sets while maintaining simplicity, supporting multiple query engines like Athena and Spark, and providing you transactional features that allow you to write into your data lakes with ease, and it does that all without locking you into a single vendor ecosystem. So excited to share today that Athena this year is 1.5 times faster on Iceberg with Parquet based on TPCDS 1 terabyte compared to this time last year, and we've been able to achieve that through a number of changes, starting with our engine but kind of um kind of cascading out to our catalog experiences as well, but want to touch on a few of those. The first is Iceberg Statistics, which is an existing feature of Athena, but two weeks ago we launched a new update where we've actually enhanced the behavior of the Athena service when it interacts with statistics files to ultimately make queries run faster. So you use iceberg statistics, basically what you'll do is you'll fire up AWS Glue and ask Glue to collect statistics on your iceberg tables, and then when you come to Athena and you query those tables, Athena will automatically apply the statistics to make more intelligent query planning decisions, so automatically accelerating your queries without a ton of work to do, so it's a great feature. Uh, and so that's an awesome feature we launched just a couple of weeks ago, is an updated experience for Iceberg stats. We also launched a net new feature, Parquet column indexing, which allows queries to skip irrelevant blocks of data, which reduces unnecessary IOL operations when reading from S3. So this is like really beneficial, uh, especially on queries that have selective filter predicates on sorted data. So that's fairly common in, you know, a lot of enterprise reporting scenarios is to have sorted data. So great feature to try out and for our customers who are using lake formation today to secure their and govern their iceberg tables, uh, just a couple of weeks ago we announced new partition pruning behaviors for lake formation tables with row filters and column masks. And additional predicate push down behaviors which altogether will help improve query performance, reduce costs, and does that without sacrificing data governance. Uh, so on Sunday we launched a new feature for Iceberg which is called Materialized Views. Uh, this is a really cool feature. Um, this is essentially a managed iceberg table that stores pre-computed query results for you in an always ready to query S3 table, uh, that you can, or, or iceberg table that you can query through Athena and other engines, uh, super easy to use. They're automatically updated when source data changes, so your, um, kind of data is flowing through materialized views and again always available for you to query, so you're always seeing the latest and greatest. And it does all of that without any infrastructure to manage. Uh, so today we support Reed support on Athena, but you can also go out to Redshift, uh, Spark on EMR and Glue and perform additional operations over there. What we like to talk about with this one is like what are some good use cases for materialized views with Athena, and a lot of customers come to Athena and sort of build these decomposed SQL pipelines on top of this service. So imagine if you have a complex data transformation pipeline today that's joining and. a bunch of tables together to produce one single output table that's now something you can kind of reflect as a materialized view in glue data catalog and express that as SQL and get the always updated benefits that we now provide through uh materialized views. So really easy to get started with this one as well. You'll fire up one of our Spark experiences in EMR or Glue or use those Glue APIs to launch or create your materialized view. And when you're creating one, you'll get to choose whether you store your materialized view in a self-managed iceberg table or an Amazon S3 table, which is a great segue to all the good work we've been doing with S3 tables. So if you remember back to this time last year at Reinvent we talked about and announced preview for Amazon S3 tables as the first fully managed uh iceberg offering in the cloud. S3 tables is really great because it simplifies the tedious stuff of running an iceberg data lake, uh, things like, uh, table compaction and expiring old snapshots. S3 Tables handles all of that for you. So this year we've been really busy taking S3 tables to GA, which we did on Pi Day in March, and in that launch we added expanded DDL operations, so it gave you create create database and create table operators plus a bunch of others that made it possible to work with S3 tables end to end through Athena SQL. So we're really excited about what that allows customers to do. And in that launch we also added a new console wizard to help you get started quickly. In August we launched launched CreateTable as select, which is a very popular feature in Athena. We call it CTAS. What CTASk lets you do is convert data from one format to another, and so it's a really no-brainer feature to have with S3 tables. So imagine what you can do with this one is, uh, just to give you an example, use case, let's assume you have an application that's writing JSON data out to S3, and you want to query that JSON data, but you need a columnar level performance to meet the latency requirements of your use case. What you can now do is run a CTAS on that JSON data, convert that into an S3 table, and then tomorrow when new records show up on your from your JSON data stream, you can run an insert operation on top of that table to bring your new records over to your S3 table and drive your queries off of the S3 table instead of your JSON data. So that's a really cool feature. So in kind of the back half of the year what we've been doing is um optimizing the rest of the S3 table's experience and what's really great about how we're working with the S3 team and the work we're doing over there is that all of the features that I mentioned for Iceberg on the previous slides you also get with S3 tables. So, uh, parquet column indexing, updated statistics, all of that good stuff applies to S3 tables. But you're also getting the performance features that our partner teams in S3 and Glue data catalog have also been delivering, uh, like Zorder and sort compaction, plus a bunch of operational tweaks that they've been delivering to reduce latency across the experience. So now we recognize that Iceberg is not the only data format out there, and when we look at the data that customers are querying today, we see lots of JSON, we see lots of text, CSV, and other data formats, and we're proud to share that the majority of those queries are quite fast indeed, completing in under 2 seconds. We're able to achieve this through how Athena's engine is designed and tuned, but also because of how Athena provides the servalless compute infrastructure that automatically scales based on query complexity and is able to run multiple queries in parallel. So this year as we kind of looked at this data we asked how else can we accelerate or bring the performance bar higher for some of these additional data formats that customers are querying today. And that took us down a pretty interesting path of actually redesigning our file system and our readers and writers for a bunch of different file formats to bring faster experience when interacting with those files through the Athena engine. So we're seeing some really awesome results there as this slide highlights and just to highlight a few. Parquet, as I mentioned, is a super popular format. Is now 1.2 times faster when used with Hive than this time last year. JSON 2x faster. CSV, uh, yeah, folks are still passing CSVs around. I'm definitely guilty of that. It's now 1.8x faster compared to last year. And then Iceberg we talked a little bit about already, but Delta Lake is seeing a good boost this year as well. So when we talk about these features kind of rolling out, normally what you kind of hear is, well, now I have to upgrade to this new thing and that's a lot of work, but where we try to do things differently on Athena is is kind of work ahead of you and figure out better ways to migrate our customers and bring them along for these journeys. So what we've been doing with this rollout is over the course of this year we've been moving customers automatically to these new formats and our new file system based on compatibility checks that we run in the background. So chances are you're already getting these benefits, or you'll be getting these very soon in your production applications, so I'm really excited about that. Now we've also been shipping performance features kind of elsewhere in the service, so I wanted to highlight a few of those today as well. Query result reuse or we call QRR is a caching feature that Athena has. What's really neat about this is it can bypass query execution under certain conditions, and what what means in that case is you're bypassing the actual latency from the query engine itself, but you're also getting a query that, uh, ran in, you know, sub 100 milliseconds as you're sort of bypassing the execution and picking up the previous query results. So here we actually changed how our hashing function works to ignore uh small variations in how different people write queries. So a good example is code comments or use of space, uh, white space characters in your queries, and we've done all of that to actually increase the cache hit ratio of when you're using query result reuse. So that's a really sweet feature to turn on. You just toggle a switch within the Athena console or our APIs to activate it and. If the result is found, Athena will again bypass execution and give you the previous matching result. We also rewrote some of our memory logic way down in the engine with the purp with the goal of um reducing failures and making queries that have heavy shuffle and joint operators uh more stable at scale. So what we've, what we've done there is again kind of tweak a lot of the very deep kind of behaviors of the the engine internals to get better memory stability and efficiency out of the um out of the engine. So you should get fewer failures on queries that have a lot of process a lot of data in memory. So we'll switch gears a little bit and talk about some administrator features. Um, one of those is capacity reservations. This is not too different from, uh, redshaft serverless reservations, uh, and this is Athena's feature for guaranteed serverless compute, uh, on top of our kind of managed compute infrastructure. Uh, it's really ideal for mission critical applications, dashboards, any SLA sensitive reporting jobs you may have, as well as user facing applications that have a requirement for significant concurrency. Uh, to use it it's pretty straightforward. You create a reservation and then you choose a number of, uh, compute units which would be called DPUs. And then you assign this reservation to your workgroups which allows your reservation to share capacity across your workgroups. And what's really great about how this feature works is there's no sequel changes needed. You basically just configure this set up here on the screen and Athena automatically knows what to do and routes queries from those work groups to your reserved capacity units. So last week we announced two new features for capacity reservations. The first is capacity and cost controls. This is a new setting that influences how Athena assigns data processing units or DPUs to each of your queries. Uh, you can set it at the work group level that gives you a centralized kind of way to apply defaults, kind of min and max range for each of your queries. But you can also uh control uh DPU allocation at the query level through new features in our uh start query execution API that now now allow this property to be set, and that gives you precise task level performance control that customers who've used our preview of this feature um have been able to get um. Uh, very low latency out of their queries through experimentation and get very high levels of utilization on the reserved capacity. So really great feature. Last but not least on that one is we now have new observability data in our get query results API. So after you run a query on capacity reservation. We now tell you exactly how many DPU it consumes, so you can use that new data to help you plan for maybe future queries that have a similar shape and may have a different or a slightly different capacity need. So we also launched an auto scaling solution. This is really neat. This works as a step function state machine, so it's completely serverless and its entire logic is kind of presented to you as the state machine. So you can go into the step functions console and kind of explore and look at how it works, but that also means it's highly open and flexible to customization for different requirements that you may have. The way it works is it basically monitors the utilization of a reservation and it'll make capacity adjustments based on the parameters that you set for look back and how frequently and by how much you should scale based on your workload. To launch it or to get started, you have a console click through button that we've provided as well as documentation that provides you a link into the cloud formation experience. Managed query results is another admin feature that we launched earlier this year. This is a really great feature if you're an administrator out there because it simplifies your experience by allowing the Athena service to automatically store query results in our own storage rather than your S3 bucket. So we're kind of taking over, um, uh, the, the, the headache of having to manage query result files for you. And what's neat about this is we store results for a day at no cost to you and automatically expire them and uh delete them after that one day period with no action needed on your part. So no, no longer you need to take action to manage query result files or their life cycle, and what we also did with this feature is we actually elevated the permissions for query results to be tied to work groups instead of an S3 bucket. So it should simplify, uh, permissions um for, for folks who are accessing Athena through different work groups. So it's been a really busy year of launches and we couldn't get to everything today, but we want to make sure there's some key launches that you had kind of at least visibility on to check out while you're here at Reinvent. The first is Amazon StageMaker notebooks. I don't know if you've seen the, the keynotes or some of the other announcements on this one. This is a really cool feature. I'm glad we had a chance to talk about it today. This is a brand new native notebook UI in StageMaker Unified Studio. It's integrated with Athena SQL and Athena Spark engines and delivered to you as a single canvas, uh, providing you a data analysis and development, uh, experience in, in 11 spot. It's also multi-dialect, so in one cell you can run a SQL query on Athena, then you can run some Python code, then you can run some Spark code all in one canvas with data frame access kind of in between each cell, so you kind of work with data in memory and in a really snappy experience. So it's really cool. Uh, best yet is you can work alongside our data agent, which is a built-in AI agent that helps you do data analysis. So it's a really great feature if you're getting started to learn some new data set and want to extract some insights from it. You can kind of task data agent with helping you with some of that work. And kind of behind the scenes here underneath we've upgraded the Athena Spark engine to a 356, and we've also given uh Spark Connect support and Spark Live UI support to the Athena Spark experience. Uh, SMUS, as I mentioned, Sage Unified Studio, we've kind of continued on the notebook experience. We've also added a one click onboarding with IAM permission experience, brought a new AI powered column and metadata rules and many other features on our unified studio. So if you get a chance to while you're here at Reinvent, drop by some of the Sagemaker sessions to, to see what that's all about. Uh, at Reinvent this year you'll probably also hear a lot about trusted identity propagation, also known as TIP. Uh, this gives you a way to authenticate end users based on their corporate identities and have that identity propagate to all of the AWS services that they're using when they're querying and analyzing data. So that's Athena, S3, Glue, Lambda, KMS, all the services that, uh, Athena will talk to when, uh, querying your data, and that gives you end to end auditability on user actions tied to identity. It also plugs into lake formation so you can define fine grade access controls based on corporate identity and have those enforced by engines like Athena and Spark, and from a red shift in Athena perspective, we've also carried TIP support out to our drivers. So if you're logging in through third party clients like Data Grip, you now have a b browser-based authentication flows that use TI to again authenticate with your corporate identity. And on MCP, you know, a growing number of customers are developing AI agents and doing new and interesting things with, um, agents to perform different tasks. So we launched earlier this year several MCP servers, uh, Redshift, Athena, EMR, and Glue just to name a few, and we're seeing customers build really cool things on top of that, for example, you know, building agents to query and analyze and extract insights from your data lakes, um, all autonomously. So really cool progress on MCP. So as we wrap up, if you want to learn more about what we shared today, definitely scan these QR codes, get your phones out. Um, a lot of really awesome blogs here, uh, one on Redshift serverless reservations and Iceberg rights, you should definitely check out. There's one on data transformation with Athena and S3 tables. So as you think about that create table that select, uh, experience we talked about earlier, that's a great blog to check out with a lot of practical takeaways. And we've also included the Odin Deep Dive deck. So our folks at Tulio and our customers at Tulio who've who've, uh, built and support the Odin platform, they've got a really neat blog that describes their journey in building that on top of Athena, which we recommend. And then we hope that you've, you know, taken the learnings like from our session today further and that you can, uh, kind of leverage other, you know, learning and knowledge, uh, capabilities and resources that we have here at AWS, uh, for that we have AWS Skill Builder, tons of great resources for you to learn, practice, and get AWS certified, so definitely check that out and share that with your teammates. And so that about wraps up the uh 206 session, uh, what's new in Redshift in Athena. Definitely on behalf of the AWS team, I wanna send a thank you to Sean at Tulio for sharing his journey with us and Imron for sharing the insights on the exciting, uh, Redshift launches. So, uh, last but not least, thank you for attending and spending the hour with us today. We'd love to hear your feedback. So pop into the events app and give us your rating and have a great rest of the reinvent. Thanks.