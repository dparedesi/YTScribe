---
video_id: JRu2GqRe9sg
video_url: https://www.youtube.com/watch?v=JRu2GqRe9sg
is_generated: False
is_translatable: True
---

Good afternoon everyone. Thank you for coming to our code talk today. We're gonna be talking about AI agents for cloud operations, how you could automate infrastructure management. One of the key topics we're gonna discuss today is also how you could operate security operations. Um, and one thing that we're gonna discuss, the way we formatted today's, uh, coal is we're gonna start off talking about the core components of what an agent is, the type of integrations, tools, and some fundamental concepts. From there we're gonna jump into the AWS console and walk. Through a real demo, uh, and talk through the code and look at some of the outputs that come out of this agentic, uh, application that we develop. So my name is Omar Tabaka. I'm a solution architect here at AWS. Everybody, my name is Sean Abdi. I'm also a solutions architect here at AWS. So before we get started, by, uh, show of hands, how many of you have built out an agent before? It could be as simple as a weather application agent. We have one hand, two hand, OK, so a decent amount of you have built out, uh, agents within the. Um, my guess is AWS, maybe some other framework that we use, but it looks like some of us have already built out an agent. Awesome, so let's go and get started with that. Let's jump right in. So let's imagine that it's 2:00 a.m. and you're a security engineer that's just received a call and now you're thrust into a high pressure situation regarding a security incident. You log in and your first task is to go through a mountain of mountains and different logs, all in different formats, whether that's JSON or plain text. That could be stuff like VPC flow logs, firewall logs, API logs, cloud trail logs, even third party vendors that you might use for your on-premise or cloud environment, nothing native to AWS. So hours pass by as you try to piece together these different fragments like I mentioned, you might have something like guard duty alerts, cloud trail logs, Dynamo DB logs, security hub findings, and you're still trying to find out how all these different resources talk to each other, you know, they're from separate systems speaking a different language and are really not meant to communicate with each other. So the clock is ticking, you try to identify a suspicious IP, see if it was really a security incident or if we have a false positive on our hands, and you've just spent hours and hours searching, trying to figure out what's going on. At this point you're starting to get exhausted. You've done all the grunt work that something like AI might be able to handle. You spent searching through these resources with different tools because they're in different formats like we mentioned, whether that's JSON, plain text, or some other standardized schema. And you're figuring out what, what do I need to do now? So what if we can take this problem and use some sort of specialized intelligence system like some agents that can comb through all of these logs, comb through these different tools, and be able to provide you some insights that are actionable on your current risk, giving you a more precise path to tackle this issue. And that gets us to YAI for security operations. So the reality today is that everything is moving super fast paced. Attacks are becoming automated. People are trying to spend hours identifying what's going on, and we just traditional approaches with this manual process just doesn't seem to work for everybody. So AI has become the game changer in the sense that we can now analyze vast and complex data sets in seconds. You can detect the patterns and anomalies that a human might miss for that error prone manual process. And you can automate the repetitive tasks like triage and log correlation, and then you use these surface surface key insights to have something to focus on real threats and not the grunt work that you might see from a typical security engineer's work on a day to day basis. So let's start off with what's a typical LLM versus agentic AI. On the left hand side here you can see that we have our usual LLM and what this does is it takes the knowledge that it's already been trained on, providing a simple, you know, concise answer without doing many iterations or any thought process or reasoning. So for example, you ask it the question. When did the French Revolution take place, and I'll respond back to you with the simple answer of 1789. Now on the right hand side we have agentic AI and this is where things get interesting. The agent is able to think on its own, so let's go ahead and check a weather website for the forecast. Let's take a look and dive in and see what the forecast looks like for the next week. We've got some rain and it, but it's also sunny and warm next Friday, and then we can evaluate and reiterate until it comes to a proper answer for us with its reasoning capabilities to say next Friday is the ideal day for Mount Fuji hiking and the difference here again, as I mentioned, rather than just having a concise answer on knowledge that it's already been trained on. This can think for itself and iterate until it has a valuable response that you can use for um your triage or to get further information from the agent by asking follow-up prompts. So the key thing that I wanted to mention to start us off is like it's very important that your data is the key source for all of this agent building. You wanna make sure that you have the right data sources that you have everything centralized in a location that's easy to read formatted specifically for your security use case. This is where Amazon Security lake comes into play. For those of you that are not familiar, this is basically a service that we have that allows you to centralize all of those different tools that I mentioned, all the different logs, whether it's JSON, plain text, or something else into a data lake in your AWS account. And by doing that you're able to normalize that data from the different schemas in an easy, ready to available format to read which is called OCSF, the Open Open cybersecurity schemia Framework, and that way you can use a singular tool or a singular method of approach for your log analysis without having to worry about these data silos and how they are disparate to begin with. Now With that I'm gonna go ahead and hand it off to Omar to do a brief overview of our solution before we dive into the console. Awesome, thank you for that, Sean. So let's cover our solution, right? The first thing I wanna talk about today we're gonna introduce a multi-agent application. So this means we're gonna have a team of different specialized agents. The way I like to introduce this is think of a team that you have today, right? Usually you have a data scientist, you have a computer, uh, scientist, you might have a sales team, marketing team. And each team or person specialized in a specific skill, and they're able to come up with skilled outputs based on the knowledge they have so we take that same exact paradigm to the agentic world for example, in this case, uh, we have this agents, right? I'm gonna call it sub agent. Because it's one of the agents of many that are gonna work together to help us find the different security events and also automate the cloud operations of remediating those problems, so this first sub agent is gonna be able to tap into Dynamo DB and pull real business context from our, um, from our environments. Another big one here is gonna be an agent that's able to use a security lake, right? A key component here is if you have servers on premise, other cloud providers, maybe you're working with, uh, different SAS providers as well, we can centralize, normalize all our data in a central security data lake with one specific schema, and that way our agent can tap in and access these logs and provide that information to the overall system. Another big one is cloud operations. Does anyone know here what MCP is? Has anyone heard that term? Great. So that stands for model context protocol. So a lot of times in today's talk we're gonna build out custom functions that you could use as tools. Again, tools are essentially APIs or data sources that are large language models can use as additional context to provide answers. And with cloud operations we could do different things, right? We have a cloud control MCP server that lets you do different things like, uh, let's say add block rules to specific IP addresses that might be malicious. Let's say you want. Wanna isolate EC2 instances or you want to, uh, take forensic snapshots of your EBS volumes, right? We could automate our cloud operations. Obviously one key concern is, hey, do we really wanna hand the keys to our production environment to an agent? That's a big question on everyone's mind and that's where we look into like different things like guard rails, um, and also implementing human loop, right? We could actually take a summary of all the findings and ask the, the user, hey, is this the, is this the correct approach? so. Like, even the best agentic applications today have around a 90 low 90% success rate. So for key critical operations, you, you always wanna have a human in the loop within that process. And of course incident response playbooks, right? Some security events might have non-sensitive data, right? We're gonna approach those a different way than in an account that's gonna have the more highly critical environments. So here we could have these agents work together and we could have a supervisor agent up front and this supervisor agent is going to be able to use his team of subspecialists to actually coordinate different tasks and get different answers and synthesize them into a final response. This could work in two different ways. The first one could be approached like an event driven, uh, uh, uh, event driven architecture where we could take guard duty findings. So guard duty is our, um, our threats service, our threat detection service. So we could take guard duty findings and automatically feed that to our agenttic system. Let's say for example that, uh, someone opened a public SVD bucket. Right, this will trigger some sort of guard duty finding and send that directly to our agent, and our agent's able to look at that, understand, OK, what account is this coming from? Let me look at the logs, who did this, how, what, what steps do I need to do to remediate this, and it could do different things. Things like either raise alerts to the security teams uh that that bucket belongs to based on the business context or we could interact directly with the agent so if you wanna dive deeper into the events or do more advanced things uh we could interact with the agent. So let's see what that actually looks like here. So this is a front end application that Sean and I developed. We're using a streamlet to come up with this application. In the left hand side we have example queries our security engineers could actually look at and use. Uh, one example here is going to be the threat detection query. So here we're saying find any failed login attempts exceeding 10 tries in the last hour, analyze the source IPs. And affected accounts and then block the suspicious IPs and create snapshots. So this prompt here is gonna take advantage of multiple different agents to complete this task, right? If we hit submit, we could actually see that the agent's gonna go like, OK, let me analyze the authentication logs. So it's gonna call those specific tools. So those tools are able to look at the different tables we have within our security leg. It finds cloud trail events and security hub findings. Uh, it's able to then go ahead and query it to find what are the source IPs. Here it finds 3 primary source IPs across different regions, and then we can execute our cloud operations agent that could actually go ahead and block it. So here you go It's showing the different IPs that are being blocked, and here it shows automated response actions. So again, this is more of an automated approach. One thing you could do is they can give you the different summaries of what I found, and then it can ask you, do you want me to block these IP addresses? Do you want me to, uh, put the limited STP controls within our account? So again, those are things that you wanna think through. Some processes you wanna give full autonomy to the agent, but other ones that are more critical, you definitely wanna, uh, think about human loop. And the left hand side again we have different agents and we're gonna go through uh a few core agents in the console today and show you how we actually wrote this. So the way we developed this is. Let's go to the next slide. OK, the way we developed this is using strands agents. This is an open source SDK, and this is gonna help you build out agents with just a few lines of code, right? It's very intuitive, easy to use. When I first started looking at agents and started building them, uh, around a year ago, uh, I don't have my Python. I haven't done Python since college, so I was a little rusty. But with strands I was able to pick up very quickly, right? You're gonna see later today how it abstracts a lot of the complexity of building an agent. Another core thing is the native tools. Today we're gonna build the tools. We're gonna define those Python functions and make them tools, but there's also community tools that you could tap into that already exist, so you don't actually have to code them. Uh, also easy integrations with different MCP servers. AWS has a, a bunch of different MCP servers that you could take advantage of the advantage of today, um, and other providers as well have MCP servers that you can use. Uh, you could support any model providers, right? You're not stuck with just, uh, Amazon models. If you go to Bedrock, we have over, uh, 10 foundation models that you could use if you wanna use OpenAI and Geminiy, right? You're not locked to any sort of provider. Uh, and one thing I wanna talk about is for these sub agents, depending on the task, we could play around with the different, uh, models that's powering that specific agent. So at its core what Strands is doing is it's implementing this agentic loop. So the way it works is if you look at the weather application example, um, let's say again we wanna check the weather for tomorrow. Again, the foundation model has no knowledge of what the weather's like next week, right? It wasn't trained on that kind of data. So what happens here is the prompt is going to be sent to the agent, and the agent has access to the model and the tools right here in this case we're using a react orchestration. Um, where we're actually going to invoke the model with the user prompt, and the model is gonna have all the context around the tools it has access to. It's gonna be able to do the reasoning like, OK, I need to get the weather for tomorrow. Wait, I have these lists of different tools like get weather, so it's gonna pass that response back to the agent, letting it know, hey, execute the specific tool. In this case it could be a Python function that goes to the weather.com, uh, API and grab that weather for tomorrow. It's gonna return that result and then send it back to the model and this loop is gonna keep on happening until you get your final response, right? It could also prompt the user, right, if some tools require specific inputs and they're not there, we could ask the user, hey, you're asking about, uh, weather for tomorrow, but I need a state, right? I can't just give you weather, whether on on what location you're talking about. So those are some things we'll talk about as well. So just a quick visual on the differences between using some sort of uh um uh so whether using an STK like strands or Q Q AI as you could see you go from 55 lines from 350 lines of code to 55 for a weather app agent. So for the tool registration, the conversation management, API communication, all that's abstracted away, it's handled automatically. And again, when we talk about when you go through the code talk, you're gonna see, uh, there's not that many things you're gonna define again. Our goal is to simplify creating agents. Uh, same with the agent loop right beforehand I had to build out the iteration logic manually, but now I could literally just call in the agent class and have it do it for me. So a process that took 4 to 6 hours can now take 15 to 30 minutes. OK, so let's go ahead and jump into the 8 of us console. Before we And look at our first agent. Go ahead, John. So before we jump into the console, I wanna take a quick look at the first agent that we'll be building out, and this is our business context, uh, metadata agent. So as you can see here, the agent is going to ask for an account ID based off of your AWS account. That's a 12 digit number that you give it. You say, tell me about so and so account. Our metadata tool that we're gonna attach to the agent before we instantiate it we'll be able to look through a Dynamo DB table or a specific API that you might be using that holds all of that information and the information that we're talking about when it comes to business context is gonna be something like your business unit, the criticality, a point of contact in case, uh, something needs to be reported, the scope of compliance, and so forth, and we'll take a look at that in the console once we open up the Dynamo DB table. But what this allows us to do is take. What most people fail to realize is that the context and the business context related to a security incident is just as important as the technical details. You'll be able to sift through all of these things within seconds to be able to identify if this is something that your team should prioritize or if you can just leave it alone, it's low risk and maybe come back to it for a more priority adverse um security incident that might have already been happening. So with that, let's go ahead and jump into the console and take a look at what we've got to build up, yeah, and real quick before we, we jump in the console and get to the fun part that we're all excited about, I just wanted to see if there's any questions on what we discussed, make sure we, we, uh, we handle those questions before we get into the, the code. Does anyone have any questions about what we discussed so far on strands or agents? Great, OK, awesome, awesome. Let's get right to it. Cool. So here you can see we have the AWS console, um, just your standard landing page, and what we wanna do is go to Amazon Sage Maker AI. Now we use Amazon Amazon Sage Maker AI because there's notebooks available for us to use. If you all are familiar with Jupiter, um, Jupiter Notebook is an interactive IDE that's available for you all to build out your code, visualize the outputs. And then experiment in real time and this is super helpful as we build out this agent together in the room step by step. So just a quick little refresher for you all. Now the first thing that we want to do for this agent is install all the dependencies that's things like Boto 3, Pandas, and other libraries that you might need. So we'll go ahead and install those dependencies. Once the dependencies are installed, we're gonna go ahead and use another import code snippet right here to get the JSON. We're gonna import from strands the agent and tool functions. We're also going to import strands, models, bedrock models that we'll be using for the sake of this code talk. So we'll go ahead and run that. And then now we're getting to the fun part. So this is where we want to start off by making a connection to our data source and in this case we're using Dynamo DB. So we'll create the Boto 3 sessions so that we can interact or interact with our AWS resources programmatically. And we are using Dynamo DB as the data source as you can see, the resource is Dynamo DB. We're based out of US East 2. Now for this particular code snippet, we defined the region that we're working out of. There are ways to auto detect that, and you don't have to hard code US East 2. You can have it detect whatever region you're working out of if you need to. And we want to create a metadata table and define that for the sake of this agent. So we're gonna name that meta metadata_ table and we are linking that to a Dynamo DB table that we've already pre-created in our console, yes. Zoom in a little bit, yeah. Is that better? Awesome. So the data, the database that we currently have set up is called SOC Account metadata, and if we switch back over to the console, Omar and look up, go to Dynamo DB. We can see that all the things that I was mentioning earlier, the account ID, the scope of compliance, the criticality, and so forth are visible right here. So as you can see we've got the account ID, account name, compliance scope, and then if we scroll over to the right a little bit more, you can see that we have the criticality, the, the environment that we're working out of determining if it's production, development, and so forth to help you prioritize what you wanna, what you want to act on. So this is just to see the type of data that we'll be pulling when we return the business context using the agent. Now if we go back to the code, the first thing we wanna do now that we've connected to our Dynamo DB table is define the tool. So we start off with the at tool decorator and for those of you that may not be super familiar with tools, these are just specialized capabilities that we give our agent that allows it to act beyond its core reasoning abilities. So in this particular case, a tool to retrieve account metadata from an AWS account. So we'll define that as get AWS account metadata. We are giving it the information, instructions to retrieve metadata about an AWS account, given an account ID. And the argument that we're giving it is again this is an AWS account we need metadata for it. The ID will always be 12 digits. It's important to distinguish that here because like I mentioned, in the case of AWS, it's a 12 digit account number. If you put in a number smaller than that, it'll return a results saying, hey, we can't find this account. So however granular you want to make that, we'll find that in the argument. And then the agent, the tool is going to return to us a dictionary containing that metadata context like we just looked at the scope of compliance, the account name, what environment belongs to, who we should contact, and if we should prioritize this, or maybe move on to something else. And as you can see here, the response is gonna be based off of the account ID string that we saw on the Dynamo DB table and we'll get that list in a dictionary format once we've instantiated and run the agent. So let's go ahead and run that code, and now we're ready to initialize our agent. So An agent who can tell me what an agent consists of? What are the three components that make an agent? No worries. So an agent is a model, a system prompt. And a tool So you can, you maybe notice here that there's no model defined here in this agent, and we'll, we'll cover that in just a second, but we'll name the meta the agent metadata agent. We'll give it the system prompt. You are a helpful agent that looks up metadata about an AWS account based on the account ID. Like I mentioned, if the account number is not 12 digits or it can't find it, it'll let the user know so that they can try that again. And then the tool that we're giving the agent is that get AWS account metadata that we just created up above in the previous code snippet. So let's go ahead and initialize the agent. And this next piece right here, as I was mentioning, we don't have a model defined, and that's because I was perfectly fine using the default model that strands provides us, which is clotssonnet 4.0. So when we run this model config command, we can verify that. Obviously if you have a specific need for your use case for a different model, you're more than welcome to change that and put the model that you wanna use. Clots on it 4 work perfectly fine for this example right here. So now that we've got all the necessary resources compiled and ready to go, we can go ahead and send a message to our agent to get a response with that dictionary. Tell me about account 679-81713. Maybe we didn't run everything, yeah, we run the blocks. There we go. So if that ever happens just make sure you ran all the previous code snippets beforehand and as you can see here we've got the account name, it's production main, the ID that we gave the agent in the first place, you can see the business unit that it belongs to. You can also see that it is a production environment, critical criticality, and then if we want to let the security contact know security at company and the incident priority, for example, highest priority. This is the type of context that you might spend hours trying to gather from the information that you have stored from your data sources. The agent was able to pull that in just a matter of seconds, given an account ID. So with that, I'm gonna hand it back off to Omar to to show how we use that business context and how we use the data that we have for log analysis. Thank you, Sean. So one key thing that I would like to bring up when we're talking about agents is if you're new to building agents always start off with a more simple use case, right, something that's a little more straightforward so you could actually get the hang of defining different tools, understanding what works and what doesn't work. So in this case we start off with the Dynamo DB obviously business context agent is uh it is simple but it's also a key component of our overall architecture. So let's look at the next agent right here that we're going to build out on the console, and this one is going to be the security lake agent. This is definitely a a core one here. So the way this agent's gonna work is it's gonna use Amazon Athena. This is going to be a serverless interactive query service that actually lets us run SQL query directly on all the data sources we have in our security lake. Um, so the way this works is the agent's gonna be able to take a specific prompt. It's able to convert natural language like for example here if you look up at the screen we're saying show security group changes followed by unusual traffic patterns. Um, so can anyone tell me where do we store security group changes? What sort of logs will that be placed in? Can anyone tell me? Cloud trail. Exactly. So we have that's one of the log sources of cloud trail. How about unusual traffic patterns? DPC logs. That's right. So just based on this natural language query that we have here, it's gonna understand, OK, I need to query. I need to create the SQL code to actually query the security like, pull in any sort of security group changes that have unusual traffic traffic patterns. So this is an example of the power of the agent by itself. Down here is sample queries for multi-agents, right? So here if we're saying correlate unusual API. It correlate unusual API activity with network anomalies and then contain the threats. Does anyone know what two agents we need to use for this? So the first one is how do you correlate the API activity? What agent would do you need for that? So hint is we're looking at right now. What was that? Yes, so one agent is gonna be the security like agent and then for the containing the threats. If anyone remembers the name of that one, let's go back. Yes, the cloud ops agent, exactly. So these two are gonna work together. To actually handle that query and solve that problem that we have. OK. And yeah let's jump in into the actual console and see what this looks like. Yeah, you want me to drive? I got it. Appre appreciate it. So let's go to demo. Good. OK, so let's go into the Security lake notebook. So like Sean I'm gonna install the dependencies. We're going to then import um different things, right? The key two things is gonna be the agent class. This is what lets us actually define the agent with the different tools, foundation model and prompt. Um, we're also importing the the tool class so we could actually define the Python function as a tool. Sure, zoom in, OK. How's that? Awesome, OK. Uh, here I'm just doing some security lake configuration. I'm giving agent permission to access my Athena work group that's associated with my security lake. Again, my security lake has, uh, the logs across the board from VPC flow logs, security hub findings, and also cloud trail events. Here I'm initializing different AWS clients so I could actually interact with Athena or not me, the agent can actually interact with Athena, Bedrock to actually access different foundation models, and then also Glue. Glue helps us, uh, grab the scheme information so we could build that, uh, so we could actually build the queries. Uh, here we're defining a bedrock model. So I think one of the strongest points here is being able to define the model that you wanna use for your agent with just one line of code. So Claude Opus 4.5 probably came out a week or two ago, and I was able just by changing one line of code to replace a different foundation model. Uh, and again, one thing you wanna think of is depending on the agent and the, the complexity of its task, you might wanna adjust which model you wanna use, right? You wanna think about latency. You wanna think about cost. So if we think about the metadata agent, it's just gonna create a dynamite EB table. It's a pretty simple task. There's not much going on there. So in that case we could use a lightweight model that's faster and cheaper, like Haiku, for example. Something like this that needs to look at a natural language, look up schema, create an actual query, grab the data, and then create the correlations between activities, we wanna use something a little more advanced with more capability, uh, in that case we're gonna use cloud Opus 4.5. OK. So now this is, so now we're gonna actually define the tools, right? So now what tools does anyone know what sort of tools my agent would need access to for the security lake agent? There's no wrong answers. There's a, there's a hint right here. I'm gonna have to pick someone up. How do we query? The data, what's the best way to query that? What was that? Athena? Athena, yes, so that's one tool we need. We need to create a tool for our agents to actually use Amazon Athena, and that's what we're doing here. Uh, here is the context. So we feed, if you notice, a lot of these, every tool is gonna have this metadata description, and that's because we feed this to the model. The model knows I have a tool that can query the security lake security lake using Athena, and the only argument I need, I need a SQL query to do this. And this is the returns, right? I'm gonna return the results um as a string table or error message if I don't get anything back from the security leak. So here I'm just connecting to Amazon Athena. And I already defined Athena database up top, so we're connecting to the Athena, uh, and we're actually gonna grab the, uh, unique query execution ID that belongs to Athena and this is gonna go through and actually pull in the results for Amazon Athena. So that's one tool. One tool is to actually do the querying. Can anyone tell me what the other tool is gonna do? There's a hint on the screen. So the second tool is going to actually list the tables, right, because the agent is not gonna know what sort of, uh, data sources I have, right? So it could be VPC flow logs, it could be cloud trail security hub, or if you're using other third party providers, security services, you could have unique tables over there as well. And that's what makes security like so powerful is you have one normalized standardized data format so the agent doesn't have to go in and and really like uh change up the queries it has one unified way of approaching it. And then finally we have get table schema, and this retrieves the schema information from the Glue glue data catalog, and the agent needs to grab the schema, right? He has to look at the column information and how the data is partitioned, and based on that it could actually create queries that can be executed against Amazon Athena. So we define the 3 tools over here we're gonna run that. And now what we're gonna do is after we define the tool we're going to go ahead and call the agent class that we have and again for the agent like Shawn said there's 3 core components. We need the Bedrock model that we wanna use or any model, right? You're not tied to only bedrock if you wanna use OpenAI or these other foundation models. Feel free to do that and then we define the different tools that this agent has access to. And I think one very important component of of agents that a lot of people uh usually sleep on is the system prompt. The system prompt is essentially the persona, the personality of the agent. In this case we're letting you know that you're a cyber security analyst with access to an AWS security data lake, and I give it important database context and uh, if you notice here I also give it a a workflow. Um, the thing is with agents it could actually go through the agentic loop and figure out the work flow, but in this case there is one, there is one good way to actually generate the actual query, and that is list the tables, the schema, and then build queries based on the tape, the actual data and the schema. That's one work flow that I define. And one thing that you wanna do when you build agents, you'll notice sometimes, hey, like it's taking 2-3 iterations to do something. I could just tell it in the prompt the information and we can get rid of a lot of the latency uh and extra tokens that would, that, that it would usually cost. And in terms of outputs I tell him give me specific remediation steps. I want data driven insights and then key findings with the risk, uh, the risk assessments. And that's it we defined our our security like agents with these different tools and the first prompt I wanna test out is what kind of data do I have in my security lake. So we run that and as you can see the first thing it does is it's gonna list the security like tables. It realizes we have 3 different tables, so let's get the scheme in details so you can get the full picture. So it runs as you see 3 different executions, one tool call for every single table, and it grabs that information, right? It tells us, OK, you have the cloud trail events which has API activity, user role, and authentication information, source API calls. Same with security help findings where it tells you everything that is usually involved with these specific findings, right, vulnerabilities, remediation guidance, compliance status. In the same way with EPC flow logs. Here you go. So now that we built that security like agent, let's look at how we could actually build out how we could build out that supervisor agent so we could use multiple agents together. So here. The next step is we have this log analysis agent that could actually generate its own SQL queries based on the different events coming in or interaction with the agents and return results and we also have the metadata, right? So how do we actually set up these two to work together in a multi-agent architecture. So when I first started we talked about native tools that are created by the community. So two ones I wanna use for this specific agent is gonna be file rights. This is gonna give our agent the ability to actually write files to our computer. Another one is current time. So if you have specific queries like tell me about events that happened yesterday, this is a native tool again you don't have to write any sort of uh logic for it. It's already built in and we just need to import it from the strand tools class. So we import those two. So here uh this metadata agent that Shawn went through was defined in another notebook so I'm just gonna redefine it so this specific um notebook has awareness of the agent. So let's look at how we actually define multiple tools. So the way we do this in this example is we define the agents as tools. So the supervisor agent, if we think back to that agentic loop, uh, the STK operates, it now has access to different agents that you can use to complete task. So in this case all I'm doing is I'm defining that circuic agent. Um, as one of the tools, right, the logic was already defined above in terms of the different tools it has access to its persona, so here I just need to provide the context so our supervisor agent knows what has access to. Same with the get AWS account metadata. I'm just gonna define the metadata and then put some error handling, uh, in case the ID is incorrect. And then here again importing the agent class and we're defining the different tools in this case we have two native tools that are native to strands and we also have two different tools uh which are actual agents. Here again, again, the big key is the personality, the persona, which is a system prompt, uh, when you start building out agents, that's one thing I highly recommend is put out the different prompts and you're gonna see, uh, wildly different results. So in this case, uh, I define an investigation workflow, uh, get the metadata, right, use the security like agent to find the findings overview, get the cloud chill network IP analysis, and then finally I want you to generate a report that gets sent to our security team. I gave it a list of available tools. Um, And then also the output, right? I want account context criticality. Account summary, the risk assessment based on available data, and also recommended actions. And if there's any data, data gaps that require manual investigation, right, sometimes the agent doesn't have all the necessary context or reasoning to actually give you some sort of remediation plan so it could tell you what sort of data gaps you have within your environment. So around that And for when you use uh different native tools like file right Strand sees it as a sensitive operation to actually write to your computer. Usually you'll ask for consent whether or not you could write to the computer in the middle of the invocation. In this case, right, agents are autonomous. We don't want it to stop halfway through and let us know that, hey, could I invoke this specific tool? So I'm just gonna run this and just say, uh, bypass any sort of tool consent. You're allowed to use any tool you want without human loop permission. So now let's actually use the sock orchestrator agent we just built out. So we have this account ID it's been compromised. What are the affected resources I should be concerned about? So if we run this So me and Sean have actually been populating the Security Lake for the last 2-3 months with almost 60 events a minute. So we have a lot of logs in there, uh, and it's still performing very well, and it's gonna actually go ahead and start querying the different data sources, getting the schema. This probably takes around maybe a minute or two to complete, so I'm gonna show you a report that I generated last night. So if I come back to home. Look at all my reports. Right here these are different reports I generated. Let's look at the one from, let's do 4 hours ago. So this is the report that I generated, right? It's gonna give me the account ID and know is based on the business context that this is a production account, so it's a highly critical environment, right? It's a P1 priority and it's critical, and it gives you a summary. It says this account has confidential information of PCI information with active compromise, right? You need immediate containment required. So it gives you all the context, the security contact within that specific AWS account. Here's the executive summary. Me and Sean are definitely in trouble. We have over 10,000 security findings with Native's account, so definitely a lot of work to do there. Again, we've been blasting the security lake with a lot of different events, and here it breaks it down, right? You have 859 critical within SG buckets, IM users, EC2 instances, security groups that have been modified, and it tells you the risk, right? Data exposure, IM user, there's a credential that's been compromised and used. Um, it shows the attack analysis, right, the source IP, which user is compromised. Uh, and also the VPC, uh, traffic. Here we go. Um, different API operations that are happening, people are there, that's user is creating security groups. It's deleting security groups, and it's authorizing specific ports within existing security groups. And actually runs down through immediate actions that are required, right rotate all the credentials immediately, especially the admin IM. And it tells you the things that are critical that need immediate action, the short term actions. And then some more information on the compliance impact assessment. And then let's see if it generated the support. So it's still working through we can come back here. But that probably takes around another minute. So yeah, let's go ahead and go to the next slide. Not you. So now that we have our agent code, we wrote it out, we're confident it's working. We tested it within Jupiter notebooks and it's doing what wanted to achieve. The next big question is how do I run this within a production environment? That's one big key thing that we're seeing. And with that we have the agent core runtime. This is a managed service that came out a few months ago and it lets you run your agent AI agent code so you don't have to worry about the underlying infrastructure. So in this case again so much to Strands SDK, any sort of foundation model you want, whether it's from OpenAI, Gemini, or one of the handful of models in Bedrock, same with Framework, right? You don't have to use Strands SDK if you're comfortable with about land graph crew AI. You're already using them. You're open to use that as well. So what happens here is let's say we have this Python file that I have. What you could do is you could either containerize it in a Gawker file or upload the code as is to Agent Core, and they'll do the packaging and host it for you. So what's gonna happen here is agent co run time is gonna create a runtime endpoint, and this is the endpoint that was integrated in that front end application that I showed in the beginning of the talk. It's sending all those prompts to the endpoint, and the endpoint is actually able to scale up the amount of computer resources based on demand, right? So when there's a lot of users coming in and using it during actual uh incidents, it's able to scale up and also scale back down by default. There's also a lot of uh cool different integrations whether that's through identity whether you wanna set up observability um and also if you wanna integrate different MCP servers, right, we talked about the cloud control MCP server to automate cloud cloud operations uh if you haven't checked those MCP servers, I strongly recommend you do. It shows you, uh, the different range of things that you can give access to your agents. What's that? So any uh any questions? Yes. I do understand Yeah, so your question is you're, you're using a lot of these services and tools. What's the, the value of using a security agent? Basically like uh right. Pro Yeah, what's the advantage I'm going to get Yeah. Is it automated currently? This OK, yeah, so I, I guess the, the complexity of setting up the, the automation, um, in terms of are you automating remediation security findings. Uh, Yeah, Who does pretty much all these things, yeah. I am Yeah. I'm missing something. Yeah, no, it's definitely a good concern. If you want, let's connect after. I'm, I'm curious to explore more about the tools what you're doing and then how this would fit in the overall. There's some questions I wanna ask you. Yeah, appreciate the question. Uh, any other questions? Yes. specific Either they supported. Yeah, for registry specification is this related to registering tools or what is this related to? Are you currently using that? Are you currently doing a registry for your for your setup because trying to allow authentication to the agent. Let's say we Yeah, and I think we need to have a unified with this. Yeah. Yeah, Yeah, so I know with agent core it's not in this specific slide, but there's also a core component a feature called agent core identity, and that's one thing where we could unify the different access, uh, and authentication for our agents within that, that, that area. And if you want we could, I could dive deeper after the talk about how that works at a higher level, yeah. Yes. We'll go this way and then we'll come back to you. Uh, so in the, in the cloud. So is it more of is ensuring that the agent is up to date with your infrastructure environment like your. You're a terraform for like I guess like let's say I. some environment sitting in somewhere, you know. Yeah. And then there's an event Changes the security Like I want to reconcile those two states. Oh, I see what you're saying reflects reality. Yeah, how should I think about that in this environment. Yeah. Yeah and that's and that's one thing I recommend for for production environment changes at the current state of agents and how they operate that's one thing where we we would look at implementing the human loop where look at the different changes recommendations and then you implementing them manually yourself within the environment. There's still some necessity for human in the loop when it comes to that because you don't want automated remediation happening behind your back, you know what I mean? And then you have to go reconcile that, yeah, yeah, we're getting there hopefully maybe next year you could automate the whole, the whole stack, but as of now we take those recommendations of what changes need to, need to be made, and you could actually feed the terraform code to you so you could have the terraform code. You could review it and then feed it and upload it to your main, uh, code repository and then sync that. Yeah, can you have that agent, you know, run through the same CSD process. Um, you could then you wouldn't have any drift because the agent would be, you know, using terraform in the same way that you're using. Exactly, yeah, so you could have a, let's say part of the overall solution you could have an agent that specifically is working to developing and working with that specific environment, so that's, that's true, that's also another option. Yeah. I had a question over here flexibility. Thank you. Into security lake? Or with other sources besides Security Lake. Um, I'd have to double check on that. Let me, let me talk with you afterwards to see what other sources that you might be trying to use. I know that, um, most of those resources, people have been centralizing them within Security Lake, but is there a reason why you want to keep them separate? Security light. I'm sorry, are you? OK. Marks. Got you. I'd have to double check on that one for you, and I'll get back to you on it. Did you have something to say on that? Uh, no, I, I, I'll talk to him after. I couldn't hear. So just to summarize, at this time, Should we say that Yeah, I'd say at this time it's more of not only monitoring but also doing the correlation for you like, for example, jumping between the different data sources, running the SQL queries, and actually looking at the correlations between this specific security group being deleted, tied to specific IPs. So a lot of that grunt work of doing that manual process definitely could be automated. And even the um in terms of like actually automating the infrastructure as code. Um, and deploying things, that's something that the cloud MCP server actually has access to, right? It could write terraform for you and actually execute it and run it, uh, but again back to the point of at this time like you're saying, um, I wouldn't feel confident enough with agents to actually perform production changes without me actually looking at what, what it's doing. Awesome. Should we, uh, let's go in and wrap it up, wrap it up, yeah, yeah. So hopefully some of that was useful, you know, to the, the people in the room, um, and on the left hand side we have our strands getting started, you know, this is all of the, uh, documentation and everything you'll need to build an experiment with strands if you'd like to test something like this out in your own environment. Highly recommend getting hands on and doing so. And if you're also interested in deploying, you know, strands agents to Amazon bedrock agent core runtime, please scan the QR code on the right hand side. Um, this gives you the operationalized capabilities to test this in your own AWS environment. Everything that you need to get started is right there. And then one last thing I wanna leave you all with, if you haven't heard of SkillBuilder AW.AWS yet, whether you're new to the cloud or new to AWS or just wanna get some hands-on, more, uh, hands-on experiment experience to enable yourself, we've got thousands of free different learning resources, hands-on labs, immersive real life situations where you can get that experience to build these things out. If you're pursuing an AWS certification and need some extra tools to study for that, we also have that available on our site. And if you need to validate some practical skills for some micro credentials, whether that's for yourself or for your team, if you need that, that's also available on AWS Skill Builder. So with that, we'd like to thank you all so much for joining us today and if you have any questions, thank you. Great job Omar. If you have any questions, we'll be here for the next 10 minutes. Please feel free to come by. We'd love to chat with you. If we didn't get to one of your questions, mentioned we didn't wanna talk to you, please stick around. We'd love to chat. Hope you all enjoy the rest of Reinvent. Thank you. Thank you.