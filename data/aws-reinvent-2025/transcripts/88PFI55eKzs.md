---
video_id: 88PFI55eKzs
video_url: https://www.youtube.com/watch?v=88PFI55eKzs
is_generated: False
is_translatable: True
---

Good morning everybody. Um, firstly, if you haven't got your earphones on, it would be a good idea, then you can hear us properly and we won't have competition from everybody. So just give me a thumbs up if you can hear us. Perfect, thank you very much. Welcome to a deep dive session onto ECS manage instances and blue-green deployments for ECS for resilient services. My name is Mash Seidel Casing. I'm a developer advocate with the uh developer relations team, and on stage I have with me. Hi everyone, my name is Malcolm Fitwi. I'm a senior principal engineer with the Serverless and Containers Org. Super excited to be here. This is a 400 level session, which means we are assuming you know what a container is, you know what ECS is, and if not, I'm sorry, this is going to be very, very deep in the weeds, but even though if you haven't give a slight interview for our slight interlude on what we're going to be doing today on our agenda is we're going to have a short overview of what Amazon ECS is for those who are not familiar but do know what containers are, and we've been using other container orchestrators. Then I'm, I'm going to hand it over to Malcolm who will give us. Good run through the weeds of how we're going to actually what manage instances is and what we've built over the past. Period of time, for a long time, we were talking about how long ago was this something which has been in the making for a very long time, and afterwards I'll come back and we'll dive into how deployments or software deployments and how you can use those properly or gain the confidence inside of them, that you can use those to deploy your services in a resilient and available way. So ECS is Amazon ECS is a big service. We are what we call a tier one service, which means we are. A prerequisite for any new region which is built within Amazon or within AWS, so it has to be there because we are a tier one service, which means a lot of the services which are built in each of these regions rely on ECS to be there in order to run. We launch more than 3 billion tasks every single week across all of our regions, across the globe. That's a big scale of amount of service, of how much traffic and how big the scale of the service actually at which we operate. Customers that are starting with AWS, more than 65% of our customers. Start actually with using Amazon ECS because if you already know how to use Amazon, the APIs, what the concepts are, what the what the constraints are, you probably really know how to use ECS as well, because it's just another compute option within inside, similar to EC2, just running containers. You don't have to know any specific systems or any kind of architecture which is different from how we build the cloud. And just as an anecdote, also helps you buy stuff because in Prime Day as well we had more than 18.4 million tasks launched. Using running on AWS Fargate in order to support all the traffic which you ran through on all the different services within through Amazon Prime Day as well. Customers love it. And there's a couple of customers here, but it goes across the breadth and depth of all the different verticals and different industries today, Customs of all sizes. These are just a couple of them on on screen, but there's a lot more than there is, but they're all. Otherwise it would look very, very, very difficult to understand, but still. The reason is is because it's a very simple container service and allows them to focus on what we call the most valuable thing is providing value for their own customers. They don't have to worry about the infrastructure in the background. But it's not only customers, as we said. A lot of the services internally in Amazon are built on ECS. Here are a couple of examples, we have Amazon Sage Maker, Amazon Lex, Amazon Poly, AWS Batch, and also the Amazon.com recommendation for the store. All of these are built on top of ECS to provide you those services when you, um, when you consume them, either in the Amazon.com store or through the AWS console. And the reason is because it's a very service which is built for security, for reliability, availability, and of course scale. We released something in September, correct, which was Amazon ECS MH instances. I'm gonna hand it over to Malcolm to um start diving into a little bit of what we did. Awesome. Thank you. Thank you so much, Mish, and thank you all very much for taking the time to come and see us, especially, you know, given that you chose us over Swami, we think you made a good choice and we really appreciate that. Thank you. Um, so I have the distinct privilege of being an engineer working in the serverless container space and working with the ECS engineering team on an ongoing basis. And one of the things that we spend a lot of time thinking about is how we can deliver what we refer to as um as uh how we can how we can deliver features and capabilities that can remove what we refer to as undifferentiated work from, from your development teams. And really what that means is we want to be able to make sure that we're delivering capabilities which allow your teams to focus as much of your energy as you possib. We can on delivering the thing that makes your product, your service, your business special for your customers. And we want to take on as much of the, as much of the responsibility and burden of that work that is important but not critical to your business. So, you know, things like managing the underlying compute, making sure that your EC2 instance is healthy. Um, and, you know, the, the infrastructure aspects of that. And then also helping you with, with things, with, with the complexities of things like deployment, software development, software development, or software application life cycle management, you know, the deployment of those applications. And so, um, the team spends a lot of time thinking about how we, what features we can deliver for you in order to deliver that, um, uh, that, that behavior. And Uh, this year, we've, we've been, we've, we've delivered a number of features, but the two that, that I'm really excited about that, that, you know, we want to chat to you about is the one is in the infrastructure that's managed instances, and then the other is, um, is the, the set of features that we've delivered in our deployment space, which Maes is gonna talk to you about. And the reason that Maish, when we, Mash and I were talking about planning and what we were gonna speak to you about, what we, what we thought would be useful is, is to explore and give you some insight into both how the service works, but also why we designed it the way we do. The, the intent here really is that we wanna make sure that we're baking in that, um, that undifferentiated work that we're taking on. We wanna bake into that the best practices that we have learned over the periods of time, um. You know, at this point, ECS has been around for 11 years, which we're super excited about. But, you know, given, given that, as Mae was pointing out, um, the number of applications, the number of services within AWS within, within Amazon and AWS and also um in terms of the set of customers, we've had an opportunity to learn a great deal. And so we've, we've try and bake a lot of what we, what we see to be best practice into the services and we want to deliver that to you in a way that's easy for you to consume. So we're gonna dive into some of these things and we're gonna try and, we're gonna work, you know, work through how this works and then point out those areas that we've, um, that we're, and, and how, how we're delivering best practice in, in these particular feature sets. So, um, ECS ECS in terms of infrastructure, uh, manages infrastructure using a concept that we call, that we talk about as a compute engine. And when it was first launched in, um, uh, 11 years ago, ECS delivered the first compute engine, which is ECS and EC2. ECS and EC2 is about bringing your own EC2 instance. Effectively, we would go about orchestrating the application itself, but you would bring your EC2 instance and you would manage that. In 2017, we delivered our serverless offering. Um, with ECS Fargate, and that's become very popular with our customers. Um, Fargate is sort of on the other end of that spectrum. If you imagine that ECS and EC2 is, is about bringing your own capabilities and bringing your own EC2 instance, but then, um, at that, you know, in, in, in that respect, you have, you have to think about managing that particular instance. On the other hand of it, uh, on the other side of that continuum, um, you have ECS on Fargate, where Fargate is taking on the responsibility and burden of that, of, of the underlying compute for you. Um, and delivering it to you entirely, in an entirely serviceless way, which allows you to then focus exclusively on your application. Now you can think of these things as, um, as two sides of a continuum. And on the one side, um, you have a high degree of control, a high degree of flexibility, um, but as a, as a side effect of that, you also have a high degree of, of, um, burden that you're taking on. You have to think about how to scale, how to manage, how to patch, etc. And that's ECS and EC2. On the other side of that spectrum, Um, you have ECS and Fargate, where you, a lot of that complexity is taken away from you. But as a trade-off, you know, and this is always true with managed services, um, when you, when you federate a set of that responsibility to the managed service, you're also giving up a degree of control. Um, you know, in order for us to be able to make sure that we're building the services so that we can, we can get you a, an, uh, and Um, expected outcome based on the, the, the, any behaviors that we need to apply to that service. We need to be able to make assumptions about how that service works. And so, we need to be able to um constrain, at least to some degree, the amount of flexibility and, um, that you have. And so, so you can imagine these two things as a continuum. And the way we, we talk about that um in, in AWS is we talk about it as the shared responsibility model. The shared responsibility model is, is really uh the partnership between AWS and, and, and yourselves, um, where we, we agree which pieces of, of, of the stack going from, you know, the bare metal all the way up to your application, um, you're looking after and which pieces we're looking after. And so if you look at ECS and EC2, that shared responsibility model um really is, is about us looking after the underlying infrastructure and compute. We will make sure that you have EC2 instances that are healthy. We will make sure that you have those EC2 instances in the availability zones that are, that, that, that are needed. We'll provide you storage, networking, monitoring capabilities, and then the ECS control plane, where the control plane is that management layer that's actually doing the provisioning of your application. But above that, in terms of making sure that you're auto scaled, making sure that your AZ distribution and your spread, your placement is correct, um, launch templates, instance types, all of that functionality, that behavior, as well as your application, all becomes your responsibility. And that can be incredibly powerful because it gives you that flexibility, right? But it also means that when we, you know, going back to what we were talking about previously, our desire to be able to give you options to hand off a bunch of that undifferentiated work, this is taking on a bunch of that undifferentiated work, right? And so, so on the other hand, we have Fargate. Now, Fargate on that continuum of shared responsibility, actually offloads a lot of that functionality to, um, to AWS. And that's, I think why uh many customers really enjoy the Fargate experience. You can focus exclusively on your application and, and as, you know, we, we then take on the responsibility of making sure that you have the server. In fact, you don't even have to worry about the fact that there's a server because you're really just focusing on the appli on the task, on the ECS ECS cluster, ECS service, ECS task. Now what our customers tell us, you know, we get an opportunity to chat to you all, we love that, and we get that feedback is. They love the the, the, the degree of flexibility that you can get on ECS and EC2, but not super excited about the fact that you've got to take on this additional responsibility. On the other hand, they love the fact that there's the simplicity of management in Fargate, but would really like an opportunity to be able to get some of that richness, that experience, the, the, the, the diversity of, of, of, of um choice on ECS and EC2. And so, In order to respond to that, we've delivered ECS managed instances. We're really excited about this as an offering and we're, and we really are, um, you know, it was delivered in September, so it's fairly new. We're very keen to get you, get you guys like having a look at it, giving it a go, and then giving us that feedback. In terms of shared responsibility. Or what, what, what ECS and, uh, what ECS managed instances looks to do is deliver the EC2 flexibility in terms of choice, but the Fargate experience. And so, if you look at what we're doing, we're basically taking on a bunch of the responsibility that we would previously have offloaded to you, um, in terms of still providing that compute, still providing the availability zone, the, the, the network, the storage, etc. the ECS control plane that's managing your application provisioning. But then on top of that, we're also managing things like the compute auto scaling, the distribution and placement. We create launch templates for you, um, we're managing the patching of your of your EC2 instances, etc. but we're giving you the flexibility to be able to make choices about what compute you need. Um. So the, the, the, the, really, the, really the, the, the intention here is to try and find that, to, to deliver that sweet spot between the ECS and EC2 and, and, and the FARG experience, and we believe that we've got that right. We're waiting for your feedback to make sure that we did. The way that ECS manages and veins compute for through these compute engines into your cluster is through capacity providers. A capacity provider is a, is a configuration that you, that you describe, which talks about the kind of underlying compute that you need, and then you associate that capacity provider either with your cluster or with your ECS service. Or if you, if you're taking control of the scheduling and you're using a run task, you can also, you can also um provide that capacity provided through the Runtask API. Uh, today, or previous to the launch of ECS managed instances, ECS delivered two capacity providers. There was the ASG, um, the managed ASG capacity provider, which is effectively managing the auto scaling group that you bring, and then, and, and then vend vending and, and leveraging that, that auto skating group in order to, in order to vend compute into your ECS cluster, um, as a, in a form of EC2 instances. And then every cluster comes with a default capacity provider, which is the Fargate capacity provider, right? Um, which you can, which you can select. That is the default experience. And so if you're launching a task without, into a cluster without a capacity provider, typically, you're gonna, you're gonna land on Fargate. With managed instances we've we've doubled down on that, so we've introduced a new capacity provider called the ECS Managed Instances Capacity Provider. And again, that sits between the, the ECS managed, um, managed order scaling and the ECS Firegate capacity provider. It's an, it's a value, it's a, it's an additional capacity provider type. These all work together if that's your choice. And, you know, one of the things that we really wanna make sure that we, that we do well at AWS is we really want to give you choice. We, you know, we acknowledge that there is no one size fits all solution. All of you bring value to your customers and to your business because of the diversity of the solution that you have. And so we want to give you the choices that you Um, that you need in order to be able to make sure that we meet you where you are. And so that's really why we deliver this, this set of choices. Now, the thing with ECS managed instances is we really wanted to optimize for that Fargate experience. But keep in mind that there is this continuum, and we're somewhere in the middle, right? So there's always gonna be a, there's gonna be slightly more configuration than the pure Fargate experience where you're just, you're just configuring your application. So the way that you go about configuring an ECS managed instances capacity provider is you'll provide to us as a minimum, 22 IM rolls. The first role is the instance role. That role is actually used, um, and associated with the EC2 instances that we provision that gives you the permissions, that gives that instance the permissions it needs in order to talk back to the ECS control plane, to that management layer. Effectively allows the agent that you have, the ECS agent that you have running on that box to be able to talk back, get the configuration that it needs for your cluster, register with that cluster, and then be able to actually work with the scheduler to be able to provision tasks. The other role that you give us is um the, the infrastructure management role. And that's really you giving us permission to take responsibility for vending um for managing these, these EC2 resources in your account. Now, the thing with ECS managed instances is that it's built on top of EC2 managed instances. EC2 managed instances is a, is a mechanism of, of, a capability that EC2 delivered last year. Um, which allows, which allows a managed service like ECS to be able to manage EC2 instances in your account. So when ECS managed instances is provisionally these EC2 instances, you can see them. When you describe them, when you call describe instances, you can see them in your list. When you look at the EC2 console, you will see the man, you'll see the instances there. And, you know, that's, uh, we, we, we really like that transparency. We like to be able to make sure that you're able to see what it is that we're doing, you know, we're not, we're not hiding anything here. Um. You will see when you look at those instances that they are tagged as managed and they'll be managed by ECS and you can exclude those from, you know, from, from having to worry about them because the reality is that we're taking care of all of that. When you provision um your capacity provider in, into your cluster, one of the things that we've indexed on, and we do this with Fargate as well, is that your capacity provider is going to be using spread placement by default. We believe spread placement is the right approach. Um, it allows you to make sure that you're maximizing your availability. And so the way that you do this, in the same way that you would with your ECS service when you're, when you're provisioning your ECS service for Fargate and specifying the subnets that you want when each subnet specifies the underlying availability zone that the task will be launched in. With the ECS managedance capacity provider, you're specifying a set of subnets that we're going to use in order to provision EC2 instances into that, um, into those availability zones. That makes, and then we will use that to do spread placement. Effectively, once you've created your capacity provider, given us those two roles and associated with your service or your cluster, at that point, you're just talking services. And now it's just like Farget. You create your ECS service, your task definition, your provision, and off the back of what you need, we will go off and we'll go down to EC2 managed fleets. We'll get the capacity you need, we'll provision it into the cluster and you're good to go. We'll start landing tasks on it. The way that we make decisions about what EC2 instance to use. Um, it's really, uh, we, what we've done is we've had a look at, at, um, what the vast majority of customers end up using when they're using EC2 managed instances and also, um, with consideration as to what it is that we're provisioning under the hood for Fargate. And we've come up with a list of, of, of, uh, defaults. And so you, unless you actually have a need for specialist compute, unless you actually need to, to be a little bit more um prescriptive. You can just leave us to, um, to take care of what instances um are available. And we're typically gonna choose from any one of the CM or R families, so the general purpose compute family, families. We're gonna make sure that we're making that choice based on, on, on your workload, and we'll get a little bit more into exactly how we go about making that choice. However, if you do need more control, and this goes back to this, this idea that, you know, we, we're, we're sitting on that continuum, we wanna make sure that we're giving you the flexibility that you need. If you need more control, if you need to be able to specify that you need a specific compute type, well, ECS managed instances allows you to capture that as a set of uh using attribute-based selection. Uh, if, if you're familiar with, uh, with EC2's auto scaling groups and fleet configurations, then you'll know that attribute-based selection is a mechanism that you can use to describe the kind of EC2 instance that you, that you might want without actually specifically stating that EC2 instance. Right? So you can, as part of attribute-based selection, specify that you want a specific C648 X large, if that's what you want. Um, but you can more generally describe the kinds of compute that you want. And the reason that that's really powerful to try and be as general as possible is because it gives us at ECS the opportunity to make sure that we find the capacity you need through with, with, with a broader, more diverse selection. So the, the, the key here is really that you can be as prescriptive as you want with the compute that you want, or you can be as general as you want, or you can just leave it up to us, right? Um, that's really goes to that shared responsibility continuum. The more prescriptive you become, the harder it can be for us to make sure that we're getting you the lowest cost, most, uh, most available instance. And so, in some workloads, that's absolutely required, you know, it may be that you need a GPU and you need that GPU and so you specifically have to target that particular instance type. You can do that with ECS managed instances. At the same time, if you really just have a general requirement for something that's network optimized, well, you want to try and probably be as broad as possible because the benefit you get is that that offloads the decision making to to us. And then what we'll do is we can actually go to EC2 and make sure that we're getting you that compute. So, so really the rule of thumb here is, as you're using this, try and be as, as general as you can be, cause that maximizes the availability posture of the service. It gives us a much better chance of getting you the compute that's available at any point in time. The way that we go about making decisions about how to size the instance that we need, how to get that compute for you is we use the task definition configuration. So, unlike with, with ASGs where, um, where, and, and, um managed, and managed order scaling groups where you're actually specifying the EC2 instances and then we're just placing the tasks. Here, we're actually using the task definition to be able to decide what EC2 instances to get for you. And the way that that works is, um, if you think back to how task definitions are configured, um, Perhaps some of you have memorized this quite as detailed as I have, but basically, the way, the way this works is that for, at a container level, or keeping in mind that you can have multiple containers in your task, you can specify as a minimum, the memory reservation for that container. Now, the memory reservation for the container is the amount, the minimum amount of memory that is reserved for that container. It doesn't prescribe an upper bound, it's just saying I need at least X. You can then add to that the memory reservation, the, the memory that you need for the container, which is the upper bound. And now you're saying this container needs to be bounded to this amount of memory. Also at the container level, you can specify the CPU you need. Now, the thing with the CPU that you need is, it's not a hard cap of this amount of CPU. For the, for the container, the way that this is configured is it becomes a ratio of the CPU available to all of the containers in that task. So it's ratioed across all of those containers. And so that's useful if you, if you have containers that are kind of bursty and they need, they need to kind of consume, you know, more or less, but it also means that you can, you can make sure that one of your containers, perhaps your primary container, is getting 80% of the compute and maybe your, maybe your, your logo or whatever it is, is, is getting, is getting less of that compute. At the task level, you can specify memory and CPU and that actually then, this is how Fargate works. That is actually specifying exactly how much memory and CPU will be allocated to that task. It's effectively a hard bound. And so, the way that we go about doing this is, um, when you, when you configure your ECS um your, your, um, your task, uh, for ECS managed instances, we walk through the set of configuration that you have and we decide based on that, what it is that we can go and get for you. As a minimum, you have to specify memory reservation. Because we will then go and, and we'll go and find an EC2 instance that's gonna fit at, at, at least that. Um, but you can specify any one of these things and it's valuable to actually try and move higher up the stack. The less you can be prescriptive about what you're getting, the more elastic you end up getting, um, getting in terms, in terms of the underlying compute. The difference between Fargate, where you're getting that compute and, um, and managed instances is that with managed instances, These workloads are multi-tenant. They're gonna rely, they're gonna, there'll be multi, multiple of these, of, of your tasks running on the same EC2 instance. And that's valuable because it allows you, depending on how you specify this, to allow those workloads to burst from one to the other. I was chatting to a gentleman earlier, and he was telling me about Java workloads and, and the fact that they run a lot of Java. A lot of Java, um, especially when it's doing just in, uh, just in time compilation, can burst CPU quite high when it first starts out. Um, and what we found with customers are told is with Fargate, the end result is you actually have to specify the CPU need at P100, effectively the maximum CPU you need. With using ECS managed instances, you can allow it, we can leverage the elasticity of the underlying compute so that you can actually specify less memory. You can specify your aggregate memory that you need, and it'll, it'll allow you to burst to be able to, to handle that jit compilation. Let's talk a little bit about um how we go about um making the decisions as to what instance to launch. Now one of the things that we, we really, really index heavily on is, um, in order to make sure that we're delivering you the most resilient service, we want to make sure that we can provision your workloads as fast as we possibly can. That's our goal. In effect, effectively, as you scale up, as your service scales up, we want to be able to make sure that we're launching those tasks quickly because we wanna get the workloads there to make sure that they're meeting your customer demand. So, the way we go about doing this is at, at a point in time, now keep in mind that a, a big cluster, depending on the size of your clusters, large clusters are very active. There's workload going on all the time. There's multiple services, there's scaling up and scaling down. So, at any point in time, the ECS scheduler will be having a look to see what work needs to be done. It takes a snapshot of all of the tasks that are currently in a pending state. Basically, tasks that need to be provisioned onto workloads. And it's going to, it's gonna index on making sure that we first use the compute that's already there. And the reason for that again goes to, we wanna make sure that we're launching at your instance, your tasks as quickly as we can. So, if we can fit the tasks that you have on the instances that you have while meeting your placement constraints, so making sure that we're spread AZ, making sure that we're, we're, if, you know, whatever placement constraints you've applied to distinct instances, etc. we will make sure that we're, we're first landing on the compute that's there. What we do next is we then look at all of those tasks that we were unable to place. We look at the placement constraints um that have been applied to those tasks, including their need for availability zone spread. And we will, um, we will then go to EC uh EC2 and based on your capacity, your, your managed instances capacity provider uh specifications, we will get all of the EC2 instances that meet that requirement. We then do, um, what we call 1st, 1st fit descending, uh, first fit decreasing rather. Uh, where what we'll do is walk through the set of, the set of, uh, of tasks that need to be placed and we'll look for the largest EC2 instance that meets those requirements in order to place it. So our goal is spread placement first. In fact, first phase is place on what we have. Second phase is spread placement, and then 3rd, 3rd phase is going to be bin pack. And the reason for that is because we want to make sure that we're optimizing cost for you. We want to make sure that we're finding the largest instance at the best possible price and putting as much onto it as we can. Let's move on a little bit, talk a little bit about how, how that can be optimized. So the, so the, the benefit that we get um in using these larger instances is um And in using ECS managed instances, is you're actually, you're reusing that underlying compute. And that's incredibly valuable because it goes to what we were saying earlier about the fact that we wanna launch quickly. We wanna get you running quickly. Um, there's 22 upsides there. The one is, you've got this elasticity that we talked about because you've got the larger instance, so you can, you can burst into it if you need. Um, Because you're landing all of your workloads on this instance, the first workload will pull the image, and the subsequent workloads are going to use that image cache. And so in fact, if you had multiple of the same tasks landing on the same, on, on the same EC2 instance, those subsequent tasks are actually reusing the image that's already there. It saves you a bunch of time. You're not pulling that image cache um every time. This, this significantly improves, improves throughput, and that's really why we index hard on the, you know, it's a combination of cost and throughput that we, that really leads us to index quite heavily on, on, on making sure that we're choosing a larger instance type. One of the things that we will be doing in your, in your cluster on, on, on sort of an, an ongoing basis is, is, um, we're gonna be doing host replacement. Uh, Remember, we talked about the fact that there's, there's this continuum, this shared responsibility model, and, you know, part of, part of this offering is that you've offloaded to us this, um, the responsibility of making sure that your host is patched and that your host is, um, is compliant. And so, we're, we're gonna be. We're gonna be looking at opportunities to be able to make sure that, that we're, we're meeting that compliance requirement. At the same time, there are a bunch of other things that could be going on that actually drive, um, drive the need for, for replacing the host. The first is that as an operator, um, you, you may decide that you don't particularly want this particular host. For some reason, you decide that you want to get rid of that one, and so you can actually drain the host, you can select it, deregister or force deregister, and then we would run it through what we call a drain cycle or a host replacement cycle. Another is maintenance events, and so ECS is continually monitoring to see if there are EC2 maintenance events applied to that particular EC2 instance so that we need to replace it, or maintenance as a result of patching, you know, where the operating system needs to be patched, and so we'll be applying those. There's auto scaling, so your service is continually scaling up and scaling down, and as you scale up and scale down, We need to make sure that we're taking corrective action, and then there's cost optimization. We're continually looking for opportunities to make sure that we're giving you the best possible value. We don't want idle instances sitting around. So the way that this works is that effectively, um, at the point that, um, at the point that we get a notification that an, an, an, uh, a, a instance needs to be replaced, for whatever reason, uh, we enter the draining life cycle and the first thing that the scheduler is gonna do is it's going to try and replace, it's going to replace all of the tasks that are managed, um, running on that, uh, running on that, on, on that instance. We always make sure, again, going to resilience, we always start a task before we stop a task. Um, it's sort of a golden rule that we have. We have this conversation, the scheduling team all the time. It is the golden rule. We start things before we stop things to make sure that we're always meeting your, um, your minimum healthy requirement for your service. So we're always meeting your resiliency goals. So the way that this will work is the scheduler is gonna, is gonna first launch a bunch of tasks. Now, let's assume in this particular case, there isn't enough compute in your cluster. Those tasks are gonna go into pending, and we're gonna go back through that workflow that we were talking about earlier where we're going to have, we're gonna look for the capacity, the compute that we need, find the compute that meets the placement requirements for these particular tasks and provision that into the, um, into the, the, the cluster. We're then going to provision those tasks onto this EC2 instance. Now, you'll notice here that we've provisioned the, um, the orange and the green tasks, the service tasks, the managed tasks. What we have not done is provisioned the unmanaged task. So if you, if you launch a task um onto ECS using run task or start task, Then the way that we think about that is effectively, that's a mechanism to tell us that your service is taking responsibility for scheduling. So rather than using our scheduler, you want to own the scheduling um through some, some external, uh, external process. And so, we don't take, we don't take, um, we don't manage those tasks. What we will do is we will make sure that those tasks are the last to be deprovisioned, and we will always honor. Um, the task shutdown grace period, and any of the mutual, the, the, the mutex, um, functionality, so the task, the, the, the task scaling protection, the task, um, that, that, that you've got enabled. So if a task, if a run, if, if an unmanaged task is currently in its grace, uh shutdown grace period, or it's currently in a, uh, a, um, a scale and protection, uh, phase, we won't take corrective action until it's exit, exited that phase. Unmanaged tasks are also the last things to be deprovisioned off of um of the instance. So the first thing we will be doing is we go and find new tasks for you. The scheduler will replace the managed tasks on the new capacity, then it will tear down the the the managed tasks, and then the very last thing, honoring these, these constraints, it will do is it will then tear down the um the, the actual uh unmanaged tasks. Once we've done that, we're then gonna actually tear down the EC2 instance and that gets us back to, get, uh, gets us back to a point where, where, you know, your, your, your processes, your, your, your services, is, is up and running, and we've, we've uh removed any of that, uh, the, the idle compute that you no longer need. Now one of the things that you might notice um in, in your services, um, remember, we talked a little bit about the fact that we're optimizing for throughput. And so we're, so, you know, we, we really wanna make sure that we're, we're launching quickly. But as a side effect of that, when we launch, it may be that in fact, the distribution of compute that you have in your cluster doesn't align with the distribution of, of pure balance that you'd need for your ECS service. And so your service can end up in a, in, for a short period of time in an unbalanced state. ECS is continually looking for opportunities to rebalance. And so, um, just before reinvent last year, we launched a feature for ECS services, um, called continual rebalance, where, where you can, you can actually select, uh, for that service that, that you want it to be rebalanced. This is now a default behavior. Um, and again, it goes to what we were saying earlier. We try very hard to bake in. We work to bake in default best practice as default into the service. And so, this is a best practice and for any new service that you created will automatically rebalance. And so what's gonna happen There is that the scheduler is looking for these opportunities. It's going to identify the fact that this particular service is unbalanced, and then it will take corrective action to make sure that it replaces, it rebalances that service by launching a new task first into, into the appropriate availability zone and tearing down the previous task. And again, that will kick off that workflow if you need new compute. Well, in that particular case, we're going to go off and get an EC2 instance and we're going to make sure we put it right where it needs to be in that availability zone, so we can provision onto that. Um, on, onto that instance, um, for, for you and make sure that you, you're continually balanced. And as a result, you will end up in a world where you, you know, you're, you're, you're continually, you're, you're continually meeting that availability requirement. Your tasks are always appropriately spread across the availability zones that you've configured. One of the things that we, that we watch for on an ongoing basis is, is um a notion that we call idle hosts. And idle hosts, um, really is just to make sure that we don't end up in a position where we've deprovisioned a bunch of tasks and we've got a bunch of EC2 capacity sitting around, um, cause that's just wasting, it's wasteful, it's wasting your money. Um, and so the way we do that is the, the ECS scheduler is continually monitoring for stop events. Effectively, anytime a task is terminated, we know that there's an opportunity potentially for us to go and, to, to go and do some optimization, to apply some optimization. Um, and so as soon as we see that event, we'll go and do a sweep of, of the entire cluster. We'll identify opportunities where, where we think we can actually do a better job, and we will move tasks to make sure that we're packed appropriately. Um, again, honoring, honoring, um, spread and, and, uh, and, uh, placement constraints, and then deprovision, deprovision those tasks. We will deprovision those instances. Again, honoring stop before stop. The other thing that we will be doing is, um, we look for opportunities to downscale. So, if you're running on a larger instance, say, you know, uh, many workloads, um, may be diurnal, so you scale up during the day, you need larger instances, you need more capacity. But, um, as you, as, as you get to the end of the day, your service scales in, your customers are, you know, customer demand has dropped. And so, Um, during that, during that case, uh, we're gonna end up in a position where, where the, those EC2 instances are no longer needed. Um, it may be, you know, there's two ways to do that. We may be deprovisioning. The other is that we may actually just scale down. And so, um, we would choose a new instance type for you and actually move you onto that newer instance type, that smaller instance type, in order to save you money. So Let's talk a little bit about how we do, how we do patching. 11 of the things that, um, that, you know, we talked about is, is we wanna take on that responsibility for patching. Our customers have told us, you know, I wanna make sure that I'm compliant, but actually, keeping your EC2 instance patched, your, your Amazon machine image patched is, is hard work. And there's a lot that goes into that. And it, it generally doesn't tend to be differentiated work for many businesses, you know, it's not the thing that's gonna really, really make you shine. It's just busy work. It's incredibly important work. And so we want to take on that responsibility. And so one of the key features that ECS managed instances delivers is, is this patching workflow. Now, the way that this works is, um, we wanna make sure that within a 30-day period, we have all, we, we have patched all of your capacity. On day zero, at the point that you provision your task, we're gonna give you a brand new EC2 instance. Um, we always favor provisioning new instances of, we, we never patch in place. We always provision a new instance. And the reason for that is because under the hood, EC2 has its, it's fleet of available capacity against which it is continually running health checks. And so getting a new EC2 instance means that, that health checks have recently been applied to that and so it is, it's going to be a good, healthy instance. We're not gonna vend one that is unhealthy. So we're gonna give you, we're gonna give you a healthy instance and then we're gonna give you the latest Amazon machine image that we're using. ECS managed instances is built on bottle Rocket. Again, going to that shared responsibility model, the thing with EC2 managed, uh, with ECS managed instances is, um, you don't get to bring your own Amazon, your, your own AMI. Um, we will, we, we deliver the AMI because again, we need to make sure that we understand how that AMI works because we need to be able to patch it and deploy it and run your workloads on it in a way, in a way that ensures that you're gonna get the outcome that you need. And so if you do need to to, to, um, to manage your own Amazon machine image, well, ECS and EC2 is probably a good fit for you. But in many cases, customers don't need that, that um responsibility. The the Amazon machine image, the, the, the AI that we're using is Bottle Rocket. Bottle Rocket is uh AWS's container um Container optimized operating system. We delivered. In fact, that team works very closely with our ECS container and our EKS teams to make sure that we're continually delivering the most optimal configuration that we can. So under the hood, we're going to be giving you the latest, the latest up to-date bottle rocket image. We're gonna deliver that on day 0, and then 14 days after that. We're going to start to look for opportunities to be able to patch that. And really that's because we want to fit into this 30 day patch cycle. We want to make sure that we've always replaced all of your Amies within, within a period of 30 days because that really helps with, making sure that we're meeting your compliance requirements for those of you in regulated industries that have that, that have that pressing pressing requirement. On day 14, we start looking for these opportunities to be able to, um, uh, to, to be able to optimize, to be able to uh replace these, replace these um instances. Now, one of the things that we do is, um, ECSR managed instances honors the EC2 maintenance window. So for those of you that have used EC2, um, extensively, you've probably encountered EC2 maintenance windows. It's a mechanism that you can use to be able to configure and tell us when is a good time for us to be able to take corrective action for your underlying compute. And so as a minimum, you need to give us two windows within a one-week period, um, where we can go and take that corrective action. ECS managed instances is leveraging that configuration. So, you can configure through EC2 managed instances when you want us to take this corrective action, and we're only going to take the corrective action during that period. So obviously, you can optimize for weekends or downtimes or whatever it is that, that, that works best for you, sort of your, your, your low traffic points in the day. At that 14-day period, we're gonna give you, like I said, brand new instance, and we're gonna give you a brand new Amy. It is possible that over that period, um, that we haven't got to everything. Um, although, under, under most circumstances, we will have completed that patch cycle. It's also important to keep in mind that this patch cycle isn't everything in your cluster on launch day 0, day 14, everything in your cluster, right? It's an ongoing process. So we're, we're continually evolving, we're continually making sure that your cluster is OK. So you'll see, you'll see a fair degree of mutation in your cluster as we're taking this corrective action. It's not high mutation, but there's an ongoing background mutation as we manage that entropy for you, as we take on the responsibility of making sure that we're keeping you compliant. At day 21, we start to get a little bit more aggressive cause we've made a commitment to you that we will make sure that um we, that we keep you compliant within this 30-day window. So at day 21, we're actually gonna start taking corrective action where we're going to start doing effectively eviction, um, where we'll find the instance that needs to be patched, and then we'll move workloads off it and patch it. Um. And at that point, at, at, so typically, we're gonna be honoring maintenance windows between day 14 and day 21, so it's very unlikely that, that, um, that we wouldn't, but at the, at day 21, we need to start being a little bit more aggressive because we have to replace that, that, that, that army. And then, having completed that cycle, we then get to a point where You now have a clean, healthy, uh, healthy cluster. Um, all of your instances are replaced. They're all replaced with brand new instances, um, and you're currently running the latest and greatest of, of, of the bottle rocket, I mean, uh, bottle rocket, Army. So one of the things that you might be asking yourself is, well, that's great. There's like this continuum and I've got all these decisions point like, you know, now I've got a bunch of things to decide. When would I use one versus the other? Our recommendation is that more often than not, most customers are best served by Farget. Typically, customers, you know, cause typically your workload, especially if it's a stateless workload, especially if it's CPU and memory, um, memory bound, um, you can get everything that you need from Fargate. And so that's where we would recommend that you start. It's, it's, it's, it's simple, it works, and customers love it. If you find that you have a need, either to have a larger instance, a larger task size, so Fregate is, um, has, um has upper bounds in terms of the amount of VCPU and memory that it, that, um that it will configure, and you need something bigger, um, ECS managed instances is for you. Equally, if you need control over the underlying compute, if you really wanna make sure that you, you are choosing that particular instance type or that family of instance types, or, you know, for example, if you need accelerated instance types, you need GPUs, etc. ECS managed instances is a really good fit. It's the sweet spot between that Fargate experience um and the ability to be able to, to, um, to own, to, to own and manage the configuration that you need in order to get that compute that you need. For those customers that need to manage the actual operating system, effectively, you want to be able to bring your own Amy, you want to be able to tune the kernel or whatever it is. ETS on managed instances is for you. That is probably the the the small minority. There are very few cases where that is needed, um, but if it is needed, you have that option. So, the way, the way we're thinking about this is we think a lot of customers today that, that, that needed that additional functionality that, um, that Fargate wasn't quite able to deliver in terms of the instance type selection and are currently running on EC2 um on uh ECS and EC2. Managed instances is probably a great fit and, and that migration is fairly seamless. So, so, you know, we'd encourage you to have a look at that. At the same time, those customers that are, that, that are running on Fargate and get to a point where you feel like you need more compute than Fargate is able to able to deliver, or you need something that's optimized for network or optimized for storage throughput, um, ECS managed instances is, is, is a great fit for you as well. So it's sort of the sweet spot between those two. Um, with that, I'm gonna hand over to my friend and colleague, Maeish. Thank you all. Thank you very much, Malcolm. Everybody hear me OK? Perfect. OK. So, we've been talking about infrastructure up until now, and um I want to move over to the software part, which essentially is again on that continuum of the shared responsibility. This actually is your responsibility. We still, as part of the shared responsibility model, still provide you with the infrastructure underneath that allows you to continue to provision your software as part of your uh application which you're providing. And in November, we kind of revamped the way we do in Amazon ECS deployments. The reason of course is because, of course, feedback from our customers. We were, they had some challenges with the way we were doing it was was dependent on an external service called CodeDeploy, and we wanted to give you a more streamlined experience. And just in the beginning of last month, we provided that experience which is called ECS deployment with 3 different options of you, for you to deploy your services and upgrade your applications running on ECS. Blue-green deployments, linear deployments, and Canary deployments. So we're gonna go through each one of these in some detail to explain exactly what it is. But first, of course, an overview for those who are not people who usually do this for their day to day life. A blue-green deployment is you have a blue service running in production across multiple availability zones. You would like to upgrade the service. In that case, you provision a new version of that service. Across the availability zones, meeting, meeting your requirements, and when you're ready, you take that service and flip it between a load balancer to the new one and direct all your traffic to the new green service, and when you're ready, remove the previous one in blue. Simple process, but let's go ahead and tell him in a little bit more detail into how this actually works. There are 6 different stages of a deployment in Amazon ECS, the preparation phase, sorry, preparation stage, deployment, testing, traffic shift, and monitoring and completion. In the preparation stage, we go back to the fact you have one version, which is blue, running 100% of your traffic, and at 100% capacity in your um in your ECS cluster. When you start a deployment, we will start to pre-scale up. The pre-scale up in this case will not provision any resources, and again, these are very distinct phases within the deployment process as part, and you can get status on each and every single one of these different phases. We will provision the infrastructure underneath, in this case is preparing the routing rules in the load balancer and the target groups, but we will not yet start to provision resources. This is the preparation phase. Deployment phase has two different things. In this case, when we start the deployment phase, we will start the scale up of the green version. Where we will provision 100% of the capacity, and again it's always the 100% of capacity which we are provisioning in the background. For the very simple reason, of course, we want to provide you the capability of always having enough capacity to move over when you're moving to a new version. In this case, of course, the post scale up, we will verify that the green service has been started and that 100% of the traffic is still being routed to the blue version and no traffic is being moved over to green. During the testing phase. We will validate that the green environment is able to actually accept traffic by injecting some test traffic into the load balancer, which will start verifying that actually it is working and able to respond with all the health checks are passing, and your application is behaving and performing the way you expected it to do that. You'll see at the bottom there is a smaller lambda icon, because at, what we, what we have tried to provide for you is the ability at every single stage of the deployment or phase of the deployment. The possibility to run your own tests, and these are done in what we call deployment life cycle hooks. And the deployment life cycle hooks is essentially a lambda function which you can configure or define as part of the deployment of what you want that lambda function to actually test or verify is working. So for example, essentially it is actually just code, like any other lambda code, where you will provide. The criteria for what is it passed, if it's currently in progress, as if it failed, and what to do, and once it either passes that phase or fails, it will automatically roll back or continue to the next phase. This can be, for example, to verify that the SHA that you are looking for on your container image matches the one that you're actually supposed to be deploying and there's no indiscrepancy or that. The target groups have already been valid validated and actually exist on the load balancer so that I know that everything is working. These are the tests that you can define exactly what you would like them to be. Traffic shift. So we verified that everything is working and now we start to move the traffic in a blue-green, of course we said was 100% capacity was running to blue. And we flip it at one time or one at one shot for moving all the capacity over to green. So in this case, now ero is running to the blue version, and all the 100% traffic is now running again to the green version, and again we have these deployment life cycle hooks where we can validate that everything is actually working. We then go into a bake time or a monitoring phase. In other words, we want to see actually that everything has, is performing the way it was. This can be, of course, some kind of a performance, or maybe a degradation test. Is my application performing exactly the way it should be, like it was before? If there's any degradation in performance, I want to automatically roll back directly to my previous version, because it's still sitting there, but there's no traffic running to it. So the switch backward and forward is almost instantaneous and allows you to. Gain the confidence and be very, very confident that you can actually not have any problems, and if there are, roll back directly, very quickly without um hurting or hitting any of, any of your any of your customers, hitting a performance issue. The last one, of course, is the cleanup phase. The clean up phase is after we've verified that everything is done, and over here specifically at cleanup, there is no deployment life cycle hook because this is the last stage of the deployment. Once we're ready, we know that everything is working, we can finally de-provision all the resources which we had on our previous version, and then green becomes blue and we continue the process to the next one, to the next version going forward, forward. Linear deployments. Similar blue version which we want to start running 100% of my traffic. I would like to start slowly, gradually moving traffic to my new version. I provision a new version and slowly move my traffic. 25% of the time you can do the definition of however much of the traffic you would like to do. Over time moving more and more and more traffic over to my new version, until I get to the fact that everything has moved, and then I can deprovision my previous version and work with. This kind of process on a regular basis. So let's dive in again. The first part is exactly the same as we saw before, the preparation, deployment and testing phase, but it is slightly different on the traffic shift. So let's zoom in over there for a second and have a look at what it looks like. In the traffic shift, we start in the beginning with 100% capacity. Running to blue, 0, 0% of the traffic running to green. We start moving our traffic. As part of the deployment, we have a definition of a bake time. Again, this is the amount of time where you want the application to sit. On both versions to see that actually everything is working, and again with the deployment life cycle hooks you can validate after every single one of these steps of the traffic shift, you can validate that everything is working. Again, if anything fails, we'll fail back all together, back to the original version, so you will not have any degradation of service. And it continues per each amount of traffic or the, the weighting of the load balancer moving more and more and more traffic to the new version until we finally complete and get to moving all of our traffic over to our new version that we've run. This is the linear, so we, we go step by step by step by step in order for us to do this. The last one I want to look at is canary. It's similar. It's a kind of a mixture of both of the two. Where we have 100% running on our blue version, we want to deploy a new service, we deploy a new one, but we only move a bit of the traffic, in this case, 10%, for example, and we let that sit to verify that everything is OK. And once we are confident enough, we're going to flip everything at once, the rest of the remainder of the traffic, so. It's pretty much just a 2 or 3 step, depends on how you look at it, 2 or 3 step motion where you only. Do one migration of traffic or part of it, and once you're confident enough that everything is working, then you will flip all the other traffic. And again, 2, and afterwards, remove the blur the blue service. Again, zooming in on the actual traffic shift because this is the only different change that we have. In the beginning we have 100% running to our blue version. No traffic going to green. We move some of the traffic, in this case 10%, over to the one version, and again we have the bake time. This is what we're there to run your tests, run your validations, to see that everything is working correctly the way you expect it is. Let it sit Better data performance is working as it should be, and once you are confident enough and have got the correct metrics and everything that you need, then you can flip everything over to the new version and allow you to migrate your service to a new version. We like to Provide ECS as what we call best practice by design. Everything which Malcolm has been talking about, the fact of start before stop, placement mechanisms, how we actually do the updates of um software of the instances, everything is the best practices which we internally use for our own fleets and our own services. We want to provide that option for you as a customer to use those same primitives as well. And by the provisioning of resources in each version, for example, best practices by design, the life cycle hooks allow you to gain that confidence. We give you that opportunity, and I, I try to refer back to that, that shared responsibility model of we're taking on toil, we're giving you some of it, but we are giving you the options. So for example, you have to validate that everything is working correctly. And you have to provide that code. It's that shared responsibility model, but we are giving you the options and the mechanisms in order for you to allow you to do that. The bake time as well is also a best practice internally. We do that for every single one of our deployments. We're also giving you that opportunity and that primitive to allow you to also make use of that uh mechanism as well to validate your applications are working correctly. So, as Malcolm asks, we have different spectrums of what you can use. So when would you, the question that a lot of customers come to us, when do I do blue-green, when do I do linear, when do I do canary? And as any good IT person will say, it depends, obviously. There are um. Benefits of one version over the other, and it will depend on your kind of application. We look at blue-green mainly optimized for speed. If I want to actually move over to a new version very, very quickly, this will be the fastest way to do it. It will of course require you to have a robust deployment mechanism and validate that everything is working, because once you switch, you're switching all of the traffic and that could cause a problem if there is some kind of degradation or bug within inside your system. When we look at linear. This is kind of a, you have to be, I want to be a bit more conservative. I want to do it slowly and move my customers or my traffic over to actually gain the confidence. I'm not 100% sure how this works. It'll take me more time to validate that the traffic is working, there's no degradation of performance. Those kinds of things, I want to be a little bit more conservative, but on the other hand, this will take more time. Your deployment time will be longer, and the rollover to a new version of a service will be longer, and as a result, of course, because we're provisioning, as we said, 100% of capacity on both versions, you'll be paying a slightly bit longer amount of, uh, slightly bit more for that amount of time that both resources are sitting up in the air waiting for the actual migration of your service from one version to another. Canary is a kind of a mix of the two. In other words, I want to do it quickly, but I don't want to wait so long, so I'll do a bit of my traffic. I will validate that I know exactly, I'm confident enough that if I do a bit of it, and it moves and everything is working fine, then I'll move everything. That's what our customers kind of look for. But it's not only on your own confidence, on your own uh operational um practices, it's also kind of depends on the application as well, because if I give you, for example, your application has some kind of um Everybody talks about AI, so let's talk about AI. Um, some kind of LLM application, in other words, where you are, um, updating a version of an LLM inside your application. So if you do, um, A migration from one version of service to the other and start on linear, for example, there could be a chance that the Customer experience coming to these uh different services, well it might be different because the one LLM version behaves slightly different than the previous one. So that might be a reason that you, for example, would like to do something which is more blue-green. I want the the the customer experience to be as standard as possible and not have any deviations where customers could get different answers, for example, from an LLM, then I would do a blue-green to move as fast as possible from one version to the other. If it is an application which is a simple web application, maybe upgrading one version to another, then maybe that won't make much of a difference. It would depend on the kind of software that you're actually using. Before we leave, we would like to leave you with a couple of QR codes and resources which we can actually dive deeper into. Into a lot of the topics that we talked about today. The first one is actually the documentation, which is very, very robust and gives a bit more than I did actually today with how exactly each of the stages are, each of the phases are exactly what you can do and how you implement that. On the right-hand side, on your right-hand side is um a blog post which we published about a deep dive into ECS managed instances with a lot of the information that Malcolm provided today as well, but not all of it. And the last two are to build a library um articles. A build a library is a website where we have White papers of how we build things in Amazon. On a regular basis. Of course, without all the details, but from the concept perspective, one, if these are white papers which are written by our principal engineers, senior principal engineers, distinguished, the top people in Amazon who build systems. Which scale to things like, as we say, 3 billion tasks a week on a regular basis. And the first one is about uh both of them are written about how we do deployments internally in Amazon, how you can ensure, for example, rollback safety with deployments. These are the kind of things which we do on a regular basis at huge scale in Amazon, and we would like to provide that information to you as well, so you can start reading about it and implement that as well inside your own organizations. I'd like to thank you very, very much for your time. Our emails are here up on the screen. If you would like to take a picture, send us an email. If you have any questions, we'd be more than happy to answer them. And of course, before you um leave and preferably before you leave, or you can also do it on the way, please don't forget to give us the feedback from the session itself in the application, the IDWS events application. We would really, really appreciate it to hear back, to hear back from you how the session was, was it useful, is there any feedback you would like to give us directly about the actual uh session and stuff, we'd be more than happy to hear from you. And if there are any questions, we will be somewhere over there towards the entrance of the um. Um, of the hall so that we don't disturb the next session coming, which is already lining up. Thank you very much for your time and enjoy the rest of your day and your event. Thank you all.