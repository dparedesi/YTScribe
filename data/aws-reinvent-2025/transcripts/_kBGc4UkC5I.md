---
video_id: _kBGc4UkC5I
video_url: https://www.youtube.com/watch?v=_kBGc4UkC5I
is_generated: False
is_translatable: True
---

Great, so let's get started here. Thank you. So this is a session on the Amazon on AWS track. In this particular track we feature solutions that were built by teams across Amazon. This particular talk, the solution was built by Amazon Ads. As you see, Amazon is a group of more than 100 different entities. Uh, you may know some of them Prime Video, Ads, Ring, but common to all of them is the scale at which they operate. As this slide builds up, you'll see some rather large numbers. Whether it's storage, whether it's throughput, or in general capacity, these numbers are really huge. And a fun fact over here, this is these numbers that you see on the screen is not for a duration of a year, it's not even for a duration of a month, it is for a duration of 4 days that happened in July, the prime day event that happened earlier this year, these are the stats for that event. Let's do a quick show of hands. How many of you here have used consumer LLMs such as GPT or Gemini? Before I could finish the sentence, I saw people raising their hands, so that's a good thing. Great. So if you use LLMs, you know that when you're using LLMs in a process that is deterministic, it is challenging, right? And at Amazon scale, that adds layers of complexity to that particular challenge. Well, if you wanted to know how you can go about solutioning this, this is the session for you, right? And I'm seeing all orange headsets that's a good thing you're listening to my session and not some other session there, so that's great. OK, so in this session. You'll learn how Amazon ads harness the power of LLMs to understand their shoppers in more deeply, and this understanding helps them deliver better outcomes for their advertisers. You also learned the architecture that they use to build the solution. This architecture was built on AWS. All the fine tunings, all the optimizations that they did, we'll share with that, we'll share those with you. So I'm Varun Kamakarran. I'm a principal customer solutions manager at AWS, and joining me are Shengwaa Bao, who's the director of sponsored advertising, uh, from Amazon Ads, and Bole Chen, who's the senior manager in Shengwe's team. He was tasked with building the solution. So our agenda for today, I'll go through the introduction. I'll talk to you about the relationship between ads and Amazon, how this whole engagement got started. I'll pass it over to Shangwa. He will talk to you the use cases they pursued. Also he'll set context to why a G AI based solution was the right option for them. Then we'll have Bolla on stage. He'll walk you through the journey about building the solution, what fine tunings he did, what architecture options they chose, and he'll wrap this up with lessons learned. So let's get started here. Amazon Ads, on one hand, is a customer of AWS just like you. They use AWS services solutions to build their own products, their own services, and give that to their customers. On the other hand, they're our partner. We package our products with theirs, and And provide that to our joint customers. So this dual relationship has been greatly beneficial to us. Now, to look a little bit about where ads came from. As you'd imagine, they were born in the cloud. Their first product, which was more than a decade ago, was Amazon advertising product, and as their customer base has grown and as more and more people have joined their platform, they've built more complex products. They were an early investor in AI and most recently they've launched two products, Creative agent, as well as ads agent. They're both agentic solutions built on AWS. A look into the services they use. Early on, as you'd imagine, they started off with EC2 with S3 foundational building block services, and they moved on to have uh more complex, more managed services, orchestrated services such as step functions or EMR. They were the early adopters of SageMaker AI and most recently Bedrock. But the keys start here. They use a vast majority of AWS services. They use more than 180 different services, and for each of these services they push us. They help mature the services. They tell us, hey, this feature is not there, this capability is missing. Hey, can you do this for us? So we take the feedback, we build those capabilities, we mature the service, and that benefits our customers such as you. So in general our partnership has been mutual, has been fostering, and to give you a little bit more context into the use case, I'm gonna bring Shenggua Bula Shengwa on stage. Shengwa, please take it over. All right. Thank you Fun for the introduction. It has indeed been a great journey with AWS together. Next, I'm going to introduce a little more about Amazon ads, how it works, and there's a new opportunity GNEI introduces. Nearly 20 years ago, Amazon asked a simple question. How can we combine great shopping experience with branded discovery through advertising? Today, hundreds of millions of shoppers engage with Amazon through different channels. They can stream a video, they can shop at Amazon, or interact with Alexa. Through these interactions, we learn how Amazon shopper. Discover product, explore brand, or make a purchase decisions. With this in mind, we use this learning applied to the full funnel advertising solutions to help advertisers to reach customers at a scale. Let's make it more concrete, using one example in the Amazon store. This is a query for Halloween costumes, and this is uh one of the favorite query for the seasonal query I have. When a shopper searching for Halloween costumes, they may start with seeing a particular brand recommendation along with the feature product to build the brand awareness. As the shop continue blossing. We may service more relevant product in the search result for consideration. Or product groups share a certain attribute to help the shopper with the navigation. When the shopper clicks the product and land on a particular page. We may also service the complementary product or alternatives to support the purchase decision. So, with that, what, what's behind the scene is how it works, given a query of Halloween costumes. The Amazon access system need to first retrieve the most relevant subset of product from. Hundreds of millions of Asians. This is often, oftentimes in the order of 10s of thousands. We then use machine learning models to score this candidate and reduce to a few 100. And then we finally select a field to present to the shoppers. On a daily basis, we need to process billions of such requests. And this experience is empowered by hundreds of models behind the scene, running in real time. Now let's zoom in a little bit even further. To the models, how it typically look like and how it match up a to a particular shopper. The model oftentimes take multiple input. Including the queries, the search context, including the product features of the candid ad. It can also include the shop signals. With all of this input, the signal flow into a neural network architecture, which oftentimes include attention mechanism or mixture of expert designs, and eventually produce a probability score, which can tell how likely a shopper is going to click or purchase a particular product item. Without going to the technical details, this model oftentimes have billions of parameters, and uh during influencing, we need to perform tens of billions of operations within a tight latency budget for 40 milliseconds. These models are useful and effective in predicting the likelihood. However, these models are not good at telling why a particular product is a good match and uh why it is not. That's where GN AI can make a difference. These are some examples we saw earlier. Imagine now we are advertisers, we might ask a question, given this product, who are the shoppers that might be interested in these items? GEI can take the model. Product and also image title description as input. And uh produce who are the shoppers likely interested in these items. They can be either looking for fantasy scenes or traditional uh costumes, as well as simply for fun options. Similarly, Gian AI can help with sharp understanding. Imagine we have two shoppers looking for the same query of Halloween costumes. Even though they start with the same query, their behavior may tell they are having different interests, and the general is good at telling the nuanced difference as a shopper engage with a different product. In this case, G AI can tell, for example, John might be looking for some fun, attention-grabbing looks, maybe for a special, special uh school event. While Alice may be looking for some fantasy scenes with elegant design. Now with the both product understanding and the sharp understanding in place, general AI can also further help a reason about the matching. We know the taste John and Alice is looking for already. And GN AI can help identify the 4th group as a potential match for John, and the 1st group as a potential match for Alice, with the rest of the product first explaining why they are not as good match or with human understandable languages. From these examples we can tell where G AI model is good at. It understood the machine learning and also understood the human language. It uh possess the word knowledge, able to tell the nuanced difference of the product attribute difference. It can also understand the evolving shop interest. With that, it can also understand and listen about why a particular product is good or not a good match for a particular shopper. These are all promising directions we are exploring. But it comes with challenges. In particular, the GEI model can go up to 10, 100s of billions. And many of the use cases require us to respond within 7 seconds. In particular, we also need to respond to the evolving shop interest and the campaign changes. All of them need to respond to the requested scale of billions a day. With many use case of ALM in place, this workload can translate to something that is 10x bigger than the common consumer language model we are using today. With that, I'm going to introduce Bulla to the stage to talk about how we build the solution and the lessons we have learned so far. Well. Oh, sorry, thank you. Uh, thanks V and Shenghua for the great setup. In the remaining of the session, I will mainly walk you, uh, take you behind the scenes of how we build a dedicated LLM inference service to support Jian use case you just heard about. Let's recap on the main factors that can shape LLM inference performance. The first idea coming to mind is model size. In general, bigger model means more compute and uh higher latency. Uh, that is straightforward. Second, the token length, longer input token as well as uh generated output token, they both add additional more work per request. And together this affect, uh, latency and the throughput each single host can deliver. But these two are just half the story. Latency SLA as well as overall traffic volume also affect how much capacity our system actually needs. And here's a universal rule, latency and support, they trade off against each other. That being said, if you need super low latency, of course you will need more capacity, which means uh more cost. With this idea in mind, let's revisit the giant use cases uh and see how each one pulls this factor in different ways. Let's go back to the product understanding example, the one which we try to figure out which shopper segment would love different type of Thanksgiving costumes. Historically, we rely on product, uh, image, title, and, uh, brand embedding to represent the product. This works, but not very well when the goal is to uh identify difference and nuanced shopping intent, like shopper who may like attention grabbing looks. Well, LOM can unlock a richer view by digesting food product description, uh, customer reviews, as well as other long-form content, which can easily go up to 100,000 tokens. Because this information is relatively static, we don't need super low latency here. What matters is highest throughput, as we need to generate hundreds of millions, millions of requests on a daily or weekly basis. On the infras side, we use AWS that function and the event bridge to orchestrate this large scale offline bachelors. Data flow from S3 into a throughput optimized LLM end point, which is capable of hack concurrency. Then we sync the data into a storage layer on elastic cache and S3, where other ecosystem will consume as additional signal. Let's move to the sharp understanding example. Here the goal is to infer what actually a shopper is looking for based on recent activities. For example, from a few click, LLM can tell join is a shopper who preferfill fair and Humira's product. To do this, we assemble uh session contexts, consuming raw signals like uh clicks, items view but not click, purchase, and even product in a shopping cart. And putting them into an input prompt. Then LM can reason about why a shopper might like or dislike a product. And typically this input range from a few 100 to a few 1000 tokens. Latency wise we aim for what matter, as long as it can return before shopper's next engagement with Amazon, which is typically a few seconds. We don't need to optimize for the uh minimum value to balance cost and efficiency and the throughput. On the infras side, we leverage AMZO managed Kafka and the FL uh to scale our streaming pipeline, which is capable of handling hundreds of thousands of QPS. Within the streaming pipeline, it makes asynchronous asynchronous call to LLM endpoint. And similar to offline batch inference, LLM output are written into a storage layer. Uh, we have covered offline batch inference and the near real-time inference. There are also scenario where real-time decision making is critical, such as shopper product matching, as well as use LM to assist with product ranking. Here latency is the most important thing, because we need to return response within a few minutes, a few 100 milliseconds, while supporting a scale of hundreds of thousands QPS. To do this, our server make direct synchronous call to a latency optimize LLM and the point, right we see it as server workflow from different components including adsourcing and the ranking. Now that we have covered several G A use cases. Let's summarize on the key tenant and what do they mean to our system requirement. First, we need the flexibility across many aspects, including model selection, uh, workloads, optimization objectives. The system need to support diverse use case as model and application continue to evolve. And second, we need high throughput wherever possible for cost efficiency. Especially for heavy load from offline batch influence and near real-time inference. And finally, for certain scenario, we do need ultra low latency, so that LLM influence does not introduce additional delay to existing shopping experience. Uh, with this tenant in mind, we developed a dedicated LLM inference, uh, solution. Uh, using a hybrid stack of both software and hardware. The service run entirely on Amazon Elastic coordinated service with a mixture of easy to instance to support different demand. This choice is not only for cost efficiency purpose. It actually also help us navigate through uh GPU constraints. Actually, many of our workflow workload carry on more readily available instance types such as G6E rather than competing uh high demand uh instances such as P5 or P6 with VDWLL GPU. At the oxalation layer, we develop a model router and the job scheduler. This module help uh route traffic to the right model. And uh schedule offline batch in jobs based on their priority and current capacity status. For software, because this space evolves so fast and uh different model servers including uh VLM, TRTLM, and SGL, they can offer different organizations. We try to keep our uh technical choice and the system flexible enough. We also benchmark different configuration, compare their performance, and pick the best set half for each use case. And finally, system organization is very critical, talking about running LM in production at scale. We integrate with VDI Dynamo to adopt several of the promising organizations, uh, which I will cover in a moment. Let's also talk about how holistically this system integrate with other added ecosystem. It provides the lightweight Java client for a server to make direct synchronous call with uh for real-time inference. It also allowed uh streaming pipeline to uh evoke for near real-time inference. Shopper streaming event including uh view, click add to cart purchase. When this signal flow into a Kafka and Flink managed streaming pipeline, it makes asynchronous call to the LM endpoint using the same Java client. And for large-scale offline batch influence, we offer a batch interface for different teams to submit and schedule their jobs. Aside from, uh, real-time inference, whereas server make a direct call to LLM end point for both offline batch inference and the near real-time inference, uh, follow this pattern, they follow the same pattern by writing LLM output to a storage layer managed by Eastic cache and S3, where a server will consume them as additional signal. In the next few slides, I will walk you through several organizations we found effective to our application use cases for your reference. Uh, let me set up some context first. In LM inference, uh, the first uh output token depends on the entire input prompt. And after that, model generates output tokens one at a time in an auto-regressive loop. The phase with precise entire input token is called pre-fill. And the subsequent token by token generation is called decode. In addition, uh, to better leverage GPU model, modern model servers normally adopt continuous batching, which pack requests of different sets together, mixing prefill and the decode stage uh within the same batch at a token level. Because pre-fill and decode can run within the uh same batch, latency is always gated by the slowest phase, and typically it's pre-filled. And uh as you have large model or a longer input token, this is uh even more visible as latency penalty to prefill stage is bigger. As you can see in the example, assuming we have 4 requests in a batch. When request 5 arrive with its highly prefill stage, it can cause interruption to the decode stage of other requests. And uh of course it delayed the other requests. To deal with this inefficiency, one idea is called disaggregated inference. We separate preview and the decode stage onto different uh container and services, allowing them to run and scale independently. This can lead to better utilization of your resource and improve both latency and throughput. Still remember the near real time uh influence shop understanding example, where our input token ranging from 2000 to 1000 to 4000 tokens is much longer than the output token size which is only 200. Uh, here, the preview interruption is unavoidable, but that is why the organization can be effective. For example, we configure this aggregated setup using uh a VR Dynamo with a setup of 12 prefill workers, each with 1 GPU and another uh decode worker with 4 GPU. Testing against the QN 235B model, you can see this set up achieved up to 50% of the throughput optimization under uh same latency as LA as compared to the aggregated setup. Another powerful idea is called the KDA vire routing. Still a little bit of context first. Uh, KV cache is a technique used to improve, uh, influence performance by storing intermediate result and computation from previously processed the tokens. As you recall, uh, a token can be represented as a vector of floating point array, also known as embedding. Uh, and for your simplification, you can imagine, uh, KV, uh, KVQ or the key, uh, key value query. These are matrix that being used to compute the internal, uh, interaction score from these embeddings. Well, multiple require they share overlapping prefix, KP caching can help, uh, avoid redundant computation. KVO vire routing take this one step further by directing request to the GPO worker which already holds the most relevant uh cache data. In this example on the right, you can tell requests from same user almost a share, uh, same input prompt except the, the last uh moving part which is shopping query. Well, when we uh uh direct uh request from same user to the same GPU worker, this can maximize KV cache hit rate and help avoid unnecessary computation. Again, in this benchmark running on a VDI dynamo against the 32B model, you can see with KV router enabled, uh, this setup help reduce latency 2 end latency by 20 to 40% at different percentile. And the difference is even more obvious if we focus on the prefill stage or the uh time to first token alone. Now, let's take a look at the key enablers for this optimization. At the data plan layer, we use Amazon ECS to orchestrate and manage a cluster of over 10,000 GPU of different instance type. This is the foundation for us to build a scalable service and also adopt some of the optimizations. For, uh, for high-performing networking, we use AWS EFA to accelerate internode communication as well to better leverage, uh, network bandwidth. This is also essential for us to run distributed inference and adopt some optimizations. And finally, uh, we integrate with uh VDI Dynamo which bring together several key, uh, system-level organizations including KV router, disaggregated inference, as well as low latency data transfer. Together this enabled organizations and help with our latency and throughput. Here's another thing I'd like to cover. So we all know GPU are expensive, right? And sometimes you may also face uh GPU supply chain constraints. Of course, you don't want to waste your money or hardware, so do we. Beyond system optimization, another practical idea is to increase your resourceization is to uh dynamically allocate your capacity across different workloads based on their traffic pattern. Fortunately, uh, real-time influence for shopping site like Amazon has a pretty predictable daily traffic pattern. As you can see, there is clear daily traffic peak and the valley in the chart. Real-time inference must meet this demand. However, offline batch inference low, they do not need to compete for capacity during this peak hour. That being said, before traffic peak hour, uh, we can allocate more GPU to real-time inference service to meet the latency SOA. Well, as we enter into the offline, uh, off-peak hour, the same GPU can be shifted to support offline batch inference workload. So overall, we do not overprovision on capacity. Another important aspect of running GPU at scale is dealing with um hardware fault. Typically, GPU has a life span from 3 to 7 years, and over time it becomes malformed, showing symptoms like um. High, higher latency or even exception error code from your response. For cluster running over 10,000 GPU this issue can actually happen on a daily basis. So we leverage. AWS called Walsh to continuously monitor this signal. And we mark a GPU instance for replacements whenever we identify uh degradation on some of the signal. This helps with our system's availability without human intervention from our engineer team. OK, so this almost uh concludes the main topic I would like to cover. Our journey is still ongoing, but we still want to share some early learning with you. First of all, uh, as you can see, we did not build everything from scratch. Actually, we heavily rely on a big range of AWS services and offerings. This is the key for us to achieve the scalability and the reliability we need. And also free us to really focus on building the application layer, business logic, and try different optimization ideas. Second, we stay close connected to industry partner and the community. This is very important for us to adopt uh organizations and new solutions, even earlier than their GA. We always keep in mind to keep our system flexible and open. So whenever we adopt new approach, new capability, we do not need to re-engineering the, the entire stack. And finally, not every emerging normalization work for your use case and application out of the box. It's equally important for you to uh work closely with your application partner to understand their use case, including their traffic pattern, their input prompt design, their organization objective. This information help you, can help you uh make the right decision for uh for solution and for organizations. Finally, we'd like to acknowledge AWS teams, as well as Avidia teams, including Nvidia, AWS and Avidia uh Deno team to their partnership. Actually, uh, there's support spanning fromization benchmark to helping us adopt Dynamo running on coordinates on EKS. This is instrumental for us to bring the technical solution into production at scale. Thank you everyone for your time and I'm handing over to Arun. Thank you, Bula. So as Bula mentioned, you saw the solution approach was rather simple. It was built on foundational building blocks, right? You can use the same approach, the techniques that Bula mentioned in your use case. Uh, that concludes our session today, but before I let you go, uh, please visit the Amazon demo in One Amazon Lane. It is located here in Caesar's Forum. If you scan the QR code there, it'll tell you the location where it is. Uh, this is a curated list of demos. If you were wanting to know what our delivery drivers use from a technology perspective, what's in the Rivian van. There's a Rivian ramp there where you can go check it out if you wanted to know what Prime Video is doing for their life boats, what X-ray means. There's a demo for that. And also finally, there's the Zooks robotaxi. It's a cool piece of technology. If you've taken a ride in Zooks, you'll know what I'm talking about. If you haven't, go check that out. So that's all in the one Amazon lane right around the corner here. That concludes our session. Thank you very much for joining us. Please do provide us feedback. Go to your app. Let us know how we did, and if you're able to, if you're having any, uh, agenda, any sessions that you'd like us to have in the next reinvent, put that in there. Thank you very much. You have a wonderful rest of the day and reinvent. Thank you guys. Take care.