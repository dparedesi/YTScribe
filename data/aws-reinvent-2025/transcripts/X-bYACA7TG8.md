---
video_id: X-bYACA7TG8
video_url: https://www.youtube.com/watch?v=X-bYACA7TG8
is_generated: False
is_translatable: True
summary: "This lightning talk, \"Capital One: From Chaos Testing to Continuous Verification (SPS328),\" details how Capital One evolved their reliability strategy to match the speed of their deployment. Troy Koss (Capital One) and Shen Liao (AWS) discuss the limitations of point-in-time chaos testing in complex, constantly changing distributed systems. They introduce \"Continuous Verification\" as a solution to close the resilience gap. The session outlines four key dimensions for enabling this transformation: 1) Providing a controlled, self-service platform (using AWS Fault Injection Simulator) with safety mechanisms and blast radius controls; 2) Implementing emergency stop buttons and rollback capabilities; 3) Establishing Service Level Objectives (SLOs) to measure impact and outcome; and 4) Automating the process to run continuously (Motion). The speakers emphasize that this approach builds engineering confidence, allows for faster recovery, and ensures that systems degrade gracefully under failure conditions."
keywords: Chaos Engineering, Continuous Verification, Reliability, AWS Fault Injection Simulator, Capital One, Resilience, SLO, Distributed Systems, Automation
---

Come on out, folks. Hello everyone, good, good afternoon. How's your rain event going? Good, good. Show of hands. Welcome to our lightning talk. Hi, um, uh, my name is Shen Liao. I lead the technical account managers within strategic segment at AWS. I have the pleasure of working with Troy. Troy, I'd like to introduce herself. Yeah, thank you so much. Uh, I'm Troy Koss. I lead reliability engineering at Capital One. a lot of what we work on is enabling other teams to build their applications more reliably. We're here to talk about how Capital One transformed from point in time chaos testing to continuous verification. First, uh, we're gonna start with the why, then we will talk about Capital One's transformation journey and their automated reliability verification framework. Finally, how they scale chaos engineering and measure outcomes. So why does chaos engineering matter? In today's complex landscape, we can't just hope everything works. We need to know. I want to highlight how we can build more resilient applications by applying chaos engineering principles early in development. First, incorporating chaos testing at the beginning ensures resilience isn't something we think about only after failures occur. It should be something that's part of our development process. By designing with failure in mind, we make sure that our applications um can degrade gracefully when there's a failure. Second, we need to make sure that our applications have no single point of failure. This means simulating network and system conditions like latency, timeouts, packet loss, or disconnections. And these tasks should be both deterministic and randomized so we uncover unexpected behavior. Finally, we should proactively anticipate production level failures. We can do this by simulating the loss of virtual machines, availability zones, or even an entire region. Running these tests ensures our applications can, can degrade gracefully and recover quickly when there's a failure. Now, here's a challenge with point in time chaos testing. In a in a complex distributed system, entropy is constant, code changes daily, configurations drift, dependencies update. If you only test during, let's say, your quarterly game day, you are creating a resilience gap. Between tasks, your application reliability can degrade, can gradually degrade because new changes happen and as a result, we introduce new vulnerabilities. Continuous verification closes this gap. And this is the shift Capital One has made. This allows them to match the speed of testing to the speed of their deployment. Now, Troy is going to walk you through it. Awesome, thank you. So, so like Ehang mentioned, right, everything's in motion, everything's moving constantly, and we had to come up with a, a, a way and a solution to really address that, close those gaps, uh, and ultimately achieve a new level of reliability. So, so we, we see our systems are growing, we have new micro services, new things coming online, everything's interconnected, dependencies. I'm sure you've all have thought about dependencies at some, some point, uh, in your journey, and What, what, what, what can't occur, and we'll start there, is really that you have uh a rigid system where you, you fix something in place one time and then you're done and you move on and wait for the next thing to fail. You, you can't assume that there's gonna be consistent happy paths, or you're gonna be able to predict all of the failure modes of your applications. And, and lastly, in a, in a environment specifically for us, we have to be regulated and compliant at all times. I don't know about you guys, but when I wanna get to my bank account, I wanna make sure it works, right? So, so what has to fundamentally change and a lot, a big part of it is the culture that we had to bring is how do we embed those learnings and findings into our systems and have them running all the time. So we've repaired something, we've made a fix, how do we check that that's happening consistently? How do we have the flexibility to understand that our system's gonna have different failure modes in different states? What happens if our database, our primary database isn't active, right, and there's a problem, how do we have a different read replica or something that we can count on and depend on in, in, in that environment? And lastly is the governance. We we we don't want to stop everything to fix everything, and I think that's kind of the mode and mindset you have to get into is that we want to be able to be regulated, be well managed, but to be able to do that in a consistent way, in an automated way, and have our systems adapt to that versus slowing down manual changes, cabs, things like that, so. In order to achieve something like this, there has to be automation at play. And, and I'll be the first person to say that automation isn't gonna solve everything, uh, but in this case, we really had to step back and say like, how do we put all of those things in place at one time and, and do, do that, and, and the real curve comes down to the automation and tooling that you can build to enable your engineering teams to achieve this, right? In order for us to get there though, it's a, it's a, it's a, it's a rather steep cliff, all right, but to actually do it and enable the teams puts us from doing our continual game days or quarterly game days as Sheng had mentioned, and really moving into this always on continuous approach. So, we'll, we'll take a look at 4 key dimensions that helped us really unlock this, uh, this capability. The first we'll start with really being able to get the capability out to our engineering teams, right? But to do it in a way that's controlled, that's regulated, that has audit compliance baked in, and also allows us to have blast radius and control. Instead of just letting every engineer go after a fault injection simulator and start turning things off, right? Especially if you want to mature from doing your tests in lower environments and working your way up into production environments, you have to have these controls in place and these safety mechanisms there. And, uh, as an institution like ourselves, right, we were able to use the FIS under the hood to achieve a lot of the different chaos tests. There are plenty to, plenty to choose from, um, and I think it's what's important, and we won't go too deep into chaos testing, but really to think beyond just compute layer and the traditional we broke things, right? You're really testing your hypothesis across different layers of your applications. We're looking at your compute layer, you're looking at your database layer, you're looking all the way across your network actions and others, and there's, there's a, a, a wide catalog that's available. Right? And, and I think lastly, the self-service platform also enables us to build our own capabilities that maybe FIS doesn't have or to introduce other chaos, uh, potentials, so. Once we have that in place, the emergency and stop button is needed, right? Uh, nobody wants to, uh, let a, a runaway train run away, uh, and, and take down a bank. So, um, making sure that we have the telemetry and the data to help us understand like, hey, this is, this is going south fast, we need to stop, and our hypothesis has failed, and we're, we're gonna eject. Able to halt on that. And then I think the general concept, and I've had a lot of conversations with people throughout the week is like being able to like roll back at all capacities, but especially with the test, like we need to stop what we're doing, roll it back to the previous state, whatever, whatever injection we've made, be able to pull that back, right? This is one that's like near and dear to me, right? It's, it's like you can't hit a target you don't have, uh, kind of deal, and, and why having service level measures are so important and foundational for your application. If you don't have these, I highly invest starting there, uh, but if, but, uh, and, and getting some, um, but even if you don't, the chaos testing is a good mechanism as well to learn what your SLOs need to be, right? You have your, like offshore safety bonds when you're working on your application to, to know that like, it's OK for us to go do CAS testing, right? Like you don't wanna start doing CS testing on an application that is in turmoil or has failed recently and your customers are unsatisfied. And then at, at the opposite end of the spectrum, you really wanna like have that measure to say we are really impacting our availability by doing these casts sets, we need to stop doing these, right? Everywhere in between, we also have opportunity for learning. If you inject latency, you expect that your system can handle it and your availability maintains consistency. If that doesn't happen and you have a drop off, well then you know that your understanding of where your cutoffs for latency uh are clearly incorrect and needs to work on that. Uh, so there's a lot of different, like, intelligence that you can get from this data and leveraging it, but without this and knowing that the outcome of your system, that it works for your customers, is working, uh, it, it makes the cast testing a lot more difficult. So the the the pinnacle of this whole thing is setting it all into motion, right? We can still do chaos test points and times, and I highly encourage people to still do active chaos testing, game days, those kinds of events. They're going to be very valuable to learn and to test things and to get them working. And then once you've achieved that, once you have a chaos that in motion and you've, you've you've you've or you've had it running, you can then set it into motion at a recurring basis, right? But I think what we're, what we're identifying here is that like our systems have behaviors, they have, uh, different, different, uh, mood swings depending on the day, right, just like we all do. And to find out what those like edge cases are that you, that happen really allow us to get ahead of problems before they surface and, and, and really build into something larger, so. What we're testing is like, is, and we're really verifying is that, is our system behaving as we expect it to behave, right, at the end of the day. Um, OK, testing is still valuable, uh, but if you, if you don't really get into motion, that gap that Shang like illustrated earlier becomes apparent, right? You, you make a fix, uh, or you have, you have an incident, you have an issue. You identify a fix Some time passes. And then the same thing happens again, right? And no matter what we do, that's gonna happen because of the change of our systems and the entropy that just naturally exists in these kind of systems. So, assuming we have those four things in place, right, and we look at those outcomes, uh, you end up, I think, most importantly on here is this, the confidence that your engineering teams are gonna get. You find that in crisis, in chaos of uh an incident situation, there's, there's always that like natural hesitancy, right? Like, what should I do? Should I fail over, Should I not fail over? What do we do? Like, like, I don't know, and like if I hit this button, I think it's gonna work. I'm not sure, right? The, the best way to, to feel confident in doing something. Is doing something right, and, and just continuously doing it and being able to uh build that confidence in your engineering team and, and, and kind of weed out fragility of the system um that exists not even just at the technical level but like even as humans like we wanna feel confident hitting that button. When you get paged at 3 o'clock in the morning and you have to go make sure your system's working, uh, you, you wanna make sure you have confidence in doing that and it's, it's kind of BAU at that point. Um, ultimately, like the, the goal is, is better reliability, better trust in our systems themselves, but this does give us that resilience. Uh, it allows us to recover fast because we know how our system behaves more. So that like learning of what happens when this breaks, um, is already has occurred, and you already had that opportunity to understand the behavior. You, you become more familiar with your system, and then you know how to react to it, right? Um, and this, these kind of tests really allow us to do that, um, especially when they're running all the time. So, to kind of put it all together and and uh kind of conclude, like, the, the, the foundation to all this is being safe, right? The first thing I opened up with is put a big safety harness around it, make sure you know what you're doing, make sure you've got audit, you've got tests, you've got logs, you've got everything in place to see how it's gonna behave, your monitoring is in place. And then that really unlocks the next level for you to be able to start doing more advanced testing, setting things into motion, um, but getting that, getting the evidence that your test is actually successful too, um, and I think I, I didn't really hit on it too much earlier, but the, the, not just testing but verifying is really important. As we move from just like breaking things, which I think is still unfortunately uh a uh a misunderstood uh uh expectation of chaos engineering, um, into actually saying like, I believe that my system can withstand this level of latency rejection or the, or degradation. And it will behave in such a way when this happens, right? Uh, it will scale up, right, if, if, uh, if computers shutting down, or if, again, the example I used earlier, if I, if my primary database isn't responsive, that I have a read replica that it'll, it'll cut to, like, we want to make sure that those things are in place and they work. Especially over time, right, if you put together failures, failure modes and failures like scenarios, and you're like, yes, this works, we have this failover, we have this resilience added, and then you don't test that and you don't actually exercise that, come to find out, like in the heat of the moment, it doesn't work, it's not like the best time to discover that. So, um, you'll continue that, and then, and then the, the, the overall theme again, really is to run continuously, um, and to make sure that, uh, you set this into motion to prevent those gaps in reliability, so. Um, thank you so much for, for listening to us chat. Um, our Capital One booth is over there by the, the, I think Expo B uh entryway. Uh, come check us out, see what we're up to. Uh, I'll be over there throughout the day, um, as well. I'm looking forward to talking with y'all. So, uh, we, we do have a little bit of time. I don't know if they wanted to open up for questions or it's up to them. Any questions?