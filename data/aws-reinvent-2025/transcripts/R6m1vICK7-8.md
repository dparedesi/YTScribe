---
video_id: R6m1vICK7-8
video_url: https://www.youtube.com/watch?v=R6m1vICK7-8
is_generated: False
is_translatable: True
---

Good to go. OK. Hello, everyone. Thanks for joining us today. I am Amanda Quinto, and I am a solutions architect in Brazil responsible for public finances for the past 6 years. Today, we're going to talk about a little bit, so, uh, secure AI agents in delivering efficiency in high regulated environments. I'm here with two really nice customers, Edson and Andres. They are over there and they will be presenting with me today. So again, thanks for joining. It's uh really nice to be in Ring England this year with you guys. There's a little bit about our agenda. So we are going to talk about high regulated environments and how we can operate AI workloads on top of those kind of environments. And then we are going to talk about how we can use Amazon AIS to run AI agents and AI workloads on AWS. EO we will explain how you see COI using this kind of environments to run. Uh, workloads for G AI in high regulated environments. We are going then to move to Amazon Bedrock and understand how can Bedrock help, help us to deliver, uh, AI agents and also stay inside of our relations. And then Holland casinos represented by Andreas will be, uh, showed here for you guys to understand how they are using Agentcore to run their workloads on AWS. OK. So let's get started. We have about an hour here and I'm, I will try to make you guys uh up. I know it's a really heavy event and you guys walk a little bit long distances to be here with us today. So thank you again. When we are, we are talking about high regulating environments, we are no longer just thinking about deploying models on AWS. We need to operate inside of dense amounts of laws, standards, and patterns to stay inside of the regulations of our current countries. Here we are representing two different countries, that's Brazil and Holland, but we can see these movements all around the globe. Europe, for example, has the Europe AI Act that is a movement with strict risk-based uh framework that bans some users completely from AI and also uh deter determines some different uh obligations for high-risk systems to run both on cloud or in the on-premises environments when we are talking about data and AI workloads. Brazil is combining their emerging AI legislations with, with, uh, the legislation that we already have on top of data, something like LGBT for example. We also took, uh, two different patterns. We have some obligations that's federal for mostly, uh, data from public sector, for example. We should stay with this data inside of Brazil and we also have some discussions around data sovereignty. And we also have some determinations around, uh, specific industries. So for example, my industry, uh, uh, finances and public finances have some obligations that is determined by central bank. So we have, we received the, the normatives from central bank and we should, uh, make our customers to run inside AWS respecting this kind of obligations. This is our worldwide. Movement we have today more than 1000 different AI regulations around the world, and this is representing more than 69 countries. So it's not easy to stay inside of the regulations as uh providers such as AWS. So we work specifically in each customer cases to deliver the best architectures that fulfill this kind of requirements. There is something in common when we are talking about this kind of frameworks and obligations when we look into Brazil or South America or United States or Europe. There are always some pillars that are similar in different scenarios. So we have compliance and governance. We have legal and privacy controls and risk management. These four pillars always appear in different normatives and regulations around the globe, and that can, uh, make for us the, the bottom layer that we can. Uh, look into this and deliver different architectures that work for mostly cases around the globe. But again, there is different obligations in different countries and different industries that we need to always make sure that we are, uh, responding correctly for this kind of environment and, and obligations. So there is some uh patterns that we can follow so we can uh understand the customer needs, the regulation needs, and stay on top of this kind of obligations. We can see this in a, a layered pattern that can help us here. So we have at the bottom, we have the Gen AI application, for example. On top of this, we have regulatory layers. So as I said, it's different from each country, it's different for Europe, it's different for Canadians, it's different from Chile and China, for example, and we need to be specific here. But there is other layers that we can uh understand in a broader way. So for example, when we're talking about responsible AI and in AWS responsible AI is a really heavy pillar that we are following from our customers and also our services to deliver GN AI solutions for the customer. So when we talk about responsible AI, we have some pillars and patterns when we talk about data, when we talk about safeguarding our customers, when we're talking about security and prompt engineering structures that we can. Uh, be aware of malicious, uh, users, for example. So, uh, all of these kind of discussions are inside of the responsible AI layer in AWS. So this is a really, uh, interesting, uh, content that you guys should, if you guys, uh, like this kind of, of documentation, it's a really heavy documentation, and you should read about it because there is a lot of good, uh, instructions over there. And if you follow the responsible AI patterns, you're probably going to be, uh, inside of the obligations in your countries. We also have compliancy and standards, and here we are talking about ISOS. We are going to talk a little bit further about ASOS, but it's important just to know that there is specifically ISOs for specifically GAI workloads already. So we are also looking to this and we are also, uh, delivering architectures on top of it. And here we are talking more the, the technical aspects here in. So it's a layered approach. So we have the bottom layers on the regulatory and on top of this, we can think about uh more technical perspectives. So we can see NIST framework, for example, NICT 600 framework, for example, is a risk management framework for AI. But as you know, NIST is just a pattern that we can follow. They don't really audit your environment for this, but we can look into this framework and see if we are, uh, architecturing our systems on GNAI following these patterns, and then when we are going to suffer some auditation from the. Responsible government or something we are going to probably be uh inside of these of these obligations. OK, so this this AI framework 600 is a really good framework. It's uh the main framework that the United States is following for their obligations and the, the, the laws and definitions that you guys are following here in the US. On top of this, there is OASP. So OASPs are really, uh, common, uh, when we are talking about, for example, uh, OASP top 10 vulnerabilities for security. There is also the OSP, uh, LLM AI security cybersecurity pattern. So you can see the, it's, it's it's improvement actually. It's been proved actually. But you can see uh what is the most common uh vulnerabilities that can appear in GNAI workloads. You can see how we can, uh, achieve some, some techniques to, to avoid this kind of problems. So, OASP is always a good idea. There's also as the top 10 vulnerabilities for LLM applications. So you should also be aware, and there are also changes because it's, it's really new. So we are always having new, new vulnerabilities here. It's not a good thing, but it's a, it's a, it's a pattern in the marketing. And on top of all of this, we already have the AWS well architected framework layer for G AI applications. So you guys should know, uh, the AWS we architecture, I believe, and. And we have, we have a bunch of different layers for our six pillars, and there is already a layer 4 GAI workloads as well. So it's also, uh, uh, it's a, it's a live document. It's been proven as well, but it's the right, uh, live for our customers to follow. OK, so this, uh, layered regulation and controls should help us to build this kind of, of, uh, workloads on AWS following and maintaining ourselves inside of the regulations from our countries, right. Regulatories should not stop the AI adoptions. We think regulatories requires AI adoptions. We can also improve regulatories, uh, obligations using AI. We can use AI to understand the regulatory and then accelerates the AI, uh, workloads on high regulated environments, OK? We can see this in government in Brazil. We see this in the financial sector. We see this in health sector as well. So we can also, uh, we can make them work together. We, we, one should not stop the other, right? OK. So now we discuss a little bit about how high regulators works around the world, how AWS positions themselves on these matters. Let's go dive deep into the technical perspective now. When we are talking about Gen AI in AWS, there is, uh, 3 layers, and you're probably going to see this slide a lot on reinvent. I'm going to be real quick, quickly here because this is most, probably the most used slide for the AWS solutions architects around the world. But this is the AWS 3 layer for generate generative AI tech, OK? So, uh, from the top to the bottom, we have the Q family. So the applications family, we have the Amazon Bedrock that is our suite, our solution for allgen AI workloads running. We can run, uh, use different models. You can use everything on top of APIs on AWS and in the bottom here, we have the infrastructure layer. So we're going to talk about, more about the infrastructure layer here because of the CCCOB use cases and then we will detail this for you. The infrastructure layer here, there is, uh, I don't know, two names here that makes, uh, uh, a lot of noise in the marketing. There is training and inferential. They are ships specifically developed by AWS for machine learning workloads. We also have Sage Maker, and in Sage Maker, there's a whole world of solutions you can deliver different kinds of models using machine learning or G AI models if you want to, but we are going to talk about specifically GPUs workloads here. So, in AWS we have a bunch of different types of GPUs. There is a, a disposability of a lot of GPUs on our, on our stack, and you're going to understand how we can use GPUs to deliver GN AI solutions for high regulated environments, OK? But in the heart of NAI. There is a model, right? There are some commonly used models here, but before I think about the model, you should understand your use case. So, uh, what I'm trying to solve here, what's my pain that I needed to solve with Gen AI and check if there is actually a Gen AI solution that I should use for this pain, for this problem in my company. Yes, it makes sense to use GN AI for this kind of, of workload. All right, so, then you should find a model for this. There's a bunch of them in the marketing. We have also Amazon Novo and AWS. After this, you're probably going to use two or three models for, for your workload, and you will probably going to change this after a while. weeks and months after month we are seeing different models arriving in the marketing, and one is better for one thing, the other is better for another thing. You should not, uh, be in love with your model. You should use the best model for your use case. But there's a bunch of them out there. You choose two or three that works better for your use case for your scenario and go ahead. So, you use a foundational model. You can both uh use it, uh, rag, for example, or prompt engineering tools to solve your problem. And just to make, uh, one quick comment here, there, this is a 300 sessions, so I'm sorry, my hair. This is a 300 session. So we're not going to explain what's rag. We're not going to explain what is a prompt engineering, but we are going to say all of the events here, the solutions architects to help you, you with that if you have some, some doubts, right? So, you choose your model, you have your, your problem solved. You need to understand if you're going to use, for example, prompt engineering or RAG to, to make your data inside of this model or in parallel with this model. That should work for you too. If that doesn't work, you can fine tune your model. So there is a model, you can find, get your data, your specific data for your specific industry, and fine tune your model, your model to fulfill your, your use case to solve your problem. That works too. It's a, it's, it's not an easy task to do. You need a lot of data to do this properly, but it works well when we are talking about specific industries. For example, in Brazil, we are seeing a movement of financial sectors using this health sector, using this, and also public sector. The government is thinking about developing, uh, and fine tuning the, the the large foundation models with specific data. So that should work for some large use cases here, but it's not an easy task to do, and you need a lot of data. Or you can train your own model, for this is really, uh, it's a heavy, uh, task to do as well, but it should work. Uh, you should, uh, have a lot of data again. You should use a bunch of GPU to train, uh, your models, but that, that works as well. So you need to understand which approach is better for your workload, for your use case, for the size of your company, and then move forward. We have a model now what we can do. We need to do the inference. So, uh, in the, in the heart, there is a model, but we need to deliver this model for our application in a way that we can do predictions. We can do, uh, run, uh, uh prompt prompt engineering top of this model faster, easier with. Latency if you're talking about uh some production workload for example you can also do batch predictions, but you also need uh uh inference and point that is generally available that's resilient, that's secure as well for your workload so this is important here. In AWS there are 3 different ways to do this. We can use Amazon Bedrock, and we are going to talk about this a little bit further. You can use Sage Maker as well. So there are specific models that you can deploy on Sage Maker, and you can also deliver endpoints for, for, for making inference on Sage Maker, and you can use Amazon AKS. There is a, a really interesting movement uh running right now, and we are seeing this growing on the AWS with Amazon AKS. So if you Search for do on AIS data on AIS. There's a bunch of interesting materials. We have a bunch of customers using this around the globe. But to use, uh, Kuberges as your main, uh, main platform, you need to understand that this is, uh, that's uh overhead on top of your infrastructure team here. So if you already are using Kubernets to run other kinds of workloads, to run your workloads in production in AWS, your team already knows the solution. That's probably a path to go. But if you don't know don't know Kubernets as well, you don't know Amazon AS. And you, you're going to discover this right you are trying to implement generative AI work clothes on uh AWS. That's probably going to be a, uh, a hard task to do. But I am a really fan of Cuban Etis, and if you are doing this, you're probably, uh, going to, to get some, some really, uh, outcomes of, of after all the overhead on your team. OK. So when you are using Kubernes as the main platform here, we probably going to already have some main pillars for uh production workloads on AWS such as disaster recovery, cost visibility. Uh, we can have deployment automations as well on Kubernets. And we can also delivers and deploy our inference endpoint and our generative AI workloads on Kuberne. So instead of deploying um Amazon EC2, for example, with GPU and run everything inside of this ECU and have problems with scalability and have problems with deployment automations and disaster recovery, you can put this inside of Kubernets with Amazon Cas and then run your GAI workloads in this kind of, of architectures, OK? I'm talking a little bit more than I used to, and it's really cold in here, so I'm going to take some water here, but let's, let's keep going. So, uh, why customers choose EKS for instance, as we were discussing, there is a bunch of good things such as disaster recovery, scalability, cost predictions as well. We can, we can have some, some tools for this. But for me, the main goal, the main, uh, reason is for, uh, open source ecosystem. So the open source ecosystem for Kubernes has been really, really good, really dense material, uh, for the past, I don't know, more than 10 years for different kinds of workloads and for GAI workloads, this isn't different. So there is a really good materials for, uh, Kuberne's GNAI workloads, uh, with the open source community. They are always developing new solutions for this. So when this started, I don't know, maybe 1 year and a half ago. We, we didn't have solutions for deploying clusters with GPU. We didn't have, for example, images that was GPU prepared. We didn't have infrastructure code solutions for this, but one year and a half later. We already have some projects that is really mature and open open source ecosystem that supports these kinds of workloads all around the globe. So again, if you guys like this kind of, of materials, you should uh search for data on AIS. There is a bunch of interesting materials over there. We also have all these pillars here for, for the reason to choose Qnets for me, uh, the, the open source ecosystem is the most interesting one. You can also run this everywhere, so you can run this on AWS. You can run this in. Different providers you can run this inside of your own infrastructure if you have for some reason you have uh GPUs over there you can make some tests on your uh on premises environment for example. So there's a bunch of different reasons here that we can use EIS. And now, Edson will share a little bit uh of the CCOB case with us and how they are using this kind of architectures to fulfill their business needs. OK, so Edson, please. Thank you. Thank you, Amanda. Uh hello everyone. As I'm on, uh, as Amanda said, my name is Edson Lisboa. I am IT executive at CIOI. And COI is one of the largest cooperative financial system in Brazil, the Creti Union in Brazil, and it's my pleasure to be here with you today during the next few minutes to share, uh, on the reinvent stage, to share a little bit more about our generative AI journey and how a highly regulated financial institution is successfully adopting generative AI. At scale, keeping security, governance, compliance as non-negotiable pillars. So, let me start with who we are. As I said, S COI is a cooperative financial institution in Brazil, and CEOI is present in all over the country. As you know, Brazil is almost a continental country, and SEOI is present there. And SEObi offers almost the, the same uh financial products and services as. A traditional bank does. CEOI is present in almost 2,000.5 Brazilian cities, and it's very interesting to emphasize that over 400 Brazilian cities, there is only COVID there. We have more than 300 credit unions integrated with our cooperative system. We have 14 central co-ops and we have 4700 brands in all over the country with more than 60,000 employees. All this structure serving more than 9 million people in our country, and it's very important to emphasize that even. Uh, considering this, this presence in our country, over 98% of our transactions are performing on our digital channels such as mobile app and Internet bank, and as a financial institution in Brazil, it means that we are, we are living under a very strict regulation from our Brazilian central bank. And for us governance is, it's not only a compliance check box, but it's really a true competitive differentiator so we decided to create a centralized structure to control the decisions about governance with clear accountability, risky classification, and continuous monitoring. And when we started our generative AI journey, we defined four unbreakable pillars such as the first one, security. As a financial institution, for us, the security of our credit unions and members' data is our priority. The second one is scalability. Considering our actual numbers as I had showed you and considering our goals for the next few years, we really needed an environment and a solution to support our size and our growth needs, so, uh, it, it was very important to us. The third one, efficient cost management. It was very important to us run the large models considering the strategy pay as you go and considering the strategy uh related of open source LLM adoption. It was very important to us too. And last but not least, multiple LLM models. We wanted to use the right model to the right task. We really believe that only one model doesn't fit our needs. And these, these are the open sources LLM models that we are using. we are running on AWS using Amazon EKS as Amanda said, and the models Liama from Meta Mistro 54, the Chinese model Deep CR1, and the model from IBM named Granite. Sometimes we are using multiple models side by side in the same use case and in other case we can choose a specific model considering the fastest or considering the most accurate to uh this that subject. And considering all this strategy and uh and this information, Ms. Amanda, will explain a little bit more about technical information of our environment. Amanda, help me. Yeah, thank you. It's something, thanks for sharing. OK. So, again, we have a model as Setson said. They choose their model, they choose their problem, what they are trying to solve with generative AI. They understand the regulated, uh, environment and what obligations they should, uh, achieve, and this is the architecture that they are running on top of, on AWS to solve these problems. So we are going to discuss a little bit about this architecture here. It is actually a simple architecture. In the core of this, we have an Amazon AKS cluster, OK? In Amazon AKS cluster, we are deploying Eitu instances with GPUs. To do this, there is uh a bunch of best practices that we are discovering with our customers from the past 1 year, 1 year and a half or so, and we are trying to bring it here for you guys when you're going to, to, to try this on your, on your specific cases. So for example, you guys should uh think about using specific AMIs. 4 AI workloads. 4 GPUs were closed. So there is, for example, drivers from Nvidia that you don't want to deploy it by your own. There's a bunch of different versions that can, uh, so that you can have some problems when you are deploying this. So use specific AMIs. Other good idea is to download all the models for Amazon. 3 for example. So instead of downloading the model every time you run a new pod inside of Ceritis, you can use this on Amazon S3 and it's really, uh, it really quickly to deploy its own EKS when you already have the model here. OK, so this is a, a, a, a good idea when you are deploying this kind of infrastructure. But actually this isn't a Kubernet's presentation, so I'm trying not to go so dive deep into the Kubernet's perspective, but there are two solutions, two open source solutions that's really helping and making this workload, uh, make sense for this workload on COP. So, uh, you know GPUs are expensive. They are not the cheapest, uh, infrastructure, we know this. So they are using car. to scale Kerns with instances on. So instead of just deploying the instances based on this model needs this shape of GPUs, I will deploy my cluster with these node groups, and that's all they are using Carpenter to do this using spotting instances on. Coberniches, so instead of just leaving the GPUs over there without utilization, they pay as you go, as they go, as you go, as Edson said, and use these GPU instances based on their needs. But there is another solution in on top of this that is also, uh, an open source solution that is Skeda that's for, uh, for the, the, the, the growing I'm trying to not use. Culminates concepts here to to to scalability the pods inside of the cluster. So whenever they received a request from the environment, so if they are not using the the final users, they have both inside users and customers using this environment, but the users are not using the environment. They don't run any pods, so they don't deploy any GPU instances, so they don't pay for it. So the Cda is, is used to scalability depots based on the request. So we receive the request in this environment, then we will deploy it for the, for the end user. It's uh It's something that you think about it because it took some time to deploy it, but if there isn't, uh, I don't know, uh, transactional workload that's running inside of the, of the cluster, maybe the customer, the, the end user or the final customer can, uh, wait, I don't know, some seconds over there to deploy this to then start using the generative AI model or something like this. So this works better. Scope and that's why these environments make so much sense with this kind of architecture combining Carpenter and Cda using EKSC clusters. They can deploy different models. They can achieve different, uh, business cases using this environment. So there is a bunch of different cases here and so we will explain for you a little bit later inside of the same cluster. So the same cluster is running this. To operationalize this, they are using other two open source solutions. So as I said before, that's one of the core reasons that I believe that Kuber H makes so much sense for GAI workloads. The open source solutions. I, I already talked about two. Carpenter. It's developed by AWS, but it is an open source solution and Kida. There's also another two. So we have Lho LM. So instead of of managing these, these models inside of the, of the cluster, I don't know, in command line we can expose this for the final users using L LLM. So this is also an open source solution. It's also, uh, somehow choose how to deploy this on AWS in our documentation. It's a really nice solution that you can use to, to manage all these models inside of the cluster. So we have an a customer interface, uh, uh, end user interface that you can. Run the models. And also VLLM VLLM is a solution that's actually uh new in this environment, in SequOI environment. Before this they were using aEA but they achieved that using VLLM they have much more, uh, it is, it is much more faster. It's more computer optimized to run the, the models and VL LLM is actually a new solution. New open source tool and is improvement is receiving improvements from the community so it's probably going to be even cheaper than we started using this kind of engine to run LLM models on top of Kubernex. So this is here are 4 open source solutions that really makes sense for them to, to run this workload, to run different kinds of of workloads inside of the same same cluster here. And then we can uh also use other AWS uh services around this. So for example we have Amazon ECR for the images we are using uh Iress to expose the, the end points of the models for the solutions with, uh, AWS load balancers and other, other solutions, but the, the main of the infrastructure. The structure of the architecture here is inside of the cluster, so this kind of solutions is really making sense for this workload for them and also deploying GPU instances on AWS when we use Carpenter, we can, we don't need to work on the heavy lift of thinking about which different instances there is available in this zone, for example, you can. Put 3 in Brazil we have 3 zones. We can put those 3 different zones inside of the of the carpenter manifests and say I, I should use these these shapes of GPUs. Which one is better? Which one is available, which one of these is cheaper? You should deploy it on the, on the cluster for me and then let Carpenter works. He does the works for you. You can use budgets, uh, on Kubernets to, to arrange the way that they took off your instance, but they only took off your instances or your spotting instance after you have another one deployed. So there is a, uh, uh, some, some ways to work with spot interruptions when we are talking about Kubernes environments. So there's a bunch of, uh, of solutions and, and. Tricks and tip tips here that we can uh uh understand how we can operate GNHUAI inside of AmazonS uh cluster on AWS. OK, but the main, the main idea here is that we can run different workloads inside of the same cluster with cost and management achieved. We, we are going to, we are being able to run this with some. Some low overhead on top of the team. OK. So let's someone now explain which are the outcomes and uh what are the, the business uh solutions that they are achieving. Thanks, Amanda. Thanks, Amanda. And now for me is the most important part, considering the real use case and considering the business results using all this platform, all this environment, using this uh LLM models as Amanda show, showed to you. First of all, I would like to share a little bit about our internal intelligence development assistant named CISBEO AI. Cisbe is the name of our core banking platform and we, we decided not to pay or not to buy a specific solution to integrate to, to our platform. We decided to create our own solution using, uh, these LLM models that I showed you and. We integrated this solution with IDs of our developers. Nowadays, we have almost 10.5 developers in our uh company. And there are a lot of benefits of using Sysbe code AI like automate coding sites, automate code recommendations to the developers, uh, accelerate the process of onboarding of new developers was another benefit, say. Time to delivering new features and new solutions to our credit unions and our members and not only save time but also improves the quality of our solutions and continue support throughout the development cycle. The second one, our Bekov automation. Nowadays we have almost 8 digital robots automating manual tasks and not only simple manual tasks, but complex manual tasks too. And it's the reason that we are using generative AI integrated. Uh, with our robots, AI agents, and it was possible to save almost 400,000 human hours with this kind of solution. Another interesting real use case is our investment advisor. It is a specialized support for investments using generative AI. And it's possible to create good recommendations about investments to our investors considering the member profile, considering the best market practices, and for sure considering our products, investment products offered by our credit unions to our members it was another very good solution to our business, uh. Every day and last but not least, I would like to share about our core bank smart assistant named CISB AAI. There are a lot of of features in this solution, and it's possible, such as it's possible to interact with documents, not only normal documents, but legal documents, contracts, and it's possible to. To, to interact using natural language and uh get good answers using generative AI. And it's possible to do intelligent search and it's possible to create analysis and generate automatically uh decision credit loan decision reports using generative AI using this solution too. And finally I would like just to emphasize that there are a lot of other benefits and and results for the business considering. Uh, these use cases using generative AI and other use case that we have, and it is a, a good option, a good option for our business using this kind of solution. So thank you for your attention and Amanda, stage is yours again, again, yeah, thank you, Edson thanks for sharing. Thanks for sharing the, the CCOP journey for me. It's, it's a pleasure to be here with CCOBI. We've been working together for the past 6 years in, in AWS with CCOP, so it's a really nice case. Thank you, Ed. So thanks for your, your presence. But, OK, uh, let's dive to the second part of the presentation. So we were talking about high regulated environments on top of Amazon AKS and GPUs infrastructures, and now we are going to talk about how we can build, uh, AI agents on Amazon Bedrock, also covering the regulations perspective, OK. So let's go again in the most used slides in the, in the ring bands. So we have the, our two layer here and now we are going to talk about the middle layer. So Amazon Bedrock is a tool focused on builders and developers. So we are using building blocks to deliver gene generative AI solutions for our customers. We start just I don't know, maybe 2023, I believe, uh, just with some models and some ending points for our customers based on APIs and now there is a fully completed solution that hosts models but also covers guard rail of security, prompt engineering, prompt caching. There's a bunch of different solutions that you can use on top of Amazon Bedrock. OK. So we are here to talk about high regulated environments again and we were, we start talking about ISOS and I said we are going to return on this matter, uh, further in the presentation. So now it's the time. When we are talking about Amazon Bedrock compliance, there is a bunch of different, uh, regulations that we are already achieving. So there are different ISOs as well. We have SOC 12 3. We are IPA eligible, and there is a different for, uh, kinds of regulation for different kinds of countries. But there is one specific ISO that I would like to highlight here. So is the ISO 42001. And it is specifically for generative AI workloads, and AWS with Amazon Bedrock was the first provider to get the XyzO. So this is a real interesting thing for when we are discussing this kind of highly regulated environments, and the XySO is a, is a really important thing for the, for the product, for the Amazon Bedrock. But here, but the most uh uh common question that I received with my customers and even around uh data and AI concerns is about the data. So I would like to highlight here is that customer data is not used for AWS to improve any foundation models. And the cus the customer data is not shared between the other customers. So your data is isolated and AWS doesn't use this to retrain models or to improve the bedrock. OK. So this is really important when we are talking about every environment but for high regulated environments as well. Uh, for going forward, so you can also choose which model you would like to use. So there's not a black box where you have an end point and then you send your data and you don't, sorry, and you don't know which model you, you are using. You choose the model that you wanna use. You also, uh, put the correct um permission for your developers and builders to use those models inside of Amazon Bedrock for your specific region. So this is also important here, OK? This is, is, it's your choice which model you're going to use. If it's size of model you're going to use, use which region you're going to use it when you put your data in a specific region on AWS, your data remains in this specific region. For some countries, as, as Brazil, this is really important. We have some regulations around data, uh, that, uh, remains inside of Brazil, uh, Brazil, the country of Brazil. We cannot took this data from the public sector of Brazil. So this is important for our customers and we'd like to highlight it here, right. But you can do cross-region inferences if you want to. So for example, you are using Amazon Bedrock in Europe, and you, you'd like to have more uh. Ladies, for example, for your models you can use different regions for this, but it is your choice which region you are going to use. So for example, you, if you are in Europe and you, you are based on Europe AIX, you can use the data inside all the, all the Europe, not only one country, one specific country. So you can create uh. Profiles inside of Bedrock with specific countries that you can use it and then make cross-region inference for your, for your data based on your regulations as well. So this is your choice. This isn't made by default on on Amazon Bedrock, but you can use it to improve the performance but staying uh in your, in your regulated environment, right? But let's talk about Amazon Bedrock agent core, all right? So over the last year we've seen AI evolves from assistants to agents, right? So now agents is the, is the, the hype of the moment. The main difference here is that when we're talking about agents, agents, we are talking about a solution that works autonomously. So it's not task defined, he can work autonomously. You can create your own agents using open source solutions, for example, and I'm gonna let Andreas talk a little bit how you can do this and what is the heavy lift on top of it. But AWS understood that this is uh a need from our customers and that's why we deliver Amazon Bedrock agent core for the customers. So if you guys doesn't know Agent Core yet, you should. Uh, take a look into it. It's a really nice, uh, it's a really nice solution, a really nice service. We are probably gonna have a bunch of new things in reinvent in the next month. Month is a really new service. But here, let's highlight some parts of the Amazon bedrock agent core that works for highly regulated environments and that can help you, uh, to stay within your, your regulations, OK? So we have agent car runtime. So you can run and host your agents first purpose-built. It's serverless. It's totally serverless, totally, uh, you can, you don't need to deploy any infrastructure inside of, of AWS, but each session of your agents, so you're defining some tasks for customers, specific customers. So for example, in financial sector you. Have a customer that is running one solution side of the internet banking, you can make each session isolated from one to another. This is really important for, for regulated environments, and Amazon agent card does this by default when you are using, uh, agent card runtime. So this is one important, 11 interesting solution on agent card for high regulated environments. The other one is agent car identity. So it, it helps you to maintain secure your authorization, authentication, and credentials for the customers. So it's by default, uh, you can put the, the credentials from the customer or in any kind of authentication, uh, flow inside of agent's car, and he will remains isolated and can keep this secure for you. So it's also helps when you are deploying, developing your own agents on AWS. OK, we also have sandbox environments for, for code interpreter and browser tools. You can also do, do, do this inside of Agent Core without needing to developing anything or deploying anything. It's, it's a solution, it's a service that is inside of Agent Core, and you can just arrange which kind of code interpreter you wanna use or which kind of. Browser to you wanna do inside of the the the the whole mechanism of Bedrock with security with authentication inside of AWS. So there is a, a bunch of other uh solutions in agent core, but I would like to highlight this tree because it's for the high regulated environments could be useful for you guys to develop your agents on AWS, OK. Uh, so going forward when we are talking about LLEM models, there's also a discussion around which data was used by this model to train. How can, how can I know if, if this data has bias or not if it follows what I believe it's correct for, for my company. Uh, I would like to highlight here the Anthropic system card. So I don't know if you guys ever heard of it. You should, uh, read. It's a really interesting reading of 140 pages probably, but you can, you can read it. It's really interesting and it explains how it was trained and why is this trustworthy. OK, so Anthropic is doing a real good job inside of the transparency of how they train their, their models and which kind of data they are using. Uh, we also have AWS scorecards for Amazon Nova, so you can see this on the documentation of the Bedrock and the, the Nova model, and there's also a bunch of information over there of how they were trained and which kind of data they are using and what uh uh AI responsible that we were talking earlier. There's also described for Amazon Nova in, in these, in these service cards, OK. But even when we are talking about all these different uh perspectives of AI of regulated AI, LLM models are a non-deterministic technology, OK, so you can also use, uh, guard rails from Amazon Bedrock to stay within what you believe is the correct answer for your end users. All right, so don't believe that the models will always write the same thing. He won't. You should be able to prepare to, to define your architecture on top of this using. Solutions such as guardrails to stay within what is correct for your end user when you are using uh LLM solutions for end users. This is really important, OK, so you can also, uh, be aware of what the model is going to respond and also, uh, prepare for those, uh, malicious user that we were, were discussing with for last 10, for example, the different kind of users for GNAI. Uh, for cyberattacks, you can also use AWS Guardrails to help, uh, help you with this kind of, of, of issues, OK. Sorry. All right. Uh, so now I will ask for Andres to join me here. He will explain how Holland Casino is building their agents on AWS. Andreas, please. Thank you, Amanda. We're running about 5 minutes late, but I'll share very useful tips at the end, so please stay. They are very important. My name is Andre Gretenberg, and I do indeed work for Holland Casino. It might surprise you, but Holland Casino was started by the Dutch government about 50 years ago with a clear mandate to offer a safe environment for players. Currently we run 13 casinos in Holland and 1 online casino. We operate under a very strict gaming oversight through the Dutch gaming authorities. This centers around player safety and detection of fraud and money laundering. Now the stakes for us are very high, right? If we fail to meet these regulations, casinos will close until compliancy has been restored. We will suffer reputational damage. We have a very strong brand name, even by people that do not play. Financial penalties, they are seriously high. It's more than a headache. And we face possible license suspension. So apart from meeting these regulations, we have other problems as well. You see, regulations are subject to political decision making, change frequent, sudden, and with fixed deadlines. Sometimes we have 3 months, 1 month even to change things. Now the casino depends on 3rd party applications for gaming concepts, gaming machines, jackpot concepts, casino management systems. And the problem is that these 3rd parties cannot move fast enough as we like them to. So we, Holland Casino, need to have an environment where we are in control and have rapid response capabilities. And that's where we ended up in Amazon Cloud. In Amazon Cloud, we build our regulatory flows, our regulatory reports, and our regulatory alerting. And we do this since 2017. And we do this with a group of very talented colleagues of mine from not only technical people like testers and developers, but everybody like architects, product owners, IT managers, and we are assisted by colleagues from easy to cloud. Now over the years, as a team, we really have to understand Amazon Cloud very well to implement our regulatory use cases. It's a growth period. Now our success in internal belief in AWS has seen us grow in Amazon Cloud with more workloads. So now we host our central casino management system, business intelligence, data analysis using Sagemaker, and even our legacy enterprise bus we host in the cloud on Easy 2 because of its stability. Now why am I telling you this? You see, our business culture, and maybe many of yours as well, is to start small with an architecture, gain confidence with it, and then build up the capacity to execute regulatory use cases. And this is exactly what we want to do with our AI architecture. We want to start small, gain company-wide confidence in the technology, and then grow and eventually have agents assist us with regulatory use cases. So briefly, our journey so far in Gen AI. About a year ago, we started using cogeneration with the anthropic models in Bedrock. Then we looked at bedrock agents. In Q2, Q3, we started looking at strands agents. And those we really like because they're really simple and they show you a laser on. Then we started evaluating Amazon Bedrock Agent Core to host these agents on. I'll show you that as well. Agent Core just came out of preview and we're right now busy putting them in production. So we've been looking for a small but important use case, and we found this in what we call the management insight gap. You see, management is always a need for costs, security and compliance oversight. They will not log into the Amazon console to get the data. It is not the right tool for them. So management, stakeholders, especially the security department, came to us, the internal team, my team, to provide them with ad hoc reports. And we created many Python scripts to provide these reports. And this creates an unnecessary dependency and it does not scale. Now the scripts that we write for these ad hoc reports, they come back later when we talk briefly about strands agents. So keep them in mind, right? So as a first use case, we give management and stakeholder agents that give them information they need on a self-service basis. And because we start off, these are single agents. We're not yet busy with orchestration. Yeah. These agents center around costs, security, and operations. And they can have a dialogue with them. That is very important, right? We use trans agents. Why? They're very simple, and I'll show you in the next slide. They're compact with tools that you could use, but it is very easy to integrate your own tools and your MCP servers. It is Python-based. We're a Python shop, so it's easy for us. And it is open source, so you know exactly what is going on. And this is a trans agent. Yeah. In the middle, I declare my agent. I give the model, in this case a Bedrock model, and sold at 4.5. And I give it a system prompt. Normally we craft a system prompt. It's it's quite long. We spend quite a lot of time creating our system prompts. But for space purposes, I redacted it here. And then you give it a tool, in this case a billing tool. Now remember the scripts I was talking about earlier on, these ad hoc scripts. It turns out that it is fairly easy to transform them into tools for your agents. So don't throw the scripts away, right? Just make sure that you write your system specification or your tool specification really well. That is really, really important. Now, I had an ad hoc report for billing and I transformed it into a tool for my strands agent. And once I've set it up, I can invoke my agent. I can say like, well, what are my costs so far this month, right? And then it gives an answer And if you use sessions IDs, you can delve into it. You can have a dialogue, you can ask like, well, what were my costs, what was it compared to last month? Or I can delve into a service. So you have a dialogue, and this is what we want to give our stakeholders. OK, you've got your strengths agent. It works on my machine, and I can guarantee you it works on every one of yours machines. It's easy. However, we need to give it to our stakeholders, right? So we need to host it in Amazon Cloud. We want it to scale. We want it surveless. We needed to stream the responses, and security is a non-negotiable. So how do we do that? So we take our agent. We have rapidly fast API for streaming. You package it in a Python project, create your docker files, your lambda stack. You build it for authentication and authorization. You make sure it versions very well and then you deploy it. And this will work. I've done it. We've done it. And if you want to go this route, You're fine. However, if you say to me like Andre, eish, that is a lot of boilerplate code for a simple strands agent. Is there another way? Yes, there is. Bedrock agent core. If you look at the right hand side of the slide. In its simplest form, what you can do, you can bring your agent into the agent quarantine. By wrapping it into an in vogue Python method. And you decorate that method with an app. entry point, decorator. You test locally as you normally would, and at some point you say, agent core configure on the command line. And this builds all the boilerplate code I've just talked about in the previous slide. Once you're happy with that, you say geo launch, and it launches it into production. Into your Amazon environment. So it's very easy for CICD integration, even more brilliant for rapid prototyping. If you go this route, you benefit from all the other agent core services that it offers that Amanda spoke about earlier on. So how does the architecture look like? Simple. We take the services as native as possible, no magic sauce. So in the middle here we have our agent, core agents. They have access to foundation models, guard rails, memories, knowledge bases. We use knowledge bases, rags as a service, a bedrock knowledge base to obtain our regulatory information, tools. And then we've got 2 sets of apps. We've got in-house apps we develop ourselves and third-party apps. The in-house apps we host on Amplify hosting. With Cognito user pools, federated to Active Directory, Cognito identity pools, we obtain temporary SDS access keys. And this gives you direct access to the Amazon APIs. In this case, invoke Agent. That is really powerful. Third party applications that do not have the capability to access STS tokens, we just put an API gateway in between. It's a bit more effort, but well worth the compromise. If you look at the overall picture on how we do this, we basically host our AI architecture in two accounts, pre-production and production. Not different than any other application we host in our environment. And it is surrounded by what we normal in our normal multi-account architecture that takes care of security, compliance, monitoring, and networking. OK, my last slide before I hand you back over to Amanda. The lessons we learned in this path of Gen AI in Holland Casino is that AWS democratizes the use of Gen AI. You do not have to study 4 years at a university in order to create Well crafted, stable, and scalable AI architectures. So you can keep your DevOp methods, you can keep on investing in your current AWS knowledge, especially Bedrock and Bedrock Agent Core. Now this is important. This is the tips I was speaking about. That's why we placed it also in the box, right? Expect non-deterministic behavior of your agents and mitigate the risks. And here are some of our tips. Fine tune your system prompt of your agent. Really important. Give very clear instructions. Give the do's and the don'ts, and give a very clear output format. That is really important. You combine this with bedrock guard rails. Then agents must focus on one thing and do this very well. When I had my first agent, I got very excited. I uploaded all the tools that I had. So one agent to rule them all. The agent did not perform. Why? Because my tool specs were not really specific. And the model struggled to select the correct tool. Right. So make sure that if you need to use a lot of tools, make sure that you're very clearly in tool specification. Make sure you're very uh that the tool specification don't overlap with each other so that the model can select the right tool. If you set up knowledge bases. What you must do is you must invest in very realistic evaluation jobs, and you can use Bedrock for that as well. They give you very nice metrics. Especially if your knowledge base is in flux when the data changes all the time, or where your agents start using different models, you need to rerun your evaluation jobs. And the last one, and that one is very simple but very important, make sure that your end users have a very easy way of giving your feedback of your agents in production. Thank you. That's it for me. I'll hand you back over to Amanda. Thank, thank you, Andrew. Thanks for sharing the Holland casino. So, all right guys, uh, some final thoughts and thank you guys for staying by the end. It's a, it's a lot of content here. In summary, we can use AI to accelerate regulatory compliances. CCOP demonstrated how we can do this on top of infrastructure using EIS and Holland Casino with Andrews showed this, how they developed this using strength agents. OK. So there is a bunch of different paths to follow, but you can achieve your, your regulatory needs using generative AI on AWS. All right. Where do I start? That depends. That's the, that's the architecture answer for all the, all the questions. You can start with a compliant foundation, so you need to have a standard standardized foundation for run your workloads on AWS, select your models as we discussed here, build your agents the way that works better for you, and then deploy it in production in AWS with your, your regulatory needs. OK. Here are some resources. So this is the first day of the event. I hope you guys are enjoying the event. There are some resources here from the both perspective that we discussed, both with infrastructures and with Amazon Bedrock. I will wait for you guys to take the, take the photo. OK. Some sessions to watch at 3 events. So, say it again, this is the first day of the event, so we're gonna have a lot of different contents here with our colleagues from AWS. You can take the pictures too, OK. Thank you all. Thank you for joining us. Thank you for watching, staying by the end. I, and I would like to also ask for you guys to go into the app and say if you like the presentation or don't. If you have any questions, we will be outside of this stage for, I don't know, maybe 5 or 10 minutes, OK? Thank you guys and have a great evening.