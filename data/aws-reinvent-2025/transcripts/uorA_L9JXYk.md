---
video_id: uorA_L9JXYk
video_url: https://www.youtube.com/watch?v=uorA_L9JXYk
is_generated: False
is_translatable: True
---

Welcome. Uh, thank you all very much for coming and for spending your last hour of what must have been a very busy day already with us, you know, here. My name is Alex Francois Cyr. I'm the business development lead for product engineering in North America. Chalander, Gotham, and I are thrilled to share with you, uh, how Apollo Tires has been accelerating the engineering workflows by leveraging high performance computing on AWS. Let me just go ahead now and jump into the agenda for this session. So I will first go through a quick introduction on how we at Amazon and AWS are envisioning engineering excellence and how we go at executing it as well. I will then take a deeper dive into engineering simulation and how it can be implemented on the cloud and discuss a few areas that are key to tackle to accelerate product development. Challender then will take over to introduce Apollo tires and the challenges that they faced with tire development. Gotham will then come in after that section in order to provide details on the solutions built to accelerate engineering simulation and the benefits that it generated to Apollo tires. We will conclude the session with a discussion on the lessons learned and also what is next for Apollo Tyre. Now, when it comes to questions, if you have any questions, we'll be very happy to answer them, but after the session ends at 6:30 p.m. and we'll be able to do this outside of the room. All right, so let me just go ahead and dive into engineering excellence now. So we see engineering excellence being successfully driven by the following 3 key aspects. The first one is really about competing in the digital first manufacturing world. Traditional manufacturers now face digital native competitors who are really moving at an unpreced unprecedented speed. Success requires two things. First, the modernizing operations through cloud technologies and automation. And also accelerating product development to reduce time to market. Digital transformation is really not an option anymore. It has become very much existential. The next one is engineering complexity and mastering the digital thread. So modern products are incredibly complex. Think about millions of lines of codes, multiple thousands of components, and then obviously multiple suppliers as well. The digital thread connects data throughout the entire product life cycle in order to enable a seamless management of requirements, configurations, and multidisciplinary design. What is interesting here is that we can share an example with you about how Apollo Tire built a tire genealogy which is a digital thread type of approach, and I do think that this is going to be quite interesting for you to see. Finally, simulations allow to test thousands of experiments, I would say virtually, and saving time and therefore enabling innovation which would be impossible if you were relying solely only on physical prototypes. The last one that I want to talk about is is related to excellence at scale by mastering quality and efficiency. Customers now demand higher quality. Shareholders expect better margins, and competitors, I would say, always constantly optimize. Advanced technologies such as AI, IT, and computer vision really become game changers. Quality has to be built into every single process and not just inspected in. And the other aspect is cost reduction. What we mean in this case is really about eliminating waste and not making smarter decisions. It's not about cutting corners. These drivers are really interconnected and mutually reinforcing. So what we mean by that is like digital first operations are really enabling the digital thread, and at the same time the digital thread provides data for quality optimization. So manufacturers are able to tackle all of those three criteria will really be able to thrive in the global marketplace. So let's dive a little bit into how at Amazon we really practice what we preach. We design much of our own hardware and we source from a global supply chain, giving us really firsthand experience with these challenges. So our approach is rooted in working backwards from our customers like you, which is really what leads us to rapid prototyping, rapid innovation, and having this really customer centric approach methodology really drives everything that we do. We have multiple globally distributed teams who work on data center design and infrastructure that powers and our global operations, but we also have teams that work on devices like the Echo or the Kindle to be able to do that product development. And if you look at it as well, we have robots, the robotics side, where we also build new AI services that are revolutionizing our fulfillment centers. So a key enabler for us to be able to do this and accelerate product development is being able to access a secure cloud-based design and collaboration. Whether we are developing integrated circuits or even satellites now as well with Amazon Leo, we really leverage the cloud that enables us, our teams, to be able to work together seamlessly, securely at speed and at scale. So what I'm going to do next is actually look into time to results, because that's really critical and that's really what drives being able to develop products faster, which is the faster you can iterate through the design cycle, the more innovative, reliable, and higher quality your products will become. So let me walk through here like what a typical simulation workflow is in case you are not familiar with that. So it starts with the design, which is like the concept of the initial product that you might be working on. And then this moves into the model preparation, which is when you add binary conditions and create that virtual representation of the component of the product to be able to assess its performance. From there you will be going into planning the different simulation studies, so it's really about exploring the design space so that you really have a good understanding about how that product behaves based on different scenarios. And then the process ends with submitting the jobs to be able to run those simulations and then to be able to get results so that you can do some post processing and understand what kind of performance you are getting out of this product. Simulations are required throughout all the phases of the product life cycle. So first, as we talked about, is to explore the design space because this is really where innovation happens and competitive advantage is built. So what customers would do, like maybe you as well as you're running stimulation, is to really able to rapidly test different scenarios or design alternatives for your products. The second one is about improving robustness and reliability. Simulation can help identify potential failure modes and also optimize designs before any physical prototypes are built. And finally, simulations can be used to prepare manufacturing options and de-risk technologies, and in this case that really allows you to identify potential production issues early on, saving significant time and cost. So why are customers increasingly choosing the cloud for engineering simulation? So let me actually break this down into four key benefit areas or categories, with the first one being about capacity benefits. So on the capacity side, having access to massive capacity can really, really means for teams not having to wait to be able to run your stimulation. And also therefore increase your productivity as well. A paper per use model provides business flexibility, while elasticity in this case will give you the ability to scale up and down depending on if you need to run more projects or if you need to run less projects. These are actual benefits that Appulo tires experience and that you will learn about a bit later on from Charoder and God. The next one is about global benefits, being able to have teams that are anywhere in the world being able to collaborate seamlessly, being able to have also access to disaster recovery and resilience and business continuity in this case built in. So that also teams are able to access resources from anywhere, anytime to be able to do their job, and we do have quite a few customers now that by using this can really do like a follow the sun kind of approach so that the design is being worked on all day long if you all across the world in this case. Then we have economic benefits such as accessing the latest technology without having to do any capital investment, and the other one from the economic side is you obviously purchase a lot of software licenses, so you want to make sure that those software licenses are being used in the most effective and efficient way. And by doing this on the cloud, you get a better return on investment on that. And finally, the one would be related to environmental benefits because as you are leveraging the cloud, this might be something that you look into with regards to achieving some of your sustainability goals within your company, and we provide this through shared infrastructure. So these benefits really fundamentally transform how engineering teams work, removing traditional constraints around compute capacity, and also enabling innovation through innovation at scale. So let me show you now how we have architected our computer aided engineering approach for HPC to deliver those benefits. The system has 3 main components, with the first one being what we call the front end, which will include a C portal so that you can go into doing. The job submission, the job management, then it also contains the virtual desktop infrastructure that would be really to be able to run your engineering applications such as applications, and finally DCV, which is a technology that really allows you for high performance remote visualization. The next one is compute. So this encompasses the core compute clusters that runs your simulation with cluster management features and also a wide choice, as I'm going to show in the next slide with regards to purpose-built compute instances. And the last one, but nonetheless I would say a very critical one, is about data, because a lot of those simulations that you run then it generates a lot of data and those files are quite big as well. So what we need to make sure here is to ensure the proper storage of the simulation data based on when this data might be needed. And used for other projects as well as the management of these data for traceability purposes and this is where our digital thread comes back into the picture right here. So in summary, this architecture really provides the flexibility to match simulation workloads with the optimal compute resources which I'm going to explore with you right now. So this is where we would differentiate on the front with each job being able to leverage the most optimized instance type. So you can imagine that typically our customers, as you have complex products, you may have to run. jobs that are requiring different types of physics to be used like multiphysics or different solvers like computational fluid dynamics or finite element analysis, and these would be like a job 1234, etc. and each job can be matched to the best instance based on compute, memory, or other specific requirements. The cost and performance optimization comes from fine-tuned matching of the jobs to the best instance type from this broad selection that we have here. Now when it comes to how you would take costs into consideration, this is where you can utilize different types of instances like we have spot instances for cost sensitive batch jobs. Then we also have on demand for critical jobs that will not tolerate any interruptions. And finally, we have savings plans for baseline always on compute capacity. So Shaolin Directory in Gotham will also show you how Apollo tires took advantage of those different instances and approaches to be able to actually optimize the cost of those runs. Looking ahead, one thing we are also doing obviously is taking into account AI to potentially being able to set up smart provisioning in order to automatically recommend the optimal instance for each workload. So talking about AI, obviously we have also based instances, and those are to be taken into consideration on the simulation on the physics-based simulation front too, because independent software vendors such as or Siemens have been redesigning their solvers to be able to run on GPUs to again accelerate time to results. So we do have those and you may have heard Mike Garman this morning also mentioning a new one on the P6 side with the B-300 Nvidia chip as well that is available as part of this family. And that really gives our customers tremendous flexibility not only to solve compute intensive simulations like I mentioned, but obviously to run AI and machine learning workloads as well. So with this breadth of options, it really gives you the opportunity to match your specific workload requirements. It could be, let's say AI driven design, for instance, optimization or stimulations to the most cost effective and performance instance type. So now with no further delay, I will pass things over to Schallender, who will go into introducing Apollo tire and take a deeper dive into the different challenges that they experimented. Thank you, Alex. So before um I go into the problem statement, the tech part, let's try to understand why are we talking about it today. What is Apolloti and why this is important to us. So we are um a well-known brand in Asia Pacific. In the US we are heavily invested and we primarily sell by the name of Fredestine tires, which is our premium brand for the luxury and the sports cars. We have a global presence. European is our second biggest market. We have 7 manufacturing plants, 5 in India, 2 in Europe. We have the entire suite of product lines, passenger car tires, farm tires, trucks, uh, uh, off-roading, two wheelers, and with that you further add the complexity of the summer tire, winter tire, all season tire, and where there is heavy snowfall, there are studs based tires. So all these ecosystems that we have, um, it's not easy to run. We are a $3.5 billion company, 20,000 people across the geography. Designing products is becoming more and more complex. There are newer, newer models of vehicles coming and then electric vehicles have their own demand for the specific type of tires. And in the competitive market, what you need is time to market should be reduced, how soon you can design a good product and sell it across the market. So for all this, essentially what you need is a good solution which can help you design a good product. And that solution should also be at the same time cost effective. It's not just that you need a solution, but then a solution which can give you the price performance benefit. So in the next 30 minutes, I'll take you through this whole journey, what we did, why we did, what insights we used to make a decision, and why we moved on to the cloud. So I shared about the polo tires, why this is important, as I mentioned about the price performance and the vision that we want to achieve. These are some of the targets that have been set by our management, our our board, and to achieve these targets, as I mentioned, time to market, price performance ratio, even for the tires, it's very critical. So it's important that we design the right product. We heavily invest in our R&D. We have 2 R&D centers in India and in Europe. And for both the R&D centers, it is important that we give them the right resources where they can do the simulations with the chemical compounds or even with the design of the tires. How we started all this digital journey, it was not suddenly an overnight decision. So back in 2021, beginning of 2021, we decided that we need to move to the cloud. We need to come out of the silos for all the plants and need to consolidate our data, our resources into a single point of view. The first project we started was the collection of the IOT data, which is a gold mine for any manufacturing company. The real-time streaming of IOT data from our manufacturing plants, machines, PLCs. So that was the first project we implemented. What do I do with this data? How do I collect? I need a good data lake to collect the data. So we created our first data lake onto the cloud using S3 and Redshift. As we grew, we started using other services like Glue, DMS, Aurora. Today we have a massive, massive data lake, more than 400 terabytes of data, which includes a lot of X-ray images, video analytics that we collect for the quality check of the tires. Other than that, if I talk about especially the structure data and the relational data that comes from systems like SAP or transactional systems which are running as a SAS product, the CRMs of the world. So altogether that data would be around 8 terabytes compressed, if you know Redshift, it heavily compresses the data, which is a multi-node cluster for us. So that was the 2nd step we did right, which gives a lot of insights to my R&D team. Before we design a new product, as I mentioned, chemical simulation, compound properties in the test results that we do in the test lab or even from the field, they're extremely important when they design a new product. What was the performance of the previous product on the field? What kind of products are giving complaints and why are they giving the complaints. The next step was we wanted to bring the rest of the systems closer to the cloud, so we moved our entire SAP solution, our ERP from on-prem to the cloud. The third thing was now that we have a majority of the workloads, production workloads running within the cloud, we focused on. The industry 4.0 journey where we said that now we want to connect more of my IOT devices where initially it was just the mixers, then later on the extruders, the belt cutters, the tire building machine, and the last point of the tire which is called the queuing where the tire is baked. By the way, I was talking to someone today, and when I told them the tire is glued together, they were surprised. So sometimes it feels like the tire is a black doughnut where you just pour the rubber in and then you know you bake a tire, whereas the tire is made of a simple tire can be assembled with 200+ components which are glued together and then baked together. The entire life cycle can be 1 hour, 2 hours, 20 to 40 minutes can go into baking itself depending on how big the tire is. And if it's a mining tire, which would be twice the height of, you know, my height, probably 12 ft, it takes a long time to build such specialized tires. So it's not an easy product and I'm proud that we made tires, whether you buy a Mercedes, a Bugatti, or you know a regular car, you can't run it. Whatever the price of the car is, you have to buy a tire to run it. So all said and done, this was our journey. This is how we started, and then the next step was that we go to the high performance computing things, simulations, and the greater good now that we have seen that the cloud is giving us the ROI the returns. On the data lake now, what I said, 400 terabytes of data is there. Structured data is there. What do I do with this data unless I don't get an answer out of that particular data. Genealogy is one of the first things that my R&D needs before they design a tire. What is a genealogy? I want to understand from the final finish of the product, what components were used. As I said, 200 plus components could be used in the manufacturing of the tire. I want to know what component was used, who supplied the material, who supplied the carbon, who supplied the natural rubber, who supplied the steel, the nylon of the tire, on which assembly line it was manufactured, what was the shift, who was the operator, what was the weather condition? Was it summer? Was it winter? Was it rainy? That's why the product is getting more moisture or bubble or failure rates. So I want to understand the whole genealogy of the tire. Sometimes the genealogy starts from top to bottom. It could be bottom to top, or it could be somewhere in the middle. What I mean is that let's say there is a particular component which is giving me a failure. I want to know this component has been used by how many other child components so that I can trace out what kind of failure I can anticipate, or if I want to know what components were used to manufacture that component so that I can control the failure. So this is the genealogy where I want to start from anywhere in between. I want to know who are the parents, who are the child. One of the most complex process which the systems collect the whole genealogy data, and this was one of the key drivers for our projects. They wanted to know before we go to the HPC what kind of components we are using and what components we can reduce, something called less complexity reduction. What I mean by complexity reduction is if I'm manufacturing 10 tires, all of them 17 inch, for a particular car or SUV. I want to know if tire one uses 200 components, why tire two is using 205 components, why 5 components are extra in the second tire. So I want to reduce the complexity of my product. So these things were done so that we can give them this input for my design team so that they can use it for the simulation when they design the new tires in the HPC. The next thing was when my, my apologies for this image, some proprietary data I had to scrub off. So the next thing was, my team wanted to know, my identity team wanted to know. A lot of questions from the data that we were collecting. One way was that every time I go and design a BI report for them, a business intelligence report in the tabular, Quicksight, Power BI, whatever you use. The problem is every time I design a report, they have a new question. I designed another report. They have another question. There are probably 500 reports running today in the company, but everybody wants a custom version of the report. They want a different question to be answered. How many reports can I design? So the good thing is that we are living in the times of LLMs. What we did, fine, Forget about everything. Let's put an LLM on the top of my data lake. Now you you put a cushion of whatever you want. You ask a question in the natural language and simply say that what kind of product is giving a problem? Who are the top products, who are the products giving in a particular time, what was my sale, anything that you want to ask. My LLM converts it into a SQL statement, fetches the data and gives it to my users. I don't need any, you know, BI solution. So that was the next project that we did, which was extremely useful for my R&D for the manufacturing. Also, it can do a root cause analysis. If there was a failure, what was the cause of that failure? Obviously you need a good data layer and as I mentioned, we started right. We had a very solid foundation of the data layer. On top of that, we are able to run it. We have clean data. We have the trusted data governance is there, so all those things are there. So based on this, we started the next project. Now my R&D has all the tools they need to design a good tire. The problem still remains, how do I design a good tire? I need the hardware, I need a high performance computing solution to design a good tire. Designing a tire is a pretty costly process and complicated process. After you design a tire, testing it on the field, it needs a lot of effort. So there are race tracks where we go and test the tire. We test the tire. What is the braking condition, how will it brake on the snow, under the rain conditions, wet surface? What is the distance of the braking, how much noise it is inducing. In case of electric vehicles, it is the other way around. You have to induce a noise in the tire so that you know the vehicle is coming. So things are very dynamic and what kind of vehicle you are designing. So designing of a tire, as I mentioned, is a complicated process and at the same time you have to reduce the cost. If you can design this tire in a virtual environment rather than the physical environment, you don't have to get a dye, bake the tire, test it. You can do a lot of testing within the simulated environment itself. A lot of softwares have these. Configurations where you can put the concepts of the physics, wind resistance, and all, and then you can test the tire virtually. So these are some of the challenges. Our teams are facing hundreds of simulations per month, then multiple jobs, multiple users, aging hardware. A quick overview of what we use industry standard softwares. Abacus is a very well known software for the simulations and Siemens XX for the design. So a typical solution from the on-prem HPC to the cloud HPC. You heavily invest in the on-prem solution, you plan an ROIU of 6 to 7 years. You put a lot of KPE, you buy a large computer of probably you know 256 CPUs, 512 CPUs. You do not know who and when are going to use that. You buy a large storage, sand storage, probably 10 terabytes, 20 terabytes. It's sitting there. Somebody will use it. Somebody may not use it. If we move this entirely to the cloud, I can use it as a pay as you go. I have a cash flow with my company instead of investing â‚¬5 on an on-prem solution. I can use this solution on a pay as you go basis. Nobody is using these things over the night. Everybody goes home, sleeps. So why do I need these things on-prem? Let's move it to the cloud. Let the team use it, hire and fire. Use it when you need it, and then it automatically kills when the thing is running, right? You can have the script running. The other thing is the cost of computing is reducing day by day. A small analogy I have put here. When the Chad GPT 2 was launched in 2019, it was trained at the cost of 40 to $50,000. If you follow the co-founder of OAI, he claimed that today you can train the same Chad GPT 2 at less than $600 and considering the keynote that was given today, there are better chips. I don't know if you go back home and you train 2, probably you can train it at $60 so you never know. By the way, if you want to train it, the whole code of chat JPT 2 is open source. You can go and train your own chat GPT 2. So let's take a tech dive now, enough of theory, what exactly we did. A high level architecture diagram. Our users, they were used to using the Windows systems. Here's my on-prem users connected through the high tunnel VPN onto my AWS Cloud, and this entire box is my AWS Cloud. We created a landing zone for my users using the Windows terminal services where multiple users can log in. At the same time, see the status of their jobs, run their jobs. At the back end there was a job simulation server which controls what jobs to allocate, what resources to allocate, and it further depended on what you have configured. It can spin up a number of clusters and then it can run the jobs. My users can submit the jobs, they can see the job status, they can see the temporary files running. We have given them the power that they can select what kind of clusters they can spin. By default, the cluster size that they spin. And this entire thing is stored on an FSX storage, which is one of the key game changers. Without it, it would be really hard to share the data across the number of users. Typically what you do is that you put the data on an ABS storage system which comes with EC2. With FSX you can share the data with the high performance computing. So this is the whole structure that we created. I'll take a deep dive in the next few slides on what kind of configuration we use and why we use that configuration. So as I mentioned, FX is one of the game changers, and if you attended the keynote today, there are more enhancements which have come to FX announced today. What basically it does is that a shared storage where it automatically scales up and down the size if you want to do that, which is very unique to FSX because in EBS you can only go one way, you can only increase the size, you can't reduce the size of the EBS. So if I'm using an FSX of 6 terabytes, 10 terabytes, and I don't reduce the size, it's going to be very costly to me. My whole concept of OPEC is not going to work. I need to keep the cost under control. It's not that all users are working at the same time, depending on what kind of jobs they are running, small jobs, big jobs, how many jobs or users are there. I want my FSX to either increase or decrease depending on what storage is required on that. This is the beauty of the FSX that you put a script. If my consumption of FSX is more than 90%, automatically scale to another 500 GB or 1 terabyte or 2 terabytes, whatever the threshold you have put. If my storage has reduced to 60%, further bring it down to whatever the number you have put, or 40%, reduce it. It can also archive the data in the same form like S3. It can put the data automatically moved between the hot, cold, and the warm storage. So that's another thing it can do. It can take the snapshot snapshot of the user's data. So if a user accidentally deletes a file, you can go and recover from it. You don't have to rely on the full backup, so that's another good thing that it can do. So this was definitely one of the most critical things in terms of the entire solution when you are working with multiple users and you want to share the data. Now, when we moved from on-prem to the cloud, of course we wanted to measure the performance. My management, my board, who were sanctioning the dollars for this project, they wanted to know what are you gaining. Plus my users also have to be convinced. For me also, I wanted to know what kind of instance should I use. There are so many instances in Linux, Graviton, AMD, Intel, GPU-based. So which one should I use? What size should I use? So measurement was important for us, critical for us. We did a lot of simulations in the POC to come to a conclusion that what works for us. So this is on-prem and we submitted various kinds of jobs to see what is the time taken by that particular job and then we tested it on multiple combinations of the instances. Here are the configurations and the Linux was giving the best price to performance ratio in the high job scenario cases. The rest of the instances either were on the Windows or AMD and then you can see what kind of CPUs you were running, Intel and all. So we came to a conclusion, what is it that we need? How much money we want to spend and how much time we can tolerate in terms of the, you know, the queue or the wait. So that's what we, you know, tested. We tested it for one of the softwares and we tested it for the second one also, so that we can conclude what software will run on what kind of instance if we want. So after we were convinced that, yes, the on-prem versus the cloud is going to give us the better price performance ratio on what kind of instance we want, we configured that in the entire solution. Originally we were using Slum as our job scheduling tool, but then it's very crude, very raw, so we opted for a custom or proprietary solution sold by the company called Invisible, so we started using. Tachyon, we were convinced, yes, it is going to work well for us. Our users were happy with the whole UII interface and the way they can control the jobs, so we worked for a long time with Tcheon to customize the product for us and make sure that it is well adopted by our users. The other thing, as I mentioned, CPU was just one of the things. The big factor was what operating system to use. My users were more comfortable with Windows, but is it really I want to give it to them? The front end is Windows, as I told you earlier. The landing zone is Windows Terminal services. What we saw in the back end, the users do not need to know what is running in the back end. It's a black box for them. So if Linux is giving me a better price performance ratio, we said in the back end we'll run the jobs on Linux. As you can see, there's a 60% cost saving between Windows and Linux for the same jobs that we were running earlier purely on Windows or in the PC environment. So many times I hear from the people or the CIO that cloud is costly, the cost is overrunning, but have you done your due diligence? Have you done your benchmarking? If you do it correctly, you can actually come to an optimal point on what you want to run Windows, Linux, AMD, Intel. I was giving a session during the day earlier where I gave some golden rules on what kind of CPUs I go with, what kind of OS I go with. I'll take probably 30 seconds to repeat. So my golden rule is if I can run it on Graviton Linux, I'll do that. That is my first choice. Amazon Linux Graviton, cheapest solution that you can get. If not, if your application doesn't work on the graviton architecture, then probably I'll go with X86. If I'm going with X86, I'll go with AMD. If not, then I'll go with Intel. If none of these combination works, Linux doesn't work, there are enterprise applications like SAP, then probably I'll go with Enterprise Linux like Suz Linux or Reddit Enterprise Linux. If specifically you need Windows, then and only I'll go with Windows. Again with Windows, my first preference is AMD if not Intel. So I have a rulebook. I follow that rulebook. Amazing cost savings that I get. And the same thing, I go with RDS. If I can run on Postgras SQL, fine. If not, then I'll probably ask for other propriety like Oracles or sequels of the world. Simple rule. The best thing with Progress SQL is it again runs on Graviton as an RDS. Tremendous cost savings by the RIS and I'm a happy customer today that AWS has announced the database saving plans. Uh, I was looking forward for a long time, so thanks for that. Operational challenges that we solved with our R&D team. Now everybody wants a self service. Nobody wants to come to IT. They don't want to be in a queue. Your ticket is logged and all, so self-service, go use the platform that we have given to you, execute the jobs, OK? So these are some of the things. Security and governance, tightly integrated Active Director users, you can do that. Cost efficiencies, I have told you, if you simulate it right, if you do your benchmarking right, tremendous cost savings. And the most important thing is the scale up of engineering for my R&D. So with that, I'll hand over to Gautham to give you a walkthrough of the Tyon platform. Hey, first of all, thanks Aender for providing a great insights on running HPC on AWS. Apollo has faced a classic HPC challenge. Their R&D team needed powerful computing resources, but as Shaliner mentioned, managing jobs, monitoring usage, and controlling costs were becoming a bottleneck to the innovation. What we did as Amazonian, we worked backwards from the specific requirement and implemented Tachyon, a partner solution. Customized for Apollo tires, which is a comprehensive management platform that puts control back in the hands of the researchers. And also they give the full control to the administrator for the complete visibility of what clusters they are running and how to optimize costs. Now let me walk you through what makes tachyon game changing. Researchers can now submit and monitor jobs through an intuitive UI. They can request workstation on demand from a managed catalog, and they can access files seamlessly through an integrated file manager. They don't have to reserve IT ticket now to get things done. Now, the admins gets the complete unified observability across all their clusters. And most importantly, they can allocate budgets at the project and user level which can give a better control of the budgets. As a result, Apollo T's R&D team now focuses focuses on innovation instead of infrastructure management, while in admin team, right, they gain the control of the entire control for the infrastructure. Now let me show you how Tachyon cloud native architecture on AWS delivers a comprehensive management while maintaining security, scalability, and seamless integration with the existing infrastructure which Shaliner was talking earlier. At the heart of the Tachyon platform, there's an EKS cluster which is running with 3 nodes, a lightweight yet powerful enough to orchestrate complex HPC workloads on AWS. The Tachyon application is set up in a dedicated VPC in the customer AWS account. The solution uses Amazon OpenSearch services as a database and search engine to store the configuration and transaction information. A lambda is used to trigger scheduled jobs that assimilate all the billing data, and it can run scheduled notification. Tachyon uses a proxy node that runs a proxy service closer to the HP cluster, and this proxy node is created in the target AWS account where HP clusters are running. Users can access the Tachyon web console through a private connectivity via a direct connect or VPN tunnel, and it can be integrated with the existing enterprise AD authentication to make it a secure communication between the on-prem to AWS. I'll give a quick demo of how this Tachyon platform works. So the first thing what users can do, they can do the job submission and tracking. So Tayon has a nice front end from where the users can go ahead, they can create the job. They can provide the account information in the FSXN cluster, which application they want to run, what is the version of that application, the template that is required. Now in the working directory they can specify where all the script scripts will run. In the config section, they can select which queue they have to run based on their CPU memory and the node requirement. They can specify the parameters that what is the total task that is required for running the simulation job, what is the CPU requirement per task, what is the total number of nodes required, what is the tasks per node that is required, the memory configuration data. So this gives a very granular level of. Are details that can be submitted. Now once they submit the job in the bottom if you see right, this gives us right, what will be the cost of running this cluster so that the users can get in advance only that if I run this right, what is the budget I'm going to allocate for this simulation? Once the job has been submitted, they can go ahead, they can see the complete details of the job in the. In the script section they can see what kind of the script had been run by the job which submitted. The observability section provides the complete their CPU and the memory utilization details. They can dive deeper if any, if any issue has come in. They can go ahead from the lock section. They can do the complete deep analysis, and they can see the simulation results and all. They can directly go and they can do it from this intuitive AI. Now the second feature which I can provide, which is a very important one, now they can manage, they can request the workstation from a managed catalog. So whatever the catalogs they have created, the users can go ahead, they can subscribe the workstation, they can specify the memory storage and the CPU requirement what they have. Once they, once they subscribe to the workstation. Now, they have a privilege to share it whether they want to share it with everyone or they want to share this workstation with any specific user. When this workstation is created and running, they get a complete visibility of how it is performing, how the CPU utilization and memory utilization, everything is working, and it also provides the details of monitoring life cycle and other events. Now I'll talk a little bit about tachyon AI. Tachyonic AI is an intelligent solution which is powering the next generation of high performance computing on AWS. Now, it delivers innovation through two powerful components. The first, the physics AI. It contains it has dramatically accelerated the simulation workloads. Researchers can now access pre-configured open source models, and they can also train custom models with fine tuning capabilities. The second, there's the Tachyon AI Assistant, which is powered by Amazon Bedrock and cloud model running at the back end, which allows end users to interact with HP resources using natural language. The users can now track jobs and troubleshoot issues using natural language. They can create their job scripts. They can access documents through conversational Q&A, and they can optimize workloads for the perfect balance of performance and cost. Together these AI capabilities make Tacheon not an management platform but an intelligent partner that accelerates research outcomes while maximizing resource efficiency. A quick demo of how the Tachyon AI assistant works. So the user can launch this tachyon AI system and they can ask the status of any job. What is the current status of this job. Now in the back end, it calls anthropic clot sonic model in Bedrock, and it fetches the information and return it back to the users. Users can go, they can ask about the details of to generate a graph plot. The tachyon can go ahead, the AI model will be. In the back end it will go and it will return it back to us. Also, if there has been any issue, any job has failed, the users can ask that what is the root cause why this job has failed. The AI assistance does the job for them. Now also, if they want to understand that what kind of infrastructure it is running for their HPC workload. The assistant, the AI model, the LLM model in the back end, it goes, faces the results, and it gives back to the users. No, we talked about Chalander talked about how they have done various benchmarking, then how with tack on AI platform we kind of solved the operational challenges with their end users were experienced. These were some of the benefits which Apollos has achieved. Ashender also talked about that the simulation time was reduced by 60% as compared to the on-premise. Now Apollo with HPC on AWS, Apollo has moved from a very high Capex model to a controlled OpE model for running HPC application. The self-service capabilities empowered by Taccheon is allowing R&D team to focus more on innovation and less on the infrastructure management. Now with Tachyon AI and accelerated product development cycle, the Apollo Tires is now doing virtual prototyping instead of physical prototyping, which is helping them to accelerate the development cycle for the tire development. With this, I'll invite Shalander again to talk about the roadmap for the HPC and the challenges and the lessons learned from this engagement. system, no doubt. It was another game changer after the FSX that I mentioned in terms of user adoption, acceptance of the whole solution. The point was ease of use. So that is the most important thing. The whole solution should be easy for the users to understand use and then see their jobs running. Now that we have, as I told you about the journey, we take these projects in bits and pieces, go easy, go small, easy wins. We started with the IT data la, scale it up to 4.0. Simulation was a big thing. HPC is not a small project to work upon, and we did a lot of testing on what to use. Now that we have tested it, it's running fine in production for one of my business units in one of the locations. We want to scale it up. That's the roadmap that we are looking for. Chemical compound simulation is a complex problem to solve. As I told you earlier, when we are designing the product, we want to know which chemical is giving me the best performance, best price performance, longevity of the tire, heat dispense when you are running at a high speed, so all those things. These chemical compound simulations are typically done within the R&D lab. We use historical data. We use a lot of insights from the physical properties of the chemicals, and then one thing is that I do it in the lab physically, which is again a time consuming and costly process. But you have that knowledge base, then you can develop a linear regression kind of a model that what chemicals I can add to create the best compound. So chemical simulation is one area we are looking for HPC to add the value addition. Then complexity reduction I told you about in the genealogy one, that is another area where I want to understand what are the best components which I can glue together to make this black doughnut as some people think. So that is another area that I want to focus upon. If I'm using 200 components, can I reduce it to 190 components and so again it needs good computational power. Global expansion. We have multiple R&D centers, so I want to expand it to multiple R&D centers. I want other R&D centers to also use the same HPC solution that we have built, created, or used in one of the locations, so we want to do that. Gautham spoke about the AI. I also gave you the use cases on my cell service J Lake AI, so we want to explore if some of the things that we are doing today in the manual process. Can I put an agent there which can run the simulation and then suggest which is the best simulation I can use, whether in compound or whether in tire. So those are the future road maps that we are looking for. Um, so that's uh pretty much from my side. I'll, um, uh. Lessons learned, OK. An important one probably. I have learned the lessons, so I forgot to tell you, but then, good to share with you guys what lessons we have learned. Benchmark analysis was one of the key. Deciding factor at one point in time when we were using Windows solution, the cost was high and it became a roadblock for us in terms of The acceptance of the solution. So when we did the right benchmark analysis, we could optimize the cost. This was one thing that we did in terms of the benchmarking and reducing the cost and having a buy of the management. Again, a subset of benchmarking, choose the right incense type. I have already disclosed my golden rule of using the right incense type in the right application. So that's what I do. Elasticity AWS is anyway famous. They started with the whole elasticity concept, so design for elasticity is what we also do. If I'm designing anything, making sure that it is scalable, easy to use, easy to scale, not just scalable, easy to scale. And as I keep repeating, Atham also repeated, Tang platform actually helped us to easily scale it. Otherwise, initially the problem was, how do I scale it? How do I manage the whole complexity of the jobs. Monitor and optimize. If you can't monitor it and optimize it again, the 0.1 benchmark, it will somewhere fall between the cracks. So you have to keep monitoring in terms of the costs. Put the thresholds on your budgets on AWS so that before the bill comes, you should know that you are exceeding the threshold. This is what we do on AWS. Automate everything. I'm sure this will become much easier with so many agents which were also announced today. Earlier today when I was doing the noon session, I also spoke about how we are using multiple agents, where one agent is termed as a judge, which is going to judge my first agent if it is working right or not. So a pretty interesting concept that we are using. In terms of the Asian tic. Planned for scale from the day one, even though we were doing the POC, uh, from the day one, our objective was, can I spin multiple clusters? How will I scale up my FSX? So we nailed down these things on the dry run itself, on the paper itself, and once we were convinced with all these things, um, we actually invested in the POC. So those were the lessons learned from my side and um let me invite Alex. For the closing notes OK, yeah, all right. All right, thank you very much, you know, again for coming here and thank you, Chalander, you know, thank you, uh, Gotham. Chalander, I'm hoping that you're coming back next week, next year, not next week, but so that you can talk more about your progress, you know, what you've been working on, and um before, you know, we close the session, I would really appreciate if you could go ahead and fill the survey, you know, on the session. It's very important for us to understand. We know if we delivered what you were expecting from this session so that we can always improve for next year. Thank you very much for attending and for spending this last hour with us. Thank you.