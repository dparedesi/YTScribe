---
video_id: kSX1utwig9k
video_url: https://www.youtube.com/watch?v=kSX1utwig9k
is_generated: False
is_translatable: True
---

Yeah, good morning. Y'all hear me good? OK, good. Awesome. Good morning. I'm Paul Vincent, um, a principal architect here at AWS. I'm joined here by Laithal Cedoon, another principal architect at AWS. Hi everybody, and we're gonna talk this morning about our unexpected journey for AWS, uh, and MCP servers. So happy you all could join us this morning for the first session of the day. Um, so we're gonna, we're gonna do a couple of things here. So let's go through the agenda. So yeah, oh yeah, here's my clicker. Which one, this one? OK, cool, um. So here's what we'll cover first. We're gonna say, um, uh, a moment of gratitude. So we're gonna talk to you about, you know, thank you for the community, for the support that we've had for MCP. Lace and I run the MCP, uh, open source on AWS. Hopefully you've all used them, uh, thus far. Um, we're also gonna talk about how we got here a little bit, then also, uh, just a little refresher on MCP to get us all in the same context, um, and then, uh, No pun intended, pun intended, um, MCP server tool design choices, why we made some of the choices when we designed our initial MCP repo, how you maybe you can use some of those, um, those techniques, uh, designing your own internal MCP repositories for, for your own usage, uh, within your own companies, and then we're gonna take, uh, a look at the architecture choices that we did. We're gonna do some walkthroughs, some code walkthroughs. We're gonna do a demo of a couple of the MCP servers just to see them working, uh, and then we'll, we'll have a good time with it. Sounds good. All right. All right, so first, yeah, I just wanna say, uh, first of all thank you for coming to our talk. That's a wrap actually we're all done. Thank you. You can go to your next session. I'm just kidding, um, so no, genuinely, uh, wanted to say thank you, uh, really it's the community, the adoption, um, your enthusiasm for those of you that have used the MCP servers that have made this a, a really great project. Um, so we've had, uh, now over 700, uh, 7400 GitHub stars, lots of forks, community contributions, service team contributions from AWS, um, 60 specialized MCP servers across use cases, personas, or role-based, uh, type of servers that you can use in specific scenarios that you need. Uh, and, uh, also a huge thank you to the, uh, AWS community builders and heroes and, and just, uh, uh, our other external advocates for AWS that have written, uh, voluntarily written blogs and articles about how to use these MCP servers, uh, in whatever tool you choose, whether that's Quiro, uh, Klein, or, or really neat coding assistant or agent framework that supports MCP. We, we really wanna thank you a lot. Uh, for your support, each of the downloads of, of these servers we've had over, uh, like several million downloads of MCP servers. This is really any developer that has this, uh, is now able to say. You know, create a full stack, uh, serverless amateur radio logging web application that was our one of our marquee use cases when we first started building this. But actually one of my favorite quotes of what developers are actually saying about the MCP servers I'll pull up in just a moment, but while you're here I have QR codes for the GitHub repo and the articles, uh, external contributors and authors. Please take a look at those. I'll have them up again at the end. Um, but this is what one of our customers, our, uh, one of the developers in the community is saying about the MCP servers, and this is exactly what we set out to do. Um, this is a quote from Robert Weston, uh, from Alpha Data, uh, it really perfectly captures what we set out, why we build these MCP servers to begin with. I said this isn't full AWS automation. It's, it helps you where you actually get stuck. Uh, and the key thing here is this is how cloud dev should feel. Uh, it just knows, uh, what I'm trying to build, how I build it on AWS, uh, and takes away some of the like guesswork of, you know, which service, which IM policy, how do I do this, it, uh, basically builds all of that into your AI coding assistant, um, and, uh. Yeah, this, uh, just really grateful for this quote here because it really validates what we set out to do to begin with and now this project has exploded with a variety of use cases whether that's in your coding assistant or you're building autonomous agents that maybe do things on your AWS uh on behalf, on your behalf on AWS like monitoring your logs or, uh, you know, changing actually changing configurations we had one customer. Uh, use one of the servers to actually do some database administration for their RDS, uh, instances in, uh, a vast multi-account set up, uh, so really, really, uh, uh, excited about what customers are saying, uh, and using our MCP server. So for, so now, um, this is a 400 level talk we did wanna just set the set the stage, uh, set the context like Paul said, um, how we got here and then we're gonna go super deep. Into uh demos and code walkthrough of some of the I would say peculiar parts of uh choices that we made in our MCP servers, uh, that may not be obvious, uh, and really just set them apart, set MCP servers apart from say, uh, API wrappers uh on top of AWS so we'll go into that but for now over to Paul. So how did we get here? So Nathan and I are part of a team. We're, we're, um, prototyping and cloud engineering, so we build prototypes for customers. Uh, we come in and we do a, a quick rapid prototype, and those rapid prototypes would be like 6 weeks, 7 weeks, 8 week type of engagements for us, right? And so we, we decided that we needed to figure out with the age of agentic code development, how can we make our, our, our engagements with our customers go faster and be more productive, right? Because we're finding we're spending a lot of time, uh, during these engagements going. Um, you know, knowledge transfer, look up, you know, trying to look at the APIs, figure out what to do because the code assistants could generate code, but they had no idea how AWS worked and so we thought, well, how can we make it know AWS, right? And so that's where. We come into uh the MCP servers and so the MCP servers allowed us to say, hey, we know CDK really well now let's put that into a server that we can then give to the model so the model then knows CDK really well, right? And so how to shorten that time frame for our um for our prototyping engagements was really the, the reason we started looking into all of this. Um, that's, if, does anybody remember in The Matrix when Neo gets jacked up to the thing in the back of his head and he all of a sudden he knows kung fu, right? That's what we wanted to give our models. We wanted to be able to jack this thing in called MCP and say, oh, now they know CDK and now they know AWS, you know, protocols, and now they know all the various services that AWS. It just the, the general models kind of have a good feeling for, but they don't really know the detail that they needed to have it to be able to actually. Perform, right? To be able to actually build quality code in a short amount of time. So that's what MCP gives us. Um, so here's our model context protocol. Let's just do a real quick overview, right? It's the open standard. Uh, a lot of people like to say it's the USB port for, uh, models, right? It's just plug and play. Um, I like to call it the HDMI port because it's, it's a little bit more, uh, um, descriptive of that because if you think of before HDMI on, on audio connections, you had like, like these red and white plugs you plug in, you had S Video, and you had all the other different things if you wanted to connect just your VCR, right? It depends on what you had on the other side of it. And it just, it was a pain in the butt, right? And so, uh, MCP is the same way because before MCP we couldn't take just any API and give it to the model and the model understand how to talk to that API, right? So MCP does that with, um, allows us to have whatever we want on this side of it, on this side of the model, and in the middle we have this universal connector, whether it's HDMI, USB, whatever you wanna call it, uh, that makes it really, really easy for the model to understand what you're talking to, OK? All right. So we have 60+ uh specialized AWS uh MCP servers in our repository, right? We have them broken up into three major buckets, right? Documentation and knowledge. That's the AWS Knowledge Server. And so if you haven't used that yet, I recommend definitely go check that out when you come out of here. Um, it is an MCP server that connects directly to AWS documentation. So if you've ever had to search through AWS documentation to try to find out how to do something, you probably know that pain, right? It's very painful to go look for things when you want to find things on AWS. The Knowledge server allows the Your, your, your coding assistant to actually just have direct access to all of our documentation really, really efficiently, um, so it understands things like, you know, how do I, how do I make a call to a dynamo table? How do I invoke, you know, this particular service, how do I use location services, all those things are in the knowledge server and really, really, really quickly accessible via the knowledge server. Uh, the second bucket is the workload specialization. So that's MCP servers that are focused on. Services, EKS, you know, uh, serverless functions, so how to actually operate the service, right? So those MCP servers are there to help you and your coding assistant understand how to deal with EKS or how to deal with lambda functions. And then we have our developer specialized buckets and that's where we come into a, a server for CDK. We have a server for terraform. So. Um, I've actually had customers where we we create a, a solution for them and it's in CDK and then they say, oh we forgot to tell you, we, we actually want terraform, right? And so it's as easy as telling that MCP server. Convert this whole CDK stack to a terraform stack and you're done, right? And so it's that simple with MCP, um, because we've given it that kung fu, right? It understands terraform. It understands CDK so it can do that translation very quickly for you using LLMs. We have the pricing, diagramming, and front-end specialty servers, right? And so, You know, how many times have you built something, you build it and then you put it in production, then you find out it costs a million dollars to run. So now you can actually create your code, do your, do your, your development, and you can run the MCP server for pricing and optimization and say how much is this actually gonna cost me to run with various scenarios, right? And it'll model that out using the direct access to the APIs. So the model again understands AWS pricing where You know, that can be a difficult challenge when you're trying to go through something you've written or some, some sort of application you've written, right? Front-end is another uh developer focused one. So, you know, I'm a I'm a back-end coder, you know, I've been coding for, you know, 3 decades. I'm not a front-end developer, but I am now because I have, I have, I know kung fu, I know how to do, um, front-end development now because is the audio working, by the way? OK, great, because my voice is about to go out. I can stop. Thank you. Um, so I can talk softer now. So the, um, yeah, so the front-end server now allows me to be able to write really, really quality front-end code without having to be a front-end developer, which is really cool for me. And here it is. I know AWS, right? And so that's the whole idea behind our MCP services. We wanna give that feeling of just instant expert knowledge to our coding assistants and whatever that might be. Now you might say that's great for AWS services. Well, internally, if you have some like internal knowledge and internal processes and things, you can create your own MCP servers, right? Plug them into your workflow and now they know whatever kung fu you have going on in your own companies, right? That's the idea behind MCP. All right. So we're gonna do a real quick demo. Uh, let's see if I can push the right button. So what I wanna show is, OK, cool. So this is my Quiro instance, uh, so because we only had 60 minutes, we can't really build the whole thing from scratch all the way in the end. So I have a solution already built here that I wanna do some things too. So we talked about amateur radio. So we built an amateur radio, uh, I'm an amateur radio guy, so we built a, a call logging system to be able to log radio contacts for, uh, amateur radio hobby, right? Um. And so I built that and it's a fully egentic workflow, so it's using Agent core. It's got 3 or 4 agents with an orchestrator. It's got a lot of things in here, but I don't really have a good visual of what it looks like, right? So I wanna use my MCP to say, um, How Uh, let's just get a quick diagram. Of this. So Now this is gonna use the MCP server, the diagramming MCP server to actually um go out, take a look at what I've built and then build me a nice graphical representation of that, right? Quick show of hands. Does anybody do amateur radio? Oh, there's a few. I told you we exist. There's dozens of you. Yeah, when I brought this up to Lai, he's like, I don't even know what that is, right? So, um, there's a lot of us, um, so while this is the one that, yeah, so it's building, it's calling our MCP tools. Notice here diagram generate diagram. So that's the MCP tool, uh, that's loaded by the diagram MCP server. Now the cool thing about this is, is that I don't have that MCP server loaded. Right, I have the MCP core server loaded and so we've done some really cool things with our MCP core server and the concept there is if we have all, if we have 60+ MCP servers, let's say you have that you're trying to do within your own organizations, how do you manage which ones to use? And so we've created MCP core which is like. The only one we need when we're doing specific developer functions and so I have a role configured on mine that says solutions architect and that role says bind dynamically when I load the core MCP server a bunch of other MCP server tools based upon the profile that I've set within the configuration for the one MCP server. So that's how we can, we can actually manage. You know, 60 plus MCP servers without having to load them all in, we can do it more profile based and Lace can actually do a deep code walkthrough of how we did that. Uh, let's see. So it looks like my diagram has been saved. Let's just pop it real quick and see what it looks like. Where did I put it? There it is. All right, let's see, we'll go bigger. And let's see, we, so we built this just now on the fly. So that MCP server went out, evaluated my code. And drew this diagram for me and it got it right. So it's got. Uh, so we got some agent systems here, multi-agent systems. This is using core, agent core. We have 3 agents. That's correct, an orchestrator. We have some remote APIs going out to get things like, uh, uh, QRZ.com for call sign lookup and verification. So a lot of good things happening all through, all diagram for us automatically through our magic of MCP. Um, another thing we can do is also, um, let's say I wanted to deploy this, um, how much would this run, would this cost? To run. 10,000 users So that again we'll do the same thing. So I've binded to since my profile says you in core says I'm a solutions architect and I need to know things like pricing and so it binds all those tools from the other MCP servers into this one instance of core and now it's gonna actually call the pricing if I approve it. Let me prove that. So it's gonna price the, it's gonna call the pricing server and actually get all the, it's gonna look at the code first and understand all the things I've built and then it's gonna call all the individual pricing APIs for those and build the full solution for me and then do some modeling on number of users and and that type of thing so we can, uh, I don't know why it's not approving automatically, but we'll, we'll keep saying yes there we go. Um, so yeah, so it's able to go out there and use the API to actually do that and it'll do some scenario-based things as well, right? 10,000 users, 100,000 users, a million users, and so it'll bring that report out for you and kind of give you a good, good feeling for what it's gonna cost to run this solution, the way it's coded today, right? And that also give you the insights to say, well, I, I need to change this area here. I want to change the architecture here because I need to cut cost here. So it gives you that upfront way before you even deployed it, right, which is nice. Who show of hands, who has used the AWS calculator or simple monthly calculator, right? OK, everybody, but basically, yeah, so if you haven't tried this MC the uh pricing MCP server, uh, with your like infrastructure as code stack or maybe something like that, uh, you're in for a treat. You probably never wanna go back to the calculator ever again. Uh, what's just an important call out, you know, with, with, uh, managed services like Lambda and Bedrock, it's a highly variable cost based on, uh, you know, on demand per, uh, per use, uh, pricing model. So we've built in, uh, in the pricing server these prompts and, and tool descriptions that bacon, um, uh, hints about, uh, assumptions, right, like assumptions about your stack so that you can actually bake it and say. 10,000 users that are calling Bedrock maybe 3 times a day with an LLM and this, so it basically uh has some hints there to build out this uh this estimate so really, really uh powerful one. Yeah, so, so you notice it created the report over here and I've asked it to write it out to a file so we can persist that over time and it's working to do that. But if you can see here we got a pretty good right monthly cost to run this if I wanted to run it at scale at 100,000 users you know. Almost $2000 to run an amateur radio. I, I think that's a little outside of my hobby budget, so I'm not gonna run this on my own, but if I wanted to provide a service maybe to other, uh, radio operators that might be something I could think of. All right, so, uh, so use cases, so things we can do, right? So if we take a look at the way we've organized these things, right? So we have that. Um, uh, my slide disappeared. There it is. Explore and plan. So explore and plan is more of that phase of. You know, hey, experimentation, what do I wanna do? Kind of quick rapid, rap rapid prototyping development, right? And so using our MCP service to do that quickly, flywheels down into create, and that's where we, OK, we've got our plan, we've got a, got our user requirements, we've got those things kind of real well defined within our workflow, and now we're gonna use, uh, we're gonna create, uh, Cre's gonna use, uh, our MCP service to get that deep knowledge again, our kung fu, so we can actually build something really, really useful and really valuable. And then we use another set of MCP servers to test and secure. So those are, we have MCP servers for architected, right? We have, well, uh, we have MCP servers for developing user test cases around the code that's being developed. And so. Um, this is the full flywheel effect here. So as you're going through there, then review and deploy. So at the end of that test, we take a look at the test outputs, we review, and then we use, uh, MCP servers for terraform, um, cloud formation or CDK to actually deploy our solution, right? So we deploy that out, uh, to our, to our, our environment and then we have another set of MCP servers that we can use for maintaining operationalizing it. So opera, we can look at security logs, we can look at. Uh, utilization, we can, we can troubleshoot and, and repair any problems that we might have, any unforeseen things that didn't come out of our test. It always happens, right? Nothing's, nothing's perfect, um, and so another set of MCP servers to help us with that phase of the, of the cycle, and it's just a continual cycle. But what we're able to do well this might have taken weeks and weeks and weeks to do now, now we can do this really, really quickly and so we're able to now develop prototypes for our customers in, in a 5 day period. Uh, where it used to take us, you know, 6 to 8 week type period. So really, really good stuff coming out of this and using MCP for that. What? Cool. All right. That's you, right? Here we go. Yeah, so first quick show of hands again, who has not used MCP with an AI coding assistant? Raise your hand. OK, awesome. Oh, there's a, there's a few. OK, uh, so if you haven't this, but if you're using AI coding assistance and you haven't used MCP at all, like any sort of MCP, you're really in for a treat. Try that right after the session, uh, even if it's not one of our MCP servers, if you use like, uh, third party open source ones like Context 7, it's gonna change the way you look at, uh, your AI coding assistants, um. I wanted to now go, you know, deeper into 400 level, some of the choices that we made with our MCP servers, the real design, the meat, uh, behind them, um, so we mentioned from the beginning we have, uh, 60+ MCP servers. Is that too many? Is that too few? Is that just right? Uh, so that's something we are constantly assessing. Uh, but the one thing that we know for sure, these are, uh, emerging standards I would say it's hard to say that there's best practices in the space because it's just now a year old, uh, but we've seen some emerging, uh, trends and, uh, things that we, we view as maybe like standards or soft standards, um. So the first is the multi-server. Uh, I'm sorry, the, uh, separation of servers, um, that's something that we've decided, uh, for now is a great way to, uh, provide, uh, uh, isolation with tools so just to have a user controlled or operator controlled constraint around the tools and context that's provided to your AI assistant or agent at any given time. So if you are, if you know definitively that you are only doing terraform. And um let's say agent core like two things on AWS um then you just need those two MCP servers you don't need all 60 and uh we think that that's a great way to uh convenient way to separate what you need at any given time, control the context that's in uh in your agent at a given time. Uh, we could have built one big monolithic server, um, but we wanted to let developers decompose and recompose the multiple specialized servers. Every project is different, so every customer is different, maybe your Terraform or CDK. Maybe you use serverless or maybe you use Kubernetes and so you're using EKS and you don't care about serverless whatever uh you know, pick your pick your flavor there you can control that definitively with the MCP config uh in your session or your coding assistant or agent. Uh, now we with that having said that, we also wanted to think about multi-server, uh, composability, uh, and we quickly recognized that, uh, when we first started we had 5 and then 10 MCP servers, pricing, diagram documentation, CDK, and, uh, I'm probably missing with core, the core, right, and, um. We, uh, and then that expanded service teams, the AWS service team product team started contributing their MCP servers to the repo, so we realized. Uh, that we needed to think about, think more deeply about the composability and what we saw emerge was, uh, role-based or, or a profile-based, uh, composability. So we took the time to hand, uh, carve groups of servers based on common jobs to be done or role-based, uh, groupings so that if you say solution architect you get a er a set of MCP servers by default and so you just have that one core MCP server in your config. Uh, let's say it's Quiro or whatever you have this MCP. JSON and you just put one core MCP server and control the environment variables. I'll walk through that in a second, um, and it just binds, uh, dynamically binds the ser the other servers that are needed to fit that profile. So we take a little bit of the guesswork, uh, from your plate, and that's another place, by the way, it's open source, so great way to get some, uh, community contributions, uh, get your input if you take a look at core and you say, hey. This might be a good profile here. I always use, or you can tell us I always use these servers together and maybe that's a place where we wanna consider a a a profile. Um That's what we think about this, uh, composibility. The third, uh, tenant sort of ground rule no 1 to 1 API wrappers, um, and so what this means, has anyone done any MCP where they basically put like one MCP tool to one open API method like get something and then there's a second tool like delete something or list something that's sort of thing, yeah, some people know what I'm talking about. Um Then that's kind of what we're seeing is that's becoming a little bit of an anti-pattern agents don't necessarily work that way, um. And uh what happens if you have these very granular tools it takes multiple turns to do something that maybe an agent could have done uh with one tool as a workflow so uh the guidance that we've set is that uh tools should generally a tool should compose more usually more than one API uh and more than even maybe even more than one service, uh, and so we don't want to wrap. APIs as individual tools. So what that means for AWS is that we don't want to have a describe EC2 instance as a tool, right? Uh, we have that covered, by the way. There's a, there's an API MCP server, uh, that covers the breadth of all AWS APIs but with one tool, and how that works is it's more parameter driven. It's, it's, uh, driven by it accepts CLI commands, but it actually executes those. And, so you don't have 15,000 tools, uh, on the API server. You have one tool and it accepts, uh, command lines, and that's very different from, say, wrapping, uh, all the Dynamo DB API methods and calling that the Dynamo MCP server. That's what we don't do, uh, in, in these projects and so we're very careful and methodical about, um, accepting anything that wraps an API method. Uh, 1 to 1, uh, it's very controversial, uh, because it, it's a quick path, it, it's, you know, no brainer, uh, so to speak, right? Like I have these APIs, let me just make them as tools, but we think it's, it's, uh, a great place to think about the surface that your agent is actually gonna work with the, uh, in a new way versus the granular tools that exist in your, uh. In your APIs. Now, maybe your REST APIs are, are, uh, more coarse, and maybe that's fine, but we, we want folks to, uh, think critically about that. Uh, and especially when it comes to all the AWS APIs. OK, so let us go through. The code walk through. OK. So, the first thing I'm gonna do is walk through, I'm gonna walk through two MCP servers on, uh, they are open source on GitHub, so you can view this yourself at any time. If you, if you scan the QR code, you can follow along with me if you have your laptop open. Uh, if not, uh, just follow along here on the screen. Um. I'm gonna walk you through two MCP servers that I think really characterize what we mean by uh the composability, the no wrappers, like all the three tenants that we talked about. Um, the first one is core. And, uh, so first thing I'll walk through just real quick is that um we are using uh fast uh MCP uh from the, uh, this is the fork of and from the anthropic MCP SDK for Python. Uh, and you can see here. Uh, it's fast MCP. So what this does in Python is it lets us, uh, proxy, um, other MCP servers. So we create a single MCP, uh, entry point, if you will, and then bind other MCP servers as and when needed based on the roles. And so that's why you see here as well we have imports for all the other servers as well. Um, so. I will walk through. Uh, so now the, the first place you start when you, uh, when you look at this code, um, and just a reminder for MCP again, this is not a, uh, MCP deep dive, we're deep diving into some of the. Uh, choices we made in our repo, uh, but just keep in mind, you know, MCP, it starts from, uh, as a standard IO. These are all standard IO local MCP servers at the moment. Uh, it's basically started as a child process from your agent. So whether that's Kiro or cursor or what have you, uh, it spawns a process on your local device, and, uh, that's what kickstarts, that's what starts this server.pi. Um And I'm gonna scroll down here. So we have a helper method that just uh imports servers um and then. We checked for the uh uh. The Profiles based on the environment. So when you create an MCP. JSON, uh you will see. Let me see if I can get crack this open real quick. Should be mine should be configured, yeah, yeah. Oh, not in the, uh, yeah, there we go. Uh, it's not in your workspace. It's mine if you flip it over, yeah. Right here. Yeah. Workspace. There we go. Yeah, so you can see here um in the environment variable. Uh, or, or, um, property of the MCP. JSON, we give it an N. ENV and we can provide these key value pairs for the roles, and those are defined by the, uh. Uh, the server here. And uh We check for the presence of those roles. It can be one or more roles, um. We import Uh, we have the default, which is the AWS Foundation. And so basically here this is, this is not super complicated here we just match those conditions based on the presence of those profiles and what we do is we import, we use our helper function to call the import server which basically binds a proxy. Uh, like this, so we have MCP import MCP server local proxy. It adds a prefix to the tools so that you know, uh, that the tool itself came from, uh, the core server plus the binded proxy server. So we add this prefix, so the tool name contains the core and the uh and the proxied MCP server name. And uh that's pretty much it. So after all that with the setup, we conditionally bind and proxy those servers. And once that's all set up. Um, you know, here we have again the security identity. Databases, no SQL, time series. Messaging and events. So again, we have this helper method. And then we get to Maine, and then we start the instance of MCP, right? So we start this, uh, let me scroll up here. We start this instance of MCP. Up here at the top, right? And so, but before we actually get to Maine, where is uh the entry point of this, of the server.pi, that's where we're binding all the tools and servers first and then start the server with MCP.run over here at the end. Just come back here one more time. And that's it. So now Uh, the core server logic is actually very simple, just conditional binding based on those, uh, key value pairs in your, uh, dot or your ENV file or sorry, ENV property in the MCP, uh, do. JSON and. It imports the existing MCP servers and the tools that it has for you, uh, and so you get all the properties of those other MCP servers they all run locally. um, the exception right now is the Knowledge MCP server runs remotely, uh, but for the ones that do need AWS authentication, those MCP servers will use your current. Uh, you can either set a profile, AWS profile, or basically the, the, uh, credential chain that VO 3 follows, which usually starts with either environment variables, uh, your AWS credentials file, or config file, it kind of cascades down so it uses your current active session that way. What we do. We set up a profile uh that we know we want to use for the MCP server so we can control that and we can say this MCP server is using a profile for maybe this AW a different AWS account or maybe has a different IAM role and policies and so we use profiles quite prolifically um when we're starting these up so as not to, so as not to mix up what we want in our current terminal. Active session with what we want on the MCP server so we control that through profiles. Highly recommend that approach. Yeah, quick, quick example of that would be, so we have, um, excuse me. We have uh Nova Canvas as one of our MCP server tools, and so you may not want your developers to have Nova Canvas access within their uh test account and so you might have a centralized account that you've you've you've approved to use inference of Nova Canvas in, and they would use that configuration for that profile that gives them a role-based access into that. So then as they're generating images and things for their application, it's gonna use that profile against that account and not their account that they're using to. Awesome. And so the second one I wanna walk through again, I think this one really distinguishes itself for its sort of macro capabilities that span across services, but they're all related to billing and cost management or FinOs, right? So I want you to picture maybe the last time you had to investigate a cost, uh, a cost anomaly on your bis account. Picture that. Um, or a picture maybe now you have multiple, uh, you have organization, multiple accounts, cost allocation tags, you know, maybe you wanna factor in, uh, cost optimization things like savings plans and spot instances, uh, and, and the multiple services that that requires, right? So now we're talking about, uh, roughly a dozen actually if you look under the covers roughly a dozen. Uh, services, uh, just around cost, uh, efficiency, cost explorer, cost and usage, um, and so this is really the billing and cost management server or BCM is, uh, basically the entire FinOps profile, uh, into one MCP server and why I think that's really, uh, really stands out. Uh, is, uh, that, that compos composition, uh, for, for a job to be done and it doesn't wrap, it does not wrap multiple, uh, uh, uh sorry, it does not wrap APIs 1 to 1, it provides higher order tools, um. So The other thing with billing and cost management, you have to deal with pagination a lot and so you don't necessarily want to rewrite pagination handling uh over and over again. Um, On, on your servers for different API calls. So if I come to. The Us. Yeah, same thing here like let me just stop right here, um, you know, handling errors, right? agents benefit from much more descriptive error messages and so, uh, having this centralized, uh, billing server lets us kind of conceptually you can think of the mental model is much more, uh, much more tractable because you can, you can craft these error messages. Based on an agent that is acting as a FinOps person, uh, a Finops employee in your company, so you can actually. Uh, add these custom error messages versus, you know, 500 or, you know, some other, uh, error message from AWS that might not necessarily help an agent, uh, do the fin ops job. So this is a place where. Uh You know, Consolidating a server as macro tools and and jobs to be done can. You know, help you with your mental model as well. So we're, we're showing you this not just because we want you to maybe use the server, uh, but we also hope that you, uh, you can see from some of the patterns here how to architect your own. Uh, another one, another reason this server stands out, um. Is that If you again, if you've used the cost and usage reporting and cost explorer APIs, uh, it has large responses, pagination, and that sort of thing, uh, if you use the API directly. Many, many folks just use the console and you know you have a much more pleasant experience that way, uh, because it's all built in. But if you're building this yourself with the APIs, you have to deal with a lot of stuff pagination, um, analytics, uh, what we do in the server is actually build in a small in memory SQL light table or tables so cost, uh, cost and usage data gets inserted or, or, um, loaded into the server into your instance of the server at runtime. And uh this also makes it really easy for the agent, uh, an AI agent to actually understand the billing. It doesn't have to understand, uh, the way you have to query cost and usage reporting on AWS. It just needs to understand SQL and we know now over maybe 2 years already Texas SQL actually works pretty well and so actually the main interface for billing and cost management is through SQL to uh to the tools that we've developed here. And I'll, I'll show you that here right now. So We have this uh unified SQL Server, uh, basically querying a, a session database that is, uh, loaded. Uh, persistent session database. And Here we go. Just go over to that guy. Again, all open source so you can take a look at all this on in your own, in your own world as well and kind of dissect it and kind of see how you can use it to either extend it or fork it, you know, for your own internal usage for sure. Insert data. There we go. So we have some, some helpers here as well for. Uh, validating the SQL queries that the agent writes, and then finally we have the execute query. Uh, this is the function that's actually put behind the tool calls, uh, that the agent uses. So the agent would see a tool. Like, um. Like this, this is generally how the fast MCP framework works. You can just put this decorator um of a tool and it will turn that into a tool that's available on the MCP server, uh, that's, uh, shared once you start it, um. Once you put this wrapper, or sorry, this uh decorator, which in effect wraps this function, now the agent will see this as a tool, execute the SQL query, um, optional column definitions, um. And uh At this point, the database has already been loaded with data from your cost and usage. And uh now your agents just interacting through the, through a local database versus calling APIs uh to AWS over and over. Um, another thing, uh actually we first built the server for our customer um. They, they said, well, look, we, we. Have a lot of people and some people that aren't necessarily fin ops uh dedicated to FinOs, but they have to do all of this cost and usage, uh, optimization all the time. Actually our biggest pain point was S3 object, uh, uh, storage and just figuring out how much are we spending on a given prefix in a bucket and an account. And so for those of you that haven't used that or have to had to do that before, what you have to do actually is use, uh, storage lens, uh, to actually get. Very fine grain, uh, details about your S3 storage costs like at the prefix and bucket level, uh, and that also gives you more insight into the, uh, storage class and, and life cycles, uh, life cycle rules that you have on a bucket and so this is actually the inception we said. Great, we can do cost explorer, we can do optim, you know, cost optimization, RDS RI reserved instances, saving plans, uh, but what about storage? Um, and so that's where, um, actually the first use case we, we decided we need to make this into one FinOs MCP server. We've kind of been noodle on noodling on it, but then when we had a customer tell us we wanted it, that really like motivated us to, to get started and so this one. Again, spanning multiple services, S3 storage lens actually puts data in um. Uh, like parquet S3 buckets or redshift, you can, you can set a few different destinations, um, but it doesn't actually have a, a data plane API by itself, right? It has a control plane API, uh, which means, you know, configure storage lens and start collecting the data from, from the S3 buckets, but it doesn't have its own data plane API to actually query. You have to then. Get the data from an S3 bucket or a redshift uh data warehouse and that sort of thing uh so you can use Athena, but we, we decided no. We just, the agent needs to be able to get, you know, make sure that S3 storage lens is enabled and then access the storage lens data and so this uh storage, the storage lens tools on the BCM actually. Uh, go through that whole workflow of validating that there's a manifest for the storage lens, uh, read the manifest file so it understands, you know, bucket names and, uh, object names and, uh, keys and that sort of thing. And We get then from the manifest we get the locations of the actual data files. So these, by the way, are just internal functions at this point. These are just functions that we use in the tool itself. We have the different handling for CSV and Parque and S3. Right now this only supports the uh the S3 target and then we have an Athena handler. So again, multiple services, but we didn't, we wanted to take out all the guesswork and, and, uh, set up with cross-service uh integration. So you just have this storage lens tool. It has Athena handler. To actually create the tables against the storage lens, uh, data uh data file locations based on the manifest and, uh, we get to. Yeah, and at this point, these functions here. Um, let me execute query, yes, right here. Uh, finally we get to this. This is actually, uh, a, uh, a method on this class of the Athena handler that this is actually presented as a tool. So this particular one we're not wrapping with a decorator, uh, but we actually just bind it out as a tool later on at the startup, uh, but this. By the time the agent actually sees this tool and calls it, it will have validated the data of the manifest, the storage lens, the location, creating the S3 data, uh, excuse me, the Athena table against that, and now it has access to real storage lens data and can even uh create recommendations like, hey, this uh uh. This bucket over here with this prefix actually has 200 terabytes that really get fine-grained details versus just saying, hey, this account is using this much S3 or even just one bucket is using this much. It really gets down to the details so you can get really fine grained information about where your spend is. On uh On an S3 bucket or your S3 infrastructure on AWS. So OK. I'll take us out here, um. Thank you. So, um, why did we, why did we set out to, to do this presentation to begin with? I think, you know, it's open source. A lot of people have already written about it, uh, and used it, hopefully most of you have used it maybe even already in, uh, in your coding agent of choice, um. But one, we wanted to share, uh, our experience actually building them. So it's, it's not like, hey, come and check out these servers, we really wanted to share with you what we've learned, how we built them, um, and maybe some, give you some tips if you're thinking about building your own set of MCP servers, uh, for your use cases in your company. In fact, uh, I had a, uh, an ISP customer, uh, I spoke with a few months ago and they said, we, we modeled our MCP server. Uh, repository based on, on what you guys did and that was like really, really, really flattering. It was, it was an honor, really. Um, so a few things I wanted to point out. Uh, one of the choices we made was a mono repo, uh, so that. You know, instead of having 60 GitHub repos, uh, we wanted to bring them all in one place, and that gives us, uh, a lot of leverage, right? It gives us consistent CICD, consistent, uh, code quality. Actually our code quality consistently increases. We won't accept, uh, PR if it lowers our, uh, lowers our code quality and test coverage. For example, we have consistent static security, uh, scanning. Uh, style and, uh, so also just discoverability. I mean, AWS is a very large company. Uh, it's no secret and so, uh, this actually helped us as well as customers find, you know, all the servers in one place so we don't have to reinvent wheels, um. The other thing we did was, uh, shared templates. So because it's a mono repo and it's all written in Python, uh, actually the code, there's a lot shared, a lot of code shared for these servers, uh, but we wanted to ship them as individual modules, right? So it's a mono repo, but it doesn't ship one module. We actually ship each server as its own Pi P package, um, so we had to use some templating to share code. Uh, even though there's so in a, in a classical sense, if this wasn't a mono repo, it would be a monolith. We'd have one package, um, and then all the code that was common would just be all in one place, but because we had a mono repo, we had templates that we built with cookie cutter, uh, an open source templating framework for Python. Uh, used for that and um. So just some, some highlights how we built that. Um, and so I'll take us out here, really, um, and we'll be available for, uh, for questions just uh uh outside of the, the speaking area here. Um, So we've, main thing, especially for coding assistance now these servers can be used anywhere that you can use MCP that like Paul mentioned from the beginning, uh, that's one of the virtues of MCP. You can use it wherever it's supported. Uh, we really wanted when we set out to do this at the beginning, it was create context aware coding that actually understands AWS patterns, best practices, and anti-patterns, uh. You know, and us not having to say over and over again like use Dynamo DB or use API gateway this way, you know, that sort of thing, we, we just wanted it all there, um. The cross-service cross API really takes out the guesswork for the complex workflows that you may have across the AWS uh portfolio. Uh, really implementing solutions in your MCP servers, not just, uh, APIs, uh, create workflows really that the highest level workflow that you can, uh, that you can conceptually wrap as a single tool is generally the, the pattern you wanna follow there. Uh, through this we've, um, accelerated our own capabilities. We've really, uh, at Amazon we call it dog fooding, which means we, we, uh, eat our own dog food, so to speak. We've used this in our prototyping practice and our professional services practice, uh, daily. Uh, there's not a day that goes by without me using one of the MCP servers. Uh, it's accelerated our prototyping and our build capabilities from weeks to hours compressed, um. And The uh um I wanna go back to that quote. This really has uh lowered the learning curve for AWS. That was one of the things that was really, really important to us. Lower the learning curve for AWS so that just feels like, you know, an extension of your code, whatever code that your, your actual business logic. So you use these AWS MCP servers. It brings in all of that context for infrastructures code, IM policies, cost optimization, all of those things, um, so you can really focus on your, on your, uh. Differentiators Uh And Faster time to value. Means that we're not having to context switch between documentation and uh and your code. So all of that context just gets brought into your coding assistant as and when needed. Um So again I'll just say thank you. Um, you can get involved. It is open source. Take a picture, yes, thank you. I see some phones out. Awesome. While you have your phones, uh, please, please, please take 2 minutes to do the, uh, the post session survey. It really, really is important to us. Your, your feedback is a gift. Um, if you wanna see more of me and Paul, or if you don't like our voices, let us know, um, but if you wanna see us, the, the feedback matters a lot. Um, we greatly, greatly appreciate that. So we baked in a few minutes here just for you. Um, you should see it on, unfortunately we don't have QR codes for the, like we used to, but you'll just go to your AWS events app and you can put in your feedback there, um. Yeah, check out the articles and resources if you wanna learn more about what other people are saying about this. Uh, I think that's kind of the greatest, uh, privilege that we have is what, uh, what other folks are actually saying about this and using it in the real world. And contribute. It's open. It's Apache too. Yes, there's a lot of AWS service teams contributing, including and and AWS, uh, field teams like us, uh, but we also have community contributions as well. Uh, a great example like I mentioned earlier, think about the core MCP server. Maybe you have a certain set of personas, um, or maybe you just wanna look at the code and get inspiration for your own, uh, use cases, please. You know, steal, uh, steal liberally from that. It's a, it's a patchy too. There's no, uh, there's no restrictions basically on that. So with that, uh, thank you very much. Thank you. I appreciate you and go give your, uh, go give your solutions kung fu, yes.