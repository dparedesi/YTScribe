---
video_id: VBwX-w8wnMc
video_url: https://www.youtube.com/watch?v=VBwX-w8wnMc
is_generated: False
is_translatable: True
summary: "This session, \"Scaling Data Transformation w/ dbt & Nasdaq: Unlocking Ent Power Global Mkts,\" features Michael Weiss from Nasdaq discussing how the company leverages dbt (Data Build Tool) within its \"Eclipse Intelligence Platform.\" The platform integrates data from Nasdaq's markets, clearing, and custody services, as well as third-party and customer data, to provide analytics, reporting, and billing solutions. Weiss explains that Nasdaq uses dbt Cloud (Enterprise) to manage data models, enforce standards, and enable multi-tenancy for their global client base. A key focus is on creating a \"hybrid tenant\" architecture where Nasdaq provides baseline data models and contracts, while customers can extend these with their own data in their own isolated environments using dbt Mesh. The session highlights the importance of the \"semantic layer\" (context) for enabling accurate AI and agentic use cases, ensuring that digital workers interact with data correctly. The ultimate vision includes a \"data model marketplace\" where customers can monetize their own data models."
keywords: Nasdaq, dbt, Data Transformation, Eclipse Intelligence Platform, Financial Technology, Data Modeling, Semantic Layer, Multi-tenancy, AI Agents, Data Governance
---

Thank you, everyone. My name is Michael Weiss. I am a product strategy for the NASDAQ Eclipse Intelligence Platform, and I'm here today to talk a little bit about how we have used DBT to enable our own markets but also enable markets on a global level. So I'm gonna start with a little bit of an agenda. Hopefully this is not gonna run too long though. I know happy hour is around the corner and everyone's more excited about that than to hear me talk. But I figured we'd start today with a little bit of an introduction to who NASDAQ is, just so we all know what we're talking about. So, when we talk about NASDAQ, you can really think about NASDAQ in 3 main pillars. First and foremost, you can think about our market services pillar, and this is the one that most people know NASDAQ for. These are the markets that NASDAQ owns and operates in both North America and in Europe. This includes the biggest trading market in the US by trading volume. Second to that is our capital access platforms. And this is the part of the company that's responsible for primarily our listings business, in which we list close to 3500 companies today. Another big part of that is our index services in that area. And if you've ever heard of the, the, the queues or anything like that, that's a big part of our portfolio. And the final pillar is our financial technology arm. And this is the one that most people probably don't think about NASDAQ in. NASDAQ services close to 4000 customers globally for various solutions to, to support their needs in a financial market. Let's drill into financial technology a little bit, just so you understand the breadth of the services NASDAQ can offer to our clients. We cover everything from financial crime management with our Verifin products to regulatory technology with AxiomSL for regulatory reporting, and surveillance to ensure compliance on the markets. And finally we have our capital markets technology, which includes both Calypso and NASDAQ Eclipse. These cover the full trading life cycle for both banks and markets. So let's just spend one more minute talking about what is NASDAQ Eclipse and the products that make up that area, and then we'll get into the product that I'm here to talk about a bit today, and then where DVD plays a role there. Eclipse includes 4 main 4 main technology areas. First is the trading system. This is the one that markets are, markets will deploy to do execute trades on. We have clearing, which is supporting the clearing needs of our customers, and then custody is kind of the final one in that main trading life cycle. And then the area I'm here to talk to you about today is our newer product, the Intelligence Platform. And the intelligence platform is responsible for collecting all this information and enabling our customers to leverage that to enable new use cases, new products, and new services to sell to their customers. So let's dive a little bit into the eclipse intelligence platform so you understand what it is. And when we think about the eclipse intelligence platform, there are really 4 main areas we kind of think about from a platform point of view. Foundational to everything is data management, and this is everything from how we acquire data from our own products to other third parties, how we store that information, how we ensure governance on that data, and how we allow, and how we keep an open ecosystem to allow a variety of different use cases and services to communicate with that data. Right, so everything from. You know, other data transformation services to AI and agentic use cases. The next main pillar we think about are the analytics, the insights you need to glean from your data to operate your markets or operate your business. And here we have a couple of things we think about, and this is the area DBT will primarily fall into. But really at the end of the day, what we're looking to service with the analytics side is the business. We're trying to give them access to as much information in the most intuitive way possible. Traditionally that's been done with more dashboards and reporting, but obviously in today's world we're looking at more agentic use cases and digital worker type use cases as well. The third pillar is around our reporting, right? Despite everyone's best efforts, sending CSV files and PDFs is still a big part of what we have to do. And so we have a service that allows them, our customers to automate that and reduce friction with their customers by providing a self-service portal and capability to create reports without needing technology or engineers to really get into it. And the final one that probably doesn't seem to fit into the rest of the other three categories is our managed billing solution, right? And this focuses on how our customers need to bill their transaction or their non-transactional related charges to their customers. When we talk about the eclipse intelligence platform, this is kind of, I won't call it an architecture diagram, but it can give you an idea about how the components are laid out. Going back to uh capital market technology, first, the first thing we're looking to do is integrate natively with other NASDAQ products and services. When a customer buys the intelligence platform, we don't want them to have to think about how they're getting data from other NASDAQ services. We take care of that for them. And that's everything there on the left-hand side. In addition to our own data that we're and from NASDAQ, we allow our customers to bring their own data into the platform so they can be infused and sit alongside the data that we're already managing. In those gray boxes, you'll see we have kind of 4 main things here. Uh, the ingestion engine really runs there, and that's the one that's pivotal for the integration with all the other products and services. Uh, the cloud data leak, right? I think everyone probably has an understanding what the data leak is, but that's the primary retention and storage for the information. We provide a variety of different query and access endpoints that our customers can use to integrate with their own services if they wanna bring their own applications in the ecosystem. And then finally, there's the contextual understanding and knowledge about the data, and that is what the data catalog services there. And then on the far right in the green, you can really think about that as the business products and applications that sit on top of all that other stuff. Right? So when I mentioned, uh, analytics, that's the insights HQ product. That's focused on your more traditional dashboard use cases, the ability to create, publish, and share dashboards both internally and externally. The report HQ product, which focuses on reporting, and then revenue HQ which is that billing component I mentioned. And as I, as I said, these can be exposed to both the internal customers who are buying the product from us, intelligence, or they can use it to provide access to their customers and reduce their friction points there and make sure their customers have the information they need to make decisions. So, let's get to the, the, what we're really here to talk about, the role of DBT in this platform, in this ecosystem, because DBT plays a major part in what we do. So, before we get into what they do, I think it's important we lay out a couple of our goals that we had going into leveraging DBT. And in a lot of ways, these are the same goals we had starting back in 2021 when all of this was focused on only NASDAQ's use cases, right? We understood we needed to integrate with existing products and services for our internal customers to be able to make that information usable. We need to be able to provide better business models and analytical models to our, to our end customers so they can actually parse and understand what is going on in their business. On top of that, we wanted to allow our internal users, and now our external users, the ability to extend and participate in the model creation, right? A lot of time those data models and historical ETL processes were locked behind engineering resources. We wanted to lower the bar of entry so that more teams could collaborate and participate in the process, and DBT really unlocked that for us internally and is now looking, allowing us to unlock that externally. And then the final part of this is to provide an access point that can enforce enforce standards and contracts to both users and AI. In today's world, data without the context is a little bit limiting, right? When you're going to turn on an agent, you want to know it's talking to the data in the right way, and that's really a key driver, uh, uh, in what we're looking to do with DBT as well. So why are we doing this? What is the strategic vision for us and, and what, why did we decide to even do the intelligence platform and why DBT, right? And it really comes down to a couple of target outcomes we were looking to get out of this. We believe that because we do all this for ourselves, we can provide faster innovation to our customers by taking all the burden of managing the data platform and building baseline data models off their plate. Let them focus on where their real differentiators are, and let us do more of the heavy lifting for you. The other thing to think about is we want to start thinking about the financial fabric of the globe more unified, right? So what we're looking to start doing is building standard understandings and semantics on that information. So whether you're operating in the US or Europe or Asia or Latin America, we're all speaking the same structure, the same ontology, the same semantics, right? And that'll be a big unlock for a more globally based economy, obviously, because now we all can share and, and provide information to each other. And that also then leads to the globally sourced model definitions, which is essentially a giant intersourcing exercise. The ability for our customers to not only use our models, but to contribute back to those models and share it with their, their, uh, partners and their uh uh their users in a more seamless fashion. So, how do we, how are we really thinking about this from a DVT integration and product point of view from an architecture? And I, and I apologize this is high level, but this just gives you an idea of how we lay this out. The way we deploy the intelligence platform is actually is what we call in a hybrid tenant approach. And not many people will probably know what that means, but really what we end up, what we have here is we have separated and segregated services based on shared components, which are all multi-tenant. And then single tenant components, which is where the customer's data resides. And we do that for a couple of reasons, locality laws being the biggest part of that, and then of course, security and all that fun stuff. For DBT we also take a similar approach. We have a main DBT account, and that's where we, that's where we, NASDAQ will do most of our DBT development, where we're gonna test our models, and then we're gonna publish those models out. And then for customers who are interested in collaborating on DBT models, we will provide them their own DBT account, pre-populate it with. A whole host of different things they don't have to think about. Everything from how code is, is sourced in GitLab to, uh, CICD processes to test things like test coverage and coding standards, all the way to how they can depend on NASDAQ data models within DBT to extend them and add their own information to that. So what does this look like from a DVT project structure point of view? And it's, it's interesting how we thought about this, right? So everything on the left in the blue is what NASDAQ is gonna do and provide out of the box. Obviously, first and foremost, we're always gonna integrate with our own products and services, and we touched on that. But you'll also notice we have a NASDAQ mapped data layer there at the contract section. And you might be asking why we would have that. And there's a couple of reasons we think about that. One, that is the lowest level of the data we want to define in a, in business terms. So think about defining transactional data in a more business-friendly manner. But two, we want to provide the flexibility and capability to support non-NASAQ entities and services to plug into the ecosystem as well. And by having that contract layer. Any other tool or service that is in the same domain as any of the products we already service can instantly plug in and map their own data to that. Once we get into the contract layer, that's where we start talking about the NASDAQ standard analytical models and insights models that we automatically give every customer within the domains they're, they're interested in. But that's not always enough. We can probably cover 80% of what people need. We understand that there's gonna be things our customers want to do that are custom to themselves. And that's where the ability for them to provide their own information in the platform, then create their own contracts and their own models or extend our models is really critical. Right? Because if, if we can't cover all use cases, we don't want to be the bottleneck to them unlocking true value and capability on top of their data. And then kind of the final interesting piece of this is the semantic layer. And. I'm not gonna, semantic layer is kind of a, a weird term because I think the term semantic is, is not. Not descriptive of what we're trying to accomplish, right? When we think about semantic, what we really mean is context. What we're looking to do is provide a layer of contextual knowledge about the data that whether it's a traditional BI use case or an an agentic use case, they can understand the data they're talking to and know they're using it in the right way. On top of that, we want to be able to enforce certain standards on the data they're, they're pulling from, right? So for example, if you have a calculation, and you only want them to take an average, if you just go to the traditional data store, you have no control over enforcement, right? They're up to do with whatever they want. With the semantic layer, we can enforce those, those type of capabilities, so they're only gonna be able to leverage data in the way that makes the most sense. So, Why did we pick DBT and why are we using DBT Enterprise? Everything I just talked about, we could probably do with open-source DBT, right? That's a great, that's a great open source project, and I think anyone who's used it would agree. So why did we go with DBT Enterprise? Well, first and foremost, they gave us the easiest path to multi-tenancy versus building our own. Second, the DBT meshing the ability to depend on projects in the DBT enterprise ecosystem is critical for us to offer that extensibility to our customers. The semantic layer, the ability to publish those rules and enforce those rules from an access point is also is also become more critical to our overall offering. Uh, governance and lineage and visibility, right? We want to make sure the right users have access to the right things, they understand how data's derived, and they know what's going on. And then the final thing is DBT Enterprise has a lot of great tooling around continuous delivery and standard enforcement. One final thing I'll mention about DBT is just the general development experience. DBT Enterprise, if you haven't used it, has a great interface for creating DBT models. Whether you're a more technical user and you wanna get into like the SQL aspects of it, or with DBT Enterprise now having something like Canvas where you can do more drag and drop, you can kind of enable a multitude of different users, both internally for NASDAQ, but for our customers to participate in the management of data contracts and the data life cycle. So, how are we thinking about rolling this out, right? It's a lot to kind of think about. When you're talking about deploying something at a global scale, there are a lot of moving pieces to it. One, how do you ensure you're doing it the right way? How are you doing it in a secure way, but also how are you rebasing on yourself to make sure you're direction going the right way. And the way we thought about this is, first, let's focus on the area we know the best, and that is defining the common ontology and semantics around the data we know. Because if we get those contracts right, those data models right, we can then move to the next phase, which is allowing the clients to offer that extensibility. Once you get to the extensibility aspect of it, now we want to talk about how we expose that information to AI and engenic use cases, right? How are you gonna let digital workers talk that data and make sure they're not going off the rails and doing the right thing. And then the final phase, which is one of our goals that we're looking to get to, is kind of a, a data model marketplace, right? When we talk about data and monetizing data, we always talk about the data itself. We never talk about the data models that you could potentially sell. If we all have a common definition of. Understanding of a of a baseline, customers now have the opportunity to create their own constructs and repackage and sell them to other consumers on the intelligence platform on the global scale. Right? So it's a new monetization methodology that previously wouldn't have been possible without something like DDT. So, just to, to wrap this up a little bit, um, what, what are some of the value drivers we're thinking about from our client point of view? What are we, what are we trying to measure against? How do we know we're being successful as we kind of think about this over the next 12 to 18 months? First and foremost, do our customers have easy access to data? Do they understand that data? Are they making better decisions because of that data? That's obviously the easiest one we could think about because that's a universal thing that's been around for 20 years at this point. The second is, have we made it easier to engage and use agentic use cases, right? Is the work we're doing allowing our customers to move faster in in AI? The third one is, are we enabling, uh, new customer, uh, data access services? Are we giving them the ability to interact with data in new and interesting ways that they otherwise could not be able to do before? We also want to measure how many of our models are actually reusable, right? We don't wanna be in the business of creating custom one-off models. We want to create models that we know every customer can benefit off, so we wanna make sure we're measuring the usability across the spectrum here. And then, you know, one of the other ones I'll I'll point out is that, is that no code experience. Are we enabling? Non-technical people to really engage with the data in ways they haven't been able to before, both in the data modeling side, again with something like Canvas and DBT Enterprise, but also in the digital work and agentic side. Is the data purpose built for their use case? Are they getting the right answers? Because if not, none of this matters, right? We need to make sure that they can trust what they're doing. So, with that being said, uh, I finished 3 minutes earlier so everyone can go get a beer or a cocktail. Uh, if you have any questions, feel free to, to ask, but I appreciate everyone coming out today and I hope you enjoy the rest of Reinvent.