---
video_id: zZXeASq9Trs
video_url: https://www.youtube.com/watch?v=zZXeASq9Trs
is_generated: False
is_translatable: True
---

Good morning folks. Uh, humble request from our side. If you can wear your headsets and give us a thumbs up, we'd be good to go. Awesome. Thank you so much. All right, um, Welcome everyone. Welcome to COP 333 scaling Open Source observability stack featuring Warner Brothers Discovery. My name is Vikram Megaraman. I'm a solutions architect here at AWS. I work with some of our strategic customers that operate AWS at large scale. Along with me, I have Abhina, product manager, AWS, and Hans Robert, director, Warner Brothers Discovery. Let's get to the point straight. Imagine it's the night of the season finale of your favorite OTT show, and you, along with millions of users are waiting to watch this show, pressing this play button simultaneously, only to find out there is this annoying revolving circle that just rotates and never stops. And on the other hand, the provider goes like, uh, thinking like what's, what's wrong with my platform? Is it the authentication system? Is it the payment system, or is it the both, right? Unfortunately, folks, that's the state of running systems at large scale. When you have hundreds of thousands of microservices running on a various compute platform, the challenge isn't about collecting metrics locks traces. It's all about making insights on top of these metric lock traces, and that's exactly where we feel modern observability can help us rather than just being a mere monitoring tool. It can help you identify where the problem is, what was the root cause, and how you can fix it. And that's going to be the focus of today's session. In minutes' time you'll find Hans here sharing the stage with us, who's going to talk about how Warner Brothers' discovery derived insights out of these metric lock spaces stored at large scale and we're able to bring down the meantime to identify and remediate operational issues. As far as the agenda, folks, uh, we'll start with observability evolution where we try to understand the basic blocks of observability which is metrics, locks, traces, and then we'll dive deep into uh some of our managed open source services, um, that can help you store these signals in a reliable and a durable way and then we'll hand it over to Hans to talk about Warner Brothers Discovery's journey, observability journey to AWS, and then we'll wrap up the session with best practices of operating these observability tools at scale. Sounds good to you all. Quick show of hands, how many of you here operate 50+ monitoring dashboards in your environment? Keep your hands raised if you have 100 + dashboards, 500. 1000 Wow, yeah, that's the challenge when you have like too many dashboards, correlating the insights out of these dashboards isn't gonna be that easy, right? Um, we'll, we'll try to, like, you know, we'll hopefully we can learn something from this session, be able to apply some of those best practices in your environment. Now I really wanted to emphasize on on the importance of having a solid observability stack, folks. Look at these examples e-commerce platform, healthcare providers, commercial banks. They all suffered a huge financial loss just because they were flying blind. They didn't have a good observability stack, and it took them hours for them to identify where the problem was and fix the issue. Right? As our CTO says, everything fails all the time, but if you engineer your applications for those failures, meaning if you have the right KPIs in place, have the ability to correlate these telemetry signals, that's gonna put you in a peaceful spot. That said, let, let's understand the, the fundamental blocks of observability, which is metrics, locks, traces. Logs in a simple term is nothing but how a human would record day to day stuff right in its daily. Like it has a time stamp in it. It has a detailed description of the event. That's pretty much it. Metrics on the other hand, gives you the ability to look at uh an application's pulse. Is my application running fast? Is it slow? How many 4 XX errors? How many 5 XX errors, right? It's gonna help you dive deep into a specific component within your application. And traces it gives you the ability to capture your end user's request as it flows through the complex microservices architecture right from let's say the user orders something on uh on an e-commerce website to getting that product delivered to him, right? It captures everything. But this talk is not going to be about collecting metrics, logs, and traces, no. It's about how you can reduce mean time to identify and resolve any operation issues, right? So we thought about it looking from a lens of an on-call engineer, right? Any on-call engineers, SREs in house. There you go. So from, from, from your perspective, right, like you wake up one early morning and find out like there are thousands of alerts waiting to be attended, and in order to get to the root cause of the problem you may have to go through like let's say 50,000 times series of metrics, 2 petabytes of logs, and remember you're already racing against the time. When a when a 5 minute fix takes 5 hours for you to troubleshoot, we aren't talking about a monitoring crisis or a monitoring challenge. The challenge is with correlating all these telemetry signals, right? So we thought about it and we came up with this layered approach where the first layer is all about. Start with something, right? Like use the out of the box monitoring, you know, you get a page alert, you get a down alert, you get a 100% CPU utilization alert. So that's gonna be your starting point. It's gonna tell, OK, a particular application is impacted, and then dive deeper into the second layer which is, you know, get to the logs of that specific application and see like what's happening. Maybe you're getting a lot of 5 XX errors, right? So let's dive deep into the logs and understand what, what could be the probable root cause of the 5XX. Then comes the third layer right where you enhance this information with trace information. Oftentimes it's not that specific application that is a root cause, right? Like, let's say you get a down alert. It's not maybe related to that application, but it might be related to our dependent services, right? And that's exactly where trace can be handy. So the 5X6 errors that I'm talking about could be a result of a backend database failing out, and trace can help you. Identify that information because it's going to give you the time spent by each of these requests in each of the microservices that forms your complex microservices architecture. And finally comes the critical piece which is the correlating piece, and this is exactly where you have to inject some kind of a metadata so that way when you troubleshoot the issues you can correlate all the information from metric block tasers and you can pinpoint what could be the probable root cause. And this is also an area where you could use AI because AI out of the box can pull all the information from your telemetry signals and also can lead you to a, uh, you know, a root cause of, of what you you might be facing. So this is what we thought about and with that I'm gonna hand it over to Ahikana to talk about some of our managed uh open source services that can help you store these signals reliably. OTB. Thank you so much, Vikram. So here at AWS we've built together a variety of different open source observability tools. From collection we support open telemetry as well as managed collectors for Prometheus Metrics. From an ingestion storage and analysis perspective, we've got the Amazon Manageder for Prometheus for Prometheus Metrics and the Open search service for logs and traces, and a variety of visualization capabilities stretching across managed grafauna, as well as open search dashboards to stitch it all together. Many customers have told us that they prefer open source. So we often ask why and the things that resonate are open standard support, allowing you to have a no vendor lock-in strategy, getting the best that the community has to offer, being able to get lots of transparency and control over what you're running, and then of course being able to tweak at your will to get the cost efficiencies you need in your observability platform. But unfortunately it's not as easy as just a helm install, put it into your production stack, and be ready and off to the races. Things don't scale, they're not reliable. They tend to, you know, get overloaded when you have too many teams using them at the same time. All of this creates friction. You have to deal with resource management and capacity planning issues, high availability problems, tenancy controls and isolation, just dealing with the sheer number of servers you have to go manage, patch them, upgrade them, and then of course you have to monitor all of these systems to make sure that they stay up and running so that your teams can use them to monitor their actual applications. All of this adds operational overhead that gets in your way of leveraging these open source technologies to help you solve these problems. No. Here at AWS we've looked at how do we provide managed open source solutions. Firstly, we're looking at how do we make them end to end so that you're not sitting there cobbling together all of the pieces. How do we bake in correlation so you can correlate across metrics, logs, and traces directly through the product experiences? How do we make sure that you have visibility into the cost elements that you have cost controls that you can use to go tweak your cost efficiency? And how do we provide this at the AWS guarantee for security at the scale you need to support your enterprise workloads seamlessly integrated with all the other AWS technologies such as IAM, cloud formation, cloud trail, and of course all the improvements we make, how do we make sure we're giving them back to the community so those communities prosper and get the same open, you know, the same benefits with regards to scaling and availability. Just in case you're not familiar, here's a quick overview of the Amazon manage service for Prometheus. It's a highly available, secure, and managed metric offering. It's serless, fully Prometheus compatible, supports the same open source Prometheus data model and query language. It's all pay as you go and of course supports any environment in any scale. Last year we launched support for a billion active time series per workspace. And you can have as many workspaces as you'd like, allowing you to have billions of metrics stored and analyzed in one system. Of course we also have Amazon managed Graffana, which is our managed offering for the Open Source Graffana project that allows you to pull from a variety of different data sources and build these beautiful dashboards, creating that single pane of glass feel. And then finally we have the open search service which gives you a real-time search and analytics capability across your logs and trace data. It supports, it's fully managed, it's secured to that AWS standard, supports log analytics and observability use cases built in with those experiences, and of course it's deeply integrated into the AWS ecosystem. Putting it all together, you now have the ability to use Open Telemetry to collect logs, metrics and traces, store your metrics in Amazon Man Search for Prometheus, logs and traces in OpenSearch, and then of course visualize it across grafauna and open search dashboards. Regardless of where in the AWS ecosystem you're collecting this data from, you now have an ability to source that data, collect it, tweak it, ingest it properly, analyze it in these systems, and of course pull it all together in a visualization layer of choice, allowing you to create those single passive, single pane of glass. Views. Our customers oftentimes deploy this in two different modes. One is a centralized mode where across all of their regions and accounts they're pulling all the data into one central location, into these central open source observability stacks that allows them to get a cohesive view of all the different places they're running their applications across their AWS footprint in one single location. Other customers have developed a more geo-local strategy where they've got a single stack per region, where they centralize across all the accounts in that region, and then they use a common visualization layer to stitch it all back together at the end. And with that, I'll hand it off to Hans to talk about the Warner Bros discovery story. Thank you, Abby. So we just heard about those amazing tools and products that AWS has made available for us together. They've never been powerful as ever before, and yet tools have limits in terms of how much they scale for really surviving at the enterprise level, you need scaling strategies, and that's what I want to talk about today. Good morning everyone. I'm Hans Robert. I'm a director of observability at Warner Bross. Discovery. Warner Bross. Discovery is fundamentally a story of a company of storytellers. We connect audiences with powerful stories and narratives, and we're the home of iconic brands like CNN, Food Network, HBO, TNT, among some others. We operate a very complex ecosystem that is very crystallized here in our streaming world comprised of HBO Max and Discovery Plus. We support 128+ million subscribers across 100+ markets, all of that on one multi-tenant platform. So the volume of logs and metrics that we collect on a daily basis is pretty staggering. And so for us collecting all those logs, metrics, and traces is more than just something that we have to do. It's really an operational imperative and a retention must have. Today I'm going to share some of the learnings and some of the strategies that we've used to scale the observatory platform that Abby was talking about to actually meet that enterprise challenge and scale to some of those numbers. So when we got started a few years ago right after the merger of Discovery and Warner Brothers, we were facing the classic technology merger challenges of having multiple different observability solutions. Different teams were using different things and tools, looking at different data, and that meant a very slow troubleshooting and identifying of issues. We also suffered from lack of data standardization. Inconsistent data formats and taxonomy made correlating insights across teams a complicated issue. And finally and most critically for what we're talking about today, this hurt our scale challenges, making every event traffic or just keeping up with subscriber growth a bit of a headache that we needed to solve. As you can imagine, this legacy state was inefficient, unsustainable. It didn't work well with our desire to reduce MTTI and be able to resolve the incidents as quickly as we wanted to. So we've built a completely new platform based on those open source technologies that Abby was talking about. Before I talk about the strategies though, I wanna talk a little bit about the open source and why open source, and Abby mentioned a few of those reasons here, but I want to just double down on some of them. First, we wanted to avoid what we call the one-way door, the kind of things that we make a decision or a choice that actually is very hard to revert or evolve over time when your needs arise. Or being able to really sort of like be pegged to one specific proprietary vendor technology or strategy. Second, we wanted to build an unity platform where we actually build a lot of the guard rails and golden pathways straight within the platform. Things like the guardrails are there to prevent user mistakes, errors, data spikes, anything that would actually make the platform in trouble, and the golden pathways are there to really help teams, engineers to onboard onto the platform and leverage as best as they can the observatory platform. And then finally, maybe most critically from a cost perspective, we wanted to have full control over our data and the pipeline, so both from a data pipeline and data retention and storage, we wanted to make sure that we knew exactly how and when we collected the data and where we store that. Let's talk about some of the strategies that allowed us to scale our platform to, you know what we have today. And I'll talk about 3 different things that matter to us a fair amount. Data organizing, applying, optimizing our system through sharing, and then finally cost monitoring and how we manage our cost. My goal today, if I can, is to give you some strategies that you can apply to your industry irrespective of whichever it is. This is not specific to video streaming, and that you can apply and hopefully use it the exact same way that we've managed to scale up. So let's just start with data organizing and how we've tamed the chaotic volume of information by organizing it. So I'll touch on a couple of things, but maybe first of all, the industry cliche is data is the new oil, but frankly, in my cautionary views, data at scale is scales by default unless we organize it. So today we focus on two pillars of how we organize the data. First, establishing an operational metadata, OMD for short, to tag and track our data flows. And secondly, implementing a unified events scheme across both logs and traces. So Operational metadata, for short, so this is not an observability specific strategy, but as you'll see later, it's foundational to a number of things that we've done later to both shard and scale. And so I want to make sure that you get a good grasp of that. So we wanted to establish a functional hierarchy for streaming ecosystem for all of the applications and infrastructure that we have there. So you can see on the screen we have this kind of simple three-tiered hierarchy of systems with business services at the top, you know, like video playback in this case, and that represents a kind of high level business capability. Below that we have the OMD service like markers, um, and that is really like a logical feature within that capability. And then the last one, the OMD component is really sort of the final micro surveys that implements a specific feature within that capability. And in case you're wondering, markers is how we track what and how you've been watching your streaming choices so that if you pause and want to resume later, we know where you were, and so that's the marker. So Establishing that we need to really distribute the authorship and the collection of that data, and we solve that by distributing it directly to having the engineers. Pressing the wrong button. Putting all that information into a standard file format in GitHub alongside their code. Easy and we implemented some automated checks to enforce the data quality. And then on top of that we're essentially now scraping all the data and collecting it and storing that in Amazon Aurora, on top of which we built a graph and a dashboard called Service Catalog that gives us the entire topology of our system. So all of this is providing a single source of truth, that is the operational and functional map of our entire platform, and by that I mean the application platform, the streaming platform. But as you'll see, this has great relevance for observability later. So now we have our operational data data established. The second critical goal was really to map all of our observability data, whether that's blogs, traces, metrics to this functional hierarchy. So here there's an example of a log on which we've essentially applied that business service, service and components, video playback markers and collectors. So that that data is now tagged and and organized. And then to scale. With this tagging had to be automated, so we engineered a solution to enforce the standardization at the collector level. So in our Communities cluster we're running FluND to collect the logs and alongside Fluani we have an add-in that essentially tags automatically the incoming data from FleuranD with the information collected at the node level that contains the business service, service, and components. And now that we've tagged all of this information directly onto the message that we sent to the observancy account. We know exactly where the data comes from and where it belongs to and what it's relevant for. And I'm not showing you here on the screen, but we've done something very similar for metrics and traces, so that all of our metrics have uh the OD Business service, service and component, and similarly traces have the same thing. So this automated automation guarantees that every piece of data that's destined to our observability account is correctly tagged regardless of volume. That's important for scale. And it's also pretty structural and fundamental to some of the sharding strategy and so the cost metering strategies that I'll talk about in a moment. But before I get there, I want to talk about the other aspects of organizing data. So that's typically for event data being logs or traces. Without establishing a strong event schema or standard. Every service and component, i.e., every team or developer within your company. Operates independently and that typically leads to like schema drift where even a simple entity like a client identifier can be represented in multiple different ways, client underscore ID client dash ID CL ID, etc. This is not just a semantic issue, it's an operational blocker. You cannot join data with different identifiers easily and efficiently. In addition, it really impacts indexing. I already mentioned the cross-service correlation, but also massively increases the query complexity and the cost of storing that data. So solving that schema proliferation was important to achieving our scaling observability platform. So we've introduced the organization-wide logging scheme has built a mouthful, but we've shortened that to hours. As is built upon a mandatory core log schema, so every log, no matter what, have non-negotiable fields like time stamp or severity here or other things like message or the OMD fields that I just talked about a minute ago. On top of that, we can have business service aligned schema that define things that are more relevant to each of those business services, like for the for the payment log schema, you might want to add the payment method type or the payment references. And then for video playback, video ID or stream ID, etc. Care log schemas and business schemas are then merged into a specific business oriented schema that it gets injected into the OpenSearch index. We now have a very dedicated schema but with common fields that are originating from the core schema. So this unenforced mapping ensures that if any event attempts to ingest something that's not defined part of the schema. Field can be rejected. We can reject the message or just the irrelevant fields and or store that in S3 for further review and then we can send a notification to the author of those logs and basically tell them to fix it, try again. Do better. One of the things that has changed is also part of the culture and the conversation, knowing that some of those events might be rejected if the log just goes outside the boundaries. Have really made people think a little bit more about what and when to lock things and potentially having a much more intentional way of sending information to the observatory platform rather than just filling a data swamp with stuff. So combining OMD, the operational metadata, and our schema, we've achieved a few critical outcomes. The serrated MTTR really comes from the data quality, but also from some of the data standardization and the ability to connect different services and microservices together. Intentional consumption, I just mentioned that a minute ago, people really think twice now about what to log and what metrics to log rather than just filling up to infinity the number of observability telemetry that they can think of. And then finally, it's a really good foundation for scale and you'll see how we're going to be using the operational metadata itself to actually build for sharding but also some of the cost metering. That's right. All right, so that gets us to Charding. Um, Abby mentioned some of those strategies earlier. We've adopted really for the geolocation approach, which we'll get to in a minute, but also talk about some of the logical partitioning that we needed to do in order to get to a point where nothing is too massive. And also about the need to abstracting shards because of the problems that shards are creating. All right, so geoshoing. Um For context, to serve our 100+ markets in our nine regions globally, our business applications are distributed across those nine regions, and most of our components and computers are running in EKS and with a supporting cast of services like database and storage that are all monitored by CloudWatch. So looking at logs first, we have FanD collecting logs directly from the pods in case and then forwarding that data to a regional open search, i.e. in the same region that the data originated from, and we replicate that across every region that we have around the world. Metrics have a very similar path. We use the graphen agent in this case to scrapey case metrics directly from each pod and then send that to an AMP workspace that lives in that same region. And then finally, to bridge the gap with Amazon CloudWatch, we've leveraged YAC, yet another cloudWatch exporter that extracts the data from CloudWatch and forward it to the same amp in that same region so they can add the correlation between infrastructure and application, which gives us a step forward, the single pane of glass. Let's move to logical sharding. While geo sharding handles the physical location and provides already some form of sharding, we also need a strategy to organize logically so that nothing within that region can become too massive. What if for instance you need 1.1 billion metrics in Prometheus, it now goes to a billion, so you need more than one. So for metrics, graphene agent reads the OMD label that's not very familiar to all of you here. And uses that to maker decisions. So rather than send everything to the same huge back end, we sort of shot that leveraging the business service or service or component as needed in order to get the optimum distribution. And then we apply the exact same logic on logs and traces, in this case leveraging Fluy that forward things to fire hose and eventually to the relevant index in OpenSearch or instance where it's essentially automatically indexed within the relevant logical index for that business service, all service, all component. So with this strategy we're really preventing not only things to become too large, but also to have one service or one component or one business service to create a traffic spike that would noise the neighbor or impact the ingestion performance of another service that happens to live on the same. In the same ecosystem and so for risky components or things that are very spiky in nature, we tend to isolate them onto their own dedicated shops so they can only impact themselves. So I talked a little bit about shard limitations. The main issue with shards and sort of like the partitioning that we explored here is that you can no longer just have Graphfana just doing one query. It really has to know intimately the topology of your system in the back end. And really sort of query the data from where it belongs. And then knowing which a workspace actually contains your data, is it the one in US East in Europe? Is it the one for video service, this can lead to something that's actually quite complex and complicated to manage. We have the same friction with logging and tracing where the open search dashboard. We also need to know where that transaction or where that log was ingested from in order to actually capture that specific message and really address the exact open search that contains that message. So as you can imagine, this is not really a practical way to implementing that. It gets pretty tedious pretty quickly and slows down that MTTR that Vikram was talking about that we're really going after. So we introduced a couple of abstractions in this case. So starting with metrics at the top, we introduced proxy, which is also hard to say for some reason. Um For those who are not familiar with proxy, it is an open source aggregating proxy that sits in front of your data. But looks from a graph and a perspective exactly like a Prometheus. So proxy acts as one giant um amp workspace, and then under the hood just like shards the query or distribute the query across all of these different workspaces that it knows about. Brings back all the data series, we put them back together and then feed that back to Grafana. So with proxy we effectively have a complete abstraction that Raffael doesn't need to know about how many workspaces there is behind the scene. Is it 1? Is it 70? I. And then for logs and traces we have leveraged the open searches native cross cluster search capability. It does pretty much the same thing as what Proxy is doing. It basically distributes that request to the different of underlying open search workspaces or clusters that you have behind the scenes, and so you re-aggregate that together before giving it back to the open search dashboard. So not only we've made it easy and efficient for people to write queries or dashboards or build alerts or any kind of visualization on top of the data. Importantly, we also completely disconnected the life cycle of the back end, i.e., the need for restructuring and rearchitecting how many shards, where they live from the consumption on the front end, and we're able to change all of that without having to impact the consumer. So a dashboard. There was a return in this architecture with 3 a work spaces. We'll work tomorrow with 2 a work spaces or 3 or 5 a work spaces as we scale because it's still talking to the exact same proxy proxy instance. And then same thing for the open search dashboard, it only knows about the one cross cluster search instance that will then do the federation. So to recap, this combination of geoshharding, logical partitioning, and the abstraction that we've built really has given us a very flexible and scalable architecture. We effectively futureproof the platform if the business decides to launch in a new region tomorrow or if we expect a specific service to have a 10 in traffic, or if there's new capabilities that would generate a ton of logs and metrics, we can do that pretty simply by just adding a new shard or launching a new cluster and without disrupting any of the SREs, engineers that are leveraging Graphana and OpenStotch dashboard to create that data. In turn you see that. All right. Final strategy this time around cost metering. If you recognize this guy on the screen, you know that. What this section is about, and if you don't recognize him, he's Roy Logan from Succession, a show I would highly recommend for you to watch on HBO. But if you still don't know who that is, you can think of the CFO, the CEO of your company. Those people don't care about, I don't know, high cardinality or index fragmentation, they care about the bottom line. And to survive at scale, we had to implement some kind of financial showback or chargeback to understand where the costs are coming from and to make sure that observability doesn't become the most expensive line item on the AWS bill. It. Um So there's a specific challenge within AWS to do cost attribution in a shared environment. So imagine we have those two services, payment and video playback. And they both send logs and traces to one open search service back end. Similarly, they're producing metrics, most likely at a very different cardinality. They might have completely different time series, numbers, and they all end up in that same sort of amp workspace or amp instance. Today The native strategy within AWS is to tag the instance, which means that if you navigate to the Coast Explorer, you're going to know the total amount that we spend for OpenSearch or for Prometheus, but not the detail of who's actually generating that cost. Is that payment? Is it video playback? How do we sort that out? Yes So to close this attribution gap, we've engineered a bespoke cost metering framework. That essentially triangulates three different types of data. One, the cost. Can't live without that. So we extract that from uh AWS and we store that into an analytics platform. The second is getting the usage of both open search services and AM. By collecting specific volume metrics, for instance, log gigabytes, number of traces, number of time series in amp, and then we generate essentially a new time series that we store in Prometheus. And then finally, we allow for teams to really enter allocation rules, like how do you want to partition and mathematically divide that amount of money based on the rules and based on the cost that we have there. So you got the 3 sets of data. The magic of the reconciliation happens in the middle here in lambda. This runs on the trigger and essentially merge all these things by mathematically dividing the cost that we've collected for both of those instances and basically saying you spent more, you pay more, etc. And then we create yet another time series that in this case we store in the Amazon Time stream. And then finally, we built a dashboard because what would you do without a dashboard in this case in Grafana, and then we have built a dedicated dashboard for each of those business services that now know very well the specific costs associated with their observa platform. Now on top of that transparency and the visualization, we've also built an accountability framework. It's not just sufficient to have that available. There also needs to be a little bit of a Accountability going behind the scenes there. And that has also kind of changed the culture a fair amount. We've sort of moved from a conversation about why is the AWS bill so high to more fair, you know, how can my team contribute to really managing the cost efficiently and appropriately. All right, so now that we have the 3 strategies implemented. We've successfully built a platform for the future of streaming. And by implementing the strategies that we shared today, we established 3 powerful pillars. Global observatory solution that's scalable leveraging managed services like Amazon Prometheus and OpenSearch. We transformed our data into a strategic asset by standardizing operational metadata and then the schemas. We've ensured that every metrics, logs and traces are immediately valuable, searchable, and trusted by the engineers that rely on them. And then finally we've unlocked scalability through intelligent sharing and our cost metering framework, we've really sort of have an infrastructure that scales alongside our subscriber growth. great And we can expand to new region or handle spike traffic if we need to. If your organization is facing similar growth challenges or issues, I highly encourage you to look at those strategies. They have been game changers for us. Thanks. Before we conclude the session, I want to hand it back to Vikram, who is going to talk to you a little bit more about how to operationalize. scalable observatory platform. Thanks. Alright, we're gonna wrap it up with, uh, sharing some of the best practices of operating observability tools at scale. The way I've divided this is into two sections. We'll talk about the best practices of collecting the telemetry signals, namely metrics, locks, traces, using your collector of your choice, open telemetry for metrics traces, and then for logs, maybe fluent bed. You also have the open search injection pipelines, so that's gonna be the first lesson or the first best practices that we're gonna talk about. And then the second one is about how can we ingest these telemetry signals into Amazon managed open source services in a reliable and durable way. It's a no brainer. Uh, start with sampling and batching. Many of you might be doing this already. Anyone who here does like sampling and batching as part of their collectors? No one? OK, so when I say sampling, we only keep the percentage of data that we care about goes a long way when you're dealing with huge volumes of traces and when batching, we kind of group the signals before we ingest them into the back end, right? So by following both sampling and ingesting. You're making sure that you're not overwhelming the back end as well as reducing the overall network overhead, right, like as a result of ingesting these telemetry signals. And when it comes to metrics, right, these are high cardinality information. So at any point if you think any of these labels might not add a huge ton of value to you, I would request you to drop those unnecessary labels, right? Especially you have this, uh, action config within your open telemetry collector or Prometheus collector. It's very straightforward. You just have to specify the label and simply drop it. What it's gonna do, it's gonna keep your time series metrics, time series database less as well as it's gonna improve the again the network traffic from your collector to the back end telemetry, uh, sorry, um, back end open source manage services also not gonna be over well. And good for us. Many of our collectors have these best practices out of the box. Like, for example, compressing, right? So there is this compressed, like out of the box open telemetry supports GSIP compression. So you could use that. So you kind of compress these telemetry signals before you send them out, and you can also enhance your telemetry data when by enhancing, I mean there are like, uh, anyone here uses Cade as attribute processors within their open telemetry or Prometheus, not really. OK, OK, one person perfect, uh, yeah, you could add. Necessary metadata into your telemetry signals, for example, you could add the deployment config, you can add the pod name, you can add the name space name. It's going to further enrich your telemetry signals that you're collecting. And when it comes to the logs, you definitely have to monitor and optimize your buffer usage. You have to configure dead letter queue so that you know if some of your requests fail, you don't drop them, but instead you move them onto the dead letter queue and try revisiting it or you know, troubleshoot on what happened. And finally, the most important pieces you have to definitely monitor your monitoring tool. You have to set up some kind of an autoscaling strategy. You have to like open telemetry out of the box gives you a lot of metrics that you'll have to um uh keep an eye for and also make sure that you have sufficient resources configured to your collector and have some kind of uh auto scaling strategy. Lesson two is about how we can better ingest these telemetry signals into Amazon managed Prometheus and Amazon Open Search Service. Again, these are out of the box features that's available whenever it comes to complex queries. It is highly recommended to use recording rules that way you don't like have a complex query or you know overwhelm your Prometheus database. Instead, all these recording rules will be computed at regular intervals and will be transformed as separate time series. Amazon managed Prometheus also announced label-based active time series in a multi-tenant environment where you have non-mission critical application, mission critical applications running on the same clusters. You can specify, you can specify a cap on how much of an active time series that you would be willing to use for a specific application. So you can scope down them by labels, non-mission critical time series. I just, I, I only need like 20,000 time series for the mission critical. Maybe I need like 1 million time series, so you can scope them based on the labels. And you, you definitely can implement query controls and query logging. This gives you the ability to, uh, uh, you know, specify how many samples should I traverse through to make sure this query runs, right? You wouldn't want to run like your query against like um uh a huge number of samples, but you can specify the number of samples that you could use. And when it comes to Amazon open search, the first and foremost thing that you have to do is you have to definitely calculate the storage requirements. You'll have to account for index index overhead. You have to account for replicas, your data flows, and make sure you select the right instance type and the storage requirements, and you absolutely must, absolutely must have a sharding strategy. A general rule of thumb, we recommend to have like less than 25 shards per gig of your heap space. You have to control the ingest flow and buffering, and again we have the open search ingestion pipeline that can take care of that. So we would recommend using open search ingestion pipeline in tandem with your Amazon open search service that way you don't again uh you don't, you don't, you kind of control the log flow. And finally you have to optimize the bulk request and compression. Use the bulk request API so that you know you can batch the batch your log flow and finally compress them and send it to the back end, which is your Amazon Open search service. That said, um, I also wanted to highlight some of our recent announcements we made on the AWS manage open source services this year. I'm gonna let the slide sink in here. If any of you have any questions on these features, we'll be more than happy to answer your questions in the hallway, um, but we kind of talked about some of those important announcements as part of our best practices slide. That said, I would like to thank you all for stopping by attending our session. Um, uh, yeah, thank you so much, folks.