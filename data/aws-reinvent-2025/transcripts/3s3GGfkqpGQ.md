---
video_id: 3s3GGfkqpGQ
video_url: https://www.youtube.com/watch?v=3s3GGfkqpGQ
is_generated: False
is_translatable: True
---

All right, we will, uh, we'll get started. So thank you for being here. I am Kristen Hughes, and I lead a team of solutions architects at AWS in ISV space. And the best part for of Reinvent for me is being able to hear about innovation from our customers. So today we have the honor of hearing from Aj Kanwar from RTO. He's the chief product officer. Flip this side. And as you can see, Reltio is revolutionizing enterprise master data management and is the first cloud MDM platform to seamlessly connect Agentic AI with a trusted data foundation. And as the chief product officer, Anche is a thought leader in agentic AI. He's spearheading Reltoio's transformation. Anche's strategic vision has enabled Reltoio to unify, cleanse, and enrich data across enterprise systems from legacy infrastructure to modern applications to data lakes. This creates the foundation for an effective AI implementation. So let's hear from Ach on the route to transformation, their purpose-built agents, and how they are delivering, delivering real business impact with real-time, unified, trusted data. Ach. Thank you Kristen everybody, thank you for being here, of course, uh, it's a small group you guys can huddle in if you want, uh, gives us a more intimate conversation, yeah, um, so. What is Retio? What does Retio do? One way to understand Relto is to, if you're aware of the master data management space, you can say Reltoio started as the premier master data management provider that is cloud-based and has brought this idea of operational master data management to the forefront. But you can also look at and what we're we're doing at this point in the market is really think of a higher level concept called data unification, and the idea behind data unification is, you know, what, what, what is the core problem that we're solving. We're bringing together records from different sources and we're saying how can we select data attributes that are the highest level of fidelity that give you the output of this process of combining all these source records that is a higher fidelity. Um, record in which you can place a lot of, ah, much more trust than the input, yeah. What has happened over over the over the years is this record, this idea of this golden record has really evolved significantly to where now we talk about trusted profiles, this idea about Knowing your customer or your supplier or a product, essentially any noun that your business cares about, how can you build a rich profile of that particular entity and keep it continuously updated as an evergreen data product that is really focused on that can really be consumed in real time, right? So your energy is focused on pushing all this data. Across your enterprise first party data, RTO combines it into a graph. That then is available for consumption on the other side. Both the input and output are available via APIs and events, so as near real time as possible. And the upshot of all of this is that now you have trusted governed layer of data, core data to your business. What is this data about? It's about customers. It's about suppliers. It's about products. It's about your financials, anything that ultimately. is meaningful for building business operations on top of. So the picture here sort of summarizes that with this idea of the knowledge graph of RealTO as this context graph that is in the middle of all of these enterprise operations, and we've been fortunate to serve some of the largest companies in the world. We started off more in the regulated space with life sciences and healthcare companies. Our footprint now in financial services and insurance is fairly large, and we are now seeing with AI this notion of this graph which was trapped in this idea of master data now really has become this underlying core data that is power. More and more of agentic AI as it spreads across the organization and and that's really the context of these 1520 minutes um which which talks about this this agent's bright box on the left hand side and what does that mean if you have a data foundation that is already built out of the of the nature that I just described. So the best way to understand. Any product in my opinion, is to understand what our customers are able to do with it. So I'll tell you two quick customer stories. The first is CarMax. CarMax is the largest retailer of used cars. They typically see a car come in and out of one of their showrooms 3 times in the car's life cycle. They also typically see customers or buyers multiple times either buying or selling a car or vehicle through them. The highest level promise as a business to their customers is that The customer can walk in, close a transaction, whether that's buying a car, selling a car, or both, and then walk out within 15 minutes. And to be able to do that, they think of to enable all of this, they've organized data underneath which is able to support this business process, so they think of 3 360s, if you will, 3 360 views, a 360 of every vehicle that they can get their hands on. So they have a very, very large understanding or a large data set that helps their understanding of this 360 view of every vehicle in the United States. Very similarly, they have a 360 degree view of of buyers, of adults in the US, and also they have a 360 view of their employees and for them a transaction is really bringing together a 360 of a vehicle with a 360 of a Of customer with a 360 of their salesperson because not only are they closing the transaction with the customer within 30 customer within 15 minutes, they're actually also closing the commission with the salesperson within that 15 minute period. And so to be able to deliver on this business promise, they use Ralio organized in the way that I described to be able to support all of this in real time across their US footprint. Now for a very, very different kind of example, Warner Brothers uses Reltio to manage their IP assets. So if you think about Daffy Duck or or one of the cartoon characters, well, the, the character itself has quite a bit of commercial footprint, right? What are the merchandizing deals that are associated with this particular character? What are all the movies or ads that this particular character appeared in, and so on? All of that. It is very critical to be able to then account for revenue against that particular IP and so you can think of that as a product that is managed by Warner Brothers within Relto as a graph, interconnected graph of their of their characters and the movies and then their merchandizing deals. So just two examples hopefully that illustrate the point of if data is organized in a certain way, then it can power business processes in a continuous manner now. Not everything is perfect in the world of data management. Despite being the most cutting edge platform in this space, there are still a lot of operations that have to be, or had to be, I should say, performed manually. Now some of these have to do with things deep in the data like entity resolution saying whether two things are the same or not. Now, our machine algorithms are able to, let's say resolve 85 to 90% of matching. However, there are still some. Percentage that gets punted to a human to get resolved. Well, for large companies, um, there, there is a, there is a role of data stewards that spend time making sure that this data is actually at the highest quality that it can be, right? That's a lot of manual effort. Data quality. That's that's almost a religious debate, right? You can spend a lot of money as a large enterprise getting data quality to a perfect level, but what is good enough? And that really depends on the application. And so data quality itself, a lot has been done in that space, but getting to the right level of quality for a particular use case, that still takes a lot of manual effort. And of course data models, all of you deal with data landscapes that are diverse. You have different products that think of they. Think of customer with certain attributes in a SQL database and may think of that as very, very differently in an application, right? So the complexity of data models makes it really hard to get an enterprise-wide view of sort of uniform view of what's happening across the data landscape. And so we set out to Solve this remaining problem in data management. How can we make data management for the largest companies in the world be an order of magnitude, two orders of magnitude better than it is today? And that means always means cheaper, faster, and provably better. And that is the context in which we worked very closely with AWS to be able to launch a new product and market. Um, and that is the bright yellow arrow here, agent flow. Um, but before I describe that, let me just describe the base of the pyramid. This is the set of products that Reltio has in market, multi-domain master data management, which is all of your core entity data. And then Intelligent 360, which allows you to bring in a lot more information and connect it with that entity data to have this complete evergreen graph, and, and both of these products then can be augmented by agentic means, and that's what this agent flow layer is and that's what I'll I'll talk about and I'll demo here in a second. I also want to sort of highlight the bottom portion which is our data cloud, which runs on Amazon. And allows us to stand on the shoulder of giants and be able to really deliver world class security, world class compliance, global data distribution at ultra low latencies, etc. using a lot of the primitive that Amazon provides. OK, so enough talking, let's uh go through a couple of demos. So, um, I'll try to do this looking down here, uh, the video will play in the background and I'll, I'll give you the talk track. Um, so, what is agent flow resolver? This is this interface, the agent flow interface. We can publish multiple agents in here, and the idea is a conversational interface that allows us to ask not in SQL but in simple English, questions like we're experiencing service delays at our customers in our top 100 segment. We think it's because of data quality problems. We think it's because of duplicates. Please resolve, right? And so the system is the system has now gone through and it's through its chain of thought reasoning, it's actually proposed an approach and it said, I'm going to do some searches. I'm going to do something for each one of the organizations that I found, and so on. And you know, here's some options for you to choose from. Now, of course, if you use cursor or any of the other AI development tools, you know this is a very familiar experience, right? We're bringing that to data management and in this case it's identified a couple of different anomalies in the data. And, and, and it's pulled data importantly from the internet. One of the key requests we've received from our customers forever, feature requests, is, look, there's so much data on the internet. Why can't we leverage that for better data management, right? So in this case we're trying to dissolve this data around the solar turbines company and you can see different columns and in that each one of the attributes has been organized and ultimately it comes down to a set of automated recommendations, right? We could do all of this in the background of course through an API and we do, but This here is illustrating that data steward role that I mentioned and acceleration of that because somebody doing this research, that's easily a 35, 40 minute task to go organize all of this data, and here it is at their fingertips, right. And then of course a very big part of being able to deliver this is to be able to deliver trust in what's happening. And so every step of the way the tools that are used are listed and the conclusion or why the conclusions are being drawn, that is listed. And what you don't see here are guardrails. Specifically, if I, if I ask, you know, make me an omelet, it's going to say great idea, but I'm not the right agent for that, right? And so being able to, being able to stay on task is very, very important, and we're able to deliver that in the data management context. So let me switch over. Let me show you a different agent, and in this case, The idea is or the setup is that there's product data that a company has in Realio. There's their customer data that they have in Realtio, and a call is coming into a call center. We're agnostic if this call is being answered by a human or it's being answered by an AI agent. But the task is we want to recommend products to be placed within that phone call and what would those products be. So really it gets down to that personalization to an audience of one, which has been the holy grail for many, many years, and the power of data being organized a certain way makes it seem almost trivial. OK, there we go. So in this case we've selected a different agent, the product recommendation agent, and what we're saying is there's a certain customer calling us. We would like to produce 3 compelling product recommendations. Do your magic, and you know the agent sort of goes through and it searches for that particular customer. Once it finds the customer, it's looking for multiple data sources that we have from our first party data about the customer to really try to. Understand how they've been interacting with us. So they've been in this case interacting. They've purchased certain things from us. They are enthusiastic about tech products. They've asked questions in our community as a retailer, and so definitely an engaged customer. And because of these signals we know that they might be interested in buying drones from us. And here's a list of the 3 products that would be interesting to them. And the why, most importantly, I mean, even, even if You produce a list of 3. The important thing is to be able to defend why you recommended those, right, because that's the only way to improve these algorithms going forward, and it does that by saying which data points it has used to make that decision. Now we go further. We say, OK, this individual may be interested in buying something for their family. So what do we know about their family? Where do we have consent? And in using the correct using using that information legitimately, I'm sorry, how can we come up with more recommendations that are relevant to the family? And so in this case we find that Sarah, who we searched on, has has a spouse, has a child, and there may be an interest where the Child here 19, the daughter 19 years old, may be interested in an FPV racing drone. So it seems like it's a family hobby and and you know we've come up with a recommendation and based on the conversation now the agent can position one or the other and and they have full ability to justify why they're positioning this new product. So as we as we go through the rest of this, we'll flip over. It's a little bit of also communication strategy, it's worth pointing out that it's guiding the agent on how to communicate um all of the data that was just sort of uh produced by this agent, yeah. Um. And then of course just to just to illustrate the point we say how many data points did you use to generate this output and in this case it used 76 different data points using 23 different attributes and multiple relationships and I think the relationships bit is super important and often overlooked if you have oversimplistic data models because it is about the connectivity of these different entity types, how dense that connectivity is, and we can infer a lot from those relationships. Um, so With that, I'm going to switch over to how we built this, which might be of interest to some of you, and we leveraged all the bits that Amazon offers at this point with Agent Core and Bedrock and the the the. The agent building SDK and being able to put that together to be able to deliver the functionality behind the scenes means that we have a lot of confidence in the output that these agents are producing. So we have a lot of trust because of our data layer. We have a lot of confidence because Agent Core is helping us with guardrails, is helping us with memory, is helping us with multi-tenancy. And so a lot of this sort of heavy lifting that we would have to do behind the scenes as independent software vendor, it's very important for us to be able to guarantee these sorts of nonfunctional capabilities, and with Agent Core we're able to leverage a lot of that without having to without having to build it. Um, sorry, I was trying to remember strands as the agent building framework a minute ago. Yeah, um, so building these agents and being able to run them at scale because the other thing to remember is our customers, customers are some of the largest companies in the world. So once they start using any of this, it gets used at scale and therefore we need to have a framework that's backing us that is able. you know it's not we're not going to run out of GPU cycles. We're not going to run out of run into token limits and those sorts of things, and so far we've had a fantastic experience being able to build and deliver that on Agent Corp and some of the things that are coming out, especially this morning, the announcement around policies in Agent Corps, I think just make this even more stronger, and I'm very excited about the direction that the team is taking. Um, with that, I was, I, I, you know, I was able to walk you through some of the videos here, but I'd invite you to come actually see the demos and interact with the Realtio team. Uh, we're out there on booth 1227. Please come talk to us, um, if you want to learn more about the things that I just touched upon. Thank you very much.