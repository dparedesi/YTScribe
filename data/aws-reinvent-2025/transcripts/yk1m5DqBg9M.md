---
video_id: yk1m5DqBg9M
video_url: https://www.youtube.com/watch?v=yk1m5DqBg9M
is_generated: False
is_translatable: True
summary: "This session, \"Revealing the northern lights: Amazon Aurora security deep dive (DAT456),\" offers a behind-the-scenes look at how AWS secures its Amazon Aurora database service, specifically against zero-day vulnerabilities. Eric Brandwein, VP and Distinguished Engineer at Amazon Security, and Andy, Principal Security Engineer, detail a real-world event where security researchers used a novel \"zero-day\" attack in the PostgreSQL engine (involving PL/Perl and PL/Rust extensions) to gain code execution on an Aurora instance. Crucially, the attack failed to compromise the broader service or other tenants because of Aurora's architecture and defense-in-depth strategy.\n\nThe speakers explain that Aurora decouples the \"head node\" (the database engine) from the storage layer. They treat the head node as untrusted and \"sacrificial\" because it runs third-party code (MySQL, PostgreSQL) that they cannot fully control or patch instantaneously without disrupting customers. Instead, they rely on a multi-layered defense system: SE Linux policies that block unauthorized process execution (like `cargo`), \"Chronicle\" (a high-fidelity telemetry agent) that alerts on suspicious system calls (like `curl`), and rigorous VPC flow log monitoring. This \"containment\" approach ensures that even if a bad actor \"pops\" a database box, they cannot pivot to the control plane or the shared storage network. The session underscores Amazon's philosophy of not treating the database engine as a security boundary and utilizing aggressive internal \"Red Teaming\" to constantly validate and improve these defenses."\nkeywords: Amazon Aurora, Database Security, Zero-Day Vulnerability, Defense in Depth, SE Linux, Chronicle, Telemetry, Red Team, Cloud Security, PostgreSQL\n---

My name is Eric Brandwein, and I'm a vice president and distinguished engineer with the Amazon Security team. And I'm here with Andy, who is a principal security engineer with the uh AWS databases team. And this is a common occurrence at AWS. We have someone on the security team and we have someone in the business and they're working together on something. This is one of the ways that we, we make sure that we actually deliver security for our customers. And today we're gonna pop the hood on Aurora and we're gonna look at a particular aspect of security in uh a cloud scale distributed database service. So there are a ton of database services out there, because there are many ways to build a database service. If you were building a cloud from scratch, you'd probably take an existing database, or maybe 6 of them, and you'd throw it on an EC2 instance and you'd offer it to customers. Like this is the obvious service to build. And so this is RDS, the relational database service. If you wanna run Postgras or Maria DB or MySQL or SQL Server or Oracle or DB2, we offer it as a service. It is the database that you know and love, or perhaps the database that you know and hate, running in the cloud as a managed service. And This is, this is awesome. Like, it turns out that we're, we're taking a huge chunk of the cost of owning one of these machines, and we're taking that on for our customers. The service provides a ton of value, but we're constrained in what we can do. We are taking what is essentially a single host. Database and we're offering it as a service so we're going to continue to offer something that is a single host database and you know maybe you've got replicas and things like that but now you've got multiple single host databases. This is a thing that was fundamentally designed to run on a piece of sheet metal in a data center and so. On the other side of the spectrum, you've got something like Dynamo DB. Dynamo DB is awesome, it is built for the cloud. It is this huge, scalable, serverless thing. You can scale up, you can scale down, it doesn't scale vertically, it's inherently multi AZ so you get better availability, you get better data durability. All of these magnificent benefits. There's so much more benefit that we can provide to our customers because of the nature of this database service. But it's Dynamo DB, it's not a SQL interface. You have to build to Dynamo DB. And so customers love this service, it's been very successful, but you've seen enough of these PowerPoint slides before. You know that there needs to be something in the middle of that spectrum. And so. This is Aurora. It's a cloud native database. And we've been able to do all sorts of interesting things, elastic storage, multi AZ availability, increased durability, but we've managed to maintain compatibility with existing engines like MySQL and Postgrass. And in the case of Aurora, it's only open source engines because of the way that we built the service. And so, let's look at this. To build Aurora effectively, we took the database and we sawed it in half. And so as a customer, the node that you're interacting with, the thing that you're interacting with, is the head node. And it's running actual MySQL or postsgress code. So the query planner, the query parser, anything that needs to be done in a single memory space like joins, like all of that is happening here on this node. But the bottom half of the database has been completely replaced. And so that's allowed us to make different design choices to provide the availability, performance, and durability that we knew we could get out of a multi-tenant cloud service. And so, you have at least one head node, but there can be many of them, and they can be spread across availability zones. And because this isn't where the storage lives, these things aren't stateful. When a transaction is committed, this node is no longer important to the availability of your data. And so inside of this box. It's a standard EC2 instance, so it's going to have an elastic network interface, an ENI. And you can connect this to your VPC. This is how you interact with the Aurora service. It appears in your VPC. You get flow logs, you get, uh, complete control over the security groups on this ENI. It's a part of your network. And so, that's great, that's how you interact with it. But we need to be able to manage this. We need to be able to send traffic in and out of this instance. So there's a second ENI connected to the Aurora network. And on this network, we also have the multi-tenant, multi AZ storage system, and it, it's a fascinating system. There's many, many talks about this storage system. It is designed specifically for this workload. It is the Aurora back end, it's not something else. And so we get lowered costs, we get lowered management overhead, increased performance, increased durability, like it's a wonderful system, but that's not this talk. So You've all launched DC2 instances, at least I hope you have, but none of them suddenly started acting like Aurora headnotes. That's because we have software that we run on there. And so this is the top half of that database. Um, it's running a chunk of the database engine, and that's what you interact with. That's where you're sending your queries, that's where they're being planned out and executed. And so, also pretty obviously, we need to have a component that is on the host that we use to manage that host. And this is one of the places where the clever naming schemes failed us. So the host manager component is called host manager. And the host manager is what talks to the control plane for the service. And so the storage system and the control plane are multi-tenant. So now that we've got this shared understanding of how we built Aurora, let's talk about the threat model. So we've got the multi-tenant storage system. And then we've got a customer VPC and it's talking to their Aurora head nodes. And of course we have more than one customer. Um, we have many more than 3, but 3 was enough to bring PowerPoint to its knees. And so all three of these. That all through these headnotes, and many more talk to the storage back end, and they also talk to the control plane. So this is the world that we're living in. Our customers interact directly with the database engine. These are standard database engines. They were written by people outside of Amazon, and they were written to do database things and not security things. Fundamentally, a database is designed to be owned by someone, hosting data for that person, executing queries for that person. It's a single party system. And now, we've got a database that was written by one company, being hosted by another company and being offered as a service to a third company. This is not where databases were designed. And so it is a mistake to treat a database as a security container. And so we do have the source code to these, like that's how we sawed it in half. But we don't want to have a whole raft of proprietary patches that we're carrying forward. And our customers want new features as soon as they're released. There's a new post-gras release. Our customers want those features immediately. We have to get those releases out to them, and the more burdensome the patching process is, the slower we're gonna move. And Our customers want deep, rich interfaces. They don't just want a little tube through which they can cram SQL queries. They wanna be able to manage their data, they wanna be able to extract value from it. They wanna be able to run complex functionality across that data. And so we need to supply those interfaces to our customers, and database upgrades are often painful. They're often breaking changes, they require downtime, no one likes downtime. And so many of our customers want to run an older version of an engine. They don't wanna upgrade. The business isn't ready for it, it's their peak season. Maybe they're having a conference in Las Vegas and they have a change control window that's closed, like whatever, they don't wanna upgrade. And so we don't wanna drive customers upgrades on our schedule. And let's face it, in many cases it's not on our schedule, it's on the vendor's schedule, because they're gonna release patches when they release patches, not when it's convenient for us, and certainly not when it's convenient for our customers. And so as a result, we do not treat the database container as a security container. We can't. And so we take security at this layer seriously, we do all sorts of patching, we keep up, but fundamentally, we made the decision that we can't rely on this layer. That's OK though. The headnote is single tenant. It's not only dedicated to a single customer, it's dedicated to a single database for that single customer. But the rest of this infrastructure, the storage system and the control plane, are multi-tenant. And so the control plane includes the API endpoint that all of our customers interact with. This is how you allocate databases and scale them up and things like that, and also the API endpoints that Host Manager interacts with. And so, the, the storage network holds all of the Aurora storage. These are all of the bits for all of the customer databases in that region. This is the foundation of our threat model. When we're thinking about threats to the service, we consider the head nodes to be sacrificial. We don't want people to gain access to the underlying host, but we assume that they can, and we have to prevent them from moving past this host. They can't gain access to shared storage or to the control plane. And so this year, in, I think this very hotel, uh, at the DEFCON security conference, Tail and Kobe from Veronas presented research that they'd done on the Post-res database. This was a novel attack. This was a zero day. This was a new vulnerability that they'd discovered in the engine. PL Perl is an extension to the database. It allows you to add new functionality to the database in the Perl programming language. This is an example of one of those deep rich interfaces that our customers love to have access to. Well, great, it's not a predicate for a query, it's not some simple constrained thing that you can test deeply. It is a complete code execution engine. So if there's gonna be a bug in the database, this is the kind of place I would expect there to be a security bug in a database. So, Tal and Kobe have found a new way to elevate privileges in Post res, and they attempted this technique against all of the cloud providers that offer Postres as a service. This is a zero day. Previously unknown, no patches available, because no one at Postres even knew that this existed. We're running postgras, so when they tried this on Aurora, it worked, like it's, it's the post gras front end. They have a bug in the post rez front end, of course it worked. But this is what they wound up presenting about AWS and Aurora on stage. The novel attack worked on the database, but it was ineffective against Aurora, despite the fact that this was a previously unknown technique. We stopped further pivoting or exploitation, even though we'd never seen this attack before, and we got some nice kudos from the researchers, which was delightful. How did we get there? The answer is not flashy. There's, there's, there's no smoke and mirrors here. We started a belief. That taking a database designed to be installed and operated by its owner and trying to treat it as a security container as part of a multi-tenant service was not feasible, it was just a non-starter. And so we have defense in depth, multiple layers of detections, automation, and this has all been built up over years of investment. We keep testing what we've built and improving it. It's not a flashy answer, but it's one that we're, I think justifiably proud of. So let's see how this event unfolded. First, the researchers installed PL rust on their instance and rebooted it in order to start experimenting. And within about an hour, they'd gotten their exploit working and we started getting alarms from SE Linux, which Andy will tell you about. Now, this is new functionality in the database. And these alarms aren't finely tuned yet. And so our initial response, we weren't sure if this was valid PLSQL behavior running into one of our SE Linux policies, or if this was some nefarious activity. And so these alarms went off, but they paged the service team. They didn't page security. However, as the researchers continue to move around on the box, They tripped another trip wire in a system that we call Chronicle. And these are mature alarms. These paged the security team immediately, and so that brought the security team into the uh uh into the mix. And so this was not a new account. This was not a fraudulent account. This was a mature customer, they had a long history. They were paying their bills. And this instance was not a newly launched instance. It wasn't tagged research or anything like that. This is not a researcher that had coordinated with us ahead of time. And so we're looking at this as a valid customer instance. We're watching it carefully, we know what's going on. We know they haven't pivoted, and so we engaged legal. And we, we, we, we talked about our path forward. We made an intentional decision here. And so had we had any evidence that they were moving past that head node, that they were successfully pivoting, we would have taken immediate action. It turns out that this customer had a TAM, a technical account manager, and so we reached out to the customer via the TAM, and we eventually took the decision to basically cut off all network access, which caused the database to go into a storage failure mode. And so, it's important to remember that the purpose of this mechanism is to protect Aurora itself. We offer guard duty advanced threat protection. And that is a service that customers opt into. And so that service can look at queries, it can look at access patterns, it can look at sources of connections, and it can alert you as to whether or not this behavior is anomalous or not for your database. But this is us acting on our own behalf. We've made some deep privacy and security guarantees to our customers. We've told them that we will not look at their data. And so we can't tell if this activity is anomalous or normal. It might be a DOS attack, it might be the best day for your business, and we can't tell the difference here. So this is for us to protect Aurora. It's always on. It's an inherent part of the Aurora system. And again, Our choice here is leave the database running or stop the database. And had we seen further activity, we would have immediately stopped it. To share some detail on how we achieve this result, I'll hand off to Andy. Thanks Eric. So let's look at some of the security controls in Aurora that helped us get to this outcome that we just described. Um, we have a number of controls in place. To start, I'm gonna talk about 3 of them. SE Linux, uh, Chronicle, and the VPC flow logs. So SE Linux, uh, permissions, Chronicle is our telemetry service, and we tune both of those to be fairly aggressive, and we treat everything from the head node as untrusted. And one of our red team members has actually done presentations uh internally within the company for our builders to learn about poison telemetry and other risks so that they harden the operational consoles they use to run these services and perform common activities. So again that's an example of working together with security and the engineers who are building the system from the ground up. So the first control I want to talk about is SE Linux, and this is security enhanced Linux. It was written by and released by the US National Security Agency in 2000, and it is a kernel security module that allows a number of stronger security controls like mandatory access control to be enforced. Um, it allows you to get very granular with the access of your resources, the permissions, the processes that are doing that. Um, it also has a lot of people complaining about SE Linux because it takes deep understanding and nuance to configure and tune this complicated system. And when you run it on a general purpose computing platform, it is tedious or it can be tedious and frustrating, right? By default, it will block a lot of activities that you may have intended to succeed. But in a specific and constrained environment like Aurora, we know exactly what processes are running and what resources they need access to, and which resources they don't need access to. So these are two of the SE Linux denials that occurred during the research that Kobe and Tao were doing. And this is dense, but you can see that the cargo process, which is the rust package manager and build tool, is getting denied when it tries to execute code. And you can see that that code was coming from the Postress engine for uh RDS and Aurora. So now looking at this head node diagram, you can see where Xity Linux fits into the headnode. And by running the environment with these strong controls, we're able to detect and enforce resource permissions, process execution, and more. And we study the history of the necessary database activities and we design the rules in concert with our security engineers to prevent inappropriate access. So Chronicle is a telemetry service which has an agent running on all of our AWS hosts. And it allows us to monitor process of execution, uh, specifically the exec VE prosecution er invocations. Kernel loads and unloads and more. And it transmits these events to a service and we can query those, uh, both security engineers and the fleet owners to analyze the data and create alarms. And as we mentioned earlier, general purpose compute environment like EC2, they're gonna be a plethora of different possible expected activities. But in the database server, we don't have a diverse set of services running. We have um alarms that have actually triggered because a database engine upgrade changed behavior, right? We make strong privacy promises to our customers, and we can't look at customer content without explicit customer consent. So even the engine log files are redacted when we view them in an incident like this. Um, we, we basically see that an error occurred and we see a time stamp, but we don't get the, the error itself because that might have customer content in it. But when we get the chronicle alarm. We can see exactly what's happening outside of the database because that's a process that's running. So we got a chronicle alarm for Curl, uh, which happened in this case. That's not expected behavior, and we can react very quickly to that. So going back to this, we've now added Chronicle, and you're seeing this broader picture unfold. And I mentioned VPC flow logs. So these are network logs that allow you to uh specify a level of granularity on what interfaces you want to capture the data and then track that IP traffic, um, the connections between resources. And within Aurora, we monitor these to detect unexpected traffic, to uh see attempts between VPCs. And we can use network excuse me, network isolation to restrict which VPCs can communicate with each other. And for example, customer VPCs can't talk directly to our storage VPC. And this combination of security groups, private subnets, and the VPCs allow for robust network monitoring and restrictions. So this is an example of flow log entry. Uh, it's fairly inscrutable at first, but we're gonna break it down, and I'm going to, uh, deemphasize fields like version number and and timestamps so that you can look at the relevant information. This is an account that ends in 010. It has a network interface that ends in 789. We're seeing that the account is sending traffic from the network interface that ends in 139 to the end, uh, the destination that ends in 21, and it's on port 22. And finally that traffic was accepted. So now you know how to interpret a flow log, you can imagine that having massive amounts of this kind of data allows us to analyze and tune our queries and our analytics to get really valuable information out of this traffic. So A little bit more readable interface for how that flow log represents the data. And now we have our VPC controls. Restrict access to both the control plane and the storage VPCs to only the ENIs that we allow, and we combine that with IP table's firewalls, and capturing those flow logs and continuing to monitor them for anomalies. So in addition to the 3 controls I just discussed, we have a number of other ones as well, that didn't come into play in this specific event. Um, but they provide protection against a wide variety of threats. The head nodes need to make API calls, and we could have used EC2 rules for instances to restrict what they could do with the credentials, uh, but that would mean that every head node has the same policy. And so that would introduce a possibility of cross instance privilege escalation. Um, instead, we have an instance, we have an instance role that only has permission to access the token broker. And then that authentication for the instance uh gets an AWS API key, and the key is scoped down to only exactly what that head node instance needs to do. Right? Similarly, our host manager APIs are only invoked by our internal head nodes. But we built those as internet facing APIs to provide another layer of defense and depth. If there was some privilege escalation on the head node. We're still not losing a trust boundary because Eric mentioned earlier, we don't consider that a trust boundary. So We have the continuous evaluation of SE Linux denials, of the chronicle detections, those are good, but we're also looking for uh what we call dogs not barking, right? We use a number of canaries to test these controls to make sure that we are getting them correct. And this is expensive, but the trade-off is not one unit of security engineering work to protect one instance. It's to protect millions of instances. We are amortizing this cost and this investment over a vast scale of the service. To be blunt, even if we started today with a near infinite gen AI software engineering capabilities, we could not rebuild the service overnight because the value of years of experience and analytics of this data is fundamental to what we're building. So we just told you a bunch of great things. What are our blind spots, right? We're proud of the work we've done. As we continue to build it, we know that we have these inherent biases. So how do we use repeatable verifiable mechanisms to demonstrate correctness, as opposed to good intentions, or just hoping everything's OK. What's for our customers. You rely on us to deliver delightful experiences. Uh, part of that is holding the high bar for security, right? We don't can't just follow best practices. So we invest a lot in proactive security. We have a strong program. I'm gonna discuss three components of that. Uh, first, our application security program has well defined processes for threat modeling, risk identification, um, security evaluation, and the security engineers work with the builders at every phase of the software development life cycle to build security in. And this is from ideation and design through implementation, the use of secure, uh, security focused application libraries, testing, deployment, monitoring. And we do penetration tests as part of this, and those are really valuable, but they're intentionally scoped and they verify the expectations of system behavior and validate the controls we have in place. We also have a bug bounty program, uh, and we work with top security researchers in the community. In fact, I fly from here to London to do an in-person event, and we have invited experts. We have researchers who have already demonstrated a track record of finding critical issues. We are confident in our services. Uh, at a recent event, we actually gave those researchers production access, uh, excuse me, route access to non-production instances to see if they could break out and access the multi-tenant storage, and they were not able to. So we have external researchers that investigate our systems, and that's great because they're independent, but we also employ some incredibly talented offensive security engineers in the company. And our database's red team is designed to simulate sophisticated threat actors and external adversaries, um, and essentially has an unbounded purview. Right, this is unfair by design. It is an open book test. They have extensive knowledge of, of the internals of our systems, the defenses, and they routinely find novel issues in the database engines themselves. Um, and then this is just a list of a few from the last few years that they've reported. But they don't just find an issue and report it and then move on, right? They work hand in hand with our builders to demonstrate how they're thinking, how they're approaching this, what controls would have stopped them, and then they validate that with the builders. So that this is a virtuous cycle, and it is constantly evolving and reinforcing itself and our confidence in these controls. Uh, they also help when we're doing forensic analysis. So in one instance we were able to detect some suspicious activity on a customer instance. We reached out to the customer, and their security team had hired pen testers. Their operations team didn't know that. They didn't catch them, but we did, and they appreciated the heads up. So, let's go back and revisit the event with all this new knowledge and see how the pieces came together. So we start with the researchers installing PL Rust, PL Pearl on their instance and rebooting. Well, from the beginning, the network isolation means they are prevented from interacting with storage service, the VP the control plane VPC. They only have access to their single tenant headnote. And this is where the SE Linux alarms are appropriately, uh, excuse me, denials are appropriately blocking access from cargo trying to execute commands. So we can see SE Linux now doing those denials. It's a new control. So as Eric mentioned, these are getting routed to the engineering team. So the engineering on call start looking to diagnose it. Like, is this unexpected upgrade behavior or is this something else? Well now Kobe and Tal have been working at it for a while, and they are able to run code by nesting uh PL Pearl functions with PL rust functions which leveraged the Rust GDB uh environment variable, and they were able to actually run a shell command. OK, well this is when we get chronicle alarms about Curl. And these alarms are well tuned. We have extremely high confidence in them. So this immediately pages the security on calls. So during that call, uh a security engineer in his first ever on call. Is paging red team, uh, escalation, legal, everybody, and we're able to move quickly but not rush because we have confidence in the controls that are in place here. And this is not expected activity. The customer account isn't part of our bug bounty program. As Eric mentioned, it's a mature, paying customer. So they have production workloads running, and we can't just terminate instances. Uh, our red team lead is actually reaching out through the community and figured out it was Kobe based on some metadata in the database name. And then we're engaging with the TA. This is a, a customer in another country, another time zone, so we're waking people up in the middle of the night. Um, but we're handling it responsibly and we're not disrupting their operations. And that kind of nuance and high judgment is difficult, if not impossible to encode into an AI response agent, right? Having humans in the loop matters here. Once we made a decision to snapshot the instance and move it to storage failure, uh, rather than disabling the entire account. We actually confirmed with Kobe and Tal what was happening, um, they were pretty surprised, uh, but we were able to safely terminate the instance, and now I'm gonna turn it back over to Eric. Thanks, Andy. So The way these things usually go down is the adversaries have done some sort of testing. Like any of you could install Postgress on your laptop right now, and you could work out this chain, and you could figure out all of the dependencies you have, and you can script it up, and you can run it and it executes like that. Takes a couple of seconds, certainly less than a minute, and you've got control over this node. And one important thing about what Andy walked us through is that the SE Linux denials actually worked. We stopped the chain. They were not able to execute cargo. Like that's great. So now we've taken them from their well designed, their baked, their tested plan, and now they're innovating. There's a ton of detail in here that Andy just alluded to, like PL Pearl pivoting to PL Rust, and then abusing an environment variable in the debugger environment. Like, these were smart researchers, but they were innovating in the middle of an attack. That places them at a disadvantage. That's a good place to be. And so, even though the SE Linux denial was not sufficient to prevent them from gaining access, it was a huge speed bump. It immediately killed their momentum. It made them start experimenting, it made them start working on new things. And the timeline in the first couple of steps here is our. This was not them coming in and when the lightning strike, gaining access. This is them trying more and more commands. And it was only after they were able to execute Karl, which should never ever be run in this context. And you know, the first thing you're gonna ask is, well, why doesn't SE Linux deny access to Carl? These database engines are complex, and it turns out that they shell out to curl on their own. There's all sorts of remote data connectors and things like that. And so the curl binary needs to be present, but we know the patterns in which it's called. We know what the parent process is gonna look like, we know what the environment is gonna look like. And so the binary is still there. We got our pager ticket, we were able to respond very quickly. And so just having a partial protection in place, slowing them down, jolting them out of their groove. And making them start innovating gave us the advantage. It changed the, the, the balance of power here. And over time, the SE Linux controls get tighter and tighter until they release a new database engine and it introduces new behavior. Like this is, this is a forever job. This is not something that we ever get to finish. Another wrinkle with what Andy just walked us through is systems like VPC flow logs are chronicle. We designed the collectors for these to be as simple as possible. I, I, I tell people to keep them stupid, because the Chronicle agent is deployed on literally 10s of millions, 100s of millions of hosts around the world. If it starts consuming excess memory, like how many exabytes of memory are we gonna have wasted on this agent? And so the more complex functionality we put in the agent. The more likely it is we're going to have a broad problem on our fleet, and we've made promises to customers that we won't have correlated problems across availability zones. We won't have correlated problems across regions. Well, if the chronicle agent starts eating memory in every region, we're gonna break that promise. So we keep these things simple. And one of the best parts about securing a cloud is that you have a cloud. And so we're hoarders We collect these chronicle logs, and we warehouse them. We have VPC flow logs. Last year at Reinvent, I gave a talk on Sonaris and active defense. We have VPC flow logs for every ENI. And so, when we get smarter. We don't have to do a fleetwide deployment. We already have the data flowing through our analytical systems. We already have the data flowing into S3. And so when we get smarter and we want to build a new detection, we just build the new detection and we deploy it to the analytics fleet, which is dramatically smaller than the entire cloud. And we can then use the logs that we've got stored as a time machine. And we can say, in the past 30, 60, 90, 4000 days, have we seen this behavior? And we can definitively say we have or have not seen this behavior. It's incredibly valuable. And so, as we build these systems in the future, I really like this design where you've got the thinnest collector possible. Smart analytics in a central system. And then a data warehouse. And the data warehouse is absolutely a cost consideration. Like, you can, you, you, you can burn a lot of money in S3, and so you get to tune the dials there. What do you keep? How do you cook it? How long do you retain the cooked stuff? How long do you keep the raw stuff, but it's incredibly valuable. It is like one of my favorite things about working in a cloud. And this is another example of a different design point. If you look at a typical client endpoint, an EDR agent or something like that, like, I might be on terrible Wi Fi, certainly not a reinventor. Wi Fi at reinvent is awesome. But uh I might be on a cellular connection, I might be intermittently connected, like who knows. And so you have to make that agent smart. If I double click on a file, and it has to call out over, you know, cellular tethering, and you know, it takes 40 seconds to open that file. Like the tickets are gonna roll in, the users are gonna burn me in effigy, and that agent's gonna get uninstalled. But in the cloud, everything's in the cloud. S3 is a millisecond away, it's glorious. And so this design point for our cloud-based systems has served us incredibly well. So We believe that this is unique in the industry. Um, I'm not aware of any capability like this anywhere else. Aurora itself is already unique. It's a, uh, an off the shelf code and SQL compatible database that provides you multi AZ multi-tenant performance. Like, it's incredible, but this is further, um. And Dealing with the database, even when you've got RDS, even when you've got Aurora, owning a database isn't free. These things are baked deep into the core of your applications. Like this is, this is the standard thing. Remember three-tier applications? Like, this is it, this is where all of your data lives. And so upgrades are hard, changes are hard, it's a huge testing burden. And one of the, the, the paths that we could have chosen to take is, look. We're offering you this service, we're making security promises. In order to deliver on those security promises, you have to keep up and you get this window. And if you don't patch within this window, I don't know, we'll force patch you, we'll turn your database off. There's no good answers here. Like, in this case it wouldn't be the users burning me in effigy, it'd be the customers burning me in effigy. Like this is a bad path to go down. And so we didn't go down this path. As I opened with, we also decided that we couldn't possibly treat. A database is a security container. And that left us with a harder problem. We have to figure out how to enable customers to run whatever database it is that they think that they need to run, and it's not my job to tell them that they're right or wrong. Like this is their choice to make. I'm not deeply familiar with the constraints of their business, um, you know, maybe, maybe there was some regulatory discovery thing and they had to exhume a five year old snapshot of an application like. I, that, that, that's what they need, like, fine, we'll support that. And so. If we learned of a security issue in a database. And that became critical to the safety of our service. You know, if a customer chooses to run an old version of a database, they're doing that with their eyes open, they're taking on some risk. That's fine, that's their risk to take on. They're allowed to place their database at risk. They're not allowed to place the service at risk. They're not allowed to place other customers at risk. And so if we learn of a security problem in a database. And we have to tell customers like now in the middle of a SE 2 pager ticket like you're upgrading like it's not the right customer experience and so. Add in the fact that this particular CVE that the researchers found was a 0 day. We didn't know of the patch, cos there wasn't a patch. No one could have prevented this, because it was previously unknown. Like. Had we gone down the path of treating the database as a security container, this would have been a real problem for us. As it was, we handled it reasonably well. And so, this is an example of working backwards from our customers. You hear Amazonians say this all the time. You know, we've got this set of leadership principles, and the leadership principles are an unordered set, but every single time you see the leadership principles printed out, customer obsession is the first one. And so you hear us talk about working backwards from our customers. What is the right outcome for customers? The right outcome for customers is their patching timelines aren't driven by us. Their databases aren't constantly taking down time. And so this is where we wound up. We've made sure that the security of our service does not depend on the security of the engine itself. And this is an inherent part of Aurora. This isn't something you have to configure, this isn't something you have to turn on. And so while it doesn't provide the kind of granularity that guard duty provides, it's always there. 100% of Aurora instances have these mechanisms on them. And while its primary purpose is making sure that the Aurora service is safe. It makes the tough decision for on the customer's part to run an older database, a lot easier to take on. Had we seen pivoting here, had we seen malicious activity, had we seen data exfiltration or destruction, we could have shut the doors. And this is another example of our deep ownership here. This is not something that you can do in a database engine, this is not something you can do in a database. It's something you can only do in the context of a service. And because we own all of the layers of this stack, we're able to to work backwards from the right customer experience, from first principles, and provide our customers with unique protection, in this case, even against zero day vulnerabilities. And so this talk was a deep dive into a single aspect of security in Aurora. It's not an exhaustive list of all of the security mechanisms we have. It didn't talk about our extensive use of encryption on the network and in storage, our integrations with KMS where you can choose which keys get used and control access to them. Um, there, there have been many talks and many blog posts and many papers about those features, but that's not this talk. We wanted to peel back the covers a bit and give you a glimpse into what goes into securing a system like Aurora. The result is something that we're really proud of, but it's not the end. There's no clever one simple trick here. Our job was definitely made easier by the investments that we'd made, you know, Chronicle is used broadly across Amazon, that was a building block that we could, we could take advantage of. VPC flow logs are ubiquitous. They come with the AWS services, that was something that we could build on. And we have a a world class streaming analytics team. Like, really, really good at it, because we stream so much data through. We've refined these systems over years. And so it's this endless process of iteration and improvement. And I, we've made it most of the way through this talk, and I have not said the term. I will now say the term. I'm contractually obligated to use the term, I'm sorry. Gen AI Is gonna change everything. And one of the things that we have seen people doing is using Gen AI to generate variants of attacks. Variants of shell code, variants of sequel injections, variants of everything you can imagine. And so as service owners here, our lives are gonna get exciting. There have been a bunch of papers, a bunch of blog posts across the industry about people using Gen AI to do code analysis, to find novel vulnerabilities, like turning it into an Oday factory. We're expecting the pace to continue to accelerate, and so we're keenly aware that our job here isn't done, that we can't rest on the on on our laurels. And We're proud of this, we like what we've built, but one of the huge ingredients here is our red team. And I described the red team as world class, like there's that list of CVEs that they found in a whole bunch of different database engines, they go deep. But the thing that makes this red team world class is the fact that in many organizations, a red team is a breaker. Their, their goal is to make the biggest crater they can. It is to, to, to demonstrate their prowess as a force of evil. And at Amazon, our red teamers are builders. You do the thing, you gain the access, you plant the flag. And your work has just started. You now need to go back to the team, you need to educate them, you need to make sure they understand not just what you did, but how you thought about this, what led you to this path, what all of the different little things are. Because unless your service is brand new and you haven't invested in security, there's no obvious path in. It's a little bit of access here, a little bit of access there, and information disclosure over there, and every time you close one of those, you make their job harder. And so our red teamers stay engaged. Until the thing is fixed. That's what they're gold on, that is what their, their aim is, is to fix things, not to break things. So thank you very much for joining us here in Las Vegas. Um, there's not that much left in the conference, but I hope it goes well for you. I hope you enjoy replay and. That's it. That's the recipe. Please do fill out the survey.