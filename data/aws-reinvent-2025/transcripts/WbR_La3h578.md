---
video_id: WbR_La3h578
video_url: https://www.youtube.com/watch?v=WbR_La3h578
is_generated: False
is_translatable: True
---

Imagine this It's a fine Monday morning, like today. And you get a call from your CEO. They're not happy that their dashboards are not loading. And you start digging into it, and you notice that the data science team has kicked off a large training job over the weekend on the same compute. And now you are starting to juggle and trying to figure things out. Does this problem sound familiar? OK. If so, you're in the right session here. We're going to be talking about scaling with Redshift's multi-warehouse architectures. My name is Naresh Chanani. I'm the engineering director at Redshift, and I'm super excited to welcome Alex Rabinowitz. He's the data, uh, he's the director of data engineering at Vanguard. And Anusha Chala, she's the senior specialist, uh, architect for A at AWS. So together we're going to walk you through some of the best practices, uh, around multivious architectures. Thanks, Nash. Thank you. So here's an overview of uh the flow for today's talk. So we're going to start with an overview of some of the design patterns of multivirouse architectures, starting with the problem statement and how some, what are some of the emerging patterns we see on our massive fleet worldwide. Then we will take a sneak peek behind the scenes on what powers multi-warehouse architectures at Redshift. And at that time, we'll segue to Alex, who is going to walk you through Vanguard's journey with, uh, uh, with how they have scaled over the last couple of years, uh, with multiviros architectures. We'll end with a demo. This is a demo that uh Anusha has specially prepared for today to demonstrate how quickly one can start with, uh, with this architecture, some of the best practices, and we hope, uh, we hope to leave some time for Q&A, uh, but don't worry if you have questions and you don't get to it, we'll also hang out after the session and happy to chat, learn from your experiences. A show of hands, how many folks here in the room are already using multivira architectures in red shift? OK, so we have a few experts in the room. That's great. This should be a great conversation. Before we dive deep into design patterns, let's look at the problem statement. So what you see on this picture here is this large hexagon that resembles a monolith compute, a single compute cluster. And what you see running on this are a bunch of applications. So you see streaming and batch ingestion, a couple of writers, uh, writer applications, so real-time ingestion and batch ingestion, and then you see a bunch of consuming teams, things like the science team, dashboards, etc. And this, this cluster, this architecture scales, however, beyond a certain point, you run into challenges like what I started off with, uh, and this is mainly around workload interference. So what happens is like the batch ingestion, if it has a ton of data to be ingested, that can start interfering with your ingestion SLAs if science and dashboard teams are also trying to do the work there. This kind of resource contention and workload interference can be completely addressed by the first pattern we'll introduce today, and this is what we refer to as hub and spoke. So see that large compute cluster that we had, the big hexagon. Now that's broken up into smaller parts. A couple of advantages here. So first is, you see, everyone gets their own endpoint or compute. So you have your streaming injection, batch injection. They, they are sized based on their compute needs. And the great thing there is you can also introduce chargeability. So for example, the science team might have a certain budget, so your size, you scale independently, and most of all there is full workload isolation. So every workload proceeds independently without having to coordinate or interfere with any other workload. A good test of any design is not that it just works today, but it's able to extend to the needs of tomorrow. With this pattern, you, um, you can spin up additional endpoints. Say for example, you have quarter end processing and you need a pretty large compute to make sure you meet the SEC guidelines for reporting, and you can just spin up within a matter of minutes, have another compute attached. In the center we have something called redshift managed storage here. So that's, that's your redshift data. And with, with this technology, one question you may have is, hey, are we creating copies of the data? And the answer is no. If in your businesses you see. Reasons that you're creating copies, I would highly encourage you to take a look as to because creating copies becomes a problem, especially as, uh as data volumes grow, governing copies becomes a nightmare. So you want to have a solution where there is a single source of truth for data and uh in this architecture you have a, like when the streaming injection batch injection are both writing to that one copy of data. And all the consumers are consuming it using snapshot isolation. So you get all the benefits of transactions that you're used to for your data warehouse. This is a flavor of this architecture, and this is at AWS we love hearing, seeing how creative our customers are. One of the things we saw several customers, and especially these are the multi-tenant ISP providers, they took advantage of this multi-warehouse architecture and the capability where compute clusters can be in different regions. And the idea there was, is, is that you can have tenants in different parts of the world. So you can have compute that is actually closer to where your tenant applications are, and this is where the physics of network latency helps in, in terms of the overall experience. Even in the multi-region use case, uh, uh, there is again one copy of the data that is stored in the primary region. The great thing is now you can also provide uh different qualities of service, different tiers to your different providers. There might be some very large, uh, customers that are that can benefit from larger compute, or you can club a bunch of smaller customers into a single endpoint. A lot of, uh, flexibility there. Let's look at another problem and then a design pattern. So this is very similar where now again you have a monolith cluster and this time instead of different applications you have different team. This is what you typically see at large enterprises. You have finance, marketing, HR, all different teams, kind of, sometimes silos within the org that are sharing. And this leads to complex coordination once again where say for example if finance is trying to get their month-end closing and marketing is trying to figure out an upcoming campaign, now they have to coordinate if their if their needs are not predictable. And one thing we have seen with data increasingly. is a lot of new use cases because you don't want to handicap or handcuff yourself with sort of use cases that you can derive, explore what values, especially with data and AI coming together. So once again, in the same pattern, you are able to break up that monolith into multiple parts. In this case, every team now has their endpoint, and this endpoint for HR finance basically chargeback is now very clean where you charge it back to the business unit within the enterprise. This architecture, so remember the previous one was Hub and Spoke where there was a centralized team that curated rights and opened it, uh, opened it up to a bunch of consumers. In this case, every unit, every enterprise unit owns their data, their portion of the data. Finance data is different from, for example, R&D data. So this architecture is referred to as data mesh. In this architecture, the data owners decide who they want to share their data, what data they want to share it. This sharing can be at either a database level, schema level, table, or you can go even more fine-grained at a row or column level, and we'll talk a bit more about that. Let's now take a look at behind the scenes of what powers multi-warehouse architecture, etc. shift. So at the, at the storage layer, uh, what we have is redshift managed storage. So this is a high, uh, high performance columnar. This uses some of the most advanced and proprietary encodings to, with, with the goal of cash efficient processing and CPU friendly uh pipelines. We actually take advantage of. Uh, hardware instructions to optimize the line rate at which we can process the data. And this is where you see the, some of the performance benefits from Redshift when it is powering your two-second SLA dashboard, for example. On the compute side, You have a mix of redshift and provision, and there is one emerging pattern that we are seeing with our customers is in hybrid architecture where you use some provision for your steady state, predictable, always on kind of workloads, while, and, and a bunch of serverless for your line of business units for to, to handle use cases for sporadic access, spiky access where properties of serverless like uh scaling up and auto scaling are beneficial. Uh, you pay only for the computer you're using, and the nice thing is you're in this architecture you can mix and match with either hub and spoke or data mesh. You can have any mix. You can go all provisioned, all serverless, or a mix. We'd highly recommend you to look at serverless. Customers love the ease of use that it brings, and again it's able to query the same single source of truth when it comes to the data layer. These compute clusters, they don't have to be in the same AWS account. You can spread them across multiple accounts, or you can, these can be across regions as well. And this last point can be important, especially for our colleagues that have to deal with data sovereignty. EU tends to be far ahead in terms of what data can leave the boundaries of the EU. And there are some amazing stuff around permissioning that comes into the picture, which I'll talk, talk about in a few minutes. So this is a new capability that we uh launched last week. This unifies governance for your multi-warehouse architecture. So, so far, I multiple times in my, uh, in my talk so far, I mentioned how there is a single copy of the data. One of the pain points our customers told us about is, well, I still have to replicate permissions. As a stand-up, these are additional, uh, compute clusters. I have to now again put my fine-grained access control, my permissions, and, and that was becoming. Making it harder for them to go scale, uh, beyond a certain point. Just the, uh, the act of managing those, uh, permissions consistently. So what happens with federated permissions is There are The data owner, so let's look at an example. In this case, the finance team, they want to have a fine-grained access control policy, so they create a low-level security policy, and in this case, this policy is tying it back to the data sovereignty where the user that is logging in, they must be from the region that uh for the customer data, data that they're trying to access. This policy is then attached to, to the customer table, OK. And the policy and then the roles, the there is the IDC, the Identity center, uh, there is a sales role, uh, for which these, uh, policy is then assigned to. Now, when a sales analyst logs in. Using their own computer cluster, they are just squaring this customer table as they normally would, and at that point, they only see the data that the policy allows them. So they will only see the customer data for the region from which they belong to. And so, once again, this, this feature is federated permissions. Uh, highly encourage folks to take a look at this launched last week. It's still rolling out worldwide. And the whole idea is just like you have one copy of the data, you also have a single copy of your permissions. So we talked a bunch about um um Redshift's managed storage. So this is your highly optimized data for analytics uh which is stored in Redshift. The other trend we see is more than half of our largest customers are also using, in addition to Redshift, also some lake data. So, and, and Iceberg is an emerging trend we see across AWS. OpenTable formats like Iceberg are extremely important because they provide you the transactionality benefits. Which are important to many applications. And then you also have the flexibility that any consumer, any engine that can read iceberg data because it's an open standard, then you have the interoperability. With Redshift, you can combine data that is in Redshift with your iceberg data or Parquet data. In fact, the same query can even join data across these two tables. So if you have, I don't know, say landing, landing zone data that's in your lake, and then you have highly curated data. So, as, as you are building applications, you are joining them within the same unit of work. This year, Since the beginning of the year till today, we have launched more than 2X performance improvements to Iceberg query processing when you're querying using serverless, uh, Redshift. And, and this is just keeping up with the custom. There are a bunch of things around advanced statistics that we collect on the data and, and including the high performance. Some of the query processing techniques that we have for Redshift, we are actually extending that to Iceberg. And we are not done yet. This journey is going to continue. We also introduced, uh, rights to Iceberg. So, so far we have been talking about rates, uh, rights for append-only workloads that started, that was launched a few weeks ago. OK, so we talked a bunch about redshift. However, analytics world has more than redshift. So there's a data warehouse, and then there is an ecosystem around it. So how does, how does Redshift fit in that and how do you extend and leverage that entire ecosystem? Last year, we launched SageMaker Unified uh Studio. This was a landing place for all things data and AI. Show of hands, folks here using Sagemaker Unified Studio today. OK, I think I see a couple of hands. OK. So what, what's SageMaker Unified Studio? It's uh basically supposed to be a one-stop shop. It has, you, you have your note, your SQL notebooks, your Jupiter notebooks, and, and you have natural language to query abilities. So the whole idea is to be able to build your data and AI applications in one place. Uh, at a, at a storage layer in the Sage Maker platform, you have all your data. This includes redshift data. This includes Iceberg, and this iceberg can be either self-managed in your regular S3 bucket or it can be managed by AWS in S3 tables. One of the advantages why you may want to consider S3 tables is you actually get better performance because S3 takes care of heavy lifting like compaction. Small data files tend to be a problem when accessing lake objects, so S3 by compacting takes sort of a heavy lifting away, and, and this is going to continue with more and more stuff coming in collaboration between engines and S3 tables. So I highly encourage folks to take a look at this. Uh, all of this data is then visible into the SageMaker catalog, so you can discover and govern that data. And so far we have been talking about how you can use a mix of redshift serverless and provisions. So all your data warehousing use cases are satisfied. Now, taking a step ahead, right? So you may have other engines like Athena, or you may have a Spark job that is doing data processing transformations. Uh, all of that is part of the same, uh, experience where you are able to consume any of this data, say, for example, from Spark. And, and the key thing that enables this is Uh, all of the interfaces are using, uh, Iceberg's, uh, IRC, which is the, uh, Iceberg Res Catalog API. It's an open API. Um, then you have your BI and orchestration tooling and the Quick Suite and, and the airflow to orchestrate your, um, your jobs. And last but not the least, then you have your Gen AI applications, and we are increasingly seeing a lot of my conversations, uh, this year with our customers have been, hey, how do I make it easy for my agentic applications to consume my redshift data, and I'll talk a bit about that in a second. We, we see data and AI are really two sides of the same coin. It's, uh, the right context can make really a huge difference to the efficacy of your, uh, agentic AI applications. Remember I mentioned that the key thing here is all the access is through the IRC, the Iceberg Rest catalog API. What that means is that also gives you the power that this, uh, access to the whole ecosystem is not limited to AWS, uh, tools and engines. You can actually bring any third-party engine that speaks that API. OK. So as promised, I'll, I'll close with a couple of um uh use cases through how GAI use cases uh of what customers are doing today with Redshift. First and foremost, uh, through either the SageMaker Unified Studio or query editor, your analysts can chat with the and their data. This is an extremely easy, uh, solution. It's like helps with productivity. No longer your analyst has to go first talk to your development team to build in SQL dashboard before they can start exploring. So, that's the first, highly encourage folks to give it a try. They, you'll, you'll the, on the query editor, it's Amazon Que Generative SQL. So you'll see this branding around queue for all the Gen AI uh applications. Next up, uh, remember how we talked about how data is important for, uh, the, the reliability and of the inference and the accuracy of inference. Uh, so you can actually use your Amazon Redshift, uh, data as a knowledge base for your Bedrock, uh, applications. We have, there used to be a time where you had to unload the data, make it available. All of that has been, uh, greatly simplified. We took it a step further. From your SQL command line, Redshift SQL command line, you can invoke your uh um uh Bedrock directly. So in this example here, so we create an external model that's say, pointing to anthropic cloud, and here you take the reviews table, you're actually accessing, you're doing sentiment analysis using cloud from your HF SQL editor, um, in this example. Lastly, we, uh, for a lot of AWS services, we have, uh, MCP servers, model context protocol. The whole idea is as the orchestrator agents are coordinating to build, uh, your extensive agentic AI applications, just making it easy and be part of the MCP standard. This was something that we launched about 3 months ago. Hey, at this point, I would like to welcome Alex, uh, to talk through uh Vanguard's journey. With uh multi-warehouse architectures, Alex. Various design patterns on how to build multi-house architectures. Raise your hand if you are a customer of Vanguard. All right, I see a few hands. Thank you so much for being a customer of Vanguard and trusting your money with us. I'm Alex Rabinovic. I'm a director of data engineering. Vanguard is one of the world's leading financial companies, investment companies, offering low-cost products such as mutual funds, ETFs, advice, and other services. I support the Financial advisory Services division. Think of us as a B2B business. We provide. Various products and services to intermediaries such as banks, insurance companies, registered investment firms. And I'm here to tell a story of how we evolved our architecture. To meet our business needs. We started our journey with building client 360 system. Client 360 is an aggregate of our internal and external data around our intermediary firms. We are bringing data from our internal operational systems as well as the data we can purchase from outside data aggregators. We use data for various use cases. Analytics and business intelligence use cases is a major one. We use data to set our sales goals. Sales compensation. As well as put yearly goals and track those goals via dashboards that sit on top of the data. In addition, we have predictive intelligence use cases such as customer segmentation. Call transcription sentiment analysis. We also use data for real-time customer experiences. For example, hyper personalization. When our external Advisors at intermediary firms log in to advisor. Vanguard.com. We customize their experience based on what products they're looking at, that intelligence that goes into our sales team, and now our sales team is equipped with information so that we could connect with the right. Advisors and give them the right information at the right time. Several years ago, we looked at our architecture and we've realized that all of our data sources are stored in data lake in various folders. On S3 We stored data in Parque CS3. It was super hard for our data analysts to build an insight based on the data. They had to manually pre-aggregate data from various data formats using different tools. In fact, that resulted in multiple versions of truth. Because one analyst couldn't easily share their insights with another analyst. Um, there was a duplication of efforts. Poor performance, it is actually very hard to join CSV files on parquet files, right, and derive your insights. Inconsistent reporting. Sometimes data analysts would replicate the business rules and give it and hand over to another data analyst. Scaling was a challenge. There was no unified golden record. At which you could look at and put your business intelligence on top of it. We had a data swamp. We have gone into our first wave of modernization. And we decided to build centralized enterprise data warehouse, Client 360 on a redshift provision platform. We took all the data from our data lake, transformed it through Amazon AWS Glue services. And loaded the data into Amazon Redshift provisions server. We have unlocked several use cases business intelligence, analytics, as well as data science. We have seen a lot of benefits after the first wave of modernization. We now had golden source. The data was in one place. It was very easy for us to put business intelligence on top of centralized data warehouse. We have seen significantly better performance because the data has been pre-aggregated and now whenever you query, you would utilize high performance compute and storage of Redshift. We've also seen better trust from business because the data would show up consistently across different business intelligence use cases. And we were able to unlock new use cases. My favorite one is comparative product analysis, where our data analysts were able to look at client 360 data, compare the products Vanguard offered to the products that our competitors would offer. As a result of that deep insight. And deep research, our analysts recommended new products to be offered by Vanguard, which Vanguard offered later on. Here's how much data we store. Over 20 terabytes of data in Redshift managed storage. Over 150 terabytes of data in a 3 data lake. Over 600 tables and 400 views. We have around 100 active users composed of analysts, data engineers, and data scientists who place over half a million queries a month. All of that is powered by thousands of batch jobs. After the first centralization wave, we have seen exponential growth in numbers of database objects, storage, as well as use cases. We have seen Number of database objects double year over the year. And we have become victims of our own success. We have started to see resource contention issues as well as challenges with scale. As an example, when we ran our ETL job, we saw competing analytical workloads such as business intelligence or ad hoc queries that would place stable locks and such our ETL jobs. would be delayed and our SLAs would not be met. We also saw issues with peak time performance. For example, if our ETL jobs would run through business hours, our data analyst. User experience. would be impacted because they are all sharing the same compute. Workload management complexity was another challenge. We have prioritized jobs to run at a higher priority compared to analytical workloads run by analysts. Right, so that resulted in subpar user experience for our analysts. In addition, we've seen Certain database objects. Take a long time to be recreated. For example, materialized views. It would take us a long time to rebuild materialized views. In fact, for certain types of database operations, we had to rebuild materialized views on the weekends because it took so much time to rebuild them. And it put a lot of strain on my data engineering department. And we have decided to take our weekends back. We moved from centralized mannerlithic architecture. Into decentralized multi-warehouse architecture or hub and spoke model. You could see we have provisioned multiple surveillance instances on the right side. There are several instances. are provisioned by workload type analytics, business intelligence, and data science workloads. On the left side, the architecture is the same. We still bring data from S3 into Redshift provision server via AWS Glue. We store data in Redshift managed service and then we use data share. As a mechanism to expose the data to our various workloads. We have seen a lot of benefits after the 2nd wave of modernization, improved SLAs because we have seen reduced contention. Now our ETL jobs didn't have to compete with analytical workloads. We have also seen significantly better analyst experience. Because we could now put analysts on a dedicated redshift surveillance compute, analysts didn't have to compete for the same compute as our batch jobs. In fact, previously, analysts workloads would run at a lower priority. In the new architecture, analytics work cloud workloads would run at the highest priority. We've also enabled the concept of sandbox for our data analysts in the new compute where analysts could also build permanent tables so that they could do more complex analysis and insights. We call it self-service analytics. We saw additional challenges. The biggest one is centralized data ownership. As you can imagine, managing 1000 database objects in centralized. Data warehouse brings complexities such as data governance. We've also seen challenges such as single endpoint for right as our ETL jobs. they still compete among each other, and one job may place a lock that impedes another job from completing. We saw resource contention issues on the right. We've also seen cross-domain interdependency challenges such as materialized views, and that reduced our agility. And you're looking at our future state architecture work in progress data mesh. On the left side, we are loading data into S3 buckets. We transform data using AWES glue into open file format, iceberg. The key difference here is We separate. Domains of data. Into separate iceberg tables such that we could run our ETL jobs in parallel. We use incremental materialized views. For high performance and we load data from iceberg into red shift surveillance instances. And then all of that data in redshift managed storage that you see in the middle is then shared across our consumers. Which are petitioned by workloads. We have business intelligence, analytics, and data science workloads. To recap, we can now fan out workloads on the right, right, all of the data producers have their own compute. And all of the data consumers have their own computers needed. Lessons learned Start simple. Start with either provision or surveillance architecture. Single server and grow into your architecture. As you hit the limits of your architecture, be ready to rearchitect. We collaborate with our AWS solution partners every time we hit the limits of our architecture, and we work with them hand in hand. Be flexible and open to adopting new features. Over several years, Redshift team has released major features such as data share, Redshift services. And every time we saw a new future, we had to step back. And understand how, how can we take advantage of the new features in our architecture. Track metrics to measure success. We track the number of active users. Number of tables, storage costs. Queries, query timeouts. To recap, we've started with data swamp. We moved into centralized. Warehouse architecture, we moved into hub and spoke model and now we're in a data mesh world. And that is the journey of our evolution. With that, I wanted to give a stage to Anusha who is going to take us for a demo. Thank you so much, Alex, for sharing your journey. It's been a pleasure for the AWS team to work with Vanguard. In this next part, I'm going to show you a demo on how to build multi-warehouse architectures. Let's start by seeing a graph. The graph that you're seeing on this slide is an example of a monolith. Both Alex and Naresh spoke about the challenges that you would face with monolith architectures. And this graph represents in numbers, the challenges. What you're seeing in red are the number of queued queries, which means that you have submitted a query. The query did not run, but it's waiting for its turn to get executed. Queuing is one of the signs of contention. And as you can see, there is a lot of red on this graph. If you rearchitect this as a multi-warehouse, with each department having their own endpoint. In the updated graphs, you hardly see any reds. Which means much lesser contention in a rearchitected world. This is the positive difference that multi warehouse architectures can bring to your workloads. Now let's move to the demo. In the demo, I'm going to show you how to build an end to end multi-warehouse architecture. The use case that we are going to see belongs to a fictitious company called Any Company. Any company has 2 main sources. One is an Oracle database. This Oracle database has any company's sales data. The second is a click stream source coming from their websites. Any company has 3 main use cases. First, they want to ingest all this data. And extract insights from it by transforming this data by applying certain business logic on them. The second is they want to run reporting on top of the extracted metrics so that their business decision makers can use them to make business decisions. Third is they want to do data democratization. They want this data to be available for anybody who wants insights out of it. Irrespective of whether they know how to write SQL or not, so they want to provide a natural language interface using which their users can start querying this data. Through the power of generative AI. So that's, those are the three main use cases. One thing that they want to make sure is that these use cases should not interfere with each other, especially the ad hoc queries that are coming through the generative AI applications. They should not be impacting the business critical ETL or the business critical reporting. The reporting should not interfere with ETL. The ETL should not interfere with reporting. So they should be completely isolated. The best way to achieve this is through a multi warehouse architecture. So let's see step by step on how any company achieved it through a multi-warehouse architecture. First, they have created 3 separate endpoints, one for each of the workloads they want to execute. On the left, you see a data load and transformation endpoint. On the right, there is the reporting warehouse on which reporting will be done completely isolated. The 3rd is the generative AI warehouse on which they will use natural language querying to access this data. Using the first endpoint, which is the data load and transform, any company will ingest data from the sales Oracle database using a zero ETL integration. Zero ETail integration, as some of you may be aware, anybody in the room using Zero Etail integrations today? I see some raised hands. If your source is supported by zero ETL integration, it is the easiest way to bring your data into Redshift. Your data will be continuously replicated. Into Redshift. All you need to do is just one simple set up once, and in your real time you'll have the data replicated. Using, we have released a zero retail integration for Oracle this year. A couple of months back, 2 months back, we have released this integration. So if you have Oracle sources, you can benefit from this. So using the data load and transform endpoint, any company is replicating the data from the Oracle RDS database into Redshift in columnar format, so their transactional data is made available for analytics within just a few seconds. The second source, Clickstream, it's coming from Firehose. Firehose data can be written in Apache iceberg format into the fully managed iceberg tables S3 tables. So there is a separate pipeline that is loading click stream data into S3 tables in Apache iceberg format. So that forms the lake house, both the data warehouse and the data lake. They've cataloged that using Sagemaker catalog where they do permissions management. They control who can see what using the catalog. Then reporting is done using the reporting warehouse. And the generative AI apps use the generative AI warehouse. This completes the architecture. There is ingestion happening. There is cataloging happening, permissions management, and on the right, consumption of the data for reporting and for generative AI and natural language querying use cases. In the next couple of minutes, we are going to break this down and see how each of these works. The first step is to create separate. Warehouses, one for each workload, one for the generative AI, one for ETL, one for reporting, 3 separate redshift serverless warehouses have been created. In this first part, we will use the very first warehouse, which is the data load and transform warehouse, using which we will see how data from the Oracle sales database is continuously replicated into the data warehouse. Let's see a video demo of this. The source is the Oracle 19C sales Oracle database. Let's quickly inspect it. It has a schema called sales schema, which has 24 tables inside that schema. Now let's see how this can be replicated into Redshift in near real-time. So for that, we'll go to zero ETL integrations and we'll go down and say create a zero ETL integration. Give this integration a name. And then you can choose your source. For source, we are going to choose that sales Oracle database. Chosen it, and now I'm going to say replicate only sales by providing an expression, saying replicate only oracle.sales.star. Then I'll choose a target here. The target that I'm going to choose is the ETL data warehouse that I've created earlier, and that endpoint will be used to do the ingestion. Once you verify everything, you can go ahead and create the zero ETL integration. The integration is active. You can go into the integration and see a variety of metrics that are related to this integration. You can see table level statistics, how many tables have been replicated, number of rows, what changes, what size, so on and forth. Now we are ready to start querying this data. I'm connecting to the ETL workgroup, which is used as a target for the Zero ETL integration. There are the 24 tables from Oracle available for querying, updated in near real time as soon as the data gets updated. At this time I can write queries. This is a query that is generating item level metrics by joining various tables in the Oracle database and transformed to generate item level metrics, and a table called item sales metrics has been created. Like that, within a few simple steps, we have been able to load the data from the Oracle sales database into Redshift, create a continuous pipeline that keeps the data updated as well. And we have transformed the data to create the item sales metrics table. Similarly, there is another pipeline that is pumping data from fire hose into the Apache Iceberg S3 table. At this time we go to the 2nd step. As you ingest this data, it's automatically cataloged in the SageMaker catalog. You don't need to take any additional steps. You have the data cataloged and available for you, and now in the catalog, you can do permissions management. And you, there are two ways you can do permissions management. You can give permissions to IAM roles, or there is a better way. You can give permissions to your identity center groups or users. The catalog has native integration with IAM Identity Center. An identity center, in turn, can be synced with most standard industry identity providers like Azuredi, Octa, Ping. You can sync them with identity Center, and at this point you can start giving permissions to your identity Centre's users and groups. Let's take an example. Let's assume that any company has 3 different groups data scientists, analysts, data engineers, all managed using their identity provider Octa. Now, in the next part, let's see how any company can give permissions on this new table that we have created to their analysts users so that they can do reports on top of them. So for that, let's go to lake formation. In Lake formation, if you go down, there is identity center integration. You see that lake formation is integrated with identity Center. This in turn is integrated with OCTA, which has those three groups that I was talking about analysts, data engineers, data scientists, which are synced with identity Center analysts, data engineers, and data scientists. At this time I can do permission management on them. So let's go to catalogs. In the catalog, you see the data from Redshift, Redshift warehouse. You also see the data from the data lake, S3 catalog. And now let's see the tables. In the Redshift catalog, you are seeing the item sales metric table that we have created by transforming the data from Oracle, that's the schema for that table. And I can now grant permissions on this table to the identity Center principals. And when I get started, I would be able to see the analysts group from the identity provider reflected here and I can start giving permissions to that. I can verify that yes, this is the right catalog, the right database, right table. I'm granting select and describe privileges. And just like that, I'm able to grant privileges centrally on both data warehouse and data lake objects to the identity provider entities using the catalog. So we have done permission management. You got objects, we have done permission management. We said analysts can now access the item sales metric table. They can log into any endpoint and start accessing. In this next part, we will get to reporting. So the analysts will log in to the reporting endpoint and start accessing the data. The analysts will use the same data set. There are no copies of data made. It is the same copy of data that will be accessed using a separate endpoint, completely isolated from the previous endpoint that we are using. Let's go and see how that reporting is done. We go to query editor Vitu. Now see that we are using the 2nd endpoint and we are logging in using the identity provider privilege. I'm logging in as an analyst person who is a part of the analyst group, logged in. At this time, analyst person can see all the objects that they have access to. The item sales metric table, the S3 tables. Let's quickly see who the current user is. The current user is an analyst person who is from the identity provider. Now this analyst user can run reporting queries by combining the data from the data warehouse and from the data lake in Apache iceberg format. Getting events data and the customer data combined together a simple query to get what is the total number of customers, how many clicks they have made, and those are the metrics. Similarly, hundreds of reporting queries can run on this reporting endpoint, and they will not impact the ETLs that are happening on the other endpoint. That way, both these critical workloads SLAs are met. Now, the last step, natural language querying using generative AI. For that, let's go to Bedrock. Any of you use bedrock here? A few hands. Are you aware of Bedrock knowledge base? Awesome. So a lot of you must be aware for Bedrock knowledge bases, you can use objects in S3. You can also use redshift data as a knowledge base in Bedrock. Let's see how that is done. You go to knowledge bases. And when you go down, you can say create, and you can create knowledge based on structured data. And there you can see redshift as the querying engine. And you can choose which specific redshift endpoint that this should be using. I'll be using the generative AI workgroup, which is different from the two other workgroups that we are using. And I can pick and choose which data set should this knowledge base have access to. And I can do additional table descriptions, column descriptions to make, to improve the performance of the agents. And that, that's it, we can create the knowledge base. And now you go into the knowledge base and let's test it out. You have 3 options. We'll go into each of them. And for the knowledge base, you can choose a model, any model that Bedrock supports. I'm choosing the Amazon Nova model, Nova Pro. And now let's ask a question. What is the total sales amount? The agent creates a query, runs it on Redshift, gives the result back to you in simple, plain English. The total sales amount is so and so. Now I can choose to just generate a SQL query. I ask the same question, what is the total sales amount? I get a SQL query as a response back. I can also just retrieve the data, not in English, but just as a tabular format. I can say retrieve only, I can get the data back in tabular format. Like that, using the generative AI workgroup, any company can expose that workgroup to any number of users without impacting the reporting, without impacting the ETL, yet democratize the data using natural language querying. Bringing it all together, this is the architecture we just saw. Ingested the data from various sources into a central lake house, did permissions management through your identity groups and users, did consumption for reporting and natural language querying. Alex was telling me offstage that last year their team has attended this session and they took this learnings from the session back and they have built their multi-warehouse architecture. We highly encourage you to do the same. For your use cases, go try out multi-warehouse architectures. Whether you're starting fresh, start with multi-warehouse, or if you're already a redshift user, try converting that into multi-warehouse architectures. You can refer to these resources to learn more about multi-warehouse architectures. I'll pause for two seconds so that you can take pictures and refer to them later. All right. That's the end of our session. Thank you so much for attending. Please do take some time to fill in the survey and feedback. It'll help us a lot to improve ourselves. A special thanks to Alex for coming out here and sharing his story. We now are happy to take any questions you may have.