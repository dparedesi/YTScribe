---
video_id: RmoGAGUe56Q
video_url: https://www.youtube.com/watch?v=RmoGAGUe56Q
is_generated: False
is_translatable: True
summary: "This session, titled 'Developers Soar at Southwest Airlines,' explores the airline's transformative journey toward an AI-powered DevSecOps model in partnership with AWS and GitLab. Featuring insights from Grant Morris (Managing Director of Technology at Southwest), Lee Faust (Global Field CTO at GitLab), and Kyle Seaman (Product Manager at AWS), the discussion delves into how a large-scale enterprise with deep legacy roots is modernizing its IT operations to enhance developer productivity, security, and resiliency.

Grant Morris outlines Southwest's technological evolution from client-server architectures and PowerBuilder applications to a modern, cloud-native ecosystem. He describes the challenges of 'suboptimal processes' and 'siloed automation' that accumulated over decades and how the organization is working to eliminate this technical debt. A key component of this transformation is the shift to a platform-as-service model, where internal platform teams provide capabilities that product teams consume self-service. Morris emphasizes the cultural shift required, moving from a rigid 'ticket-based' do-for model to one that empowers autonomous product teams while maintaining enterprise-grade governance.

The session highlights the 'duo Agentic platform' (internally called DA) from GitLab, which serves as an orchestration layer for defining custom agents across the software development lifecycle (SDLC). The speakers discuss how these agents can handle tasks ranging from automated merge request reviews and CI/CD troubleshooting to vulnerability remediation. For instance, an agent could identify a security flaw in one project and autonomously scan the entire organization for similar vulnerabilities, creating merge requests to fix them across the board. This capability represents a significant leap from traditional 'shift-left' security, moving toward automated remediation and 'security as a service.'

A significant portion of the conversation focuses on the practicalities and challenges of adopting AI in a brownfield environment. Kyle Seaman introduces the concepts of 'vibe coding' (iterative, local changes where low-cost errors allow for experimentation) versus 'spec coding' (grounded, specification-driven development for more complex tasks). He argues that while IDE assistants and CLIs are gaining traction, the real challenge lies in integrating these tools into a cohesive toolchain that includes source control and issue tracking. The panel also addresses the crucial issue of authorization for autonomous agents, debating how to manage identity and privileges for agents that operate independently—a concept compared to 'IT becoming the HR for AI agents.'

Ultimately, the speakers advise a patient, iterative approach to AI adoption. They suggest starting with small wins to build confidence before tackling massive refactoring projects. Morris underscores the importance of fostering a culture of curiosity and ownership, where developers are encouraged to experiment with AI to solve daily problems. The session concludes with a vision of the near future—possibly just months away—where persona-based agents (e.g., product owner agents, security agents) collaborate within the SDLC, fundamentally redefining team structures and workflows."
keywords:
  - AWS
  - GitLab
  - Southwest Airlines
  - DevSecOps
  - AI agents
  - cloud migration
  - modernization
  - developer productivity
  - security
  - duo Agentic platform
  - autonomous agents
  - platform engineering
  - CI/CD
  - legacy modernization
  - automated governance
  - generative AI---

Great. Welcome everybody. Um, we're here for a session. Developer soar at Southwest Airlines. Uh, we're gonna be talking about, uh, how Southwest uses AWS and GitLab. So let me introduce myself. My name is Lee Faust. I am a global field CTO at Git Lab. I'll hand it over to Grant. Hi, my name is Grant Morris. I'm managing director of technology at Southwest Airlines. I lead, uh, our business of IT platform teams, so it includes Get Lab and other, uh, DevOps tooling teams. And hey everyone, I'm Kyle Seaman. I'm a product manager at AWS, um, for the last couple of years, but I'm, I'm now on the Quiro team building out our AI developer experiences and been lucky enough to work with these folks for a while. Great. So I'm sure if anybody here went in the expo hall, there's this new thing that is emerging called AI. So, um. Uh, there's an explosion right now going on around AI coding agents and how people look at using AI to solve critical business problems, and what we're gonna talk about a little bit today is, um, a, a history about looking through past, uh, today challenges and tomorrow challenges and how AI is helping our customers be able to do that at Gitlab. So right now there's a bit of an AI uh paradox going on. So there's a lot of organizations right now. Um, I've been in many meetings this week where I'll have one company that is all in and they're moving really fast and they've got, uh, you know, 80% of their teams are using AI for their software development and then we have other teams that I'm talking to that they're just getting started. And they're like, like, point me in the right direction, give me some guidance, and everything that I hear from everybody is we're at a point right now, it's now been about 2.5 years since this has really taken off, and a lot of people want to know what the impacts are to their business. It's AI is still a tool that a lot of them are using, but just like any other tool that we use in our environments is how do we measure that tool. So, GitLab has built a solution, um, uh, we, it's a part of our duo suite. We have a beta right now of what we're calling our uh duo Agenic platform, um, we call it DA internally. And this is an orchestration layer allowing you to be able to define your own custom agents around the software development life cycle and how you can use the data inside of GitLab to be able to make better informed decisions, also being able to do things like having a merge request reviewer, uh, that's. AI powered, uh, being able to have one for, uh, being able to have conversations around your issues and your ethics and then being able to do things like, uh, CI resolution. So somebody built a CI job 10 years ago and there's a batch grip and it's failing. Why is it failing? We can plug in duo in there and have it go ahead and do some troubleshooting for you. This is all built around GitLab's unified data model. So what we have is GitLab is an end to end suite of capabilities and it's all driven off of a common data platform that allows us to be able to use that data across every project, every group, every subgroup, every CI job, every release, all. Of those things we provide data that we can use to be able to plug into these agents where we can then use natural language to be able to ask really uh really important questions. So like some of the things, uh, we talk about we go from left to right, but a lot of the things our customers want and we'll talk a little bit about today is how can I take that vertically as well. So let's say you've got a security vulnerability in your organization, a security engineer fixes a critical in one project. What if you were able to go find every other project that has that same security vulnerability, have an AI agent that goes ahead and automatically creates a branch and goes ahead and does a merger request, and then you can go ahead and blanket, go ahead and approve those using an AI agent on the back end. So let's talk a little bit about Southwest because that's what everybody is here to be able to listen to you talk about your journey. So, um, talk to me a little bit about. Just your general IT operations. What does that look like today? Well, excuse me, I've, uh, been at Southwest for 26 years. So whenever we talk about the IT journey, it goes back to, you know, client server power builder apps, cybase databases, and that type of thing, um, so really, you know, we've, we've evolved with the industry over time. And that's led to, you know, suboptimal processes, um, you know, automation that's overly siloed, uh, and just kind of the, the natural craft that some of the large organizations, uh, deal with on a daily basis. So today we're working really hard to work ourselves out of that craft and we're seeing some really great results, uh, with our, um. Uh, cloud migrations and with our DevOps maturity migrations built on GitLab, so we've got about, uh, 3000 technical resources, about 5000 folks in the department, uh, overall we have 72,000 employees, uh, you know, in the hemisphere. Uh, we run 4000 flights a day, uh, 24/7. So it's really, really critical for us that we have the most resilient and, uh, best platforms out there. So if you went back and you looked at your introduction to Southwest way back when and you thought about how people built software. How is that for you, that journey, was it something that you were, uh, uh, you know, because you probably had smaller teams, you probably were able to move a lot faster, there was, um, uh, people who were able to make decisions on their own. Tell me a little bit how that original, like the culture and what that looked like inside of Southwest. Yeah, so. Like everything is a pendulum swing, right? And so way back in the day, pretty autonomous product teams, we built, deployed, um, supported the, the product had a lot of autonomy. Uh, and then as the industry matured, we started taking things, uh, more seriously like security, socks, PCI, all that kind of stuff that caused us to start separating out some of those functions. Uh, and then, you know, we, we started building up these external dependencies throughout the delivery process. So I think that throughout my career that's been something that I've been kind of striving to get back to, right? How do you empower those autonomous, uh, product teams to be self-sufficient, act within their boundaries, make decisions, uh, while still. Uh, providing the enterprise concerns that, you know, are important to all of us today. So it really kind of separating out the, the things that are important to the product teams from the things that are important to the enterprise and being able to provide a seamless experience there has been, I think, something we've been striving for. Great, and, um, you and I, we had a conversation before, uh, um, talking about the session and. Um, one of the items that I think a lot of people in the room struggle with even before we start talking about AI and all the other great things is, um, like back then when you talked about security, like when we look at AWS, AWS does a phenomenal job around a lot of the edge security type things, but. A lot of times the IT department gets handed all of the stuff to be able to try to secure it where now we're trying to shift some of that left. So what was that journey like on the security side? Where did you, where was security in the beginning in the process and where are you trying or where is it today? Yeah, this was, um, so back before our cloud journey, you know, it was kind of something that we secured on the edges, um, as we got into our, our cloud migration, security was. Probably the hardest topic to tackle, um, so from an application scanning perspective, you know, we had the tools out there but they were out of band out of process, so it was kind of based on, uh, a calendar requirement, uh, so people would forget about it and you know you'd have to go remind them and go, OK, let's get your, uh, you know, let's get your application scan done, let's get your pen testing done. Um, so really, then as we shifted into cloud, we started taking a more purposeful approach to integrate, uh, IAC scanning, application scanning, uh, runtime scanning, and really tried to harden things on behalf of the application teams and provide that kind of security as a service. Uh, we're not great yet. We're very secure, like, don't get me wrong, but, uh, I think we could do better at still providing a more seamless, uh, feedback loop to our, uh, to our engineers there. Great. So, um, When we think about all of the different roles you and I were talking earlier and we were, uh, discussing the personas of everybody in the software development life cycle and, um, uh, how do you think about like just think about like hiring and the type of people like a lot of people have good skill sets, but what makes a good Southwest employee for your team? Uh, so that's a great question. Um, I, I look for people that have this natural curiosity, uh, you know, we, we all go through our, our day to day jobs, or, you know, building and deploying our applications and that type of thing, but we're, we're looking for those folks that go, you know what, I think there's a better way to do this, and then beyond that, not only improving their, their team, um, and creating that flywheel effect for their team. But really sharing that back with the organization so that everybody can take advantage of it. So I think natural curiosity, you know, a high sense of ownership also like you gotta have a good sense of humor. It's Southwest, we, we wanna have a good time. So, uh, you know, we, we look for the right personality fit, but you know, again going back to the industry, things have shifted so much, so. Uh, you've, you've really gotta have that high sense of ownership and not only your product but also the technology and the innovation that's being presented to you. Like nobody's gonna, nobody's going to, um. Drive your career for you. So you've got to own that. You've got to accept those, uh, those changes like AI and be a self-starter and really take advantage of that on your own. Great. So. When we think about transformation, this is something that Kyle I know you and I, we've talked about this a lot, um, and the transformations that we've seen. I mean I've now been in IT for 30 years, you've been at Southwest for 25+ and, um, as we think about all the different areas of transformation, I think one of the things a lot of people miss is the cultural transformation that companies need to go through, um. When you started introducing GitLab to the teams, if we sort of look at today, and you went through that transformation of getting people on boarded into GitLab, and getting them starting to use cloud and using it effectively, what was the cultural transformation internally? Did you have to reorganize people? How did you structure things? What, what was, what was sort of some of the biggest challenges that you had with that? Yeah, I mean, any transformation comes with people processing tools, right, or technology, and so if you start getting one of those on a, um, uh, a plane that is ahead of the rest, then you're just gonna create some bottlenecks. So uh we started small, I think with, uh, 4 applications that we wanted to take to the cloud. Now they were really, really important applications also, so that was fun, but. Um, you know, start small, have the team, take the team through, uh, you know, that change curve, and whenever we did those 1st 4 applications, we had a ton of learning and we didn't go straight to a cloud native architecture. Uh, we did kind of a lift and massage containerization and shift type of thing. Um, so it was, you know, we, we kind of dipped our toes in the water to some extent, uh, through that process. Now it was probably about 9 to 12 months after that that we really started seeing the engineers get comfortable with cloud native technologies and so we're able to start, OK, we're gonna take this out of the container, we're gonna take this out of the container, so single page applications, um. Lamb does, you know, now we're getting big into event-driven architecture and that type of thing. So, you know, it was, it was nice to kind of meter it in and not try to go from step one to step 10 all in one leap, uh, but it's definitely a change curve. You definitely have to, uh, think about how this is, how the ownership model changes, right. Uh, you can't go back to your Unix engineer or your DBA and go, OK, help me with this stuff. And they're like, I don't know. Like, I don't own this stuff. I didn't know you, so you know, getting the, uh, getting the teams to realize that they are also on the hook and accountable for their products, uh, has been, has been another interesting shift. So Kyle, you and I, we met when, um, you were working with Q and building out the Q product line, and one of the items that we saw for a lot of our joint customers was. Um, around being able to do remediation type tasks, um, especially around things like CI and, um, uh, issue resolution, what are some of the key transformations that you're seeing in your role today that, that people are sort of still struggling with where they're looking for guidance. From AWS and from the teams there and how your role's impacting that, yeah, I think 11 interesting thing with like what we loved with the GitLab integration we did was it worked out of the box so you didn't have to be an adopter of it. It was like it just happened like if it broke it remediated if it there was a UI button that said help solve this and we found that to be a really nice way to like you don't have to know how to use AI it just sort of showed up. So where we're seeing the bigger challenge is more on the client side where everyone has choice and I think you know so you give a lot of choice to your developers and local tooling and of course you have golden paths and things so the big thing is like, well I have my ID or I have my flow. Sometimes I don't know how to prompt. Sometimes I don't have the time to set up context windows, steering files, MCP, so that's the opportunity we're seeing and basically what happens is you find champions. We do a lot of work. I've seen this with Southwest too, like find champions internally and then have them turn around and do internal working sessions and show and tells and. Kinda highlight that work. So that's been, I think, on your peop I like your people, you know, that if they're off out of sync it's a problem and the tech is way ahead right now. And so that part I think has been neat. And so then you get the wind of you could put in a get lab, oh, I have a CI issue. This would have sucked to copy locally fix whatever, and he starts to say, oh, maybe there's some benefits here. So that has actually helped with this adoption of bringing these sort of automatic AI loops into it, um, specifically for the AI transformations that's what we're saying. So, um, Grant, when we look at how GitLab is helping developers from a dev secops perspective, um, a lot of times we'll hear, especially in today's world, you know, it's the cursors of the world, the, the, uh, heroes of the world, that the developers are now more focused than ever, but these IDEs are now always attached, and now we've got all of this code that's starting to explode. In these environments, how are you thinking about using your dev secs, uh, platform to be able to provide guard rails to making sure that people are doing the right things? Yeah, so what we've been focused on over the last, uh, year or so is really creating more of that automated governance, um, and integrating, say, the metadata across all of our IT tooling, uh, so. Being able to allow the, the customers, our customers, uh, are, we're an internal service provider, so our customers are engineers and stuff, but allowing them to maintain that creativity and the flexibility, uh, to really solve the business problems, leveraging AWS, uh, in a creative manner while we're trying to focus on providing that enterprise, enterprise grade hardening and that type of thing, so. Leveraging Get Lab has been, uh, really helpful for us. Uh, we have, we're able to leverage things out of the box as well as integrate with third party tools to, you know, get the things done that we need to get done, um, but really, uh, being able to. Provide, uh, the flexibility of tooling such as duo agent and, uh, that type of thing to get the AI capabilities at our customers' fingertips wherever they want to consume it, right? So we've seen a lot of great success on troubleshooting pipeline issues, especially. Um, code generation, all that kind of stuff, wherever they want to consume that, whether it's with an IDE or Get Lab platform, like we can provide that for them. When we look at um uh one of the things that was really interesting when I had joined Git Lab about 4 years ago, one of the things that I was very intrigued by is GitLab is like a tier 1 application for you like it, it cannot go down. So talk to me a little bit about that and how you're solving some of those problems. Sure, uh, well, again, it's, it's been a little bit of a journey, but since we started our cloud migration with some of our most critical applications, we have some very, very, uh, tight resiliency standards. So being able to take in, um, multi-region architecture from the get-go. Um, data, you know, having data available in, in a multi-region environment was very important to us and even as we start getting more advanced in our application development and our operations, you know, starting to shift that mindset from a fail back mentality to a fail forward mentality, uh, has required us to work together with you guys and really have some, uh, high level resiliency standards. Uh, we, you know, if we start getting really nervous when some of our systems are unavailable for even 15 minutes, so, uh, resiliency is, is very much top of mind for us, and it's, it's been, like I said, a journey to get there based on, uh, our architectural patterns, capabilities, and even working with AWS to, uh, expand some of the, the capabilities that they offer. Yeah, a lot of people don't understand that like get being distributed, you know, if, if your Git server goes down it's fine, but when you start dealing with the operational aspects of keeping the lights running across all of the different mobile applications and you've got people sitting in airports and trying to find new flights and things like that, that. Piece cannot go down and so when we think about the uh automation side on the CI side and how you're managing not just your application source code but also the infrastructure, your best practices, your guidelines, things like that. Talk to me a little bit about that automation aspect and how you're using that with AWS. Sure, I mean, we've, we've been in the early days down the path of a very opinionated like cloud infrastructure, uh, platform, and we kind of tried to put everything into a single platform product, um. And so we're, we're kind of, we've worked ourselves out of that, uh, we've taken advantage of more industry standard tooling like terraform and, and that type of thing, and we very much want all of our application code as well as all of our infrastructure code to be in get to be deployed through the CICD processes so that they're scanned, so that they're governed, and we very much look at console access as. An exception based thing, right? So, uh, we do not want the, the console to be part of our daily deployment like toolkit. So we have invested, like I said, we have invested a lot in trying to adopt more industry standards, making the kind of breaking up the, the platform as a product into multiple products that are consumable, uh, kind of in a a la carte type of way to our customers. Uh, while we still get the, the governance, the security, and the automation through that. So it's really been a partnership with, with our application teams as well as our platform teams, uh, to be able to get the outcomes that we're after there. So I'm sure that there's a lot of people here in the room if they go around the expo hall, they, uh, one of the words that you use, I would love for you to define it how it fits for Southwest, and that is how do you think about a platform inside your organization in all the ways I think, um, so there is a lot of local context there. But we kind of view, uh, being an internal service provider, we kind of view the, the platforms as things that are consumed by our customers that help them do their jobs and it's, it's been a journey to get from, um, kind of the do for model, uh, where, you know, there's a ticket-based environment and you, you know, you request something and then somebody goes and does something in the background. Uh, to more of a self-service consumption model, so we're still on that journey, but we've been really making a lot of great progress towards, um, kind of decomposing a platform into a set of capabilities that we can then go have a more targeted conversation with our customers on, uh, so, you know, nobody ever comes to us and says, oh, I need a platform. Right, and they say, oh, I need to do something we're like, OK, great, well we've got something we've got a capability that can help you go do something and then in the back we manage, uh, we manage those integrations those dependencies and that type of thing on their behalf. So the people that are managing all of that on the back end, um, one of the things a lot of other customers that I talked to that are challenged with is, um. A lot of times they end up becoming siloed like I end up building a CI silo and an agile silo. Are you doing anything to be able to sort of break down some of those barriers to be able to allow people to use that platform in a way as a capability consumer to be able to say, oh well, yeah, you can just reach out to the team and anybody in the team can sort of help out in that aspect? Yeah, definitely, uh, it's. I, I mean, look, people try to self-s solve, and, and that's, I think, human nature, um, but it does take a, a little bit of thought and kind of taking yourself out of your day to day, looking more broadly across, well, what is the value stream of delivering software, right? How do my customers consume my services? How do they deliver value to their customers. And then you start unlocking those opportunities to get people out of their silo and look more broadly at what's the overall objective or the goal that we're trying to uh accomplish, um, but at the end of the day, man, we can't be all we can't be specialists in everything, right? so we still have to. Uh, work hard with our upstream and downstream dependencies and our partners and just maintain relationships and communication so that we're partnering towards a common goal. So Kyle, along those lines with the people that you're working with right now and as they think about plugging into the AWS as a platform and how AI is now a key component of that platform, how are your customers looking at that platform aspect? I think it's, it's the modules and kind of the tool chain and the supply chain and everything, and I think we hit on it with like the CI story of one, you shouldn't trust it. Like there's a lot of and like you're going to as extreme as don't trust a human in the console, so like don't probably don't trust AI like there's a lot of interesting guardrails being built, and I think there's some really interesting work being done around authorization layers and controls, of course, and. Having autonomous monitoring is a feature you just have to do it right, but it, we're seeing basically, do you have the right stack to even get there? So CI, I think, is like when I talk to a customer, it's like, well, how ready are you? And it's, do you have CICD? OK, good, because if you don't, we're in problem because that's like a, a check. It's a check and balance. Get Lab has some cool things where you can basically say if it's AI don't merge, don't run the CI system. Right, you might have prompt abstraction and data things like that. So a lot of it is grounding and like what's your tool chain look like right now, how much of it is in the cloud? Like are you running Jira in the cloud or Jira on prem because that's gonna make connections different and. It's a lot of time spent on that and that's why I think the clients like IDE and CLI are catching on because it's the computer, but stitching together these systems as every customer's migrating more and more to the cloud, but they're not there yet is the biggest challenge we're trying to work through. And I mean, as you're going forward, right, we really wanna be able to leverage those AI benefits and we want that kind of that SDLC cycle to go as quickly as possible, right? We wanna iterate, iterate, iterate. And so if our, if our pipeline and our processes become too distributed, that's just gonna slow us down. So I think it's an interesting Kyle, you brought up a really interesting point on how you manage that overall stack so that you can move quickly as you were saying that I've, I realized that we were sort of a slide behind as we started getting into that consistency of, um, you know. When we think about iterations, especially at the size of the Southwest, like there's a lot of other customers that I work with, you know, if I've got a small engineering team of say 50 people, yeah, moving fast is easy, but moving fast at your scale is remarkable. Um, what, what do you attribute some of that success to? Well, I mean, yeah, you look across the organization as a whole and there's different levels of maturity in, in different spots so I think as we're engaging with our customers we really have to meet them where they're at and we've offered, um, a ton of different ways to engage with us, engage with our products, and, you know, everywhere from. I've got a team that'll go work with this team and help you on board to the, uh, to the platform products, uh, to self-service consumption, and even now we're starting to think about, OK, how do we enable our customers to consume our products through an AI based interface and so it, it is, uh, it's kind of like, well, you've gotta be a little bit of all things to all people at the right time. Uh, and really understand where they're coming from so that you can guide them towards their, um, towards their desired outcomes, but we've had some really great successes with, uh, our automated governance, AI. We're seeing, uh, even the teams that are just experimenting with it, we're seeing great results there where timelines are getting cut by 13%, getting cut by 50%. Uh, and it's, it's really just, I think, meeting people where they're at to start helping them along that journey. Yeah, I think one of the things that I'm seeing as well is on the consistency front as we look at digging into how AI is able to assist you. A lot of people want to go for the, the big wins and what I try to tell a lot of customers is you should probably do a lot of little wins first, um, because there's a lot of people out there like, um, you read some of the different studies and stuff where you know senior engineers don't like the results that are coming out and, um, there's some fear that's going into it. Um, as we think about how that consistency around small little wins, are there any little wins that you've had, let's say over the last year where you've had like an aha moment where you're like, yeah, this is something we need to double down on? Yeah, definitely, we, we did the same thing, right? There was the promise of kind of agentic back in the day, you know, maybe a year ago we started talking about agentic and we were like, oh, that sounds really great, we should try that. And what we figured out was. We, we hadn't really made it over the hump with just generative AI, so we needed to get really good at that before we could start dipping our toes in the agentic workflow type of, uh, solutions, um, again. Like we, uh, reached out to our customers through lunch and learns through communities of practice, uh, we started gathering up examples from other teams to use so we've got an internal development, you know, effort on, on my team where. Uh, we really challenged them to leverage AI and something that they thought were gonna take, uh, you know, a year is coming in at, you know, 34 months, and I mean the, the results are really great, but you do have to, uh, you do have to engage with it, you know, not, don't just treat it like Google, right? You can't just go, hey, tell me the answer. You have to be able to guide it. Uh, give it the context. We've got a team that, uh, came up with a lot of, uh, prompts, a lot of examples to kind of give people the flavor of, hey, here's how you take a simple application and can go from nothing to fully working application through a generative AI prompts. So I think, you know, the, the answer is a little bit of all of the above, right? You've got a. Uh, be able to provide those, uh, examples and, and really help walk people through it. And, and Kyle, Southwest is a great example of, of what that looks like. How do you handle the inverse when you're working with a customer like you were describing? Like, do they have CICD? Do they have, but they, they, they think that, hey, you know what, if I just go grab this AI model and you tell me what the two or three prompts are, then magic happens and everything's fixed. How are you handling this conversation? Good point. I, I spent a lot of time setting expectations and almost undersell. There, there's so much hype. There's real utility, but it gets lost in like the, the chaos of that. So I, I find what works really well is finding champions like we met because you cared about this. I met a bunch of folks on your team and we just spent time together and that's what I end up doing. It's just like we're the champions because they're gonna build golden paths that are applicable to all customers and I tried just to level set on the expectations and, and then what's cool is, yeah, and it's not gonna happen overnight but like 3 months of learning this and playing with it if they have time, yeah, you ship stuff a lot faster all of a sudden because. You're just not worrying about the bug fix because it's getting fixed autonomously or something, so I think finding the champion, setting the right expectations, and then once you're there and you've had wins and people are buying in like I think even uh with Southwest we've had a rolling deployment of, of the IDE and the CLI experiences intentionally, you know, we wanna see people using it and you've got some pretty cool systems to have people on board. Self serve themselves and that's right it's like great use it when you're ready but then you get the wins and so that that's just build it up and it's like takes time but it's it's really worth it we're seeing because a year now in and a ton of these customers are just yeah moving really fast like you guys so I, I know one of the things Kyle is there's just so much tool sprawl around AI right now that is out there. Um, when you've got customers that are looking at different products and looking at different things that are out there, um, how are you looking at, uh, having a discussion around what's different with AWS and what you all are building? We, so we're very like by design we are open to tools. And to a fault in some cases we have lots of versions of the same thing if you want it, but we think about that with customers too. Like Git Lab's an AWS customer. I'm happy to pitch GitHub. We use GitLab internally. We use GitHub as well. It's like it's tool choicing is important. The way I think about this though is, is like the hierarchy of needs. So like you'll have your infrastructure hierarchy. Are you a container shop, a server shop, but then also. Realistically it's your issue manager and your source code. Those are like everything now because it's your source of truth. And if you nail those, then you can actually pretty quickly change out tools. Like you can experiment with this lambda function, you know, over the weekend, but it's running from code and CI. So what I do is I try to understand what their priorities are if it's a, and I spend a lot of time on the developer tool side, so I'm, I'm really focused up to the merge. And then it comes back to what's your source code, where's your issue tracker, because if we get those right, then we can build out. And we're open like it's my whole thing is, yeah, whatever makes the most sense for your company. So I think one of the things that a lot, a lot of the customers I've spoken to this week is a lot of them have had little wins when they start from scratch. But what happens when you've got a legacy code base that has been around for 20 years and now all of a sudden you're injecting AI into it and you're trying to deal with it finding things that maybe you didn't want to find or maybe there's refactoring um one of the items that I had a customer ask me this week is, um, everything is always adding code very rarely is it ever deleting code even when we refactor it still adds more code. How are you looking at some of those, let's say, brownfield applications and what are the ways that you're thinking about using AI for some of those? Yeah, I think, you know, just, just getting familiar with your bag of tricks, right, with AI. So it doesn't always have to be, hey, go to this, uh, code base and transform it from X to Y. Uh, it'd be great if it did, but like a lot of our code bases have a lot of external dependencies, third party libraries, that type of thing. And trying to upgrade 10 things from a tech deb perspective into the, the newest, uh, architecture like that's a big heavy lift so we've found success and kind of OK well let's try to upgrade one of those things at a time and kind of work it through uh a continuous pattern and so over time you can do that. The other thing that we've done is, uh, also like, OK, this, this code base is something that. Yikes, right? We all know, we all have some of those. And, uh, so leverage AI to inspect the code. So tell me what it's doing, what are the business rules, and then we can reverse engineer the specs or the, you know, user stories, whatever capabilities out of that, and then get with a more, uh, a, a different tool or, or AI, uh, capability to start building back up and kind of replacing those capabilities from a, a, a newer stack. So Kyle, I, I'm gonna sort of put you on the spot because this is a really good question I got from a customer yesterday and um I promised them I would ask you on stage, um, so the scenario that they put me in is I agree with you having a good foundational model for how your issues are managed, things like that, um, how we define sprints in the future will probably change or how we build work items, tasks, things like that. What happens when, let's say I am in uh Quiro CLI and it goes off and it generates 1000 lines of code off of a single task that was met, that was there, but Yeah, there are 4 lines of code in there that solves that task, but the other 900 lines of code, there's actually 6 other issues that are in the backlog that it maps to. How are you thinking about being able to sort of create sort of this two-way sync between knowing what's in my backlog, what code's being written, and the two of them could sort of work together? Yeah, it's, it's tough too as a builder in the space like. Like Opus 4.5 just came out and it's totally different behavior. Sonnet 45 loves writing markdown files, so there's just like these like interesting nuances with the model you're using as well. But the way we think about this, and we're sort of formalized, we'll talk, we can talk a little bit about Kiiro as well, but like we think of it as vibe coding and spec coding. Vibe is really good where you're, oh, I need to update this string or I need to do kind of contained a session. I'm gonna be in here for maybe 2 hours. I'm gonna run through the session. That the good news is it's local on your IDE and the cost of iteration is so cheap now that you can just be like you did that wrong, redo it. Um, things like checkpointing are really common in these, it's like putting tools around the AI like you almost think of like your CI pipeline as a, a guard rail checkpointing, keeping it local to a branch always don't ever write on your main always go to a branch because the worst case scenario, the way I actually kind of reflect on this is like. If that looks bad then just trash your branch like you can, it's so cheap to start over again, but whenever it gets more interesting and serious is the spec stuff which you've talked about, which is, and that's one of the big areas we're focused on now is what is the definition of this thing? How do I get a spec and what we're seeing is you can. The LLMs are getting really good at following instructions, and I suspect they're even better and better in, you know, 6 months from now. So take a spec, put it out there, and we're, we're exploring things like property-based testing which uses like, uh, automated reasoning to look at the data patterns, not the actual unit tests, and. How do you ground that agent so don't have to write your unit test, write your unit tests or have a different agent write the unit tests and verify it. So we're seeing multi-agent systems as verifiers. We're seeing specs as a way to like guide it for this feature like Ticket your ticket, blow it out into a spec so you go through it, uh, and then vibing on your branch that you can trash is kind of. The happy middle right now, yeah. Oh, also don't have it. The LMs like to push code a lot, so I'm always like, I'm one of my prompts is keep it local. Don't push this. But yeah, you, you'll learn and then you come up with prompt libraries, and one of the things you're seeing like with Kiro we have steering files. There's lots of versions, Agent MD. But like what's really nice is the power user goes to this project first, populates it with architecture, best practices, and some other things, and then every person coming next is like, oh we use, uh, you know, we use Postress, we don't use SQL Lite or whatever like you, you start to benefit as you use it a bit too. So yeah, those are the kind of things we're seeing, but it is definitely messy. Great, so we've sort of gone through, uh, some of the different pieces of, uh, where we are today, um, which is kind of crazy because when we started having this conversation three months ago, like a lot of things that we talked about we're already implementing a lot of those things, um, so let's think about like the future and like if you, if you could. Envision what this is gonna look like and and before we used to say 5 years now we've gotta pull that back like 2 years. Um, what, what do you see that world looking like for your teams and how they work with AI and how they're working with GitLab and sort of that autonomy and how do you see all those things fitting together in the future? Yeah, it's, I, I don't even know if 2 years is appropriate, right, like maybe, maybe 6 to 9 months, but I, I think we're definitely curious about, you know, the persona-based AI agents, so kind of taking that, that again in an effort to try to iterate as quickly as possible. You know, let's have a product owner persona, let's have a developer persona, let's have the security persona and be able to iterate through those different personas to get to a working prototype capability whatever in a very short amount of time and then using our CICD capabilities to really get it out the door and reduce and release features to our customers as quickly as possible so I think the persona based uh agentic workflows are gonna be really, really interesting. Uh, additionally, I think, you know, the, the DevOps, uh, agents being able to start pulling things in from multiple, uh, areas whether it's a, you know, a runtime concern, a, uh, security concern, or, or a new feature concern like there's multiple entry points into a team for generating work and being able to have those AI agents kind of receive that request and then go, OK, yeah, here's what we need to do in a code base. Here's our merger request. You know, here you go, team. So I think it's gonna really redefine how we think about our, our work and how we, uh, redefine our, our teams as well. Yeah, how about you? There's two, there's a, there's two things that just stand out as like gaps that we haven't solved yet. One is the code review process, I think, is just gonna transform in lots of different ways, especially when you consider, you know, 11 of the products we launched yesterday is around this autonomous agent. It can run 10 tasks at a time and can work for hours and. Um, it's been really cool. I've been working on it for a while now, but it, yeah, you still have to review the code, and that's gonna, we need a better way to do that. So that's something we're starting to think about a lot. And then the other one, so we, along with the autonomous development agent, we, we announced the DevOps agent, which is one flavor of what you're talking about, and then the security agent, we're calling them frontier agents in the sense that it's a mix of agents. It's actually like a dozen agents in there doing different work. One of the biggest themes that I think we'll have to solve for, and I don't think we're thinking about it enough, is great, I want a persona agent that does autonomy. What's the authorization story? How is it a user in your ID? Is it, is it on behalf? Like, we, we have a policy at, at, uh, AWS. Every agent will be owned by someone. That doesn't mean you can't ask that agent to do work for you, but there is an owner for it. And every customer, I think, is rethinking this process right now. Do you hook in with, you know, you've got vendors that build out really nice authorization layers that map from your because you're talking about GitHub, Jira, GitLab, a mix of all of those, and you might have, you might have 4 or 5 source code providers in the same company. How do you write authorization policies that says that this agent, when Lee's using it, can do this thing? And I think that's a, a really interesting problem that we're starting to think about now. And ideally what you do is you have a container with a. Proxy policy around it and the agent is free to operate. It can't destroy anything. It's, it's got its own file system it can pull down legacy code and build it. That's what we're striving for is, is this autonomy and, and I think authorization is gonna be the interesting thing. So one of the items, um, that I was talking to a customer a few weeks ago and they, they sort of, uh, pointed me to a diagram that I had shared with them a few years back and. Going back to sort of pull the thread forward is um before we used to think about platforms with capabilities were these things that we sort of stacked horizontally and what they're what they said is it's no longer us going from left to right it's now always interactive there's always something like an event driven architecture there's always events happening everywhere. So now if you took all of those platform services, and created a circle, and then in the middle was a human plus AI. And you're gonna have some like you said like I might have an identity associated with one platform and an identity associated with another platform they'll go do the work on my behalf, but I will be the one directing where the uh outputs go so they'll go figure out what the input should be and then I'll define the outputs and then we'll be able to get into a point that you can have that kind of uh a workflow and this goes back to um. Where Git Lab sees this is, uh, when we talk about our duo agent platform, um, we've had duo in multiple different incarnations. Um, so with our new CEO, uh, we've really doubled down on this idea of Instead of thinking about Gitlab saying here's how you should manage your issues, here's how you should manage your merge request, we've now opened this up completely to allow you to define your own agents. So not just having persona based but also task-based. So if I'm somebody uh who like I could have an agent responsible for combing the backlog, hey, go aggregate all of these issues together, summarize them and pull them together and let's put those back into our backlog, you might have somebody who's responsible for looking at all the critical security issues across all of the different projects. So as you're looking forward to using Git Lab and building your own agents and using the agent catalog, how does that impact some of your thoughts for where we're going in the future? Yeah, I mean, I think it's a great capability and we're really excited about it. Um, have we figured everything out like, heck no, right? Uh, but. Some of the early ones that we've we've had with AI like you mentioned were just hey go comb all these repos and find out where I may have a, a concern. Maybe there's an outdated library, maybe there's uh something that we need to update across a large swath and hey just go find this stuff for me. So I think it's gonna be an interesting mix of. How do you create the agents, Kyle, like what you, you were saying with every agent has an owner that is kind of based in that, uh, domain knowledge and being able to leverage each one of those domain based agents in that workflow where the product engineer is kind of at the at the center of managing the workflow so, um. I think it's, uh, I think it's really exciting. I like we're gonna have to go figure some stuff out, obviously there's a couple things on this one that gets me excited too because we've been working for the last year. We announced this, the, the Q integration which is evolving now to be more adapt oriented with Get Lab, and over the last year we've talked to so many customers together who just love the GitLab experience of. Of the controls they put in, but what we learned this with, we had a Java transform agent. Problem was you had to have, we wrote the agent, we AWS is like you had to be on version 8 going to like whatever and it's like, OK, if you hit that, and so many times I was so disappointed not having be able to help the customer. What I love about this approach with the duo agent platform is you write the agent. Using the same framework and guard rails, one of the cool things we built with the integration is every request the agent makes gets scoped down to lease privilege. So if that token got intercepted or something went wild, it will never be able to do more than this very narrow set, and I think that's like you, the power you get when it's the tool provider and why we want to do this integration, um. But now I look at the duo agent platform it's building on that and now you can define your own transformation or your own upgrade. It doesn't have to be one that I've vended and then it solves the other problem which is with the catalog. How do you discover this stuff now? You don't need everyone in your company to be pros. Now you actually have a home. I think that's another trend we're gonna find is where you er, where are you storing your MCP servers, where are you storing, um, your prompt catalogs, where are the best practices? And so I think there's gonna be some pretty neat manifestations. I'm sure Duo will continue to evolve, but this iteration, I think, hits on most of the problems we hit last year, which was rigidness and, um, you still had to kind of figure out. How to be great with AI. So those things I'm excited to see in the hands of customers. So one of the items that has come up in almost every single one of my conversations with customers this week, specifically around the app and how AI is being used, is, um, one customer I talked to who was a very heavy Q user. What they loved was the, uh, logging and the analytics and the dashboards you could get around that. How, how are you working with customers to be able to show that how that impacts the overall business of how people are using AI internally? This, this is one you'll probably hit this too. We, we talk about this a lot internally at Amazon, which is we're, we're enabling, we just announced Quiro will be the default tool for all of Amazon and, and we're seeing a lot of progression with like our transformations and stuff, but we were in a review with and, uh, we realized, you know, they're, they're gonna forget how productive it's becoming. And in 6 months from now they're not gonna notice this background work getting done like what am I paying for? And so there's this interesting balance where we, I think metrics and analytics and like measuring the impact is gonna be necessary to keep us around. It's like showing that ongoing impact. But then the important thing is everyone has unique ways that they wanna measure this. So we're looking at, we use cloud trail is a pretty popular way to do that. We report to cloud trail. We're adopting more and more open telemetry so that you can just take the data and go where you wanna go and. That way if you have your own cus everyone has their own custom dashboards and every and I actually think it's OK you're pulling it in you wanna use this tool you download CSVs so we just wanna make the data available um you get into some things I think we probably hit this with you all is like user mappings, user IDs you don't want PII in there you don't wanna be weird about like so there's some stuff we're still working through, but the idea is that it can be highly accessible to customers because one, we wanna show the impact ongoing so that you don't forget the benefit you're getting. How was Southwest thinking about using that data to be able to um determine how to better educate and use AI? Oh yeah, I mean the like let's face it, the, the hype cycle associated with AI was just off the charts, right? And so you know all the executives were, were getting sold the art of the possible and there was, wow this is really cool, but like what are the returns we're really getting here. So we've gotten everything from anecdotal to user accounts and like Southwest is kind of a a verbal storytelling type of organization so we do a lot of storytelling highlighting uh successes that teams have had using AI. Definitely looking to, uh, continue our journey with, you know, taking in uh hotel sources and the uh cloud watch AMP AMG, all that kind of stuff and creating our kind of adding this to our overall observability platform so it is, uh, I think it's something that we're working on. It is something that we're, we're, uh, that is important to us to really make sure that, you know, Kyle, as you said. We don't lose line of sight to what are the benefits because it is, uh, it's, it's started as a kind of a developer tool, right? Well, our developers don't code all day every day, so it's kind of like, well, it's for the fraction of time that they do code we're seeing improvement. But then as we start thinking about Agentic and and uh leveraging Agentic across the SDLC, then the amount of time that is subject to become more efficient is is broader now. So I think we've really got to keep on that path of, uh, metrics and logging to be able to show that, hey, not only are the capabilities expanding, but the efficiency across those capabilities are also increasing. Great. So, um As we think about where we're going and all of these different pieces are we, we've got a few minutes left, if there was something that you wanted the audience to take away from the session, what would be that nugget of information you'd like to share with them? Well, I think just, just don't be afraid of it. Just try it first and foremost. So we've kind of changed from asking folks, hey, are you using AI to why are you not using AI. Uh, so really getting a part of your everyday vocabulary, uh, getting yourself familiar with it. I mean, some of the first things that I did was just, OK, like I, I'm gonna go, go build me a C++ craps game, right? Just try it with the different things that you're doing throughout your day and see how you can leverage it and like I started out just asking it questions and then started to over time learn how to steer it to a better response. So I think that's the thing is it's, you know, Kyle, what you were saying it's, you know, put it in its boundary so that it's not gonna do anything harmful. I mean, you're, you're just working on a, a file, right? And so just go experiment with it and try it and then as you get good at it, then turn that into an enterprise capability, put your guardrails around it, harden it, uh, and so on. And Kyle, I'll throw it over to you like where you're going and your road map and things that you're excited about. What, what nuggets of information would you leave the audience with? I, I think the, the autonomy is coming really fast. Like the agentic, the instruction following, tool following, and agent calling agents is very much doable now. And so my, my focus right now is we're doing it, it's not ready yet for wide deployment. I talked about the authorization thing. You can do it at a user level. So that's what I'm focused on is like how do we get these agents access to the tools they need? How do you do the authorization layer, and how do you just let them go and how, and like I think you have source of truth systems and get lab between filing issues so you can keep things up to date. Tho those are the things I'm really thinking about. But, but the. My bet right now or the general bet we're making is like the models are only gonna get better. What, what's gonna be unique is the context and how you wanna manage that context and the kind of guard rails you're building around that, which I, I don't see again CICD is not going away. Context management's not even developed yet. Agent memories like you wanna be able to experiment with lots of agents and it's moving away from models to agents now because it's multi-agent systems, so. I'm thinking a lot about how do we help customers get their context in place, how do they have their CI in place so that as those new things come up, more people can experiment without issues, and it's definitely been a journey. Yeah, and that's one of the things I wanna leave everybody with as well is, um, uh, the one thing I've learned from many different customer journeys is patience. Um, if you are an engineering leader, be patient with your team. This is something where it's new and you've gotta let the people experiment and try different things. Don't ask them to go and refactor, uh. 10 million line of code application and make it microservices and stuff like that and yell at them on Friday because it didn't happen in 3 days with AI. Um, the other thing is, is hallucinations happen, um, and it's really important if you think that you're just gonna hit the go button. And everything happens. I have the opportunity to be able to work with a lot of university students, and I'm like the part of the exams that you all hate the most is where your job is gonna be to have the most impact, and that is somebody's gonna give you 100 lines of code and you're gonna know there's 5 bugs in it. Point to me where those 5 bugs are. And you're not going to have a way to be able to just sit there and nope, I'm OK. I'm just gonna go build it again and I'm gonna start over. You have to learn how to use AI, how to possibly create a very unique prompt to be able to say, here's these 100 lines of code, help me debug this, and then have it almost like a debugger walk you through the steps of helping you debug it as a peer, um, so. That's one of the things that I hope everybody can take away from this is, um, you know, everybody's going, uh, on different journeys at different paces. We're here to help, uh, AWS and GitLab and um. Southwest has been a great partner for us along this journey, so I'm hoping that you all learn something new and if you guys have any questions, I'd, uh, welcome for you to, uh, stop by our booth. Um, you can also start a free trial. Go ahead and, uh, uh, join us on this journey and come play with the tools and thank you all very much for coming.