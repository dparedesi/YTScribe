---
video_id: DipWKtNdIs8
video_url: https://www.youtube.com/watch?v=DipWKtNdIs8
is_generated: False
is_translatable: True
---

OK, Thank you, thank you, and welcome. We're gonna go ahead and start today with a few interesting facts. The first being, did you know that for every breakthrough drug that makes it to market, pharmaceutical companies analyze over 10 million patient data points. And yet 80% of healthcare data remains unstructured and difficult to access. In today's rapidly evolving healthcare landscape, real world data and real world evidence are a game changer in how we understand patient outcomes and develop new treatments. In fact, studies show that leveraging real world evidence in clinical trials can reduce clinical trial times by up to 40% and cut costs by millions. I'm Anne Evans and I serve as the strategic solution development program leader in AWS Industries Healthcare and life sciences. Today I'm joined by Greg Cunningham from Eli Lilly, who will share how Lily is revolutionizing their approach to real world data. And Eric Brooks, our AWS principal solution architect who will dive into agentic workflows and how they're, we are transforming data accessibility. So whether you're a pharmaceutical researcher, a healthcare provider, or a data scientist, we're genuinely excited to share how we've been collaborating with our partners and customers to accelerate and simplify access to real world data. So let's dive into how data is used in healthcare and life sciences and how AWS is revolutionizing data access. In the last decade, global data generation, all data being generated. Has increased elevenfold from 16 zettabytes in 2015 to 181 zettabytes of data floating around at the end of 2025. The same is true in healthcare data. Between the imaging data, the lab data, all the data that is becoming available in healthcare, we are projecting a 10 zettabytes of healthcare data being available at the end of 2025. This proliferation of data availability has driven transformative growth. Evolving the use of real world data and real world evidence from an emerging field to a mainstream component in pharmaceutical development and healthcare decision making. But while this exponential growth creates unprecedented opportunities, it also creates critical challenges, such as how do you organize, access, and make value from such massive volumes of information. So before we dive into more specifics about this, with a quick show of hands, how many of you are familiar with real world data and how it is used in healthcare and life sciences? Excellent, thank you. So for those newer to this space, real world data is healthcare data collected during routine patient care, which means it's everything from electronic health records to wearable devices, to the insurance claims we all make. Real world evidence is what we learn from analyzing this data. This evidence is used to provide insights into the possible benefits and risks of medical products. It's also used to study clinical conditions and biomarkers when developing new drugs. Driven by an increasing demand for data-backed decision making, for accelerated drug development, for value-based care models, and of course the continual regulatory expectations across pharmaceutical, biotech, and med devices. This increasing demand for evidence and insight is driving a surge in the demand for real world data-backed studies. The evidence that is being generated is used to. Help boost preventative efforts to identify patients that are at risk of illness, or eligible for clinical trials. It's used to shape clinical trials. It's also used to create public policy and expand drug safety testing. It's used to determine the value of medical-based intervention and to establish reimbursement strategies while supporting regulatory expectations. We create this flywheel because the more data that is available, the more evidence is being used, creating that demand for yet more data. This is why establishing a strong data foundation layer is essential. To effectively use the data and allow agents to use the data to generate these insights and evidence. We must start with a strong data foundation layer. In life sciences, this includes historical data that is sitting in your warehouses or on individual scientists workstations. It's experimental data being generated in your labs, computational data being run in simulations, and then when you take all of your data. And you mix it with third party data that will enrich your data, increasing the breadth and the depth of the questions that you can answer with information previously unavailable. The ability to make that data work for you is limitless, but we have to remember it all starts with the data foundation layer, because the most sophisticated AI in the world will only ever be as smart and effective as the data that it can access. Despite this tremendous growth that we've talked about, finding the right data, the data that you need, continues to be messy and challenging, it's unpredictable, which in this case is fitting for data that comes from the real world. Today, 40% of data purchases are incomplete. Which really means ineffective. And given that real world data is collected all along that patient care journey, there's no one source, no one provider that will have all of the data that you need. And yet 70% of studies require very specific fit for purpose data that is linked across multiple data sets. And with HIPAA requirements that all of our data remains de-identified. Connecting patient data across multiple data sets is extremely challenging. When you're looking for very specific fit for purpose data. This fragmentation. Creates major hurdles. For example, it typically takes 4 to 12 weeks just to find and obtain what looks to be the necessary third party data. This is further complicated by data producers' reluctance to share data outside of their control. And once you've obtained the data. Each, because each producer will use their own unique format. You still have to harmonize that data, which is going to add another 4 weeks to your process. And then once you've done all of that, and you have your data, your different team members from the computational scientists to the experimental scientists, they all need their own tools to make that data effectively work for them. All of these challenges create timelines exceeding 10 years and cost in the millions, depending on the therapeutic area, even in the 2 to $3 billion dollar range, just to put that new drug out to market. At AWS we're approaching this complex challenge with a comprehensive multi-part solution. In the first part shown here, we're streamlining data discovery and access. We've been collaborating with our AWS partner Datavan. To build a solution on AWS clean rooms that uses data advance privacy preserving technology to help researchers quickly find, analyze, and obtain linked patient level data sources. In this solution, data producers host their tokenized personally identifiable information in their AWS account. Data consumers can then discover and evaluate multi-modal fit for purpose data across Datavan's broad network of data producers. Consumers will then negotiate directly with the producer. Subscribe to the data in a private offering with billing directly through AWS data exchange. Dataman's proprietary software takes an individual's personally identifiable information such as name, gender, and date of birth. And converts it into a unique string of characters known as a token that de-identifies the information. Now this is a token that is different than an LLM token. This token is specific to Datavan that is within their proprietary software. Each token in each data set is unique to an individual patient. And yet with Datavan's proprietary software, consumers are able to connect patient level data with this token. All without revealing any underlying personally identifiable information. Because that same patient will always generate that same encrypted, non-reversible, deterministic token. That same patient unique token can be used to link data sets with that token across all of these multiple data sets at the patient level, which will provide that longitudal patient journey. And this tokenization happens in the data producer's container behind that data producer's firewall, ensuring that no personal identifiable information is actually leaving the data producer's secure system. Through our collaboration with Datavan, we have been working with healthcare organizations and life sciences companies to bring this tokenized data into AWS clean rooms, transforming how we access and analyze these valuable data sets. Datava Connect, powered by AWS cleanrooms, gives data producers the ability to discover and evaluate fit for purpose multi-modal data in a HIPAA compliant privacy preserving manner. For data producers, this new discovery and evaluation method allows them to share their data with data consumers while maintaining complete control. Because no data has actually moved in between these environments, data consumers retain the ability to share as much or as little of the data as that data producer deems necessary. For data consumers, multiple personas are able to discover and evaluate fit for purpose data with an easy to use front end agent that acts as their own virtual assistant, helping to ensure that they find the right data. So how does this all work? Data sources host their tokenized personally identifiable information. In that, in the AWS account. They registered directly with the Databan Connect platform. They do this by creating a glue database and a glue table that has pointers to their S3 bucket and table schema. Consumers then evaluating different data sets will use AWS Cle rooms as a secure neutral evaluation space, again, meaning that no data is moving between these environments. While researchers are able to evaluate multiple data sets simultaneously, drastically reducing evaluation time. Data producers don't have to manually update their information in the data connect platform because that live connection with AWS cleanrooms ensures that their latest data is always available. Then once the right data is found, Datava will certify the data, and it can be seamlessly procured through AWS data exchange. Pilot customers have been testing this solution. And they've been sharing how they are unlocking access to data that once available is accelerating timelines like never before. For the second part of this solution, shown here in blue. We have been working with partners such as ATon and their Activate platform, Manifold and their data layer, and Palvo with the IHD cloud. To enable faster insights through modern scalable data harmonization platforms. Which allows us to support both technical and non-technical users. Technical users, like biostatisticians and data scientists will use open source tools like R and Python. But non-technical users can access these agentic capabilities to build cohorts, applying that inclusion exclusion criteria when analyzing that longitudinal patient journey. Through our collaboration with Eli Lilly, we have developed this agent. That allows non-technical users to analyze complex healthcare data sets using natural language. Built on AWS's AI stack. Our agent seamlessly. seamlessly connects to data sources across Redshift, S3, Athena, and data bricks. Now, with data delivered directly to the S3 bucket. With automated harmonization. Technical users continue using R and Python, and non-technical users can access the agent to act as a persona-driven virtual assistant, making sense of the data and providing insights instantaneously. All within a responsible auditable framework. The streamlined approach eliminates the traditional bottlenecks while maintaining data security and data privacy. And what used to take 4 months now can be done in under 4 weeks. As we continue to navigate the evolving field of real world data and evidence, AWS continues to actively collaborate with multiple partners and customers to tackle the complex challenges of real world data discovery and analysis. Companies such as Manifold, Deloitte. Penalgo, ATon, EPA, and Atropos are all leveraging AWS infrastructure and incorporating AI agents to transform real world data analysis. It's important to remember, these solutions are not just about data processing. These solutions are about intelligent automated decision making. That is identifying patterns, generating evidence and insights, that is accelerating that path from data through evidence to discovery. But as we stated earlier. It all starts with a data foundation layer. Because even the most sophisticated AI in the world will only ever be as smart and effective as the data that it can access. So with that I am really excited to welcome our next speaker to the stage. Who will share firsthand experience of putting this technology to work. It's my pleasure to introduce Greg Cunningham, senior director of real world data from Eli Lilly. Greg brings an impressive 12 years of real world data across Lily's pharmaceutical portfolio. Greg's insights will be invaluable as he shares their real life experience and concrete examples of using this technology. So please join me in giving a warm welcome to Greg. Thanks. Thank you, Anne. It's great to be here. This is the first time I've been at Reinvent. Um, like I said, I, Anne said, I work at Eli Lilly and Company. We're a pharmaceutical company based in Indianapolis, Indiana. Um, uh, unlike my 4 sons, I've been at one company for 39 years. I've been the last 12 in real world evidence responsible for our licensing of data. And also the data platforms that we have the data on. Prior to that, I worked in our clinical trial area working on new drugs that made it to the market. And today I want to talk like Ann said. About kind of a couple of things the business need for a scientific company, but I think this goes beyond just a scientific company. I think any company can follow the pattern that we have done with AWS, so we'll we'll dive into that. In pharma companies, the gold standard for data, that data foundation is clinical trial data. This data is very clean. It follows very rigid standards. It's, it's really looked after to make sure it's high quality. And when I first started with Lily, we would do a lot of phase 4 studies after we got approved. These studies. Helped us understand what was going on with the patient after the drug was approved because there was really no other way to know what was going on at that time. But then healthcare changed with the You know, health care records going from paper to electronic. If you go to a doctor's office today, they aren't writing a lot down on paper. They're doing most of it in tablet or computer to enter directly. So we found a new source of healthcare research data that allowed us to stop doing costly and slow clinical trials to do more real-time work of ingesting data like this that I'll talk about in a minute. But first, what is real world evidence? I have a definition that the FDA uses, and you know, it's really, it's clinical evidence about the use and the potential benefits and risk of a product derived solely from these electronic health care sources in the system that we all use in our US health care system and in other countries too now. And at Lilly we kind of have formulated our own formula to get this evidence you see to the right. There's 3 key ingredients. First, you have to have a scientific question or a hypothesis you want to test. The foundational thing I think Anne covered really well is the data, getting the right data. Our group spends time helping people get the data that has the best quality for their question, but also looking at what variables they need and ensuring those variables are present in the data sets they use. And then finally having a statistical analysis plan and then the execution and the analytics. This RW is increasingly more vital and important to life science companies because we want to have actionable real-time insights to what's happening to our drug now that it's left the clinical trial area and it's being used out in the real world and the results are not always the same and there's sometimes surprises on what patients you're getting in the market. Real world evidence is used across the whole product life cycle that we have, um, you know, when I look at the uses, we use it in our early phase discovery as we're trying to discover new molecules. 2, we use it in our clinical trials and we use it post launch like I mentioned earlier. The clinical trial examples I can give. We have protocols that we are always trying to improve to make them easier for the physician to do the trial and in doing that we use real world evidence to identify what's the best. You know, inclusion and exclusion criteria we can have, it's best to use real data to then make those decisions at what cut points you get patients with the disease. In the health outcomes function that I am part of, we use, we do observational studies. So these studies, we're looking to demonstrate what's the economic value of the product. And what's the comparative effectiveness? We'll look at one of our drugs versus competitors. We know the clinical trial results. We also will publish papers to show is this drug performing the same or higher or lower in the real world. And our end goal with that exercise is to publish in scientific journals, and this is one of the high priority things we do in our area. But after you launch in the market, we also use it to look at patient characteristics, who did we think we were going to get in our marketing plans and who actually is taking your drug and how is it performing. There are a variety of RWD types and talked about it at kind of a macro level. I'll I'll give you some perspectives of how we see data and using it today. The last 10 to 15 years we've used that foundational data you see on the screen. It's been the tried and true data for real world data, electronic health records as it became electronic. It's got great clinical variables for people to use to understand. Lab data is usually in there. The one we use the most in health outcomes is claims and billing data from insurance because it gives you every health care transaction someone had over a long period of time because most of a lot of us are with the same insurer over time. It's very nice to give you that long view. I would say that the foundational data is typically broad. It's all diseases because they're big databases and that's what they're used for predominantly. The most exciting thing in the last few years is this emerging data. Our holy grail is to have data that really is disease specific that tells you is the disease getting worse or is it getting better, so wearables, sensors, and health apps are really, really critical. An example of how pharma companies use, you know, an app. I've seen migraine studies where they've had. People doing using the app who are migraine patients, they enter every time they have a migraine and they enter the severity. Well then that gives us the ability to see what drug they were on and then which, which drug is actually reducing the severity and or the amount of migraines. So this merging of broad data and this deep, rich data is extremely helpful. As the human genome is continuing to evolve, biomarkers and genomic data are critical. Our oncology team uses a lot of this data with our real world data to match up and see which biomarkers are really making an impact with our drugs and are modifying the disease or treating it. The last thing Anne talked about the token, so I won't kind of talk about the process, but the token is allowed us to take this broad longitudinal data and match it up with deep and rich data from the disease and we're able to do much, much richer analysis that is helping us understand the patient journey that people are on and how do we help impact their lives. The acquiring of real world data and ingesting it is a process that I think you're starting to see Datavan and AWS really crack and change where this is going. Our process isn't much different than you see here from other pharma companies. We spend time, OK, what's the scientific question? What's that need that someone has? And then our team, the real world data team, looks at how do you evaluate what data is going to best meet that need. We have data already purchased and licensed inside versus the external market, which is changing every, every week. So we spend on those first two boxes. I've seen it take, I think in your numbers was months. I've actually seen it take up to a year at times. So we're trying to figure out how can you do this better and faster. Once you've made the decision, the transfer, ingestion, and moving the data, we use Redshift database for our analysis. Those things we're real excited because the Data van AWS. Solution they're looking to put in place will take that from I've seen this take a couple months and you know if this can get down to a month or less it allows us to use more current data, real time data faster and so we're real excited with where the technology is going in this space. You know, I've talked about real world evidence. I've talked about real world data to set the context of the challenges we're facing in generating evidence from real world data. Having been in this space more than a decade, I've seen us try to hire. Analysts, statisticians, data scientists, and we have made huge leaps, but we're still dependent on analytic pros who are those data scientists and statisticians. These end users, our scientists use and count on these folks to do those publications, to do regulated work, so it's for us it's the core. It's the most important work we do. We have troubles just staffing that with all the improvements we've done with all the changes we've done that is really keeping us busy and we've added more and more people and it's not solving the problem so we we took on. I'll talk in a minute on the next slide about what we did to try to address this. The second thing we've had, it's a significant learning curve. This data is not collected for like a clinical trial. This data is collected for insurance. It's collected for your doctor's notes, for billing. It is not meant for this, so it is not as clean and accurate, and with that and then the completeness issues, this data is much more difficult and when new analysts or new statistician, it takes up to 6 months for them to be really productive in this space. The third issue is real world data does not have clear standards. There are several floating around. Not one of them has just not taken over and made it easy, and this hinders the ability to make tools or write standard code that can be used and leveraged. So what we did to expand on, we had the first two columns you see, um, our original users were doing complex coding using AWS cloud platform. We've we've done a lot of great work in this space, um. We then said, OK, we've got to do something different because we can't get enough work. We took some of our scientists who actually did coding in college. They did their own analysis. We'd let them use an analytic tool that's on the market. It's a low code environment. They're able to enter parameters and get output themselves, answer questions themselves, so. To the left, we use, we do those publications, we do those regulated activities. This middle is to let them generate their own answers, but we found that not all the scientists have the same background. It's not easy for them all to use a tool. So we worked with Amazon to say how can we change the game, how can we have a no code environment where people can enter a text question to get results running across our data. So we've been working on this for over a year and it has come to fruition. We're going live next week. And those scientists users are able to get questions answered themselves. They can even do it in a meeting where someone on the clinical team asks, Hey, how many patients? What's the percent of patients that have disease X? or what's the most common drug in this space? These are questions that we don't have time to give to a statistician. It may take a week or longer for them to do it if they can get to it at all. So this is a way to let self-service take over. I would say that the biggest things we've learned through doing this is the agent, the LLMs, and all those things. They do great work, but they need context. They need business context that that's in my head. It's in our analysts, our statistician's head. So you know, really putting those things that run before the agents so that we can put that context in and allow the agent to write better code. The second thing we learned in the testing was your questions, the more general, the more assumptions you're going to have from your, your agent because it's trying to figure out what you're saying. So we've told our users be very specific. If you want data from the prescription claims, say that in your question. Don't leave it to be hanging out there. And this has produced a greater accuracy. The other, the last thing I would say is we, as we had people do questions, I thought you just put in a big block of questions and it would spit out the answers and be enter it, get your answers, move on. It's a dialogue. You have to ask question, then ask another question and another question, and the answers and the accuracy of those answers are have gotten better because of that. Scientists by nature are very skeptical, so we're worried about releasing a tool that people who are very skeptical will say, Well, is this answer right? And I've had that question 4 times already from our scientists. And so we're going to do some tests of real questions that they ask as we go live to just give them a little more sense of, yes, this is on the right path, the code being written. The sequel code being written has actually been very good and it's improved every, every step of the way so far so we're very, very excited to go live with this next week. We did a pilot group and with the pilot group we Our intent was to take people who are more tech savvy, take people who are into this, and they're going to be the soft launch that we go with, and then we'll go with a hard launch with the people maybe who are less techie, and those people who sit with them can work with them more. So I think that's been some of the learnings we've seen so far. So I will finish here and I'm very excited to have Eric Brooks uh join us and give his firsthand experience of implementing what we've just discussed. Um, Eric is a principal solution architect from Amazon and he has worked with us over the last year and a half to bring this to life. Please give Eric a warm welcome. Hey everyone, uh, thanks, Greg, I appreciate it. Um, you know, I, what we've seen so far is this, this journey from really data acquisition. You know, we're buying the data from various providers. We're loading it into Redshift or S3, you know, they were governing access with permissions through Redshift Access, uh, through Sagemaker Lakehouse, uh, and we've gotten to the point now we've made data available. But we have to tackle this complex multimodal disparate data set kind of landscape because as Greg described, what we're not talking about here is a simple sales data set. This is not simple data that can simply be queried by a text to SQL agent. And what we're also not talking about is a simple text to SQL use case. One of the most common questions we get is how does this type of a solution differ from A common text to SQL, we've seen this a million times already, you know, nothing new. And so, uh, as we explored this use case uh with Greg and his team, what we found is that this is very much well fit for an agentic use case. But as we try to move through this process of taking this mountain of data, one data provider, I think, provides something like 15 terabytes on a monthly basis of new data, how do we go ahead and tackle that kind of an evolving landscape where data might be siloed across different services, maybe different data modalities, perhaps even different data schemas or oftentimes different data schemas. You know, with a research team and ultimately a consumer team which aren't SAS coders, they're not our coders, they're not Python developers, they're not deeply knowledgeable about this use case. They don't know that things like continuous enrollment matter when it comes to claims data from a quality perspective. And so what we're trying to tackle is this how do we build a system that uses the most recent frontier models from Anthropic and Amazon to make sure that we can democratize the access to this data and make sure, as Greg mentioned, that it's the most accurate and the highest quality outputs because these are these are critical things that that people are interested in at Lilly. Uh, and so, you know, and then we have to decide how are we gonna architect that system, and then, and of course that's what I'm here for, uh, to talk about today, um, but I, I always think it's good to start with a really solid mental model. So as we think about agents, there's a lot of talk about agents in the market. We heard a lot of updates from Matt this morning about Agent Core and about all the capabilities within AWS to build agents, but I always like to start with a good mental model so that I can understand the system I'm trying to build. And so the thing I always start with is what's the goal, you know what's the user experience I'm trying to provide? Am I just trying to provide SQL queries or am I trying to build a system which outputs real analysis that has rich insights, that has, you know, solid visualizations, that has a really high speed experience, sort of high quality, high performance experience. So, so what am I, what am I working backwards from? At Amazon we like to talk about working backwards, and that's a very important thing to define is what's the goal and what's the goal of the agent. And then I start to think about, OK, well, if I understand the goal, then I know maybe what tools I'm going to use as a as a human. I have lots of tools that are available to me. I have a Python notebook or I have a SasS console or I have my email client. Anything that I use that lives within the technology realm is a tool of some kind. I have a tool in my hand right here. It doesn't do that much, but it's pretty important to the job that I'm doing. So what are the tools that I need if, if let's say I was a human doing this process or doing this job, what are the tools I would need to get that job done and probably more importantly, what are the tools that I need to be able to develop the knowledge. In the moment where I've been asked that question, to be able to answer that question, that's a really important thing to know. Then I have to start to define the protocol. What are the steps I have to take to get to that thing that I'm trying to go towards? And the goal and the protocol are really what's going to help you start to work towards what's the prompt that I'm going to hand to the agent, and we'll talk about this in a few slides in order for it to be able to do its job. And then finally if I'm thinking about a multi-agent system, what then I also have to start thinking about are what are the handoffs because what I don't want to do is say I've got 6 or 7 or 8 agents in a system and I'm gonna hand every agent all the context of all of the conversations of all the things that have come before because that's not a a a successful architecture approach. So what are the handoffs if if I have a team of of people working on a problem, I'm not just gonna do a. Data dump of all the things that have come before I'm gonna say here's the things you need here's what I'm asking of you, and here's maybe some guidance on perhaps how to do the job please, you know, use your tools, come back to me and that's very much the mental model for how to start to think about and align multi-agent teams to be able to accomplish a task. And so there are critical decisions we have to make along the way. The first one is what type of system is going to fit my use case the most effectively? Can I build it with a single agent? Can I simply just have a Texas SQL agent which uses the myriad of tools and the capabilities that I need that has a prompt, that has additional context, that's going to be able to answer the questions that that total. Set of questions that I think I'm gonna have to answer or do I wanna think about this more in the sense of specialized skills and capabilities and knowledge sets context that's gonna be able to break a problem down and be able to answer it effectively one of the biggest learnings for me in working with Greg and his team early on was that. In the context of real world data analysis, going to that real world, you know, getting to that real world evidence outcome is there are lots of different personas or skills that are involved in this process. It's not just a Texas SQL agent that looks at the data and says, you know, here's your SQL statement, and you move on. That's not the problem we're trying to solve here at all. And I think, as Greg mentioned, in a lot of cases, even if you're not looking at real world evidence or real world data. You may find that some of these similar kinds of aspects are also a part of your use case. It's not just about the data set. There's adjacent data. There's context that you need to be able to answer complex questions to really build a system that scales and that generalizes to a larger problem set. And so as we look at the single agent versus multi-agent comparison, we have this trade-off. It's the classic architecture trade-off. It's complexity for capability. And so in the multi-agent system as we did in this case, what we ended up with is this more complex system which was orchestrated by an underlying framework which enabled us to define agents which encapsulated the key capabilities, those functionalities that were necessary to be able to scale to meet the needs of the team at Lilly. And so as we look at a multi-agent system, then we start to realize that maybe we need a manager of sorts, you know, if we have a larger team, we like to talk about two pizza teams at Amazon, and usually that team has a manager and that manager is sort of the decider of what the business wants, say if you have a developer team. And so that manager, in this case, the supervisor. Is an agent that works with the user that's kind of that front door so the agent is going to be able to identify what the user is looking for, do some entity recognition, some, some sort of, um, you know, some summarization, and then it's going to work with its partners. It's the the Sega SQL expert, a medical coder. These are some of the aspects of, of this use case that are really important. Even a research planner, an agent who's familiar with a general methodology of how to answer questions in this context. As well as the data navigator, one of the things that we found very quickly too is because of what Greg mentioned around the fact that there's not a consistent standard for most of these data sets and the fact that we don't have the convenience of saying, hey, let's ETL this data all into the same format. Into the same schema that we need to be able to meet the data where it is and so we need to be able to navigate multiple data sets to understand very quickly on where to find the answer to the question in a in a data set which has tens of tables. And then finally we need to be able to actually interact with that data set. So how do we interact with the landscape? And it's not just the underlying data. One of the biggest learnings for me was that there are many, many code sets, for example, as reference data that's critical to translating a human question, which we'll see here in a couple of slides, into what we actually need to know. So it's not necessarily in the data set, the thing that we're trying to find, but I need to take a couple of stops along the way to be able to get the right context. And so as we started to think more and more about this problem, this is where the module 3 from what Anne was talking about earlier really came out. And so what we landed on was 7 specialized agents. The first, of course, is the assistant, that's the supervisor. That's the thing that chats with the user. The next one is the SQL creator, of course we can't do this without interacting with a structured data set, so of course we need to write some SQL, which is great. Then we need a medical coder. So if I ask a question about, say, a diagnosis or some medication or some lab procedure or other things, or maybe I ask about all three, I need an expert in medical coding to make sure that I can translate that human question if I don't know all those codes, and most people don't, neither do the LLMs, by the way, without the right context. Into the real question. Then we need to create visualizations. We need to make a research plan potentially for more complex questions. We may even need to consult some publications. And then finally, we need to be able to navigate that data and do so in a way which integrates with that complex harmonized data set that you see in module 2. And so it's always good to zoom in on one of these agents to really see what it's doing. And so the neat thing about an agent is it works very much like my brain does, like, like your brain probably does, where you say I've got a problem and I want to solve it, but I have to iterate over it. I have to think about the problem and understand the problem before I can go and solve that, and that's exactly what an agent does. It has this iterative execution loop. It's a cyclical node on a graph in this case that takes a number of inputs. So it starts with the prompt and the reasoning loop, this notion of iterating over a problem until it's solved and really until the agent decides it's solved, which is even more interesting. And so it's got to perform some kind of structured reasoning. This is something that's built into LLMs, so you hear about tool calling, function calling, instruction following. These are all aspects of what makes LLMs the reasoning engine behind this process. Then I need to be able to do things like in this case, since my task is to plan and execute SQL queries and provide analysis, I need to be able to plan the query structure and potentially have examples. And I've got the graph state. This is all the things that happened before. Have there been other actions, other agents? Has the medical coding agent already deciphered all that complex human language into a set of codes, or have there been queries run before? Has somebody asked a question and then ask a follow-up question as the process that Greg described? But there's there's something missing here because this isn't enough to be able to accomplish this. We need a set of tools and we need a set of curated tools. To be able to make this agent able to do the things it's doing just like as a human, I would use a say a query reference database or I would use a data set metadata. So for example, I might use a data dictionary or I might go into the database and look at the actual table DDL. So I'm going to be looking through table and column names, foreign key relationships, data types and constraints as I'm constructing a query, and that's exactly what an agent has to do. It has to have the context to be able to solve the problem in front of it. And by the way, this is something that applies to any agent. Remember back to our mental model. This is about defining the tools that enable the agent to accomplish the task. But then there's more to it. There's few shot prompts, so we've got that in as a part of the tool set. So an interesting part about this is this is a good way to integrate that human in the loop. So what we found here was that a great set of baseline ground truth queries that answer related questions are a really, really good way to encourage an LLM to make the right decisions when it comes to query generation. And so taking those golden queries as we've stated here. And making them available through a semantic search, so a similarity search or a similar meaning search to the agent on demand through a tool is a great way to encourage and improve outputs. Also having SQL validation, so we have a very quick way to validate those queries, and then the ability to actually execute and manage the outputs of those queries is really important. And then finally what that leads us to is this idea of being able to not only generate multiple queries, execute them, troubleshoot them, gather, gather information and context around them, to then take that and actually turn that into analysis. And so that's all part of that define the role, define the tools, define the protocol, and then define the handoff that results in a set of agents which can answer a complex set of questions through this iterative cyclical execution loop with context gathering and then eventual analysis. So let's talk about how this actually works. So we've got a question here. What is the age distribution of patients diagnosed with hypertension who are prescribed drug X in the last 12 months? So there are about 4 or 5 different attributes depending on how you look at that question. So how do we break that down? Well, it starts with the supervisor, of course, that's the entry point to this graph. And so the supervisor is going to look at that question and say, well, I've got to come up with a plan. I see that there's demographic information, there's a diagnosis in there, there's a drug in there. Uh, there's a time frame in there, so I've got a bunch of things I gotta figure out. So the thing I'm gonna do first is let's stop, let's take a stop at the medical coding specialist. I gotta figure out what's the coding for hypertension. Is it one? Is it many? Uh, what's the coding for the drug I'm looking for, right? And then what's additional information that might be of interest? What are there lab codes that might represent that drug? It's one we, we ran into that was unexpected. So once we've stopped at the medical coding agent, that's great. Now we got to figure out where this is in the data set. So we stop at the data navigator agent and each one of these stops is going through this iterative loop of understanding the task that's given to it and providing a high quality output for the rest of the system to use. So then we stop, of course, at the SQL expert agent which we just talked about, and then finally we land on the visualization creator because the supervisor decided that this would be a great use case to generate some visualizations for. So not only do we get a high quality output that's iteratively followed almost what it feels like a very human logic kind of a process, then we've also been able to give some some high quality visualizations based on the context of the question that was asked. And so as we built this system, we learned some pretty valuable lessons. The first one was a great thing about multi-agent systems is multi-agents can be evaluated individually. And so it's really important as you start to think through the problem set that you're addressing and you start to build individual agents that do these these critically encapsulatable, almost atomic kind of skills things. That you make sure that you validate them individually. Do they do the thing that you think that they're supposed to do, given the problem set that you've defined when you thought about the original problem set? You got a deep dive on guardrails is the second one. So one of the things that we found very quickly was that the complexity and the nuance in language of the questions that people would ask in this space is really, really important to get exactly right because the criticality of answering, say, a safety related question or not is, is paramount. And so being able to understand some of the nuances and deep diving with the subject matter experts on what exactly are some of the nuances that they do or do not want to allow from an inputs and outputs perspective, what's in the underlying data, are there, say, de-identification kinds of concerns or data privacy issues. Deep dive on guardrails is really, really important. I'm sorry, um, and so the next one then is perform system level validation. So just because you've you've validated the individual agents doesn't mean that when you put them all together that the system is going to do what you expect. So make sure that you come up with a good set, a good test set of questions that you can run through on a regular basis as not only you move towards production, but then also as you change language models. GP or Sonnet 4.5 was just released, I don't know, a month ago or so, maybe not even, and. The pace of model change is going to continue to increase and so as new models come up, it's very important to be able to switch quickly to new models, which means validation, which means testing, which means automation, and these are all really key things to build into your original system. Including human in the loop is also very important. Improving the quality of the outputs based on the quality of the inputs. I think Anne mentioned this early on. This is almost a garbage in garbage out situation. So including a human in the loop in this case in a very passive way to provide approval of golden queries and human feedback on the quality of that ground truth reference for the agents is really, really important. And then finally develop and track metrics. Human feedback from experience, uh, quality of outputs, uh, even just latency of query execution, these are all things as you start to instrument your system with observability capabilities like say as an agent cops, you really have the opportunity to continually and iteratively improve the solution that you're building by developing those and tracking those metrics early on. And so finally, from a from an overall standpoint as you think about building agents, the key aspect here is taking that raw data into actionable insights, and as you see, that's a multi-step process. A person can't do this in an instant, and neither can an agent system, but the key is to build yourself that research team. To be able to accomplish the task that you're going for, the second one is really key, and Greg touched on this, is this context aspect. So it's about architecting a set of tools, you know, an overall kind of system architecture which results in the ability to capture the right tools that are going to curate that context to each agent in the right place at the right time. Um, and actually one thing I'll mention on that past one is model choice. A lot of attention is paid to model choice these days, and it's very, very important, but increasingly so as the models continue to evolve, context becomes a critical aspect of high quality, uh, high performance agent execution. So the next one of course is testing and observability, and a lot of this has to do with rigor around application or solution development and then also picking the right framework like a strands agent running on Agent Core to be able to quickly and easily instrument your system with the observability capabilities that you need to move from dev all the way to production. And then, you know, building an extensive ecosystem, one of the great things about multi-agent systems is you can extend them and add more capabilities as your problem set evolves. You can build reusable agents for different things. You can imagine that the medical coding agent can be used for a lot of things. It's not just exclusive to this use case and so. Being able to build individual capable agents and then continuing to add more and more agents, you know, agent to agent communication now as a protocol is becoming very popular for this exact reason. So that's a really important thing to be thinking about is building that ecosystem of agents so that they can plug together is critical. Safer and auditable workflows. So you know one of the things that like in this case, use case embedding compliance into the reasoning process, being able to manage things like citations as a part of the output, is a really important thing to build trust in consumers as they interact with an AI system because they don't see all of the reasoning, so they need to know how they, how the agent arrived at the output. And then finally that human plus agent collaboration is really, really important. Greg touched on it with regard to that, you know, kind of interaction process of question and answer, question and answer dialogue, but it's also really important, as I mentioned earlier, for a human to be involved in a less direct capacity to provide that ground truth, that subject matter expertise is critical for both this and probably many of the use cases that you might be thinking about today. And then finally, of course, we always want you to build on AWS. The, the agent services continue to evolve on the AWS platform to an incredible set of capabilities that enable you to move forward with your development to production process. And with that in mind, uh, we will be at Sushi Samba tomorrow, 12:45 to 2:45, so please come and see us. Uh, we'll be doing a demo of this actual solution on a real world data set. Um, so please come join us, uh, and uh hopefully we'll see you at Sushi Samba tomorrow. With that in mind, the life sciences team at AWS has also built an incredible set of accelerators in addition to the RWD agent. This is things like competitive intelligence, what we call clinical supply chain control tower, and the intent of these solutions is to say how do you get from idea to production in the fastest way. You can take that native build process, which I know as builders we like to do, but then also you can use. That AWS OS toolkit for life sciences as a next step. And then finally, of course, with the accelerators that we're building and iterating over with customers like yourselves, we're able to get there even faster by doing, say, 40 or 50 or 60% of the work already so we can hand you something and partner with you to move forward to production. So really excited to hopefully see a lot of you at Sushi Samba tomorrow to talk about real world data as well as the other accelerators that we have. If you're more interested or if you're interested in more information, grab the QR code here. Um, get some more information on the agentic accelerators, of course, also in the massive expo hall, please come and see us at the industry's pavilion. The Life Sciences team from AWS will be there today and, and for the rest of the week, and we'll be excited to see any one of you as you come out. And you know, please again thank you for your time today. Hopefully you learned something from this session. Please provide feedback on the session content with the survey. Um, and, uh, I hope everybody has a great rest of their day. Thank you.