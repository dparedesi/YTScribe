---
video_id: uFIRieQbPvg
video_url: https://www.youtube.com/watch?v=uFIRieQbPvg
is_generated: False
is_translatable: True
---

Awesome, thanks everyone for coming out today. Uh, my name is Vishal. I'm the product manager for EC2 Mac at AWS. I'm super excited to be joined today with two of my customers that I work very closely with. We have Miranda from Riot and we have Tony from SuperSell. And so super excited for you guys to hear from them as well about how they use EC2 Mac. To help build and test their own applications for their organizations, so I'm gonna kick us off today with a little bit of the history of EC2 Mac and specifically, you know, how we came about EC2 Mac on AWS, uh, what are some of the big pain points and key struggles customers told us they faced, how we at Amazon use EC2 Mac internally. And then we'll actually pass it over to Miranda and Tony to actually give us insights into their own organizations about how they've implemented EC2 Mac and some of the benefits that they've seen. Cool, I'm just gonna make sure that everyone can still hear me all right you guys good? All right, awesome. So let's take a little step back around understanding the application development life cycle for Apple platforms. In the earlier stages, we have the design code and develop stage. These necessarily aren't Mac OS dependent, but as we go into the build stage and onwards, you need X code, which everyone knows Xcode needs to work on Mac hardware, and that unfortunately means that you absolutely need to buy Mac hardware and scale as your applications increase as well. So it's this dependency between Xcode and Mac OS that made Apple development on any hyperscale impossible until we introduced EC2 Mac back in 2020. Today we have the total market for this opportunity is well over 34 million developers. Many of you who are sitting in this room or will be watching on YouTube when this gets posted, and the last time I made this deck, I think it was about a million apps. I think that we've now scaled to 2 million apps on the App Store and more than 2 billion. Active devices as well, um, and we've also introduced a lot of new or Apple's introduced a lot of new platforms including Vision OS that came about a year or two ago and today with EC2 Mac we have customers across the board, different industries, different functions building, testing and developing applications for each of these platforms. I wanna share with you today some of the key struggles that some of our customers told us when they, uh, when they were giving us feedback around EC2 Mac and the, you know, the business case as we start to do it. The first thing was infrastructure overhead. Um, Miranda's gonna talk a little about this, uh, uh, in the, in the future, but I'm super excited to, you know, talk to you guys about how the actual, uh, ability to go and deal with physical hardware in their data centers. Customers absolutely hated it. They had to drive multiple hours, fly to different data centers. Unplug deal with data center technicians, all that headache cost time, money, and other resources. Uh, Pinterest here give us a really good quote here around how they, the, the physical challenge of actually going to physical data centers was a real big struggle for them, and that's one of the big reasons why they use EC2 Mac. The next thing was the actual ability, inability to go and have automatic fleet management. Groupon here told us about this definitely increased the time for their time to market, and every time something went wrong they had to get IT involved, and this was a really big struggle that they've solved by moving to Easy 2 Mac. And the last thing was inability to scale. Sometimes your workloads require less number of resources. Sometimes they need more. We see a lot of customers on the weekends maybe not have as many builds or tests, so they scale down and then they scale back up on weekdays. This was one of the big benefits that IQvia saw in EC2 Mac and how they are optimizing using various different resources like auto scaling groups to come into EC2 Mac. All in all this reduces time to market and essentially slows down innovation for your organization. So this is where we brought in EC2 Mac uh back in 2020 here at Reinvent actually. So this was the picture that we always use for EC2 Mac as you can probably tell. This is a 2018 Intel Mac Mini which was our first ever uh Mac hardware that we retrofitted in our AWS data centers. These are connected with different PCIE to Thunderbolt cables, and we essentially, uh, this is an example of one droplet as we call them on AWS, but essentially, essentially it's one server, um, and. So with when you as a customer use EC2 Mac, you have the full access to the underlying hardware offered by these, uh, offered by Apple as well and what they have. So, uh, essentially the PCIE to Thunderbolt connections allow us to do, uh, implement with AWS Nitro so that gives you all the networking and EBS bandwidth that you get and enjoy with the rest of, uh, AWS and so. Engineering teams had to do a lot of manual work to make sure that Apple hardware works with uh EC-2 hardware, and we've even gone in so far as some of these older platforms actually even have robotic fingers that actually press and hold the power button to turn the machines on and off. So it's a lot of uh huge credit that I give to my engineering teams to go in and, uh, you know, build the patents and everything that's needed to operate these machines. But this photo is too outdated, so I decided to bring in some new photos, um, and today here we see some of these are our M2 Pro, M2, and the most recently announced M4 and M4 Pro Mac minis as well. Um, and this, this is exactly how it looks in our data center. So on the upper left hand side you can see that the form factor remains about the same. Uh, we still have PCIE to Thunderbolt connections, but, uh, the Mac has changed, as you know, the M4 and M4 Pro Mac minis have gotten. Almost the size of an Apple TV essentially and as many of you know the power button has gone to the bottom, so there was a lot of engineering needed to make sure that the same uh hardware functionality, the, the reboot mechanisms, all that work, uh. Some of these slides you might not be able to see it on the screen, but the M4 and M4 Pro also, which we announced, I think, uh, on September 12th, come with new, uh, internal SSD drives that a lot of our customers are using. I know Miranda and I talk a lot about this as well, but, uh, this is something that we've added, um, it's not really visible on the pictures here, but it definitely gives customers a lot more performance for local caching mechanisms as well. So, uh, I wanna just quickly cover why, you know, customers continue to use EC2 Mac and continue to stay with EC-2 over the course of now what's gonna be about 6 years. So number 11 of the pain points that we talked about earlier, you know, with IQvia, Groupon, and Pinterest was they really wanted to improve their automation story and so without going to, uh, much, uh, much of the details around every single spec that we have, we've now built automation for some. The highest requested features that customers have, this includes the ability to disable and enable SIP, which many developers, uh, need to, uh, uh, test their custom code. And for those that don't know, SIP is System integrity protection. It's one of the most painful things about working with Mac hardware and and Mac OS. And so we've, our engineering teams have done a really cool job to do so. Just a little look behind the scenes of what SIP actually involves. We actually have custom. Software that uh goes in and mimics what a mouse movement and graph and GUI uh interaction looks like when you actually click in on a user um and so all every time we change a particular Mac OS version we have to make sure that that uh microservice that we actually have is able to recognize the newest uh GUI elements. So for example with the latest release of Tahoe that we just announced a few weeks ago. Liquid glass made everything very tough for us, uh, with the, with different pixels, different placement, the way that, uh, you know, the user icon actually shows up on the screen. Our engineering teams had to spend a lot of time to rework everything to make sure that it works and meets our security standards. So SIPP is just one of those example features that we've had to do a lot of effort on to make sure that it works as effectively, uh, as it did on Sonoma or Sequoia before. Uh, we talked a little bit about operational efficiency struggles that customers had before. Um, one of the big things that we introduced a year or two ago was the ability to integrate with ImageBuilder on, on AWS. And so customers today have, uh, both Intel machines that run on, uh, X86, uh, AMIs. Then we also have Apple Silicon AMIS, and they've been able to use that for, um, using various different services like ImageBuilder, but you can also use Packer and whatnot to uh make your custom images. And then a few years ago we introduced replaced with volume which allows you to kind of provision fresh new Mac OS environments very quickly, um. A lot of customers also uh have a very important monitoring and uh logging story and so obviously CloudWatch is natively integrated, but then we also have other uh uh integrations with other monitoring services and lastly is everything is secured through your Amazon VPC um and so with IM controls access it works just like any other EC2 instance that we have. I'm excited to kind of share what we've delivered this year. Um, firstly, we talked a little bit about this, but M4 and M4 Pro are now GA, um, have been GA for about 2.5 months now. Uh, we have a really cool video on our website if you guys wanna go check that out, uh, afterwards. Uh, we also improved our resiliency story by introducing support for dedicated host auto recovery and, uh, reboot based migration, for instances. So if you, if, uh, uh, you know, hardware will fail as our CTO always says in our, uh, keynotes, but essentially any time it does, we've now introduced better resiliency measures so that your instances can reboot onto other hardware, um, and you don't need to worry about, uh, you know, maintaining that transition of, uh, when an instance or a hardware goes down as well, um, and, uh, quick, uh, going back to number one here. Uh, we talked a little bit about what the, uh, why I want to point out the 2 terabyte instant store volume is historically Apple gives us an internal SSD, but you know, unfortunately due to the way that we do our terminations and reboots, something that, you know, if something goes wrong we won't be able to capture that data and move it to another instance we, uh, with the 2 terabyte instances uh. Sorry, local store volume, you are, we're essentially giving you more space for your caching mechanisms all at no additional charge to what we have, uh, organically with our pricing mechanisms with AC2 Mac, uh, and this has been one of our biggest customer asks. You still can use EBS volumes, but this is essentially free storage for you guys to go and, uh, you know, optimize using your AMIs. Just I think two weeks ago we introduced uh Amazon DCV support for EC2 Mac. DCV, uh, you may know it as Amazon DCV previously. It's a high display resolution protocol that can give you up to 4K resolution when you access Mac instances via GUI. Um, so this is currently free to use and is GA as well. So a lot of customers have been using DCV for better, uh, uh, simulator-based testing or unit tests for any of their applications that need a GUI interface. Um, we delivered, as I said, system integrity protection configurations earlier this year, um, and obviously most recently we did deliver Mac OS 26 as Amazon machine images as well. So here is the current lay of the land around where our platforms are today, um. You can see we we continue to support Intel, uh, oops, uh, we continue to support Intel Max as well, um, as we all know, Apple is is transitioning away from Intel, but we still support those with, uh, Sonoma and Sequoia images, oops, and then along with, uh, the latest M4 and M4 Pro which now come with 2 terabyte discs, and, uh, highly recommend you guys to tune into Matt Garman's keynote tomorrow for some additional future announcements as well coming on EC2 Mac. So super proud and also uh happy to share that these are all just a handful of customers that are using EC2 Mac today um as you can see they're customers across different platforms across different industries verticals um and we have a good selection of partners as well that uh we can help you with to make sure that you get the right and best usage out of EC2 Mac. So at a high level, these are some of the benefits that customers have shared with us and this is also available on our public pages, you know. I think the one that really speaks to me is the reduction in build time as well as the reduction in build failures. Um, this is one of the biggest reasons why customers continue to come to EC2 Mac is because they view the, uh, the entire AWS ecosystem with the networking and the resiliency measures to be one of the best and compared to a lot of our customers have had their own data centers go down and so they come, they are like, I never wanna face that challenge again, and they come to AWS and they have been very happy with the way that the resiliency, uh, aspect of it, especially. Um, I think all in all we continue to invest in this platform very heavily, uh, looking forward, we're continuing to always look forward and to support the latest Mac hardware, increase our regional availability, increase our global, just in general global availability of Mac hardware across different types, um, and as always. Ways and as a product manager we always work backwards from the customer. So if any of you have feedback and or interest in trying out EC2 Mac, uh, there's no room time for Q&A, uh, on stage, but I'm happy to chat with anyone afterwards as well, uh, uh, to discuss your interest and how we can get you started with EC2 Mac as well. With that being said, I'm super excited to pass it off to Miranda who's gonna talk to you a little bit about Riot's game with EC Tumac. Hi, uh, I'm Miranda Pearson. I'm an IT systems engineer on the IT infrastructure team at Riot, and we support, uh, the Build Farm by providing infrastructure as a service to those teams so they don't have to think about Build infra and they can just focus on what they do best. Building games. Um, this journey we're gonna start from our starting point. We're gonna work through the journey we took to get there, how we rebuilt the choices we made, what we got out of that, and what's next. At Riot, um, just to give context, we use Apple development workflows for 3 different things. We use them for signing workflows, game builds, and audio asset processing. Um, those 3 workflows have different shapes, different requirements, different needs, and that ultimately shapes how we meet those needs with ECG Mac. Um, our story starts in Vegas actually at Switch Data Center. We had a pool of about 60 hardware, uh, Macs on-prem. We were losing about 2% a year and we were plagued with issues. So let's dig into that. Um, we hit the limits of what we were able to patch out performance wise when we started coming up against the constraints of the data center design and the custom mounts that we had to use in that space. Um That was actually a picture of our on-prem fleet, the cheese graters in the rack on the previous slide. And this next one is also the trash can from our original on-prem fleet. um, we had drives literally melting, not an exaggeration at all, literally melting, um, VMs kept running if storage was gone and we didn't know until someone else told us they were gone and the builds weren't working. Um, doing one set of repair took away about 8% of the fleet every time we had to repair. Next we'll get into the forcing functions and the pain that Pushed us into a point where we had to do something about it. Um, as we talked about earlier, the heat, the heat, heating, cooling, IO storage, um, compute limits were being hit, and that was trickling down into developers which impacted build times, which impacted build velocity, which impacted the way we were able to deliver games and changes to players. The biggest pain for our team was the on-prem physically being here to maintain our stack, um, to do a firmware update, to do a patch, to do a quick fix. We had to get on a plane and actually come to Vegas to do that, um. And all of that just became unsustainable. The first trigger that started our journey was that Riot made the call to get out of data centers as a business. We were no longer going to be maintaining on-prem hardware, so we had to start the journey of moving out. The second was the forcing function of a security incident that required us to rebuild completely from the ground up and we couldn't reuse any of our original infrastructure and we had to rebuild in a safe space, so AWS was a great landing spot for us for that. Another issue is future incompatibility. We were on VMware at the time and ESXI 8 was dropping support for Mac, so we also had no choice. Pretty much speaks for itself, but we hit the breaking point where. It was not going to scale for us to get on a plane and physically come in person to maintain a fleet that wasn't going to scale with the workflows and workloads of developers that are building games and experiences for players. Next we'll get into how we rebuilt. Um, EC2 Mac was our savior. Uh, AWS had our back with the capacity that we couldn't get easily on prem. We were able to stretch and expand that as needed, and we no longer had to get on a plane to come here and physically maintain our infra. So the turnaround time for life cycle was a lot more smooth that we were able to automate things. Terraform became our lever to get out of our trash can trauma cycle and turn our physical fragility into something that we could maintain as code. For our customers, um, the journey starts when a customer submits a PR to a GitHub repo that triggers Jenkins to run a build to do a terraform plan. To provision those resources. Once that plan goes into place, it starts to pull in an AMI, which is maintained by our product security team, and then shared in a centralized account and shared out with customer accounts. So once that terraform hits the customer account, it'll use that AMI which has baked in. Our baked in certificates, security features, and basic tooling that product security mandates for our production workloads. Um, in that account, once it's launched, or sorry, it uses, um, Jenkins uses STS to assume a centralized role that we share and maintain. Um, so that teams don't have to do that locally in their accounts and there's a codified representation of that access, um, with that temporary credential they're able to go into the account with that secure base AMI, launch the instance in the account, and then from there we have an event driven automation which will pick up, oh hey, there's a new instance I should bootstrap it or do whatever it needs to do in response to that new instance spinning up. If, if there are any failures there, that will go down to cloudwatch and we'll get notified. The bootstrap is done via lambda and we're able to do deterministic. And dynamic bootstrapping with SSM. The SSM documents are maintained and shared from a centralized account as well, which is a tooling account. And those are shared down from our customer accounts so that we can maintain a single source of truth for those bootstrap documents, patching documents. So what changed after we made the lift and shift? Our first shift from on-prem Intel to EC2 Mac Intel 3X are. Main our primary build time signing increased by 2X and uh code build 2.5 uh. Just the mere lift and shift we were able to. Get rid of a lot of the noise that we were seeing with on-prem hardware failures and really see the signal. Our first shift after migration was to move to Silicon, um, we chose and. We elected to do a hardware and software upgrade at the same time which was able to amplify our gains and get us faster, cheaper, better scalable infrastructure for our build teams. Uh, for us, fleet diversity isn't just an exercise for fun. We have a lot of different shapes of workloads, and these are how we map the shapes of those workloads to the different EC2 Mac instance types and when we switch. And the greatest thing about EC-2 MacC that we get now is that teams don't have to lock into a specific instance type. They're able to evolve as their workloads, as their workflows do, as pipelines do to meet the needs of what those builds are. So something like TFT for instance, um, or UnREAL might rely on M4 M4 Pro, um. And we're continuing to see how to evolve our fleet for that. Where we're going and what's next. Um, As we continue to evolve and operationalize, optimize our fleet, um, our next steps are to explore shared capacity, um, to see how we can make have that capacity feel more elastic for teams rather than being tied into a specific node. Um, switching more of our, our. Armed workloads to M4 M4 Pro to get more. To get better performance, to get. Increased build throughput Um, asset caching, because we have huge, one of the biggest issues we face is that we have huge perforce depots of about 2 terabytes for a build, and those can get really size heavy, so any of anything that we can do to cache those assets ahead ahead of time makes it easier for build engineers and developers. Um, finally, we want to explore unlocking better, uh. Better storage bottlenecks by using local MBME across different workloads. Now that I've shared our journey about what we've done with ECHA, I'll hand it to Tony. To talk about how Superell did the same thing a slightly different way. Excellent, thank you, Miranda. Yeah, let's, let's go into looking into a little bit different. There you go, Marinda, yeah. So let's take a look at what Supercell did in terms of the journey to using Easy 2 Max. Uh, Supercell is a Finnish-based gaming company well known for prostars, Class Royale, and, and Class of Clans, of course, and, and we kind of had the whole similar type of thing. We didn't escape from a, from a Nevada desert. We escaped from our own own basement, in fact, where we had a lot of, a lot of Macs running the same way, those cheese graters and all. The other stuff, uh, my name is Tony Subanen. I'm, I'm a senior cloud governance engineer working for Supercell, and, and I first off, I want to acknowledge on this presentation that my colleague Jarno Morimai, who actually led the creation of this solution and also created this presentation as well, and my part was in this was actually work on the architecture, work on, on working on troubleshooting the problems from from the setup itself. But as a background we had a lot of those Intel-based Macs downstairs as well, and a lot of these teams, what we call cells, hence the name Supercell, are really, really independent, so each of those teams had their own. Hardware that our central IT team and the build team was maintaining for these teams as well and because those cells are super independent as well, they of course had quite a different looking built infrastructure as well, which was part of the challenge here. But let's go through the story. So we have a similar story. We had a lot of zombies in the basement. They were growing in the dark, moist environment. I don't know how they got there. We're going to look at how did we go through getting them quarantined, basically, what was the solution. We'll look a little bit deeper into the architecture just like Miranda did. We do have a little bit different kind of architecture though, so, so bear with me. We'll look at the results and then learnings and what are we actually going to build next. But let's start with the infestation. The on-premises was all IntelMax back in 2023. They were. It going fine in there in the darkness, but there was a lot of problems with the hardware itself. We didn't have to fly somewhere. We had to just go downstairs, but still it was taking quite a lot of time for our IT engineers, reinstalling the Macs, trying to troubleshoot them, troubleshoot the hardware, and so on and so on. Also, like with Riot, the VMware was basically saying that they're not going to be supporting the DSXI on Mac hardware, especially on the Mac, the Apple Silicon. They're not going to support it at all, so we have to find some kind of solution. How do we go on running our built infrastructure as well. And in the worst days it could be that 1/3 of the whole Mac fleet, we had multiple, multiple racks of this hardware was actually not operable at all, so our game teams, those sales were suffering because the build times were getting longer and longer. They had a lot of builds in the queue, so it was causing quite a lot of problems as well. And at the same time, while we had 13 of the fleet broken. They wanted more. They, they, we had new game teams popping up. They wanted to be able to develop, sign, build their games, and, and also the game teams themselves, the live games, something like Prostars and Clash Royale. They were growing, they wanted to do more builds, so we had a whole conflict, this, this friction of broken hardware and at the same time high demand for new things. So we had either option. One option was that we started looking, do we start building a data center somewhere to actually get out from the basement. But then we remembered back in 2023 that a wise man had told us, friends don't let friends build data centers. This is a quote from 2013. It's still valid in 2023 and it's still valid today as well. So instead we did go with looking at the EC 2 Macs, which already had launched at that point, I said, multiple years earlier, but we in Supercell hadn't looked at them until the Mac instances actually became available on Easy 2 Mac. Or the Apple silicon instances. So our solution is based on these four pillars, basically the weapons that we use to start slaying the zombies in the basement. First off, of course, I said, the easy to max, getting, especially the Apple silicon ones running right out of the gate. So that would actually improve the performance like we saw from Miranda's numbers and also we don't have to go to the basement anymore. We can actually look into the sky and the clouds and just maintain it that way. Unlike the riot games, we actually ended up using virtualization on top of the EC 2 Max. I'll go into details why, but we use er lapstart virtualization, which is based on the. Official virtualization framework. So it's actually using Apple's own MacO OS native virtualization to actually spawn officially supported virtual Mac OS on top of the Mac hardware as well. The reason for that is that the Easy 2 Max still have that if you switch the AMI, it has this scrubbing dime that takes quite a while for that to go around. There is, of course, now the quick replacement mechanism as well, but back when we started developing this solution, that was not available yet, so we went with the virtualization, and the benefit of that is that we can actually really quickly switch between different. TART VM images which more or less behave if you're familiar with containers, it behaves like containers as well, so you can quickly start the machine as well. On top of that, of course, we use Terraform to manage this whole infrastructure, and we use the packer to create those TART VM images as well. Um, and Last part, the fourth pillar is, is, I'm not going to pronounce this correct, it's I guess French, miss in place. So it's a nice tool that you can use to define what kind of tools you want to have installed in the in the operating system itself. So you can list. I want this version of X code. I want this version of whatever other tools that you want to use like AWS SDK. You can list those and then just run the MSA in place to customize the image on the fly. And this happens on every build actually. So when the build starts running, the missile in place customizes quickly the machine on the fly just before the build itself self starts as well. Yeah, and lastly what what I want to maybe mention also why the Amazon Easy to Mac was natural for for Supercell is that Supercell actually born in the cloud. So when we were founded in 20 2010, already everything was built on AWS as well, so we had years and years of AWS experience and especially really good experience on using Easy-2 and all the other surrounding services as well. But then let's look at how did we actually put this. These tools, these pillars in the work, well, everyone's familiar, we had the AWS Region, we had the VPC and multiple ACs as well, and on top of that we have the auto scaling group. What is kind of special with the EC2 Max concerning auto scaling groups is that if you only put an auto scaling group in place, it won't work. And the reason for that is that E2 Macs are actually dedicated hosts, so you actually have to go in and provision a dedicated host, and then on top of that you can go and provision an easy to instance. So there's like an extra life cycle step here, but luckily AWS has another service that can help you manage the relationship with the outer scaling group instances and the physical on demand machines as well, and that is the. AW's license manager, it's maybe not directly clear to you why it does that, but the license manager has a feature called host resource group, so it can actually automatically provision you on-demand machines based on the auto scaling group saying that I need a new hardware to run on, and then this license manager goes in and actually provisions the machine for you. What is really neat about it as well is that if you don't have any instance running on that on-demand machine for a while, it will try to get rid of it as well for you. And with easy to max, that means that after the 24 hours it's gone, which is the Apple demanded minimum time to have the on-demand machine allocated to you, the license manager will make sure that that gets thrown away. And basically the cost for that Mac instance will stop at that, not instance, but the dedicated host will stop at that point. And the magical clue there, how do you attach the auto scaling group to the license manager is through the easy to launch template. In there you're basically saying I want to use this host resource group to be the source of the hardware for this auto scaling group. And then later on, because we ended up licensing also the TART virtualization engine which is open source, but if you want to have official support, you can license it as well. The license manager is a nice way to make sure that we never go above the number of licenses that we have purchased as well, so it actually came pretty handy for us as well. But then let's go forward. Now we can get those physical machines, those easy to max. So it will launch an easy to instance and we just use those out of the box default AMIs that AWS provides us. We don't actually do any customization of them in terms of AMI. We do use the user data though, and what the user data does is when we launch, for example, now the latest AMI from from AWS, the user data will actually make sure that the Siru Lab TART gets installed and then it quickly kicks in the image that we want to kick in on top of that easy to instance as well. So in the user data we just quickly launch a virtual machine. And in that virtual machine, what comes out of the image, the TART VM image that we custom built with the terraform packer, it has several things. It has, first of all, it has the GitHub actions, and it has the GitHub Actions runner, and it has the Jenkins agent. That's because we are now in the transition from Jenkins to GitHub actions more and more per team, per those cells, so each image has support for both of them. To make the decision which one is being used, we actually coded our own little agent called the Coco agent, which is our own little scripting code as well, that it looks at the metadata and then knows, oh, I belong to this team. This team uses Jenkins. The Jenkins controller is in this IP address. I'm going to connect this there, or is it GitHub. Actions runner, it knows, oh, this team is actually using the runner. Let's connect to the GitHub.com to actually get the jobs and the machine into the queue. Also, into the image we have built, of course Xcode, we have all the other command line tools there as well, and any other like default tooling you need to do the signing and so on. And of course we have the miss in place in there that the teams themselves can customize whatever they want at the runtime point as well. So that's what's in here and how does this all get built. Um, The Supercell Developer Services, which is the team taking care of the running of this environment, they use GitHub actions themselves with the Packer to build the TART images. Those VM images are then saved on EFS, the elastic file storage, which is mounted to the easy 2 Mac instance itself on on the base layer Mac OS, and that's how the TART can quickly switch between any kind of X code versions, Mac OS versions, etc. etc. We have all of those different variations, different combinations built on the EFS storage, allowing us to quickly take them into use. Some scripts are stored in S3 as well, and what's kind of unique for us as well is that what you don't see in this picture at all, and it's actually missing, is we don't have any EBS volume other than the AMI. Locked into this because we use the local NVME storage as the disc for the TART VMs as well. We could use EBS for it, but we actually found that the easy to max, the NVME is really, really high performing, and as Visha said, it comes for free. So why not use it as well. So we're taking the full advantage of that local NVME to make the builds really, really snappy. We also use the parameter store to store any secrets like how do you sign the applications because all of our games are mobile games, so all of the time we need to sign them officially as well. And also none of the people have direct access into the EC 2 Mac or even the TART VM. I said, we are running the TART VM on top of a Mac OS and we do need to sometimes go with the VNC inside the TART VM. And how we achieve that is that we actually use the system manager to tunnel ourselves in into the Mac OS itself so nothing is exposed to the outside world. You have to have AWS access and you have to have the permissions given to you through the SSM and IAM to be able to even connect to the machine itself. So this is how we run overall the whole infrastructure and we do use the cloud watch locks and the cloud to metrics to get basic stuff out, but on top of that, our guys in the developer services are also using telegraph and open telemetry to get really detailed tracing of how long does each stage of the build take and what kind of things happen there as well, and that is something we have developed in-house. So we are using both CodeWatch and we're using Grafana Prometheus to kind of have our own metrics as well, and this is how we run the majority of the build farm now for building our games for iPhone, for iPad, and so on. It's run by this auto scaling group. There are some standalone machines as well for for really, really corner cases where they actually need to get the hardware. Directly, for example, if they need the GPU, we cannot use the TART VM because GPUs are not exposed directly into the virtual machine itself. So we have some cases of even standalone machines as well. So after we got the solution running, how's our life after the zombie apocalypse was dealt with? We did manage to escape basement. I think we only have a few of those Intel Max still running in there for some really old build jobs because we still have some some. Some developers who still believe in the Intel MacBook Pros as well, so we kind of have to still have a couple of Intel-based builders just for those guys until we can convince them to move towards the Apple Silicon as well. We do use the auto scaling groups, so we have now an automation that every weekend we shed the load, we scale down to minimum number of machines as well. And then on Monday, or actually nowadays also on the Sunday side, we start scaling up the fleet of builders so we can get some cost optimization in the play as well. And at least our IT is really aesthetic because they don't have to go to the basement anymore at all and go and give some daylight into the Mac and just switch the cable around and now it works type of thing so they we actually reduced the operational overhead significantly. There used to be like multiple guys from IT just going downstairs to do the physical work on fixing the Macs. Now we just have basically one person. In taking care of the whole thousands of machines on AWS, which is a really, really good improvement as well. And in terms of numbers, I don't have as detailed numbers as Miranda had, but even for us we now have 10x more build shops than before taking the solution in use, but we only had 33 times the number of Macs needed as well. So that is a really, really good, good result, at least from our point of view and also considering that one person is maintaining this. So nowadays we used to have like 150 build shops per day on Macs in the basement in the zombie land, and nowadays in the cloud we have over 1800 builds happening daily. So as you can see there's quite a lot of growth that we were able to achieve with this solution. And also now we can a lot quicker take, which is not on the list here, but we can take new easy to, not easy to, but we can take new versions of the Mac OS into use quite quickly as well because everything is virtualized so we can even without updating the underlying the Mac host itself, we can in the TART VM already switch to the latest and greatest version of Mac OS even before the official AMI is released as well. So what did we learn in this journey in the last 2 to 3 years? In the beginning back in 2023, a lot of this stuff was completely undocumented. There was no prior experience on on how to use the, well, there was some experience on the EC 2 Max, but because we wanted to do the virtualization, that was more or less no documentation. So there was a lot of trial and error of figuring out how the TART VMs worked in production as well. Also, I said, the Mac OS and Apple can always give us a really, really a lot of curveballs in terms of the Mac OS updates as well, so that has crippled when we tried the new version that has crippled like, oh, there's a new security feature that we have to glow and click on every machine to enable it as well. And and it has helped us that we have worked closely with Visual and and our AWS account team to plan out our capacity as well. Uh, we are mostly European based uh with some studios in in also in Shanghai and also in North America. It has helped us immensely to communicate clearly that, oh, we're gonna need this many Mac machines as well, so that has helped us to go, go forward, um, but overall it, it, we can say that. It's not a cheap solution. Overall, the EC 2 Macs are not the cheapest types available, first off, being dedicated hosts, of course, but neither is the manual work we were doing downstairs. I said, we went for, let's say 5 to 7 people maintaining this environment in the basement to just 1.5 people basically maintaining this bigger environment into the cloud. So for us overall it was still a win-win situation. And, and it had a huge impact, it unlocked a lot of things in the game teams themselves to be able to develop faster, try new things faster as well. Yeah And then let's look at what is the future bringing us beyond the zombie apocalypse. So what are we building next? We're really excited for the new M4s and the 2 terabyte local NVMMs because it allows us to make the images bigger. The old machines had 256 gigabytes of disks, and if you look at any kind of modern game building. Luckily we're a mobile game company, we can still fit into that, but I would imagine like a lot of like riot games, they're not going to fit into 256 gigs to build anything at all, yeah, she, she's shaking her head, yeah. So we're kind of lucky with the mobile games, they're reasonably smaller than your average PC game or or console game as well. But we're still looking forward for the 2 terabytes. Because that would unlock us for the first time to run 2 VMs per physical EC2 Mac. So we're basically going to be able to double our capacity without any extra cost as well using the VM approach here. What we don't have right now as well is we don't have fully dynamic out of scaling right now. It's basically on calendar, so it knows that every Monday it must scale up to this number and every Friday it has to scale down, but in the future we will have through the cocoa agent that was shown there we will have dynamic following of the size of the queues, so we'll be able to automatically with our own little coordinator tool to. Deploy new machines and retire them quickly if the queue size of the builds is at 0, for example. And in general this has also now unlocked the people to do more work on the actual improvement of the pipelines themselves instead of trying to do the troubleshooting day to day on the fleet as well. So there will be better pipelines coming and we're going to now be able to extend with all the automation in place, the terraforms, the packers, and all this stuff we can now easily expand it to new regions as well. For example, we are now deploying it in the US as well because we have a couple of game teams now based in the US in Seattle and Los Angeles. We can now quickly with the already built terraforms just bring up the whole environment for them in the US as well that they can have quick and snappy builds in the US. Yeah, So, so, lastly, on the zombie team, it's like they're they're coming for you, Barbara, so if you're not careful, uh, the, the zombie apocalypse can also happen to you if you're still stuck on your, your EC2 Max, not easy 2 Max, on your Intel Max somewhere in the basement or closet, hiding in the dark, dark corners of the world. Excellent, I'm gonna actually hand it back to Visal and he's gonna tell you more about what else else coming in terms of the EC2 Mac presentations. But other than that, you survived the zombie apocalypse with me and thank you. Thank you. Awesome. So, uh, before we Uh, before we wrap up here, guys, thank you so much for spending time with us here today. Thank you to Tony and Miranda for working with me through the multiple months that we've had to kind of put this together. Before I let you guys go, I did wanna tell you what else to expect at Reinvent for EC2 Mac and just Mac OS in general. So first thing is, uh, as I've hinted. Please make sure to not miss our AWS CEO's keynote tomorrow. Uh, Matt Garman will be speaking, I believe, at around 8 a.m. up until 10:30. So please tune in for some exciting news on EC Tumac uh during his presentations. Uh, secondly, we do have a few sessions that I wanted to call out and make sure that they're on your radar. So the first one is going to be actually on Wednesday the 3rd, um, the ID is CMP 306, um, and that one is actually gonna be a hands-on workshop where you guys can actually play around with a lot of the features that we talked about today such as, uh, system integrity protection, replace root volume, um, and so super excited to actually get your laptops out and actually play around with that, and two of my colleagues will be presenting that as well. Uh, on the same day we have another session that's called CMP 346, and this one is actually really cool because it's the first time we're gonna be talking about how we can do ML and inference using EC2 Mac and the, the amazing GPU power that comes with Apple silicon, and, um, some of my colleagues are gonna be presenting on that as well. It's the first time that we're gonna, uh, be talking about running actual, uh, ML and inference workloads using Apple silicon chips, so make sure not to miss that. Then if you wanna see me again, I'll be talking at CMP 344, uh, which is again on Wednesday, um, and that is actually me and my colleague will actually go through and actually do a live demo on using EC2 Mac and GitHub actions, um, or Gitlab Circle CI, whatever, uh, automation tool that you guys use, and he'll actually be going through and, uh, running an actual build change, uh, using a live demo x code, um, to see. How it actually runs on EC2 Mac and you'll be able to see the live change uh on this uh uh test flight app that that will be screen sharing as well. So, uh, 3 other sessions that I highly encourage you guys to join as well as, uh, Matt Gartman's keynote tomorrow and any of the other keynotes as well. So with that being said, I think all, a few of us will hang around if you have any questions, but thank you so much for coming and, uh, please make sure to complete the session survey at the in the app as well. Thank you.