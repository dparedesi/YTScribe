---
video_id: ojzl56Uq9yA
video_url: https://www.youtube.com/watch?v=ojzl56Uq9yA
is_generated: False
is_translatable: True
---

Good morning, good morning everyone, and welcome to session STG 338, data protection and resilience with AWS storage. Just by way of a quick introduction, my name's Danny Johnston. And for the last 5 years, I've been helping some of our largest customers within global financial services build out their cyber event recovery platforms and solutions at AWS so really excited about today's discussion. Absolutely delighted to be joined today by Brian and by Steve. Guys. Hey, I'm Brian Benscoder. I'm a principal solutions architect in the healthcare and life sciences space, but I also have a passion for cloud operations and data protection. Steve, hello everyone. I'm Steve Deveau. After 28 years of building on-premise backup solutions, I joined AWS 88 years ago, uh, to basically build the AWS backup service. I'm a principal engineer and I look forward to discussing data protection with you all today. Fantastic. Thank you guys. So we've got a packed agenda for you this morning. We're going to be kicking off talking about proactive defense and why it's so important to have proactive defense to meet today's threat landscape. We're going to talk about what we mean by the term data resilience. We'll look at some methodologies and strategies to bolster resilience. We'll look at AWS's approach to cyber event recovery and then wrap the session up into some firm next steps and actions. So kicking off then, proactive defense and. I don't need to remind any of you just why it's so important we protect our most mission critical data assets from external and internal threats. Now there's a huge amount of material out there pointing to this, but one report I refer back to time and time again is the state of ransomware by SOPOS. And in their report titled The State of Ransomware Cross Financial Services in 2024, SOO estimates that 59% of organizations experienced some form of cyberattack. Nearly half of those attacks were successful. But what really concerns me is that even when an organization paid its ransom, they were only able to retrieve about 70 to 80% of their data. Not only do our security agencies like the FBI, GCHQ, and the NCC recommend not paying a ransom, even if you do pay a ransom, that's not going to necessarily guarantee retrieval of all your data assets. And then you start building on top of that, the cost of recovery from each incident now sits at about the 1.5 to $1.6 million dollar mark on average, but look, we all know that cost of recovery excludes the reputational damage inflicted by these attacks. And once you start building in that reputational damage, the cost can be absolutely horrendous. In fact, Cybersecurity Ventures estimates that by the end of this year, the cost of cybercrime across all worldwide industry is going to be in the region of $10.5 trillion. Now that's even more profitable, they claim, than the entire illegal drug trade. In fact, they claim that if cybercrime was a country, it would be the 3rd largest country in the world using GDP as a metric, so there really are some quite startling and eye-watering claims and stats out there. But we also need to be very mindful and consider internal threats to data. You know, think about disgruntled employees, other internal malicious actors, or even well-intentioned employees with access to critical material can really jeopardize an organisation's stability through human error, such as accidental file deletion, or as we saw in the case of UniSuper in Australia about 18 months ago. Accidental account deletion. And that human error put at risk $145 billion worth of pension funds, which in turn put at risk the future savings and futures of 640,000 senior citizens in Australia. But what's it like to actually experience a, what some of the UK regulators call a digital extinction level event? What's the actual business impact on the ground and what steps can you take to recover your business in the event of a catastrophic breach? We work with an industry leader in this space, the former CIO of AP Molon Maersk, who experienced just such an attack in June 2017. The World Economic Forum classed this attack as the most costly and catastrophic cyberattack to date. So much so that they've now published an external case study so that other organizations can learn from Maersk's unfortunate experience, and I'll come on to talk about that experience shortly, but before I do, just a bit of background on Maersk. They're a huge organization. Maersk is responsible for 20% of all global trade. The ship we see in front of us is one of their 750 ships in their fleet, and each of these ships is capable of carrying a staggering 19,000 containers. In fact, the newer generation too of these carriers can carry up to 24,000 containers. They own and operate 74 ports and terminals worldwide, and every single port can expect to receive about 5 of these ships on a daily basis, so that's around 100,000 containers per port per day. I mean, it's a colossal operation. Now, 5 years before Russia invaded Ukraine, they were attacking Ukraine with cyber weapons, the largest of which was detonated on the 28th of June 2017. Russia targeted the Inland Revenue Platform of Ukraine, a system and platform called ME Docs in Kyiv. And any organization that happened to be on the platform at the time of the attack was impacted, about 3380 organizations in total. But what sets Maersk apart from those other organizations is just how transparent they were in terms of their experience, so other organizations could learn from it, and I'll try and paint a picture of what happened that day. Within the 1st 9 minutes of the attack happening, most of the damage had already been done. The network was taken down. Their online backups were completely destroyed and taken out. 49,000 servers were rendered completely useless, 3500 servers were destroyed. A further 1200 applications were destroyed and they couldn't access a further 1000 applications because by doing so would lead to reinfection. In terms of communication, all they had in the early hours and days of the attack was using WhatsApp because their BlackBerries had been completely wiped. In fact, they couldn't even enter their offices because their office passes were linked to the IT system. I mean, a pretty disastrous and brutal situation to find yourself in, particularly if you're responsible for 20% of all global trade. So what happened next was very much through luck rather than design. It just so happened that at the time of the attack in Ukraine, due to a major storm in Nigeria, there was a power outage. And a system, an X86 box, due to the power outage was obviously offline at the time of the attack, but even more fortunate than that, this X86 box had a copy of Active Directory, that key infrastructure service that supported so much of Maersk's operations. So as soon as the recovery team identified this system, they put it on the first available flight in a first-class cabin, I might add, from Lagos to Heathrow. They chauffeur drove at lightning speed that X86 box up the M4 motorway to their UK headquarters in Maidenhead and gradually started refiring up their network using Active Directory. Now it took days and days to get to any semblance of normal operational capacity and weeks, weeks and weeks to recover fully. But they did eventually recover their business. So why is their experience, why is this story so important for this morning's discussion? Well, I think it really focuses us on a few core principles of data resilience. First of all, it teaches us the importance of having offline backups, completely segregated from your production environment. Hopefully through design rather than luck in this case. I think it also teaches us the importance of really knowing your IBS is, your important business services. Again, in this case it was that key infrastructure service, Active Directory that so many of us use. And I think finally, it really underlines the fact that even if you're not the target of an attack, you can still be the victim of an attack. So it really is the case of not if, but when you will fall foul of a cyberattack. And given the systemic risk, ransomware in particular poses industry, we've seen a lot of guidance, prescriptive guidance coming out from governments and regulators worldwide. 2 years ago now, we saw the Department of Home Affairs in Australia issuing a really strong action plan against ransomware and the well publicized um Medibank and Optus data breaches. In Hong Kong, if you want to operate in that region, the Association of Banks and the Monetary Authorities stipulate you must have what they call an, a secure tertiary data backup. At the beginning of the year, a lot of eyes were on Europe and Dora, the Digital Operation Resilience Act, that came into force in Europe on the 17th of January. A lot of those articles and provisions pertaining to network segregation and backup. If you're interested in learning more, take a look at Article 12. And we worked really closely with the Bank of England and a body called Seymour, the Cross Market Operational Resilience Group, in terms of what it means to design, deploy, and implement what they call CHDVs or cloud hosted data vaults. They see CHDVs as a key mechanism to level up resilience across the entire financial services sector. Now the guys are going to go into a lot more technical detail and depth around what a data vault and a CHDV is. But in very simple terms, a CHDV is an independent recovery environment built in AWS where you maintain a golden copy of your most mission critical data assets in an air gapped immutable vault, so you can recover your business in the event of a catastrophic breach. Exactly this time last year, the NYDFS in Wall Street in New York um stipulated that if you want to continue doing business in New York, you must have an external backup that is, and I quote, free from alteration and destruction. So I think we see a lot of guidance around data protection. I think it's fair to say that what we see less of are actual standards, and I mean specific surgical standards organizations should be meeting. And where there's perhaps pockets of standards, we see very little in the way of a pathway to achieve those standards and certainly pretty much nothing in the way of official certifications of standards. And that's why sheltered harbor was established. Sheltered harbor set the gold standard for operational resilience across the financial services sector, and I'm absolutely delighted to say that AWS is the first and only cloud service provider to be an alliance partner with sheltered harbor and to have a sheltered harbor certified platform in the cloud. And when I talk to my customers about that, that really gives them confidence that we're, you know, got some fantastic solutions and credibility in helping them overcome the challenges that ransomware poses. So what else do my customers say, what's important to them when it comes to data protection? Well, they tell me speeds of the essence. It's really important to minimize that threat window when it comes to cyberattacks, and just given the inherent flexibility you get with cloud solutions, you can achieve so much more, so much more quickly in the cloud than you can on premise. They also tell me that having an agile and modular data protection platform that can bend and flex to meet the ever changing threat landscape out there is crucially important. Now Marketplace is a key component of supporting that modular platform. And finally, they tell me they want the ability to start small but scale fast, to take advantage of the utility model that AWS provides them, and to completely avoid those heavy investments, upfront Capex investments on expensive and complicated on-premise infrastructure that actually may or may not be fit for purpose in the future. And having that ability to optimize their data platform, which you can achieve in AWS, is super, super important to them. So it's around speed, it's around agility, and it's around scale, we really hone our messaging and go to market when it comes to cyber event recovery. But I think it's important as well to sometimes take a step back and kind of talk about what we mean by the term data resilience, and with that, really looking forward to Brian now sharing some key insights with us. Thank you, Steve. So I think it's important to start with some fundamentals. What exactly is resiliency? So this is your workload's ability to withstand partial or intermittent component failures across multiple services and eventually recover efficiently. But resilience isn't monolithic. It consists of multiple facets and components, and we need to understand those when we're architecting our own, um, resilience strategies. So that first pillar is high availability. So think of it this way when you have network issues or component failures, high availability is your system's ability to fail over, keeping your your applications running. Um, as an example, so imagine you're running a very popular e-commerce website and it's Black Friday. One of your web servers goes down because of a hardware failure, but your load balancer detects this and immediately starts directing traffic to the remaining available healthy servers. So that's high availability in action. Automatic failover so you can maintain service continuity when individual components fail. But um in that example, right, your customers continue shopping, everybody gets what they're looking for and there's no detection in the background. So even though that failure occurred, we're up continuing our, our, our website. But failover is not necessarily the whole or the end game of everything, right? On the flip side, we have data protection, and this is when your application is completely down and you have to be able to full recover that site. So while high availability helps prevent the outages, it's data protection that recovers from more catastrophic failures, and you can think of this in 3 layers. We have first, backup and recovery. So these are regular snapshots that let you restore to a point in time before that data corruption event occurred. So things like restoring from an accidental deletion using the previous night's backup fall into this category. Now disaster recovery is your ability to restore operations to an alternative site in the event that your primary location is unavailable. And closing it out, we have business continuity and that encompasses the people, the processes, the communications that are needed to to run through the requirements of these events. So the key difference here is that high availability operates in seconds to keep your application running while data protection typically operates more in hours, but that's going to get you back to running and in your resilience planning strategies you're typically going to need both of these. Now failure comes in many forms, and understanding their likelihood versus the impact of a failure is gonna drive your resilience strategy. So the most common types of failure, anyone can guess. It's human error, right? So we have somebody fat fingering, adding an extra 0, misconfiguring a parameter in the application deployment, uh, moving a bit further along, you have the load induced type errors. So if we're thinking again about that e-commerce website, you know, perhaps there's heavier traffic than expected that's overloading our systems and things go down. Now these types of low likely, I mean high likelihood but low impact events call for high availability type solutions. So we have um automatic failover, load balancing, redundancies in place to help handle these events. But as we move across the continuum, we need to shift our approach. So here we have on the extreme end we have natural disasters, we have regional disruptions, and we have cyber events like Danny kicked us off with today. These are lower likelihood but much higher impact types of scenarios. And for these we need data protection type solutions. So here's where our disaster recovery, our comprehensive backup solutions, our business continuity planning all come into play. So it's high availability that manages the frequent but manageable disruptions. Data protection is there for those rare catastrophic ones. Your investment in each one of these is going to ultimately depend on the business criticality of your applications. So if you have say financial systems or ticketing systems, you're likely designing for both simultaneously because even a rare disruption can't be tolerated. Ultimately though, you want to make sure that you're aligning your matrix to help you decide where to make these investments. So for those common but manageable errors, invest heavily in HA, but don't neglect data protection for those rare catastrophic events that could ultimately end your business. So we've all probably heard frequently about at AWS we talk a lot about shared responsibility models um in the security context I'm sure everybody has has happened upon that in some other talks, but we also operate in a shared responsibility model that direct for resiliency that directly impacts the strategies that that you're going to implement that we're going to discuss. So what that means is you're responsible for the resiliency in the cloud while AWS is responsible for the resiliency of the cloud. Now what exactly does that mean? So AWS handles the core infrastructure and the resiliency of that. So things like regions, availability zones, edge locations, these are all things to ensure that our compute, our storage, our network, our database services are all highly available and redundant. And that gives you the building blocks that you need for building your own resilience strategies. But here's a critical distinction, right? So if you think of a service like Amazon S3, that we talk about 11 9s of durability, so I write that object to it properly replicated, we're giving you 11 9s of durability. If I corrupt that object in S3, we're going to make that corruption 11 9s durable as well, right? So the important takeaway there is it doesn't remove your responsibility of architecting your applications for that data protection, right? So as we talk about the shared responsibility model again, we're providing enterprise grade infrastructure that allows you to design highly available redundant systems, but at the application level it's still ultimately your responsibility to put in place the resilient patterns that are going to work for your business application. So let's quickly review what we mean by regions and availability zones. These are the infrastructure foundations for all of the resilience strategies that we'll talk about. Regions are geographically distributed locations and they enable things like disaster recovery, and they meet data residency requirements. So within each region we have multiple availability zones, and the availability zones are your isolated failure domains, and they enable different types of high availability designs. The key architectural principle here is that the AZs are separated enough to avoid correlated failures, so things that are in that high impact area that we were talking about on the matrix, but they're also close enough to enable you to have synchronous replication and automatic failure, so you can incorporate incorporate those into your HA designs. This gives you both building blocks. You have the AZs for high availability, and you have the regions for data protection across different geographic boundaries. All of our AWS services are architected to take advantage of these infrastructure boundaries as well, so understanding how they're designed helps you make the right choices when you're designing your own applications. So you can categorize the AWS services into three types based on their um, their infrastructure patterns that they implement. And the first one is zonal services, so EBS, um. EC2, they operate individually within zones or availability zones within that environment, and they fail independently as well. So this gives you some granular control. You can design how you want to distribute your application across multiple zones, but you're ultimately also responsible for defining how you're going to fail over or direct traffic between those zones as well. Now regional services like S3 and Dynamo DB, they abstract away a lot of this complexity for you, so AWS automatically distributes the data across multiple availability zones and handles the failover for you automatically. So if your application is using S3 for example, and there's a disruption, that's invisible to the application, and that's AWS handling the high availability for you. And finally we have global services. These have a distributed architecture where there's a control plane in a single region, but the data plane spans worldwide. So if you look at a service like Route 53, that operates in 200+ points of presence around the globe. So this gives you high availability capabilities as well as data protection or disaster recovery capabilities within a single service. So the important takeaway here is that you want to focus on trying to leverage things like regional services wherever possible so that that that high availability is provided at the service level for you automatically and you can couple that with zonal services but ensure that you're aware of how they operate and how you distribute your data so that you can handle the resiliency of that application based on how the application should operate then. Utilize the global services to add additional HA and more importantly, the disaster recovery capabilities should you need to move between regions. One important caveat is that Even regional services can have global dependencies. So if we look at S3 for example, the S3 naming service is global. So um with S3 you have. The naming service, which has a control plane in US East 1, but the data plane is global. So if US East 1 becomes unavailable, what that would mean is I'm no longer able to create new buckets because I need a globally unique name, but my remaining buckets are going to continue to be highly available and active for you. So consider these types of things when you're designing your application and your disaster recovery planning specifically. So How you prepare for availability in corruption events is directly related to your objectives when such an event arises, but it also ties to your budget. And The questions that you need to answer when these events occur is how soon after the event happens do I need my system to become available again. So that's your recovery time objective or RTO. The other question is how much of my data can I stand to lose after such an event, that's your recovery point objective or RPO. So if you're configuring things like synchronous replication, that's going to provide you with the shortest time to recovery, but it doesn't give you a lot of capability should you need to address data corruption or accidental deletions because I can't necessarily go back to a point in time before then. Um On the other hand, creating continuous replication, or excuse me, continuous or periodic backups that allows me to go back in time to points before a data corruption event occurred, but it's going to take me typically longer than it would in a replication type solution. But I do have more flexibility in terms of where I can recover to. Now traditionally, enterprises maintained multiple data centers, and they replicated their critical data from one data center to another and maintained backups that were stored off site. So today AWS provides a sophisticated framework that's organized around two dimensions. So, one dimension is a zonal versus a regional scope, and the other is our recovery versus our availability approach. And the matrix I'm going to talk through shows you 4 distinct strategies that you can tie to those two dimensions. And the first one is multi-zonal availability, and this means that your applications stay available despite an individual zonal failure. So in these designs you're configuring things like EBS, EFS, FSX, RDS to replicate data between multiple zones so that data is active and available in the event of individual zonal failure. This is a proactive availability approach, so systems are spread across multiple zones so that you can easily handle or fail over from an individual zonal failure. But it doesn't help much in the case of data corruption, right? The next approach is multi-zonal recovery. So in multi-zonal recovery we are using backups. We're taking backups of our file, our block, our object, our database storage systems, and we're copying those to another availability zone. So in the event of our primary zone outage, we're restoring that data into the alternate zone to get those systems back online before the corruption event occurred. Multi-zonal. Multi-zonal availability or multi-regional availability, excuse me, is very similar to multi-zonal, but in this case, right, we're replicating our critical business data from our primary region to a secondary region. Again, this is a proactive approach because now we have multiple systems spread across different geographic boundaries so that we can fail over to the secondary region in the event of a regional disruption and fail back when that region comes back online. Again, we're not in that type of solution protecting against data corruption type scenarios. So rounding out our quadrant is multi-region recovery. In this case we're taking backups in our primary region of our critical data and we're copying those backups to a secondary region again reactive in approach, but if we are to have that rare scenario where we have a data corruption event and there's some challenge in accessing the primary region, we can restore these systems back to that point in time in the secondary region. So the key thing to think about is usually you're not picking just one of these quadrants and implementing your resilience strategies. You're going to use multiple, multiple of these to align against the different types of applications you're running and the business criticality of those applications. So let's take a look at that first quadrant, multi-zonal availability and see how it looks in practice with a typical three-tier application. So here we have a very common type of straightforward multi AZ architecture where we're leveraging a combination of zonal and regional services. We're going to use things like nat gateways and load balancers to direct traffic between the different zones. So as a best practice, you start off by placing your web servers behind a load balancer, right? But we're not doing this just for performance in this case, but also to enable the ability to direct to healthy servers across multiple zones. Additionally, we want to set up EC2 auto scaling groups across multiple zones. So in the case of a single zone failure, the load balancer is going to start directing traffic to the healthy web servers, and the auto scaling groups are going to spin up additional resources in those services so that we can maintain our level of service. Amazon, Aurora and RDS, they can replicate data across multiple zones as well, and then support automatic failover. Additionally, we can add in multiple read replicas if we want redundancy at the rep read level of our databases. And then rounding this out using services like S3 and Dynamo DB within the applications gives us that high availability at a regional level by default. So we've just seen how multi AZ availability can keep you running through application or availability zone failures, but availability alone isn't enough. I, I hope, I hope I'm stressing that significantly for you. Um, you also need a strategy to recover from data corruption events, um, deletions, and those ransomware or cyber events like Danny kicked us off with today. So while on premises we had a classic 321 backup model and that served us well. The cloud enables some more flexible and cost effective strategies on top of that. At AWS we put forth a strategy or a framework called the 32,110 framework. And this is our gold standard for data protection, and what this entails is, you have 3 copies of your data that's separate from the primary resource. You have two copies that are in different locations, so that can be cross account, across region. One copy is there for local recovery, so you have fast restore of operational issues. And one copy that is immutable, that it's isolated and stored in a vault so that you're protecting against these cyber event or ransomware type scenarios. Probably often overlooked, but most importantly, you have some sort of process that's regularly testing your backups to ensure that you're able to recover with zero errors in the event that you actually need to use them. But here's a critical point, not every application needs this level of protection. In fact, it carrying multiple copies can be significant in cost, so you ultimately want to tie your protection plans to the business criticality, the requirements that drive the strategy for your applications. But the framework is flexible, so one copy can solve multiple purposes within within the solution. So your cross region copy could also be immutable for your cyber recovery. The local operational copy can leverage native service capabilities like EBS snapshots or S3 S3 replication. So ultimately you want to match your protection strategy to the business impact. Your financial trading system, you probably want 32,110 because this is very mission critical, but that monthly reporting system, maybe just a 110 is going to be sufficient. Match the RPO and the RTO of the different applications to the strategies you ultimately select. Now 32,110, use this as a North Star for your mission critical, the types of applications that are highly dependent in your business, but ultimately you want to scale the approach based on the business criticality and the tolerance of risk within a given application. Now the data protection strategies that we talked about, they map ultimately to 3 different or distinct types of copies, and each one of those serves different scenarios within your recovery processes. And the first one is a local copy for fast recovery. So this is a copy that's going to be in the same region as the original resources, so that way you have the shortest RTO when you need to recover. This is for those typical kind of operational accidental deletions that occur. And they map more to those higher likelihood but the lower impact type failure scenarios from our matrix matrix earlier. Now the trade off here is that you get fast recovery, but they're not super useful in the case of say like regional disasters or we have ransomware type events. Our remote copies for disaster recovery. Are meant to handle these more geographic type. Disaster type solutions. So here we're typically using some sort of replication type solution with a failover mechanism in place and that's meant to handle those higher likelihood or excuse me, lower likelihood but higher impact type of regional disasters. And finally we have our cyber recovery copies. And if we think back to Danny's talk earlier, Maersk kind of happened upon this accidentally by having a system that was offline, right? So similarly we want to do that but in a more planned method. So you want to have a mutable, isolated, and a copy that's air gapped from your existing operational systems because this is your break glass in case of emergency copy in a ransomware scenario. You want to make sure that these are regularly tested so you know that they're going to be able to recover because this is the ultimate protection when all of your other defenses have been taken down by a sophisticated attack. You want to be able to get back to this particular point in time. So the key takeaway from this is that ultimately we want each copy to map to a different failure scenario. The local copies for that fast operational restore, these are the common things we've always dealt with in our IT world, those cross regional copies so that we can fail over in the event of regional type disasters, and then that cyber recovery copy which is well protected, isolated in that sort of last line of defense against ransomware events. So I'd like to hand it over next to Steve, who's going to go into a bit more depth of how you implement a lot of these things in practice. Thank you, Brian. So when considering the number, the type and location of your recovery copies, you need to consider. The criticality of each of your resources as well as the risks you are trying to mitigate. Do you need to be able to recover or are you concerned about a geographic loss of an entire region that you need to be able to bring up and move your entire systems to another uh another region in your area, um, or are you, you know, likewise concerned about ransomware and needing to be able to recover. In the case of losing access to your account, uh, malicious corruption to your data, or maybe even loss of your, uh, AWS organization's master account. Replication to other AWS regions. Allowed to um allow you to maximize. Your availability In the case of a regional disruption. They allow you to quickly fail over. In case of such an event, many AWS services, including S3 EFS FSXON tap. Dynamo DB, RDS, and Aurora support native replication functionality. Typically application owners. Build replication into their architecture since they're the ones who understand the relationship of the data that they're storing as well as the criticality of each individual resource they built their applications on top of. Based on your budget and your RTO. Application owners. may choose to Set up their replica target with minimal. Infrastructure for cost conscious uh. Applications But Um, for more critical applications they may set up a hot standby by having fully provisioned instances and storage services. Likewise. To simplify the fail back after a failure of the merging of the transactions. They may create a warm standby. By making the target a read only. As with any resiliency solution, it is critical that your application owners periodically verify the recoverability of their application. If for example they've configured a warm standby in another region, you may want to have them periodically, perhaps twice a year, perform a fail over to another region and then fail back. Elastic disaster recovery. is a replication service designed specifically for instance-based workloads. Regardless of where your application. I hosted DRS provides automated replication and recovery into into an AWS region of your choice. To configure DRS, you first install AWS replication agents on your source servers. These agents are responsible for capturing all disk activity at the block level. And transferring those, replicating those. Over to the region of your choice. Now These agents Capture Each change at the block level. And send them after compressing and encrypting them by only sending the modified change blocks. They're minimizing their impact on your network bandwidth. Now in the AWS region where the target is, DRS provisions lightweight uh instances that are designed to simply capture the changes and create. Um, a synchronized copy of your volumes from your primary site. DRS also configures snapshots at the frequency and uh with the retention of your choosing. And In the case of an event. Uh DRS in case of an event and you choose to go ahead and initiate a recovery, DRS will provision production instances in the subnet of your choice. Restore volumes from your snapshots using a time of your choosing attach those to those provision instances, allowing you to bring up your application within minutes. This architecture provides near zero RPO through continuous replication and minimal RTO through automated recovery orchestration. This is a good solution for your instance-based lift and shift applications as it provides the ability to fail over in the case of a regional issue. And it allows you to roll back to a previous point in time in case there was a data corruption. Now let's dive in. To some of the native uh AWS replication service services that support replication. Amazon S3 offers cross-region and cross-account replication. With a couple of options for delivery. You can choose best effort delivery for your. Uh, the cost sensitive, uh, applications. Or you can choose time controlled replication. That provides SLA of delivery within 15 minutes. Oops. For FSX ONTA, it offers 1 to 1 and one to many replication. To the AWS region of your choice. Dynamo DB, uh, Global Tables offers continuous, uh, active, sorry, active. Active replication across AWS regions. Um, it's a great solution for your, uh. Globally distributed applications that require low latency access as well as automatic failover. Where replication allows for quick failover to new infrastructure with little to no data loss. Backups allow you to restore your application to a point prior to an unintended data mutation. Now these mutations could be caused from a bug, a user error, or malware. Many AWS services offer native snapshot and backup functionality. Application owners rely on these features for operational tasks, for example, An application owner may Create a backup before deploying a Schema update to their database, or they may use a backup to clone their resource for performing test and analytics tasks, but after that. task is complete, they typically need to be able to delete those backups. But to comply with data protection requirements from the organization and the industry. Many organizations create data protection teams. These teams use AWS backup to define and deploy backup policies. These policies create periodic or continuous backups of their critical applications across all production accounts. And then from an AWS delegated administrator account in the organization, these teams can monitor. The job status allowing them to Address any issues that arise. Mm. Mm. To protect against An unauthorized user. Deleting your backups. You can choose to store your backups in a locked vault. The lock Will prevent any attempt to delete or otherwise modify the life cycle of the backups within it. For further isolation. You can store your backups in a logical air gap vault. Or copy them to a locked vault in an isolated account within your organization. A logical air gap vault. Can be shared with another account whether inside your organization or out. This allows your application owners. To perform tests without impacting the primary production account, so you can share your your vault with a test account. In that test account You can go ahead and perform restores, prove that you can rebuild your application on a regular basis, but to automate this process. You can build restore testing plans with AWS backup, and these plans. Well, at the frequency of your choosing. Select the latest backup of your selected resources. Go ahead and automatically restore that for you. Upon the completion of the restore, you'll be notified with an event. When you get notified. You can have a Lamdor script go validate the content of the thing that was just restored. To determine whether the data is as you expect. And the result of that validation can be sent back to AWS backup. Now that centralized report that I talked to you about a little bit earlier, uh, that will also include with your restore job reports the status of the restore as well as the result of your validation. One thing else I want to point out is the backups created by AWS backup can only be deleted through AWS backup, APIs, CLIs, or console. This makes it simple for you. To create a separation of control. Whereby the data protection team owns the life cycle of the backups they create. Where the application owners own the life cycle of the primary application. Likewise, not just with regards to the life cycle. You have a separation of control of access. The backup administrators can maintain the access to their backups. Through policies placed on the vaults and the application owners have control over the the access to the application itself, the people who are managing the data the data protection team folks themselves, they don't have direct access to the primary resources. They simply have access to pass a role to AWS backup. Now let's take a look at a simple example. Here we have an application, uh, simple, built on top of RDS EBS, which are zonal services, and Amazon EFS, which is a regional service. You can configure AWS backup through backup plans to back up all three of those resources, and if you're concerned about a regional disaster, you may consider copying those backups to another AWS region. Now Let's say the availability zone is no longer there. You can use AWS backup to restore your application into another availability zone by restoring the regional services RDS and EBS, then building a new compute instance and connecting the resources you restored as well as the regional non-impacted EFS file system. Similarly, if the whole region has been destroyed by a natural disaster. You can use AWS backup in the target region to restore all three resources, once again building a new production instance and connecting all three resources. The well architected framework provides 6 pillars for building robust cloud architectures. I'm going to bring together many of the things that the three of us talked about. To help you build a strategy, define a strategy for your data protection. Since the criticality of your data varies. From application to application as well as within an application. The first step is working with your application owners to classify the data that they manage. Then for each class. Defining your strategy with regards to what replication requirements do you have in order to mitigate the risks that you're concerned about. And build backup policies at the organization level. That will provide the backups required for doing recovery after an unintended mutation of your data. From a security standpoint. Please ensure that. You use the same rigor. For managing access to your backups as you do for your primary resources. Because every copy that you are creating. To protect yourself from availability or malware, every one of those copies creates another opportunity for data exfiltration. And your application owners probably have configured all sorts of controls to manage the access to their primary resources. As a data protection team, you need to make sure you're having the same sorts of controls to ensure that the data that you're holding in your backups don't get leaked. Finally, It is critical for you to test your recovery, your application owners. Need to be able to not just show that they can restore a given resource, but they need to be able to practice recovering their entire application. In another account, basically, if you are trying to mitigate a risk where you lose access to your primary account. Your application owners need to not only make sure that things are backed up, but they need to be able to prove that they can recover their full application in another account. Now, from a delegated administration account. Please make sure your data protection teams are monitoring the status of your data protection jobs. So that if there are failures that occur, They can go back and address those failures. Uh, any configuration issues that are causal to those errors. And you can also create compliance reports from that centralized admin account allowing you to know. That you have the backups for your critical resources within the time that you expect. Now, let's take a look at another example. This one's still a simple EC2 based application, but this time we're gonna talk about more uh holistically about the various things that we've talked about. In this situation in your workload account. You have your first backup copy using AWS backup and it's just stored in a simple, uh, backup vault. Your backup plan can be configured to also copy those backups to a secondary account, this one perhaps storing the data in a logical air gap vault that will allow you to assure you have access to your backups even if you lose access to your accounts. Now when the backups are copied over, you can choose to Capture that event. And go ahead and build orchestrated testing or analytics in that account here, of course, like I said earlier, you can use automated restore testing to verify the recoverability of the backups you created and be able to report that back to your auditors. Now for a 3rd copy. Consider using for this basic instance-based application. DRS elastic disaster recovery. DRS will go ahead and replicate your volumes for your application. Over to a region of your choice, thereby allowing you to fail over to another region if there's a regional disruption or a regional disaster. So with these 3 copies you're protecting yourself from Loss of uh. Your data due to a simple mistake by the user, a malware attack that maybe takes over your account and destroys it, or destroys your organization account, and you also have protection against a regional outage. So at the top of this presentation, Danny talked about a massive cyberattack. I'm gonna walk through how you can configure a logical air gap vault to support, to protect you against such an attack. The first step is oops. The first step is creating a recovery organization. And in that recovery organization in the management account. You need to create identities for a handful of trusted individuals in your organization. Now you don't want to use the same IDP for those identities because you do not want to have any shared. Dependencies between your recovery organization and your proud organization. Create an approval team with those identities. Specify how many approvals you need to successfully approve a request. Then you can share that approval team with your workload accounts and associate them with your logical air gap faults. Then if a disaster occurs and you lose access to everything you own, You still have access to your backups. From a newly created. Uh, account you can use AWS backup. To request access to that logical air gap vault. Using the ARN for that vault, the request will be forwarded to the approval team. If they vote and say yes, you'll be given access to your vault where you'll be able to restore your applications. With this system, you are guaranteed to be able to recover even if you lose access, as was described by Danny, to everything you own in your production environments. With that, let me go ahead and pass you back to Danny for him to close us out thank you. Great, thank you. Fantastic, thank you Steve. Um, so just rounding out the session then, um, I think it's fair to say we should be preparing for the worst. Assume the breach, assume that digital extinction level event and work back from that. And I think it's important as well to really understand those IBS, those critical services that make your business a business. Now, as I mentioned at the top of the session, I work in financial services. I speak to a lot of banks. So typically in a bank, for example, you'd have a list of business services and a list of infrastructure services, probably a dozen aside, those mission critical ones, so you'd have payments. Payroll is a pretty important one as well, because without employees you haven't got a business. For a bank, you'd again you'd have mobile banking, you'd have online banking, ATM access to cash, access to balances, and if they've got a big trading division, then a Mux front office. And then on the infrastructure side, we've talked about Active Directory already, using the Maersk example, but there'd be others. I'd include DNS in that for a mainframe environment, you'd have LPAR. And once you've buttoned down those IBSs, you need to test and test and test again and validate that end to end recovery. And make sure it's aligned with your business processes. Please do take us up as well on the opportunity to run a cyber event maturity assessment workshop. We'll be hanging about after the session here. Now the customers I've ran the workshop with have gleaned a significant amount of value and benefit from it, and it's really helped inform their business continuity plans and strategy moving ahead. Um please do as well continue your AWS storage journey, your learning journey. There's a lot more information to be found at AWS.training/storage. And um finally, thank you ever so much for your time. Please, we'd really appreciate the session survey feedback, we'd love to come back next year and share some key insights with you. So please do complete the form. Most importantly, thank you so much for your time, we hope you've enjoyed this morning and we hope you have a fantastic rest of your day. Thank you so much.