---
video_id: a52lS-7Vq4c
video_url: https://www.youtube.com/watch?v=a52lS-7Vq4c
is_generated: False
is_translatable: True
---

Here's the paradox we're facing in 2025. We're building AI agents that can reason, act, and plan autonomously, agents that should be able to achieve your business outcomes, optimize business operations, and also make real-time decisions. Yet most organizations are sitting on petabytes, sometimes exabytes of data, that their agents simply can't use effectively. Your customer service logs in S3, product documentation, some sort of a distributed file system. Years of transaction history in S3. Data that has been perfectly stored, governed and backed up, but there's a gap, a critical gap in terms of where the data sits and how your agent needs to access it. Here's the irony the data is already there. You've invested in terms of storing it, backing it up, and protecting it. But when the agent needs to recall a customer interaction from years ago, or find a pattern across millions of transaction history, or tap into institutional knowledge when it needs to make a decision, it hits a wall. Because agents don't just need data, they need memory, they need context, they need to be able to learn, collaborate, and adapt in real time. The breakthrough everyone's chasing isn't about building smarter models. We're already getting pretty good at that. The real breakthrough lies when your existing data becomes agent ready. So turning passive storage into active memory, transforming archived information into accessible intelligence that your agents can actually work with. And that's exactly what we hear today. Because the most powerful agent is only as good as the data architecture behind it. So let's talk about putting your data to work, but not for, not just for analytical or compliance, but for AI agents that are shaping up the next generation of applications and enterprises. I'm Venara Sisler. I'm a senior worldwide specialist solutions architect here at the AWS. So from today's agenda perspective, we'll cover these topics. So first of all, we'll start off with covering a few fundamentals in terms of what makes an agent truly agentic versus just another simple chatbot. Then we'll dive deep into what it takes to build context of our agents. We'll also explore building scalable agents using managed versus self-managed approach. Then we'll dive deeper into how AWS storage becomes the external brain for your AI agents. Finally, we'll leave you with a few architectural examples and references and and a few resources that you can take away after the session so you can learn and dive deeper. These numbers should grab your attention, because we're looking at a massive transformation that's happening right now. Gartner predicts by 2028 that 1 in 3 enterprise applications will have agentic AI. That's nearly up from less than 1% from 2024. And we're also talking about a $120 million transformation that's happening right now, sorry, $120 billion market by 2030. Which tells you that this isn't just a hype, and enterprises are making serious investments. And I'm also seeing this firsthand from my customers where they're where they're transitioning from building simple Q&A chatbots to really complex multi-agents as part of their enterprises to solve complex problems. The question isn't whether this will happen, whether would, would you be ready when it happens. And some of the customers that I've worked with include Ericsson, Thomson Reuters, Expedia, so on, who are actually going really deep in in terms of deploying agentic architectures as part of their enterprise applications. Agentic AI promises to enhance productivity and efficiency, you know, taking on problems that wasn't or that were difficult to be solved by traditional software, simplifying integration and also finding answers and data that were previously invisible. Agentic AI systems autonomously decide how to accomplish a task, taking on prompts in the form of a natural language and adapting their plan as they're learning about new information. But there is still a human in the mix. So a human essentially gives the overall sets the goal in the form of natural language and exercises supervisory control. So what makes it truly special is their ability to sort of learn and improve over time. So every interaction, every new, um, I guess piece of information that you give it to the agent, it essentially stores it in memory and then adapts for its future conversation. This diagram illustrates what transforms a simple LLM into an autonomous agent. Which is capable of solving complex problems. The key difference here is autonomy. Because these systems don't just respond to prompts, but they're actively working towards accomplishing towards a specific objective or goal set by the human. So going with the components, the first we have is LLM. LLM is more capable or responsible for. Reasoning capabilities, so it'll give you the understanding of the intent as well as making decisions about next steps. Second, you have tools. Tools give the ability of the agent to interact with the internal and external environment, whether that's extracting data, executing certain functions, or also interacting with systems. Third, memory. Memory ensures continuity. Agent doesn't need to start from scratch all the time. It essentially builds or resumes from where it left off. Third is context awareness. Essentially allows the agent to understand its environment so that it can adapt its behavior based on who it's interacting with. Finally, prompt engineering. Prompt engineering defines agent's role, capabilities and constraints that it needs to operate with. What makes this powerful is the shift in how we interact with AI because instead of step by step instructions that we are used to over the last couple of years ago, you're now providing the overall goal in the form of natural language, and the agent then autonomously plans the required steps to achieve that goal and continues to deliver so. This is a million dollar question that every organization is grappling with right now. Most context aware agents don't just need a good LLM, that's just stable stakes. You need persistent memory, real-time data access, and the ability to learn and adapt over time. Most importantly, you need all of this with enterprise scale, enterprise grade scale, security and governance. Let's dive into what this actually looks like in practice. Agent memory is a computational exocortex for AI agents. Which means it's a system that combines LLM's built-in memory with persistent storage, so that it's able to remember, retrieve, and adapt over time with past experiences and the new information that it's trying to learn. Just like human memory, it helps agents to build knowledge over time, maintain context across conversations, also learn behavior adaptations based on interactions. Transforming from one-off responders to reliable and truly intelligent systems. Our customer conversations show that one-off interactions are really uh good at capturing the initial or the first of impression, but the real value comes from the agents having ability to remember the context, learn from history, and also adapt the overall experience. So memory management isn't just a feature, it's the core infrastructure that turns reactive agents into truly intelligent systems that deliver sustained value for your enterprises. Let me show you a real world example. That perfectly illustrate why memory matters. So the difference in user experience is really dramatic, and that actually translates directly to the business value. Pay attention to how the customer conversation, sorry, how this particular conversation flow changes completely when the agent is able to remember the past conversation and build upon. Without age and memory, there are several critical limitations that we will end up hitting. First, inability to maintain conversation continuity. It cannot really build upon or reference previous dialogue. No behavioral adaptation. So essentially you cannot learn user feedback or adjust approaches based on your preferences. Third, lack of personal objectives. So without personal objectives, then it cannot really sustain the overall session or achieve the goal set by you. And lastly, missing personalization, so it cannot really develop user specific preferences, and I've got some examples in the future slides that will demonstrate why your personalization is a very important trait for AI agents. There's a research that was done by Microsoft and Salesforce, and the study is called LLMs get lost in multi-tone conversations. So the study found that most of the LLMs experience significant performance drop. Uh, in extended conversations. Primarily because those LLMs make premature assumptions at the very early on and they essentially fail to recover when they're proved wrong on those assumptions that exactly goes back to the conversation continuity limitation that we just discussed above. Here's another frustrating example of, you know, agents without memory. Every conversation starts from scratch. Notice how the agent asked for the iPhone model twice within just 2 days apart. This creates a terrible user experience and feels the agent really robotic and unhelpful. I love this quote which says, you know, AI agent without memory is like a goldfish. Everything's new every 3 seconds. When we implement proper memory, we unlock 3 key capabilities. That transform the user experience. First, contextual intelligence, meaning the agent. Not just understands what you're asking, but why you're asking it. Second, user preferences. It truly personalizes interactions. The agents are able to remember or adapt based on how you work and communicate. Third is knowledge retention. So with the continued interactions, agent builds its own knowledge base about the world, the facts, the things, the participants, and it gets essentially smarter with every interaction. So these aren't nice to have features. They're really essential for enterprise level adoption. So now let's dive a little deeper into agentic memory. Understanding these core concepts will help you design AI systems to work at scale. And think of it also, you're building neural pathways for your AI. Because if you get this right, everything else from an AI agentic deployment becomes much easier. Agentic memory is really critical, especially when you're building personalized AI systems. Because it enables adaptive learning through each interaction. Allows agents to understand individual preferences, communication styles, and also behavioral patterns. Without access to persistent memory, even the most sophisticated agents or chatbots. Cannot really provide the personalized user experience that we are demanding from this day and age of AI applications. This is probably a personal reflection of me looking at the last couple of years. You know, GPT models have provided broad general knowledge to begin with. So we quickly introduced RAG to ground those GPT models to our proprietary data. But what we quickly realized was as rag architecture scale, we encountered that there is a limitation with finite context windows which limited us with the conversation continuity problem that we, you know, tried to uncover previously as well as personalization in the overall experience of how we interact with AI. So age and memory essentially extends the rack capabilities by providing persistent memory across sessions. Enabling agents to build context over multiple interactions and delivering really relevant and personalized experience to users, so agent, so the rag concept doesn't go away, it is essentially getting extended into the concept of agent memory where it's trying to give us multiple benefits. Data retrieval becomes a fundamental. To agents' memory architecture. Effective systems must intelligently surface the relevant context from vast stores of user stories and past interaction data. So this requires sophisticated algorithms to precisely identify which elements of past conversation. Would really supplement the current context of existing interaction. Enabling truly adaptive learning that compounds over time. And advanced systems still require higher order forms of information retrieval. Organization and retention that mirrors human cognitive process which we now define as agents memory. So there are 2 fundamental types of memory that every agent needs. Short term memory It's like a ram. In a computer which is mostly temporary and session based. Long term memory is like a hard drive, which is more persistent and supplements evolutionary learning. So this dual memory architecture enables both immediate responsiveness and also sustained improvement over time. Long term memory is also crucial for enabling AI AI self evolution where agents automatically learn, adapt, and refine their reasoning based on accumulated examples and interactions of data. So by incorporating long-term memory, AI agents become like adaptive teammates where they're really getting specialized in their skill and the knowledge over time as the subject matter experts just like humans as we would evolve into. So let's dive a little deeper into the short-term memory concept. So short-term memory is all about maintaining the conversation flow and immediate context. So in this particular example, see how the agent is able to remember the iPhone model. From the earlier conversation and provide really specific help to the user. So this requires storing and retrieving conversation history in real time. The agent needs to quickly quickly access recent messages and understand the current context and maintain that state. Across multiple interactions. From a storage perspective, this means that we require really fast and low latency access to the data. Think of this as agents working memory. It needs immediately Accessible, but also doesn't necessarily need to persist forever. Short-term memory units last anywhere from seconds to days, depending upon the application needs. So there are two terminologies when we were when we mention short term memory. There is working memory and there is short-term memory. Often the terms short-term memory and working memory, sorry, uh, working memory are interchangeably used, but there is a clear distinction. Working memory is a special type of short-term memory that is used specifically for actively processing information for that specific task. Short term memory is a broader temporary storage for the overall session. So not all working memory is short term memory. But all short term memory is working memory. So working memory is the doing part. On the other hand, short term memory is the holding part for that session. Episodic memory is the agent's record of specific events and interactions. Just like humans' personal memory of our own life experiences. It stores conversation history, summaries of important events, and individual occurrences with specifically attached metadata such as time stamps and participants. Conversation memory is a specific type of episodic memory that is essentially focused on chat history or user preferences that you see here. It keeps a complete record of conversations who said what and when. And also helping agents staying consistent throughout interactions. And it is also able to refer back to the earlier parts of the conversation and really uh you know, provide the contextualized responses as part of the interaction. So the system continuously updates its memory blocks. As the conversation progresses. So in short, episodic memory is the what happened storage. And the conversational memory is the what we talked about storage. So long term memory is where things get really interesting, and this is about learning and personalization over time. Notice how the agent's able to remember the specific preferences from a few days ago. It is able to remember the uh brand, the employee discount, and also the color of the headphones. So this isn't just storing data, it's about extracting and organizing those insights that can be applied to the future interactions. So essentially agent builds up a profile of user preferences that gets richer and more accurate over time with every interaction. So from a technical standpoint, this requires a very sophisticated uh storage and retrieval system that can quickly find relevant res relevant preferences based upon the given context. So this is where the vector databases and semantic search becomes really crucial for the overall agentic performance. Next we have semantic memory. So semantic memory is an agent's organized knowledge base. Everything the agent knows about the world, including the facts, the concepts, how things relate to each other. This includes knowledge bases. Essentially that's a collection of factual information. Entity memory, which is specific details about people's, facts and things. Third is persona memory, which is role-based knowledge that essentially guides the agent in terms of how it should behave with every interaction. So semantic memory is essentially agent structured world knowledge that enables consistent reasoning. So the most common real world example is RAG. So we've used RAG over the last couple of years, which is able to take the factual documents or the proprietary data that we supplied and able to provide contextualized responses only from the given factual information. So in simple terms, semantic memory is an agent's encyclopedia of facts and concepts that it can further reference back for to be able to answer any questions or future interactions, also make decisions and separate memories from each other's memories, so it is able to through this mechanism, the agent is able to sort of separate out the memories for each every user profile. Sorry. Um, in this particular example, the agent has learned the specific business values or business rules, so it, it's learned the return policy, the employee discount, and also the, um, overall product specification. So the type of This type of memory is really part of uh particularly powerful in the in the context of enterprises because especially when you have, you know, company policies, factual documentation, or, uh, various product catalogs when you have different dimensions of information when you want agent to find, uh, semantic meaning or context across different dimensions, semantic memory is where comes into play. So rather than just storing the raw conversation data, the agent essentially builds up a knowledge base, as you're seeing on the right hand side as an example, with full of facts and relationships. So this is where the integration between your agent's memory and your existing data sources with your organization becomes really crucial. Third, summary memory, essentially shortly known as distilling for key insights. So summary memory is another type of episodic memory that distills key insights from longer interactions. So practically we can store every single interaction in the storage. But when we're looking to scale the overall agentic performance, retrieving every single interaction from the storage and then extracting the insights out of it will be a very laborious task or it would be very performance uh intensive, so which is why the agent essentially stores a summary of the very long conversation uh with really key messages or the storyline of what that was what was the interaction about. So if you in this particular example you can see it's talking about the user bought the headphones, there's a price match or you know found a cheaper price and it was able to you know do the price match as a result of it. So it is also where you start to see the value of agents, you know, having the ability to sort of learn and adapt, you know, with the summarization over time. OK, um, thanks, I'm, uh, John Mallory. I'm a, uh, go to market specialist here at AWS, and so I wanna switch gears now and, you know, Venata just walked us through the importance of both short term and persistent memory for building agents. What I'd like to do is double click into some common approaches of how you do this and then layer in how storage, um, supports all of that. So let's get started with how you're going to build and deploy and host agents. Um, you know, a very common design pattern is a lot of AI builders wanna start off using open source frameworks, you know, like strands agent, um, Lang chain, Lama agent, Autogen, um, there's a whole host of them out there, and they're really good because, um, you know, they can accelerate. Experimentation and uh building and learning um they're you know have a lot of packaged um tools built in and um you know they really um can simplify particularly using um interfaces and protocols like MCP and agent to agent um stitching all the components together um but the challenge lies in how do you start to move this into production. Um, because then you have to start to worry about scaling infrastructure, managing security, managing the various types of, uh, memory that, uh, Venkata talked about, and, you know, gluing all these pieces together, um, you know, as you want to scale to hundreds of thousands of users, have very complex orchestrated agentic workflows, make sure you've got the right guard rails and safety and security around it, it can quickly get very complicated. Um, you know, so this is evidenced by a Gardner study that over 40% of, um, agentic AI projects, um, will be canceled in 2027, you know, due to unclear business value, um, increasing cost, um, and, um. You know, questionable, um, security and governance policies that aren't going to meet enterprise requirements, um, so but that being said, we do see a lot of sophisticated customers using self-managed frameworks. Um, there are a couple of other approaches, um, you know, down on the left, um, if you're just starting out your journey, you wanna leverage the power of agents, um, we have Amazon Que. Um, in our, uh, quick suite which has agents packaged in for common enterprise workloads, um, you know, so you can really leverage the power of agenttic AI without needing to build anything. Next stop is the fully managed approach using, um, Amazon Bedrock, um, agents, um, which, um, you know, really starts to stitch all these pieces together. Um, handle, um, key parts of the infrastructure like hosting the LLMs, building knowledge bases and reg, and orchestrating, um, multi-step agent workflow, so that gives you a lot of flexibility, um, and, um, you know, can help you get started quickly. Um, but The key thing is you don't really have to choose between a do it-yourself approach using these open source frameworks and some of the features and capabilities of Bedrock, um, you know what we released, um, back in July and went GA in, um, October was a Bedrock agent core. Um, which removes a lot of the undifferentiated heavy lifting of, uh, building agents where you can kind of take a mix and match approach. You can use your open source frameworks and, um, you know, choose the, uh, framework of your choice, um, have your choice of models both inside and outside of Bedrock, but then have these agent core components that really start to make it a managed experience, you know, so if you take like run time. That handles uh compute where you don't have to worry about provisioning scaling compute up and down it does that um agent core memory reduces a lot of the uh work you have to do to build both the short term and the long term uh memory that uh Venkata talked about. It automates a lot of that and then um. You know identity um helps with all the security and permissions and um gateway can help orchestrate between all the tools you're gonna use you know so really um this is a great way to um get started but even if you take this approach and you use agent core memory um you still need to think about storage um when it comes to building agents. Um, so before we dive into the various components, let's talk about a few of the challenges you need to be mindful of as you start this journey or, you know, even as you evolve in this journey. Um, the first is you've gotta make all of your data. Um, accessible, discoverable, and actionable, you know, agents that create business value are going to need to access all forms of your data that you have, um, in your, uh, organization or your company or your enterprise, um, and this is everything from structured data that may live in data warehouses, databases, um, you know, transactional data that, um, you know, drives all of your business, uh, transactions and billing and, um. You know, key use cases that are gonna power the business daily all the way to depending on what industry you're in unstructured data of all forms, you know, things like images, video call record logs if you're in specific industries like health care, medical records, pathology reports, images, agents really need to be able to understand and choose from all of these data sets. Um, the second key challenge is really privacy and security and governance, um, because ultimately you want the agents to access as much data as they need to be effective and to add value, but you need to put guardrails on that because God forbid you build an agent that exposes sensitive, let's say patient data. To, you know, the world that can't happen, so you've got to have proper governance and guard rails and then as uh Venkata discussed, um, agents are going to act if they're built and um really adding value autonomously without human intervention. And achieve complex goals so you really need to monitor this both to make sure you don't have any data leakage or you know governance breaches, but also to collect data on how to improve accuracy and iterate as your agents learn and evolve so um you need to think about governance and finally semantic uh search and storage is um key. Both for grounding agents with data via rag as well as the long term memory and this isn't just theoretical. I had a customer meeting this morning with a large enterprise software customer who is they've built some agents they've seen some business value and so they really want to scale it, but now they're thinking about how do we build the data foundation to do this. Essentially they want an agenic layer that they. Can bring the agents to the data because if you start to bring um data to the agents now you're back with data sprawl and trying to maintain data fidelity across multiple copies so it really kind of brought all these challenges to life and they've got multiple data warehouses, snowflake data bricks, um. Uh, Redshift, Athena, you know, all these different pieces, and so they're trying to figure out how to stitch this all together into a common data foundation before they can even start their journey. So let's switch gears now and talk about some of our storage services and how it can help with all this. Um, this is a map I like to use to start to layer in how storage, um, interacts with agents and the memory requirements, um, and really no matter if you're going to use Bedrock Agent Core, our memory, uh, Bedrock Agent core memory which manages the short term and long term, uh, memory. You've still got to think about that data foundation back to this, you know, enterprise, uh, software provider challenge, so you wanna start with a data lake, um, ideally built on S3 if you're building an Amazon and then using services like S3 tables, you know, that's really going to. Aggregate your data, allow you to provide um common security and governance, and then make the data cataloged um and discoverable and usable by the agents, um, so that um you know, is why we use S3 because of the durability, scalability. Uh, both the AWS first party services that integrate with it, but then all these third party providers as well that use it as a common data hub, building on that as Vinata discussed, um, short term memory is really the temporary storage or working memory for information that the agents are processing, you know, so it's going to be stored typically in raw data formats, um, and if you have multiple agents coordinating with each other. Um, they're going to need to exchange information with each other, so they're going to need to have a shared storage layer. Like if you think of, um, you know, a research agent. Workflow you're probably gonna have an agent that's um you know discovering data you know maybe um looking searching through academic papers another one that's verifying citations and rights to use that data and then another one that's summarizing and this is all gonna be iterative so they have to communicate so given this we typically recommend starting with um our FSX family of file services there's a whole family to choose from. They're low latency, um, scalable, you know, you have a lot of options to save cost to make it easy to manage for scratch space, um, and for the shared memory access. S3 does play a part here because ultimately you're going to want to snapshot that state, um, so that, um, you know, if things, uh, crash you can quickly recover, um, but then you're also going to wanna start to stage and consolidate all that short term memory. And run it through a processing pipeline to start to create the longer term memory, the semantic memory, um, that, uh, Vencaa talked about. And so, um, you know, really you can use S3 for that. A couple of other components, um, of short term memory are a lot of these agents can be highly transactional, so people may wanna use a high performance key value store to capture state like Dynamo DB. Um, and then another layer you want to build at that level is, um, a, uh, semantic cache layer oftentimes which, um, you know, might be um something like um elastic cash which recently introduced vector search because if you put caching in front of your LLM and your rag and your prompting. Even though those storage layers can be expensive on a dollar basis, if you reduce the interaction back and forth with the LLM and cache responses, you know, you're quickly driving down cost and making it a much more cost effective architecture. And then finally, um, you know, as Venkata also indicated, you need to think about semantic memory and rag to ground. You know, the LLMs and get the most accurate results and so you know we recommend um S3 vectors which um you know just went GA today and uh Garmin's keynote is a very cost effective, highly durable, you know, very scalable approach for that. So What do you want to consider when you're building that data lake foundation? I talked about this enterprise customer who has all these different platforms that they're trying to integrate, you know, with their own formats, their own catalogs, their own security and governance. The industry has built a open source solution to start to address these challenges called Iceberg, um, so if you're building a new data lake or even if you have an existing one, you probably wanna think about how to adopt Iceberg as your data lake foundation, um, because a couple of key advantages of it are. It um brings a lot of the data warehouse capabilities um like um asset transactions rollback um and um a number of other uh transactional integrity fields into an object based data lake um, secondly, it has um. Iceberg Rest catalog endpoints which start to make it easy to integrate different catalogs, data catalogs that different provider solutions have together with each other. So now you can start to get better interoperability between multiple data engines, both AWS native ones, as well as a lot of third party and even open source ones. So you know, definitely consider building Iceberg as your data foundation. Um, and to help with that, we launched S3 tables, which is a native iceberg managed table within S3. We launched that back um a year ago here at Reinvent and have continued to iterate on it to the point where today we just announced intelligent tiering for it, so really help you optimize cost for Iceberg and native iceberg replication across regions so you can really if you're an enterprise start to build, um, you know, highly resilient, highly available iceberg. Um, I wanna divert a minute and talk about model context protocol because that's really the key to stitching all these components together. It's an open source standard that allows AI agents via MCP clients to, uh, communicate with external tools, data, and services in a structured way. You know, so it allows agentic AI vendors and builders to start to um have access to potentially thousands of tools that your agents can call upon without having to learn all those individual interfaces. You just need to know how to speak MCP. Pick the right MCP servers um from a catalog of tools and now you can start to build and if you have data over all these different devices um you can start to um you know quickly stitch it all together your agents can start to understand how to access without knowing how to speak individually to each one of these devices so it's like a USB port for agents is the way I like to think of it. Um, and we've started to adopt that in the storage family as well, um, you know, we released an MCP server for S3 tables, um, so that, you know, now, um, you know, agents that, uh, want to use tabular data stored in iceberg format can, um, speak to and understand. Uh, and access data and S3 tables, um, you know, one tip I would recommend when you're thinking about, uh, MCP, particularly if you're just starting out, is you should really consider only using it, uh, with a read-only capability and scoping down, you know, to the least needed access privileges via IAM. And these MCP servers like uh the 14S3 tables support all that through IAM, but um you know really be deliberate about, you know, not doing an allow right type capability unless you're absolutely sure you have a specific use case that needs that. Um, another key is data discovery. I mean, you're gonna build this data lake, you're gonna have all your data aggregated. How are agents going to discover this, um. Structured data catalogs have been the foundation of analytics and data lakes for decades, but a lot of the data that AI agents are going to use is unstructured data of a lot of different types, and traditional analytic catalogs don't really deal with that well, so you really need to think about a metadata strategy. And then you know having that metadata really be discoverable and consumable by agents so that they can kind of self discover and just you know self describe and understand the data that they're drawing upon so you know metadata is really what makes it actionable. And to help with that we released um S3 metadata which is a built-in metadata service in S3 um that will take system collected metadata, allow you to augment it with user generated metadata, and then, um, query it through an iceberg table so um you can actually use the um. MCP server for S3 tables to also let agents start to discover metadata and the data itself. And so the way it works essentially is you turn it on in S3 um and it will S3 will populate a metadata table both with inventory of all the objects but also with lineage and um you know essentially mutations of the objects so you can also start to use it for governance and so when you combine all these pieces together you know with S3 tables MCP. Um, you know, AI agents can now really have true data discovery, um, particularly if they have MCP servers that also speak to things like iceberg catalogs. Um, another key thing we just introduced, um, in the keynote today, uh, for NetApp was, uh, S3 access points for FSX because one of the challenges is. It's well and good to say build a data lake, but a lot of people's data still lives on premises and traditional file systems. So what do you do about that? Because there are a lot of legacy applications out there. That don't know how to speak to object storage. They, you know, were built to speak to traditional file systems. I mean, IDC indicated a study they did, you know, that while things are moving to the cloud, um, you know, 48% of data still lives on premises and 29% in the cloud by the end of 2028, so it is a gradual transition and really you need to start to bring all this data together in a coherent way. Um, you know, because while you could bring the agents to data now back to this customer I talked about, um, you start to run into data fidelity issues and so, um, this is where FSX access points, uh, for S3 really help is because you can take data that lives in traditional file systems and if it's in NetApp file systems. Which has been, you know, a standard on-prem for decades, you can start to use their replication capabilities, snap a copy, maintain a coherent copy with the cloud, even if you're running on premises, expose that into S3 as if the data is living there natively, and now when you're building that data foundation, you've integrated your file data without having to maintain a separate copy. And this is really important because the same IT or Gartner um IDC study indicated that today an enterprise has 6.4 data silos per org and has to manage 13 copies of data. So anything you can do to simplify that is really going to help with, um, you know, making your agents have access to data in a more simplified way. And then finally, um, vectors vectors really are the language of AI because, um, you know, you can take increasingly sophisticated embedding models, um, create vectors of any data type, and then find things that are semantically similar both in context, meaning, and if it's agents, you know, things that even look the same so, um, they really power a lot of this everything from rag to, um, you know, intelligent search to long term memory. And so we uh launched S3 vectors, um, this summer. It went GA today, um, and it really is designed to have good enough performance for a lot of agentic workflows and rag workflows, um, lower cost of, uh, similarity search and semantic search by up to 90% when you look at TCO. Um, and scale to billions of vectors. In fact, with the limits we introduced today, 2 billion vectors per index, um, 10 billion in 10,000 indexes per bucket, you can scale to 20 trillion vectors in a single S3, um, vector bucket. So you're not gonna run into scalability challenges. Um, and so starting to stitch all these pieces together you can start to do things like agentic search, um, you know, where agents can start to use conversational information and choose strategies, um, you know, so Vencaa talked about, um, you know, a shopping assistant and mapping that to short and long term memory. You can really start that shopping assistant can start to pull in, um. You know, image data, um, you know, and user preference data and make intelligent recommendations to buyers and even start to create images, you know, using a multimodal model so that they can start to visualize what that dress, um, that they found this one dress they want one another one like it might look like on them so, um, you know, super powerful and then just to ground it in a real world customer example um. BMW Group has a 20 petabyte data lake. They call it their cloud data hub of all of their, uh, data about their vehicles, their manufacturing processes, their warranty information, and really they wanted to make it simple for all of their users to access all these different data types without being data experts and so they built an agent powered search capability for this. Um, which really can provide, um, three modes of search including direct structured search, hybrid search across structured and semantic data, and peer similarity search based on the context of the, uh, you know, uh, natural language query the user's trying to do, you know, so it really does democratize access to data, um, for all of their users. So with that I'm gonna turn it back to Venkata. Thank you, John. So this is probably my favorite slide of the overall content, um, so we're gonna talk about when does data actually become memory. So we saw the first part we understood the memory concept constructs, right? And then John covered what are the building blocks that facilitate the overall memory building process. So now we're bringing both together. So essentially how when data becomes memory. So I guess in agent AI agent uh development, data and memory are interchangeably used. But that's fair because memory is actually data, but there is a clear distinction between both the terms. Data actually becomes memory when it transforms a passive information into an active component that informs agents behavior and reasoning. So it's a 5 stage transformation process. So to understand the overall uh transformation in an easier way, let's understand there's a use case. Let's pick a use case which is a customer chatbot, um, on an e-commerce website so that we'll pick the overall flow and then build the overall transformation pipeline. So let's say you got your source data stream. So on the e-commerce website you got different channels of you're getting the order information, you're getting the, um, like supply chain merchandise, right, all sorts of information coming through the sources. So the first step is to aggregate them. So you consolidate them. Into one single structure, right, so you bring them all together typically in a data lake and you aggregate them in uh at the at the first stage. The second step is encoding or structuring, so we then take the structured and unstructured data that we've aggregated and then create vector embeddings out of them with enhanced metadata. So not just creating vector embeddings would be helpful, but adding more extensive metadata to it takes a long way. So the second step of structuring is creating vector embeddings using your preferred model. All the models available out of Bedrock. So the third step is storage, which is where the persistent encoded data, like the data that we have the embeddings that we've created as part of the stage 2, we create them in stage 3. So this is where S3 vectors shines as a long-term persistent memory store because it has the scalability attributes from a latency perspective, storage, and the performance. So S3 vectors becomes the preferred long-term memory store for AI agents. Fourth is organization. So essentially we structure the data through modeling, indexing, and relationships. So conversations are organized chronologically. Product information is organized hierarchically, and order data is, you know, spread across multiple conversations. So it's mostly structured across either time-based, topic-based from this particular use case perspective. The last stage is activation or retrieval. So this is a crucial stage where information becomes actionable memory. Through text search for exact matches, vector search for semantic similarity, and also graph reversal for complex relationships. And then we have the LLM. So we then utilize the LLM's built-in capabilities and go through an iterative process. So as LLMs, as we're interacting with the chatbot more iteratively, it's able to take the the learnings from the interaction, put it in the storage again, goes through a reorganization. It then gets activated and then sends back to the LLM. So it's this is the iterative process which essentially gives the ability for AI agents to become truly intelligent and adaptive systems. So a memory unit is a structured container that holds information plus metadata and also relationships about the information it's learned, so that essentially makes it useful for agent reasoning. So unlike traditional data storage, which essentially treats all information as equal, there is a little bit of distinction that we're trying to form here. So essentially memory units carry several entities, so the first one being the temporal context, which is when the information was learned. The 2nd 1 is a strength indicator, which means how relevant the information is and how reliable it is. The 3rd is the associative links, how it connects to other memories that it has stored. 4th, the semantic context, which is the overall meaning captured with the interaction. The last retrieval metadata, how and when it should be accessed. So data essentially becomes memory at the point of storage, which is step #3. So when it's collected and stored with the intent of enabling adaptation and coherent interaction over time. So this transformation is what enables, you know, the stateless applications into truly intelligent agents. So here's a comprehensive architecture of, you know, deploying agents using Amazon EKS. So this is more of a self-managed approach I would call. So this essentially highlights all the components working together. So you have authentication, model serving, memory management, and monitoring. So notice how S3 vectors are integrated within the memory construct. So you have the elastic ash and Dynamo DB as the short-term memory offerings, and then you've got S3 as the working memory or the scratch pad as John covered, and then we have S3 vectors covering the long-term memory. So this architecture also includes proper security with cognito authentication and monitoring built in using Amazon Cloud Watch. So this is a production ready pattern that can scale to support multiple agents and high user volumes. So now let's look at a weather agent and how it handles a simple query workflow. So the agent maintains the session state or its session state in S3 as you can see. Um, It accesses its long-term memory through S3 vectors and also stores its user content in Amazon S3 Data Lake House. So the MCP server essentially enables the standardized integration with external APIs. So John covered the MCP, the concept of MCP, and the MCP offering for S3 tables. So we're essentially leveraging an MCP server in this particular example here. So even this simple example demonstrates the power of having a persistent memory and context. So notice how the agent just doesn't answer the question. It actually remembers the use of location preferences and also able to provide personalized recommendations as a result. The cognitor now. OK, so, uh, I know you've all got places to be, so to wrap this up, um, you know, I won't belabor this slide other than to say, you know, rocket companies is a home financing, uh. Uh, organization, um, that helps people, you know, through the whole process of home ownership. There's a good blog out there, uh, if you search rocket companies, AWS, and agents that'll go into a lot more depth of how they did this, their architecture. But um you know they used Amazon Bedrock agents to build an agent powered engagement platform for their customers that really helped their customers resolve their queries in their ass much more quickly and gave much improved um guidance which really helped with their customer satisfaction. So look up the Rocket Mortgages blog if you want to learn more um so closing out with a couple of key learnings um to hopefully tie all this together. You know, the first is, no matter where you're gonna go in your agentic journey, make your data actionable by building a modern data foundation, ideally iceberg based, um, on Amazon S3, and then use cataloging and metadata. Um, particularly in combination with MCP servers and tools so that agents can discover data and um use it and learn and evolve from it. Second is scale cost effectively because your data is going to grow if you're successful and so use Aventasan S3 as your data foundation if it works. And use um S3 vectors as your semantic data foundation if that meets your needs. Secondly, you want to iterate rapidly, so this means you're going to implement observability and look at things like your accuracy, you're retrieving, how your agents are interacting with each other, and how they're evolving over time. So you need to have a good observability story in place. And then second is. Make your agents quickly move from POC to production and drive business value so you're not one of those 40% companies that's going to fail in your AI project, um, you know, which means consider using Agent Core even if you want to take an open source framework just to reduce the undifferentiated heavy lifting you have to do on the security, the infrastructure, the storage side. Um, so to wrap up with a few actions and resources, um, you know, we've got a couple of blogs here. There's the second one is the rocket story that I mentioned, and then the third one for that EKS design pattern is, uh, how to build a self-managed rag, um, using EKS and Amazon S3 vectors that Venata wrote. So thank you so much for your time. We're happy to take questions.