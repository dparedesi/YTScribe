---
video_id: 9co9Du-9Wdk
video_url: https://www.youtube.com/watch?v=9co9Du-9Wdk
is_generated: False
is_translatable: True
---

Good morning. How's everyone doing today? Cool. Alright, I'm Normal Metha. I'm a principal special solution architect and containers tech lead at AWS. I'm joined today with my lovely colleague Isaac Moscara today, and we're gonna talk about what are we gonna talk about today? From monolith to microservices. Yay, who here is on that, on this journey? We just talked to a few people here. Awesome. You're in the right session. So let's get started with something that might look, you know, if you squint look familiar to you. Let's get started with a real scenario. Imagine that you're an ISV. And you've got a successful monolithic application in your own data center. Does that resonate with y'all? Something similar to what you were working on? Right now, currently And you've grown from something like 4 to 10 developers to maybe 400 developers and things are OK, they're evolving naturally. Weekend scaling is manual. The CPUs are idle during off-peak hours. They're doing their thing and you have to coordinate deployments with all these other teams and go through that build, test, and release cycle as a unit is that. Resonate with y'all, some of you, yeah, I'm seeing some head nods. And so that means that like if one if one part one developer's QA test fails then the whole thing stops. You gotta fix that piece, go through the whole entire process again with that monolith. So this is our starting point and oftentimes right after this slide you'll see something, a slide that says. Why not microservices? Right. Or why micro? Why not use microservices? Well, why not microservices? So at first, monoliths, they've got a lot of pros, right? So they're simple, they're not overengineered. Um, you can, you can understand the complexity of that monolithic application. You understand it. Um, it has lower operational, uh, complexity and the resource efficient efficiency at small scale. But All of a sudden Your CTO comes or your CEO and what did they say? Where's the G AI, right? Who's going, who's who's got that question? Yeah, I've I've, I've heard that from a few folks. So this is what our customers are coming to us. They're saying, hey, we've got this monolithic application, but all of a sudden there's all this competitive pressure, and we can't integrate Jenny I into application, and we're being asked to, and we just don't move fast enough. And so this is where the monolith kind of breaks down and we start talking to our customers about adopting another architectural principle such as microservices. So what is the promise of microservices? Well, it's just that. It's the ability to be more agile, flexible, innovate faster. Um, have a little bit more autonomy so the development teams can make their own choices about programming languages, databases, the data, the API. The functional components become a little bit more scoped down and at the same time. Productivity can go up because of this architecture. So that's the promise of these loosely coupled microservices. So what do those look like? So microservices, they solve the limitations of monolithic applications through functional isolation. So you take that one application and you start to break it down into functional services, many, many more than one functional service. Each of those functional services are independently built and can scale independently of other components, right? Those functional independent components are organized around individual teams. So now you have a team that takes ownership of that functional service and another team that takes ownership of that other the next functional service, and they are, they have the ability to go at their own pace, pick their own technologies if it's a well functioning organization, and you can start to. Um, organize your team structure to best support the different services and their needs. So that means you can also harness individual skills that are maybe focused and really good at specific types of services and not have them scattered across the monolithic application necessarily. And on top of that, Microservices play really well with elastic infrastructure, which, you know, we're at an AWS conference is what we're all about. And once you get more of these functional services, you can start to reuse those patterns and you can have some standardization about what those services look like, so those API contracts between them. are cohesive, comprehensive, and Teams are able to communicate better. So a microservice is a collection of APIs, compute, and storage for that specific service. All right So you're buying into microservices, right? We're on the early stage of this journey that we're gonna go on for the next uh 54 minutes and so let's. OK, cool. I love microservices. How do I get there? So there's lots of different ways to start tackling your monolithic application and break them down into microservices. One pattern that we see a lot of our customers use is the strangler fig pattern, and this is how you, you know, you don't wanna just jump in and start hacking away at your monolithic application. You wanna be prudent and understand the risks and, and be, um. You know, pointed at what you wanna break out first and so the strangler fig pattern is a, uh, well it's a pattern that allows you to take that monolithic application, identify the services that you wanna start breaking apart and taking them out while leaving the rest of the monolithic application alone and you do it over time at your own pace and as you know services come online. And you get more familiar with the microservices architecture, you can increase that pace and start breaking down the monolith over time. Cool, so let's see what our new architecture looks like we've been doing this for a little bit. And so our molith application we've been breaking it down we still have our web front end we have our identity service, some shared database services on our corporate data center, but we've now started breaking out microservices onto EC2 instances. So now we have payment service, metric service, billing service, all these other services as EC2 instances in AWS Cloud. Cool. Things are going great. But we just have 3 services here and over time you're gonna get a web of microservices and all of a sudden, wait a second, I've got hundreds of EC2 instances all with different configurations, all with different deployment pipelines, no standardization and. Starting to lose control over the complexity and how do I manage that? Well, As I'm a specialist solution architect focused on containers. That's where why containers comes in, right? So containers came to the stage to kind of aimed at exactly this issue, right? The issue of, hey, it works on my developer laptop in my microservices team, but we throw it over the fence, put it into EC2 instance, and the environment's not the same, and I, there's no service discovery, there's no isolation, resources are over the place. It's difficult to comprehend what these different teams need. And how do you orchestrate and manage all these services, so containers allow you to have a. Uh, environment and a consistent, a consistent environment, the dependencies within the application, uh, definition, and you create a container image and that's owned by that team or multiple container images owned by that microservices team and you get that opera, you know, reduced operational, uh, converting. You have a consistent environment from the developer's environment to production. And it increases the speed and of an ease of testing and iterating on those microservices, but now you have a bunch of containers. Seems like you have the same kind of problem again, right? Now instead of hundreds of EC2 instances, you have thousands of containers. You still need to figure out what service like service discovery, you need to understand load balancing. You need to figure out the kind of platform capabilities that you need to operate thousands or hundreds or hundreds or thousands of containers in production on AWS. So that's where Kubernetes came onto the scene. Um, primarily to orchestrate and manage containers at scale, right, so the the Kubernetes open source project allows, creates a control plane that schedules and orchestrates multiple containers at scale, and there's other sessions here, um, way on the other spectrum of our track and containers track. There's an INV 500 session and a, uh, EKS under the hood session which goes very deep into the massive scale that we can achieve today with, uh, Kubernetes. But initially this is what it was for, right? Managing all these containers at scale and it provides out of the box service discovery, load balancing, auto scaling, rolling deployments, all this good stuff, but. Managing just open source Kubernetes is complex to get all those goodies. The Kubernes control plane has a lot. It has some complexity to it, and a lot of the complexity is undifferentiated heavy lifting that you shouldn't need to do. It's maintaining that control plane and nowadays with EKS auto mode, which Isaac will go into a little bit later in this presentation. Uh, well, you with EKS you can manage the, the nodes and the compute as well, but I'm getting a little bit ahead of myself. So you've used open source Kubernetes, but you're now managing this, uh, control plane can get complicated, complex very quickly, a little bit hairy at times. So that's where ECS comes in. So Elastic Kuberneti service is our service for a managed Kubernetes environment. And we focus, we help you focus on building, running, and scaling your workloads on a production ready Kubernetes control plane and cluster. So with EKS you can accelerate that innovation because you don't have to worry about the control plane, you don't have to worry about all the complexity of managing the Kubernetes platform. You can optimize cost and performance, especially with tools like Carpenter and all the goodness in the open source community. You can enhance availability, scalability, and reliability with all the well architected best princi best practices from AWS built into EKS. And then on top of that you can run Kubernetes with EKS in any environment, anywhere. So the key features are runs in any environment, you get a fully managed cluster including automatic updates with auto mode. And with EKS you get, uh, automatic updates of the, uh, control plane. You have native AWS integration. So thinking about database services, IAM, security groups. The list goes on. Isaac will go through it in more detail, and it's Kubernetes compliant. So you get all the goodness of the Cloud Native Compute Foundation open source ecosystem that's built around the Kubernetes open source project. Makes sense. Cool. So let's put it all back together. Let's go back to our environment. So we bought into microservices. We've been breaking down the monolith. Now we have an EKS cluster with all of our microservices managed on managed instances. Awesome. But one more thing, we still have those resources in the data center. Maybe there are some services still in your data center, um, that you are just not gonna be able to move anytime soon. Maybe they are related to uh data availability or compliance or some other uh regulation that you need to to meet. That's where EKS Hybrid Notes comes into play. So this is a feature we launched last year at Reinvent. And it allows you as a customer to hook up an Amazon EKS cluster to your VMware nodes or other on-premise nodes, uh, in your data center and take advantage of the EKS goodness in the cloud and the compute resources and other resources that you already purchased and have available in your data center. So let's take hybrid nodes and put it into our architecture. So now we have an awesome single tenant microservices based ECS auto mode in a bit with hybrid nodes. Environment where you have your microservices running on manage instances and you have your on premise microservices still running in your data center using hybrid nodes and that connectivity. Awesome, so this was a really quick journey, but we're now in microservices world, cool. So, things start to get better, right? You're taking advantage of the microservices architecture. You've got this well-oiled platform development teams are accelerating. You're starting to make progress towards innovating and, um, being more competitive with your competitors. And things start to grow. And with that awesomeness in your application you're getting more customers. Awesome, more customers is always great, right? But with that growth, growth, you start to have to manage all that operations of managing multiple customers customers in that single tenant kind of architecture. So are you gonna spin up a new cluster for each customer? Probably. Isaac, can you help me figure out how to make sure that I'm not losing my head in operational efficiency with all these customers that I'm seeing on my new microservices architecture? Abs absolutely, absolutely. Just can you give me a thumbs up if you can hear me? Alright, good, this is my first silent session ever, so this is good. All right, so. When Normal got us to, we were breaking down this monolith into microservices. We now have a tenant, uh, on a single cluster, and inside of that tenant we have a bunch of individual microservices running. And as Nirma was explaining, We started to gain success. We're moving faster. We're shipping features faster because we broke down this monolith and allows our teams to run independently. And soon enough, we get some more customers. And we get a few more customers and then we get a few more customers and now we are running individual clusters with a single tenant on each cluster to make sure we have that isolation. The sales teams are excited because every time they're selling, uh, these deals they're cheering it on, but you who have to manage these pets, right, are just thinking about what you're gonna have to deal with at 2 a.m. waking up having to manage individual clusters for individual tenants. Now from a compliance and a security point of view, this looks very advantageous. They're gonna love this. A whole bunch of isolation. Accept that your CFO is not gonna love it because it's extremely inefficient, and you, as the people who have to manage this, these pets. Also have to deal with it every single time we sell a new deal, it's it's gonna be more work for you. But you have to ask yourself, what, why now? Like why are you all sitting here? Why am I talking about this now? This isn't isn't that new. But what has changed over the last 5-6 years is that we moved into the remote first economy, right? Everybody is working, or at least partially working from home, um, and our expectations as consumers has also changed when we go online and we sign up for Netflix, we don't wait weeks for them to give us an account as we consume more cloud services, we expect that all to be instantaneous. Right? And so do your customers, right? Your customers are also expecting this. The competitive landscape has also changed. Your boards, your investors are all expecting SAS, uh, returns in multiples, right? The ability to provision your, your tenants instantaneously so that you can start billing instantaneously. And lastly, cloud native technology now is extremely mature. Nobody thinks of it as a risk. It is an extremely safe bet. In fact, actually it's probably the opposite. If you're, if you're going into the data center, someone's gonna ask you why are we moving back into the data center. So how do we get to to a multi-tenant architecture? Something that allows us to scale, something that allows us to not scream every time the sales team makes a new deal, right? Now while this is relatively simple architecture diagram here, there is a lot of complexity that we'll get into in the next couple in the next few slides. On the left hand side we have our, uh, control plane just like Kuberneti's has a control plane. We have to think about building our own control plane as well for ISV or SAS oriented businesses. We have onboarding, identity, metrics, tenant provisioning all happening from a centralized place. We have to move away from pipelines and being able to just provision. Customers from pipeline, which is automated-ish, somebody still has to push that button and has to coordinate a bunch of different pipelines in order to get a tenant, uh, up and running. So instead we move into something that's more automated using services using APIs to get that going. On the right hand side we have our application plan. Now you'll notice we're moving into more multi-tenancy. Of course there are contracts. There are certain compliance there's certain things that a security team just won't allow us to do that we will still have a single tenant on a single cluster. I don't think we'll ever really get away from that, those uh types of requirements, but what we wanna do for ourselves and for the business, right, is make sure that we are using infrastructure efficiently. We wanna make sure that our lives are good that we are, uh, we wake up and we're able to automate things and we're not just rushing around taking care of individual clusters so how do we get here? How do we get into this particular architecture? So that's what we're gonna get into now. So we have all of our clusters. And the first thing that we wanna do is create name space isola name name space based isolation. It helps us reduce cluster count, creates a logical boundary around uh tenants, centralizes some of our operations purely by using name spaces, right? And so as we use namesakes, we'll be able to reduce the number of clusters that we use and some clusters, uh, some tenants will be able to share the same cluster. And while from a CTO or an engineer perspective you're screaming yay, right, looks really pretty on an architecture diagram, what nobody tells you is that all Kubernetti's networking by default is a flat network architecture. Pods and name spaces kind of mean nothing at this level. We haven't done anything to secure it down, so a pod from tenant 1 can go reach out to a pod in tenant 2, and there's nothing really to block it. So what do we do? We have our cluster. We have our name spaces, and this is what it could look like if we don't lock it down. Somebody from a basic tier name space can reach out to a tenant 1 and tenant 2, and we wanna be able to block that, and we do so building digital fences, right? Building network policies. Now this policy that you're looking at here. Is saying 10 at 1. Has a has a potter with a role of type analytics or a potter role of type subscriber can reach out and talk to the back end, the name space back end, right? And by doing so we start creating these digital fences limiting the amount of network flow that goes through our systems and our pods and our, our name spaces, right? And we've created our first little digital fence. The next thing that we're gonna wanna do is we're gonna create gateways or ingresses and make sure that we have path-based routing to our tenants. We don't want to just have pods communicate with each other. We wanna make sure that we are controlling it through a gateway. And last, if you're not using uh AWSCNI, we're gonna do that as well. We're gonna make sure that we have CNI, um, the, the AWSCNI enabled and installed, and what that allows us to do is create I, uh, um, ENIs for each individual pod. What this means is that you're able to use the security groups, all of your network policies that you have for the rest of your AWS constructs or resources, and we can treat pods in the same way. So we have secure we can now start enforcing security groups as well. And in doing so, we've now created a full digital fence. We've turned this open house more into a gated community, and now we're able to control which tenants can communicate with other tenants. All right, so everything looks pretty good up to now. Right? However, we still have to deal with the noisy neighbor problem. Which I'm sure you've all had to deal with at some point or at least heard of it. It sounds pretty scary at first when you, when you say, oh my gosh, we're gonna put a whole bunch of tenants on the same cluster. There's ways to help us deal with this, right? Because tenant one can potentially have a runaway workload, consume all of the CPU, right? Inside of pool one, because you have a lot of different tenants running inside of that pool, they may consume all of the memory on this box. Leaving 10 at 1 starved, right? And so the way that we normally deal with that sometimes is we overprovision, but overprovisioning is gonna make the CFO very, very, uh, sad because that's where those bills start to kind of mount up purely because we wanna make sure nothing bad happens. So there has to be a balance between this O provisioning and making sure we have the right resources at the right time. So the first thing that we're gonna do is is we're gonna install resource quotas which are name space based. If you think about a name space sort of like an apartment, right? Uh, the resource quotas are, what are all the things that this apartment could possibly use? What are the resources that it can consume? Here, pretty simple limitation, we're saying this name space or this apartment can only use 8 CPUs, 16 gigabytes of memory, have 20 pods running at most, right? The next thing that we're gonna wanna look at to make sure that we have the appropriate resources running on our uh on our machine is make sure that we install limit ranges. So in this apartment analogy, right, if resource quotas are about the entire apartment, pods are think about them as individual rooms, and we can set limit ranges for these individual rooms to make sure that each individual pod does not starve out the machine. Here we have a max CPU of 2 and then memory limitation as well, right? It allows us to apply it to a type container. So Actually, let me go back one real quick. So we have this figured out we have our limit ranges we have our resource quotas, we're able to make sure that one individual tenant isn't starving out the other tenants. And we're still managing these, we still have to manage the EC2 nodes. That doesn't go away. We still have to provide it infrastructure, right? We are limiting the tenant, but what if the tenant needs more? Like, actually needs more, right? We'll have to provision that ourselves and the way that most of our customers start, they start with managed node groups and that works up to a certain point, but the challenge with managed node groups, right, is that you have to specify a particular instance type. Right, and as you start to grow, you have to make sure that you're still doing capacity planning, you're still estimating what this tenant is gonna use in the future. What if you need different types of uh uh instances? What if you have different types of workloads running on those machines? How will you set that up? You have to create more and more managed node groups, as well as as you get more tenants to continue to have this type of isolation, you'll need even more managed node groups. And now we went from managing a bunch of clusters to managing a bunch of tenant node groups. So the way that we solve that is through auto mode. How many of you, have, have you all heard of Auto mode? Yes? Some of you, how many are you using auto mode? OK, good, good. So what Automode allows us to do is have a Kubernete-centric view of managing our infrastructure or our nodes underneath the covers. While giving us the flexibility. We can now set up a a um. A manifest that provides us flexibility in the types of infrastructure that we use, it's automatically scales because it is Kubernete's native. We can now scale based on the workload types, right? We can look at what's happening inside of that cluster, look at the, the, the metrics and scale based on those metrics. All of this is part of EKS right now. It's a feature that's part of EKS. It's built into EKS and we'll manage it for you. So let's look a little bit as to what um these manifests start to look like. Part of auto mode is, is, is, of course, managing carpenter. Have you all heard of carpenter? Yeah? OK, that's good. So underneath the covers, part of it is carpenter, and you can see on the left hand side for tenant 1. We have 2 types of instances that we can use now obviously we are limited to 2, but you could put other values in there as well, other instance types and mix and match. Now for tenant one because they signed a particular contract with us and we have a particular SLA with them, we're gonna say, you know what, actually we only want spot instances from them, right? And if the availability is there, great, if not, it's, it's not there that's part of their contract. Right? And we can limit the number of CPU and memory again. On the right-hand side, I think that's the right-hand side for you, yeah. Um, we can have bigger instances because they signed a bigger contract. We have a different SLA with them. They have more, they need more resources. And as you can see in the values there we have spotting on demand because their SLA is different. The SLA requires us to provide resources to them. And so what Auto mode will do Is provision these resources for us and as our tenants grow or shrink or need more resources. Auto mode will take care of that for you. Right? And it may put tenant one on an entirely, on it on its own node. And isolated completely from everybody else. All right. Let's go back a little bit here, so. We've gone through networking, we've gone through resource quotas, we have auto mode running helping us scale up and down. But we still need access to AWS resources, things like S3, RDS. Dynamo, right? Nothing runs on its own inside of Amazon. It's always dependent on other resources. And so the way that we get to this next is customers start managing secrets and AWS credentials. They may be storing them as environment variables or hard coding them directly into the container or providing them as config maps or secrets. And what we start to see is this proliferation of AWS hard coded AWS keys everywhere. Long lasting AWS keys and it becomes hard to manage compliance and security because ultimately what we want to be able to do is to rotate them frequently. But of course if we're doing this all manually, what happens next is we forget to clean those out or to rotate them. And sometimes, right, we see a lot of the key leakage. Somebody, if somebody escapes out of tenant one's pod, they're able to grab secrets and reach out to a different bucket. So the way that we deal with this inside of EKS is called EKS pod identity. Pod identity is AWS native uh IM policies for pods themselves, sort of equivalent to the AWSCNI where we're applying uh security groups and AWS native constructs for for networking. It's the same thing here for IAM. I wanna make sure that you're using pot identity because it does allow you to continue to have this separation of duties which again is really important for SAS companies and compliance. Right, we're able to manage what these pods can access not in a Kubernetes native way but through AWS I roles and, or sorry, IM roles and policies, and this allows you to continue to use the exact same tooling and infrastructure and everything you have set up to manage those policies to make sure that they're intact. In a Kuber through um and still map those to Kuberneti. So what we're doing here is we're mapping an AWS role and policy to a service account and we'll talk into talk about what that looks like. Additionally, it allows you to have auditability through the same tools that you audit everything else inside of AWS and allows you to scale more efficiently through the teams in the ways that you're already managing identity across all of your other AWS resources. So let's put one of these things together. It's a pretty simple policy. All we're saying here is tenant one, giving something or somebody access to tenant one at bucket. Pretty simple, get put, delete list. The next thing that we're going to want to do. It's create a service account inside of Kubernetes. Right? And then we map that service account. To the pod and what this allows us to do is that allows the pod to have access to that bucket that we had earlier here. And as we deploy this into the Kubernetes cluster we're following the exact same processes that we had before, the exact same security and compliance policies that we had before. Nothing has really changed there. And what's great about this, you notice that I didn't create an AWS key, I didn't put it into an environment variable, I didn't put it into a secrets manager. I didn't do any of that stuff. I don't have to. AWS is handling that for you and automatically rotating the keys behind the scenes. Great. And by doing this, What it'll do is make sure that tenant one does not have access to the wrong bucket. All right, So now let's get a little bit deeper into the internals. Remember we still have to get to to that control plane application plane, uh, architecture, but to do so we're gonna have to dig in a little bit deeper into Kubernetes, um, and as we do so we're gonna talk a little bit more about auditability here first and why this is important. So let's go through the steps. I know that the numbers are off, so we're gonna go through the numbers. I didn't miss it too, and the numbers will be off in a couple other slides as well, but we'll get through it, I promise. So first we start off with a DevOps engineer deploying into into Kubernetes. Behind the scenes it's just an API server. As we get into step 3 here, it stores it into SCD. And then the scheduler picks up the change. And create or puts the pod inside of this uh inside of the tenant name space in 5. And now the pod can communicate out that was created, can communicate out to RDS and S3. So as we get into compliance and auditing. Right, it's understanding what is happening inside of my system. What do we wanna understand here? We wanna understand what's happening at the Kubernetes level, who is making these changes? What changes were made, and what is communicating with what out into an AWS account. Right, so we wanna be able to make sure that we understand what's happening inside of these name spaces with these tenants and make sure that we have auditable proof across the chain, which is pretty difficult to do if you think about all of the pieces that are happening and all the things that are changing across the, the, the system. So we're gonna get into how we deal with deployments, system access, auditable proof, uh, and open cover for, uh, compliance enforcement. So first thing that we can do here is make sure that we are auditing the right logs out of Kubernetes. We can control this in many different ways, but here you can see that we have a request response just for any user or anything that is asking about pods. So we're literally logging the request and the response. So you can limit it to the things that you need to prove to your auditor, uh, to make sure that you're, you're doing the right things and you can prove who's actually making these changes or querying the system. The next thing we're gonna do is we're gonna talk about um OPA and and Cavern. Well, we'll talk about OPA today, but Cverna is a very similar system to talk about policy as code. So we're gonna go through the same workflow. DevE's engineer is submitting a manifest or making a change into the API server. What's actually happening behind the scenes is we're going to then go to the admission controller. The admission controller is going to let us know whether this is a valid document or manifest that can be submitted into the system from the right user at the right time and for the right tenant. And it does so by looking at OPA. We're going to look at an OPA rule here in a second. So it goes out to OPA and it says that's 3. And it says, Is this a valid manifest and submitted by the right person at the right time? Right? And if no, it'll reject it, it'll just say no, right? And these become auditable provable declarative rules that we can give to compliance officers to make sure that we're doing the right things. Same for security. Security loves this. So if it's a yes, it follows the same flow. We're going to go 4567. We're back out into the same place. As we've had before, one other change though, we have number 8, which is monitoring this through cloud trail. And so now as we monitor what's happening at the API server, we can log all of the changes that are happening from which users and why same from OA, we can see what was rejected and why. Or approved and why what rule sets are inside of OPA that allows us to um make sure that those manifests are accepted or rejected we now have provability across the entire system. This is what a compliance or policy as code looks like. Is anybody here using OPA or Kverna for policy? Just a few? OK. What this is saying is here is saying, if Isaac were to submit a manifest, right, the first thing, uh, a deployment object, let's just say, or a deployment manifest, it's gonna look At the pod Make sure if it has the the label classification PHI right? Privacy information. We wanna make sure that it also has the rule not run as run as not start run as non route is there, right? We wanna make sure that that's there because that's very sensitive private information. And if it's not there, it's gonna reject it. Right? And this helps us build trust into the system. It allows developers to move quicker, right? Because we know that as people who have set up the system that nobody can submit a manifest that will break our rules. Now this is a pretty simple rule, but you can imagine all the very complex rules that you can create. And with things like Open Converno you can actually integrate it into other systems inside of your organization. Right, so, uh, LA, LA, and all the other systems that you have there around users and groups, you can verify all of that, uh, at deploy time so nothing gets into the system. That shouldn't be in there. Right. All right, so now what do we have? We have compliance and auditing. We've dealt with the uh noisy neighbor problem we've dealt with networking, uh, we've now have logical name spaces, but what we find still. Is that tenant on boarding still takes weeks, months, sometimes, honestly I've I've seen years to on board a particular tenant. And uh we were talking to a few of you uh before we got started and uh somebody had mentioned we're still going through approvals I think you were mentioning that right to just get access to something. This happens frequently and actually repetitively. So if you need a new Kubernetes cluster, what sometimes it'll take 6 weeks to months. So what does it take so long? We see as the organization grows, there's more and more individual teams, more specialization. Somebody's the landing zone team. They'll have the IM policies. They'll create the accounts. And if you go ahead and ask them, you go, Hey, is everything automated? They'll say yes. All right, cool, and then you go to the uh OK, the networking team and you ask them, is everything automated? Yes, and you go through them and they'll they'll do their thing. And then it, for me it always begs the question is everybody is always so automated. Why does it take so long? if shouldn't automation be instant? And what ends up happening is what kills us is the coordination between teams. When we say we're automated, what we really mean is my stuff is automated. Right, but the way that you kick off that automation is through Jira tickets, ServiceNow tickets. That isn't automated. Somebody has to manually type that in. Somebody has to accept that ticket. Somebody has to actually execute the pipelines. So it's all of this coordination that happens that kills us, right? And that's what takes, uh, a really long time, uh, for onboarding. The other thing too, is it lacks version control. Some of those things aren't as automated as we want them to be, and there are a lot of manual steps, and there are a lot of kicking of the pipeline. Those become things like tribal knowledge. Hey, Normal, first kick off this pipeline, then this one, but if you do it the other way around, it'll create havoc, right? And when I, I'll tell Normal this, and then I'll leave the company, and then maybe Normal leaves the company. Where does that go? That's all in our heads. So it creates a lot of risk and just doesn't scale. You know, but there is a solution to this, and we see it inside of Kubernetes already. If you ever noticed with inside of Kubernetes when you let somebody deploy a manifest, what's actually happening behind the scenes, we already talked about it. We talked about how something like auto mode can create EC2 nodes. We talked about networking automatically being there. We talked about everything being managed, uh, through manifest creating ingresses. There's a lot of automation that happens already inside of Kubernetes that creates underlying infrastructure. And I've been, I've been using Kubernetes for, uh, close to 1112 years, and I remember when I first started using Kubernetes, nothing was automated. You wanted an EC2 instance, you had to create it yourself and attach it to the system yourself. All that managed were pods. But then over a period of time, right? Not just AWS but the community started creating more and more controllers. We have now controllers for ELBs, ALBs, things like Route 53, Secrets. We have managed node groups. I just told you about auto mode, right? We have AW the the the CNIs. Right? Oh, now we're creating more and more infrastructure. We can actually now with the the cluster API create Kubernetes clusters with Kubernetes. And when I first started with Kubernetes, I remember everybody saying this will never work. It will never be a generalized compute platform because it will never do things like stateful, uh, workloads. But look at us now. We have EBS volumes and many other ways to get attached, uh, attached storage. We're dealing with AI workloads, G AI workloads, uh, databases all run on Kubernetes. And so it begs the question. Why can't we manage more of our infrastructure in this way? Because if we manage more of our infrastructure in this way, what do we get? The same deployment system, the same auditability, the same networking, everything becomes the same, right? And what we see in our customer base is that we are moving from Kubernetes just as a container or the application plane for our, our pods and moving it into a platform. Because ultimately this is what you need to do to scale your uh multi-tenancy. Luckily for us, there's a great Kubernetes open source community that has been creating controllers. There's thousands upon thousands of controllers. I haven't listed them all out here. On the lower left hand side you see it's not much effort to run some of these controllers. Some of them will manage them for you actually, inside of auto mode, we do. As you get to the, but we may not have full coverage. As you get to the right, as we get into customer custom controllers and the operator SDK, it allows you to manage any anything. I had one customer ask me, can we manage satellites with this? Yes, uh, you can manage anything with an API, right? And it wouldn't be so crazy to think because I thought it was crazy too. At first I was like, why would you wanna do that? Well, they go, well, sometimes we lose connectivity with satellites, and we wanna be able to have that reconciliation loop, and we have hundreds of satellites up in the sky, and they all have different APIs and different versions. We would like to abstract that out from the people who have to operate and manage all of the different satellites. And that, and it like clicked for me. I was like, you know, I'd never thought about that before, but it conceptually it should work. And if you think about that and you just take satellites, the same analogy, and use that for Kuberneti's clusters or AWS resources, then it all becomes the same. Yeah, you can do that because sometimes you do lose connectivity, sometimes we do have network partitions, sometimes we may have rate limits, which I'm sure you've all dealt with. I know I've dealt with my fair share, right? So today we're gonna just focus on one, which is AWS Controllers for Kubernetes called ACK. Just a quick show of hands, who's seen, who, who's heard of this? A fair amount. OK. So what ACK is, it's an open source project. Mostly run by by AWS that is controllers for a lot of our resources, right? Our most popular AWS resources and services, RDS, Dynamo, S3 IM policies are all there, which means you can start actually managing your AWS resources through Kubernetes in the same way. This is what a pretty simple manifest looks like for us 3. Right, we have our tenant tenant one, we can create a bucket and so now when we're provisioning our tenant. I don't have to go to that other team. I don't have to go to another pipeline. I don't have to go do all the other things. I'm just provisioning it along with my name space and all of the other resources that tenant needs all at one go in the same pipeline and now we, you see how we can get to like true automation because it's all going through the exact same thing. We just highlighted here you can start setting tags, right? So you, you can still see where these resources are going. You can have public access blocks. You can see where this who. To whom this resource belongs to. All right, so let's put this together now now we're gonna get into our control plane and our application plane architecture. This is how we get here. All right, this is what I told you I messed up the numbers, so I added a few things here, but I don't have the numbers, so we're gonna ignore the numbers for a little bit. So on the left hand side we have this get pushed developers are no longer actually submitting manifests directly into the cluster. We're gonna do this all through Git. Git will store that and have a web hook into Argo. Argo is a controller that is is a GitOps controller, the, the most popular one. I'm sure many of you heard it, probably many of you are using it. The thing that's different here is we're not trying to use this to provision it uh to deploy into a name space we're using it to actually create AWS infrastructure. Um, from Argo goes uh. It'll submit it into the API server. And we'll follow the same flow, right? We still have it going to OPA. And if it's not a valid manifest, it'll reject it. So let's go back actually to this manifest here. Some of the rules that we can create with Opus saying do not give me a manifest that doesn't have a key for tenant, and we can reject that. We can also say if it doesn't have the proper block, uh, public access block rules, reject it. We can give it values that say actually only accept these values or these types or this type of rejects. So now we have control in the same way that we control our deployments, we can control. Our AWS resources. Now the difference here, you notice in this tenant name space, I'm not deploying a pod, I'm deploying a CRD. And ACK is listening to that, listening for, hey, what's a new S3 bucket, a new RDS or just changes into resources that already exist. And then ACK will reach out to the AWSI uh AWS API and create those resources for us. You can still be deploying into remote EKS clusters, which become your application plane. So on the right hand side. We have that uh application plan on the left hand side, look what we have now. You notice there's no workloads running in there. Right, we haven't deployed a workload into the left hand side that becomes our control plane. That becomes the center of everything that we do with our operations. This is where we can have other applications like billing. Metrics metering we start employing more and more resources here or centralized uh resources here. But wait, where's the Gen AI? Promise I won't talk too much about Gen AI, but all of our leadership, right, all of your leadership is still asking about Gen AI, right? Normal was talking about this earlier. And We've now have microservices. We now have this control plane. We have our application plan, right? How do we start thinking about enabling Gen AI because that's ultimately what our leadership is looking for. But nothing prevents us actually from creating bedrock or Sagemaker resources in the exact same way. We have Bedrock and StageMaker uh controllers inside of ACK. And so now not only can we provision our S3 buckets, our RDS for a particular tenant, we can start provisioning resources that we need inside of Sagemaker or Bedrock for that tenant as well. All through the exact same pipeline, same compliance, same auditing. Same rules. Let's tie it in a little bit more and bring hybrid nodes back into it because maybe you already have GPU resources running in your data center or you already have bought them and you wanna use them and leverage them. Right, while still leveraging the exact same infrastructure. Right, the exact same control plan. You notice we haven't changed anything on that left hand side in order to get our application plan to have different resources and our tenants to be able to access those different resources. All of it could be managed and controlled through EKS itself, whether it's in the data center, whether it's a GPU, uh, uh, in the data center, whether it's a hybrid node in the data center, whether it's AWS resources. We talked about how we can manage AWS networking through Kubernetes. We can manage, uh, IM policies. All through the same interface which simplifies your operations, you know, going back to why does it take so long, it's all that coordination. But when you have a system like this running, way less coordination. The rules are declaratively written. People trust the system and reduces the amount of back and forth you have to do through Jira tickets, through meetings. All of that is written down in code and in your Git repo. And this is where we ended up. Right? We talked a little bit about how that left hand side started to have that and we didn't go into every single box here. But we gave you the foundations of what you can build upon. On the right hand side, we still have our application plan. Super simplified, of course, right, we didn't talk about GPUs on the right hand side or the other resources inside of AWS that you can provision, but you have this now. You have the foundations of what to build upon, and we see many, many customers as they start breaking down monoliths coming in from the data center needing AWS resources while still leveraging EKS and Kubernetes getting into this particular architecture because that allows them to scale. At the end of the day what we're all trying to do is scale. We're not getting more headcount to help us grow, or if we do, it's not gonna be a linear linear investment. Right? We need to figure out how to scale ourselves through automation, and this is the way that our customers are going when they're leveraging EKS. So I'd like to invite Normal back on here, close it out. Thank you. Awesome. Just a sec. So what do you think? We went through that journey, we, we started with our monolithic application and now we have a multi-tenant multi-cluster highly available with all the services that you can imagine from AWS and your ability to adapt to those new requests from your developers. Now this is just a kind of. Uh, even though we did go into a lot of depth, this is kind of just a teaser of all the capabilities that EKS can offer and a starting point for your journey to adopt all these new features, um, one of which was just launched while we were sitting here. So this is super hot fresh news, um, we now have a new feature called ECS capabilities which is, and I think I can go back to kind of this slide, which is our ECS. Managed controllers for Argo, ACK, and KRO, which Kuberne Kuberne's Resource Orchestration, which we didn't really mention here but is a component, a potential component of this, um, that's new. So check that out. Um, there's a session tomorrow at, I believe, 1 o'clock CNS 378, where they're gonna go into details of EKS capabilities. So that's like super fresh. They were messaging me while while we're all sitting here so y'all are the first to know. So check that out so it even gets better. Isaac mentioned these things that you have to put into your cluster to manage and do this kind of automation deployment. Well, now that's even easier because we can manage those controllers for you and make it streamlined for you to be able to implement the architectures that you need to support your customers. So in addition to that there's some other sessions this week. Um, we have a very amazing EKS track this year with lots of different sessions. This is just a sampling of some of the ones that you might be interested in. In addition to that, we have a hybrid nodes workshop as well. I don't have the details in here, but you can find out all the details here. So get out your phones. Um, here are a lot of resources that you can use to get started today, including. Labs, uh, workshops for auto mode, for hybrid nodes, um, more information on ECAS blueprints, so you don't have to figure this stuff out for, for yourself. You can use pre-made blueprints to get started and if you're just getting started with ECS, don't know all many of the words that we said today such as containers, Kubernetes. We also have an ECS digital badge which you can use to get started and understand the basics. So we have all that there. Um, here's the session resources for this session specifically, but it's mostly the same thing as this one, so I'm just gonna leave this up for a second, um. And then, yeah, we're good. Awesome. So ECA capabilities just launched, check that out. CNS 378 tomorrow. And with that, I'd like you to take the survey, please. Thank you.