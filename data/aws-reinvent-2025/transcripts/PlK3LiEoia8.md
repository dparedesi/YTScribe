---
video_id: PlK3LiEoia8
video_url: https://www.youtube.com/watch?v=PlK3LiEoia8
is_generated: False
is_translatable: True
summary: "This session explores the strategic evolution of Informatica's Intelligent Data Management Cloud (IDMC) towards \"agentic AI\" and demonstrates how enterprises can leverage this shift to build autonomous workflows, featuring a detailed customer case study from Citizens Bank. Gopi Sankaran, Vice President for Strategic Ecosystems at Informatica, introduces the next phase of their native AI engine, \"Claire,\" moving beyond earlier iterations of chatbots and co-pilots to fully autonomous agents capable of executing complex data management tasks without human intervention. He emphasizes that while the \"random walk\" of AI at re:Invent has moved from RAG to GPTs, the core requirement remains a trusted data foundation. A key announcement is the integration with Amazon Bedrock Agent Core via the Model Context Protocol (MCP). This integration allows developers to expose Informatica's robust data management capabilities—such as data quality checks, master data management (MDM), and address verification—as secure tools that AI agents can invoke. Rajiv Srinivasan, Director of Technical Alliances, provides a technical deep dive into this \"agentic blueprint,\" explaining how the architecture uses a planner, orchestrator, and executor model to handle complex user intents. The presentation includes a live demonstration of these capabilities in action, showcasing an AI agent designed to schedule hardware installation appointments. The demo highlights how the agent autonomously interacts with disparate enterprise systems: it first authenticates the user via multi-factor authentication, retrieves a \"golden record\" of the customer's profile from Informatica MDM to understand their relationship with the organization, and then verifies shipping addresses using real-time data quality rules. When the user provides an incorrect address, the agent automatically standardizes and corrects it before finalizing the schedule, significantly reducing \"first call response time\" and logistical errors. Crucially, the system provides full explainability and audit trails for every step, a mandatory feature for regulated industries. The session concludes with Anand Vijay, Unit CIO at Citizens Bank, who shares the bank's visionary approach to adopting generative AI within a highly regulated financial environment. Vijay outlines their \"platform engineering strategy,\" which focuses on the metric of \"Time to Insights\" (TTI)—reducing the latency between a data event and actionable intelligence. He details how Citizens Bank avoids the common pitfall of creating \"tech debt\" by building new AI silos; instead, they follow a philosophy of \"increment, augment, and complement.\" By leveraging their existing 100% public cloud footprint and deep investment in Informatica for customer and reference data governance, they have seamlessly extended their architecture to support agentic workflows. Vijay explains that their transition from predictive ML to generative AI did not require a \"rip and replace\" of their infrastructure but rather the addition of vector stores and embedding models to their established pipelines, allowing them to rapidly deploy secure, compliant AI solutions that enhance experiences for customers, partners, and engineers alike."
keywords: Informatica, Agentic AI, Amazon Bedrock, Model Context Protocol, Citizens Bank
---

Thank you for going through that. But first, uh, I wanna appreciate everyone for taking your time to come into our session. Uh, today, we have a very exciting session which will have almost every buzzword bingo you can think of for An AWS reinvent which talks about agentic AI. You can see that already in our title, but, uh, let me introduce myself. So I'm Gopi Sankaran. I'm the vice president for strategic ecosystems at Informatica. Today I'm going to be joined by my colleague Rajiv Shrinivasan. He is a director of technical alliances and followed by that we have an uh exciting new person here who is gonna join our presentation because we have been doing this for every year for quite some time and that will be Anand Vijay from Citizens Bank. He is a customer of Informatica and AWS and one of your peers, and he will also share some of his AI journey, agentic journey, which he was able to accomplish with both AWS and Informatica. Um, quick, um, Uh Uh, quick housekeeping activities as in, uh, it is gonna be a pretty standard kind of a presentation. We will talk about what some slides. There will be demos. There are architecture slides which will be coming up, and then we will do a panel, and there will be Q&A. So please do stick for Q&A, and if you have any questions, we are, we are here to answer. So with that, let me start to. How many of you guys. are using Informatica or have gone through the Informatica journey. Uh, he, OK, there are a few of you, um, so we have not been following Informatica. Uh, we have been in the business of data management, analytics, ETL ELT data governance, uh, in a lot of spaces, everything on the data foundation space. And, um, recently we were acquired by Salesforce, so we're moving forward, we are Informatica from Salesforce. It's been two weeks into our Salesforce journey. And, uh, Let me first introduce about Informatica in the context of what we do, which is solving critical data challenges. Some of the problem statements we address, it is around how do you find the data in the enterprise, discovery of the data, how when you find the data, where do you go and how do you go and do the ingestion of the data, the integration of the data, and once when you integrate the data. How can you make or ensure that the data is usable? This is the million dollar problem today which is around data quality, which is not just about assessing the data quality but also cleansing your data, um, and then data access management. Can you securely govern, uh, securely democratize the data so that the right people have the right access to all the data you have. Once when you have all the data, can you get a 360 degree view of the data, whether it in any domain, whether it is product, supplier, customer, and get that golden record of truth. Um, if you get all these data, can you govern this data across the enterprise, whether it is in AWS, whether it is in a multi-cloud environment, or in a hybrid environment, any sort of enterprise environment, can you govern these data? These are a critical problem statements that Informatica's platform solves. And these are also the feeder into many of your data analytics and AI initiatives. We live in the world of data, we breathe data, we address data management challenges. That is our core focus and it has always been and it will always be. So how do we do that? Our platform is the intelligent Data Management cloud. It is cloud native built on uh AWS and also other hyperscales. Uh, it is a completely a microservices-based platform. It is AI powered not just from a namesake. We actually, based on the scale what we operate, we are huge adopters of MLNgen AI routines. Um, we support 300+ enterprise sources and using our high scale, high performance pipelines, and, uh, we are available globally. If you, when I talk about scale scale, the when we were a public company that is before the Salesforce acquisition, the last announcements we made, which was around 3 to 4 months back, it was around, uh, from a scale standpoint we do. Around 143 trillion plus cloud transactions per month that gives you a scale of what we do and by the way, that scale does not include the 360 degree glass in the last slide which I showed, which is a master data management that is just purely on the data integration data governance in all those world we do our our scale is today at 143 trillion plus cloud transactions. You will hear a much higher number in a couple of hours. And uh that is. Uh, because at the Salesforce earning support. So Before we start to discuss more into this, I do wanted to call out around the evolution of Informaticus IDMC, the Intelligent Data Management Club, that's the acronym we use in the era of a generative AI. Actually, the funny thing is that this also flows really well into the random walk which we took over the last 3 to 4 years with AWS at reinvent. I remember Rajiv and I in 2023, when, uh, we came here and presented in the same Mandalay Bay in one of the oceanside, uh, rooms was around racks. That time it was all about racks that we will augment a generation. And at that point we came to talk about one of the core part of our platform which is the no code low code pipeline framework. It was all about connectivity to vector databases and how do you bring high quality trusted context so that you can build trusted RAG applications. That was the thing that was only 2 years back. It was all about RA. Then last year when we came, it was all about the year of the GPTs and co-pilots. The native AI engine of Informatica is called as Claire, with the AI being the different color you'll see here, um, and it is we introduced, I think 3 years back our first GPT and co-pilot as part of our platform, but when we came to reinvent it was around, it was the first time we launched something called this here is a blueprint to develop Gen AI applications in the enterprise using Informatica for Amazon Bedrock. We were one of the early adopters, probably the launch partners, and we have been starting to work with them very closely from the early days. Again, if you see into it what is a value proposition is we still our core focus has always been around what we do around building that trusted context and the trusted data foundation so that you can build your GAA applications based on the data foundations. So today, this year, The next stage of that evolution and there are 3 things I would definitely call out in the next stage of the evolution. The first one is around Claire. Claire, as our native AI engine, earlier this year, we announced our purpose-built data management agents. The next one you will also uh look here will be around the evolution of Informatica's blueprint for building an agentic enterprise. So we have gone through the rag world, we have gone through GPT co-pilots. Now is the world of the agentic workflows. And the third one you will hear in our presentation and also in the demo and everything you will see is around MCP. We adopted MCP early in the cycle because through MCP now you can get the power of Informatica data management cloud through the MCP protocol so that if you have to do anything around data quality or address verification or anything like. That the MCV protocol helps for you to use um for your applications by the way, recently we also got the Gen AI and the gente care specializations competence all the badges that AWS provides we get it because we have been deep partners with them. So that, let me quickly go through, even though the uh presentation will be around agentic workflows, I'll give a quick introduction around what is Claire and around Claire co-pilot. The Clare GPT and uh Clare agents, so the co-pilot is the context intelligence that helps to build the artifacts whether it is for quality rules or um when you have technical metadata, how does it align with the business glossary and things like that. It is targeted for a certain persona, mostly the data engineers, ETL developers, ELT developers. You can use the co-pilot to accelerate your development of the low code, no code pipelines and other data management tasks. There is also a GPT version, the next canvas. So the way how we look at it is like from a user experience standpoint there is a co-pilot for certain personas, then the GPT from a conversational standpoint again how you can go and talk with your data in your complex enterprise data ecosystems, and this is everything with respect to whether you want to do like a. Um, data management artifacts, data discovery, where do I find my data earlier I talked about some of the critical challenges you can use natural languages like that as part of your problem statement in your enterprise, in your context to get the right responses. Again earlier this year this was a big leap what we did here one of the earliest guys to uh um launch our agents which is now used in many of the enterprises but these are purpose built agencies are purpose built around like a data discovery agent, ETL agent, data lineage agent, product experience agent, um, and of course the data quality agent which is one of our highly adopted in the preview and it should be going, um, GA. You will be seeing in weeks as part of our other announcements in other forums. Again this is like complete automation end to end data management goals without human intervention and uh this is uh one of the next big leap which we did as part of evolution of our purpose built agents so the way to look at from Informatica standpoint is we enable customers partners to go build their own agents we also support them for the data management tasks with our purpose built agents and again. If you think of it like when you do uh at the scale of 143 trillion plus cloud transactions and you're looking at data quality agents, this is like a very targeted use case for many in the enterprise how you can go assess the quality of your warehouses or your analytics engines and then drive the cleansing of it. So with that, let me actually going to invite uh Rajiv who is gonna go through the evolution what we call that the agentic AI blueprint for AWS. Thank you, Gobi. Hello everyone. So what Gopi covered so far was around Claire, which is our AI engine to improve the productivity of data professionals on our platform. So what I'm gonna cover next is as you're building your own agents on AWS, how can you leverage the power of our platform and the metadata system of intelligence to power your agents and agenttic workflow. I'm Rajeev Srinivasan. I'm the Director of Technical Alliance. Before I start going into the architecture and all the technical details, we always start with what are our customer requirements specifically around what are our enterprise customer requirements, right? When you're building your AI agents within an enterprise, the first and the primary uh. Uh, part of getting the data is to get the information from within the enterprise, right? This is unlike the chat GPTs of the world that you see. This is primarily around getting the information from various parts of the enterprise where the data may be fragmented across various depart different, uh, departments and organizations. The second part to it is. Able to get not just the data but the business context so that these large language models or these LLMs that you use within your AI applications are able to reason better rather than having to guess what that data is or trying to interpret what that data is and what it what that data means in terms of the enterprise. And one common theme that we see talking to our customers is that data is not always of high quality, right? Data they always know all data is not the same data may be of different data quality. So the question is how can we improve the quality of data so that we can make these AI applications more trustworthy and accurate. One other area that talking to data professionals that we see is today is a lot of these AI applications are built using code, a lot of code, primarily by developers, so we are making an effort. How can we bring that same level of uh. Agents are building agents and those frameworks using a no code low code framework so that's where we're investing in some areas and this has been an ask and the last one is this is very, very critical for an enterprise is being able to govern the data. Right? Even within these AI applications they want these governance policies to be enforced so that only the right people have the right access and the right level of access to the data within the AI applications. So what are the areas that we are focusing on? One, we are looking at how can we bring data from various parts of your enterprise into embedded into embeddings, which is a format that that that these large language models understand. Right, so as Gopi was mentioning before, we have 300+ out of the box connectors that can connect to information from various parts within the enterprise, right? So we are able to bring that data in real time and as well as bring that data into an embedding format that these large language models understand and we are able to we support today being able to continuously bring data either initial load or with incremental loading. Other area which we are focusing on is we see that a lot of our customers they depend on Informatica MDM, which is our Business 360 solution to bring trustworthy high quality data within their systems. So this a think of it like the data within the MDM for those of you who are not um aware of master data management. So these are systems that hold the true. source or the true identity of a particular entity within your company. So, we are making it easy for you to bring that high quality data within AI applications. As I was mentioning before. Right? Large language models work based on reasoning. Right? and if you leave it to guessing you're not going to get the accurate result so in order for it to be able to provide very accurate response you just don't have to bring the data you also need to bring in the metadata and today we see a lot of enterprises putting a lot of effort into creating this metadata and also building highways or pipelines to bring these data into large language uh into uh generative AI applications. Other than uh helping the large language models to accurately work on the data, the few other benefits that we see one is dynamic data selection based on the data quality, right? Large language models can choose one data over the other depending on the data quality scores or the thresholds that you can provide. Another area is being able to respond using the business information or the business glossary or the business context, being able to provide the response in the business jargon itself that the enterprise employees or the users or the enterprise users within the organization can easily understand. The third area that we're focusing around is being able to build no code low code agents using our no code low code framework within our platform, right? And all the way we also provide recipes which can enable you to jumpstart your development of these agents and also you'll be able to expose these agents using MCPs where you can call these agents from other remote agents as well. So let's look at the blueprint, the generative AI blueprint that we announced last year, right? And what are the different components of, of an agent. So the first one of the first uh part of the agent is generally a planner, right? The planner receives the user's intent and based on the intent it basically also it collects all of the required metadata any kind of previous conversations that are going on and then based on that it's actually providing you a chain of thought or a planning or an execution plan. Once the execution plan is provided, it's then sent it, it's the sent to an orchestrator which is now going to evaluate all of these steps and be responsible for managing the state of the entire agent and orchestrate all of those steps in the fashion, uh, either parallel or sequential depending on, uh, the, the dependencies between the different steps. And then it's sent to the executor which is responsible for either invoking your MCP or the different tools uh and then being able to bring in the necessary data and the metadata from within the different enterprise systems and cuss our customers leverage our executors for bringing data from within our metadata system of intelligence. And also we do have what is called an access management proxy which enables you to govern and filter out what data can be brought in within the agent application and this is these policies are defined by your data stewards who kind of define what are what is the data that one can and cannot access. And finally it goes to the summarizer which provides the summarization of the response based on both the data and the metadata in the language or in the jargon that the enterprise understands. And finally, usually every application will have some kind of a front end interface which is like a chatbot, uh, whereby you can do the necessary user authorization and authentication and then an API layer for ability to do various throttling and fin ops controls. So this is typically what we see as an agent that's built within an enterprise. Moving on, so as Gopi was mentioning before, we are announcing, we just announced our MCP connector for Amazon Bedrock Agent Core. So just by show of hands, how many of you know what Amazon Bedrock Agent Core is? OK, there are a few, so I'll just give a quick primer. So Amazon Bedrock Agent Core is um AWS's agentic AI service that allows users or customers to build. Orchestrate and operate agents on their platform. They provide a serverless runtime engine and also any context that are that is happening within the conversation is handled using a memory so there is both an option of short term and long term memory and then there is a gateway which allows you to have secure access to the MCP tools and the different data that's behind it. So we have our MCP integration announcement uh that went out two days ago and so now we are exposing our platform or providing the power of our platform through MP MCPs with an agent core so we announced two MCP servers, uh, as part of which one is the ability to search and bring in the necessary metadata. From our metadata system of intelligence and the second one is the data verification or the data quality engine that allows you to verify email addresses, phone numbers, etc. Our physical addresses and various other things. And now you can, you will be able to securely connect these MCP servers to Bedrock agent core gateways using OO and as you're building your own um. Agents on Agent Core, you will be able to leverage the power of our platform. So here is a high level overview or a blueprint of how you can as you're building so as you see here on the screen I'll just go back a little so with using our MCP you'll be able to bring in the metadata you'll be able to bring in our MDM so. Your product 360 customer 360 and then also connect to our AI agents that you built using our no code low code framework on our platform and then securely connected via OO to our agent core gateway and configure the necessary OO using the agent core identity. Additionally you'll be also able to connect via the agent core uh gateway the different data systems, their enterprise data sources and knowledge sources that you have on your platform and then build as you're building your agents within agent core you're able to. Bring in the necessary data and the metadata and the true quality of data from our Informatica MDM into your agents as you're building them on Agent Core. So with that, I'll move into a quick demo. OK. So imagine a scenario where Uh, you are first introducing your AI agent that can schedule appointments using which your customers can schedule appointments for the hardware too for installation of the hardware that they have recently ordered with you, right? And what you have defined is a are built and you're introducing is an AI assistant before this the experience would have been that they would have to call your data center or your support center, right? And then. They'll have to go through the various different systems, right? Usually to serve to service something like this. I'm gonna go back to my PowerPoint here. To be able to serve something like this, right, the sources of data that you need are usually some kind of an order information you need to know what are some of the, if there were any existing appointments, right, and customer details or order information and product details, bill of material and various other things and the challenge that these support uh folks face is that these are not in a single system they're scattered across multiple different systems, right? So with Informatica MDM now you can build, bring all of those transactional data and build that reference data. And build that true quality of the data and high quality data and then bring that into your agents. And with you'll also we see that you know sometimes there are user inputs like they may provide you a different address you need to verify these address now using our MCP you'll be able to quickly verify these addresses um for ensuring that they are right. Imagine a scenario where in a shipping or an appointment scheduling if the address is wrong, the cost that it you incur if you shipped it to the wrong address and more than that it's the customer frustration. With our MCP now you will be able to connect to your Amazon agent core gateway using OO and then be able to leverage. And build your scheduler or your AI agent within uh Amazon Bedrock agent core so I'm gonna just quick. We go through the demo here. So what you see here is AI, uh, assistant. So imagine Rajesh, who is an IT, uh, procurement manager, is now trying to schedule an appointment, right? So he comes in and he says, schedule an appointment. So Now the agent is trying to understand, OK, what's the intent of the user and then trying to invoke the necessary uh systems that are behind it. So it's actually invoking the necessary gateway. Let me just do a quick refresh just to make sure. Connected to the Internet. Yep, I am. I'm just going to go back here. Give me one minute. I just wanna make sure I'm connected. I'll have to reboot. OK, let's see now if it, yep, it's now picked it up. So you can see here now the agent has started working so it has understood what my intent is, right? Now, Rajesh comes in here, right? And he just gives his name. So first what the system tries to do is it tries to identify whether Rajesh is within the system, right? So what it's actually doing here is that it's actually calling Informatica MDM behind the scenes, right? and then trying to bring in and identify whether Rajesh exists in my system and also it has sent an email to Rajesh asking for a verification code which is for a multi-factor auth. So once uh we've as for for the demo we've just given we always send an email with the default number 700 so once he has verified himself using a multi-factor art we are bringing in a number of information, right? We're getting the customer information we're also bringing in the relationship of Rajesh with the organization because we are harnessing the multi-domain relationship within our MDM and bringing that information into. The AI application and then further than that what we're doing is we're also bringing in product information right based on again based on the organization and the account ID we're bringing in the product information and from there we're bringing in the product I uh the orders as well and in addition to that what we're also bringing in is all of the supplier and the bill of material information all of this the agent is able to do it using RMCP in one shot. Just imagine when you call a support agent, the amount of time that you actually spend holding the call is because these agents are shifting through various different uh systems within the enterprise and then they have to correlate in all of this rather than that this basically reduces your first call response time and your first you can also achieve better first call resolution. So as you can see here it pulled all of the information about Rajesh, right? It also pulled what are the usual delivery addresses that he has now Rajesh goes in and says, no, I want to ship it to a different address, so I'm gonna give a wrong address here, right? OK. So I gave a wrong address, correct? This address doesn't exist. So just imagine if you were to give a wrong address, what is the cost of shipping to the wrong address and the customer frustration that you have? So now it invokes our data quality tool and our data verification tool in order to basically standardize those addresses so as you can see here, right, the address that I gave was 21 Seaport Road, but then it basically does the validation, the standardization, and corrected it to the correct address, right? And once that's done now I can say hey I want yes that is this, uh, I'm gonna say um. December 13th, this week I'm at reinvent, right? And I want it to be delivered by 2 p.m. Right, so as you can see here now it's going to based on my response is going to finalize all the details in for scheduling the appointment and it's also bringing in information from the different policy on what needs what what are the cust what how the customer has to prepare in order for the installation to happen right? your team will be there they'll be at this location installation of hardware, what are the initial configuration so bringing all of that right? And all of this within a few minutes. Rather than having to you holding the call and we all listen to some kind of a bad music on the phone, right? I'm gonna say, do you need, is there anything else? I'm gonna say thank you. And there you go. Rajesh has scheduled an an uh an appointment so as you can see here with just. A few clicks, right and entering a few prompts I was able to get get information from various parts of the enterprise and was able to get harness the relationship across the different domains and the also supplied the relevant metadata right on what the semantic context of each of the data was verify the user input and then being able to deliver a seamless experience back to the customer. Right, with that, I'll call Gopi back onto stage. Uh, hold on here one second. I do have a couple of questions since, uh, regarding the demo too. So, um, By the way, that was a live demo, as proved by the 1st 30 seconds of it not working, um, but I do have you use QuickSuite for that, right? Why don't you talk about what we have done with QuickSuite because I know it got announced at this conference if I'm not mistaken. And we are using Amazon's QuickSuite to build the agent canvas. So why don't you talk about it? Yeah, so what we have done is we've also built an integration within QuickSuite as well, right, with our MCP servers and also where we're leveraging is the agent core gateway. So imagine a scenario where you're able to connect to the agent core gateway and from the agent core gateway we are able to connect that back into QuickSuite and then bring that relevant experience within QuickSuite as well. Excellent. And there was another unique thing which you guys, I don't know whether you noticed when, uh, he, when Reggie was going through the demo agent. One of the unique things you will see whenever we have our agents is because we come from the world of data governance, access quality, all these stuff, every step there is an explainability when it says that working on it, thinking it through, and then it calls out like, hey, am I going through these systems to get. Uh, the customer information, the product information, the reference information. Here are the pipelines that are getting used. So this also addresses, especially in the enterprise where you need the information around, can I actually track what is the lineage of all these information. And what is the explainability behind all these agent frameworks so that is a unique thing which is also you get as part of when we are building agents and when we are working on agents that is a core part of what we want to also address yeah right Gopi there again these agents as they work through. There's auditability there because there's conversation history and everything so you really know that what are the systems that are accessed you'll be able to evaluate what what the entire workflow is what are the different systems that are getting access, what information is getting pulled, how are the information's going back and forth so you have the entire audit ability, uh, and that helps you debug and improve these agent performances better. Excellent, good you didn't get sacrificed to the demogods today. Thank you. So for the next section this is actually quite interesting because we have been talking about blueprints, architecture, all these stuff. Now we actually have a customer who has, let me introduce Anand Vijay, the unit CIO at Citizens Bank, to come and join me in this panel. So I'm wearing the jacket and everything. Please take a seat. And um You have been a long term customer for both Informatica and AWS and um there's some unique things about Anan which is he is definitely a visionary in this space. He is one of the people who were at the forefront of adoption of AI agents but through the lens of data addressing in a complex enterprise. But Anand, why don't you first talk about Citizens Bank, many of uh. Uh, folks here, they may not be fully aware of what you guys do and also give a vision of, um, what do you think of, uh what is your mission and how you work through your daily, uh, when you wake up with respect to how you think of this world around AI agents in the context of a regulated industry like Citizens Bank. Sure, uh, good afternoon, everybody. Thank you, uh, for joining us, uh, for the session, um. Um, Citizens Bank is, uh, I think the 11th or 12th largest bank, uh, in the US, uh, you know, as an established entity, it's been here for, I think around 150 years. I joined Citizens 4 years back, and, you know, my primary charter when I joined was to build, uh. An enterprise wide data strategy and you know again before the advent of generative AI and Agentic AI our focus was always ensuring to you know make sure that the the data ecosystem that we had is very trustworthy but also I think you know from a fundamental standpoint we wanna make sure that anybody who touches or interacts with data. They have a really good experience. So we looked at data from 3 different lenses from our customers, our own business partners internally. I, you know, I, I work in technologies, and then I also wanted to make sure that the experience that my own engineers have with data is also taken care of. So that's that, that was kind of our, uh, you know, mission vision statement where we want to build incredible data experience, uh, for our customers, business partners, and engineers. So we set out on that, um. And, uh, Scopi said, you know, we, we, we've been, uh, going at it for, uh, you know, close to 4 years now. If you look at the bottom, uh, section of the, the slide that's shown, these were the 6 strategic pillars, uh, that we started building towards. We've made tremendous strides on these pillars. The last pillar, you know, used to read 2 years back as, uh, primarily advancing our, uh, AI and ML platforms. Now it reads predictive and generative AI engineering, so it's different, slightly twined. But the intent has always been to ensure that we build data that's actually really trustworthy, uh, and we can actually build off of use case on top of that. Oh, this is excellent. The one of the coolest thing whenever, when, when he initially showed the slide was you guys really focus on the personas and everybody or your customers depending on the persona, and that is a really excellent for all these stuff. But, um, why don't you share with this audience around what is Citizens Banks. The enterprise data platform, your vision, and then uh how that is getting evolved as you see. So you know I'll, I'll flash this slide, uh, this, this might look like a typical, um, you know, a data ecosystem, but this is, this is basically a citizens ecosystem. Uh, our focus for the data ecosystem was very, very simple, right? We wanted to focus on one key aspect which is time to insights we call it TTI, which is from the moment a transaction or an event happens anywhere, uh, within the citizens' ecosystem whether it's a customer going and doing a. Uh, you know, a point of sale or we actually essentially calculating a statement for a customer doesn't matter. We wanna make sure that we reduce the cycle time from when the event happens to when it's actually available for insights, whether it's available for insights with to a customer or to, you know, my business partner who actually has to take business decisions or for my engineer, right, when it comes to actual jobs that are running, if they fail, they need to know immediately, right? That was our intent. So this kind of gives you a, you know, broader picture around, uh, how our, uh, ecosystem is stacked. It's primarily AWS. We acquire data from a lot of different systems in, in a multitude of fashions batch API streaming. Uh, we, we land them, we curate them, we create purpose-built data structures and, uh, data products that are business line specific and then make that available through, uh, our own internal data marketplace that we've built. And then if you look at. The, the slide pretty closely. There is also a box for Informatica there. This is, yeah, I was about to ask how critical is Informatica and how it fits into this complex ecosystem of, so a couple of things like last year we actually turned on R 360, which was essentially the art reference 360 product. So one of the things we found out very earlier is that we didn't have a capability to essentially manage and govern reference data. Which is where that came into picture and this year, uh, we turned on, uh, customer 360, um, it's, uh, it's the Informatica MDM product but we call within, you know, citizens we call it as Citizens Customer master. Uh, we are, I think one of the first to actually orchestrate a real-time MDM platform which is, uh, entirely operational, um, high performance, low latency, 4.9 availability, uh, essentially directly interfacing with our customers. So it's, it's one of the first, uh, we were, uh. Uh, we partnered, you know, very heavily with Informatica to kind of make that happen, but as you can see, our data ecosystem. Uh, as it stands on the top, relies on those two core engines when it comes to customer and reference data, we are essentially anchoring on, uh, what we build with Informatica. OK, again, we talked a lot about your enterprise data ecosystem. Why don't you share the vision of earlier you talked about the predictive and generative AI right pillar, uh, your vision and how that vision has evolved when it comes to AI engineering and how you are looking at it from the journey or from the ML world to a gen AI world or the gentech world now. Yeah, as you can see, you know, I want you to kind of uh keep this ecosystem slide in your mind. And then when you look at this, our approach, whether it's data. Our AI is always focused on uh a platform or an engineering approach. So as you can see this is our platform engineering strategy when it comes to artificial intelligence, uh, whether it's predictive AI generative AI, gentic AI, we don't look at them differently. We look at our strategy as a singular entity across all those three streamlines and we focus on data. It's very similar to, I think the slide that Rajiv was, uh, talking through earlier, um, from a data standpoint, from a security and governance standpoint, tooling. In terms of workflows, how we created, uh, and then the last one which is around culture, skill and literacy, we, we have a very, very clear focus around essentially replicating the model that we created for our data ecosystem for the AI world as well. I don't think there is necessarily, uh, um, you know, a need to kind of reinvent the wheel in terms of what your strategy should be, but I think there are really important aspects across these, you know, 5, swim lines that you see specifically around culture, skill, and literacy. Uh, we have a badging program within citizens. Um, we have engineering academies that we run for our own colleagues, uh, and our business, so we've just essentially expanded that and now provide AI specific content. We allow people to go and take, uh, AI badging. We have turned on products and tooling. Uh, for our colleagues, which is basically again, uh, tapping into potential, uh, that you know we were not able to tap into before, but it's very, very soundly anchored against uh anchored on, uh, what our data strategy principles were we're just expanding that, um, to a significant extent from a security and governance standpoint. Obviously as you can see there is a little bit more um you know secret sauce that is required, uh, especially around you know kill switches and making sure that we protect the customers' data and also you are, you are in a highly compliant like compliance is very important in your industry so highly regulated, um, we are also one of the. Uh, I think, uh, banks that actually have most of our footprint in the public cloud, uh, I believe we are 97 or 98%, uh, of our entire technology footprint is in the public cloud. We're very close to shutting down our data centers. We'll, we'll be, uh, in, in literally a few weeks to a month, uh, time we would be 100% running on the public cloud, so. A lot of the kind of railing that you would have required plumbing and railing to kind of secure has already been built around data and exactly to your point we wanna make sure that we expand that and cover um AI as well. Yeah, but then, um, help us understand how was that evolution when it came from the predictive to the agentic. Yeah, this is a, this is a good slide we use, um, to kind of explain what our focus is and how we kind of sequence out, uh, a typical predictive AI use case. Predictive AI is basically, uh, it could be even. Uh, predating a predictive AI, which is again a machine learning algorithm that you wanna write, right, it could be a machine learning algorithm that operates on batch data. It could be a machine learning algorithm that's actually sitting in, uh, you know, orchestrating a real-time flow. It doesn't matter, but you know, we went from ideation, modeling, operationalizing it, consuming and monitoring on the bottom. It's very similar. Ideation still stays the same. The second phase is not just building the model and basically capturing the model's accuracy, it's building and evaluating, right? There is a subtlety behind that, you know, you don't just build and then it gives you the right answer exactly. Uh, but, but there is a, there is a, a heavy focus on the evaluation phase. We take quite a bit of time there, uh, because the evaluation criteria when it comes to a machine learning model versus a Gen AI use case is, is, is slightly different. It's not just purely on the outcome. It's, it's also some of the other. Aspects around um whether it's hallucinating whether it's actually you know applying the same thought or if the thought is essentially something that I have to make sure that is not biased right so there's those aspects that are slightly different than uh a machine learning model or even a typical statistical algorithm that you could run that is a significant difference between that and again in terms of slicing we develop it and deployed uh and then you know there is a consumption and integration pattern. The integration pattern here again. Um, more than a machine learning model, there is a lot more conversational based experience, as you might all know when it comes to Gen AI, so we focus on that and then we obviously monitor them as with uh predictive AI, but we also essentially have a feedback loop which is again you, you would have noticed in a lot of these AI interactions there's also an optional for the customers or whoever is consuming it to provide feedback. Excellent. Um, I know that you have, uh, when we have discussed about your architecture, if you're willing to share the architecture of how your workflow falls in the Gen AI platform. Um, not much different to what you saw with Informatica, but this is again our equivalent of, uh, a generative AI platform. This is our autonomous agenttic placemat. Um, there is again different sections here you can kind of run through it. There is a, uh, the, the first line is essentially the different personas that can essentially interact with our core services. The core services are backed by capabilities that we have built and turned on. This is where our platform approach comes into picture. And then behind that, um, there is actual components that we leverage whether it's large language models or MCPs or you know code repositories where we maintain our code etc. It's a combination of a really good combination of capabilities that have been turned on in public loads in this case it's AWS as well as core foundational data capabilities such as Informatica, right? We're not. Building something new that's separate and operates independent of our data ecosystem, we're just tapping into our data ecosystem and then a heavy focus on observability and governance which you would see on, you know, 6 and 7. Excellent. No, that's, uh, that's amazing, and, um, I know the, so when you have like this vision and a new GAA platform. How does that append to your current data ecosystem because some of the functionality, some of the needs are slightly different, but you earlier mentioned that, hey, we build, it is an evolution of what we already have. So if you can, uh, give a, a quick overview about uh Tarviga. Yeah, I think the intent is always to, uh, increment, augment, and complement what we have, not to really essentially build something new. I think the, the focus for a lot of our institutions, you know, who are here, you know, for all of us is how can I tap into what I already have and then, you know, make it more efficient and drive immediate, uh, benefit out of it, immediate ROI out of it. Um, a lot of times what we intend, what we tend to do when it comes to technology is we end up. You know, taking something new and build something really new, but we completely lose sight on what's already there. We don't build a plan to decommission that. There's a lot of instances where people would have gone through multiple iterations of modernizing or transforming something that existed, and then you will have 9 versions of it, right? And all 9 are running. None of them have been decommed, and you basically run support and you have to make sure that all of them run, right? So we have been very, very clear on that front which is. If we do want to build something new, we want to make sure that we are committed to also providing a path to wind down what's already there. And most of, you know, the cases when it comes to our data ecosystem. We haven't had a necessity to turn down anything, for example, our data framework that we have that acquires data, enriches it, and provisions it for use cases, we just went and added more adapters that can essentially do chunking and embedding and vectorizing. As simple as that. So it's basically again adding to what we have in, in that instance. Our persistent store, we just expanded additional persistent stores that can specialize storing vectors, for example, rather than completely taking our, you know, data lake, replicating it somewhere else. We didn't do that, you know, uh, there's a cost aspect to this as well, so we're pretty economical that way, um, I think to your question. This is our new data ecosystem. It's literally exactly the slide that you saw, you know, you know, for 2-3 slides before. The only addition, if you see, is on the top right corner where we essentially have hooked our entire data ecosystem, uh, to the gay platform or genery AI, uh, platform services. It's as simple as that. Um, it, it looks ridiculously simple that it's simple. It's an arrow, of course. There's a lot of technology complexity behind it, as you all know, but I think the intent here is that we have not gone through a route of building any sort of data redundancy or any sort of data duplication or any sort of, you know, kind of tool churn with what we have built. That's, that's, uh, that's the most important point. That you know we wanted to anchor ourselves and you know something that you want to take uh from this uh session is to kind of focus on this. If you have a lot of technical depth, obviously you have an opportunity to transform, but for someone like us where we started in the last 3-4 years, uh, this is kind of where we are right now in our journey. Now, this is brilliant because uh initially we started about, hey, you are in a regulated industry, you are a BFSI, uh, in the banking industry how complex is your ecosystem, but when it came to generative AI and with your AI vision, which is actually pretty cool, is because I know that in this conference in previous years at Rin when we always talk about when it comes to G AI, one of the big problems of adopt adoption is from pilot to production. But you guys actually crossed the chasm by not just taking away what you did but looking at it as like how do you, what, what, what was it, it was beautiful how you mentioned it was like in increment augment and complement compliment, yeah. And uh that was that was a great learning thing because as a uh we are fascinated because our technologies are getting used by a customer like yours and your vision is really solid when it comes to how to really build this using your framework of increment augment complement. So thank thanks Anan for uh for your time and also sharing all this information with our peers here and also for our folks on the screen. Um, this would be very helpful. I'm pretty sure this was also a learning experience for many of them. So thank you very much for that. Thank you. So I wanna fucking crab. I think that comes to the end of a slide. Uh, here is the standard QR code. Uh, we do have an expo booth and there is also a lot of, um, virtual artifacts available as part of that. Uh, let me bring in, uh, Reggie here, um. Again, I'm gracious for you that you are willing to also take Q&A, whatever you can answer as part of, uh, the presentation here, but, uh, uh, we wanted to open it up for any Q&A if you have whether it is here on stage or whether it is also outside the room, so. Thank you very much, Rajiv. Thank you, Anand, and thanks everyone for taking your time to come here. Let me open this up for a Q&A. OK. If not, um, this is great so thanks very much. Thank you, thanks, Angie. Thank you.