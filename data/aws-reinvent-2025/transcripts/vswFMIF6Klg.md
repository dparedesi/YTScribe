---
video_id: vswFMIF6Klg
video_url: https://www.youtube.com/watch?v=vswFMIF6Klg
is_generated: False
is_translatable: True
---

Good morning, everybody. Uh, hope, can you hear me? So thanks for being here today. Uh, my name is Christian Finale. I'm a senior solution architect at AWS, part of the Telco Business unit. Today we'll be learning how Ericsson are, are transforming their in-house, um, in-house development experience with AI on AWS. We'll have Bastian and Doug talking about that in the next session. Now, Ibrahim and I will talk you through on how Ericsson are optimizing their customers' mobile network experience with AI on AWS. So AWS is working, is working with CSP communication service providers to achieve their business objectives, which are increase revenue, optimize cost, and reduce customer churn. CSPs are migrating and modernizing complex BSS and OSS tax on AWS. They do that to reduce time to market and increase product agility. At the same time, they migrate complex IT workloads to increase operational efficiency through AI automation. CSPs are also migrating and virtualizing network functions on AWS. They do that to increase elasticity, scalability, and also to increase, uh, business continuity through disaster recovery options on AWS. Also, CSPs are transforming their customer experience for, um, to basically everything starts with data, the way they model, uh, the customer's behavior and the way they can provide a personalized customer experience for their CSPs. Lastly, also. Ericsson, so um AWS and CSPs are working, working together tightly to um to, to build and think beyond connectivity. In build new revenue streams and protecting and monetizing existing um network investment. They explore new business models and they, as part of that software as a service, for example, is something that gives them the option to explore new pricing structures and build and, and increase agility of their product line. AI is a key component in all these areas and uh we are seeing that Ericsson as a key AWS partner is best positioned to scale this initiatives, some of these initiatives we'll see. At scale, um, globally. As a matter of fact, Ericsson are transforming their business and their product lines with AI and ML. So let me bring a few examples on how they're achieving that. So they're reducing the product delivery time by leveraging agents to integrate complex software stacks in the field. They're optimizing mobile network performance for their customers CSPs by leveraging a combination of traditional ML and agentic AI solutions. They're also reducing time to bring new features to market. Features that are delivered and they are implemented by team, internal teams by building a MLOps platform to accelerate adoptions of ML technologies. And also they are empowering them, their large base of software developers to improve their efficiency for to build better products. We are, we, we're working closely with uh Ericsson. We are with at firsthand experience of what it means building a successful agentic AI uh solution. It's not just identifying and picking up on a, picking an LLM and or adopting an agentic SDK. Um And everything that they complete AI stack, so the, the accuracy and economical viability of the AI stack underpinning the AgenticI solution. Is a critical factor for its success. There are few areas we've identified. We start off from the fact that this, the business must be able to trust agents and the output of these agents. So this means access to data and to knowledge authorized and at the, at the, at the lowest ground level. At the same time, it's important to, in a, in a traditional ML um training, data lineage plays a key role in keeping track of origin of data for data sovereignty requirements. Also, trust comes with explainability, being able to determine, to dissect how agents behave, the turns they take, the reasoning process, and also the different, the different tools they call and the performance of this. Also, as a next step is the fact that context is king. Um, so the way context, the way knowledge and data are delivered at the right time to the right agent concisely. Because size means cost. It's also another element for success. We also see that um in a multi-agentic solution where there are multiple agents working in combination, it's important to attribute value to each agent and when we say value is that the, the overall outcome of the multi-genic solution. Is the value is split across different agents and determining what value each agents bring along with the cost allows an atomic determination of atomic, atomic ROI for each agent. This goes into really explainability, observability of the foundation for operational excellence. So being able to understand and monitor and monitor um end to end the performance of these agents as we said and also being able to operate them. But when we said agents is also not just operations, agent operations, but also everything that falls into the underlying AI stack. So LLM operations, ML operations. Last, um, Also being able to run these operations efficiently through AI SDSC capabilities. Um, Ibrahim, you might wanna talk to us what the importance for why explainability is important for customers. Thank you very much, Christian. Um, one thing I think we, we understand, people who are working very closely with Agentech AI and agents are not just about deployment of technology. When a CSP, a service provider, is basically sourcing agents from Ericsson. They trust an outcome that comes out from this agent. They are basically outsourcing network operation for a national infrastructure to someone like Ericsson. So for them it's extremely important that we have explainability in the whole stack why a decision by an agent is like that, why my workflow is designed this way, and what are the tools and why the tools that, as I will explain in the coming slides, why these tools are making such kind of decisions to implement changes in a national infrastructure like telco. So explainability is extremely important, and the stack that AWS provides actually gives us the tools and the capabilities to have this explainability to CSP, explainability and observability, right, Christian? Absolutely. Thank you, Ibrahim. Thank you. And the observability, as Ibrahim said, is Across not just, as I said, not just across the agentic AI layer, but across the whole AI stack. Ericsson are leveraging different services from the AI, AWS AI stacks. We will see both, um, use cases how they leverage these services, but the, the stacks covers serve multiple personas for different business cases. So we start off from, um, the bottom layer where data scientists use AI compute to run training jobs and host model for interference. On from GPU to Amazon on chips. Sage Amazon Sage maker AI is uh is used by data scientists to run the entire MLOs um on for their, for their ML um models. So preparing data, training model. Evaluating models continuously throughout their life cycle. Hosting models for inference and then monitoring the model performances. Moving up the stack engineers and developer use. Amazon Bedrock to run agents at scale securely in production. This gives them choice of models of foundational models, and gives them capabilities to allow to run that at scale and securely. Also, Agentic SD SDLC applications are leveraged by developers to increase their efficiency in, in deli in coding, in developing software, but also in maintaining um throughout all the operations. So Today, we have firsthand experience, two teams from Ericsson coming on stage, um, to discuss these two use cases. First one is cognitive Network Solutions, um, CNS. So they're leveraging, they're using, um, um, a combination of ML, traditional ML interference and a multi-genic solution to optimize CSP's, uh, mobile network performance. And then system comprehension lab, they Empower their soft engineer, um, Ericsson engineers to build better products through AI. I'll leave the stage to you, Ibrahim. Thank you again, Christian. So, uh, Let me take you through our journey of partnership together with in building this agentic AI for, but before I take you through this journey, I would like you to understand a little bit what we stand for for Ericsson, what's our mission and what is the impact that we are actually having on society. 5G networks today, there are around 340, 189 of them are actually running using Ericsson innovation. So that's more than 50% of 5G networks worldwide and more than 50% of worldwide 5G traffic outside China is running in our own equipment. I can tell you that here you are in Vegas. If you are having mobile services from AT&T, Verizon, or T-Mobile, it's 100% the chance that you are using Ericsson Innovate. We are using CSPs are using Ericsson Innovation to deliver the service to you. This is the impact that we are having on society. This is the impact that we are providing to the world. And in order to make sure that these services are provided like best in class, we have a heritage of innovation that extends 150 years. Most like the most obvious thing that you see is the number of patents that we bring for mobile technology, more than 60,000 patents. What actually proves that this kind of technology or this kind of innovation is providing value is that more than 74% of worldwide CSPs, they always come first when they are benchmarked against their competition in the market when they are using Ericson equipment. So for us and Ericsson, it's extremely important to make sure that this impact on society, what we bring to CSPs when it comes to performance, to innovation, is top notch, and that's why it's important for us to partner with someone like AWS to provide such kind of value, and AI is a cornerstone of that transformation that we provide to our customers. So for us, we worked in transforming our portfolio offering on the different parts of the stack in mobile technology and also for network operation using AI. So we leveraged our own competence, so we shifted our own competence in order to make sure that people inside are capable are capable of delivering such kind of new value to customers using AI. We also worked on shifting the mindset of people and understanding what it means to provide AI in each and every offering, making sure that AI is we built our networks for such kind of AI traffic that we see today and we see growing in the future, and we are also using AI into the network operation and also across the stack, as is going to explain to us a little bit later. So we provide this in the portfolio of how we, how this mobile innovation is delivered to CSPs and also the way that we operate these networks and this is the framework that we talk about for autonomous networks. So, and if I give a little bit of um. A view on what we mean by autonomous networks. Autonomous networks is a framework that is defined by TMF on how we operate networks for today and for the future, and it is actually across 5 different levels that you see here on the slide. One thing that I would like to bring to your attention is the graph that you see below. The percentage of CSPs today who are barely reaching level 3 or beyond level 3 is less than 10%. And it's actually when you start to move, when you start to have humans, uh sorry, when you start to have AI into these systems and into network operation. Is the Level 3 plus that we are talking about. So you might hear around in the community or right here actually in CSP talking about reaching a higher level of network autonomy and reaching level 4. We cannot reach level 4 without making sure that we are embedding AI in the way that we operate networks, and that's why autonomous networks is a very important framework to bring this to our CSPs. And the way that we realize that in our portfolio for Telco AI is what you see here, what we refer to as the cognitive loop, 5 stage, 5 stage of actually the workflow that we have for network operation, starting by measurement where we have measurement agents that understand the network data and making sure that you know we are capturing the information we want in order to move to the next step, which is assurance. And then assurance is where we have agents like classifiers and root root cause analyzers who can actually tell us what's happening in the network, what kind of anomalies are there, and what are the root root causes of such kind of anomalies, and then we move to the next step, which is proposal, where we have AI recommender systems and AI agents equipped with recommendation tools in order to propose what is the next best action. But obviously we understand networks are complex, so in order for us to make sure that the best decision is the final one implemented in a network, which is again reminding everyone is a national infrastructure for each country, this is the evaluation step that you can see there. And after the evaluation is done, this is where the execution happens and the activation, and the cycle continues. This is what we call the cognitive loop. And as you can see, agentic AI framework, agentic AI fabric is across, is implemented in order to have this cognitive loop fully functioning from end to end, and this is achieved through different agents, as you can see there. So we have agents who are specialized in certain workflows who are getting runtime context from the network as the traffic shifts. Things are happening every day, I would say also every hour. And in order to achieve that, we believe in openness and we believe in horizontalization. So the architecture that we have built together with AWS is basically encompassing these two main objectives openness and how we get the data and make sure that such kind of data is exposed to the upper layers or like actually towards agents through the different services from AWS. And these services they actually provide the agents with different capabilities as you see on the slide. So leveraging, for example, as I mentioned, about the RAs which are tools for these agents. This is how we actually provide the agents with the capability to understand what's happening in the network and also execute the changes on these networks. Maybe here again observability is extremely important and not only the observability but the different services in the stack that we have from AWS. If you can share with the audience a little bit more about that. Absolutely, thank you, Ibrahim. So, um, it all starts by ingesting large volume of data from the radio access network. And as you see at the bottom there, there's, uh, the ETL layer which is run on AWS services, serverless services, containers, and fully managed service by AWS like, uh, Athena and Glue. And this is for processing, as I said, data coming from the radio access network and prepare them for, to be, to be consumed by an ML pipeline. Sage Maker, Amazon Sage Maker AI, that's where the ML pipeline is built, where data are prepared, the models are trained, model gets evaluated, um, and then they are hosted for interference. And point interference are then invoked by On our app, so our app, our application which are ML powered which consume predictions, um, so they, they produce predictions about natural performance and they, this, um, um, these are apps contributes uh Ibrahim said to the context of the associated agents. The, the agents themselves also rely on other uh data sources, other knowledge bases uh through rack techniques, data that are, that are basically product guide, network configuration. Na natural data structure and correlations amongst these, amongst different um natural parameters. So an agent is then able to invoke on our app to consume the prediction and to um retrieve no relevant knowledge. Amazon, Amazon uh Bedrock Agent Core then provides Ericsson with capabilities like memory and observability to To run, to offload, for Erickson to offload these functions on AWS managed services. So in essence, the full stack it plays a role, sage maker for producing accurate predictions through ML and agents, agent core to manage and the full life cycle of agents. Thank you very much, Christian. And if I go back again to the tools that we provide for these agents, which basically the our apps, the radio applications that can actually provide such kind of prediction and also understanding what's happening on the network. One example of this is what we call the cell shapers, and Cell shapers are reinforcement learning agents where we have a digital twin representation of the network. This is how we start. for those of you who are familiar with how networks are built from a radio perspective, The way that the network has been designed since the early times of is we have an antenna, and this antenna has a certain coverage area, and the very first problem that engineers try to solve is how to optimize coverage and quality. This is a multi-generational problem that started from 2G, 3G, 4G, 5G, and we also see it in 6G. So how can we make sure that we have the optimal coverage and capacity and the quality in this network? So the typical ways of working that have been around in the industry was to do simulations before you even have a deployment and before the signal is in the air. What we do with reinforcement learning agents here, which again these are the tools that we provide for agents on the on AWS stack. They reconstruct the radio environment based on live traffic, and they always learn from what's happening in the network. This learning is happening through a multi-objective reinforcement learning algorithms. So in order to make sure that we are always providing the best trade-off between coverage capacity and quality for the different layers, and they are always learning from the network. So the traffic changes and there are different aspects that impact that. For example, introduction of new spectrum or densification of sites or maybe addition of new technology, for example, like cloud run or like 6G on top of 5G. So these kind of cell shapers, they actually belong to what we call the proposed stage, as I explained in the cognitive loop, and they help us to realize such kind of autonomy on top of a stack like AWS. And this is not the only, these are not the only agents that we use. So as you can see here, I provided a little bit of more details about what kind of agents that we have developed, and we are now working together with AWS to bring it on their stack. So in different parts to improve user experience, to improve network operation, and also to improve efficiency in operating networks. The numbers that you see here are not lab results. They are actually coming from real field deployments for these agents. Basically we are able to measure the impact and we understand that the impact that we bring through Ericson technology, as I explained in the very first slide, is very profound for the society and for. And here I would like to hand over to my colleague Dag so to tell us, I told you a little bit about how we operate, so Dag can tell us about how we build that. Thank you, Ibrahim. Thank you everybody, um. So, let's maybe take, take a little bit of a, of a step back, right? Because we have um a lot of sort of AI applications in the, in the, in the networks that that Ibrahim talked about. But where does the actual Like intelligence come from. I mean, it partly comes from like observing the network and applying, you know, control function theory and so on, but it also comes from the fact that we built uh mobile network infrastructure for 4 decades, right? So if, if we're talking about intelligent networks, lots of this is actually about representing the intelligence that we have institutionally, the people we have, and, and the, the data assets that we have in the company. Um, so this is why we created something called the System Comprehension Lab, which has a couple of different objectives that, that, uh, that I'll, that I'll talk about. Um, but maybe the first thing to notice is, um, when we look out in the world, like we look at our customers' networks, we realized that like a mobile network today across a big, big country, especially here in the US, they're astoundingly complex and they're very, very, uh, complicated distributed systems and they're, they're complicated both in deployment managing like, like Abraham talked about, but they're also, you know, quite complicated in design, right? We have. You know, uh, all the different kinds of radio products. We have, you know, 2G, 4G, 3G, 5G, 6G co-existing. We have millions of configurations that, that are set to, to operate and configure these networks. Um, so our AI journey back some years ago with when we started working with with AWS started from this, this notion that to really solve an interesting problem with AI for, for Ericson and for our customers, it is about sort of realizing that we have this massive, massive system complexity out there in the real world. I mean, and I would say that this is actually not like a particularly Ericson specific problem and not even a telco specific problem, right? If you're, if you're designing aircraft or if you're designing uh nuclear power plants or if you're designing something of that nature, you are dealing with massive system complexity and, and AI somehow has to help you. Um, so, so we basically get to this, um, fundamental question for us, um, are we comprehension bound, or it's not even a question at this point, we are comprehension bound, and when I say us here, I mean Erickson's R&D, uh, we're several tens of thousands of people, and we work in the space of, of distributed systems. And so when you think about sort of OK building 6G or building the next generation of, of, of 5G sort of are we bounded by the traditional sort of three pillars of project execution of cost and scope and and time or, or are we, can we actually say that we are probably more limited by the amount of, you know, comprehension that that the humans involved can actually apply to these, to these processes. So, so I mean I'm, I'm not gonna be super self critical about this as, as Abraham said, half of the world's 5G traffic goes through our products so we're doing a good job, but we can still say that OK, we're probably comprehension bound as humans developing mobile network infrastructure. Um, so, so then our AI mission is this, this notion of, OK, how can we actually make AI work in the limit of, of, of complex systems engineering, right? So again, it doesn't matter if you think about telco, you can think about nuclear power or, or, or, or aircraft or whatever, you know, cars you whatever you wanna think about, right? So, so for us, the, the comprehension lab, it starts from the point that, OK, so we have millions and millions of documents in-house. I mean, we have, we've been doing what we're doing for 3 decades or 4 decades without AI and, um, and, and sort of, OK, so we have a first mission to take all of our data, our in-house data assets and just make us better at, you know, designing mobile networks. Um, and if we do that well, and we do that with Amazon Web Services, with Bedrock and so on, that I'll speak to you in a little bit. We sort of put down a foundation of institutional knowledge and sort of intelligence about how uh mobile networks are built that we can then layer on top the the kind of data that Abraham was talking about which is when we actually see these enormous data streams of what's happening in the live networks out there we can explain what that means we can act on that we can, we can be sort of intelligently integrated into that into that data flow. And then that lays again the foundations for thinking about sort of, OK, how do we make our products understand our human intents in, in ways that, that, you know, again, derived from the fact that we're sort of consolidating our institutional knowledge um that makes us better at, at designing our products and then we're layering on top sort of the, the, uh, the, uh, the, the real network to dimmetry that's, that's out there. So, I mean, what, what I wanna also mention here is that there is a real disconnect out there in the world. I mean, I, I think we all are a part of this information flow where. The frontier AI capabilities that get a lot of attention like you know competition level programming and competition physics and competition mathematics and so on. They're, they're opening up a very wide gap towards the sort of reality of, of, of AI impact in, in, in, uh, in an industrial setting or in a product development setting and And, and this gap is actually much, much bigger than what, what I, what I, what I think I, I try to visualize here, right? I mean, the gap between how much value we can generate on our own with AI in, in product development versus the, the state of the art, so to speak. So we've been working a lot with, with AWS to, to sort of say, OK, what does it take to close this gap for a company of Ericson's size? I mean, again, we're, we're, we're a fairly large R&D organization. We need to close this gap so, so we actually can deliver the value that that Ibrahim spoke about. Um, and this is about the challenge that, OK, every, every organization has a lot of data, but nobody has clean data, right? So all of these, you know, demos you see about competition programming and so on, underneath of that is an enormous amount of data work. So one has to do that and one has to combine sort of different kinds of data assets, um, across this very complicated, um, design space that I'm gonna mention. Um, then of course multi-step reasoning makes this harder, right? So we were used to saying, OK, wow, super nice. Now there's a new multi multi-step reasoning LLM capable of helping us, but. You know, the more steps you have in reasoning, if you're applying this to some complex technical problem, every step has a chance to take you off, off rails and then you sort of diverge into nothing very, very quickly unless you can constrain that reasoning, um, as we, uh, as we move forward. So, these are some of the challenges that inspired us to work on, on Agent Core and working with, with, uh, with Bastian and and AWS over the last couple of years. So, um, what we basically mean is, is information fusion, right? I mean, we have, um, again, this is different in, in, in a, in a DevOps setting, but we're from my standpoint an embedded systems company, right? So we, we collect, uh, requirements, we systemize, we develop, we test and integrate, we release and deploy, and then our products are out in the field for a decade or more. So it, it is about taking a vastly complicated sort of set of um data assets throughout this flow and fusing them in a, in a way that we haven't been able to been able to do before. The, the key question then, I mean, for us is, OK, how do we then, what are, what are the assumptions or what's the architecture that we need to be able to do this at the scale of, of our R&D organization which, as I said, is several tens of thousands of developers. Um, the first thing we want to accomplish is a kind of decoupling. So you can say, OK, um, we have tools like Amazon Que Developer or Quiro as, as a sort of user front-end facing tool, but, uh, that in itself doesn't know anything about Ericsson's, you know, internal R&D assets. So through MCP we sort of said, OK, let's standardize on MCP as that decoupling layer between the front-end tools and whatever we design in-house. And then we put a lot of effort in. Not being sort of end to end AI products, but building sort of these specialized AI agents, the specialized AI technology that, that is, you know, somehow differentiating for Ericsson, specialized models, specialized developer tooling, and, and specialized agents. Um, so I will, I mean, especially in the, in the specialized agents category, I would like to invite Bastian to talk a little bit about, you know, different aspects of this and how, uh, this all came together for us. Thank you. Thank you so much, Doug. Thank you. Right, so thanks doc for talking about I think the, the concept of being comprehension bound is important to keep in mind here. Uh, it's something that is important for all enterprises that have history, I would say, right? And especially uh in R&D heavy enterprises. So it's it's also great how we tackle that with the System Comprehension Lab, and for this I'm going to talk a little bit about what are important factors within the System Comprehension Lab. What are the key pillars that we have to work on to To make this successful, so on one hand we say that the agents are the ones that interface with the humans, but they need the right data. We already talked about this quite a lot now that there are different data, different data silos. They have to be fused together. So how do we do that? One part of that is that while multi-step reasoning has its own problems, multi-step reasoning is one of the things that is really important for that because we have to move beyond the traditional traditional, or some people call it naive rag where it's just let's let me find similar text in a text space that you have, it's just impossible to get something sensible. Or you get something, but is it, is it right? So it's important to have the system where you actually evaluate the user query where you where you query different user, where we query different data sources, combine this data, and maybe step back, think again, maybe interact with the user again to get to the right data that your agent needs. So on top of that, so, so this is, this is getting the right data, uh, the concise data at the right time for the agent. The other thing that an agent needs to, to, to act upon this, right, is sometimes you need to go a step further, and I'll talk about this in a little bit more detail, but yes, context depends on, on, on traditional AIML models at some point, but it also, uh, also might need that you need a custom large language model. And uh I'll, I'll go a little bit into why that is important in the, in, in, in, in the, in the next step. Multi multi-modal processing is also important. I mean, the data that we see is, it's, it's not, it's not like we just ingest very well structured text documents. There's a lot of different modalities in the data and again also there I'll I'll show a little bit more detail in the following slide. So now, we are ready to see like this is the the the Asian can get the right data now. Well, even though this is a simplified view of this, that the Asian has the right data, what else is important for it? Well, it is important that it can integrate two downstream tools. Uh, you didn't mention that now, but I mean there are these agents that need to interact with the physical or digital world, right? I mean this is supposed to be for engineers to build better products. Engineers work with a multitude of different tools and when, when, when, when they, when they do their coding, when they, when they, when they are engineering. So they maybe they need a connection to the project planning tool that they're using, um, knowledge that they have there, um, the, the. The source control tool that they're using different kinds of tools, and you need to have access to that and you would like to remove all of these paper cuts for for for for the for for the user so you hand it over to the to the agent. So again here you said this nicely earlier MCP is the is the layer here that connects all of this. How do you actually work with this downstream system? Well, you make, make these tools available through MCP and the agent can use them and they can use each other through that as well. So now we have downstream integration. The other thing that you want to have, well, you want to surface this information to the user, and Ericsson follows a nice principle here of making it available to the user where they want to work. So sometimes it may be the right thing to work on a deep research task within a browser, so they have a dedicated front end for that. And sometimes it may be that you want to do that directly on the ID where you do your coding for a specific project and then you want to utilize it directly through, for example, Amazon Q Developer or Que. So agent has the right tools, agent has the right data, but there's more to it, maybe the more, more fluffy, the non-functional pieces that are important. So we've already talked about observability quite a lot. It is important to understand what tools are called. Did we select the right tools? Are they relevant for the task? Do they get the right output? Do we get the right context? Is, is the context that we have we have created through the multi-step reasoning actually relevant for the question that was asked or for the for the task that the agent is supposed to work on? Observability also goes beyond that. I mean, these systems are not just agent and maybe a model somewhere. Those are those are complex or at least complicated systems with many many moving pieces. You have your regular application. You have your your models, but you also have data sources. You have pipelines to process data and all of that. So there's a lot of things I need to keep track on. So observability is utmost importance. Then scalable architecture. I'm not gonna focus too much on the technical piece of that. Also, that's important, right? And Ericsson has, uh, has, has, is, is using Agent Core as one of the Bedrock agent core as one of the main core pieces of that to make that easy so you get this serverless, um, serverless experience of, of running and running your agents. But there is another part to scalability, and that's the organizational scalability. In essence, what you have here is a two-sided marketplace where you have teams that have expertise in certain domains and they want to make this available. They want to make this available through these agents, but they may not necessarily want to become experts in. In agentic AI or how to train models, that's maybe not what they want to do. So you need to be able to onboard these teams and their knowledge quickly and this is what Ericsson is doing really nicely with blueprints and actually agents to help onboard onboard teams onto the system. So two-sided marketplace you have the producer side and you have the consumer side. The consumers also need to be able to be on board and there's there's enablement. There is um making sure that that they're using the system in a in a way that is, it's, it's, it makes sense from a from a from a from a from a limit perspective. So are they overusing that? I mean there's cost involved with LLMs. So is this, is, is this working right? You also need to be able to make sure that these. That these users can access the tools that they should, but probably not everything, right? So there's there's a certain amount of uh authentication authorization involved here as well. So that's the scalable architecture piece. Lastly, but it's not the least, it's actually something that I personally talk about first in every meeting when there is a GI use case evaluation, it needs to come first. The only constant that you have in Geni I projects is change, and it's a very high velocity change that we see here. So the only way that you can actually keep up with the latest model that you want to try, the latest technique and pattern that you want to try, or simply update the data that you have is you need to be able to understand, is this change that I introduced now at least as good as yesterday. And that only works if you build a robust evaluation framework from from day one. All right. Moving on into something, uh, into, I, I, I'll, I'll take, take a, take some, some, some time to explain certain parts of the system. If we wanna talk about this in its entirety, that is, we have a couple of days uh in front of us, so I'll focus on some things that I think are really critical. First one is the data processing piece. Data processing is complicated in the sense, it's actually one of the most complicated pieces of the entire thing. Uh, and, and, and this is highly simplified, but I just wanna run through the most important pieces. So we basically have, well, the data, we have the data, the raw data, and this is, this is the hardest part. We have a pipeline that makes sense of this data, that processes this data, and then makes it available in data storage that is useful for the specific task for the agents. On the data side, problem is, well, This is not homogeneous data. You have data in many different formats, whether that is your PDF, a Word document, an image of a napkin somewhere. You have lots of different data. On top of that, you also have data that is super well structured and that may come from a relational database. And on the other hand, you have super unstructured data spread around your data lake. How do you make sense of that? And then the third dimension in in data is maturity or authority. So what, what does it mean? Is this something that I can trust? Is this something that, that can give me a definitive and authoritative answer? So since, since in this data sources, you have everything from specifications like maybe you have a 33 GPP specification to coming back to the napkin, some napkin of an idea that some engineers drew during lunch. It's different, different kind of data that you need to evaluate differently by the agent. So now you have these pieces moving over the, the, to the data processing part. Um, Ericson uses AWS batch and other services in here from AWS to, uh, to to, to make the most out of this data. Sometimes it's about integration to the data sources, but the more interesting parts parts are, how do I actually extract the metadata, because you, when, when you extract an image from a document that you have, you want to make sure that it, where does it, where does it come from? Does it fit to that document? Where is this document coming from? Can I resurface this later on? But you may also have more interesting tasks. I mean, obviously this is, this is engineering, so you'll have mathematical expressions, uh you will have tables of, of, of, of, of data that you need to extract, and you need to do that well. It's not good enough that you get close enough in these systems. So you have these different processing pipelines. In the end, they make that available to the agent through, for example, Amazon Bedrock Knowledge bases. If you need more of a like the, the, the traditional rack approach. But again, this can be combined with multi-step reasoning later on in the agents. Uh, on top of the, if you need a structured structured search through, through, through, through semantic search, you can use Amazon Bedrock knowledge bases, for example, on open search serverless. If you need more information about the entities, how they relate to each other, a graph database is a good choice, and that's also used in, in, in the system. And lastly, you may want to use some ad hoc analytics and uh or other structured queries, and you can use Amazon Redshift in there. Also, all of this can actually be behind the Amazon Bedrock knowledge base which makes, which can make it relatively easy for you to get started with that. But sometimes you need a little bit more customization as we've done in this case. All right, next piece. So we have the right data where we need it. There are other parts though, right? I already hinted that there is a custom large language model that has to be trained. Why does it have to be trained? Well, Ericsson provides or creates their own hardware, their own silicon. That silicon, well, you have your own proprietary frameworks and language on top of that. So going, helping your engineers with off the shelf large language models for for coding related tasks is just not going to do the trick. You need to be able to produce the right content and in that case, Eriksson realized it's important to train our own model on that, and this is what we have on this on this slide. I'm not going to go into super much detail here. This is yet another two day session if we want to go into detail, but suffice to say you need, you need the source code, you need the system documentation or code documentation for that. You need to run it through a, through a pre-processing pipeline to get high data quality, high quality data asset for training. Uh, in, in this case, this is done with continued pre-training, instruct fine tuning, and reinforcement learning with human feedback on top of that to get the best possible model that you can then host on, depending on the different models because there's there's always a change in what's the best model. Sometimes that works well on Bedrock agent on Bedrock custom model import, and other times it's hosted on on EC2. And then of course evaluation is king, so this is something that you, that you constantly, constantly evaluate and retrain. Moving on to the to to the last architecture slide where I want to just bring the whole thing a little bit together. How does it look at runtime? Let's start with the engineer again. The engineer, well, again, they want to use it where it makes sense, where they work, so either in a Dedicated front end or you use it um through Amazon Q Developer or Qiro. The the point of that is that, well, how do you utilize that? How do you actually integrate that? or use the MCP gateway that Ericsson has created for that. The MCP gateway can then surface tools for, for, for the, for the, for the assistance that we have on the left side here for the, for the network engineer. Uh, it can be agents, uh, and the MCP gateway is actually the, uh, the, the, the. Integration point where agents can talk to tools, agents can talk to agents, and even tools could talk to agents. Both of the tools that we have there, sometimes it's a tool that already exists and you just want to make it available, and you can do that through Agent Core gateway, for example. Sometimes it may be an original MCP server or a tool, and then you can run that on the Agent core runtime. The agent called runtime is mainly used, however, now for running agents, and here is just a selection of different agents that we have here. So maybe it is a 3GPP agent, maybe an agent that is more specialized in a specific feature. So you have LTE experts, for example, that that you're running there and plenty of other specialized agents. And then you have, for example, the code generation agent which then in turn, and you look to the right side of this slide, you'll see that we have integration with large language models and agent obviously is just a piece of code that needs to integrate with a large language model for orchestration. And here you can either fall fall back on the, the, the many models available on Bedrock Bedrock as a as a as a model provider, or you can use your custom models and again, Agent Core, the beauty of that is it's super flexible, so if you do have a On-prem model, for example, you can integrate that easily there as well. All right, last piece, agents need knowledge and they can integrate with many of the different knowledge layers here. It's relatively easy. Here, for example, if you use an agent framework like strands, you can relatively easily integrate directly into, into, for example, the Bedrock knowledge bases. Right, this wraps up, wraps up the the the main pieces of the architecture here, but then you have observability again. That's important. We said a couple of times now. Asian core observability makes this possible. It's compatible with open telemetry. It's very easy to look at the entire end to end stack of your of your application, and it makes a lot of sense if you integrate it with other APM. traces that you have maybe across the entire stack, and the entire stack means, well, there's applications, front and applications. There's lambdas here and there, there's containers running there and there, so you want to have an entire picture of that. That's why observ observability is important, specifically for agent traceability, you have agent core observability as a primitive. Then we also said identity is important, so you need to make sure that an agent can only do what they're supposed to do. um agent core identity makes it possible for you to manage inbound and outbound, um, outbound uh identities so you can make sure that the agents and the users can only get to what they need to and you can all go all the way down to the row level security, uh, row level authorization of your, of your data in your, in your data sources. And last but not least, memory. Memory wraps it up. I mean, there's getting, getting the right context for your agent is important and sometimes you want to transport it over sessions. So memory is one of the pieces that is used to to to transport uh preferences and topics across, across sessions. So agent core was core, right? So of course I have a I have a, I have a quote here, but I think it's better if you just maybe narrate this one, right, um, so thank you, Bastian, um, great to see the, the work of, of the lab kind of compressed, uh, I mean, we, we're, we're quite serious. I mean this is quite a big challenge to take sort of the complexity that we face when, when we look at, uh, mobile network in deployment, um, or in engineering, right, and all of that complexity and, and sort of how do we actually. You know, pull together all these pieces so that we, we tackle the challenge that I posed there, right, with the, the frontier of AI advancing much faster than the out of the box kind of industrial use cases of AI do, um, so we've been throwing requirements towards, uh, Bastian and the AWS team. Uh, I mean, I would say almost, I mean, to test you for maybe a little bit, no, not quite, but, but to really push the collaboration and to, to a new gear and so that ended up being sort of. Uh, one of the drivers for Agent Core that for, for us is the foundation where we can sort of say, OK, let's get help at this knowledge fusion layer and get, I mean, again, to, to kind of drink our own champagne to take complexity out, right? So we don't wanna be in the business of running Cubinetos clusters, much rather have that offloaded to, to the core platform and, um, and sort of agent core has been a, a key piece of how we, how we scale. Good. Thank you so much.